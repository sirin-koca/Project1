[
    {
        "id": "http://arxiv.org/abs/2104.05314v2",
        "title": "Machine learning and deep learning",
        "abstract": "  Today, intelligent systems that offer artificial intelligence capabilities\noften rely on machine learning. Machine learning describes the capacity of\nsystems to learn from problem-specific training data to automate the process of\nanalytical model building and solve associated tasks. Deep learning is a\nmachine learning concept based on artificial neural networks. For many\napplications, deep learning models outperform shallow machine learning models\nand traditional data analysis approaches. In this article, we summarize the\nfundamentals of machine learning and deep learning to generate a broader\nunderstanding of the methodical underpinning of current intelligent systems. In\nparticular, we provide a conceptual distinction between relevant terms and\nconcepts, explain the process of automated analytical model building through\nmachine learning and deep learning, and discuss the challenges that arise when\nimplementing such intelligent systems in the field of electronic markets and\nnetworked business. These naturally go beyond technological aspects and\nhighlight issues in human-machine interaction and artificial intelligence\nservitization.\n",
        "published": "2021",
        "authors": [
            "Christian Janiesch",
            "Patrick Zschech",
            "Kai Heinrich"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.02527v1",
        "title": "Visual Analytics for Explainable Deep Learning",
        "abstract": "  Recently, deep learning has been advancing the state of the art in artificial\nintelligence to a new level, and humans rely on artificial intelligence\ntechniques more than ever. However, even with such unprecedented advancements,\nthe lack of explanation regarding the decisions made by deep learning models\nand absence of control over their internal processes act as major drawbacks in\ncritical decision-making processes, such as precision medicine and law\nenforcement. In response, efforts are being made to make deep learning\ninterpretable and controllable by humans. In this paper, we review visual\nanalytics, information visualization, and machine learning perspectives\nrelevant to this aim, and discuss potential challenges and future research\ndirections.\n",
        "published": "2018",
        "authors": [
            "Jaegul Choo",
            "Shixia Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.06552v1",
        "title": "Introduction to intelligent computing unit 1",
        "abstract": "  This brief note highlights some basic concepts required toward understanding\nthe evolution of machine learning and deep learning models. The note starts\nwith an overview of artificial intelligence and its relationship to biological\nneuron that ultimately led to the evolution of todays intelligent models.\n",
        "published": "2017",
        "authors": [
            "Isa Inuwa-Dutse"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.12678v2",
        "title": "Deep Learning, Natural Language Processing, and Explainable Artificial\n  Intelligence in the Biomedical Domain",
        "abstract": "  In this article, we first give an introduction to artificial intelligence and\nits applications in biology and medicine in Section 1. Deep learning methods\nare then described in Section 2. We narrow down the focus of the study on\ntextual data in Section 3, where natural language processing and its\napplications in the biomedical domain are described. In Section 4, we give an\nintroduction to explainable artificial intelligence and discuss the importance\nof explainability of artificial intelligence systems, especially in the\nbiomedical domain.\n",
        "published": "2022",
        "authors": [
            "Milad Moradi",
            "Matthias Samwald"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.08169v1",
        "title": "Recent Advances in Deep Learning: An Overview",
        "abstract": "  Deep Learning is one of the newest trends in Machine Learning and Artificial\nIntelligence research. It is also one of the most popular scientific research\ntrends now-a-days. Deep learning methods have brought revolutionary advances in\ncomputer vision and machine learning. Every now and then, new and new deep\nlearning techniques are being born, outperforming state-of-the-art machine\nlearning and even existing deep learning techniques. In recent years, the world\nhas seen many major breakthroughs in this field. Since deep learning is\nevolving at a huge speed, its kind of hard to keep track of the regular\nadvances especially for new researchers. In this paper, we are going to briefly\ndiscuss about recent advances in Deep Learning for past few years.\n",
        "published": "2018",
        "authors": [
            "Matiur Rahman Minar",
            "Jibon Naher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.10744v1",
        "title": "Interpretable deep-learning models to help achieve the Sustainable\n  Development Goals",
        "abstract": "  We discuss our insights into interpretable artificial-intelligence (AI)\nmodels, and how they are essential in the context of developing ethical AI\nsystems, as well as data-driven solutions compliant with the Sustainable\nDevelopment Goals (SDGs). We highlight the potential of extracting\ntruly-interpretable models from deep-learning methods, for instance via\nsymbolic models obtained through inductive biases, to ensure a sustainable\ndevelopment of AI.\n",
        "published": "2021",
        "authors": [
            "Ricardo Vinuesa",
            "Beril Sirmacek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.11295v1",
        "title": "Artificial Intelligence Technology analysis using Artificial\n  Intelligence patent through Deep Learning model and vector space model",
        "abstract": "  Thanks to rapid development of artificial intelligence technology in recent\nyears, the current artificial intelligence technology is contributing to many\npart of society. Education, environment, medical care, military, tourism,\neconomy, politics, etc. are having a very large impact on society as a whole.\nFor example, in the field of education, there is an artificial intelligence\ntutoring system that automatically assigns tutors based on student's level. In\nthe field of economics, there are quantitative investment methods that\nautomatically analyze large amounts of data to find investment laws to create\ninvestment models or predict changes in financial markets. As such, artificial\nintelligence technology is being used in various fields. So, it is very\nimportant to know exactly what factors have an important influence on each\nfield of artificial intelligence technology and how the relationship between\neach field is connected. Therefore, it is necessary to analyze artificial\nintelligence technology in each field. In this paper, we analyze patent\ndocuments related to artificial intelligence technology. We propose a method\nfor keyword analysis within factors using artificial intelligence patent data\nsets for artificial intelligence technology analysis. This is a model that\nrelies on feature engineering based on deep learning model named KeyBERT, and\nusing vector space model. A case study of collecting and analyzing artificial\nintelligence patent data was conducted to show how the proposed model can be\napplied to real world problems.\n",
        "published": "2021",
        "authors": [
            "Yongmin Yoo",
            "Dongjin Lim",
            "Kyungsun Kim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.03593v2",
        "title": "Quadratic Suffices for Over-parametrization via Matrix Chernoff Bound",
        "abstract": "  We improve the over-parametrization size over two beautiful results [Li and\nLiang' 2018] and [Du, Zhai, Poczos and Singh' 2019] in deep learning theory.\n",
        "published": "2019",
        "authors": [
            "Zhao Song",
            "Xin Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.00523v2",
        "title": "Drug discovery with explainable artificial intelligence",
        "abstract": "  Deep learning bears promise for drug discovery, including advanced image\nanalysis, prediction of molecular structure and function, and automated\ngeneration of innovative chemical entities with bespoke properties. Despite the\ngrowing number of successful prospective applications, the underlying\nmathematical models often remain elusive to interpretation by the human mind.\nThere is a demand for 'explainable' deep learning methods to address the need\nfor a new narrative of the machine language of the molecular sciences. This\nreview summarizes the most prominent algorithmic concepts of explainable\nartificial intelligence, and dares a forecast of the future opportunities,\npotential applications, and remaining challenges.\n",
        "published": "2020",
        "authors": [
            "Jos\u00e9 Jim\u00e9nez-Luna",
            "Francesca Grisoni",
            "Gisbert Schneider"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.02767v1",
        "title": "Seq2Seq AI Chatbot with Attention Mechanism",
        "abstract": "  Intelligent Conversational Agent development using Artificial Intelligence or\nMachine Learning technique is an interesting problem in the field of Natural\nLanguage Processing. With the rise of deep learning, these models were quickly\nreplaced by end to end trainable neural networks.\n",
        "published": "2020",
        "authors": [
            "Abonia Sojasingarayar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.10025v2",
        "title": "Modern Deep Reinforcement Learning Algorithms",
        "abstract": "  Recent advances in Reinforcement Learning, grounded on combining classical\ntheoretical results with Deep Learning paradigm, led to breakthroughs in many\nartificial intelligence tasks and gave birth to Deep Reinforcement Learning\n(DRL) as a field of research. In this work latest DRL algorithms are reviewed\nwith a focus on their theoretical justification, practical limitations and\nobserved empirical properties.\n",
        "published": "2019",
        "authors": [
            "Sergey Ivanov",
            "Alexander D'yakonov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.15337v1",
        "title": "Homological Neural Networks: A Sparse Architecture for Multivariate\n  Complexity",
        "abstract": "  The rapid progress of Artificial Intelligence research came with the\ndevelopment of increasingly complex deep learning models, leading to growing\nchallenges in terms of computational complexity, energy efficiency and\ninterpretability. In this study, we apply advanced network-based information\nfiltering techniques to design a novel deep neural network unit characterized\nby a sparse higher-order graphical architecture built over the homological\nstructure of underlying data. We demonstrate its effectiveness in two\napplication domains which are traditionally challenging for deep learning:\ntabular data and time series regression problems. Results demonstrate the\nadvantages of this novel design which can tie or overcome the results of\nstate-of-the-art machine learning and deep learning models using only a\nfraction of parameters.\n",
        "published": "2023",
        "authors": [
            "Yuanrong Wang",
            "Antonio Briola",
            "Tomaso Aste"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.07987v2",
        "title": "Deep Learning",
        "abstract": "  Deep learning (DL) is a high dimensional data reduction technique for\nconstructing high-dimensional predictors in input-output models. DL is a form\nof machine learning that uses hierarchical layers of latent features. In this\narticle, we review the state-of-the-art of deep learning from a modeling and\nalgorithmic perspective. We provide a list of successful areas of applications\nin Artificial Intelligence (AI), Image Processing, Robotics and Automation.\nDeep learning is predictive in its nature rather then inferential and can be\nviewed as a black-box methodology for high-dimensional function estimation.\n",
        "published": "2018",
        "authors": [
            "Nicholas G. Polson",
            "Vadim O. Sokolov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.01653v2",
        "title": "Review of Deep Learning",
        "abstract": "  In recent years, China, the United States and other countries, Google and\nother high-tech companies have increased investment in artificial intelligence.\nDeep learning is one of the current artificial intelligence research's key\nareas. This paper analyzes and summarizes the latest progress and future\nresearch directions of deep learning. Firstly, three basic models of deep\nlearning are outlined, including multilayer perceptrons, convolutional neural\nnetworks, and recurrent neural networks. On this basis, we further analyze the\nemerging new models of convolution neural networks and recurrent neural\nnetworks. This paper then summarizes deep learning's applications in many areas\nof artificial intelligence, including speech processing, computer vision,\nnatural language processing and so on. Finally, this paper discusses the\nexisting problems of deep learning and gives the corresponding possible\nsolutions.\n",
        "published": "2018",
        "authors": [
            "Rong Zhang",
            "Weiping Li",
            "Tong Mo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.00631v1",
        "title": "Deep Learning: A Critical Appraisal",
        "abstract": "  Although deep learning has historical roots going back decades, neither the\nterm \"deep learning\" nor the approach was popular just over five years ago,\nwhen the field was reignited by papers such as Krizhevsky, Sutskever and\nHinton's now classic (2012) deep network model of Imagenet. What has the field\ndiscovered in the five subsequent years? Against a background of considerable\nprogress in areas such as speech recognition, image recognition, and game\nplaying, and considerable enthusiasm in the popular press, I present ten\nconcerns for deep learning, and suggest that deep learning must be supplemented\nby other techniques if we are to reach artificial general intelligence.\n",
        "published": "2018",
        "authors": [
            "Gary Marcus"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.06540v1",
        "title": "Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try\n  After Deep Learning",
        "abstract": "  We found an easy and quick post-learning method named \"Icing on the Cake\" to\nenhance a classification performance in deep learning. The method is that we\ntrain only the final classifier again after an ordinary training is done.\n",
        "published": "2018",
        "authors": [
            "Tomohiko Konno",
            "Michiaki Iwazume"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.06622v1",
        "title": "Concept-Oriented Deep Learning: Generative Concept Representations",
        "abstract": "  Generative concept representations have three major advantages over\ndiscriminative ones: they can represent uncertainty, they support integration\nof learning and reasoning, and they are good for unsupervised and\nsemi-supervised learning. We discuss probabilistic and generative deep\nlearning, which generative concept representations are based on, and the use of\nvariational autoencoders and generative adversarial networks for learning\ngenerative concept representations, particularly for concepts whose data are\nsequences, structured data or graphs.\n",
        "published": "2018",
        "authors": [
            "Daniel T. Chang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.12961v1",
        "title": "Next-Day Bitcoin Price Forecast Based on Artificial intelligence Methods",
        "abstract": "  In recent years, Bitcoin price prediction has attracted the interest of\nresearchers and investors. However, the accuracy of previous studies is not\nwell enough. Machine learning and deep learning methods have been proved to\nhave strong prediction ability in this area. This paper proposed a method\ncombined with Ensemble Empirical Mode Decomposition (EEMD) and a deep learning\nmethod called long short-term memory (LSTM) to research the problem of next-day\nBitcoin price forecast.\n",
        "published": "2021",
        "authors": [
            "Liping Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.03735v1",
        "title": "Security of Deep Learning Methodologies: Challenges and Opportunities",
        "abstract": "  Despite the plethora of studies about security vulnerabilities and defenses\nof deep learning models, security aspects of deep learning methodologies, such\nas transfer learning, have been rarely studied. In this article, we highlight\nthe security challenges and research opportunities of these methodologies,\nfocusing on vulnerabilities and attacks unique to them.\n",
        "published": "2019",
        "authors": [
            "Shahbaz Rezaei",
            "Xin Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1708.08296v1",
        "title": "Explainable Artificial Intelligence: Understanding, Visualizing and\n  Interpreting Deep Learning Models",
        "abstract": "  With the availability of large databases and recent improvements in deep\nlearning methodology, the performance of AI systems is reaching or even\nexceeding the human level on an increasing number of complex tasks. Impressive\nexamples of this development can be found in domains such as image\nclassification, sentiment analysis, speech understanding or strategic game\nplaying. However, because of their nested non-linear structure, these highly\nsuccessful machine learning and artificial intelligence models are usually\napplied in a black box manner, i.e., no information is provided about what\nexactly makes them arrive at their predictions. Since this lack of transparency\ncan be a major drawback, e.g., in medical applications, the development of\nmethods for visualizing, explaining and interpreting deep learning models has\nrecently attracted increasing attention. This paper summarizes recent\ndevelopments in this field and makes a plea for more interpretability in\nartificial intelligence. Furthermore, it presents two approaches to explaining\npredictions of deep learning models, one method which computes the sensitivity\nof the prediction with respect to changes in the input and one approach which\nmeaningfully decomposes the decision in terms of the input variables. These\nmethods are evaluated on three classification tasks.\n",
        "published": "2017",
        "authors": [
            "Wojciech Samek",
            "Thomas Wiegand",
            "Klaus-Robert M\u00fcller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.03120v1",
        "title": "The Landscape of Modern Machine Learning: A Review of Machine,\n  Distributed and Federated Learning",
        "abstract": "  With the advance of the powerful heterogeneous, parallel and distributed\ncomputing systems and ever increasing immense amount of data, machine learning\nhas become an indispensable part of cutting-edge technology, scientific\nresearch and consumer products. In this study, we present a review of modern\nmachine and deep learning. We provide a high-level overview for the latest\nadvanced machine learning algorithms, applications, and frameworks. Our\ndiscussion encompasses parallel distributed learning, deep learning as well as\nfederated learning. As a result, our work serves as an introductory text to the\nvast field of modern machine learning.\n",
        "published": "2023",
        "authors": [
            "Omer Subasi",
            "Oceane Bel",
            "Joseph Manzano",
            "Kevin Barker"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.00225v2",
        "title": "The Past, Current, and Future of Neonatal Intensive Care Units with\n  Artificial Intelligence",
        "abstract": "  Machine learning and deep learning are two subsets of artificial intelligence\nthat involve teaching computers to learn and make decisions from any sort of\ndata. Most recent developments in artificial intelligence are coming from deep\nlearning, which has proven revolutionary in almost all fields, from computer\nvision to health sciences. The effects of deep learning in medicine have\nchanged the conventional ways of clinical application significantly. Although\nsome sub-fields of medicine, such as pediatrics, have been relatively slow in\nreceiving the critical benefits of deep learning, related research in\npediatrics has started to accumulate to a significant level, too. Hence, in\nthis paper, we review recently developed machine learning and deep\nlearning-based solutions for neonatology applications. We systematically\nevaluate the roles of both classical machine learning and deep learning in\nneonatology applications, define the methodologies, including algorithmic\ndevelopments, and describe the remaining challenges in the assessment of\nneonatal diseases by using PRISMA 2020 guidelines. To date, the primary areas\nof focus in neonatology regarding AI applications have included survival\nanalysis, neuroimaging, analysis of vital parameters and biosignals, and\nretinopathy of prematurity diagnosis. We have categorically summarized 106\nresearch articles from 1996 to 2022 and discussed their pros and cons,\nrespectively. In this systematic review, we aimed to further enhance the\ncomprehensiveness of the study. We also discuss possible directions for new AI\nmodels and the future of neonatology with the rising power of AI, suggesting\nroadmaps for the integration of AI into neonatal intensive care units.\n",
        "published": "2023",
        "authors": [
            "Elif Keles",
            "Ulas Bagci"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.05889v1",
        "title": "Comparison of Deep Learning and the Classical Machine Learning Algorithm\n  for the Malware Detection",
        "abstract": "  Recently, Deep Learning has been showing promising results in various\nArtificial Intelligence applications like image recognition, natural language\nprocessing, language modeling, neural machine translation, etc. Although, in\ngeneral, it is computationally more expensive as compared to classical machine\nlearning techniques, their results are found to be more effective in some\ncases. Therefore, in this paper, we investigated and compared one of the Deep\nLearning Architecture called Deep Neural Network (DNN) with the classical\nRandom Forest (RF) machine learning algorithm for the malware classification.\nWe studied the performance of the classical RF and DNN with 2, 4 & 7 layers\narchitectures with the four different feature sets, and found that irrespective\nof the features inputs, the classical RF accuracy outperforms the DNN.\n",
        "published": "2018",
        "authors": [
            "Mohit Sewak",
            "Sanjay K. Sahay",
            "Hemant Rathore"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.08908v1",
        "title": "Techniques for Automated Machine Learning",
        "abstract": "  Automated machine learning (AutoML) aims to find optimal machine learning\nsolutions automatically given a machine learning problem. It could release the\nburden of data scientists from the multifarious manual tuning process and\nenable the access of domain experts to the off-the-shelf machine learning\nsolutions without extensive experience. In this paper, we review the current\ndevelopments of AutoML in terms of three categories, automated feature\nengineering (AutoFE), automated model and hyperparameter learning (AutoMHL),\nand automated deep learning (AutoDL). State-of-the-art techniques adopted in\nthe three categories are presented, including Bayesian optimization,\nreinforcement learning, evolutionary algorithm, and gradient-based approaches.\nWe summarize popular AutoML frameworks and conclude with current open\nchallenges of AutoML.\n",
        "published": "2019",
        "authors": [
            "Yi-Wei Chen",
            "Qingquan Song",
            "Xia Hu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.11236v4",
        "title": "Psychophysical Machine Learning",
        "abstract": "  The Weber Fechner Law of psychophysics observes that human perception is\nlogarithmic in the stimulus. We present an algorithm for incorporating the\nWeber Fechner law into loss functions for machine learning, and use the\nalgorithm to enhance the performance of deep learning networks.\n",
        "published": "2022",
        "authors": [
            "B. N. Kausik"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.02354v2",
        "title": "Geometrization of deep networks for the interpretability of deep\n  learning systems",
        "abstract": "  How to understand deep learning systems remains an open problem. In this\npaper we propose that the answer may lie in the geometrization of deep\nnetworks. Geometrization is a bridge to connect physics, geometry, deep network\nand quantum computation and this may result in a new scheme to reveal the rule\nof the physical world. By comparing the geometry of image matching and deep\nnetworks, we show that geometrization of deep networks can be used to\nunderstand existing deep learning systems and it may also help to solve the\ninterpretability problem of deep learning systems.\n",
        "published": "2019",
        "authors": [
            "Xiao Dong",
            "Ling Zhou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.06292v2",
        "title": "A short review on Applications of Deep learning for Cyber security",
        "abstract": "  Deep learning is an advanced model of traditional machine learning. This has\nthe capability to extract optimal feature representation from raw input\nsamples. This has been applied towards various use cases in cyber security such\nas intrusion detection, malware classification, android malware detection, spam\nand phishing detection and binary analysis. This paper outlines the survey of\nall the works related to deep learning based solutions for various cyber\nsecurity use cases. Keywords: Deep learning, intrusion detection, malware\ndetection, Android malware detection, spam & phishing detection, traffic\nanalysis, binary analysis.\n",
        "published": "2018",
        "authors": [
            "Mohammed Harun Babu R",
            "Vinayakumar R",
            "Soman KP"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.07201v3",
        "title": "Holistic Adversarial Robustness of Deep Learning Models",
        "abstract": "  Adversarial robustness studies the worst-case performance of a machine\nlearning model to ensure safety and reliability. With the proliferation of\ndeep-learning-based technology, the potential risks associated with model\ndevelopment and deployment can be amplified and become dreadful\nvulnerabilities. This paper provides a comprehensive overview of research\ntopics and foundational principles of research methods for adversarial\nrobustness of deep learning models, including attacks, defenses, verification,\nand novel applications.\n",
        "published": "2022",
        "authors": [
            "Pin-Yu Chen",
            "Sijia Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.10345v1",
        "title": "Artificial Intelligence Approaches",
        "abstract": "  Artificial Intelligence (AI) has received tremendous attention from academia,\nindustry, and the general public in recent years. The integration of geography\nand AI, or GeoAI, provides novel approaches for addressing a variety of\nproblems in the natural environment and our human society. This entry briefly\nreviews the recent development of AI with a focus on machine learning and deep\nlearning approaches. We discuss the integration of AI with geography and\nparticularly geographic information science, and present a number of GeoAI\napplications and possible future directions.\n",
        "published": "2019",
        "authors": [
            "Yingjie Hu",
            "Wenwen Li",
            "Dawn Wright",
            "Orhun Aydin",
            "Daniel Wilson",
            "Omar Maher",
            "Mansour Raad"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.00093v2",
        "title": "Imagine All the People: Citizen Science, Artificial Intelligence, and\n  Computational Research",
        "abstract": "  Machine learning, artificial intelligence, and deep learning have advanced\nsignificantly over the past decade. Nonetheless, humans possess unique\nabilities such as creativity, intuition, context and abstraction, analytic\nproblem solving, and detecting unusual events. To successfully tackle pressing\nscientific and societal challenges, we need the complementary capabilities of\nboth humans and machines. The Federal Government could accelerate its\npriorities on multiple fronts through judicious integration of citizen science\nand crowdsourcing with artificial intelligence (AI), Internet of Things (IoT),\nand cloud strategies.\n",
        "published": "2021",
        "authors": [
            "Lea A. Shanley",
            "Lucy Fortson",
            "Tanya Berger-Wolf",
            "Kevin Crowston",
            "Pietro Michelucci"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.16310v1",
        "title": "Crime Prediction Using Machine Learning and Deep Learning: A Systematic\n  Review and Future Directions",
        "abstract": "  Predicting crime using machine learning and deep learning techniques has\ngained considerable attention from researchers in recent years, focusing on\nidentifying patterns and trends in crime occurrences. This review paper\nexamines over 150 articles to explore the various machine learning and deep\nlearning algorithms applied to predict crime. The study provides access to the\ndatasets used for crime prediction by researchers and analyzes prominent\napproaches applied in machine learning and deep learning algorithms to predict\ncrime, offering insights into different trends and factors related to criminal\nactivities. Additionally, the paper highlights potential gaps and future\ndirections that can enhance the accuracy of crime prediction. Finally, the\ncomprehensive overview of research discussed in this paper on crime prediction\nusing machine learning and deep learning approaches serves as a valuable\nreference for researchers in this field. By gaining a deeper understanding of\ncrime prediction techniques, law enforcement agencies can develop strategies to\nprevent and respond to criminal activities more effectively.\n",
        "published": "2023",
        "authors": [
            "Varun Mandalapu",
            "Lavanya Elluri",
            "Piyush Vyas",
            "Nirmalya Roy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.05255v1",
        "title": "DeepFair: Deep Learning for Improving Fairness in Recommender Systems",
        "abstract": "  The lack of bias management in Recommender Systems leads to minority groups\nreceiving unfair recommendations. Moreover, the trade-off between equity and\nprecision makes it difficult to obtain recommendations that meet both criteria.\nHere we propose a Deep Learning based Collaborative Filtering algorithm that\nprovides recommendations with an optimum balance between fairness and accuracy\nwithout knowing demographic information about the users. Experimental results\nshow that it is possible to make fair recommendations without losing a\nsignificant proportion of accuracy.\n",
        "published": "2020",
        "authors": [
            "Jes\u00fas Bobadilla",
            "Ra\u00fal Lara-Cabrera",
            "\u00c1ngel Gonz\u00e1lez-Prieto",
            "Fernando Ortega"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.07643v1",
        "title": "A Review of the Convergence of 5G/6G Architecture and Deep Learning",
        "abstract": "  The convergence of 5G architecture and deep learning has gained a lot of\nresearch interests in both the fields of wireless communication and artificial\nintelligence. This is because deep learning technologies have been identified\nto be the potential driver of the 5G technologies, that make up the 5G\narchitecture. Hence, there have been extensive surveys on the convergence of 5G\narchitecture and deep learning. However, most of the existing survey papers\nmainly focused on how deep learning can converge with a specific 5G technology,\nthus, not covering the full spectrum of the 5G architecture. Although there is\na recent survey paper that appears to be robust, a review of that paper shows\nthat it is not well structured to specifically cover the convergence of deep\nlearning and the 5G technologies. Hence, this paper provides a robust overview\nof the convergence of the key 5G technologies and deep learning. The challenges\nfaced by such convergence are discussed. In addition, a brief overview of the\nfuture 6G architecture, and how it can converge with deep learning is also\ndiscussed.\n",
        "published": "2022",
        "authors": [
            "Olusola T. Odeyomi",
            "Olubiyi O. Akintade",
            "Temitayo O. Olowu",
            "Gergely Zaruba"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.12150v2",
        "title": "Design of a dynamic and self adapting system, supported with artificial\n  intelligence, machine learning and real time intelligence for predictive\n  cyber risk analytics in extreme environments, cyber risk in the colonisation\n  of Mars",
        "abstract": "  Multiple governmental agencies and private organisations have made\ncommitments for the colonisation of Mars. Such colonisation requires complex\nsystems and infrastructure that could be very costly to repair or replace in\ncases of cyber attacks. This paper surveys deep learning algorithms, IoT cyber\nsecurity and risk models, and established mathematical formulas to identify the\nbest approach for developing a dynamic and self adapting system for predictive\ncyber risk analytics supported with Artificial Intelligence and Machine\nLearning and real time intelligence in edge computing. The paper presents a new\nmathematical approach for integrating concepts for cognition engine design,\nedge computing and Artificial Intelligence and Machine Learning to automate\nanomaly detection. This engine instigates a step change by applying Artificial\nIntelligence and Machine Learning embedded at the edge of IoT networks, to\ndeliver safe and functional real time intelligence for predictive cyber risk\nanalytics. This will enhance capacities for risk analytics and assists in the\ncreation of a comprehensive and systematic understanding of the opportunities\nand threats that arise when edge computing nodes are deployed, and when\nArtificial Intelligence and Machine Learning technologies are migrated to the\nperiphery of the internet and into local IoT networks.\n",
        "published": "2020",
        "authors": [
            "Petar Radanliev",
            "David De Roure",
            "Kevin Page",
            "Max Van Kleek",
            "Omar Santos",
            "La Treall Maddox",
            "Pete Burnap",
            "Eirini Anthi",
            "Carsten Maple"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.02825v2",
        "title": "A Survey on Deep Learning for Human Mobility",
        "abstract": "  The study of human mobility is crucial due to its impact on several aspects\nof our society, such as disease spreading, urban planning, well-being,\npollution, and more. The proliferation of digital mobility data, such as phone\nrecords, GPS traces, and social media posts, combined with the predictive power\nof artificial intelligence, triggered the application of deep learning to human\nmobility. Existing surveys focus on single tasks, data sources, mechanistic or\ntraditional machine learning approaches, while a comprehensive description of\ndeep learning solutions is missing. This survey provides a taxonomy of mobility\ntasks, a discussion on the challenges related to each task and how deep\nlearning may overcome the limitations of traditional models, a description of\nthe most relevant solutions to the mobility tasks described above and the\nrelevant challenges for the future. Our survey is a guide to the leading deep\nlearning solutions to next-location prediction, crowd flow prediction,\ntrajectory generation, and flow generation. At the same time, it helps deep\nlearning scientists and practitioners understand the fundamental concepts and\nthe open challenges of the study of human mobility.\n",
        "published": "2020",
        "authors": [
            "Massimiliano Luca",
            "Gianni Barlacchi",
            "Bruno Lepri",
            "Luca Pappalardo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.17473v2",
        "title": "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU",
        "abstract": "  Deep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.\n",
        "published": "2023",
        "authors": [
            "Farhad Mortezapour Shiri",
            "Thinagaran Perumal",
            "Norwati Mustapha",
            "Raihani Mohamed"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.08717v1",
        "title": "Deep learning in radiology: an overview of the concepts and a survey of\n  the state of the art",
        "abstract": "  Deep learning is a branch of artificial intelligence where networks of simple\ninterconnected units are used to extract patterns from data in order to solve\ncomplex problems. Deep learning algorithms have shown groundbreaking\nperformance in a variety of sophisticated tasks, especially those related to\nimages. They have often matched or exceeded human performance. Since the\nmedical field of radiology mostly relies on extracting useful information from\nimages, it is a very natural application area for deep learning, and research\nin this area has rapidly grown in recent years. In this article, we review the\nclinical reality of radiology and discuss the opportunities for application of\ndeep learning algorithms. We also introduce basic concepts of deep learning\nincluding convolutional neural networks. Then, we present a survey of the\nresearch in deep learning applied to radiology. We organize the studies by the\ntypes of specific tasks that they attempt to solve and review the broad range\nof utilized deep learning algorithms. Finally, we briefly discuss opportunities\nand challenges for incorporating deep learning in the radiology practice of the\nfuture.\n",
        "published": "2018",
        "authors": [
            "Maciej A. Mazurowski",
            "Mateusz Buda",
            "Ashirbani Saha",
            "Mustafa R. Bashir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.05127v2",
        "title": "Model Complexity of Deep Learning: A Survey",
        "abstract": "  Model complexity is a fundamental problem in deep learning. In this paper we\nconduct a systematic overview of the latest studies on model complexity in deep\nlearning. Model complexity of deep learning can be categorized into expressive\ncapacity and effective model complexity. We review the existing studies on\nthose two categories along four important factors, including model framework,\nmodel size, optimization process and data complexity. We also discuss the\napplications of deep learning model complexity including understanding model\ngeneralization, model optimization, and model selection and design. We conclude\nby proposing several interesting future directions.\n",
        "published": "2021",
        "authors": [
            "Xia Hu",
            "Lingyang Chu",
            "Jian Pei",
            "Weiqing Liu",
            "Jiang Bian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.03691v1",
        "title": "Feature Mining for Encrypted Malicious Traffic Detection with Deep\n  Learning and Other Machine Learning Algorithms",
        "abstract": "  The popularity of encryption mechanisms poses a great challenge to malicious\ntraffic detection. The reason is traditional detection techniques cannot work\nwithout the decryption of encrypted traffic. Currently, research on encrypted\nmalicious traffic detection without decryption has focused on feature\nextraction and the choice of machine learning or deep learning algorithms. In\nthis paper, we first provide an in-depth analysis of traffic features and\ncompare different state-of-the-art traffic feature creation approaches, while\nproposing a novel concept for encrypted traffic feature which is specifically\ndesigned for encrypted malicious traffic analysis. In addition, we propose a\nframework for encrypted malicious traffic detection. The framework is a\ntwo-layer detection framework which consists of both deep learning and\ntraditional machine learning algorithms. Through comparative experiments, it\noutperforms classical deep learning and traditional machine learning\nalgorithms, such as ResNet and Random Forest. Moreover, to provide sufficient\ntraining data for the deep learning model, we also curate a dataset composed\nentirely of public datasets. The composed dataset is more comprehensive than\nusing any public dataset alone. Lastly, we discuss the future directions of\nthis research.\n",
        "published": "2023",
        "authors": [
            "Zihao Wang",
            "Vrizlynn L. L. Thing"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.11241v1",
        "title": "Data Science and Digital Systems: The 3Ds of Machine Learning Systems\n  Design",
        "abstract": "  Machine learning solutions, in particular those based on deep learning\nmethods, form an underpinning of the current revolution in \"artificial\nintelligence\" that has dominated popular press headlines and is having a\nsignificant influence on the wider tech agenda. Here we give an overview of the\n3Ds of ML systems design: Data, Design and Deployment. By considering the 3Ds\nwe can move towards \\emph{data first} design.\n",
        "published": "2019",
        "authors": [
            "Neil D. Lawrence"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.00351v1",
        "title": "Advancing from Predictive Maintenance to Intelligent Maintenance with AI\n  and IIoT",
        "abstract": "  As Artificial Intelligent (AI) technology advances and increasingly large\namounts of data become readily available via various Industrial Internet of\nThings (IIoT) projects, we evaluate the state of the art of predictive\nmaintenance approaches and propose our innovative framework to improve the\ncurrent practice. The paper first reviews the evolution of reliability\nmodelling technology in the past 90 years and discusses major technologies\ndeveloped in industry and academia. We then introduce the next generation\nmaintenance framework - Intelligent Maintenance, and discuss its key\ncomponents. This AI and IIoT based Intelligent Maintenance framework is\ncomposed of (1) latest machine learning algorithms including probabilistic\nreliability modelling with deep learning, (2) real-time data collection,\ntransfer, and storage through wireless smart sensors, (3) Big Data\ntechnologies, (4) continuously integration and deployment of machine learning\nmodels, (5) mobile device and AR/VR applications for fast and better\ndecision-making in the field. Particularly, we proposed a novel probabilistic\ndeep learning reliability modelling approach and demonstrate it in the Turbofan\nEngine Degradation Dataset.\n",
        "published": "2020",
        "authors": [
            "Haining Zheng",
            "Antonio R. Paiva",
            "Chris S. Gurciullo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.06471v2",
        "title": "Towards Explainable Deep Learning for Credit Lending: A Case Study",
        "abstract": "  Deep learning adoption in the financial services industry has been limited\ndue to a lack of model interpretability. However, several techniques have been\nproposed to explain predictions made by a neural network. We provide an initial\ninvestigation into these techniques for the assessment of credit risk with\nneural networks.\n",
        "published": "2018",
        "authors": [
            "Ceena Modarres",
            "Mark Ibrahim",
            "Melissa Louie",
            "John Paisley"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.10729v1",
        "title": "PAMS: Platform for Artificial Market Simulations",
        "abstract": "  This paper presents a new artificial market simulation platform, PAMS:\nPlatform for Artificial Market Simulations. PAMS is developed as a Python-based\nsimulator that is easily integrated with deep learning and enabling various\nsimulation that requires easy users' modification. In this paper, we\ndemonstrate PAMS effectiveness through a study using agents predicting future\nprices by deep learning.\n",
        "published": "2023",
        "authors": [
            "Masanori Hirano",
            "Ryosuke Takata",
            "Kiyoshi Izumi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.10317v2",
        "title": "Introduction to Neural Network Verification",
        "abstract": "  Deep learning has transformed the way we think of software and what it can\ndo. But deep neural networks are fragile and their behaviors are often\nsurprising. In many settings, we need to provide formal guarantees on the\nsafety, security, correctness, or robustness of neural networks. This book\ncovers foundational ideas from formal verification and their adaptation to\nreasoning about neural networks and deep learning.\n",
        "published": "2021",
        "authors": [
            "Aws Albarghouthi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.00699v1",
        "title": "Geometric Deep Learning: a Temperature Based Analysis of Graph Neural\n  Networks",
        "abstract": "  We examine a Geometric Deep Learning model as a thermodynamic system treating\nthe weights as non-quantum and non-relativistic particles. We employ the notion\nof temperature previously defined in [7] and study it in the various layers for\nGCN and GAT models. Potential future applications of our findings are\ndiscussed.\n",
        "published": "2023",
        "authors": [
            "M. Lapenna",
            "F. Faglioni",
            "F. Zanchetta",
            "R. Fioresi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.04806v1",
        "title": "The Unreasonable Effectiveness of Deep Learning in Artificial\n  Intelligence",
        "abstract": "  Deep learning networks have been trained to recognize speech, caption\nphotographs and translate text between languages at high levels of performance.\nAlthough applications of deep learning networks to real world problems have\nbecome ubiquitous, our understanding of why they are so effective is lacking.\nThese empirical results should not be possible according to sample complexity\nin statistics and non-convex optimization theory. However, paradoxes in the\ntraining and effectiveness of deep learning networks are being investigated and\ninsights are being found in the geometry of high-dimensional spaces. A\nmathematical theory of deep learning would illuminate how they function, allow\nus to assess the strengths and weaknesses of different network architectures\nand lead to major improvements. Deep learning has provided natural ways for\nhumans to communicate with digital devices and is foundational for building\nartificial general intelligence. Deep learning was inspired by the architecture\nof the cerebral cortex and insights into autonomy and general intelligence may\nbe found in other brain regions that are essential for planning and survival,\nbut major breakthroughs will be needed to achieve these goals.\n",
        "published": "2020",
        "authors": [
            "Terrence J. Sejnowski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.08843v2",
        "title": "Fairness in Deep Learning: A Computational Perspective",
        "abstract": "  Deep learning is increasingly being used in high-stake decision making\napplications that affect individual lives. However, deep learning models might\nexhibit algorithmic discrimination behaviors with respect to protected groups,\npotentially posing negative impacts on individuals and society. Therefore,\nfairness in deep learning has attracted tremendous attention recently. We\nprovide a review covering recent progresses to tackle algorithmic fairness\nproblems of deep learning from the computational perspective. Specifically, we\nshow that interpretability can serve as a useful ingredient to diagnose the\nreasons that lead to algorithmic discrimination. We also discuss fairness\nmitigation approaches categorized according to three stages of deep learning\nlife-cycle, aiming to push forward the area of fairness in deep learning and\nbuild genuinely fair and reliable deep learning systems.\n",
        "published": "2019",
        "authors": [
            "Mengnan Du",
            "Fan Yang",
            "Na Zou",
            "Xia Hu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.11819v1",
        "title": "When Machine Learning Meets Privacy: A Survey and Outlook",
        "abstract": "  The newly emerged machine learning (e.g. deep learning) methods have become a\nstrong driving force to revolutionize a wide range of industries, such as smart\nhealthcare, financial technology, and surveillance systems. Meanwhile, privacy\nhas emerged as a big concern in this machine learning-based artificial\nintelligence era. It is important to note that the problem of privacy\npreservation in the context of machine learning is quite different from that in\ntraditional data privacy protection, as machine learning can act as both friend\nand foe. Currently, the work on the preservation of privacy and machine\nlearning (ML) is still in an infancy stage, as most existing solutions only\nfocus on privacy problems during the machine learning process. Therefore, a\ncomprehensive study on the privacy preservation problems and machine learning\nis required. This paper surveys the state of the art in privacy issues and\nsolutions for machine learning. The survey covers three categories of\ninteractions between privacy and machine learning: (i) private machine\nlearning, (ii) machine learning aided privacy protection, and (iii) machine\nlearning-based privacy attack and corresponding protection schemes. The current\nresearch progress in each category is reviewed and the key challenges are\nidentified. Finally, based on our in-depth analysis of the area of privacy and\nmachine learning, we point out future research directions in this field.\n",
        "published": "2020",
        "authors": [
            "Bo Liu",
            "Ming Ding",
            "Sina Shaham",
            "Wenny Rahayu",
            "Farhad Farokhi",
            "Zihuai Lin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.03488v1",
        "title": "A Comparison of Methods for Neural Network Aggregation",
        "abstract": "  Deep learning has been successful in the theoretical aspect. For deep\nlearning to succeed in industry, we need to have algorithms capable of handling\nmany inconsistencies appearing in real data. These inconsistencies can have\nlarge effects on the implementation of a deep learning algorithm. Artificial\nIntelligence is currently changing the medical industry. However, receiving\nauthorization to use medical data for training machine learning algorithms is a\nhuge hurdle. A possible solution is sharing the data without sharing the\npatient information. We propose a multi-party computation protocol for the deep\nlearning algorithm. The protocol enables to conserve both the privacy and the\nsecurity of the training data. Three approaches of neural networks assembly are\nanalyzed: transfer learning, average ensemble learning, and series network\nlearning. The results are compared to approaches based on data-sharing in\ndifferent experiments. We analyze the security issues of the proposed protocol.\nAlthough the analysis is based on medical data, the results of multi-party\ncomputation of machine learning training are theoretical and can be implemented\nin multiple research areas.\n",
        "published": "2023",
        "authors": [
            "John Pomerat",
            "Aviv Segev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.01943v1",
        "title": "Machine Learning: Algorithms, Models, and Applications",
        "abstract": "  Recent times are witnessing rapid development in machine learning algorithm\nsystems, especially in reinforcement learning, natural language processing,\ncomputer and robot vision, image processing, speech, and emotional processing\nand understanding. In tune with the increasing importance and relevance of\nmachine learning models, algorithms, and their applications, and with the\nemergence of more innovative uses cases of deep learning and artificial\nintelligence, the current volume presents a few innovative research works and\ntheir applications in real world, such as stock trading, medical and healthcare\nsystems, and software automation. The chapters in the book illustrate how\nmachine learning and deep learning algorithms and models are designed,\noptimized, and deployed. The volume will be useful for advanced graduate and\ndoctoral students, researchers, faculty members of universities, practicing\ndata scientists and data engineers, professionals, and consultants working on\nthe broad areas of machine learning, deep learning, and artificial\nintelligence.\n",
        "published": "2022",
        "authors": [
            "Jaydip Sen",
            "Sidra Mehtab",
            "Rajdeep Sen",
            "Abhishek Dutta",
            "Pooja Kherwa",
            "Saheel Ahmed",
            "Pranay Berry",
            "Sahil Khurana",
            "Sonali Singh",
            "David W. W Cadotte",
            "David W. Anderson",
            "Kalum J. Ost",
            "Racheal S. Akinbo",
            "Oladunni A. Daramola",
            "Bongs Lainjo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1602.00203v1",
        "title": "Greedy Deep Dictionary Learning",
        "abstract": "  In this work we propose a new deep learning tool called deep dictionary\nlearning. Multi-level dictionaries are learnt in a greedy fashion, one layer at\na time. This requires solving a simple (shallow) dictionary learning problem,\nthe solution to this is well known. We apply the proposed technique on some\nbenchmark deep learning datasets. We compare our results with other deep\nlearning tools like stacked autoencoder and deep belief network; and state of\nthe art supervised dictionary learning tools like discriminative KSVD and label\nconsistent KSVD. Our method yields better results than all.\n",
        "published": "2016",
        "authors": [
            "Snigdha Tariyal",
            "Angshul Majumdar",
            "Richa Singh",
            "Mayank Vatsa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.03614v1",
        "title": "Gradient Descent based Optimization Algorithms for Deep Learning Models\n  Training",
        "abstract": "  In this paper, we aim at providing an introduction to the gradient descent\nbased optimization algorithms for learning deep neural network models. Deep\nlearning models involving multiple nonlinear projection layers are very\nchallenging to train. Nowadays, most of the deep learning model training still\nrelies on the back propagation algorithm actually. In back propagation, the\nmodel variables will be updated iteratively until convergence with gradient\ndescent based optimization algorithms. Besides the conventional vanilla\ngradient descent algorithm, many gradient descent variants have also been\nproposed in recent years to improve the learning performance, including\nMomentum, Adagrad, Adam, Gadam, etc., which will all be introduced in this\npaper respectively.\n",
        "published": "2019",
        "authors": [
            "Jiawei Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.08618v1",
        "title": "Compositional properties of emergent languages in deep learning",
        "abstract": "  Recent findings in multi-agent deep learning systems point towards the\nemergence of compositional languages. These claims are often made without exact\nanalysis or testing of the language. In this work, we analyze the emergent\nlanguage resulting from two different cooperative multi-agent game with more\nexact measures for compositionality. Our findings suggest that solutions found\nby deep learning models are often lacking the ability to reason on an abstract\nlevel therefore failing to generalize the learned knowledge to out of the\ntraining distribution examples. Strategies for testing compositional capacities\nand emergence of human-level concepts are discussed.\n",
        "published": "2020",
        "authors": [
            "Bence Keresztury",
            "Elia Bruni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.14564v1",
        "title": "Evaluating Resilience of Encrypted Traffic Classification Against\n  Adversarial Evasion Attacks",
        "abstract": "  Machine learning and deep learning algorithms can be used to classify\nencrypted Internet traffic. Classification of encrypted traffic can become more\nchallenging in the presence of adversarial attacks that target the learning\nalgorithms. In this paper, we focus on investigating the effectiveness of\ndifferent evasion attacks and see how resilient machine and deep learning\nalgorithms are. Namely, we test C4.5 Decision Tree, K-Nearest Neighbor (KNN),\nArtificial Neural Network (ANN), Convolutional Neural Networks (CNN) and\nRecurrent Neural Networks (RNN). In most of our experimental results, deep\nlearning shows better resilience against the adversarial samples in comparison\nto machine learning. Whereas, the impact of the attack varies depending on the\ntype of attack.\n",
        "published": "2021",
        "authors": [
            "Ramy Maarouf",
            "Danish Sattar",
            "Ashraf Matrawy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1610.01178v1",
        "title": "A Tour of TensorFlow",
        "abstract": "  Deep learning is a branch of artificial intelligence employing deep neural\nnetwork architectures that has significantly advanced the state-of-the-art in\ncomputer vision, speech recognition, natural language processing and other\ndomains. In November 2015, Google released $\\textit{TensorFlow}$, an open\nsource deep learning software library for defining, training and deploying\nmachine learning models. In this paper, we review TensorFlow and put it in\ncontext of modern deep learning concepts and software. We discuss its basic\ncomputational paradigms and distributed execution model, its programming\ninterface as well as accompanying visualization toolkits. We then compare\nTensorFlow to alternative libraries such as Theano, Torch or Caffe on a\nqualitative as well as quantitative basis and finally comment on observed\nuse-cases of TensorFlow in academia and industry.\n",
        "published": "2016",
        "authors": [
            "Peter Goldsborough"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1708.03704v1",
        "title": "Deep Incremental Boosting",
        "abstract": "  This paper introduces Deep Incremental Boosting, a new technique derived from\nAdaBoost, specifically adapted to work with Deep Learning methods, that reduces\nthe required training time and improves generalisation. We draw inspiration\nfrom Transfer of Learning approaches to reduce the start-up time to training\neach incremental Ensemble member. We show a set of experiments that outlines\nsome preliminary results on some common Deep Learning datasets and discuss the\npotential improvements Deep Incremental Boosting brings to traditional Ensemble\nmethods in Deep Learning.\n",
        "published": "2017",
        "authors": [
            "Alan Mosca",
            "George D Magoulas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.06202v1",
        "title": "Risk Bounds for Robust Deep Learning",
        "abstract": "  It has been observed that certain loss functions can render deep-learning\npipelines robust against flaws in the data. In this paper, we support these\nempirical findings with statistical theory. We especially show that\nempirical-risk minimization with unbounded, Lipschitz-continuous loss\nfunctions, such as the least-absolute deviation loss, Huber loss, Cauchy loss,\nand Tukey's biweight loss, can provide efficient prediction under minimal\nassumptions on the data. More generally speaking, our paper provides\ntheoretical evidence for the benefits of robust loss functions in deep\nlearning.\n",
        "published": "2020",
        "authors": [
            "Johannes Lederer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.02804v3",
        "title": "Stop overkilling simple tasks with black-box models and use transparent\n  models instead",
        "abstract": "  In recent years, the employment of deep learning methods has led to several\nsignificant breakthroughs in artificial intelligence. Different from\ntraditional machine learning models, deep learning-based approaches are able to\nextract features autonomously from raw data. This allows for bypassing the\nfeature engineering process, which is generally considered to be both\nerror-prone and tedious. Moreover, deep learning strategies often outperform\ntraditional models in terms of accuracy.\n",
        "published": "2023",
        "authors": [
            "Matteo Rizzo",
            "Matteo Marcuzzo",
            "Alessandro Zangari",
            "Andrea Gasparetto",
            "Andrea Albarelli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.07980v2",
        "title": "Information Theoretic Interpretation of Deep learning",
        "abstract": "  We interpret part of the experimental results of Shwartz-Ziv and Tishby\n[2017]. Inspired by these results, we established a conjecture of the dynamics\nof the machinary of deep neural network. This conjecture can be used to explain\nthe counterpart result by Saxe et al. [2018].\n",
        "published": "2018",
        "authors": [
            "Tianchen Zhao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.04169v1",
        "title": "Defense Against the Dark Arts: An overview of adversarial example\n  security research and future research directions",
        "abstract": "  This article presents a summary of a keynote lecture at the Deep Learning\nSecurity workshop at IEEE Security and Privacy 2018. This lecture summarizes\nthe state of the art in defenses against adversarial examples and provides\nrecommendations for future research directions on this topic.\n",
        "published": "2018",
        "authors": [
            "Ian Goodfellow"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.02364v1",
        "title": "Indoor room Occupancy Counting based on LSTM and Environmental Sensor",
        "abstract": "  This paper realizes the estimation of classroom occupancy by using the CO2\nsensor and deep learning technique named Long-Short-Term Memory. As a case of\nconnection with IoT and machine learning, I achieve the model to estimate the\npeople number in the classroom based on the environmental data exported from\nthe CO2 sensor, I also evaluate the performance of the model to show the\nfeasibility to apply our module to the real environment.\n",
        "published": "2022",
        "authors": [
            "Zheyu Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.09777v1",
        "title": "On the Implicit Bias of Dropout",
        "abstract": "  Algorithmic approaches endow deep learning systems with implicit bias that\nhelps them generalize even in over-parametrized settings. In this paper, we\nfocus on understanding such a bias induced in learning through dropout, a\npopular technique to avoid overfitting in deep learning. For single\nhidden-layer linear neural networks, we show that dropout tends to make the\nnorm of incoming/outgoing weight vectors of all the hidden nodes equal. In\naddition, we provide a complete characterization of the optimization landscape\ninduced by dropout.\n",
        "published": "2018",
        "authors": [
            "Poorya Mianjy",
            "Raman Arora",
            "Rene Vidal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.01432v1",
        "title": "Knowledge-augmented Column Networks: Guiding Deep Learning with Advice",
        "abstract": "  Recently, deep models have had considerable success in several tasks,\nespecially with low-level representations. However, effective learning from\nsparse noisy samples is a major challenge in most deep models, especially in\ndomains with structured representations. Inspired by the proven success of\nhuman guided machine learning, we propose Knowledge-augmented Column Networks,\na relational deep learning framework that leverages human advice/knowledge to\nlearn better models in presence of sparsity and systematic noise.\n",
        "published": "2019",
        "authors": [
            "Mayukh Das",
            "Devendra Singh Dhami",
            "Yang Yu",
            "Gautam Kunapuli",
            "Sriraam Natarajan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.10644v1",
        "title": "Using Neural Architecture Search for Improving Software Flaw Detection\n  in Multimodal Deep Learning Models",
        "abstract": "  Software flaw detection using multimodal deep learning models has been\ndemonstrated as a very competitive approach on benchmark problems. In this\nwork, we demonstrate that even better performance can be achieved using neural\narchitecture search (NAS) combined with multimodal learning models. We adapt a\nNAS framework aimed at investigating image classification to the problem of\nsoftware flaw detection and demonstrate improved results on the Juliet Test\nSuite, a popular benchmarking data set for measuring performance of machine\nlearning models in this problem domain.\n",
        "published": "2020",
        "authors": [
            "Alexis Cooper",
            "Xin Zhou",
            "Scott Heidbrink",
            "Daniel M. Dunlavy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.02069v1",
        "title": "Deep learning for in vitro prediction of pharmaceutical formulations",
        "abstract": "  Current pharmaceutical formulation development still strongly relies on the\ntraditional trial-and-error approach by individual experiences of\npharmaceutical scientists, which is laborious, time-consuming and costly.\nRecently, deep learning has been widely applied in many challenging domains\nbecause of its important capability of automatic feature extraction. The aim of\nthis research is to use deep learning to predict pharmaceutical formulations.\nIn this paper, two different types of dosage forms were chosen as model\nsystems. Evaluation criteria suitable for pharmaceutics were applied to\nassessing the performance of the models. Moreover, an automatic dataset\nselection algorithm was developed for selecting the representative data as\nvalidation and test datasets. Six machine learning methods were compared with\ndeep learning. The result shows the accuracies of both two deep neural networks\nwere above 80% and higher than other machine learning models, which showed good\nprediction in pharmaceutical formulations. In summary, deep learning with the\nautomatic data splitting algorithm and the evaluation criteria suitable for\npharmaceutical formulation data was firstly developed for the prediction of\npharmaceutical formulations. The cross-disciplinary integration of\npharmaceutics and artificial intelligence may shift the paradigm of\npharmaceutical researches from experience-dependent studies to data-driven\nmethodologies.\n",
        "published": "2018",
        "authors": [
            "Yilong Yang",
            "Zhuyifan Ye",
            "Yan Su",
            "Qianqian Zhao",
            "Xiaoshan Li",
            "Defang Ouyang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.01674v1",
        "title": "Diagnosis of Paratuberculosis in Histopathological Images Based on\n  Explainable Artificial Intelligence and Deep Learning",
        "abstract": "  Artificial intelligence holds great promise in medical imaging, especially\nhistopathological imaging. However, artificial intelligence algorithms cannot\nfully explain the thought processes during decision-making. This situation has\nbrought the problem of explainability, i.e., the black box problem, of\nartificial intelligence applications to the agenda: an algorithm simply\nresponds without stating the reasons for the given images. To overcome the\nproblem and improve the explainability, explainable artificial intelligence\n(XAI) has come to the fore, and piqued the interest of many researchers.\nAgainst this backdrop, this study examines a new and original dataset using the\ndeep learning algorithm, and visualizes the output with gradient-weighted class\nactivation mapping (Grad-CAM), one of the XAI applications. Afterwards, a\ndetailed questionnaire survey was conducted with the pathologists on these\nimages. Both the decision-making processes and the explanations were verified,\nand the accuracy of the output was tested. The research results greatly help\npathologists in the diagnosis of paratuberculosis.\n",
        "published": "2022",
        "authors": [
            "Tuncay Yi\u011fit",
            "Nilg\u00fcn \u015eeng\u00f6z",
            "\u00d6zlem \u00d6zmen",
            "Jude Hemanth",
            "Ali Hakan I\u015f\u0131k"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.10362v2",
        "title": "The Final Frontier: Deep Learning in Space",
        "abstract": "  Machine learning, particularly deep learning, is being increasing utilised in\nspace applications, mirroring the groundbreaking success in many earthbound\nproblems. Deploying a space device, e.g. a satellite, is becoming more\naccessible to small actors due to the development of modular satellites and\ncommercial space launches, which fuels further growth of this area. Deep\nlearning's ability to deliver sophisticated computational intelligence makes it\nan attractive option to facilitate various tasks on space devices and reduce\noperational costs. In this work, we identify deep learning in space as one of\ndevelopment directions for mobile and embedded machine learning. We collate\nvarious applications of machine learning to space data, such as satellite\nimaging, and describe how on-device deep learning can meaningfully improve the\noperation of a spacecraft, such as by reducing communication costs or\nfacilitating navigation. We detail and contextualise compute platform of\nsatellites and draw parallels with embedded systems and current research in\ndeep learning for resource-constrained environments.\n",
        "published": "2020",
        "authors": [
            "Vivek Kothari",
            "Edgar Liberis",
            "Nicholas D. Lane"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.01890v2",
        "title": "RMDL: Random Multimodel Deep Learning for Classification",
        "abstract": "  The continually increasing number of complex datasets each year necessitates\never improving machine learning methods for robust and accurate categorization\nof these data. This paper introduces Random Multimodel Deep Learning (RMDL): a\nnew ensemble, deep learning approach for classification. Deep learning models\nhave achieved state-of-the-art results across many domains. RMDL solves the\nproblem of finding the best deep learning structure and architecture while\nsimultaneously improving robustness and accuracy through ensembles of deep\nlearning architectures. RDML can accept as input a variety data to include\ntext, video, images, and symbolic. This paper describes RMDL and shows test\nresults for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB,\nand 20newsgroup. These test results show that RDML produces consistently better\nperformance than standard methods over a broad range of data types and\nclassification problems.\n",
        "published": "2018",
        "authors": [
            "Kamran Kowsari",
            "Mojtaba Heidarysafa",
            "Donald E. Brown",
            "Kiana Jafari Meimandi",
            "Laura E. Barnes"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.02304v1",
        "title": "Morphological Computation and Learning to Learn In Natural Intelligent\n  Systems And AI",
        "abstract": "  At present, artificial intelligence in the form of machine learning is making\nimpressive progress, especially the field of deep learning (DL) [1]. Deep\nlearning algorithms have been inspired from the beginning by nature,\nspecifically by the human brain, in spite of our incomplete knowledge about its\nbrain function. Learning from nature is a two-way process as discussed in\n[2][3][4], computing is learning from neuroscience, while neuroscience is\nquickly adopting information processing models. The question is, what can the\ninspiration from computational nature at this stage of the development\ncontribute to deep learning and how much models and experiments in machine\nlearning can motivate, justify and lead research in neuroscience and cognitive\nscience and to practical applications of artificial intelligence.\n",
        "published": "2020",
        "authors": [
            "Gordana Dodig-Crnkovic"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.02591v1",
        "title": "Synthetic Information towards Maximum Posterior Ratio for deep learning\n  on Imbalanced Data",
        "abstract": "  This study examines the impact of class-imbalanced data on deep learning\nmodels and proposes a technique for data balancing by generating synthetic data\nfor the minority class. Unlike random-based oversampling, our method\nprioritizes balancing the informative regions by identifying high entropy\nsamples. Generating well-placed synthetic data can enhance machine learning\nalgorithms accuracy and efficiency, whereas poorly-placed ones may lead to\nhigher misclassification rates. We introduce an algorithm that maximizes the\nprobability of generating a synthetic sample in the correct region of its class\nby optimizing the class posterior ratio. Additionally, to maintain data\ntopology, synthetic data are generated within each minority sample's\nneighborhood. Our experimental results on forty-one datasets demonstrate the\nsuperior performance of our technique in enhancing deep-learning models.\n",
        "published": "2024",
        "authors": [
            "Hung Nguyen",
            "Morris Chang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.01622v1",
        "title": "Theory of Generative Deep Learning : Probe Landscape of Empirical Error\n  via Norm Based Capacity Control",
        "abstract": "  Despite its remarkable empirical success as a highly competitive branch of\nartificial intelligence, deep learning is often blamed for its widely known low\ninterpretation and lack of firm and rigorous mathematical foundation. However,\nmost theoretical endeavor is devoted in discriminative deep learning case,\nwhose complementary part is generative deep learning. To the best of our\nknowledge, we firstly highlight landscape of empirical error in generative case\nto complete the full picture through exquisite design of image super resolution\nunder norm based capacity control. Our theoretical advance in interpretation of\nthe training dynamic is achieved from both mathematical and biological sides.\n",
        "published": "2018",
        "authors": [
            "Wendi Xu",
            "Ming Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.12014v1",
        "title": "Asset Pricing and Deep Learning",
        "abstract": "  Traditional machine learning methods have been widely studied in financial\ninnovation. My study focuses on the application of deep learning methods on\nasset pricing. I investigate various deep learning methods for asset pricing,\nespecially for risk premia measurement. All models take the same set of\npredictive signals (firm characteristics, systematic risks and macroeconomics).\nI demonstrate high performance of all kinds of state-of-the-art (SOTA) deep\nlearning methods, and figure out that RNNs with memory mechanism and attention\nhave the best performance in terms of predictivity. Furthermore, I demonstrate\nlarge economic gains to investors using deep learning forecasts. The results of\nmy comparative experiments highlight the importance of domain knowledge and\nfinancial theory when designing deep learning models. I also show return\nprediction tasks bring new challenges to deep learning. The time varying\ndistribution causes distribution shift problem, which is essential for\nfinancial time series prediction. I demonstrate that deep learning methods can\nimprove asset risk premium measurement. Due to the booming deep learning\nstudies, they can constantly promote the study of underlying financial\nmechanisms behind asset pricing. I also propose a promising research method\nthat learning from data and figuring out the underlying economic mechanisms\nthrough explainable artificial intelligence (AI) methods. My findings not only\njustify the value of deep learning in blooming fintech development, but also\nhighlight their prospects and advantages over traditional machine learning\nmethods.\n",
        "published": "2022",
        "authors": [
            "Chen Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.05717v1",
        "title": "Forecasting Lithium-Ion Battery Longevity with Limited Data\n  Availability: Benchmarking Different Machine Learning Algorithms",
        "abstract": "  As the use of Lithium-ion batteries continues to grow, it becomes\nincreasingly important to be able to predict their remaining useful life. This\nwork aims to compare the relative performance of different machine learning\nalgorithms, both traditional machine learning and deep learning, in order to\ndetermine the best-performing algorithms for battery cycle life prediction\nbased on minimal data. We investigated 14 different machine learning models\nthat were fed handcrafted features based on statistical data and split into 3\nfeature groups for testing. For deep learning models, we tested a variety of\nneural network models including different configurations of standard Recurrent\nNeural Networks, Gated Recurrent Units, and Long Short Term Memory with and\nwithout attention mechanism. Deep learning models were fed multivariate time\nseries signals based on the raw data for each battery across the first 100\ncycles. Our experiments revealed that the machine learning algorithms on\nhandcrafted features performed particularly well, resulting in 10-20% average\nmean absolute percentage error. The best-performing algorithm was the Random\nForest Regressor, which gave a minimum 9.8% mean absolute percentage error.\nTraditional machine learning models excelled due to their capability to\ncomprehend general data set trends. In comparison, deep learning models were\nobserved to perform particularly poorly on raw, limited data. Algorithms like\nGRU and RNNs that focused on capturing medium-range data dependencies were less\nadept at recognizing the gradual, slow trends critical for this task. Our\ninvestigation reveals that implementing machine learning models with\nhand-crafted features proves to be more effective than advanced deep learning\nmodels for predicting the remaining useful Lithium-ion battery life with\nlimited data availability.\n",
        "published": "2023",
        "authors": [
            "Hudson Hilal",
            "Pramit Saha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.09705v1",
        "title": "A Review of Deep Learning Techniques for Protein Function Prediction",
        "abstract": "  Deep Learning and big data have shown tremendous success in bioinformatics\nand computational biology in recent years; artificial intelligence methods have\nalso significantly contributed in the task of protein function classification.\nThis review paper analyzes the recent developments in approaches for the task\nof predicting protein function using deep learning. We explain the importance\nof determining the protein function and why automating the following task is\ncrucial. Then, after reviewing the widely used deep learning techniques for\nthis task, we continue our review and highlight the emergence of the modern\nState of The Art (SOTA) deep learning models which have achieved groundbreaking\nresults in the field of computer vision, natural language processing and\nmulti-modal learning in the last few years. We hope that this review will\nprovide a broad view of the current role and advances of deep learning in\nbiological sciences, especially in predicting protein function tasks and\nencourage new researchers to contribute to this area.\n",
        "published": "2022",
        "authors": [
            "Divyanshu Aggarwal",
            "Yasha Hasija"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.11161v1",
        "title": "Neuro-Symbolic Bi-Directional Translation -- Deep Learning\n  Explainability for Climate Tipping Point Research",
        "abstract": "  In recent years, there has been an increase in using deep learning for\nclimate and weather modeling. Though results have been impressive,\nexplainability and interpretability of deep learning models are still a\nchallenge. A third wave of Artificial Intelligence (AI), which includes logic\nand reasoning, has been described as a way to address these issues.\nNeuro-symbolic AI is a key component of this integration of logic and reasoning\nwith deep learning. In this work we propose a neuro-symbolic approach called\nNeuro-Symbolic Question-Answer Program Translator, or NS-QAPT, to address\nexplainability and interpretability for deep learning climate simulation,\napplied to climate tipping point discovery. The NS-QAPT method includes a\nbidirectional encoder-decoder architecture that translates between\ndomain-specific questions and executable programs used to direct the climate\nsimulation, acting as a bridge between climate scientists and deep learning\nmodels. We show early compelling results of this translation method and\nintroduce a domain-specific language and associated executable programs for a\ncommonly known tipping point, the collapse of the Atlantic Meridional\nOverturning Circulation (AMOC).\n",
        "published": "2023",
        "authors": [
            "Chace Ashcraft",
            "Jennifer Sleeman",
            "Caroline Tang",
            "Jay Brett",
            "Anand Gnanadesikan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.16914v1",
        "title": "A New Deep Learning and XAI-Based Algorithm for Features Selection in\n  Genomics",
        "abstract": "  In the field of functional genomics, the analysis of gene expression profiles\nthrough Machine and Deep Learning is increasingly providing meaningful insight\ninto a number of diseases. The paper proposes a novel algorithm to perform\nFeature Selection on genomic-scale data, which exploits the reconstruction\ncapabilities of autoencoders and an ad-hoc defined Explainable Artificial\nIntelligence-based score in order to select the most informative genes for\ndiagnosis, prognosis, and precision medicine. Results of the application on a\nChronic Lymphocytic Leukemia dataset evidence the effectiveness of the\nalgorithm, by identifying and suggesting a set of meaningful genes for further\nmedical investigation.\n",
        "published": "2023",
        "authors": [
            "Carlo Adornetto",
            "Gianluigi Greco"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.04491v1",
        "title": "Explainable AI for Earth Observation: Current Methods, Open Challenges,\n  and Opportunities",
        "abstract": "  Deep learning has taken by storm all fields involved in data analysis,\nincluding remote sensing for Earth observation. However, despite significant\nadvances in terms of performance, its lack of explainability and\ninterpretability, inherent to neural networks in general since their inception,\nremains a major source of criticism. Hence it comes as no surprise that the\nexpansion of deep learning methods in remote sensing is being accompanied by\nincreasingly intensive efforts oriented towards addressing this drawback\nthrough the exploration of a wide spectrum of Explainable Artificial\nIntelligence techniques. This chapter, organized according to prominent Earth\nobservation application fields, presents a panorama of the state-of-the-art in\nexplainable remote sensing image analysis.\n",
        "published": "2023",
        "authors": [
            "Gulsen Taskin",
            "Erchan Aptoula",
            "Alp Ert\u00fcrk"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.05125v2",
        "title": "Learning Task-aware Robust Deep Learning Systems",
        "abstract": "  Many works demonstrate that deep learning system is vulnerable to adversarial\nattack. A deep learning system consists of two parts: the deep learning task\nand the deep model. Nowadays, most existing works investigate the impact of the\ndeep model on robustness of deep learning systems, ignoring the impact of the\nlearning task. In this paper, we adopt the binary and interval label encoding\nstrategy to redefine the classification task and design corresponding loss to\nimprove robustness of the deep learning system. Our method can be viewed as\nimproving the robustness of deep learning systems from both the learning task\nand deep model. Experimental results demonstrate that our learning task-aware\nmethod is much more robust than traditional classification while retaining the\naccuracy.\n",
        "published": "2020",
        "authors": [
            "Keji Han",
            "Yun Li",
            "Xianzhong Long",
            "Yao Ge"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.20360v1",
        "title": "Mathematical Introduction to Deep Learning: Methods, Implementations,\n  and Theory",
        "abstract": "  This book aims to provide an introduction to the topic of deep learning\nalgorithms. We review essential components of deep learning algorithms in full\nmathematical detail including different artificial neural network (ANN)\narchitectures (such as fully-connected feedforward ANNs, convolutional ANNs,\nrecurrent ANNs, residual ANNs, and ANNs with batch normalization) and different\noptimization algorithms (such as the basic stochastic gradient descent (SGD)\nmethod, accelerated methods, and adaptive methods). We also cover several\ntheoretical aspects of deep learning algorithms such as approximation\ncapacities of ANNs (including a calculus for ANNs), optimization theory\n(including Kurdyka-{\\L}ojasiewicz inequalities), and generalization errors. In\nthe last part of the book some deep learning approximation methods for PDEs are\nreviewed including physics-informed neural networks (PINNs) and deep Galerkin\nmethods. We hope that this book will be useful for students and scientists who\ndo not yet have any background in deep learning at all and would like to gain a\nsolid foundation as well as for practitioners who would like to obtain a firmer\nmathematical understanding of the objects and methods considered in deep\nlearning.\n",
        "published": "2023",
        "authors": [
            "Arnulf Jentzen",
            "Benno Kuckuck",
            "Philippe von Wurstemberger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.05232v2",
        "title": "A Survey From Distributed Machine Learning to Distributed Deep Learning",
        "abstract": "  Artificial intelligence has made remarkable progress in handling complex\ntasks, thanks to advances in hardware acceleration and machine learning\nalgorithms. However, to acquire more accurate outcomes and solve more complex\nissues, algorithms should be trained with more data. Processing this huge\namount of data could be time-consuming and require a great deal of computation.\nTo address these issues, distributed machine learning has been proposed, which\ninvolves distributing the data and algorithm across several machines. There has\nbeen considerable effort put into developing distributed machine learning\nalgorithms, and different methods have been proposed so far. We divide these\nalgorithms in classification and clustering (traditional machine learning),\ndeep learning and deep reinforcement learning groups. Distributed deep learning\nhas gained more attention in recent years and most of the studies have focused\non this approach. Therefore, we mostly concentrate on this category. Based on\nthe investigation of the mentioned algorithms, we highlighted the limitations\nthat should be addressed in future research.\n",
        "published": "2023",
        "authors": [
            "Mohammad Dehghani",
            "Zahra Yazdanparast"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.10714v1",
        "title": "Automated Architecture Design for Deep Neural Networks",
        "abstract": "  Machine learning has made tremendous progress in recent years and received\nlarge amounts of public attention. Though we are still far from designing a\nfull artificially intelligent agent, machine learning has brought us many\napplications in which computers solve human learning tasks remarkably well.\nMuch of this progress comes from a recent trend within machine learning, called\ndeep learning. Deep learning models are responsible for many state-of-the-art\napplications of machine learning. Despite their success, deep learning models\nare hard to train, very difficult to understand, and often times so complex\nthat training is only possible on very large GPU clusters. Lots of work has\nbeen done on enabling neural networks to learn efficiently. However, the design\nand architecture of such neural networks is often done manually through trial\nand error and expert knowledge. This thesis inspects different approaches,\nexisting and novel, to automate the design of deep feedforward neural networks\nin an attempt to create less complex models with good performance that take\naway the burden of deciding on an architecture and make it more efficient to\ndesign and train such deep networks.\n",
        "published": "2019",
        "authors": [
            "Steven Abreu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.04372v1",
        "title": "Enhancing Malware Detection by Integrating Machine Learning with Cuckoo\n  Sandbox",
        "abstract": "  In the modern era, malware is experiencing a significant increase in both its\nvariety and quantity, aligning with the widespread adoption of the digital\nworld. This surge in malware has emerged as a critical challenge in the realm\nof cybersecurity, prompting numerous research endeavors and contributions to\naddress the issue. Machine learning algorithms have been leveraged for malware\ndetection due to their ability to uncover concealed patterns within vast\ndatasets. However, deep learning algorithms, characterized by their\nmulti-layered structure, surpass the limitations of traditional machine\nlearning approaches. By employing deep learning techniques such as CNN\n(Convolutional Neural Network) and RNN (Recurrent Neural Network), this study\naims to classify and identify malware extracted from a dataset containing API\ncall sequences. The performance of these algorithms is compared with that of\nconventional machine learning methods, including SVM (Support Vector Machine),\nRF (Random Forest), KNN (K-Nearest Neighbors), XGB (Extreme Gradient Boosting),\nand GBC (Gradient Boosting Classifier), all using the same dataset. The\noutcomes of this research demonstrate that both deep learning and machine\nlearning algorithms achieve remarkably high levels of accuracy, reaching up to\n99% in certain cases.\n",
        "published": "2023",
        "authors": [
            "Amaal F. Alshmarni",
            "Mohammed A. Alliheedi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.06034v1",
        "title": "Does Deep Learning REALLY Outperform Non-deep Machine Learning for\n  Clinical Prediction on Physiological Time Series?",
        "abstract": "  Machine learning has been widely used in healthcare applications to\napproximate complex models, for clinical diagnosis, prognosis, and treatment.\nAs deep learning has the outstanding ability to extract information from time\nseries, its true capabilities on sparse, irregularly sampled, multivariate, and\nimbalanced physiological data are not yet fully explored. In this paper, we\nsystematically examine the performance of machine learning models for the\nclinical prediction task based on the EHR, especially physiological time\nseries. We choose Physionet 2019 challenge public dataset to predict Sepsis\noutcomes in ICU units. Ten baseline machine learning models are compared,\nincluding 3 deep learning methods and 7 non-deep learning methods, commonly\nused in the clinical prediction domain. Nine evaluation metrics with specific\nclinical implications are used to assess the performance of models. Besides, we\nsub-sample training dataset sizes and use learning curve fit to investigate the\nimpact of the training dataset size on the performance of the machine learning\nmodels. We also propose the general pre-processing method for the physiology\ntime-series data and use Dice Loss to deal with the dataset imbalanced problem.\nThe results show that deep learning indeed outperforms non-deep learning, but\nwith certain conditions: firstly, evaluating with some particular evaluation\nmetrics (AUROC, AUPRC, Sensitivity, and FNR), but not others; secondly, the\ntraining dataset size is large enough (with an estimation of a magnitude of\nthousands).\n",
        "published": "2022",
        "authors": [
            "Ke Liao",
            "Wei Wang",
            "Armagan Elibol",
            "Lingzhong Meng",
            "Xu Zhao",
            "Nak Young Chong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.09664v1",
        "title": "Emerging Artificial Intelligence Applications in Spatial Transcriptomics\n  Analysis",
        "abstract": "  Spatial transcriptomics (ST) has advanced significantly in the last few\nyears. Such advancement comes with the urgent need for novel computational\nmethods to handle the unique challenges of ST data analysis. Many artificial\nintelligence (AI) methods have been developed to utilize various machine\nlearning and deep learning techniques for computational ST analysis. This\nreview provides a comprehensive and up-to-date survey of current AI methods for\nST analysis.\n",
        "published": "2022",
        "authors": [
            "Yijun Li",
            "Stefan Stanojevic",
            "Lana X. Garmire"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.09337v1",
        "title": "Deep Learning in Business Analytics: A Clash of Expectations and Reality",
        "abstract": "  Our fast-paced digital economy shaped by global competition requires\nincreased data-driven decision-making based on artificial intelligence (AI) and\nmachine learning (ML). The benefits of deep learning (DL) are manifold, but it\ncomes with limitations that have - so far - interfered with widespread industry\nadoption. This paper explains why DL - despite its popularity - has\ndifficulties speeding up its adoption within business analytics. It is shown -\nby a mixture of content analysis and empirical study - that the adoption of\ndeep learning is not only affected by computational complexity, lacking big\ndata architecture, lack of transparency (black-box), and skill shortage, but\nalso by the fact that DL does not outperform traditional ML models in the case\nof structured datasets with fixed-length feature vectors. Deep learning should\nbe regarded as a powerful addition to the existing body of ML models instead of\na one size fits all solution.\n",
        "published": "2022",
        "authors": [
            "Marc Andreas Schmitt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.10535v2",
        "title": "A Survey of Deep Learning for Mathematical Reasoning",
        "abstract": "  Mathematical reasoning is a fundamental aspect of human intelligence and is\napplicable in various fields, including science, engineering, finance, and\neveryday life. The development of artificial intelligence (AI) systems capable\nof solving math problems and proving theorems has garnered significant interest\nin the fields of machine learning and natural language processing. For example,\nmathematics serves as a testbed for aspects of reasoning that are challenging\nfor powerful deep learning models, driving new algorithmic and modeling\nadvances. On the other hand, recent advances in large-scale neural language\nmodels have opened up new benchmarks and opportunities to use deep learning for\nmathematical reasoning. In this survey paper, we review the key tasks,\ndatasets, and methods at the intersection of mathematical reasoning and deep\nlearning over the past decade. We also evaluate existing benchmarks and\nmethods, and discuss future research directions in this domain.\n",
        "published": "2022",
        "authors": [
            "Pan Lu",
            "Liang Qiu",
            "Wenhao Yu",
            "Sean Welleck",
            "Kai-Wei Chang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.07213v2",
        "title": "Human-computer Interaction for Brain-inspired Computing Based on Machine\n  Learning And Deep Learning:A Review",
        "abstract": "  The continuous development of artificial intelligence has a profound impact\non biomedical research and other fields.Brain-inspired computing is an\nimportant intersection of multimodal technology and biomedical field. This\npaper presents a comprehensive review of machine learning (ML) and deep\nlearning (DL) models applied in human-computer interaction for brain-inspired\ncomputing, tracking their evolution, application value, challenges, and\npotential research trajectories. First, the basic concepts and development\nhistory are reviewed, and their evolution is divided into two stages: recent\nmachine learning and current deep learning, emphasizing the importance of each\nstage in the research state of human-computer interaction for brain-inspired\ncomputing. In addition, the latest progress and key techniques of deep learning\nin different tasks of human-computer interaction for brain-inspired computing\nare introduced from six perspectives. Despite significant progress, challenges\nremain in making full use of its capabilities. This paper aims to provide a\ncomprehensive review of human-computer interaction for brain-inspired computing\nmodels based on machine learning and deep learning, highlighting their\npotential in various applications and providing a valuable reference for future\nacademic research. It can be accessed through the following url:\nhttps://github.com/ultracoolHub/brain-inspired-computing\n",
        "published": "2023",
        "authors": [
            "Bihui Yu",
            "Sibo Zhang",
            "Lili Zhou",
            "Jingxuan Wei",
            "Linzhuang Sun",
            "Liping Bu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.09483v1",
        "title": "Significant Wave Height Prediction based on Wavelet Graph Neural Network",
        "abstract": "  Computational intelligence-based ocean characteristics forecasting\napplications, such as Significant Wave Height (SWH) prediction, are crucial for\navoiding social and economic loss in coastal cities. Compared to the\ntraditional empirical-based or numerical-based forecasting models, \"soft\ncomputing\" approaches, including machine learning and deep learning models,\nhave shown numerous success in recent years. In this paper, we focus on\nenabling the deep learning model to learn both short-term and long-term\nspatial-temporal dependencies for SWH prediction. A Wavelet Graph Neural\nNetwork (WGNN) approach is proposed to integrate the advantages of wavelet\ntransform and graph neural network. Several parallel graph neural networks are\nseparately trained on wavelet decomposed data, and the reconstruction of each\nmodel's prediction forms the final SWH prediction. Experimental results show\nthat the proposed WGNN approach outperforms other models, including the\nnumerical models, the machine learning models, and several deep learning\nmodels.\n",
        "published": "2021",
        "authors": [
            "Delong Chen",
            "Fan Liu",
            "Zheqi Zhang",
            "Xiaomin Lu",
            "Zewen Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.02150v3",
        "title": "Harmonizing Feature Attributions Across Deep Learning Architectures:\n  Enhancing Interpretability and Consistency",
        "abstract": "  Ensuring the trustworthiness and interpretability of machine learning models\nis critical to their deployment in real-world applications. Feature attribution\nmethods have gained significant attention, which provide local explanations of\nmodel predictions by attributing importance to individual input features. This\nstudy examines the generalization of feature attributions across various deep\nlearning architectures, such as convolutional neural networks (CNNs) and vision\ntransformers. We aim to assess the feasibility of utilizing a feature\nattribution method as a future detector and examine how these features can be\nharmonized across multiple models employing distinct architectures but trained\non the same data distribution. By exploring this harmonization, we aim to\ndevelop a more coherent and optimistic understanding of feature attributions,\nenhancing the consistency of local explanations across diverse deep-learning\nmodels. Our findings highlight the potential for harmonized feature attribution\nmethods to improve interpretability and foster trust in machine learning\napplications, regardless of the underlying architecture.\n",
        "published": "2023",
        "authors": [
            "Md Abdul Kadir",
            "Gowtham Krishna Addluri",
            "Daniel Sonntag"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.06233v1",
        "title": "Continual Learning with Deep Learning Methods in an Application-Oriented\n  Context",
        "abstract": "  Abstract knowledge is deeply grounded in many computer-based applications. An\nimportant research area of Artificial Intelligence (AI) deals with the\nautomatic derivation of knowledge from data. Machine learning offers the\naccording algorithms. One area of research focuses on the development of\nbiologically inspired learning algorithms. The respective machine learning\nmethods are based on neurological concepts so that they can systematically\nderive knowledge from data and store it. One type of machine learning\nalgorithms that can be categorized as \"deep learning\" model is referred to as\nDeep Neural Networks (DNNs). DNNs consist of multiple artificial neurons\narranged in layers that are trained by using the backpropagation algorithm.\nThese deep learning methods exhibit amazing capabilities for inferring and\nstoring complex knowledge from high-dimensional data. However, DNNs are\naffected by a problem that prevents new knowledge from being added to an\nexisting base. The ability to continuously accumulate knowledge is an important\nfactor that contributed to evolution and is therefore a prerequisite for the\ndevelopment of strong AIs. The so-called \"catastrophic forgetting\" (CF) effect\ncauses DNNs to immediately loose already derived knowledge after a few training\niterations on a new data distribution. Only an energetically expensive\nretraining with the joint data distribution of past and new data enables the\nabstraction of the entire new set of knowledge. In order to counteract the\neffect, various techniques have been and are still being developed with the\ngoal to mitigate or even solve the CF problem. These published CF avoidance\nstudies usually imply the effectiveness of their approaches for various\ncontinual learning tasks. This dissertation is set in the context of continual\nmachine learning with deep learning methods. The first part deals with the\ndevelopment of an ...\n",
        "published": "2022",
        "authors": [
            "Benedikt Pf\u00fclb"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.04950v1",
        "title": "Deep Learning in the Wild",
        "abstract": "  Deep learning with neural networks is applied by an increasing number of\npeople outside of classic research environments, due to the vast success of the\nmethodology on a wide range of machine perception tasks. While this interest is\nfueled by beautiful success stories, practical work in deep learning on novel\ntasks without existing baselines remains challenging. This paper explores the\nspecific challenges arising in the realm of real world tasks, based on case\nstudies from research \\& development in conjunction with industry, and extracts\nlessons learned from them. It thus fills a gap between the publication of\nlatest algorithmic and methodical developments, and the usually omitted\nnitty-gritty of how to make them work. Specifically, we give insight into deep\nlearning projects on face matching, print media monitoring, industrial quality\ncontrol, music scanning, strategy game playing, and automated machine learning,\nthereby providing best practices for deep learning in practice.\n",
        "published": "2018",
        "authors": [
            "Thilo Stadelmann",
            "Mohammadreza Amirian",
            "Ismail Arabaci",
            "Marek Arnold",
            "Gilbert Fran\u00e7ois Duivesteijn",
            "Ismail Elezi",
            "Melanie Geiger",
            "Stefan L\u00f6rwald",
            "Benjamin Bruno Meier",
            "Katharina Rombach",
            "Lukas Tuggener"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.11688v2",
        "title": "Rallying Adversarial Techniques against Deep Learning for Network\n  Security",
        "abstract": "  Recent advances in artificial intelligence and the increasing need for\npowerful defensive measures in the domain of network security, have led to the\nadoption of deep learning approaches for use in network intrusion detection\nsystems. These methods have achieved superior performance against conventional\nnetwork attacks, which enable the deployment of practical security systems to\nunique and dynamic sectors. Adversarial machine learning, unfortunately, has\nrecently shown that deep learning models are inherently vulnerable to\nadversarial modifications on their input data. Because of this susceptibility,\nthe deep learning models deployed to power a network defense could in fact be\nthe weakest entry point for compromising a network system. In this paper, we\nshow that by modifying on average as little as 1.38 of the input features, an\nadversary can generate malicious inputs which effectively fool a deep learning\nbased NIDS. Therefore, when designing such systems, it is crucial to consider\nthe performance from not only the conventional network security perspective but\nalso the adversarial machine learning domain.\n",
        "published": "2019",
        "authors": [
            "Joseph Clements",
            "Yuzhe Yang",
            "Ankur Sharma",
            "Hongxin Hu",
            "Yingjie Lao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.07930v1",
        "title": "Ludwig: a type-based declarative deep learning toolbox",
        "abstract": "  In this work we present Ludwig, a flexible, extensible and easy to use\ntoolbox which allows users to train deep learning models and use them for\nobtaining predictions without writing code. Ludwig implements a novel approach\nto deep learning model building based on two main abstractions: data types and\ndeclarative configuration files. The data type abstraction allows for easier\ncode and sub-model reuse, and the standardized interfaces imposed by this\nabstraction allow for encapsulation and make the code easy to extend.\nDeclarative model definition configuration files enable inexperienced users to\nobtain effective models and increase the productivity of expert users.\nAlongside these two innovations, Ludwig introduces a general modularized deep\nlearning architecture called Encoder-Combiner-Decoder that can be instantiated\nto perform a vast amount of machine learning tasks. These innovations make it\npossible for engineers, scientists from other fields and, in general, a much\nbroader audience to adopt deep learning models for their tasks, concretely\nhelping in its democratization.\n",
        "published": "2019",
        "authors": [
            "Piero Molino",
            "Yaroslav Dudin",
            "Sai Sumanth Miryala"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.12963v1",
        "title": "Error Bounds for a Matrix-Vector Product Approximation with Deep ReLU\n  Neural Networks",
        "abstract": "  Among the several paradigms of artificial intelligence (AI) or machine\nlearning (ML), a remarkably successful paradigm is deep learning. Deep\nlearning's phenomenal success has been hoped to be interpreted via fundamental\nresearch on the theory of deep learning. Accordingly, applied research on deep\nlearning has spurred the theory of deep learning-oriented depth and breadth of\ndevelopments. Inspired by such developments, we pose these fundamental\nquestions: can we accurately approximate an arbitrary matrix-vector product\nusing deep rectified linear unit (ReLU) feedforward neural networks (FNNs)? If\nso, can we bound the resulting approximation error? In light of these\nquestions, we derive error bounds in Lebesgue and Sobolev norms that comprise\nour developed deep approximation theory. Guided by this theory, we have\nsuccessfully trained deep ReLU FNNs whose test results justify our developed\ntheory. The developed theory is also applicable for guiding and easing the\ntraining of teacher deep ReLU FNNs in view of the emerging teacher-student AI\nor ML paradigms that are essential for solving several AI or ML problems in\nwireless communications and signal processing; network science and graph signal\nprocessing; and network neuroscience and brain physics.\n",
        "published": "2021",
        "authors": [
            "Tilahun M. Getu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.06345v1",
        "title": "A Survey on Societal Event Forecasting with Deep Learning",
        "abstract": "  Population-level societal events, such as civil unrest and crime, often have\na significant impact on our daily life. Forecasting such events is of great\nimportance for decision-making and resource allocation. Event prediction has\ntraditionally been challenging due to the lack of knowledge regarding the true\ncauses and underlying mechanisms of event occurrence. In recent years, research\non event forecasting has made significant progress due to two main reasons: (1)\nthe development of machine learning and deep learning algorithms and (2) the\naccessibility of public data such as social media, news sources, blogs,\neconomic indicators, and other meta-data sources. The explosive growth of data\nand the remarkable advancement in software/hardware technologies have led to\napplications of deep learning techniques in societal event studies. This paper\nis dedicated to providing a systematic and comprehensive overview of deep\nlearning technologies for societal event predictions. We focus on two domains\nof societal events: \\textit{civil unrest} and \\textit{crime}. We first\nintroduce how event forecasting problems are formulated as a machine learning\nprediction task. Then, we summarize data resources, traditional methods, and\nrecent development of deep learning models for these problems. Finally, we\ndiscuss the challenges in societal event forecasting and put forward some\npromising directions for future research.\n",
        "published": "2021",
        "authors": [
            "Songgaojun Deng",
            "Yue Ning"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.00727v1",
        "title": "Review of deep learning in healthcare",
        "abstract": "  Given the growing complexity of healthcare data over the last several years,\nusing machine learning techniques like Deep Neural Network (DNN) models has\ngained increased appeal. In order to extract hidden patterns and other valuable\ninformation from the huge quantity of health data, which traditional analytics\nare unable to do in a reasonable length of time, machine learning (ML)\ntechniques are used. Deep Learning (DL) algorithms in particular have been\nshown as potential approaches to pattern identification in healthcare systems.\nThis thought has led to the contribution of this research, which examines deep\nlearning methods used in healthcare systems via an examination of cutting-edge\nnetwork designs, applications, and market trends. To connect deep learning\nmethodologies and human healthcare interpretability, the initial objective is\nto provide in-depth insight into the deployment of deep learning models in\nhealthcare solutions. And last, to outline the current unresolved issues and\npotential directions.\n",
        "published": "2023",
        "authors": [
            "Hasan Hejbari Zargar",
            "Saha Hejbari Zargar",
            "Raziye Mehri"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.14808v1",
        "title": "Bringing AI To Edge: From Deep Learning's Perspective",
        "abstract": "  Edge computing and artificial intelligence (AI), especially deep learning for\nnowadays, are gradually intersecting to build a novel system, called edge\nintelligence. However, the development of edge intelligence systems encounters\nsome challenges, and one of these challenges is the \\textit{computational gap}\nbetween computation-intensive deep learning algorithms and less-capable edge\nsystems. Due to the computational gap, many edge intelligence systems cannot\nmeet the expected performance requirements. To bridge the gap, a plethora of\ndeep learning techniques and optimization methods are proposed in the past\nyears: light-weight deep learning models, network compression, and efficient\nneural architecture search. Although some reviews or surveys have partially\ncovered this large body of literature, we lack a systematic and comprehensive\nreview to discuss all aspects of these deep learning techniques which are\ncritical for edge intelligence implementation. As various and diverse methods\nwhich are applicable to edge systems are proposed intensively, a holistic\nreview would enable edge computing engineers and community to know the\nstate-of-the-art deep learning techniques which are instrumental for edge\nintelligence and to facilitate the development of edge intelligence systems.\nThis paper surveys the representative and latest deep learning techniques that\nare useful for edge intelligence systems, including hand-crafted models, model\ncompression, hardware-aware neural architecture search and adaptive deep\nlearning models. Finally, based on observations and simple experiments we\nconducted, we discuss some future directions.\n",
        "published": "2020",
        "authors": [
            "Di Liu",
            "Hao Kong",
            "Xiangzhong Luo",
            "Weichen Liu",
            "Ravi Subramaniam"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.09636v1",
        "title": "Performance Analysis and Comparison of Machine and Deep Learning\n  Algorithms for IoT Data Classification",
        "abstract": "  In recent years, the growth of Internet of Things (IoT) as an emerging\ntechnology has been unbelievable. The number of networkenabled devices in IoT\ndomains is increasing dramatically, leading to the massive production of\nelectronic data. These data contain valuable information which can be used in\nvarious areas, such as science, industry, business and even social life. To\nextract and analyze this information and make IoT systems smart, the only\nchoice is entering artificial intelligence (AI) world and leveraging the power\nof machine learning and deep learning techniques. This paper evaluates the\nperformance of 11 popular machine and deep learning algorithms for\nclassification task using six IoT-related datasets. These algorithms are\ncompared according to several performance evaluation metrics including\nprecision, recall, f1-score, accuracy, execution time, ROC-AUC score and\nconfusion matrix. A specific experiment is also conducted to assess the\nconvergence speed of developed models. The comprehensive experiments indicated\nthat, considering all performance metrics, Random Forests performed better than\nother machine learning models, while among deep learning models, ANN and CNN\nachieved more interesting results.\n",
        "published": "2020",
        "authors": [
            "Meysam Vakili",
            "Mohammad Ghamsari",
            "Masoumeh Rezaei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.03754v1",
        "title": "Deep Learning Methods for Credit Card Fraud Detection",
        "abstract": "  Credit card frauds are at an ever-increasing rate and have become a major\nproblem in the financial sector. Because of these frauds, card users are\nhesitant in making purchases and both the merchants and financial institutions\nbear heavy losses. Some major challenges in credit card frauds involve the\navailability of public data, high class imbalance in data, changing nature of\nfrauds and the high number of false alarms. Machine learning techniques have\nbeen used to detect credit card frauds but no fraud detection systems have been\nable to offer great efficiency to date. Recent development of deep learning has\nbeen applied to solve complex problems in various areas. This paper presents a\nthorough study of deep learning methods for the credit card fraud detection\nproblem and compare their performance with various machine learning algorithms\non three different financial datasets. Experimental results show great\nperformance of the proposed deep learning methods against traditional machine\nlearning models and imply that the proposed approaches can be implemented\neffectively for real-world credit card fraud detection systems.\n",
        "published": "2020",
        "authors": [
            "Thanh Thi Nguyen",
            "Hammad Tahir",
            "Mohamed Abdelrazek",
            "Ali Babar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.02004v1",
        "title": "Investigation of Random Laser in the Machine Learning Approach",
        "abstract": "  Machine Learning and Deep Learning are computational tools that fall within\nthe domain of artificial intelligence. In recent years, numerous research works\nhave advanced the application of machine and deep learning in various fields,\nincluding optics and photonics. In this article, we employ machine learning\nalgorithms to investigate the feasibility of predicting a stochastic phenomena:\nrandom laser emissions. Our results indicate that machine and deep learning\nhave the capacity to accurately reproduce fluctuations characteristic of random\nlasers. By employing simple supervised learning algorithms, we demonstrate that\nthe random laser intensity fluctuations can be predicted using spontaneous\nemission and pump intensity as input parameters in the models. Applications\nbased on the demonstrated results are discussed.\n  Keywords: Machine Learning, Deep Learning, Random Laser.\n",
        "published": "2023",
        "authors": [
            "Emanuel P. Santos",
            "Rodrigo F. Silva",
            "C\u00e9lio V. T. Maciel",
            "Daniel F. Luz",
            "Pedro F. A. Silva"
        ]
    }
]