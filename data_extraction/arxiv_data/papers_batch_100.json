[
    {
        "id": "http://arxiv.org/abs/2006.14091v2",
        "title": "Learning Reward Functions from Diverse Sources of Human Feedback:\n  Optimally Integrating Demonstrations and Preferences",
        "abstract": "  Reward functions are a common way to specify the objective of a robot. As\ndesigning reward functions can be extremely challenging, a more promising\napproach is to directly learn reward functions from human teachers.\nImportantly, data from human teachers can be collected either passively or\nactively in a variety of forms: passive data sources include demonstrations,\n(e.g., kinesthetic guidance), whereas preferences (e.g., comparative rankings)\nare actively elicited. Prior research has independently applied reward learning\nto these different data sources. However, there exist many domains where\nmultiple sources are complementary and expressive. Motivated by this general\nproblem, we present a framework to integrate multiple sources of information,\nwhich are either passively or actively collected from human users. In\nparticular, we present an algorithm that first utilizes user demonstrations to\ninitialize a belief about the reward function, and then actively probes the\nuser with preference queries to zero-in on their true reward. This algorithm\nnot only enables us combine multiple data sources, but it also informs the\nrobot when it should leverage each type of information. Further, our approach\naccounts for the human's ability to provide data: yielding user-friendly\npreference queries which are also theoretically optimal. Our extensive\nsimulated experiments and user studies on a Fetch mobile manipulator\ndemonstrate the superiority and the usability of our integrated framework.\n",
        "published": "2020",
        "authors": [
            "Erdem B\u0131y\u0131k",
            "Dylan P. Losey",
            "Malayandi Palan",
            "Nicholas C. Landolfi",
            "Gleb Shevchuk",
            "Dorsa Sadigh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.14796v5",
        "title": "AvE: Assistance via Empowerment",
        "abstract": "  One difficulty in using artificial agents for human-assistive applications\nlies in the challenge of accurately assisting with a person's goal(s). Existing\nmethods tend to rely on inferring the human's goal, which is challenging when\nthere are many potential goals or when the set of candidate goals is difficult\nto identify. We propose a new paradigm for assistance by instead increasing the\nhuman's ability to control their environment, and formalize this approach by\naugmenting reinforcement learning with human empowerment. This task-agnostic\nobjective preserves the person's autonomy and ability to achieve any eventual\nstate. We test our approach against assistance based on goal inference,\nhighlighting scenarios where our method overcomes failure modes stemming from\ngoal ambiguity or misspecification. As existing methods for estimating\nempowerment in continuous domains are computationally hard, precluding its use\nin real time learned assistance, we also propose an efficient\nempowerment-inspired proxy metric. Using this, we are able to successfully\ndemonstrate our method in a shared autonomy user study for a challenging\nsimulated teleoperation task with human-in-the-loop training.\n",
        "published": "2020",
        "authors": [
            "Yuqing Du",
            "Stas Tiomkin",
            "Emre Kiciman",
            "Daniel Polani",
            "Pieter Abbeel",
            "Anca Dragan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.01009v1",
        "title": "Human-centered collaborative robots with deep reinforcement learning",
        "abstract": "  We present a reinforcement learning based framework for human-centered\ncollaborative systems. The framework is proactive and balances the benefits of\ntimely actions with the risk of taking improper actions by minimizing the total\ntime spent to complete the task. The framework is learned end-to-end in an\nunsupervised fashion addressing the perception uncertainties and decision\nmaking in an integrated manner. The framework is shown to provide more fluent\ncoordination between human and robot partners on an example task of packaging\ncompared to alternatives for which perception and decision-making systems are\nlearned independently, using supervised learning. The foremost benefit of the\nproposed approach is that it allows for fast adaptation to new human partners\nand tasks since tedious annotation of motion data is avoided and the learning\nis performed on-line.\n",
        "published": "2020",
        "authors": [
            "Ali Ghadirzadeh",
            "Xi Chen",
            "Wenjie Yin",
            "Zhengrong Yi",
            "M\u00e5rten Bj\u00f6rkman",
            "Danica Kragic"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.05798v2",
        "title": "Planning on the fast lane: Learning to interact using attention\n  mechanisms in path integral inverse reinforcement learning",
        "abstract": "  General-purpose trajectory planning algorithms for automated driving utilize\ncomplex reward functions to perform a combined optimization of strategic,\nbehavioral, and kinematic features. The specification and tuning of a single\nreward function is a tedious task and does not generalize over a large set of\ntraffic situations. Deep learning approaches based on path integral inverse\nreinforcement learning have been successfully applied to predict local\nsituation-dependent reward functions using features of a set of sampled driving\npolicies. Sample-based trajectory planning algorithms are able to approximate a\nspatio-temporal subspace of feasible driving policies that can be used to\nencode the context of a situation. However, the interaction with dynamic\nobjects requires an extended planning horizon, which depends on sequential\ncontext modeling. In this work, we are concerned with the sequential reward\nprediction over an extended time horizon. We present a neural network\narchitecture that uses a policy attention mechanism to generate a\nlow-dimensional context vector by concentrating on trajectories with a\nhuman-like driving style. Apart from this, we propose a temporal attention\nmechanism to identify context switches and allow for stable adaptation of\nrewards. We evaluate our results on complex simulated driving situations,\nincluding other moving vehicles. Our evaluation shows that our policy attention\nmechanism learns to focus on collision-free policies in the configuration\nspace. Furthermore, the temporal attention mechanism learns persistent\ninteraction with other vehicles over an extended planning horizon.\n",
        "published": "2020",
        "authors": [
            "Sascha Rosbach",
            "Xing Li",
            "Simon Gro\u00dfjohann",
            "Silviu Homoceanu",
            "Stefan Roth"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.09101v1",
        "title": "Reinforcement Learning-Enabled Decision-Making Strategies for a\n  Vehicle-Cyber-Physical-System in Connected Environment",
        "abstract": "  As a typical vehicle-cyber-physical-system (V-CPS), connected automated\nvehicles attracted more and more attention in recent years. This paper focuses\non discussing the decision-making (DM) strategy for autonomous vehicles in a\nconnected environment. First, the highway DM problem is formulated, wherein the\nvehicles can exchange information via wireless networking. Then, two classical\nreinforcement learning (RL) algorithms, Q-learning and Dyna, are leveraged to\nderive the DM strategies in a predefined driving scenario. Finally, the control\nperformance of the derived DM policies in safety and efficiency is analyzed.\nFurthermore, the inherent differences of the RL algorithms are embodied and\ndiscussed in DM strategies.\n",
        "published": "2020",
        "authors": [
            "Teng Liu",
            "Xiaolin Tang",
            "Jinwei Zhang",
            "Wenbo Li",
            "Zejian Deng",
            "Yalian Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.10675v4",
        "title": "Trade-off on Sim2Real Learning: Real-world Learning Faster than\n  Simulations",
        "abstract": "  Deep Reinforcement Learning (DRL) experiments are commonly performed in\nsimulated environments due to the tremendous training sample demands from deep\nneural networks. In contrast, model-based Bayesian Learning allows a robot to\nlearn good policies within a few trials in the real world. Although it takes\nfewer iterations, Bayesian methods pay a relatively higher computational cost\nper trial, and the advantage of such methods is strongly tied to dimensionality\nand noise. In here, we compare a Deep Bayesian Learning algorithm with a\nmodel-free DRL algorithm while analyzing our results collected from both\nsimulations and real-world experiments. While considering Sim and Real\nlearning, our experiments show that the sample-efficient Deep Bayesian RL\nperformance is better than DRL even when computation time (as opposed to number\nof iterations) is taken in consideration. Additionally, the difference in\ncomputation time between Deep Bayesian RL performed in simulation and in\nexperiments point to a viable path to traverse the reality gap. We also show\nthat a mix between Sim and Real does not outperform a purely Real approach,\npointing to the possibility that reality can provide the best prior knowledge\nto a Bayesian Learning. Roboticists design and build robots every day, and our\nresults show that a higher learning efficiency in the real-world will shorten\nthe time between design and deployment by skipping simulations.\n",
        "published": "2020",
        "authors": [
            "Jingyi Huang",
            "Yizheng Zhang",
            "Fabio Giardina",
            "Andre Rosendo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.12397v1",
        "title": "Learning the Solution Manifold in Optimization and Its Application in\n  Motion Planning",
        "abstract": "  Optimization is an essential component for solving problems in wide-ranging\nfields. Ideally, the objective function should be designed such that the\nsolution is unique and the optimization problem can be solved stably. However,\nthe objective function used in a practical application is usually non-convex,\nand sometimes it even has an infinite set of solutions. To address this issue,\nwe propose to learn the solution manifold in optimization. We train a model\nconditioned on the latent variable such that the model represents an infinite\nset of solutions. In our framework, we reduce this problem to density\nestimation by using importance sampling, and the latent representation of the\nsolutions is learned by maximizing the variational lower bound. We apply the\nproposed algorithm to motion-planning problems, which involve the optimization\nof high-dimensional parameters. The experimental results indicate that the\nsolution manifold can be learned with the proposed algorithm, and the trained\nmodel represents an infinite set of homotopic solutions for motion-planning\nproblems.\n",
        "published": "2020",
        "authors": [
            "Takayuki Osa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.13202v3",
        "title": "CAMPs: Learning Context-Specific Abstractions for Efficient Planning in\n  Factored MDPs",
        "abstract": "  Meta-planning, or learning to guide planning from experience, is a promising\napproach to improving the computational cost of planning. A general\nmeta-planning strategy is to learn to impose constraints on the states\nconsidered and actions taken by the agent. We observe that (1) imposing a\nconstraint can induce context-specific independences that render some aspects\nof the domain irrelevant, and (2) an agent can take advantage of this fact by\nimposing constraints on its own behavior. These observations lead us to propose\nthe context-specific abstract Markov decision process (CAMP), an abstraction of\na factored MDP that affords efficient planning. We then describe how to learn\nconstraints to impose so the CAMP optimizes a trade-off between rewards and\ncomputational cost. Our experiments consider five planners across four domains,\nincluding robotic navigation among movable obstacles (NAMO), robotic task and\nmotion planning for sequential manipulation, and classical planning. We find\nplanning with learned CAMPs to consistently outperform baselines, including\nStilman's NAMO-specific algorithm. Video: https://youtu.be/wTXt6djcAd4 Code:\nhttps://git.io/JTnf6\n",
        "published": "2020",
        "authors": [
            "Rohan Chitnis",
            "Tom Silver",
            "Beomjoon Kim",
            "Leslie Pack Kaelbling",
            "Tomas Lozano-Perez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.14632v1",
        "title": "Tracking Emotions: Intrinsic Motivation Grounded on Multi-Level\n  Prediction Error Dynamics",
        "abstract": "  How do cognitive agents decide what is the relevant information to learn and\nhow goals are selected to gain this knowledge? Cognitive agents need to be\nmotivated to perform any action. We discuss that emotions arise when\ndifferences between expected and actual rates of progress towards a goal are\nexperienced. Therefore, the tracking of prediction error dynamics has a tight\nrelationship with emotions. Here, we suggest that the tracking of prediction\nerror dynamics allows an artificial agent to be intrinsically motivated to seek\nnew experiences but constrained to those that generate reducible prediction\nerror.We present an intrinsic motivation architecture that generates behaviors\ntowards self-generated and dynamic goals and that regulates goal selection and\nthe balance between exploitation and exploration through multi-level monitoring\nof prediction error dynamics. This new architecture modulates exploration noise\nand leverages computational resources according to the dynamics of the overall\nperformance of the learning system. Additionally, it establishes a possible\nsolution to the temporal dynamics of goal selection. The results of the\nexperiments presented here suggest that this architecture outperforms intrinsic\nmotivation approaches where exploratory noise and goals are fixed and a greedy\nstrategy is applied.\n",
        "published": "2020",
        "authors": [
            "Guido Schillaci",
            "Alejandra Ciria",
            "Bruno Lara"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.15375v1",
        "title": "Bayesian Optimization for Developmental Robotics with Meta-Learning by\n  Parameters Bounds Reduction",
        "abstract": "  In robotics, methods and softwares usually require optimizations of\nhyperparameters in order to be efficient for specific tasks, for instance\nindustrial bin-picking from homogeneous heaps of different objects. We present\na developmental framework based on long-term memory and reasoning modules\n(Bayesian Optimisation, visual similarity and parameters bounds reduction)\nallowing a robot to use meta-learning mechanism increasing the efficiency of\nsuch continuous and constrained parameters optimizations. The new optimization,\nviewed as a learning for the robot, can take advantage of past experiences\n(stored in the episodic and procedural memories) to shrink the search space by\nusing reduced parameters bounds computed from the best optimizations realized\nby the robot with similar tasks of the new one (e.g. bin-picking from an\nhomogenous heap of a similar object, based on visual similarity of objects\nstored in the semantic memory). As example, we have confronted the system to\nthe constrained optimizations of 9 continuous hyperparameters for a\nprofessional software (Kamido) in industrial robotic arm bin-picking tasks, a\nstep that is needed each time to handle correctly new object. We used a\nsimulator to create bin-picking tasks for 8 different objects (7 in simulation\nand one with real setup, without and with meta-learning with experiences coming\nfrom other similar objects) achieving goods results despite a very small\noptimization budget, with a better performance reached when meta-learning is\nused (84.3% vs 78.9% of success overall, with a small budget of 30 iterations\nfor each optimization) for every object tested (p-value=0.036).\n",
        "published": "2020",
        "authors": [
            "Maxime Petit",
            "Emmanuel Dellandrea",
            "Liming Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.16162v3",
        "title": "Imitative Planning using Conditional Normalizing Flow",
        "abstract": "  A popular way to plan trajectories in dynamic urban scenarios for Autonomous\nVehicles is to rely on explicitly specified and hand crafted cost functions,\ncoupled with random sampling in the trajectory space to find the minimum cost\ntrajectory. Such methods require a high number of samples to find a low-cost\ntrajectory and might end up with a highly suboptimal trajectory given the\nplanning time budget. We explore the application of normalizing flows for\nimproving the performance of trajectory planning for autonomous vehicles (AVs).\nOur key insight is to learn a sampling policy in a low-dimensional latent space\nof expert-like trajectories, out of which the best sample is selected for\nexecution. By modeling the trajectory planner's cost manifold as an energy\nfunction, we learn a scene conditioned mapping from the prior to a Boltzmann\ndistribution over the AV control space. Finally, we demonstrate the\neffectiveness of our approach on real-world datasets over IL and\nhand-constructed trajectory sampling techniques.\n",
        "published": "2020",
        "authors": [
            "Shubhankar Agarwal",
            "Harshit Sikchi",
            "Cole Gulino",
            "Eric Wilkinson",
            "Shivam Gautam"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.03787v1",
        "title": "Neural Manipulation Planning on Constraint Manifolds",
        "abstract": "  The presence of task constraints imposes a significant challenge to motion\nplanning. Despite all recent advancements, existing algorithms are still\ncomputationally expensive for most planning problems. In this paper, we present\nConstrained Motion Planning Networks (CoMPNet), the first neural planner for\nmultimodal kinematic constraints. Our approach comprises the following\ncomponents: i) constraint and environment perception encoders; ii) neural robot\nconfiguration generator that outputs configurations on/near the constraint\nmanifold(s), and iii) a bidirectional planning algorithm that takes the\ngenerated configurations to create a feasible robot motion trajectory. We show\nthat CoMPNet solves practical motion planning tasks involving both\nunconstrained and constrained problems. Furthermore, it generalizes to new\nunseen locations of the objects, i.e., not seen during training, in the given\nenvironments with high success rates. When compared to the state-of-the-art\nconstrained motion planning algorithms, CoMPNet outperforms by order of\nmagnitude improvement in computational speed with a significantly lower\nvariance.\n",
        "published": "2020",
        "authors": [
            "Ahmed H. Qureshi",
            "Jiangeng Dong",
            "Austin Choe",
            "Michael C. Yip"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.04007v1",
        "title": "Imitation Learning for Autonomous Trajectory Learning of Robot Arms in\n  Space",
        "abstract": "  This work adds on to the on-going efforts to provide more autonomy to space\nrobots. Here the concept of programming by demonstration or imitation learning\nis used for trajectory planning of manipulators mounted on small spacecraft.\nFor greater autonomy in future space missions and minimal human intervention\nthrough ground control, a robot arm having 7-Degrees of Freedom (DoF) is\nenvisaged for carrying out multiple tasks like debris removal, on-orbit\nservicing and assembly. Since actual hardware implementation of microgravity\nenvironment is extremely expensive, the demonstration data for trajectory\nlearning is generated using a model predictive controller (MPC) in a physics\nbased simulator. The data is then encoded compactly by Probabilistic Movement\nPrimitives (ProMPs). This offline trajectory learning allows faster\nreproductions and also avoids any computationally expensive optimizations after\ndeployment in a space environment. It is shown that the probabilistic\ndistribution can be used to generate trajectories to previously unseen\nsituations by conditioning the distribution. The motion of the robot (or\nmanipulator) arm induces reaction forces on the spacecraft hub and hence its\nattitude changes prompting the Attitude Determination and Control System (ADCS)\nto take large corrective action that drains energy out of the system. By having\na robot arm with redundant DoF helps in finding several possible trajectories\nfrom the same start to the same target. This allows the ProMP trajectory\ngenerator to sample out the trajectory which is obstacle free as well as having\nminimal attitudinal disturbances thereby reducing the load on ADCS.\n",
        "published": "2020",
        "authors": [
            "RB Ashith Shyam",
            "Zhou Hao",
            "Umberto Montanaro",
            "Gerhard Neumann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.06073v1",
        "title": "Visuomotor Mechanical Search: Learning to Retrieve Target Objects in\n  Clutter",
        "abstract": "  When searching for objects in cluttered environments, it is often necessary\nto perform complex interactions in order to move occluding objects out of the\nway and fully reveal the object of interest and make it graspable. Due to the\ncomplexity of the physics involved and the lack of accurate models of the\nclutter, planning and controlling precise predefined interactions with accurate\noutcome is extremely hard, when not impossible. In problems where accurate\n(forward) models are lacking, Deep Reinforcement Learning (RL) has shown to be\na viable solution to map observations (e.g. images) to good interactions in the\nform of close-loop visuomotor policies. However, Deep RL is sample inefficient\nand fails when applied directly to the problem of unoccluding objects based on\nimages. In this work we present a novel Deep RL procedure that combines i)\nteacher-aided exploration, ii) a critic with privileged information, and iii)\nmid-level representations, resulting in sample efficient and effective learning\nfor the problem of uncovering a target object occluded by a heap of unknown\nobjects. Our experiments show that our approach trains faster and converges to\nmore efficient uncovering solutions than baselines and ablations, and that our\nuncovering policies lead to an average improvement in the graspability of the\ntarget object, facilitating downstream retrieval applications.\n",
        "published": "2020",
        "authors": [
            "Andrey Kurenkov",
            "Joseph Taglic",
            "Rohun Kulkarni",
            "Marcus Dominguez-Kuhne",
            "Animesh Garg",
            "Roberto Mart\u00edn-Mart\u00edn",
            "Silvio Savarese"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.06626v1",
        "title": "Safe Reinforcement Learning in Constrained Markov Decision Processes",
        "abstract": "  Safe reinforcement learning has been a promising approach for optimizing the\npolicy of an agent that operates in safety-critical applications. In this\npaper, we propose an algorithm, SNO-MDP, that explores and optimizes Markov\ndecision processes under unknown safety constraints. Specifically, we take a\nstepwise approach for optimizing safety and cumulative reward. In our method,\nthe agent first learns safety constraints by expanding the safe region, and\nthen optimizes the cumulative reward in the certified safe region. We provide\ntheoretical guarantees on both the satisfaction of the safety constraint and\nthe near-optimality of the cumulative reward under proper regularity\nassumptions. In our experiments, we demonstrate the effectiveness of SNO-MDP\nthrough two experiments: one uses a synthetic data in a new, openly-available\nenvironment named GP-SAFETY-GYM, and the other simulates Mars surface\nexploration by using real observation data.\n",
        "published": "2020",
        "authors": [
            "Akifumi Wachi",
            "Yanan Sui"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.06696v1",
        "title": "Autonomous Braking and Throttle System: A Deep Reinforcement Learning\n  Approach for Naturalistic Driving",
        "abstract": "  Autonomous Braking and Throttle control is key in developing safe driving\nsystems for the future. There exists a need for autonomous vehicles to\nnegotiate a multi-agent environment while ensuring safety and comfort. A Deep\nReinforcement Learning based autonomous throttle and braking system is\npresented. For each time step, the proposed system makes a decision to apply\nthe brake or throttle. The throttle and brake are modelled as continuous action\nspace values. We demonstrate 2 scenarios where there is a need for a\nsophisticated braking and throttle system, i.e when there is a static obstacle\nin front of our agent like a car, stop sign. The second scenario consists of 2\nvehicles approaching an intersection. The policies for brake and throttle\ncontrol are learned through computer simulation using Deep deterministic policy\ngradients. The experiment shows that the system not only avoids a collision,\nbut also it ensures that there is smooth change in the values of throttle/brake\nas it gets out of the emergency situation and abides by the speed regulations,\ni.e the system resembles human driving.\n",
        "published": "2020",
        "authors": [
            "Varshit S. Dubey",
            "Ruhshad Kasad",
            "Karan Agrawal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.07284v2",
        "title": "Forward and inverse reinforcement learning sharing network weights and\n  hyperparameters",
        "abstract": "  This paper proposes model-free imitation learning named Entropy-Regularized\nImitation Learning (ERIL) that minimizes the reverse Kullback-Leibler (KL)\ndivergence. ERIL combines forward and inverse reinforcement learning (RL) under\nthe framework of an entropy-regularized Markov decision process. An inverse RL\nstep computes the log-ratio between two distributions by evaluating two binary\ndiscriminators. The first discriminator distinguishes the state generated by\nthe forward RL step from the expert's state. The second discriminator, which is\nstructured by the theory of entropy regularization, distinguishes the\nstate-action-next-state tuples generated by the learner from the expert ones.\nOne notable feature is that the second discriminator shares hyperparameters\nwith the forward RL, which can be used to control the discriminator's ability.\nA forward RL step minimizes the reverse KL estimated by the inverse RL step. We\nshow that minimizing the reverse KL divergence is equivalent to finding an\noptimal policy. Our experimental results on MuJoCo-simulated environments and\nvision-based reaching tasks with a robotic arm show that ERIL is more\nsample-efficient than the baseline methods. We apply the method to human\nbehaviors that perform a pole-balancing task and describe how the estimated\nreward functions show how every subject achieves her goal.\n",
        "published": "2020",
        "authors": [
            "Eiji Uchibe",
            "Kenji Doya"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.07965v2",
        "title": "Analysis of Social Robotic Navigation approaches: CNN Encoder and\n  Incremental Learning as an alternative to Deep Reinforcement Learning",
        "abstract": "  Dealing with social tasks in robotic scenarios is difficult, as having humans\nin the learning loop is incompatible with most of the state-of-the-art machine\nlearning algorithms. This is the case when exploring Incremental learning\nmodels, in particular the ones involving reinforcement learning. In this work,\nwe discuss this problem and possible solutions by analysing a previous study on\nadaptive convolutional encoders for a social navigation task.\n",
        "published": "2020",
        "authors": [
            "Janderson Ferreira",
            "Agostinho A. F. J\u00fanior",
            "Let\u00edcia Castro",
            "Yves M. Galv\u00e3o",
            "Pablo Barros",
            "Bruno J. T. Fernandes"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.07971v2",
        "title": "Super-Human Performance in Gran Turismo Sport Using Deep Reinforcement\n  Learning",
        "abstract": "  Autonomous car racing is a major challenge in robotics. It raises fundamental\nproblems for classical approaches such as planning minimum-time trajectories\nunder uncertain dynamics and controlling the car at the limits of its handling.\nBesides, the requirement of minimizing the lap time, which is a sparse\nobjective, and the difficulty of collecting training data from human experts\nhave also hindered researchers from directly applying learning-based approaches\nto solve the problem. In the present work, we propose a learning-based system\nfor autonomous car racing by leveraging a high-fidelity physical car\nsimulation, a course-progress proxy reward, and deep reinforcement learning. We\ndeploy our system in Gran Turismo Sport, a world-leading car simulator known\nfor its realistic physics simulation of different race cars and tracks, which\nis even used to recruit human race car drivers. Our trained policy achieves\nautonomous racing performance that goes beyond what had been achieved so far by\nthe built-in AI, and, at the same time, outperforms the fastest driver in a\ndataset of over 50,000 human players.\n",
        "published": "2020",
        "authors": [
            "Florian Fuchs",
            "Yunlong Song",
            "Elia Kaufmann",
            "Davide Scaramuzza",
            "Peter Duerr"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.10066v5",
        "title": "Learning Off-Policy with Online Planning",
        "abstract": "  Reinforcement learning (RL) in low-data and risk-sensitive domains requires\nperformant and flexible deployment policies that can readily incorporate\nconstraints during deployment. One such class of policies are the\nsemi-parametric H-step lookahead policies, which select actions using\ntrajectory optimization over a dynamics model for a fixed horizon with a\nterminal value function. In this work, we investigate a novel instantiation of\nH-step lookahead with a learned model and a terminal value function learned by\na model-free off-policy algorithm, named Learning Off-Policy with Online\nPlanning (LOOP). We provide a theoretical analysis of this method, suggesting a\ntradeoff between model errors and value function errors and empirically\ndemonstrate this tradeoff to be beneficial in deep reinforcement learning.\nFurthermore, we identify the \"Actor Divergence\" issue in this framework and\npropose Actor Regularized Control (ARC), a modified trajectory optimization\nprocedure. We evaluate our method on a set of robotic tasks for Offline and\nOnline RL and demonstrate improved performance. We also show the flexibility of\nLOOP to incorporate safety constraints during deployment with a set of\nnavigation environments. We demonstrate that LOOP is a desirable framework for\nrobotics applications based on its strong performance in various important RL\nsettings. Project video and details can be found at\nhttps://hari-sikchi.github.io/loop .\n",
        "published": "2020",
        "authors": [
            "Harshit Sikchi",
            "Wenxuan Zhou",
            "David Held"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.12624v1",
        "title": "A Framework for Studying Reinforcement Learning and Sim-to-Real in Robot\n  Soccer",
        "abstract": "  This article introduces an open framework, called VSSS-RL, for studying\nReinforcement Learning (RL) and sim-to-real in robot soccer, focusing on the\nIEEE Very Small Size Soccer (VSSS) league. We propose a simulated environment\nin which continuous or discrete control policies can be trained to control the\ncomplete behavior of soccer agents and a sim-to-real method based on domain\nadaptation to adapt the obtained policies to real robots. Our results show that\nthe trained policies learned a broad repertoire of behaviors that are difficult\nto implement with handcrafted control policies. With VSSS-RL, we were able to\nbeat human-designed policies in the 2019 Latin American Robotics Competition\n(LARC), achieving 4th place out of 21 teams, being the first to apply\nReinforcement Learning (RL) successfully in this competition. Both environment\nand hardware specifications are available open-source to allow reproducibility\nof our results and further studies.\n",
        "published": "2020",
        "authors": [
            "Hansenclever F. Bassani",
            "Renie A. Delgado",
            "Jos\u00e9 Nilton de O. Lima Junior",
            "Heitor R. Medeiros",
            "Pedro H. M. Braga",
            "Mateus G. Machado",
            "Lucas H. C. Santos",
            "Alain Tapp"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.02293v1",
        "title": "Using Soft Actor-Critic for Low-Level UAV Control",
        "abstract": "  Unmanned Aerial Vehicles (UAVs), or drones, have recently been used in\nseveral civil application domains from organ delivery to remote locations to\nwireless network coverage. These platforms, however, are naturally unstable\nsystems for which many different control approaches have been proposed.\nGenerally based on classic and modern control, these algorithms require\nknowledge of the robot's dynamics. However, recently, model-free reinforcement\nlearning has been successfully used for controlling drones without any prior\nknowledge of the robot model. In this work, we present a framework to train the\nSoft Actor-Critic (SAC) algorithm to low-level control of a quadrotor in a\ngo-to-target task. All experiments were conducted under simulation. With the\nexperiments, we show that SAC can not only learn a robust policy, but it can\nalso cope with unseen scenarios. Videos from the simulations are available in\nhttps://www.youtube.com/watch?v=9z8vGs0Ri5g and the code in\nhttps://github.com/larocs/SAC_uav.\n",
        "published": "2020",
        "authors": [
            "Gabriel Moraes Barros",
            "Esther Luna Colombini"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.03110v4",
        "title": "Causal Curiosity: RL Agents Discovering Self-supervised Experiments for\n  Causal Representation Learning",
        "abstract": "  Animals exhibit an innate ability to learn regularities of the world through\ninteraction. By performing experiments in their environment, they are able to\ndiscern the causal factors of variation and infer how they affect the world's\ndynamics. Inspired by this, we attempt to equip reinforcement learning agents\nwith the ability to perform experiments that facilitate a categorization of the\nrolled-out trajectories, and to subsequently infer the causal factors of the\nenvironment in a hierarchical manner. We introduce {\\em causal curiosity}, a\nnovel intrinsic reward, and show that it allows our agents to learn optimal\nsequences of actions and discover causal factors in the dynamics of the\nenvironment. The learned behavior allows the agents to infer a binary quantized\nrepresentation for the ground-truth causal factors in every environment.\nAdditionally, we find that these experimental behaviors are semantically\nmeaningful (e.g., our agents learn to lift blocks to categorize them by\nweight), and are learnt in a self-supervised manner with approximately 2.5\ntimes less data than conventional supervised planners. We show that these\nbehaviors can be re-purposed and fine-tuned (e.g., from lifting to pushing or\nother downstream tasks). Finally, we show that the knowledge of causal factor\nrepresentations aids zero-shot learning for more complex tasks. Visit\nhttps://sites.google.com/usc.edu/causal-curiosity/home for website.\n",
        "published": "2020",
        "authors": [
            "Sumedh A. Sontakke",
            "Arash Mehrjou",
            "Laurent Itti",
            "Bernhard Sch\u00f6lkopf"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.03152v1",
        "title": "Projection-Based Constrained Policy Optimization",
        "abstract": "  We consider the problem of learning control policies that optimize a reward\nfunction while satisfying constraints due to considerations of safety,\nfairness, or other costs. We propose a new algorithm, Projection-Based\nConstrained Policy Optimization (PCPO). This is an iterative method for\noptimizing policies in a two-step process: the first step performs a local\nreward improvement update, while the second step reconciles any constraint\nviolation by projecting the policy back onto the constraint set. We\ntheoretically analyze PCPO and provide a lower bound on reward improvement, and\nan upper bound on constraint violation, for each policy update. We further\ncharacterize the convergence of PCPO based on two different metrics:\n$\\normltwo$ norm and Kullback-Leibler divergence. Our empirical results over\nseveral control tasks demonstrate that PCPO achieves superior performance,\naveraging more than 3.5 times less constraint violation and around 15\\% higher\nreward compared to state-of-the-art methods.\n",
        "published": "2020",
        "authors": [
            "Tsung-Yen Yang",
            "Justinian Rosca",
            "Karthik Narasimhan",
            "Peter J. Ramadge"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.03848v2",
        "title": "Guided Curriculum Learning for Walking Over Complex Terrain",
        "abstract": "  Reliable bipedal walking over complex terrain is a challenging problem, using\na curriculum can help learning. Curriculum learning is the idea of starting\nwith an achievable version of a task and increasing the difficulty as a success\ncriteria is met. We propose a 3-stage curriculum to train Deep Reinforcement\nLearning policies for bipedal walking over various challenging terrains. In the\nfirst stage, the agent starts on an easy terrain and the terrain difficulty is\ngradually increased, while forces derived from a target policy are applied to\nthe robot joints and the base. In the second stage, the guiding forces are\ngradually reduced to zero. Finally, in the third stage, random perturbations\nwith increasing magnitude are applied to the robot base, so the robustness of\nthe policies are improved. In simulation experiments, we show that our approach\nis effective in learning walking policies, separate from each other, for five\nterrain types: flat, hurdles, gaps, stairs, and steps. Moreover, we demonstrate\nthat in the absence of human demonstrations, a simple hand designed walking\ntrajectory is a sufficient prior to learn to traverse complex terrain types. In\nablation studies, we show that taking out any one of the three stages of the\ncurriculum degrades the learning performance.\n",
        "published": "2020",
        "authors": [
            "Brendan Tidd",
            "Nicolas Hudson",
            "Akansel Cosgun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.05134v2",
        "title": "Deep Imitation Learning for Bimanual Robotic Manipulation",
        "abstract": "  We present a deep imitation learning framework for robotic bimanual\nmanipulation in a continuous state-action space. A core challenge is to\ngeneralize the manipulation skills to objects in different locations. We\nhypothesize that modeling the relational information in the environment can\nsignificantly improve generalization. To achieve this, we propose to (i)\ndecompose the multi-modal dynamics into elemental movement primitives, (ii)\nparameterize each primitive using a recurrent graph neural network to capture\ninteractions, and (iii) integrate a high-level planner that composes primitives\nsequentially and a low-level controller to combine primitive dynamics and\ninverse kinematics control. Our model is a deep, hierarchical, modular\narchitecture. Compared to baselines, our model generalizes better and achieves\nhigher success rates on several simulated bimanual robotic manipulation tasks.\nWe open source the code for simulation, data, and models at:\nhttps://github.com/Rose-STL-Lab/HDR-IL.\n",
        "published": "2020",
        "authors": [
            "Fan Xie",
            "Alexander Chowdhury",
            "M. Clara De Paolis Kaluza",
            "Linfeng Zhao",
            "Lawson L. S. Wong",
            "Rose Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.07935v1",
        "title": "Multi-Agent Motion Planning using Deep Learning for Space Applications",
        "abstract": "  State-of-the-art motion planners cannot scale to a large number of systems.\nMotion planning for multiple agents is an NP (non-deterministic\npolynomial-time) hard problem, so the computation time increases exponentially\nwith each addition of agents. This computational demand is a major stumbling\nblock to the motion planner's application to future NASA missions involving the\nswarm of space vehicles. We applied a deep neural network to transform\ncomputationally demanding mathematical motion planning problems into deep\nlearning-based numerical problems. We showed optimal motion trajectories can be\naccurately replicated using deep learning-based numerical models in several 2D\nand 3D systems with multiple agents. The deep learning-based numerical model\ndemonstrates superior computational efficiency with plans generated 1000 times\nfaster than the mathematical model counterpart.\n",
        "published": "2020",
        "authors": [
            "Kyongsik Yun",
            "Changrak Choi",
            "Ryan Alimo",
            "Anthony Davis",
            "Linda Forster",
            "Amir Rahmani",
            "Muhammad Adil",
            "Ramtin Madani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.07968v2",
        "title": "Constrained Model-based Reinforcement Learning with Robust Cross-Entropy\n  Method",
        "abstract": "  This paper studies the constrained/safe reinforcement learning (RL) problem\nwith sparse indicator signals for constraint violations. We propose a\nmodel-based approach to enable RL agents to effectively explore the environment\nwith unknown system dynamics and environment constraints given a significantly\nsmall number of violation budgets. We employ the neural network ensemble model\nto estimate the prediction uncertainty and use model predictive control as the\nbasic control framework. We propose the robust cross-entropy method to optimize\nthe control sequence considering the model uncertainty and constraints. We\nevaluate our methods in the Safety Gym environment. The results show that our\napproach learns to complete the tasks with a much smaller number of constraint\nviolations than state-of-the-art baselines. Additionally, we are able to\nachieve several orders of magnitude better sample efficiency when compared with\nconstrained model-free RL approaches. The code is available at\n\\url{https://github.com/liuzuxin/safe-mbrl}.\n",
        "published": "2020",
        "authors": [
            "Zuxin Liu",
            "Hongyi Zhou",
            "Baiming Chen",
            "Sicheng Zhong",
            "Martial Hebert",
            "Ding Zhao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.08707v2",
        "title": "Constrained Motion Planning Networks X",
        "abstract": "  Constrained motion planning is a challenging field of research, aiming for\ncomputationally efficient methods that can find a collision-free path on the\nconstraint manifolds between a given start and goal configuration. These\nplanning problems come up surprisingly frequently, such as in robot\nmanipulation for performing daily life assistive tasks. However, few solutions\nto constrained motion planning are available, and those that exist struggle\nwith high computational time complexity in finding a path solution on the\nmanifolds. To address this challenge, we present Constrained Motion Planning\nNetworks X (CoMPNetX). It is a neural planning approach, comprising a\nconditional deep neural generator and discriminator with neural gradients-based\nfast projection operator. We also introduce neural task and scene\nrepresentations conditioned on which the CoMPNetX generates implicit manifold\nconfigurations to turbo-charge any underlying classical planner such as\nSampling-based Motion Planning methods for quickly solving complex constrained\nplanning tasks. We show that our method finds path solutions with high success\nrates and lower computation times than state-of-the-art traditional\npath-finding tools on various challenging scenarios.\n",
        "published": "2020",
        "authors": [
            "Ahmed H. Qureshi",
            "Jiangeng Dong",
            "Asfiya Baig",
            "Michael C. Yip"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.09170v5",
        "title": "Belief-Grounded Networks for Accelerated Robot Learning under Partial\n  Observability",
        "abstract": "  Many important robotics problems are partially observable in the sense that a\nsingle visual or force-feedback measurement is insufficient to reconstruct the\nstate. Standard approaches involve learning a policy over beliefs or\nobservation-action histories. However, both of these have drawbacks; it is\nexpensive to track the belief online, and it is hard to learn policies directly\nover histories. We propose a method for policy learning under partial\nobservability called the Belief-Grounded Network (BGN) in which an auxiliary\nbelief-reconstruction loss incentivizes a neural network to concisely summarize\nits input history. Since the resulting policy is a function of the history\nrather than the belief, it can be executed easily at runtime. We compare BGN\nagainst several baselines on classic benchmark tasks as well as three novel\nrobotic touch-sensing tasks. BGN outperforms all other tested methods and its\nlearned policies work well when transferred onto a physical robot.\n",
        "published": "2020",
        "authors": [
            "Hai Nguyen",
            "Brett Daley",
            "Xinchao Song",
            "Christopher Amato",
            "Robert Platt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.09842v4",
        "title": "Robot Design With Neural Networks, MILP Solvers and Active Learning",
        "abstract": "  Central to the design of many robot systems and their controllers is solving\na constrained blackbox optimization problem. This paper presents CNMA, a new\nmethod of solving this problem that is conservative in the number of\npotentially expensive blackbox function evaluations; allows specifying complex,\neven recursive constraints directly rather than as hard-to-design penalty or\nbarrier functions; and is resilient to the non-termination of function\nevaluations. CNMA leverages the ability of neural networks to approximate any\ncontinuous function, their transformation into equivalent mixed integer linear\nprograms (MILPs) and their optimization subject to constraints with industrial\nstrength MILP solvers. A new learning-from-failure step guides the learning to\nbe relevant to solving the constrained optimization problem. Thus, the amount\nof learning is orders of magnitude smaller than that needed to learn functions\nover their entire domains. CNMA is illustrated with the design of several\nrobotic systems: wave-energy propelled boat, lunar lander, hexapod, cartpole,\nacrobot and parallel parking. These range from 6 real-valued dimensions to 36.\nWe show that CNMA surpasses the Nelder-Mead, Gaussian and Random Search\noptimization methods against the metric of number of function evaluations.\n",
        "published": "2020",
        "authors": [
            "Sanjai Narain",
            "Emily Mak",
            "Dana Chee",
            "Todd Huster",
            "Jeremy Cohen",
            "Kishore Pochiraju",
            "Brendan Englot",
            "Niraj K. Jha",
            "Karthik Narayan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.11344v2",
        "title": "Trajectory Prediction using Equivariant Continuous Convolution",
        "abstract": "  Trajectory prediction is a critical part of many AI applications, for\nexample, the safe operation of autonomous vehicles. However, current methods\nare prone to making inconsistent and physically unrealistic predictions. We\nleverage insights from fluid dynamics to overcome this limitation by\nconsidering internal symmetry in real-world trajectories. We propose a novel\nmodel, Equivariant Continous COnvolution (ECCO) for improved trajectory\nprediction. ECCO uses rotationally-equivariant continuous convolutions to embed\nthe symmetries of the system. On both vehicle and pedestrian trajectory\ndatasets, ECCO attains competitive accuracy with significantly fewer\nparameters. It is also more sample efficient, generalizing automatically from\nfew data points in any orientation. Lastly, ECCO improves generalization with\nequivariance, resulting in more physically consistent predictions. Our method\nprovides a fresh perspective towards increasing trust and transparency in deep\nlearning models.\n",
        "published": "2020",
        "authors": [
            "Robin Walters",
            "Jinxi Li",
            "Rose Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.11370v1",
        "title": "A Multi-Componential Approach to Emotion Recognition and the Effect of\n  Personality",
        "abstract": "  Emotions are an inseparable part of human nature affecting our behavior in\nresponse to the outside world. Although most empirical studies have been\ndominated by two theoretical models including discrete categories of emotion\nand dichotomous dimensions, results from neuroscience approaches suggest a\nmulti-processes mechanism underpinning emotional experience with a large\noverlap across different emotions. While these findings are consistent with the\ninfluential theories of emotion in psychology that emphasize a role for\nmultiple component processes to generate emotion episodes, few studies have\nsystematically investigated the relationship between discrete emotions and a\nfull componential view. This paper applies a componential framework with a\ndata-driven approach to characterize emotional experiences evoked during movie\nwatching. The results suggest that differences between various emotions can be\ncaptured by a few (at least 6) latent dimensions, each defined by features\nassociated with component processes, including appraisal, expression,\nphysiology, motivation, and feeling. In addition, the link between discrete\nemotions and component model is explored and results show that a componential\nmodel with a limited number of descriptors is still able to predict the level\nof experienced discrete emotion(s) to a satisfactory level. Finally, as\nappraisals may vary according to individual dispositions and biases, we also\nstudy the relationship between personality traits and emotions in our\ncomputational framework and show that the role of personality on discrete\nemotion differences can be better justified using the component model.\n",
        "published": "2020",
        "authors": [
            "Gelareh Mohammadi",
            "Patrik Vuilleumier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.11917v2",
        "title": "Batch Exploration with Examples for Scalable Robotic Reinforcement\n  Learning",
        "abstract": "  Learning from diverse offline datasets is a promising path towards learning\ngeneral purpose robotic agents. However, a core challenge in this paradigm lies\nin collecting large amounts of meaningful data, while not depending on a human\nin the loop for data collection. One way to address this challenge is through\ntask-agnostic exploration, where an agent attempts to explore without a\ntask-specific reward function, and collect data that can be useful for any\ndownstream task. While these approaches have shown some promise in simple\ndomains, they often struggle to explore the relevant regions of the state space\nin more challenging settings, such as vision based robotic manipulation. This\nchallenge stems from an objective that encourages exploring everything in a\npotentially vast state space. To mitigate this challenge, we propose to focus\nexploration on the important parts of the state space using weak human\nsupervision. Concretely, we propose an exploration technique, Batch Exploration\nwith Examples (BEE), that explores relevant regions of the state-space, guided\nby a modest number of human provided images of important states. These human\nprovided images only need to be collected once at the beginning of data\ncollection and can be collected in a matter of minutes, allowing us to scalably\ncollect diverse datasets, which can then be combined with any batch RL\nalgorithm. We find that BEE is able to tackle challenging vision-based\nmanipulation tasks both in simulation and on a real Franka robot, and observe\nthat compared to task-agnostic and weakly-supervised exploration techniques, it\n(1) interacts more than twice as often with relevant objects, and (2) improves\ndownstream task performance when used in conjunction with offline RL.\n",
        "published": "2020",
        "authors": [
            "Annie S. Chen",
            "HyunJi Nam",
            "Suraj Nair",
            "Chelsea Finn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.11940v1",
        "title": "Motion Planner Augmented Reinforcement Learning for Robot Manipulation\n  in Obstructed Environments",
        "abstract": "  Deep reinforcement learning (RL) agents are able to learn contact-rich\nmanipulation tasks by maximizing a reward signal, but require large amounts of\nexperience, especially in environments with many obstacles that complicate\nexploration. In contrast, motion planners use explicit models of the agent and\nenvironment to plan collision-free paths to faraway goals, but suffer from\ninaccurate models in tasks that require contacts with the environment. To\ncombine the benefits of both approaches, we propose motion planner augmented RL\n(MoPA-RL) which augments the action space of an RL agent with the long-horizon\nplanning capabilities of motion planners. Based on the magnitude of the action,\nour approach smoothly transitions between directly executing the action and\ninvoking a motion planner. We evaluate our approach on various simulated\nmanipulation tasks and compare it to alternative action spaces in terms of\nlearning efficiency and safety. The experiments demonstrate that MoPA-RL\nincreases learning efficiency, leads to a faster exploration, and results in\nsafer policies that avoid collisions with the environment. Videos and code are\navailable at https://clvrai.com/mopa-rl .\n",
        "published": "2020",
        "authors": [
            "Jun Yamada",
            "Youngwoon Lee",
            "Gautam Salhotra",
            "Karl Pertsch",
            "Max Pflueger",
            "Gaurav S. Sukhatme",
            "Joseph J. Lim",
            "Peter Englert"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.11944v1",
        "title": "Accelerating Reinforcement Learning with Learned Skill Priors",
        "abstract": "  Intelligent agents rely heavily on prior experience when learning a new task,\nyet most modern reinforcement learning (RL) approaches learn every task from\nscratch. One approach for leveraging prior knowledge is to transfer skills\nlearned on prior tasks to the new task. However, as the amount of prior\nexperience increases, the number of transferable skills grows too, making it\nchallenging to explore the full set of available skills during downstream\nlearning. Yet, intuitively, not all skills should be explored with equal\nprobability; for example information about the current state can hint which\nskills are promising to explore. In this work, we propose to implement this\nintuition by learning a prior over skills. We propose a deep latent variable\nmodel that jointly learns an embedding space of skills and the skill prior from\noffline agent experience. We then extend common maximum-entropy RL approaches\nto use skill priors to guide downstream learning. We validate our approach,\nSPiRL (Skill-Prior RL), on complex navigation and robotic manipulation tasks\nand show that learned skill priors are essential for effective skill transfer\nfrom rich datasets. Videos and code are available at https://clvrai.com/spirl.\n",
        "published": "2020",
        "authors": [
            "Karl Pertsch",
            "Youngwoon Lee",
            "Joseph J. Lim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.14641v3",
        "title": "Learning to Plan Optimistically: Uncertainty-Guided Deep Exploration via\n  Latent Model Ensembles",
        "abstract": "  Learning complex robot behaviors through interaction requires structured\nexploration. Planning should target interactions with the potential to optimize\nlong-term performance, while only reducing uncertainty where conducive to this\nobjective. This paper presents Latent Optimistic Value Exploration (LOVE), a\nstrategy that enables deep exploration through optimism in the face of\nuncertain long-term rewards. We combine latent world models with value function\nestimation to predict infinite-horizon returns and recover associated\nuncertainty via ensembling. The policy is then trained on an upper confidence\nbound (UCB) objective to identify and select the interactions most promising to\nimprove long-term performance. We apply LOVE to visual robot control tasks in\ncontinuous action spaces and demonstrate on average more than 20% improved\nsample efficiency in comparison to state-of-the-art and other exploration\nobjectives. In sparse and hard to explore environments we achieve an average\nimprovement of over 30%.\n",
        "published": "2020",
        "authors": [
            "Tim Seyde",
            "Wilko Schwarting",
            "Sertac Karaman",
            "Daniela Rus"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.14712v8",
        "title": "Socially-Compatible Behavior Design of Autonomous Vehicles with\n  Verification on Real Human Data",
        "abstract": "  As more and more autonomous vehicles (AVs) are being deployed on public\nroads, designing socially compatible behaviors for them is becoming\nincreasingly important. In order to generate safe and efficient actions, AVs\nneed to not only predict the future behaviors of other traffic participants,\nbut also be aware of the uncertainties associated with such behavior\nprediction. In this paper, we propose an uncertain-aware integrated prediction\nand planning (UAPP) framework. It allows the AVs to infer the characteristics\nof other road users online and generate behaviors optimizing not only their own\nrewards, but also their courtesy to others, and their confidence regarding the\nprediction uncertainties. We first propose the definitions for courtesy and\nconfidence. Based on that, their influences on the behaviors of AVs in\ninteractive driving scenarios are explored. Moreover, we evaluate the proposed\nalgorithm on naturalistic human driving data by comparing the generated\nbehavior against ground truth. Results show that the online inference can\nsignificantly improve the human-likeness of the generated behaviors.\nFurthermore, we find that human drivers show great courtesy to others, even for\nthose without right-of-way. We also find that such driving preferences vary\nsignificantly in different cultures.\n",
        "published": "2020",
        "authors": [
            "Letian Wang",
            "Liting Sun",
            "Masayoshi Tomizuka",
            "Wei Zhan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.15335v3",
        "title": "Learning Sampling Distributions Using Local 3D Workspace Decompositions\n  for Motion Planning in High Dimensions",
        "abstract": "  Earlier work has shown that reusing experience from prior motion planning\nproblems can improve the efficiency of similar, future motion planning queries.\nHowever, for robots with many degrees-of-freedom, these methods exhibit poor\ngeneralization across different environments and often require large datasets\nthat are impractical to gather. We present SPARK and FLAME , two\nexperience-based frameworks for sampling-based planning applicable to complex\nmanipulators in 3 D environments. Both combine samplers associated with\nfeatures from a workspace decomposition into a global biased sampling\ndistribution. SPARK decomposes the environment based on exact geometry while\nFLAME is more general, and uses an octree-based decomposition obtained from\nsensor data. We demonstrate the effectiveness of SPARK and FLAME on a Fetch\nrobot tasked with challenging pick-and-place manipulation problems. Our\napproaches can be trained incrementally and significantly improve performance\nwith only a handful of examples, generalizing better over diverse tasks and\nenvironments as compared to prior approaches.\n",
        "published": "2020",
        "authors": [
            "Constantinos Chamzas",
            "Zachary Kingston",
            "Carlos Quintero-Pe\u00f1a",
            "Anshumali Shrivastava",
            "Lydia E. Kavraki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.15469v1",
        "title": "Emergence of Spatial Coordinates via Exploration",
        "abstract": "  Spatial knowledge is a fundamental building block for the development of\nadvanced perceptive and cognitive abilities. Traditionally, in robotics, the\nEuclidean (x,y,z) coordinate system and the agent's forward model are defined a\npriori. We show that a naive agent can autonomously build an internal\ncoordinate system, with the same dimension and metric regularity as the\nexternal space, simply by learning to predict the outcome of sensorimotor\ntransitions in a self-supervised way.\n",
        "published": "2020",
        "authors": [
            "Alban Laflaqui\u00e8re"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.15920v2",
        "title": "Recovery RL: Safe Reinforcement Learning with Learned Recovery Zones",
        "abstract": "  Safety remains a central obstacle preventing widespread use of RL in the real\nworld: learning new tasks in uncertain environments requires extensive\nexploration, but safety requires limiting exploration. We propose Recovery RL,\nan algorithm which navigates this tradeoff by (1) leveraging offline data to\nlearn about constraint violating zones before policy learning and (2)\nseparating the goals of improving task performance and constraint satisfaction\nacross two policies: a task policy that only optimizes the task reward and a\nrecovery policy that guides the agent to safety when constraint violation is\nlikely. We evaluate Recovery RL on 6 simulation domains, including two\ncontact-rich manipulation tasks and an image-based navigation task, and an\nimage-based obstacle avoidance task on a physical robot. We compare Recovery RL\nto 5 prior safe RL methods which jointly optimize for task performance and\nsafety via constrained optimization or reward shaping and find that Recovery RL\noutperforms the next best prior method across all domains. Results suggest that\nRecovery RL trades off constraint violations and task successes 2 - 20 times\nmore efficiently in simulation domains and 3 times more efficiently in physical\nexperiments. See https://tinyurl.com/rl-recovery for videos and supplementary\nmaterial.\n",
        "published": "2020",
        "authors": [
            "Brijen Thananjeyan",
            "Ashwin Balakrishna",
            "Suraj Nair",
            "Michael Luo",
            "Krishnan Srinivasan",
            "Minho Hwang",
            "Joseph E. Gonzalez",
            "Julian Ibarz",
            "Chelsea Finn",
            "Ken Goldberg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.05970v1",
        "title": "Affordance-based Reinforcement Learning for Urban Driving",
        "abstract": "  Traditional autonomous vehicle pipelines that follow a modular approach have\nbeen very successful in the past both in academia and industry, which has led\nto autonomy deployed on road. Though this approach provides ease of\ninterpretation, its generalizability to unseen environments is limited and\nhand-engineering of numerous parameters is required, especially in the\nprediction and planning systems. Recently, deep reinforcement learning has been\nshown to learn complex strategic games and perform challenging robotic tasks,\nwhich provides an appealing framework for learning to drive. In this work, we\npropose a deep reinforcement learning framework to learn optimal control policy\nusing waypoints and low-dimensional visual representations, also known as\naffordances. We demonstrate that our agents when trained from scratch learn the\ntasks of lane-following, driving around inter-sections as well as stopping in\nfront of other actors or traffic lights even in the dense traffic setting. We\nnote that our method achieves comparable or better performance than the\nbaseline methods on the original and NoCrash benchmarks on the CARLA simulator.\n",
        "published": "2021",
        "authors": [
            "Tanmay Agarwal",
            "Hitesh Arora",
            "Jeff Schneider"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.06798v1",
        "title": "MPC-MPNet: Model-Predictive Motion Planning Networks for Fast,\n  Near-Optimal Planning under Kinodynamic Constraints",
        "abstract": "  Kinodynamic Motion Planning (KMP) is to find a robot motion subject to\nconcurrent kinematics and dynamics constraints. To date, quite a few methods\nsolve KMP problems and those that exist struggle to find near-optimal solutions\nand exhibit high computational complexity as the planning space dimensionality\nincreases. To address these challenges, we present a scalable, imitation\nlearning-based, Model-Predictive Motion Planning Networks framework that\nquickly finds near-optimal path solutions with worst-case theoretical\nguarantees under kinodynamic constraints for practical underactuated systems.\nOur framework introduces two algorithms built on a neural generator,\ndiscriminator, and a parallelizable Model Predictive Controller (MPC). The\ngenerator outputs various informed states towards the given target, and the\ndiscriminator selects the best possible subset from them for the extension. The\nMPC locally connects the selected informed states while satisfying the given\nconstraints leading to feasible, near-optimal solutions. We evaluate our\nalgorithms on a range of cluttered, kinodynamically constrained, and\nunderactuated planning problems with results indicating significant\nimprovements in computation times, path qualities, and success rates over\nexisting methods.\n",
        "published": "2021",
        "authors": [
            "Linjun Li",
            "Yinglong Miao",
            "Ahmed H. Qureshi",
            "Michael C. Yip"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.09391v2",
        "title": "Learning Setup Policies: Reliable Transition Between Locomotion\n  Behaviours",
        "abstract": "  Dynamic platforms that operate over many unique terrain conditions typically\nrequire many behaviours. To transition safely, there must be an overlap of\nstates between adjacent controllers. We develop a novel method for training\nsetup policies that bridge the trajectories between pre-trained Deep\nReinforcement Learning (DRL) policies. We demonstrate our method with a\nsimulated biped traversing a difficult jump terrain, where a single policy\nfails to learn the task, and switching between pre-trained policies without\nsetup policies also fails. We perform an ablation of key components of our\nsystem, and show that our method outperforms others that learn transition\npolicies. We demonstrate our method with several difficult and diverse terrain\ntypes, and show that we can use setup policies as part of a modular control\nsuite to successfully traverse a sequence of complex terrains. We show that\nusing setup policies improves the success rate for traversing a single\ndifficult jump terrain (from 51.3% success rate with the best comparative\nmethod to 82.2%), and traversing a random sequence of difficult obstacles (from\n1.9% without setup policies to 71.2%).\n",
        "published": "2021",
        "authors": [
            "Brendan Tidd",
            "Nicolas Hudson",
            "Akansel Cosgun",
            "Jurgen Leitner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.11981v1",
        "title": "Embedding Symbolic Temporal Knowledge into Deep Sequential Models",
        "abstract": "  Sequences and time-series often arise in robot tasks, e.g., in activity\nrecognition and imitation learning. In recent years, deep neural networks\n(DNNs) have emerged as an effective data-driven methodology for processing\nsequences given sufficient training data and compute resources. However, when\ndata is limited, simpler models such as logic/rule-based methods work\nsurprisingly well, especially when relevant prior knowledge is applied in their\nconstruction. However, unlike DNNs, these \"structured\" models can be difficult\nto extend, and do not work well with raw unstructured data. In this work, we\nseek to learn flexible DNNs, yet leverage prior temporal knowledge when\navailable. Our approach is to embed symbolic knowledge expressed as linear\ntemporal logic (LTL) and use these embeddings to guide the training of deep\nmodels. Specifically, we construct semantic-based embeddings of automata\ngenerated from LTL formula via a Graph Neural Network. Experiments show that\nthese learnt embeddings can lead to improvements in downstream robot tasks such\nas sequential action recognition and imitation learning.\n",
        "published": "2021",
        "authors": [
            "Yaqi Xie",
            "Fan Zhou",
            "Harold Soh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.12719v1",
        "title": "Predicting Nanorobot Shapes via Generative Models",
        "abstract": "  The field of DNA nanotechnology has made it possible to assemble, with high\nyields, different structures that have actionable properties. For example,\nresearchers have created components that can be actuated. An exciting next step\nis to combine these components into multifunctional nanorobots that could,\npotentially, perform complex tasks like swimming to a target location in the\nhuman body, detect an adverse reaction and then release a drug load to stop it.\nHowever, as we start to assemble more complex nanorobots, the yield of the\ndesired nanorobot begins to decrease as the number of possible component\ncombinations increases. Therefore, the ultimate goal of this work is to develop\na predictive model to maximize yield. However, training predictive models\ntypically requires a large dataset. For the nanorobots we are interested in\nassembling, this will be difficult to collect. This is because high-fidelity\ndata, which allows us to characterize the shape and size of individual\nstructures, is very time-consuming to collect, whereas low-fidelity data is\nreadily available but only captures bulk statistics for different processes.\nTherefore, this work combines low- and high-fidelity data to train a generative\nmodel using a two-step process. We first use a relatively small, high-fidelity\ndataset to train a generative model. At run time, the model takes low-fidelity\ndata and uses it to approximate the high-fidelity content. We do this by\nbiasing the model towards samples with specific properties as measured by\nlow-fidelity data. In this work we bias our distribution towards a desired node\ndegree of a graphical model that we take as a surrogate representation of the\nnanorobots that this work will ultimately focus on. We have not yet accumulated\na high-fidelity dataset of nanorobots, so we leverage the MolGAN architecture\n[1] and the QM9 small molecule dataset [2-3] to demonstrate our approach.\n",
        "published": "2021",
        "authors": [
            "Emma Benjaminson",
            "Rebecca E. Taylor",
            "Matthew Travers"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.00771v2",
        "title": "Learning swimming escape patterns for larval fish under energy\n  constraints",
        "abstract": "  Swimming organisms can escape their predators by creating and harnessing\nunsteady flow fields through their body motions. Stochastic optimization and\nflow simulations have identified escape patterns that are consistent with those\nobserved in natural larval swimmers. However, these patterns have been limited\nby the specification of a particular cost function and depend on a prescribed\nfunctional form of the body motion. Here, we deploy reinforcement learning to\ndiscover swimmer escape patterns for larval fish under energy constraints. The\nidentified patterns include the C-start mechanism, in addition to more\nenergetically efficient escapes. We find that maximizing distance with limited\nenergy requires swimming via short bursts of accelerating motion interlinked\nwith phases of gliding. The present, data efficient, reinforcement learning\nalgorithm results in an array of patterns that reveal practical flow\noptimization principles for efficient swimming and the methodology can be\ntransferred to the control of aquatic robotic devices operating under energy\nconstraints.\n",
        "published": "2021",
        "authors": [
            "Ioannis Mandralis",
            "Pascal Weber",
            "Guido Novati",
            "Petros Koumoutsakos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.04607v1",
        "title": "Efficient Self-Supervised Data Collection for Offline Robot Learning",
        "abstract": "  A practical approach to robot reinforcement learning is to first collect a\nlarge batch of real or simulated robot interaction data, using some data\ncollection policy, and then learn from this data to perform various tasks,\nusing offline learning algorithms. Previous work focused on manually designing\nthe data collection policy, and on tasks where suitable policies can easily be\ndesigned, such as random picking policies for collecting data about object\ngrasping. For more complex tasks, however, it may be difficult to find a data\ncollection policy that explores the environment effectively, and produces data\nthat is diverse enough for the downstream task. In this work, we propose that\ndata collection policies should actively explore the environment to collect\ndiverse data. In particular, we develop a simple-yet-effective goal-conditioned\nreinforcement-learning method that actively focuses data collection on novel\nobservations, thereby collecting a diverse data-set. We evaluate our method on\nsimulated robot manipulation tasks with visual inputs and show that the\nimproved diversity of active data collection leads to significant improvements\nin the downstream learning tasks.\n",
        "published": "2021",
        "authors": [
            "Shadi Endrawis",
            "Gal Leibovich",
            "Guy Jacob",
            "Gal Novik",
            "Aviv Tamar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.04758v1",
        "title": "Zero-Shot Reinforcement Learning on Graphs for Autonomous Exploration\n  Under Uncertainty",
        "abstract": "  This paper studies the problem of autonomous exploration under localization\nuncertainty for a mobile robot with 3D range sensing. We present a framework\nfor self-learning a high-performance exploration policy in a single simulation\nenvironment, and transferring it to other environments, which may be physical\nor virtual. Recent work in transfer learning achieves encouraging performance\nby domain adaptation and domain randomization to expose an agent to scenarios\nthat fill the inherent gaps in sim2sim and sim2real approaches. However, it is\ninefficient to train an agent in environments with randomized conditions to\nlearn the important features of its current state. An agent can use domain\nknowledge provided by human experts to learn efficiently. We propose a novel\napproach that uses graph neural networks in conjunction with deep reinforcement\nlearning, enabling decision-making over graphs containing relevant exploration\ninformation provided by human experts to predict a robot's optimal sensing\naction in belief space. The policy, which is trained only in a single\nsimulation environment, offers a real-time, scalable, and transferable\ndecision-making strategy, resulting in zero-shot transfer to other simulation\nenvironments and even real-world environments.\n",
        "published": "2021",
        "authors": [
            "Fanfei Chen",
            "Paul Szenher",
            "Yewei Huang",
            "Jinkun Wang",
            "Tixiao Shan",
            "Shi Bai",
            "Brendan Englot"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.05983v1",
        "title": "An Open-Source Tool for Classification Models in Resource-Constrained\n  Hardware",
        "abstract": "  Applications that need to sense, measure, and gather real-time information\nfrom the environment frequently face three main restrictions: power\nconsumption, cost, and lack of infrastructure. Most of the challenges imposed\nby these limitations can be better addressed by embedding Machine Learning (ML)\nclassifiers in the hardware that senses the environment, creating smart sensors\nable to interpret the low-level data stream. However, for this approach to be\ncost-effective, we need highly efficient classifiers suitable to execute in\nunresourceful hardware, such as low-power microcontrollers. In this paper, we\npresent an open-source tool named EmbML - Embedded Machine Learning that\nimplements a pipeline to develop classifiers for resource-constrained hardware.\nWe describe its implementation details and provide a comprehensive analysis of\nits classifiers considering accuracy, classification time, and memory usage.\nMoreover, we compare the performance of its classifiers with classifiers\nproduced by related tools to demonstrate that our tool provides a diverse set\nof classification algorithms that are both compact and accurate. Finally, we\nvalidate EmbML classifiers in a practical application of a smart sensor and\ntrap for disease vector mosquitoes.\n",
        "published": "2021",
        "authors": [
            "Lucas Tsutsui da Silva",
            "Vinicius M. A. Souza",
            "Gustavo E. A. P. A. Batista"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.06517v1",
        "title": "Reinforcement Learning Based Safe Decision Making for Highway Autonomous\n  Driving",
        "abstract": "  In this paper, we develop a safe decision-making method for self-driving cars\nin a multi-lane, single-agent setting. The proposed approach utilizes deep\nreinforcement learning (RL) to achieve a high-level policy for safe tactical\ndecision-making. We address two major challenges that arise solely in\nautonomous navigation. First, the proposed algorithm ensures that collisions\nnever happen, and therefore accelerate the learning process. Second, the\nproposed algorithm takes into account the unobservable states in the\nenvironment. These states appear mainly due to the unpredictable behavior of\nother agents, such as cars, and pedestrians, and make the Markov Decision\nProcess (MDP) problematic when dealing with autonomous navigation. Simulations\nfrom a well-known self-driving car simulator demonstrate the applicability of\nthe proposed method\n",
        "published": "2021",
        "authors": [
            "Arash Mohammadhasani",
            "Hamed Mehrivash",
            "Alan Lynch",
            "Zhan Shu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.07091v1",
        "title": "Verification of Image-based Neural Network Controllers Using Generative\n  Models",
        "abstract": "  Neural networks are often used to process information from image-based\nsensors to produce control actions. While they are effective for this task, the\ncomplex nature of neural networks makes their output difficult to verify and\npredict, limiting their use in safety-critical systems. For this reason, recent\nwork has focused on combining techniques in formal methods and reachability\nanalysis to obtain guarantees on the closed-loop performance of neural network\ncontrollers. However, these techniques do not scale to the high-dimensional and\ncomplicated input space of image-based neural network controllers. In this\nwork, we propose a method to address these challenges by training a generative\nadversarial network (GAN) to map states to plausible input images. By\nconcatenating the generator network with the control network, we obtain a\nnetwork with a low-dimensional input space. This insight allows us to use\nexisting closed-loop verification tools to obtain formal guarantees on the\nperformance of image-based controllers. We apply our approach to provide safety\nguarantees for an image-based neural network controller for an autonomous\naircraft taxi problem. We guarantee that the controller will keep the aircraft\non the runway and guide the aircraft towards the center of the runway. The\nguarantees we provide are with respect to the set of input images modeled by\nour generator network, so we provide a recall metric to evaluate how well the\ngenerator captures the space of plausible images.\n",
        "published": "2021",
        "authors": [
            "Sydney M. Katz",
            "Anthony L. Corso",
            "Christopher A. Strong",
            "Mykel J. Kochenderfer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.07998v1",
        "title": "Controlling an Inverted Pendulum with Policy Gradient Methods-A Tutorial",
        "abstract": "  This paper provides the details of implementing two important policy gradient\nmethods to solve the inverted pendulum problem. These are namely the Deep\nDeterministic Policy Gradient (DDPG) and the Proximal Policy Optimization (PPO)\nalgorithm. The problem is solved by using an actor-critic model where an\nactor-network is used to learn the policy function and a critic network is to\nevaluate the actor-network by learning to estimate the Q function. Apart from\nbriefly explaining the mathematics behind these two algorithms, the details of\npython implementation are provided which helps in demystifying the underlying\ncomplexity of the algorithm. In the process, the readers will be introduced to\nOpenAI/Gym, Tensorflow 2.x and Keras utilities used for implementing the above\nconcepts.\n",
        "published": "2021",
        "authors": [
            "Swagat Kumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.10266v2",
        "title": "Ensemble Quantile Networks: Uncertainty-Aware Reinforcement Learning\n  with Applications in Autonomous Driving",
        "abstract": "  Reinforcement learning (RL) can be used to create a decision-making agent for\nautonomous driving. However, previous approaches provide only black-box\nsolutions, which do not offer information on how confident the agent is about\nits decisions. An estimate of both the aleatoric and epistemic uncertainty of\nthe agent's decisions is fundamental for real-world applications of autonomous\ndriving. Therefore, this paper introduces the Ensemble Quantile Networks (EQN)\nmethod, which combines distributional RL with an ensemble approach, to obtain a\ncomplete uncertainty estimate. The distribution over returns is estimated by\nlearning its quantile function implicitly, which gives the aleatoric\nuncertainty, whereas an ensemble of agents is trained on bootstrapped data to\nprovide a Bayesian estimation of the epistemic uncertainty. A criterion for\nclassifying which decisions that have an unacceptable uncertainty is also\nintroduced. The results show that the EQN method can balance risk and time\nefficiency in different occluded intersection scenarios, by considering the\nestimated aleatoric uncertainty. Furthermore, it is shown that the trained\nagent can use the epistemic uncertainty information to identify situations that\nthe agent has not been trained for and thereby avoid making unfounded,\npotentially dangerous, decisions outside of the training distribution.\n",
        "published": "2021",
        "authors": [
            "Carl-Johan Hoel",
            "Krister Wolff",
            "Leo Laine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.10919v3",
        "title": "Continual World: A Robotic Benchmark For Continual Reinforcement\n  Learning",
        "abstract": "  Continual learning (CL) -- the ability to continuously learn, building on\npreviously acquired knowledge -- is a natural requirement for long-lived\nautonomous reinforcement learning (RL) agents. While building such agents, one\nneeds to balance opposing desiderata, such as constraints on capacity and\ncompute, the ability to not catastrophically forget, and to exhibit positive\ntransfer on new tasks. Understanding the right trade-off is conceptually and\ncomputationally challenging, which we argue has led the community to overly\nfocus on catastrophic forgetting. In response to these issues, we advocate for\nthe need to prioritize forward transfer and propose Continual World, a\nbenchmark consisting of realistic and meaningfully diverse robotic tasks built\non top of Meta-World as a testbed. Following an in-depth empirical evaluation\nof existing CL methods, we pinpoint their limitations and highlight unique\nalgorithmic challenges in the RL setting. Our benchmark aims to provide a\nmeaningful and computationally inexpensive challenge for the community and thus\nhelp better understand the performance of existing and future solutions.\nInformation about the benchmark, including the open-source code, is available\nat https://sites.google.com/view/continualworld.\n",
        "published": "2021",
        "authors": [
            "Maciej Wo\u0142czyk",
            "Micha\u0142 Zaj\u0105c",
            "Razvan Pascanu",
            "\u0141ukasz Kuci\u0144ski",
            "Piotr Mi\u0142o\u015b"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.11617v1",
        "title": "A Comparison of Reward Functions in Q-Learning Applied to a Cart\n  Position Problem",
        "abstract": "  Growing advancements in reinforcement learning has led to advancements in\ncontrol theory. Reinforcement learning has effectively solved the inverted\npendulum problem and more recently the double inverted pendulum problem. In\nreinforcement learning, our agents learn by interacting with the control system\nwith the goal of maximizing rewards. In this paper, we explore three such\nreward functions in the cart position problem. This paper concludes that a\ndiscontinuous reward function that gives non-zero rewards to agents only if\nthey are within a given distance from the desired position gives the best\nresults.\n",
        "published": "2021",
        "authors": [
            "Amartya Mukherjee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.12277v1",
        "title": "Learning Bipedal Robot Locomotion from Human Movement",
        "abstract": "  Teaching an anthropomorphic robot from human example offers the opportunity\nto impart humanlike qualities on its movement. In this work we present a\nreinforcement learning based method for teaching a real world bipedal robot to\nperform movements directly from human motion capture data. Our method\nseamlessly transitions from training in a simulation environment to executing\non a physical robot without requiring any real world training iterations or\noffline steps. To overcome the disparity in joint configurations between the\nrobot and the motion capture actor, our method incorporates motion re-targeting\ninto the training process. Domain randomization techniques are used to\ncompensate for the differences between the simulated and physical systems. We\ndemonstrate our method on an internally developed humanoid robot with movements\nranging from a dynamic walk cycle to complex balancing and waving. Our\ncontroller preserves the style imparted by the motion capture data and exhibits\ngraceful failure modes resulting in safe operation for the robot. This work was\nperformed for research purposes only.\n",
        "published": "2021",
        "authors": [
            "Michael Taylor",
            "Sergey Bashkirov",
            "Javier Fernandez Rico",
            "Ike Toriyama",
            "Naoyuki Miyada",
            "Hideki Yanagisawa",
            "Kensaku Ishizuka"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.14074v3",
        "title": "Learning Neuro-Symbolic Relational Transition Models for Bilevel\n  Planning",
        "abstract": "  In robotic domains, learning and planning are complicated by continuous state\nspaces, continuous action spaces, and long task horizons. In this work, we\naddress these challenges with Neuro-Symbolic Relational Transition Models\n(NSRTs), a novel class of models that are data-efficient to learn, compatible\nwith powerful robotic planning methods, and generalizable over objects. NSRTs\nhave both symbolic and neural components, enabling a bilevel planning scheme\nwhere symbolic AI planning in an outer loop guides continuous planning with\nneural models in an inner loop. Experiments in four robotic planning domains\nshow that NSRTs can be learned after only tens or hundreds of training\nepisodes, and then used for fast planning in new tasks that require up to 60\nactions and involve many more objects than were seen during training. Video:\nhttps://tinyurl.com/chitnis-nsrts\n",
        "published": "2021",
        "authors": [
            "Rohan Chitnis",
            "Tom Silver",
            "Joshua B. Tenenbaum",
            "Tomas Lozano-Perez",
            "Leslie Pack Kaelbling"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.14127v1",
        "title": "Risk-Aware Transfer in Reinforcement Learning using Successor Features",
        "abstract": "  Sample efficiency and risk-awareness are central to the development of\npractical reinforcement learning (RL) for complex decision-making. The former\ncan be addressed by transfer learning and the latter by optimizing some utility\nfunction of the return. However, the problem of transferring skills in a\nrisk-aware manner is not well-understood. In this paper, we address the problem\nof risk-aware policy transfer between tasks in a common domain that differ only\nin their reward functions, in which risk is measured by the variance of reward\nstreams. Our approach begins by extending the idea of generalized policy\nimprovement to maximize entropic utilities, thus extending policy improvement\nvia dynamic programming to sets of policies and levels of risk-aversion. Next,\nwe extend the idea of successor features (SF), a value function representation\nthat decouples the environment dynamics from the rewards, to capture the\nvariance of returns. Our resulting risk-aware successor features (RaSF)\nintegrate seamlessly within the RL framework, inherit the superior task\ngeneralization ability of SFs, and incorporate risk-awareness into the\ndecision-making. Experiments on a discrete navigation domain and control of a\nsimulated robotic arm demonstrate the ability of RaSFs to outperform\nalternative methods including SFs, when taking the risk of the learned policies\ninto account.\n",
        "published": "2021",
        "authors": [
            "Michael Gimelfarb",
            "Andr\u00e9 Barreto",
            "Scott Sanner",
            "Chi-Guhn Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.00306v2",
        "title": "MHER: Model-based Hindsight Experience Replay",
        "abstract": "  Solving multi-goal reinforcement learning (RL) problems with sparse rewards\nis generally challenging. Existing approaches have utilized goal relabeling on\ncollected experiences to alleviate issues raised from sparse rewards. However,\nthese methods are still limited in efficiency and cannot make full use of\nexperiences. In this paper, we propose Model-based Hindsight Experience Replay\n(MHER), which exploits experiences more efficiently by leveraging environmental\ndynamics to generate virtual achieved goals. Replacing original goals with\nvirtual goals generated from interaction with a trained dynamics model leads to\na novel relabeling method, model-based relabeling (MBR). Based on MBR, MHER\nperforms both reinforcement learning and supervised learning for efficient\npolicy improvement. Theoretically, we also prove the supervised part in MHER,\ni.e., goal-conditioned supervised learning with MBR data, optimizes a lower\nbound on the multi-goal RL objective. Experimental results in several\npoint-based tasks and simulated robotics environments show that MHER achieves\nsignificantly higher sample efficiency than previous model-free and model-based\nmulti-goal methods.\n",
        "published": "2021",
        "authors": [
            "Rui Yang",
            "Meng Fang",
            "Lei Han",
            "Yali Du",
            "Feng Luo",
            "Xiu Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.00339v1",
        "title": "Policy Transfer across Visual and Dynamics Domain Gaps via Iterative\n  Grounding",
        "abstract": "  The ability to transfer a policy from one environment to another is a\npromising avenue for efficient robot learning in realistic settings where task\nsupervision is not available. This can allow us to take advantage of\nenvironments well suited for training, such as simulators or laboratories, to\nlearn a policy for a real robot in a home or office. To succeed, such policy\ntransfer must overcome both the visual domain gap (e.g. different illumination\nor background) and the dynamics domain gap (e.g. different robot calibration or\nmodelling error) between source and target environments. However, prior policy\ntransfer approaches either cannot handle a large domain gap or can only address\none type of domain gap at a time. In this paper, we propose a novel policy\ntransfer method with iterative \"environment grounding\", IDAPT, that alternates\nbetween (1) directly minimizing both visual and dynamics domain gaps by\ngrounding the source environment in the target environment domains, and (2)\ntraining a policy on the grounded source environment. This iterative training\nprogressively aligns the domains between the two environments and adapts the\npolicy to the target environment. Once trained, the policy can be directly\nexecuted on the target environment. The empirical results on locomotion and\nrobotic manipulation tasks demonstrate that our approach can effectively\ntransfer a policy across visual and dynamics domain gaps with minimal\nsupervision and interaction with the target environment. Videos and code are\navailable at https://clvrai.com/idapt .\n",
        "published": "2021",
        "authors": [
            "Grace Zhang",
            "Linghan Zhong",
            "Youngwoon Lee",
            "Joseph J. Lim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.00359v1",
        "title": "Model Mediated Teleoperation with a Hand-Arm Exoskeleton in Long Time\n  Delays Using Reinforcement Learning",
        "abstract": "  Telerobotic systems must adapt to new environmental conditions and deal with\nhigh uncertainty caused by long-time delays. As one of the best alternatives to\nhuman-level intelligence, Reinforcement Learning (RL) may offer a solution to\ncope with these issues. This paper proposes to integrate RL with the Model\nMediated Teleoperation (MMT) concept. The teleoperator interacts with a\nsimulated virtual environment, which provides instant feedback. Whereas\nfeedback from the real environment is delayed, feedback from the model is\ninstantaneous, leading to high transparency. The MMT is realized in combination\nwith an intelligent system with two layers. The first layer utilizes Dynamic\nMovement Primitives (DMP) which accounts for certain changes in the avatar\nenvironment. And, the second layer addresses the problems caused by uncertainty\nin the model using RL methods. Augmented reality was also provided to fuse the\navatar device and virtual environment models for the teleoperator. Implemented\non DLR's Exodex Adam hand-arm haptic exoskeleton, the results show RL methods\nare able to find different solutions when changes are applied to the object\nposition after the demonstration. The results also show DMPs to be effective at\nadapting to new conditions where there is no uncertainty involved.\n",
        "published": "2021",
        "authors": [
            "Hadi Beik-Mohammadi",
            "Matthias Kerzel",
            "Benedikt Pleintinger",
            "Thomas Hulin",
            "Philipp Reisich",
            "Annika Schmidt",
            "Aaron Pereira",
            "Stefan Wermter",
            "Neal Y. Lii"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.00722v1",
        "title": "Neural Task Success Classifiers for Robotic Manipulation from Few Real\n  Demonstrations",
        "abstract": "  Robots learning a new manipulation task from a small amount of demonstrations\nare increasingly demanded in different workspaces. A classifier model assessing\nthe quality of actions can predict the successful completion of a task, which\ncan be used by intelligent agents for action-selection. This paper presents a\nnovel classifier that learns to classify task completion only from a few\ndemonstrations. We carry out a comprehensive comparison of different neural\nclassifiers, e.g. fully connected-based, fully convolutional-based,\nsequence2sequence-based, and domain adaptation-based classification. We also\npresent a new dataset including five robot manipulation tasks, which is\npublicly available. We compared the performances of our novel classifier and\nthe existing models using our dataset and the MIME dataset. The results suggest\ndomain adaptation and timing-based features improve success prediction. Our\nnovel model, i.e. fully convolutional neural network with domain adaptation and\ntiming features, achieves an average classification accuracy of 97.3\\% and\n95.5\\% across tasks in both datasets whereas state-of-the-art classifiers\nwithout domain adaptation and timing-features only achieve 82.4\\% and 90.3\\%,\nrespectively.\n",
        "published": "2021",
        "authors": [
            "Abdalkarim Mohtasib",
            "Amir Ghalamzan E.",
            "Nicola Bellotto",
            "Heriberto Cuay\u00e1huitl"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.02339v1",
        "title": "Multi-Modal Mutual Information (MuMMI) Training for Robust\n  Self-Supervised Deep Reinforcement Learning",
        "abstract": "  This work focuses on learning useful and robust deep world models using\nmultiple, possibly unreliable, sensors. We find that current methods do not\nsufficiently encourage a shared representation between modalities; this can\ncause poor performance on downstream tasks and over-reliance on specific\nsensors. As a solution, we contribute a new multi-modal deep latent state-space\nmodel, trained using a mutual information lower-bound. The key innovation is a\nspecially-designed density ratio estimator that encourages consistency between\nthe latent codes of each modality. We tasked our method to learn policies (in a\nself-supervised manner) on multi-modal Natural MuJoCo benchmarks and a\nchallenging Table Wiping task. Experiments show our method significantly\noutperforms state-of-the-art deep reinforcement learning methods, particularly\nin the presence of missing observations.\n",
        "published": "2021",
        "authors": [
            "Kaiqi Chen",
            "Yong Lee",
            "Harold Soh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.02912v1",
        "title": "Supervised Bayesian Specification Inference from Demonstrations",
        "abstract": "  When observing task demonstrations, human apprentices are able to identify\nwhether a given task is executed correctly long before they gain expertise in\nactually performing that task. Prior research into learning from demonstrations\n(LfD) has failed to capture this notion of the acceptability of a task's\nexecution; meanwhile, temporal logics provide a flexible language for\nexpressing task specifications. Inspired by this, we present Bayesian\nspecification inference, a probabilistic model for inferring task specification\nas a temporal logic formula. We incorporate methods from probabilistic\nprogramming to define our priors, along with a domain-independent likelihood\nfunction to enable sampling-based inference. We demonstrate the efficacy of our\nmodel for inferring specifications, with over 90% similarity observed between\nthe inferred specification and the ground truth, both within a synthetic domain\nand during a real-world table setting task.\n",
        "published": "2021",
        "authors": [
            "Ankit Shah",
            "Pritish Kamath",
            "Shen Li",
            "Patrick Craven",
            "Kevin Landers",
            "Kevin Oden",
            "Julie Shah"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.03974v4",
        "title": "Offline Meta-Reinforcement Learning with Online Self-Supervision",
        "abstract": "  Meta-reinforcement learning (RL) methods can meta-train policies that adapt\nto new tasks with orders of magnitude less data than standard RL, but\nmeta-training itself is costly and time-consuming. If we can meta-train on\noffline data, then we can reuse the same static dataset, labeled once with\nrewards for different tasks, to meta-train policies that adapt to a variety of\nnew tasks at meta-test time. Although this capability would make meta-RL a\npractical tool for real-world use, offline meta-RL presents additional\nchallenges beyond online meta-RL or standard offline RL settings. Meta-RL\nlearns an exploration strategy that collects data for adapting, and also\nmeta-trains a policy that quickly adapts to data from a new task. Since this\npolicy was meta-trained on a fixed, offline dataset, it might behave\nunpredictably when adapting to data collected by the learned exploration\nstrategy, which differs systematically from the offline data and thus induces\ndistributional shift. We propose a hybrid offline meta-RL algorithm, which uses\noffline data with rewards to meta-train an adaptive policy, and then collects\nadditional unsupervised online data, without any reward labels to bridge this\ndistribution shift. By not requiring reward labels for online collection, this\ndata can be much cheaper to collect. We compare our method to prior work on\noffline meta-RL on simulated robot locomotion and manipulation tasks and find\nthat using additional unsupervised online data collection leads to a dramatic\nimprovement in the adaptive capabilities of the meta-trained policies, matching\nthe performance of fully online meta-RL on a range of challenging domains that\nrequire generalization to new tasks.\n",
        "published": "2021",
        "authors": [
            "Vitchyr H. Pong",
            "Ashvin Nair",
            "Laura Smith",
            "Catherine Huang",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.04457v2",
        "title": "Aligning an optical interferometer with beam divergence control and\n  continuous action space",
        "abstract": "  Reinforcement learning is finding its way to real-world problem application,\ntransferring from simulated environments to physical setups. In this work, we\nimplement vision-based alignment of an optical Mach-Zehnder interferometer with\na confocal telescope in one arm, which controls the diameter and divergence of\nthe corresponding beam. We use a continuous action space; exponential scaling\nenables us to handle actions within a range of over two orders of magnitude.\nOur agent trains only in a simulated environment with domain randomizations. In\nan experimental evaluation, the agent significantly outperforms an existing\nsolution and a human expert.\n",
        "published": "2021",
        "authors": [
            "Stepan Makarenko",
            "Dmitry Sorokin",
            "Alexander Ulanov",
            "A. I. Lvovsky"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.04533v1",
        "title": "Behavior Self-Organization Supports Task Inference for Continual Robot\n  Learning",
        "abstract": "  Recent advances in robot learning have enabled robots to become increasingly\nbetter at mastering a predefined set of tasks. On the other hand, as humans, we\nhave the ability to learn a growing set of tasks over our lifetime. Continual\nrobot learning is an emerging research direction with the goal of endowing\nrobots with this ability. In order to learn new tasks over time, the robot\nfirst needs to infer the task at hand. Task inference, however, has received\nlittle attention in the multi-task learning literature. In this paper, we\npropose a novel approach to continual learning of robotic control tasks. Our\napproach performs unsupervised learning of behavior embeddings by incrementally\nself-organizing demonstrated behaviors. Task inference is made by finding the\nnearest behavior embedding to a demonstrated behavior, which is used together\nwith the environment state as input to a multi-task policy trained with\nreinforcement learning to optimize performance over tasks. Unlike previous\napproaches, our approach makes no assumptions about task distribution and\nrequires no task exploration to infer tasks. We evaluate our approach in\nexperiments with concurrently and sequentially presented tasks and show that it\noutperforms other multi-task learning approaches in terms of generalization\nperformance and convergence speed, particularly in the continual learning\nsetting.\n",
        "published": "2021",
        "authors": [
            "Muhammad Burhan Hafez",
            "Stefan Wermter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.04775v2",
        "title": "LS3: Latent Space Safe Sets for Long-Horizon Visuomotor Control of\n  Sparse Reward Iterative Tasks",
        "abstract": "  Reinforcement learning (RL) has shown impressive success in exploring\nhigh-dimensional environments to learn complex tasks, but can often exhibit\nunsafe behaviors and require extensive environment interaction when exploration\nis unconstrained. A promising strategy for learning in dynamically uncertain\nenvironments is requiring that the agent can robustly return to learned safe\nsets, where task success (and therefore safety) can be guaranteed. While this\napproach has been successful in low-dimensions, enforcing this constraint in\nenvironments with visual observations is exceedingly challenging. We present a\nnovel continuous representation for safe sets by framing it as a binary\nclassification problem in a learned latent space, which flexibly scales to\nimage observations. We then present a new algorithm, Latent Space Safe Sets\n(LS3), which uses this representation for long-horizon tasks with sparse\nrewards. We evaluate LS3 on 4 domains, including a challenging sequential\npushing task in simulation and a physical cable routing task. We find that LS3\ncan use prior task successes to restrict exploration and learn more efficiently\nthan prior algorithms while satisfying constraints. See\nhttps://tinyurl.com/latent-ss for code and supplementary material.\n",
        "published": "2021",
        "authors": [
            "Albert Wilcox",
            "Ashwin Balakrishna",
            "Brijen Thananjeyan",
            "Joseph E. Gonzalez",
            "Ken Goldberg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.06405v1",
        "title": "Shortest-Path Constrained Reinforcement Learning for Sparse Reward Tasks",
        "abstract": "  We propose the k-Shortest-Path (k-SP) constraint: a novel constraint on the\nagent's trajectory that improves the sample efficiency in sparse-reward MDPs.\nWe show that any optimal policy necessarily satisfies the k-SP constraint.\nNotably, the k-SP constraint prevents the policy from exploring state-action\npairs along the non-k-SP trajectories (e.g., going back and forth). However, in\npractice, excluding state-action pairs may hinder the convergence of RL\nalgorithms. To overcome this, we propose a novel cost function that penalizes\nthe policy violating SP constraint, instead of completely excluding it. Our\nnumerical experiment in a tabular RL setting demonstrates that the SP\nconstraint can significantly reduce the trajectory space of policy. As a\nresult, our constraint enables more sample efficient learning by suppressing\nredundant exploration and exploitation. Our experiments on MiniGrid, DeepMind\nLab, Atari, and Fetch show that the proposed method significantly improves\nproximal policy optimization (PPO) and outperforms existing novelty-seeking\nexploration methods including count-based exploration even in continuous\ncontrol tasks, indicating that it improves the sample efficiency by preventing\nthe agent from taking redundant actions.\n",
        "published": "2021",
        "authors": [
            "Sungryull Sohn",
            "Sungtae Lee",
            "Jongwook Choi",
            "Harm van Seijen",
            "Mehdi Fatemi",
            "Honglak Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.06629v2",
        "title": "Model-free Reinforcement Learning for Robust Locomotion using\n  Demonstrations from Trajectory Optimization",
        "abstract": "  We present a general, two-stage reinforcement learning approach to create\nrobust policies that can be deployed on real robots without any additional\ntraining using a single demonstration generated by trajectory optimization. The\ndemonstration is used in the first stage as a starting point to facilitate\ninitial exploration. In the second stage, the relevant task reward is optimized\ndirectly and a policy robust to environment uncertainties is computed. We\ndemonstrate and examine in detail the performance and robustness of our\napproach on highly dynamic hopping and bounding tasks on a quadruped robot.\n",
        "published": "2021",
        "authors": [
            "Miroslav Bogdanovic",
            "Majid Khadiv",
            "Ludovic Righetti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.06661v1",
        "title": "Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks",
        "abstract": "  In high-dimensional state spaces, the usefulness of Reinforcement Learning\n(RL) is limited by the problem of exploration. This issue has been addressed\nusing potential-based reward shaping (PB-RS) previously. In the present work,\nwe introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the\nstrict optimality guarantees of PB-RS to a guarantee of preserved long-term\nbehavior. Being less restrictive, FV-RS allows for reward shaping functions\nthat are even better suited for improving the sample efficiency of RL\nalgorithms. In particular, we consider settings in which the agent has access\nto an approximate plan. Here, we use examples of simulated robotic manipulation\ntasks to demonstrate that plan-based FV-RS can indeed significantly improve the\nsample efficiency of RL over plan-based PB-RS.\n",
        "published": "2021",
        "authors": [
            "Ingmar Schubert",
            "Ozgur S. Oguz",
            "Marc Toussaint"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.06875v1",
        "title": "DULA: A Differentiable Ergonomics Model for Postural Optimization in\n  Physical HRI",
        "abstract": "  Ergonomics and human comfort are essential concerns in physical human-robot\ninteraction applications. Defining an accurate and easy-to-use ergonomic\nassessment model stands as an important step in providing feedback for postural\ncorrection to improve operator health and comfort. In order to enable efficient\ncomputation, previously proposed automated ergonomic assessment and correction\ntools make approximations or simplifications to gold-standard assessment tools\nused by ergonomists in practice. In order to retain assessment quality, while\nimproving computational considerations, we introduce DULA, a differentiable and\ncontinuous ergonomics model learned to replicate the popular and scientifically\nvalidated RULA assessment. We show that DULA provides assessment comparable to\nRULA while providing computational benefits. We highlight DULA's strength in a\ndemonstration of gradient-based postural optimization for a simulated\nteleoperation task.\n",
        "published": "2021",
        "authors": [
            "Amir Yazdani",
            "Roya Sabbagh Novin",
            "Andrew Merryweather",
            "Tucker Hermans"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.07696v1",
        "title": "Constrained Feedforward Neural Network Training via Reachability\n  Analysis",
        "abstract": "  Neural networks have recently become popular for a wide variety of uses, but\nhave seen limited application in safety-critical domains such as robotics near\nand around humans. This is because it remains an open challenge to train a\nneural network to obey safety constraints. Most existing safety-related methods\nonly seek to verify that already-trained networks obey constraints, requiring\nalternating training and verification. Instead, this work proposes a\nconstrained method to simultaneously train and verify a feedforward neural\nnetwork with rectified linear unit (ReLU) nonlinearities. Constraints are\nenforced by computing the network's output-space reachable set and ensuring\nthat it does not intersect with unsafe sets; training is achieved by\nformulating a novel collision-check loss function between the reachable set and\nunsafe portions of the output space. The reachable and unsafe sets are\nrepresented by constrained zonotopes, a convex polytope representation that\nenables differentiable collision checking. The proposed method is demonstrated\nsuccessfully on a network with one nonlinearity layer and approximately 50\nparameters.\n",
        "published": "2021",
        "authors": [
            "Long Kiu Chung",
            "Adam Dai",
            "Derek Knowles",
            "Shreyas Kousik",
            "Grace X. Gao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.07844v1",
        "title": "Versatile modular neural locomotion control with fast learning",
        "abstract": "  Legged robots have significant potential to operate in highly unstructured\nenvironments. The design of locomotion control is, however, still challenging.\nCurrently, controllers must be either manually designed for specific robots and\ntasks, or automatically designed via machine learning methods that require long\ntraining times and yield large opaque controllers. Drawing inspiration from\nanimal locomotion, we propose a simple yet versatile modular neural control\nstructure with fast learning. The key advantages of our approach are that\nbehavior-specific control modules can be added incrementally to obtain\nincreasingly complex emergent locomotion behaviors, and that neural connections\ninterfacing with existing modules can be quickly and automatically learned. In\na series of experiments, we show how eight modules can be quickly learned and\nadded to a base control module to obtain emergent adaptive behaviors allowing a\nhexapod robot to navigate in complex environments. We also show that modules\ncan be added and removed during operation without affecting the functionality\nof the remaining controller. Finally, the control approach was successfully\ndemonstrated on a physical hexapod robot. Taken together, our study reveals a\nsignificant step towards fast automatic design of versatile neural locomotion\ncontrol for complex robotic systems.\n",
        "published": "2021",
        "authors": [
            "Mathias Thor",
            "Poramate Manoonpong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.08249v2",
        "title": "Gait-learning with morphologically evolving robots generated by L-system",
        "abstract": "  When controllers (brains) and morphologies (bodies) of robots simultaneously\nevolve, this can lead to a problem, namely the brain & body mismatch problem.\nIn this research, we propose a solution of lifetime learning. We set up a\nsystem where modular robots can create offspring that inherit the bodies of\nparents by recombination and mutation. With regards to the brains of the\noffspring, we use two methods to create them. The first one entails solely\nevolution which means the brain of a robot child is inherited from its parents.\nThe second approach is evolution plus learning which means the brain of a child\nis inherited as well, but additionally is developed by a learning algorithm -\nRevDEknn. We compare these two methods by running experiments in a simulator\ncalled Revolve and use efficiency, efficacy, and the morphology intelligence of\nthe robots for the comparison. The experiments show that the evolution plus\nlearning method does not only lead to a higher fitness level, but also to more\nmorphologically evolving robots. This constitutes a quantitative demonstration\nthat changes in the brain can induce changes in the body, leading to the\nconcept of morphological intelligence, which is quantified by the learning\ndelta, meaning the ability of a morphology to facilitate learning.\n",
        "published": "2021",
        "authors": [
            "Jie Luo",
            "Daan Zeeuwe",
            "Agoston E. Eiben"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.08829v2",
        "title": "Visual Adversarial Imitation Learning using Variational Models",
        "abstract": "  Reward function specification, which requires considerable human effort and\niteration, remains a major impediment for learning behaviors through deep\nreinforcement learning. In contrast, providing visual demonstrations of desired\nbehaviors often presents an easier and more natural way to teach agents. We\nconsider a setting where an agent is provided a fixed dataset of visual\ndemonstrations illustrating how to perform a task, and must learn to solve the\ntask using the provided demonstrations and unsupervised environment\ninteractions. This setting presents a number of challenges including\nrepresentation learning for visual observations, sample complexity due to high\ndimensional spaces, and learning instability due to the lack of a fixed reward\nor learning signal. Towards addressing these challenges, we develop a\nvariational model-based adversarial imitation learning (V-MAIL) algorithm. The\nmodel-based approach provides a strong signal for representation learning,\nenables sample efficiency, and improves the stability of adversarial training\nby enabling on-policy learning. Through experiments involving several\nvision-based locomotion and manipulation tasks, we find that V-MAIL learns\nsuccessful visuomotor policies in a sample-efficient manner, has better\nstability compared to prior work, and also achieves higher asymptotic\nperformance. We further find that by transferring the learned models, V-MAIL\ncan learn new tasks from visual demonstrations without any additional\nenvironment interactions. All results including videos can be found online at\n\\url{https://sites.google.com/view/variational-mail}.\n",
        "published": "2021",
        "authors": [
            "Rafael Rafailov",
            "Tianhe Yu",
            "Aravind Rajeswaran",
            "Chelsea Finn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.08865v1",
        "title": "Ab Initio Particle-based Object Manipulation",
        "abstract": "  This paper presents Particle-based Object Manipulation (Prompt), a new\napproach to robot manipulation of novel objects ab initio, without prior object\nmodels or pre-training on a large object data set. The key element of Prompt is\na particle-based object representation, in which each particle represents a\npoint in the object, the local geometric, physical, and other features of the\npoint, and also its relation with other particles. Like the model-based\nanalytic approaches to manipulation, the particle representation enables the\nrobot to reason about the object's geometry and dynamics in order to choose\nsuitable manipulation actions. Like the data-driven approaches, the particle\nrepresentation is learned online in real-time from visual sensor input,\nspecifically, multi-view RGB images. The particle representation thus connects\nvisual perception with robot control. Prompt combines the benefits of both\nmodel-based reasoning and data-driven learning. We show empirically that Prompt\nsuccessfully handles a variety of everyday objects, some of which are\ntransparent. It handles various manipulation tasks, including grasping,\npushing, etc,. Our experiments also show that Prompt outperforms a\nstate-of-the-art data-driven grasping method on the daily objects, even though\nit does not use any offline training data.\n",
        "published": "2021",
        "authors": [
            "Siwei Chen",
            "Xiao Ma",
            "Yunfan Lu",
            "David Hsu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.08942v1",
        "title": "Untangling Dense Non-Planar Knots by Learning Manipulation Features and\n  Recovery Policies",
        "abstract": "  Robot manipulation for untangling 1D deformable structures such as ropes,\ncables, and wires is challenging due to their infinite dimensional\nconfiguration space, complex dynamics, and tendency to self-occlude. Analytical\ncontrollers often fail in the presence of dense configurations, due to the\ndifficulty of grasping between adjacent cable segments. We present two\nalgorithms that enhance robust cable untangling, LOKI and SPiDERMan, which\noperate alongside HULK, a high-level planner from prior work. LOKI uses a\nlearned model of manipulation features to refine a coarse grasp keypoint\nprediction to a precise, optimized location and orientation, while SPiDERMan\nuses a learned model to sense task progress and apply recovery actions. We\nevaluate these algorithms in physical cable untangling experiments with 336\nknots and over 1500 actions on real cables using the da Vinci surgical robot.\nWe find that the combination of HULK, LOKI, and SPiDERMan is able to untangle\ndense overhand, figure-eight, double-overhand, square, bowline, granny,\nstevedore, and triple-overhand knots. The composition of these methods\nsuccessfully untangles a cable from a dense initial configuration in 68.3% of\n60 physical experiments and achieves 50% higher success rates than baselines\nfrom prior work. Supplementary material, code, and videos can be found at\nhttps://tinyurl.com/rssuntangling.\n",
        "published": "2021",
        "authors": [
            "Priya Sundaresan",
            "Jennifer Grannen",
            "Brijen Thananjeyan",
            "Ashwin Balakrishna",
            "Jeffrey Ichnowski",
            "Ellen Novoseller",
            "Minho Hwang",
            "Michael Laskey",
            "Joseph E. Gonzalez",
            "Ken Goldberg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.08981v2",
        "title": "Hierarchical Few-Shot Imitation with Skill Transition Models",
        "abstract": "  A desirable property of autonomous agents is the ability to both solve\nlong-horizon problems and generalize to unseen tasks. Recent advances in\ndata-driven skill learning have shown that extracting behavioral priors from\noffline data can enable agents to solve challenging long-horizon tasks with\nreinforcement learning. However, generalization to tasks unseen during\nbehavioral prior training remains an outstanding challenge. To this end, we\npresent Few-shot Imitation with Skill Transition Models (FIST), an algorithm\nthat extracts skills from offline data and utilizes them to generalize to\nunseen tasks given a few downstream demonstrations. FIST learns an inverse\nskill dynamics model, a distance function, and utilizes a semi-parametric\napproach for imitation. We show that FIST is capable of generalizing to new\ntasks and substantially outperforms prior baselines in navigation experiments\nrequiring traversing unseen parts of a large maze and 7-DoF robotic arm\nexperiments requiring manipulating previously unseen objects in a kitchen.\n",
        "published": "2021",
        "authors": [
            "Kourosh Hakhamaneshi",
            "Ruihan Zhao",
            "Albert Zhan",
            "Pieter Abbeel",
            "Michael Laskin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.09996v2",
        "title": "MarsExplorer: Exploration of Unknown Terrains via Deep Reinforcement\n  Learning and Procedurally Generated Environments",
        "abstract": "  This paper is an initial endeavor to bridge the gap between powerful Deep\nReinforcement Learning methodologies and the problem of exploration/coverage of\nunknown terrains. Within this scope, MarsExplorer, an openai-gym compatible\nenvironment tailored to exploration/coverage of unknown areas, is presented.\nMarsExplorer translates the original robotics problem into a Reinforcement\nLearning setup that various off-the-shelf algorithms can tackle. Any learned\npolicy can be straightforwardly applied to a robotic platform without an\nelaborate simulation model of the robot's dynamics to apply a different\nlearning/adaptation phase. One of its core features is the controllable\nmulti-dimensional procedural generation of terrains, which is the key for\nproducing policies with strong generalization capabilities. Four different\nstate-of-the-art RL algorithms (A3C, PPO, Rainbow, and SAC) are trained on the\nMarsExplorer environment, and a proper evaluation of their results compared to\nthe average human-level performance is reported. In the follow-up experimental\nanalysis, the effect of the multi-dimensional difficulty setting on the\nlearning capabilities of the best-performing algorithm (PPO) is analyzed. A\nmilestone result is the generation of an exploration policy that follows the\nHilbert curve without providing this information to the environment or\nrewarding directly or indirectly Hilbert-curve-like trajectories. The\nexperimental analysis is concluded by evaluating PPO learned policy algorithm\nside-by-side with frontier-based exploration strategies. A study on the\nperformance curves revealed that PPO-based policy was capable of performing\nadaptive-to-the-unknown-terrain sweeping without leaving expensive-to-revisit\nareas uncovered, underlying the capability of RL-based methodologies to tackle\nexploration tasks efficiently. The source code can be found at:\nhttps://github.com/dimikout3/MarsExplorer.\n",
        "published": "2021",
        "authors": [
            "Dimitrios I. Koutras",
            "Athanasios Ch. Kapoutsis",
            "Angelos A. Amanatiadis",
            "Elias B. Kosmatopoulos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.10253v1",
        "title": "Demonstration-Guided Reinforcement Learning with Learned Skills",
        "abstract": "  Demonstration-guided reinforcement learning (RL) is a promising approach for\nlearning complex behaviors by leveraging both reward feedback and a set of\ntarget task demonstrations. Prior approaches for demonstration-guided RL treat\nevery new task as an independent learning problem and attempt to follow the\nprovided demonstrations step-by-step, akin to a human trying to imitate a\ncompletely unseen behavior by following the demonstrator's exact muscle\nmovements. Naturally, such learning will be slow, but often new behaviors are\nnot completely unseen: they share subtasks with behaviors we have previously\nlearned. In this work, we aim to exploit this shared subtask structure to\nincrease the efficiency of demonstration-guided RL. We first learn a set of\nreusable skills from large offline datasets of prior experience collected\nacross many tasks. We then propose Skill-based Learning with Demonstrations\n(SkiLD), an algorithm for demonstration-guided RL that efficiently leverages\nthe provided demonstrations by following the demonstrated skills instead of the\nprimitive actions, resulting in substantial performance improvements over prior\ndemonstration-guided RL approaches. We validate the effectiveness of our\napproach on long-horizon maze navigation and complex robot manipulation tasks.\n",
        "published": "2021",
        "authors": [
            "Karl Pertsch",
            "Youngwoon Lee",
            "Yue Wu",
            "Joseph J. Lim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.11722v2",
        "title": "Learning Risk-aware Costmaps for Traversability in Challenging\n  Environments",
        "abstract": "  One of the main challenges in autonomous robotic exploration and navigation\nin unknown and unstructured environments is determining where the robot can or\ncannot safely move. A significant source of difficulty in this determination\narises from stochasticity and uncertainty, coming from localization error,\nsensor sparsity and noise, difficult-to-model robot-ground interactions, and\ndisturbances to the motion of the vehicle. Classical approaches to this problem\nrely on geometric analysis of the surrounding terrain, which can be prone to\nmodeling errors and can be computationally expensive. Moreover, modeling the\ndistribution of uncertain traversability costs is a difficult task, compounded\nby the various error sources mentioned above. In this work, we take a\nprincipled learning approach to this problem. We introduce a neural network\narchitecture for robustly learning the distribution of traversability costs.\nBecause we are motivated by preserving the life of the robot, we tackle this\nlearning problem from the perspective of learning tail-risks, i.e. the\nConditional Value-at-Risk (CVaR). We show that this approach reliably learns\nthe expected tail risk given a desired probability risk threshold between 0 and\n1, producing a traversability costmap which is more robust to outliers, more\naccurately captures tail risks, and is more computationally efficient, when\ncompared against baselines. We validate our method on data collected a legged\nrobot navigating challenging, unstructured environments including an abandoned\nsubway, limestone caves, and lava tube caves.\n",
        "published": "2021",
        "authors": [
            "David D. Fan",
            "Sharmita Dey",
            "Ali-akbar Agha-mohammadi",
            "Evangelos A. Theodorou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.11762v1",
        "title": "DR2L: Surfacing Corner Cases to Robustify Autonomous Driving via Domain\n  Randomization Reinforcement Learning",
        "abstract": "  How to explore corner cases as efficiently and thoroughly as possible has\nlong been one of the top concerns in the context of deep reinforcement learning\n(DeepRL) autonomous driving. Training with simulated data is less costly and\ndangerous than utilizing real-world data, but the inconsistency of parameter\ndistribution and the incorrect system modeling in simulators always lead to an\ninevitable Sim2real gap, which probably accounts for the underperformance in\nnovel, anomalous and risky cases that simulators can hardly generate. Domain\nRandomization(DR) is a methodology that can bridge this gap with little or no\nreal-world data. Consequently, in this research, an adversarial model is put\nforward to robustify DeepRL-based autonomous vehicles trained in simulation to\ngradually surfacing harder events, so that the models could readily transfer to\nthe real world.\n",
        "published": "2021",
        "authors": [
            "Haoyi Niu",
            "Jianming Hu",
            "Zheyu Cui",
            "Yi Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.12931v2",
        "title": "Autonomous Reinforcement Learning via Subgoal Curricula",
        "abstract": "  Reinforcement learning (RL) promises to enable autonomous acquisition of\ncomplex behaviors for diverse agents. However, the success of current\nreinforcement learning algorithms is predicated on an often under-emphasised\nrequirement -- each trial needs to start from a fixed initial state\ndistribution. Unfortunately, resetting the environment to its initial state\nafter each trial requires substantial amount of human supervision and extensive\ninstrumentation of the environment which defeats the goal of autonomous\nacquisition of complex behaviors. In this work, we propose Value-accelerated\nPersistent Reinforcement Learning (VaPRL), which generates a curriculum of\ninitial states such that the agent can bootstrap on the success of easier tasks\nto efficiently learn harder tasks. The agent also learns to reach the initial\nstates proposed by the curriculum, minimizing the reliance on human\ninterventions into the learning. We observe that VaPRL reduces the\ninterventions required by three orders of magnitude compared to episodic RL\nwhile outperforming prior state-of-the art methods for reset-free RL both in\nterms of sample efficiency and asymptotic performance on a variety of simulated\nrobotics problems.\n",
        "published": "2021",
        "authors": [
            "Archit Sharma",
            "Abhishek Gupta",
            "Sergey Levine",
            "Karol Hausman",
            "Chelsea Finn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.12942v1",
        "title": "Reinforcement Learning with Formal Performance Metrics for Quadcopter\n  Attitude Control under Non-nominal Contexts",
        "abstract": "  We explore the reinforcement learning approach to designing controllers by\nextensively discussing the case of a quadcopter attitude controller. We provide\nall details allowing to reproduce our approach, starting with a model of the\ndynamics of a crazyflie 2.0 under various nominal and non-nominal conditions,\nincluding partial motor failures and wind gusts. We develop a robust form of a\nsignal temporal logic to quantitatively evaluate the vehicle's behavior and\nmeasure the performance of controllers. The paper thoroughly describes the\nchoices in training algorithms, neural net architecture, hyperparameters,\nobservation space in view of the different performance metrics we have\nintroduced. We discuss the robustness of the obtained controllers, both to\npartial loss of power for one rotor and to wind gusts and finish by drawing\nconclusions on practical controller design by reinforcement learning.\n",
        "published": "2021",
        "authors": [
            "Nicola Bernini",
            "Mikhail Bessa",
            "R\u00e9mi Delmas",
            "Arthur Gold",
            "Eric Goubault",
            "Romain Pennec",
            "Sylvie Putot",
            "Fran\u00e7ois Sillion"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.13393v1",
        "title": "Meaning Versus Information, Prediction Versus Memory, and Question\n  Versus Answer",
        "abstract": "  Brain science and artificial intelligence have made great progress toward the\nunderstanding and engineering of the human mind. The progress has accelerated\nsignificantly since the turn of the century thanks to new methods for probing\nthe brain (both structure and function), and rapid development in deep learning\nresearch. However, despite these new developments, there are still many open\nquestions, such as how to understand the brain at the system level, and various\nrobustness issues and limitations of deep learning. In this informal essay, I\nwill talk about some of the concepts that are central to brain science and\nartificial intelligence, such as information and memory, and discuss how a\ndifferent view on these concepts can help us move forward, beyond current\nlimits of our understanding in these fields.\n",
        "published": "2021",
        "authors": [
            "Yoonsuck Choe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.00817v2",
        "title": "Do Differentiable Simulators Give Better Policy Gradients?",
        "abstract": "  Differentiable simulators promise faster computation time for reinforcement\nlearning by replacing zeroth-order gradient estimates of a stochastic objective\nwith an estimate based on first-order gradients. However, it is yet unclear\nwhat factors decide the performance of the two estimators on complex landscapes\nthat involve long-horizon planning and control on physical systems, despite the\ncrucial relevance of this question for the utility of differentiable\nsimulators. We show that characteristics of certain physical systems, such as\nstiffness or discontinuities, may compromise the efficacy of the first-order\nestimator, and analyze this phenomenon through the lens of bias and variance.\nWe additionally propose an $\\alpha$-order gradient estimator, with $\\alpha \\in\n[0,1]$, which correctly utilizes exact gradients to combine the efficiency of\nfirst-order estimates with the robustness of zero-order methods. We demonstrate\nthe pitfalls of traditional estimators and the advantages of the $\\alpha$-order\nestimator on some numerical examples.\n",
        "published": "2022",
        "authors": [
            "H. J. Terry Suh",
            "Max Simchowitz",
            "Kaiqing Zhang",
            "Russ Tedrake"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.00907v4",
        "title": "Using Deep Learning to Bootstrap Abstractions for Hierarchical Robot\n  Planning",
        "abstract": "  This paper addresses the problem of learning abstractions that boost robot\nplanning performance while providing strong guarantees of reliability. Although\nstate-of-the-art hierarchical robot planning algorithms allow robots to\nefficiently compute long-horizon motion plans for achieving user desired tasks,\nthese methods typically rely upon environment-dependent state and action\nabstractions that need to be hand-designed by experts.\n  We present a new approach for bootstrapping the entire hierarchical planning\nprocess. This allows us to compute abstract states and actions for new\nenvironments automatically using the critical regions predicted by a deep\nneural network with an auto-generated robot-specific architecture. We show that\nthe learned abstractions can be used with a novel multi-source bi-directional\nhierarchical robot planning algorithm that is sound and probabilistically\ncomplete. An extensive empirical evaluation on twenty different settings using\nholonomic and non-holonomic robots shows that (a) our learned abstractions\nprovide the information necessary for efficient multi-source hierarchical\nplanning; and that (b) this approach of learning, abstractions, and planning\noutperforms state-of-the-art baselines by nearly a factor of ten in terms of\nplanning time on test environments not seen during training.\n",
        "published": "2022",
        "authors": [
            "Naman Shah",
            "Siddharth Srivastava"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.00914v2",
        "title": "Lipschitz-constrained Unsupervised Skill Discovery",
        "abstract": "  We study the problem of unsupervised skill discovery, whose goal is to learn\na set of diverse and useful skills with no external reward. There have been a\nnumber of skill discovery methods based on maximizing the mutual information\n(MI) between skills and states. However, we point out that their MI objectives\nusually prefer static skills to dynamic ones, which may hinder the application\nfor downstream tasks. To address this issue, we propose Lipschitz-constrained\nSkill Discovery (LSD), which encourages the agent to discover more diverse,\ndynamic, and far-reaching skills. Another benefit of LSD is that its learned\nrepresentation function can be utilized for solving goal-following downstream\ntasks even in a zero-shot manner - i.e., without further training or complex\nplanning. Through experiments on various MuJoCo robotic locomotion and\nmanipulation environments, we demonstrate that LSD outperforms previous\napproaches in terms of skill diversity, state space coverage, and performance\non seven downstream tasks including the challenging task of following multiple\ngoals on Humanoid. Our code and videos are available at\nhttps://shpark.me/projects/lsd/.\n",
        "published": "2022",
        "authors": [
            "Seohong Park",
            "Jongwook Choi",
            "Jaekyeom Kim",
            "Honglak Lee",
            "Gunhee Kim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.01741v4",
        "title": "How to Leverage Unlabeled Data in Offline Reinforcement Learning",
        "abstract": "  Offline reinforcement learning (RL) can learn control policies from static\ndatasets but, like standard RL methods, it requires reward annotations for\nevery transition. In many cases, labeling large datasets with rewards may be\ncostly, especially if those rewards must be provided by human labelers, while\ncollecting diverse unlabeled data might be comparatively inexpensive. How can\nwe best leverage such unlabeled data in offline RL? One natural solution is to\nlearn a reward function from the labeled data and use it to label the unlabeled\ndata. In this paper, we find that, perhaps surprisingly, a much simpler method\nthat simply applies zero rewards to unlabeled data leads to effective data\nsharing both in theory and in practice, without learning any reward model at\nall. While this approach might seem strange (and incorrect) at first, we\nprovide extensive theoretical and empirical analysis that illustrates how it\ntrades off reward bias, sample complexity and distributional shift, often\nleading to good results. We characterize conditions under which this simple\nstrategy is effective, and further show that extending it with a simple\nreweighting approach can further alleviate the bias introduced by using\nincorrect reward labels. Our empirical evaluation confirms these findings in\nsimulated robotic locomotion, navigation, and manipulation settings.\n",
        "published": "2022",
        "authors": [
            "Tianhe Yu",
            "Aviral Kumar",
            "Yevgen Chebotar",
            "Karol Hausman",
            "Chelsea Finn",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.02654v1",
        "title": "Doing Right by Not Doing Wrong in Human-Robot Collaboration",
        "abstract": "  As robotic systems become more and more capable of assisting humans in their\neveryday lives, we must consider the opportunities for these artificial agents\nto make their human collaborators feel unsafe or to treat them unfairly. Robots\ncan exhibit antisocial behavior causing physical harm to people or reproduce\nunfair behavior replicating and even amplifying historical and societal biases\nwhich are detrimental to humans they interact with. In this paper, we discuss\nthese issues considering sociable robotic manipulation and fair robotic\ndecision making. We propose a novel approach to learning fair and sociable\nbehavior, not by reproducing positive behavior, but rather by avoiding negative\nbehavior. In this study, we highlight the importance of incorporating\nsociability in robot manipulation, as well as the need to consider fairness in\nhuman-robot interactions.\n",
        "published": "2022",
        "authors": [
            "Laura Londo\u00f1o",
            "Adrian R\u00f6fer",
            "Tim Welschehold",
            "Abhinav Valada"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.03481v3",
        "title": "A Ranking Game for Imitation Learning",
        "abstract": "  We propose a new framework for imitation learning -- treating imitation as a\ntwo-player ranking-based game between a policy and a reward. In this game, the\nreward agent learns to satisfy pairwise performance rankings between behaviors,\nwhile the policy agent learns to maximize this reward. In imitation learning,\nnear-optimal expert data can be difficult to obtain, and even in the limit of\ninfinite data cannot imply a total ordering over trajectories as preferences\ncan. On the other hand, learning from preferences alone is challenging as a\nlarge number of preferences are required to infer a high-dimensional reward\nfunction, though preference data is typically much easier to collect than\nexpert demonstrations. The classical inverse reinforcement learning (IRL)\nformulation learns from expert demonstrations but provides no mechanism to\nincorporate learning from offline preferences and vice versa. We instantiate\nthe proposed ranking-game framework with a novel ranking loss giving an\nalgorithm that can simultaneously learn from expert demonstrations and\npreferences, gaining the advantages of both modalities. Our experiments show\nthat the proposed method achieves state-of-the-art sample efficiency and can\nsolve previously unsolvable tasks in the Learning from Observation (LfO)\nsetting. Project video and code can be found at\nhttps://hari-sikchi.github.io/rank-game/\n",
        "published": "2022",
        "authors": [
            "Harshit Sikchi",
            "Akanksha Saran",
            "Wonjoon Goo",
            "Scott Niekum"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.05244v2",
        "title": "REvolveR: Continuous Evolutionary Models for Robot-to-robot Policy\n  Transfer",
        "abstract": "  A popular paradigm in robotic learning is to train a policy from scratch for\nevery new robot. This is not only inefficient but also often impractical for\ncomplex robots. In this work, we consider the problem of transferring a policy\nacross two different robots with significantly different parameters such as\nkinematics and morphology. Existing approaches that train a new policy by\nmatching the action or state transition distribution, including imitation\nlearning methods, fail due to optimal action and/or state distribution being\nmismatched in different robots. In this paper, we propose a novel method named\n$REvolveR$ of using continuous evolutionary models for robotic policy transfer\nimplemented in a physics simulator. We interpolate between the source robot and\nthe target robot by finding a continuous evolutionary change of robot\nparameters. An expert policy on the source robot is transferred through\ntraining on a sequence of intermediate robots that gradually evolve into the\ntarget robot. Experiments on a physics simulator show that the proposed\ncontinuous evolutionary model can effectively transfer the policy across robots\nand achieve superior sample efficiency on new robots. The proposed method is\nespecially advantageous in sparse reward settings where exploration can be\nsignificantly reduced. Code is released at https://github.com/xingyul/revolver.\n",
        "published": "2022",
        "authors": [
            "Xingyu Liu",
            "Deepak Pathak",
            "Kris M. Kitani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.07013v2",
        "title": "Robust Policy Learning over Multiple Uncertainty Sets",
        "abstract": "  Reinforcement learning (RL) agents need to be robust to variations in\nsafety-critical environments. While system identification methods provide a way\nto infer the variation from online experience, they can fail in settings where\nfast identification is not possible. Another dominant approach is robust RL\nwhich produces a policy that can handle worst-case scenarios, but these methods\nare generally designed to achieve robustness to a single uncertainty set that\nmust be specified at train time. Towards a more general solution, we formulate\nthe multi-set robustness problem to learn a policy robust to different\nperturbation sets. We then design an algorithm that enjoys the benefits of both\nsystem identification and robust RL: it reduces uncertainty where possible\ngiven a few interactions, but can still act robustly with respect to the\nremaining uncertainty. On a diverse set of control tasks, our approach\ndemonstrates improved worst-case performance on new environments compared to\nprior methods based on system identification and on robust RL alone.\n",
        "published": "2022",
        "authors": [
            "Annie Xie",
            "Shagun Sodhani",
            "Chelsea Finn",
            "Joelle Pineau",
            "Amy Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.08266v2",
        "title": "Open-Ended Reinforcement Learning with Neural Reward Functions",
        "abstract": "  Inspired by the great success of unsupervised learning in Computer Vision and\nNatural Language Processing, the Reinforcement Learning community has recently\nstarted to focus more on unsupervised discovery of skills. Most current\napproaches, like DIAYN or DADS, optimize some form of mutual information\nobjective. We propose a different approach that uses reward functions encoded\nby neural networks. These are trained iteratively to reward more complex\nbehavior. In high-dimensional robotic environments our approach learns a wide\nrange of interesting skills including front-flips for Half-Cheetah and\none-legged running for Humanoid. In the pixel-based Montezuma's Revenge\nenvironment our method also works with minimal changes and it learns complex\nskills that involve interacting with items and visiting diverse locations. The\nimplementation of our approach can be found in this link:\nhttps://github.com/amujika/Open-Ended-Reinforcement-Learning-with-Neural-Reward-Functions.\n",
        "published": "2022",
        "authors": [
            "Robert Meier",
            "Asier Mujika"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.09265v1",
        "title": "Deep Movement Primitives: toward Breast Cancer Examination Robot",
        "abstract": "  Breast cancer is the most common type of cancer worldwide. A robotic system\nperforming autonomous breast palpation can make a significant impact on the\nrelated health sector worldwide. However, robot programming for breast\npalpating with different geometries is very complex and unsolved. Robot\nlearning from demonstrations (LfD) reduces the programming time and cost.\nHowever, the available LfD are lacking the modelling of the manipulation\npath/trajectory as an explicit function of the visual sensory information. This\npaper presents a novel approach to manipulation path/trajectory planning called\ndeep Movement Primitives that successfully generates the movements of a\nmanipulator to reach a breast phantom and perform the palpation. We show the\neffectiveness of our approach by a series of real-robot experiments of reaching\nand palpating a breast phantom. The experimental results indicate our approach\noutperforms the state-of-the-art method.\n",
        "published": "2022",
        "authors": [
            "Oluwatoyin Sanni",
            "Giorgio Bonvicini",
            "Muhammad Arshad Khan",
            "Pablo C. Lopez-Custodio",
            "Kiyanoush Nazari",
            "Amir M. Ghalamzan E."
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.10019v1",
        "title": "Autonomous Warehouse Robot using Deep Q-Learning",
        "abstract": "  In warehouses, specialized agents need to navigate, avoid obstacles and\nmaximize the use of space in the warehouse environment. Due to the\nunpredictability of these environments, reinforcement learning approaches can\nbe applied to complete these tasks. In this paper, we propose using Deep\nReinforcement Learning (DRL) to address the robot navigation and obstacle\navoidance problem and traditional Q-learning with minor variations to maximize\nthe use of space for product placement. We first investigate the problem for\nthe single robot case. Next, based on the single robot model, we extend our\nsystem to the multi-robot case. We use a strategic variation of Q-tables to\nperform multi-agent Q-learning. We successfully test the performance of our\nmodel in a 2D simulation environment for both the single and multi-robot cases.\n",
        "published": "2022",
        "authors": [
            "Ismot Sadik Peyas",
            "Zahid Hasan",
            "Md. Rafat Rahman Tushar",
            "Al Musabbir",
            "Raisa Mehjabin Azni",
            "Shahnewaz Siddique"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.10341v1",
        "title": "Efficient Learning of Safe Driving Policy via Human-AI Copilot\n  Optimization",
        "abstract": "  Human intervention is an effective way to inject human knowledge into the\ntraining loop of reinforcement learning, which can bring fast learning and\nensured training safety. Given the very limited budget of human intervention,\nit remains challenging to design when and how human expert interacts with the\nlearning agent in the training. In this work, we develop a novel\nhuman-in-the-loop learning method called Human-AI Copilot Optimization\n(HACO).To allow the agent's sufficient exploration in the risky environments\nwhile ensuring the training safety, the human expert can take over the control\nand demonstrate how to avoid probably dangerous situations or trivial\nbehaviors. The proposed HACO then effectively utilizes the data both from the\ntrial-and-error exploration and human's partial demonstration to train a\nhigh-performing agent. HACO extracts proxy state-action values from partial\nhuman demonstration and optimizes the agent to improve the proxy values\nmeanwhile reduce the human interventions. The experiments show that HACO\nachieves a substantially high sample efficiency in the safe driving benchmark.\nHACO can train agents to drive in unseen traffic scenarios with a handful of\nhuman intervention budget and achieve high safety and generalizability,\noutperforming both reinforcement learning and imitation learning baselines with\na large margin. Code and demo videos are available at:\nhttps://decisionforce.github.io/HACO/.\n",
        "published": "2022",
        "authors": [
            "Quanyi Li",
            "Zhenghao Peng",
            "Bolei Zhou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.10765v3",
        "title": "Transporters with Visual Foresight for Solving Unseen Rearrangement\n  Tasks",
        "abstract": "  Rearrangement tasks have been identified as a crucial challenge for\nintelligent robotic manipulation, but few methods allow for precise\nconstruction of unseen structures. We propose a visual foresight model for\npick-and-place rearrangement manipulation which is able to learn efficiently.\nIn addition, we develop a multi-modal action proposal module which builds on\nthe Goal-Conditioned Transporter Network, a state-of-the-art imitation learning\nmethod. Our image-based task planning method, Transporters with Visual\nForesight, is able to learn from only a handful of data and generalize to\nmultiple unseen tasks in a zero-shot manner. TVF is able to improve the\nperformance of a state-of-the-art imitation learning method on unseen tasks in\nsimulation and real robot experiments. In particular, the average success rate\non unseen tasks improves from 55.4% to 78.5% in simulation experiments and from\n30% to 63.3% in real robot experiments when given only tens of expert\ndemonstrations. Video and code are available on our project website:\nhttps://chirikjianlab.github.io/tvf/\n",
        "published": "2022",
        "authors": [
            "Hongtao Wu",
            "Jikai Ye",
            "Xin Meng",
            "Chris Paxton",
            "Gregory Chirikjian"
        ]
    }
]