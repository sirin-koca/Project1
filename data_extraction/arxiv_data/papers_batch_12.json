[
    {
        "id": "http://arxiv.org/abs/1712.02779v4",
        "title": "Exploring the Landscape of Spatial Robustness",
        "abstract": "  The study of adversarial robustness has so far largely focused on\nperturbations bound in p-norms. However, state-of-the-art models turn out to be\nalso vulnerable to other, more natural classes of perturbations such as\ntranslations and rotations. In this work, we thoroughly investigate the\nvulnerability of neural network--based classifiers to rotations and\ntranslations. While data augmentation offers relatively small robustness, we\nuse ideas from robust optimization and test-time input aggregation to\nsignificantly improve robustness. Finally we find that, in contrast to the\np-norm case, first-order methods cannot reliably find worst-case perturbations.\nThis highlights spatial robustness as a fundamentally different setting\nrequiring additional study. Code available at\nhttps://github.com/MadryLab/adversarial_spatial and\nhttps://github.com/MadryLab/spatial-pytorch.\n",
        "published": "2017",
        "authors": [
            "Logan Engstrom",
            "Brandon Tran",
            "Dimitris Tsipras",
            "Ludwig Schmidt",
            "Aleksander Madry"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1712.03351v1",
        "title": "Peephole: Predicting Network Performance Before Training",
        "abstract": "  The quest for performant networks has been a significant force that drives\nthe advancements of deep learning in recent years. While rewarding, improving\nnetwork design has never been an easy journey. The large design space combined\nwith the tremendous cost required for network training poses a major obstacle\nto this endeavor. In this work, we propose a new approach to this problem,\nnamely, predicting the performance of a network before training, based on its\narchitecture. Specifically, we develop a unified way to encode individual\nlayers into vectors and bring them together to form an integrated description\nvia LSTM. Taking advantage of the recurrent network's strong expressive power,\nthis method can reliably predict the performances of various network\narchitectures. Our empirical studies showed that it not only achieved accurate\npredictions but also produced consistent rankings across datasets -- a key\ndesideratum in performance prediction.\n",
        "published": "2017",
        "authors": [
            "Boyang Deng",
            "Junjie Yan",
            "Dahua Lin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1712.03541v2",
        "title": "An Architecture Combining Convolutional Neural Network (CNN) and Support\n  Vector Machine (SVM) for Image Classification",
        "abstract": "  Convolutional neural networks (CNNs) are similar to \"ordinary\" neural\nnetworks in the sense that they are made up of hidden layers consisting of\nneurons with \"learnable\" parameters. These neurons receive inputs, performs a\ndot product, and then follows it with a non-linearity. The whole network\nexpresses the mapping between raw image pixels and their class scores.\nConventionally, the Softmax function is the classifier used at the last layer\nof this network. However, there have been studies (Alalshekmubarak and Smith,\n2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited\nstudies introduce the usage of linear support vector machine (SVM) in an\nartificial neural network architecture. This project is yet another take on the\nsubject, and is inspired by (Tang, 2013). Empirical data has shown that the\nCNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST\ndataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax\nwas able to achieve a test accuracy of ~99.23% using the same dataset. Both\nmodels were also tested on the recently-published Fashion-MNIST dataset (Xiao,\nRasul, and Vollgraf, 2017), which is suppose to be a more difficult image\nclassification dataset than MNIST (Zalandoresearch, 2017). This proved to be\nthe case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax\nreached a test accuracy of ~91.86%. The said results may be improved if data\npreprocessing techniques were employed on the datasets, and if the base CNN\nmodel was a relatively more sophisticated than the one used in this study.\n",
        "published": "2017",
        "authors": [
            "Abien Fred Agarap"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1712.08314v3",
        "title": "Benchmarking Decoupled Neural Interfaces with Synthetic Gradients",
        "abstract": "  Artifical Neural Networks are a particular class of learning systems modeled\nafter biological neural functions with an interesting penchant for Hebbian\nlearning, that is \"neurons that wire together, fire together\". However, unlike\ntheir natural counterparts, artificial neural networks have a close and\nstringent coupling between the modules of neurons in the network. This coupling\nor locking imposes upon the network a strict and inflexible structure that\nprevent layers in the network from updating their weights until a full\nfeed-forward and backward pass has occurred. Such a constraint though may have\nsufficed for a while, is now no longer feasible in the era of very-large-scale\nmachine learning, coupled with the increased desire for parallelization of the\nlearning process across multiple computing infrastructures. To solve this\nproblem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are\nintroduced as a viable alternative to the backpropagation algorithm. This paper\nperforms a speed benchmark to compare the speed and accuracy capabilities of\nSG-DNI as opposed to a standard neural interface using multilayer perceptron\nMLP. SG-DNI shows good promise, in that it not only captures the learning\nproblem, it is also over 3-fold faster due to it asynchronous learning\ncapabilities.\n",
        "published": "2017",
        "authors": [
            "Ekaba Bisong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.06187v3",
        "title": "Study and Observation of the Variations of Accuracies for Handwritten\n  Digits Recognition with Various Hidden Layers and Epochs using Convolutional\n  Neural Network",
        "abstract": "  Nowadays, deep learning can be employed to a wide ranges of fields including\nmedicine, engineering, etc. In deep learning, Convolutional Neural Network\n(CNN) is extensively used in the pattern and sequence recognition, video\nanalysis, natural language processing, spam detection, topic categorization,\nregression analysis, speech recognition, image classification, object\ndetection, segmentation, face recognition, robotics, and control. The benefits\nassociated with its near human level accuracies in large applications lead to\nthe growing acceptance of CNN in recent years. The primary contribution of this\npaper is to analyze the impact of the pattern of the hidden layers of a CNN\nover the overall performance of the network. To demonstrate this influence, we\napplied neural network with different layers on the Modified National Institute\nof Standards and Technology (MNIST) dataset. Also, is to observe the variations\nof accuracies of the network for various numbers of hidden layers and epochs\nand to make comparison and contrast among them. The system is trained utilizing\nstochastic gradient and backpropagation algorithm and tested with feedforward\nalgorithm.\n",
        "published": "2018",
        "authors": [
            "Rezoana Bente Arif",
            "Md. Abu Bakr Siddique",
            "Mohammad Mahmudur Rahman Khan",
            "Mahjabin Rahman Oishe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.02956v2",
        "title": "LNEMLC: Label Network Embeddings for Multi-Label Classification",
        "abstract": "  Multi-label classification aims to classify instances with discrete\nnon-exclusive labels. Most approaches on multi-label classification focus on\neffective adaptation or transformation of existing binary and multi-class\nlearning approaches but fail in modelling the joint probability of labels or do\nnot preserve generalization abilities for unseen label combinations. To address\nthese issues we propose a new multi-label classification scheme, LNEMLC - Label\nNetwork Embedding for Multi-Label Classification, that embeds the label network\nand uses it to extend input space in learning and inference of any base\nmulti-label classifier. The approach allows capturing of labels' joint\nprobability at low computational complexity providing results comparable to the\nbest methods reported in the literature. We demonstrate how the method reveals\nstatistically significant improvements over the simple kNN baseline classifier.\nWe also provide hints for selecting the robust configuration that works\nsatisfactorily across data domains.\n",
        "published": "2018",
        "authors": [
            "Piotr Szyma\u0144ski",
            "Tomasz Kajdanowicz",
            "Nitesh Chawla"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.02080v1",
        "title": "Exploring Deep Spiking Neural Networks for Automated Driving\n  Applications",
        "abstract": "  Neural networks have become the standard model for various computer vision\ntasks in automated driving including semantic segmentation, moving object\ndetection, depth estimation, visual odometry, etc. The main flavors of neural\nnetworks which are used commonly are convolutional (CNN) and recurrent (RNN).\nIn spite of rapid progress in embedded processors, power consumption and cost\nis still a bottleneck. Spiking Neural Networks (SNNs) are gradually progressing\nto achieve low-power event-driven hardware architecture which has a potential\nfor high efficiency. In this paper, we explore the role of deep spiking neural\nnetworks (SNN) for automated driving applications. We provide an overview of\nprogress on SNN and argue how it can be a good fit for automated driving\napplications.\n",
        "published": "2019",
        "authors": [
            "Sambit Mohapatra",
            "Heinrich Gotzig",
            "Senthil Yogamani",
            "Stefan Milz",
            "Raoul Zollner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.11991v5",
        "title": "Parabolic Approximation Line Search for DNNs",
        "abstract": "  A major challenge in current optimization research for deep learning is to\nautomatically find optimal step sizes for each update step. The optimal step\nsize is closely related to the shape of the loss in the update step direction.\nHowever, this shape has not yet been examined in detail. This work shows\nempirically that the batch loss over lines in negative gradient direction is\nmostly convex locally and well suited for one-dimensional parabolic\napproximations. By exploiting this parabolic property we introduce a simple and\nrobust line search approach, which performs loss-shape dependent update steps.\nOur approach combines well-known methods such as parabolic approximation, line\nsearch and conjugate gradient, to perform efficiently. It surpasses other step\nsize estimating methods and competes with common optimization methods on a\nlarge variety of experiments without the need of hand-designed step size\nschedules. Thus, it is of interest for objectives where step-size schedules are\nunknown or do not perform well. Our extensive evaluation includes multiple\ncomprehensive hyperparameter grid searches on several datasets and\narchitectures. Finally, we provide a general investigation of exact line\nsearches in the context of batch losses and exact losses, including their\nrelation to our line search approach.\n",
        "published": "2019",
        "authors": [
            "Maximus Mutschler",
            "Andreas Zell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.01137v3",
        "title": "Mixing autoencoder with classifier: conceptual data visualization",
        "abstract": "  In this short paper, a neural network that is able to form a low dimensional\ntopological hidden representation is explained. The neural network can be\ntrained as an autoencoder, a classifier or mix of both, and produces different\nlow dimensional topological map for each of them. When it is trained as an\nautoencoder, the inherent topological structure of the data can be visualized,\nwhile when it is trained as a classifier, the topological structure is further\nconstrained by the concept, for example the labels the data, hence the\nvisualization is not only structural but also conceptual. The proposed neural\nnetwork significantly differ from many dimensional reduction models, primarily\nin its ability to execute both supervised and unsupervised dimensional\nreduction. The neural network allows multi perspective visualization of the\ndata, and thus giving more flexibility in data analysis. This paper is\nsupported by preliminary but intuitive visualization experiments.\n",
        "published": "2019",
        "authors": [
            "Pitoyo Hartono"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.01816v1",
        "title": "Handwriting-Based Gender Classification Using End-to-End Deep Neural\n  Networks",
        "abstract": "  Handwriting-based gender classification is a well-researched problem that has\nbeen approached mainly by traditional machine learning techniques. In this\npaper, we propose a novel deep learning-based approach for this task.\nSpecifically, we present a convolutional neural network (CNN), which performs\nautomatic feature extraction from a given handwritten image, followed by\nclassification of the writer's gender. Also, we introduce a new dataset of\nlabeled handwritten samples, in Hebrew and English, of 405 participants.\nComparing the gender classification accuracy on this dataset against human\nexaminers, our results show that the proposed deep learning-based approach is\nsubstantially more accurate than that of humans.\n",
        "published": "2019",
        "authors": [
            "Evyatar Illouz",
            "Eli David",
            "Nathan S. Netanyahu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.02292v1",
        "title": "Deep Double Descent: Where Bigger Models and More Data Hurt",
        "abstract": "  We show that a variety of modern deep learning tasks exhibit a\n\"double-descent\" phenomenon where, as we increase model size, performance first\ngets worse and then gets better. Moreover, we show that double descent occurs\nnot just as a function of model size, but also as a function of the number of\ntraining epochs. We unify the above phenomena by defining a new complexity\nmeasure we call the effective model complexity and conjecture a generalized\ndouble descent with respect to this measure. Furthermore, our notion of model\ncomplexity allows us to identify certain regimes where increasing (even\nquadrupling) the number of train samples actually hurts test performance.\n",
        "published": "2019",
        "authors": [
            "Preetum Nakkiran",
            "Gal Kaplun",
            "Yamini Bansal",
            "Tristan Yang",
            "Boaz Barak",
            "Ilya Sutskever"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.02707v1",
        "title": "A Novel Hybrid Scheme Using Genetic Algorithms and Deep Learning for the\n  Reconstruction of Portuguese Tile Panels",
        "abstract": "  This paper presents a novel scheme, based on a unique combination of genetic\nalgorithms (GAs) and deep learning (DL), for the automatic reconstruction of\nPortuguese tile panels, a challenging real-world variant of the jigsaw puzzle\nproblem (JPP) with important national heritage implications. Specifically, we\nintroduce an enhanced GA-based puzzle solver, whose integration with a novel\nDL-based compatibility measure (DLCM) yields state-of-the-art performance,\nregarding the above application. Current compatibility measures consider\ntypically (the chromatic information of) edge pixels (between adjacent tiles),\nand help achieve high accuracy for the synthetic JPP variant. However, such\nmeasures exhibit rather poor performance when applied to the Portuguese tile\npanels, which are susceptible to various real-world effects, e.g.,\nmonochromatic panels, non-squared tiles, edge degradation, etc. To overcome\nsuch difficulties, we have developed a novel DLCM to extract high-level\ntexture/color statistics from the entire tile information.\n  Integrating this measure with our enhanced GA-based puzzle solver, we have\ndemonstrated, for the first time, how to deal most effectively with large-scale\nreal-world problems, such as the Portuguese tile problem. Specifically, we have\nachieved 82% accuracy for the reconstruction of Portuguese tile panels with\nunknown piece rotation and puzzle dimension (compared to merely 3.5% average\naccuracy achieved by the best method known for solving this problem variant).\nThe proposed method outperforms even human experts in several cases, correcting\ntheir mistakes in the manual tile assembly.\n",
        "published": "2019",
        "authors": [
            "Daniel Rika",
            "Dror Sholomon",
            "Eli David",
            "Nathan S. Netanyahu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.02983v1",
        "title": "DeepEthnic: Multi-Label Ethnic Classification from Face Images",
        "abstract": "  Ethnic group classification is a well-researched problem, which has been\npursued mainly during the past two decades via traditional approaches of image\nprocessing and machine learning. In this paper, we propose a method of\nclassifying an image face into an ethnic group by applying transfer learning\nfrom a previously trained classification network for large-scale data\nrecognition. Our proposed method yields state-of-the-art success rates of\n99.02%, 99.76%, 99.2%, and 96.7%, respectively, for the four ethnic groups:\nAfrican, Asian, Caucasian, and Indian.\n",
        "published": "2019",
        "authors": [
            "Katia Huri",
            "Eli David",
            "Nathan S. Netanyahu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.03201v1",
        "title": "A Neural Spiking Approach Compared to Deep Feedforward Networks on\n  Stepwise Pixel Erasement",
        "abstract": "  In real world scenarios, objects are often partially occluded. This requires\na robustness for object recognition against these perturbations. Convolutional\nnetworks have shown good performances in classification tasks. The learned\nconvolutional filters seem similar to receptive fields of simple cells found in\nthe primary visual cortex. Alternatively, spiking neural networks are more\nbiological plausible. We developed a two layer spiking network, trained on\nnatural scenes with a biologically plausible learning rule. It is compared to\ntwo deep convolutional neural networks using a classification task of stepwise\npixel erasement on MNIST. In comparison to these networks the spiking approach\nachieves good accuracy and robustness.\n",
        "published": "2019",
        "authors": [
            "Ren\u00e9 Larisch",
            "Michael Teichmann",
            "Fred H. Hamker"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.08866v2",
        "title": "Continuous Meta-Learning without Tasks",
        "abstract": "  Meta-learning is a promising strategy for learning to efficiently learn\nwithin new tasks, using data gathered from a distribution of tasks. However,\nthe meta-learning literature thus far has focused on the task segmented\nsetting, where at train-time, offline data is assumed to be split according to\nthe underlying task, and at test-time, the algorithms are optimized to learn in\na single task. In this work, we enable the application of generic meta-learning\nalgorithms to settings where this task segmentation is unavailable, such as\ncontinual online learning with a time-varying task. We present meta-learning\nvia online changepoint analysis (MOCA), an approach which augments a\nmeta-learning algorithm with a differentiable Bayesian changepoint detection\nscheme. The framework allows both training and testing directly on time series\ndata without segmenting it into discrete tasks. We demonstrate the utility of\nthis approach on a nonlinear meta-regression benchmark as well as two\nmeta-image-classification benchmarks.\n",
        "published": "2019",
        "authors": [
            "James Harrison",
            "Apoorva Sharma",
            "Chelsea Finn",
            "Marco Pavone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.10103v2",
        "title": "TentacleNet: A Pseudo-Ensemble Template for Accurate Binary\n  Convolutional Neural Networks",
        "abstract": "  Binarization is an attractive strategy for implementing lightweight Deep\nConvolutional Neural Networks (CNNs). Despite the unquestionable savings\noffered, memory footprint above all, it may induce an excessive accuracy loss\nthat prevents a widespread use. This work elaborates on this aspect introducing\nTentacleNet, a new template designed to improve the predictive performance of\nbinarized CNNs via parallelization. Inspired by the ensemble learning theory,\nit consists of a compact topology that is end-to-end trainable and organized to\nminimize memory utilization. Experimental results collected over three\nrealistic benchmarks show TentacleNet fills the gap left by classical binary\nmodels, ensuring substantial memory savings w.r.t. state-of-the-art binary\nensemble methods.\n",
        "published": "2019",
        "authors": [
            "Luca Mocerino",
            "Andrea Calimera"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.10422v2",
        "title": "NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural\n  Architecture Search",
        "abstract": "  One-shot neural architecture search (NAS) has played a crucial role in making\nNAS methods computationally feasible in practice. Nevertheless, there is still\na lack of understanding on how these weight-sharing algorithms exactly work due\nto the many factors controlling the dynamics of the process. In order to allow\na scientific study of these components, we introduce a general framework for\none-shot NAS that can be instantiated to many recently-introduced variants and\nintroduce a general benchmarking framework that draws on the recent large-scale\ntabular benchmark NAS-Bench-101 for cheap anytime evaluations of one-shot NAS\nmethods. To showcase the framework, we compare several state-of-the-art\none-shot NAS methods, examine how sensitive they are to their hyperparameters\nand how they can be improved by tuning their hyperparameters, and compare their\nperformance to that of blackbox optimizers for NAS-Bench-101.\n",
        "published": "2020",
        "authors": [
            "Arber Zela",
            "Julien Siems",
            "Frank Hutter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.03229v5",
        "title": "Non-linear Neurons with Human-like Apical Dendrite Activations",
        "abstract": "  In order to classify linearly non-separable data, neurons are typically\norganized into multi-layer neural networks that are equipped with at least one\nhidden layer. Inspired by some recent discoveries in neuroscience, we propose a\nnew model of artificial neuron along with a novel activation function enabling\nthe learning of nonlinear decision boundaries using a single neuron. We show\nthat a standard neuron followed by our novel apical dendrite activation (ADA)\ncan learn the XOR logical function with 100% accuracy. Furthermore, we conduct\nexperiments on six benchmark data sets from computer vision, signal processing\nand natural language processing, i.e. MOROCO, UTKFace, CREMA-D, Fashion-MNIST,\nTiny ImageNet and ImageNet, showing that the ADA and the leaky ADA functions\nprovide superior results to Rectified Linear Units (ReLU), leaky ReLU, RBF and\nSwish, for various neural network architectures, e.g. one-hidden-layer or\ntwo-hidden-layer multi-layer perceptrons (MLPs) and convolutional neural\nnetworks (CNNs) such as LeNet, VGG, ResNet and Character-level CNN. We obtain\nfurther performance improvements when we change the standard model of the\nneuron with our pyramidal neuron with apical dendrite activations (PyNADA). Our\ncode is available at: https://github.com/raduionescu/pynada.\n",
        "published": "2020",
        "authors": [
            "Mariana-Iuliana Georgescu",
            "Radu Tudor Ionescu",
            "Nicolae-Catalin Ristea",
            "Nicu Sebe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.04285v1",
        "title": "Deep Inverse Feature Learning: A Representation Learning of Error",
        "abstract": "  This paper introduces a novel perspective about error in machine learning and\nproposes inverse feature learning (IFL) as a representation learning approach\nthat learns a set of high-level features based on the representation of error\nfor classification or clustering purposes. The proposed perspective about error\nrepresentation is fundamentally different from current learning methods, where\nin classification approaches they interpret the error as a function of the\ndifferences between the true labels and the predicted ones or in clustering\napproaches, in which the clustering objective functions such as compactness are\nused. Inverse feature learning method operates based on a deep clustering\napproach to obtain a qualitative form of the representation of error as\nfeatures. The performance of the proposed IFL method is evaluated by applying\nthe learned features along with the original features, or just using the\nlearned features in different classification and clustering techniques for\nseveral data sets. The experimental results show that the proposed method leads\nto promising results in classification and especially in clustering. In\nclassification, the proposed features along with the primary features improve\nthe results of most of the classification methods on several popular data sets.\nIn clustering, the performance of different clustering methods is considerably\nimproved on different data sets. There are interesting results that show some\nfew features of the representation of error capture highly informative aspects\nof primary features. We hope this paper helps to utilize the error\nrepresentation learning in different feature learning domains.\n",
        "published": "2020",
        "authors": [
            "Behzad Ghazanfari",
            "Fatemeh Afghah"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.04286v2",
        "title": "Manifold Regularization for Locally Stable Deep Neural Networks",
        "abstract": "  We apply concepts from manifold regularization to develop new regularization\ntechniques for training locally stable deep neural networks. Our regularizers\nare based on a sparsification of the graph Laplacian which holds with high\nprobability when the data is sparse in high dimensions, as is common in deep\nlearning. Empirically, our networks exhibit stability in a diverse set of\nperturbation models, including $\\ell_2$, $\\ell_\\infty$, and Wasserstein-based\nperturbations; in particular, we achieve 40% adversarial accuracy on CIFAR-10\nagainst an adaptive PGD attack using $\\ell_\\infty$ perturbations of size\n$\\epsilon = 8/255$, and state-of-the-art verified accuracy of 21% in the same\nperturbation model. Furthermore, our techniques are efficient, incurring\noverhead on par with two additional parallel forward passes through the\nnetwork.\n",
        "published": "2020",
        "authors": [
            "Charles Jin",
            "Martin Rinard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.10596v2",
        "title": "Adversarial Perturbations Fool Deepfake Detectors",
        "abstract": "  This work uses adversarial perturbations to enhance deepfake images and fool\ncommon deepfake detectors. We created adversarial perturbations using the Fast\nGradient Sign Method and the Carlini and Wagner L2 norm attack in both blackbox\nand whitebox settings. Detectors achieved over 95% accuracy on unperturbed\ndeepfakes, but less than 27% accuracy on perturbed deepfakes. We also explore\ntwo improvements to deepfake detectors: (i) Lipschitz regularization, and (ii)\nDeep Image Prior (DIP). Lipschitz regularization constrains the gradient of the\ndetector with respect to the input in order to increase robustness to input\nperturbations. The DIP defense removes perturbations using generative\nconvolutional neural networks in an unsupervised manner. Regularization\nimproved the detection of perturbed deepfakes on average, including a 10%\naccuracy boost in the blackbox case. The DIP defense achieved 95% accuracy on\nperturbed deepfakes that fooled the original detector, while retaining 98%\naccuracy in other cases on a 100 image subsample.\n",
        "published": "2020",
        "authors": [
            "Apurva Gandhi",
            "Shomik Jain"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.11842v1",
        "title": "Robust Classification of High-Dimensional Spectroscopy Data Using Deep\n  Learning and Data Synthesis",
        "abstract": "  This paper presents a new approach to classification of high dimensional\nspectroscopy data and demonstrates that it outperforms other current\nstate-of-the art approaches. The specific task we consider is identifying\nwhether samples contain chlorinated solvents or not, based on their Raman\nspectra. We also examine robustness to classification of outlier samples that\nare not represented in the training set (negative outliers). A novel\napplication of a locally-connected neural network (NN) for the binary\nclassification of spectroscopy data is proposed and demonstrated to yield\nimproved accuracy over traditionally popular algorithms. Additionally, we\npresent the ability to further increase the accuracy of the locally-connected\nNN algorithm through the use of synthetic training spectra and we investigate\nthe use of autoencoder based one-class classifiers and outlier detectors.\nFinally, a two-step classification process is presented as an alternative to\nthe binary and one-class classification paradigms. This process combines the\nlocally-connected NN classifier, the use of synthetic training data, and an\nautoencoder based outlier detector to produce a model which is shown to both\nproduce high classification accuracy, and be robust to the presence of negative\noutliers.\n",
        "published": "2020",
        "authors": [
            "James Houston",
            "Frank G. Glavin",
            "Michael G. Madden"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.02003v1",
        "title": "Open-Set Recognition with Gaussian Mixture Variational Autoencoders",
        "abstract": "  In inference, open-set classification is to either classify a sample into a\nknown class from training or reject it as an unknown class. Existing deep\nopen-set classifiers train explicit closed-set classifiers, in some cases\ndisjointly utilizing reconstruction, which we find dilutes the latent\nrepresentation's ability to distinguish unknown classes. In contrast, we train\nour model to cooperatively learn reconstruction and perform class-based\nclustering in the latent space. With this, our Gaussian mixture variational\nautoencoder (GMVAE) achieves more accurate and robust open-set classification\nresults, with an average F1 improvement of 29.5%, through extensive experiments\naided by analytical results.\n",
        "published": "2020",
        "authors": [
            "Alexander Cao",
            "Yuan Luo",
            "Diego Klabjan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.02797v1",
        "title": "Overcoming Overfitting and Large Weight Update Problem in Linear\n  Rectifiers: Thresholded Exponential Rectified Linear Units",
        "abstract": "  In past few years, linear rectified unit activation functions have shown its\nsignificance in the neural networks, surpassing the performance of sigmoid\nactivations. RELU (Nair & Hinton, 2010), ELU (Clevert et al., 2015), PRELU (He\net al., 2015), LRELU (Maas et al., 2013), SRELU (Jin et al., 2016),\nThresholdedRELU, all these linear rectified activation functions have its own\nsignificance over others in some aspect. Most of the time these activation\nfunctions suffer from bias shift problem due to non-zero output mean, and high\nweight update problem in deep complex networks due to unit gradient, which\nresults in slower training, and high variance in model prediction respectively.\nIn this paper, we propose, \"Thresholded exponential rectified linear unit\"\n(TERELU) activation function that works better in alleviating in overfitting:\nlarge weight update problem. Along with alleviating overfitting problem, this\nmethod also gives good amount of non-linearity as compared to other linear\nrectifiers. We will show better performance on the various datasets using\nneural networks, considering TERELU activation method compared to other\nactivations.\n",
        "published": "2020",
        "authors": [
            "Vijay Pandey"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.03179v5",
        "title": "Discovering Parametric Activation Functions",
        "abstract": "  Recent studies have shown that the choice of activation function can\nsignificantly affect the performance of deep learning networks. However, the\nbenefits of novel activation functions have been inconsistent and task\ndependent, and therefore the rectified linear unit (ReLU) is still the most\ncommonly used. This paper proposes a technique for customizing activation\nfunctions automatically, resulting in reliable improvements in performance.\nEvolutionary search is used to discover the general form of the function, and\ngradient descent to optimize its parameters for different parts of the network\nand over the learning process. Experiments with four different neural network\narchitectures on the CIFAR-10 and CIFAR-100 image classification datasets show\nthat this approach is effective. It discovers both general activation functions\nand specialized functions for different architectures, consistently improving\naccuracy over ReLU and other activation functions by significant margins. The\napproach can therefore be used as an automated optimization step in applying\ndeep learning to new tasks.\n",
        "published": "2020",
        "authors": [
            "Garrett Bingham",
            "Risto Miikkulainen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.06676v2",
        "title": "Training Generative Adversarial Networks with Limited Data",
        "abstract": "  Training generative adversarial networks (GAN) using too little data\ntypically leads to discriminator overfitting, causing training to diverge. We\npropose an adaptive discriminator augmentation mechanism that significantly\nstabilizes training in limited data regimes. The approach does not require\nchanges to loss functions or network architectures, and is applicable both when\ntraining from scratch and when fine-tuning an existing GAN on another dataset.\nWe demonstrate, on several datasets, that good results are now possible using\nonly a few thousand training images, often matching StyleGAN2 results with an\norder of magnitude fewer images. We expect this to open up new application\ndomains for GANs. We also find that the widely used CIFAR-10 is, in fact, a\nlimited data benchmark, and improve the record FID from 5.59 to 2.42.\n",
        "published": "2020",
        "authors": [
            "Tero Karras",
            "Miika Aittala",
            "Janne Hellsten",
            "Samuli Laine",
            "Jaakko Lehtinen",
            "Timo Aila"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.06880v4",
        "title": "Reintroducing Straight-Through Estimators as Principled Methods for\n  Stochastic Binary Networks",
        "abstract": "  Training neural networks with binary weights and activations is a challenging\nproblem due to the lack of gradients and difficulty of optimization over\ndiscrete weights. Many successful experimental results have been achieved with\nempirical straight-through (ST) approaches, proposing a variety of ad-hoc rules\nfor propagating gradients through non-differentiable activations and updating\ndiscrete weights. At the same time, ST methods can be truly derived as\nestimators in the stochastic binary network (SBN) model with Bernoulli weights.\nWe advance these derivations to a more complete and systematic study. We\nanalyze properties, estimation accuracy, obtain different forms of correct ST\nestimators for activations and weights, explain existing empirical approaches\nand their shortcomings, explain how latent weights arise from the mirror\ndescent method when optimizing over probabilities. This allows to reintroduce\nST methods, long known empirically, as sound approximations, apply them with\nclarity and develop further improvements.\n",
        "published": "2020",
        "authors": [
            "Alexander Shekhovtsov",
            "Viktor Yanush"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.08679v5",
        "title": "Feature Space Saturation during Training",
        "abstract": "  We propose layer saturation - a simple, online-computable method for\nanalyzing the information processing in neural networks. First, we show that a\nlayer's output can be restricted to the eigenspace of its variance matrix\nwithout performance loss. We propose a computationally lightweight method for\napproximating the variance matrix during training. From the dimension of its\nlossless eigenspace we derive layer saturation - the ratio between the\neigenspace dimension and layer width. We show that saturation seems to indicate\nwhich layers contribute to network performance. We demonstrate how to alter\nlayer saturation in a neural network by changing network depth, filter sizes\nand input resolution. Furthermore, we show that well-chosen input resolution\nincreases network performance by distributing the inference process more evenly\nacross the network.\n",
        "published": "2020",
        "authors": [
            "Mats L. Richter",
            "Justin Shenk",
            "Wolf Byttner",
            "Anders Arpteg",
            "Mikael Huss"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.08761v1",
        "title": "Towards Understanding the Effect of Leak in Spiking Neural Networks",
        "abstract": "  Spiking Neural Networks (SNNs) are being explored to emulate the astounding\ncapabilities of human brain that can learn and compute functions robustly and\nefficiently with noisy spiking activities. A variety of spiking neuron models\nhave been proposed to resemble biological neuronal functionalities. With\nvarying levels of bio-fidelity, these models often contain a leak path in their\ninternal states, called membrane potentials. While the leaky models have been\nargued as more bioplausible, a comparative analysis between models with and\nwithout leak from a purely computational point of view demands attention. In\nthis paper, we investigate the questions regarding the justification of leak\nand the pros and cons of using leaky behavior. Our experimental results reveal\nthat leaky neuron model provides improved robustness and better generalization\ncompared to models with no leak. However, leak decreases the sparsity of\ncomputation contrary to the common notion. Through a frequency domain analysis,\nwe demonstrate the effect of leak in eliminating the high-frequency components\nfrom the input, thus enabling SNNs to be more robust against noisy\nspike-inputs.\n",
        "published": "2020",
        "authors": [
            "Sayeed Shafayet Chowdhury",
            "Chankyu Lee",
            "Kaushik Roy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.09510v1",
        "title": "On sparse connectivity, adversarial robustness, and a novel model of the\n  artificial neuron",
        "abstract": "  Deep neural networks have achieved human-level accuracy on almost all\nperceptual benchmarks. It is interesting that these advances were made using\ntwo ideas that are decades old: (a) an artificial neuron based on a linear\nsummator and (b) SGD training.\n  However, there are important metrics beyond accuracy: computational\nefficiency and stability against adversarial perturbations. In this paper, we\npropose two closely connected methods to improve these metrics on contour\nrecognition tasks: (a) a novel model of an artificial neuron, a \"strong\nneuron,\" with low hardware requirements and inherent robustness against\nadversarial perturbations and (b) a novel constructive training algorithm that\ngenerates sparse networks with $O(1)$ connections per neuron.\n  We demonstrate the feasibility of our approach through experiments on SVHN\nand GTSRB benchmarks. We achieved an impressive 10x-100x reduction in\noperations count (10x when compared with other sparsification approaches, 100x\nwhen compared with dense networks) and a substantial reduction in hardware\nrequirements (8-bit fixed-point math was used) with no reduction in model\naccuracy. Superior stability against adversarial perturbations (exceeding that\nof adversarial training) was achieved without any counteradversarial measures,\nrelying on the robustness of strong neurons alone. We also proved that\nconstituent blocks of our strong neuron are the only activation functions with\nperfect stability against adversarial attacks.\n",
        "published": "2020",
        "authors": [
            "Sergey Bochkanov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.09989v6",
        "title": "Classifier-independent Lower-Bounds for Adversarial Robustness",
        "abstract": "  We theoretically analyse the limits of robustness to test-time adversarial\nand noisy examples in classification. Our work focuses on deriving bounds which\nuniformly apply to all classifiers (i.e all measurable functions from features\nto labels) for a given problem. Our contributions are two-fold. (1) We use\noptimal transport theory to derive variational formulae for the Bayes-optimal\nerror a classifier can make on a given classification problem, subject to\nadversarial attacks. The optimal adversarial attack is then an optimal\ntransport plan for a certain binary cost-function induced by the specific\nattack model, and can be computed via a simple algorithm based on maximal\nmatching on bipartite graphs. (2) We derive explicit lower-bounds on the\nBayes-optimal error in the case of the popular distance-based attacks. These\nbounds are universal in the sense that they depend on the geometry of the\nclass-conditional distributions of the data, but not on a particular\nclassifier. Our results are in sharp contrast with the existing literature,\nwherein adversarial vulnerability of classifiers is derived as a consequence of\nnonzero ordinary test error.\n",
        "published": "2020",
        "authors": [
            "Elvis Dohmatob"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.10848v3",
        "title": "Understanding Anomaly Detection with Deep Invertible Networks through\n  Hierarchies of Distributions and Features",
        "abstract": "  Deep generative networks trained via maximum likelihood on a natural image\ndataset like CIFAR10 often assign high likelihoods to images from datasets with\ndifferent objects (e.g., SVHN). We refine previous investigations of this\nfailure at anomaly detection for invertible generative networks and provide a\nclear explanation of it as a combination of model bias and domain prior:\nConvolutional networks learn similar low-level feature distributions when\ntrained on any natural image dataset and these low-level features dominate the\nlikelihood. Hence, when the discriminative features between inliers and\noutliers are on a high-level, e.g., object shapes, anomaly detection becomes\nparticularly challenging. To remove the negative impact of model bias and\ndomain prior on detecting high-level differences, we propose two methods,\nfirst, using the log likelihood ratios of two identical models, one trained on\nthe in-distribution data (e.g., CIFAR10) and the other one on a more general\ndistribution of images (e.g., 80 Million Tiny Images). We also derive a novel\noutlier loss for the in-distribution network on samples from the more general\ndistribution to further improve the performance. Secondly, using a multi-scale\nmodel like Glow, we show that low-level features are mainly captured at early\nscales. Therefore, using only the likelihood contribution of the final scale\nperforms remarkably well for detecting high-level feature differences of the\nout-of-distribution and the in-distribution. This method is especially useful\nif one does not have access to a suitable general distribution. Overall, our\nmethods achieve strong anomaly detection performance in the unsupervised\nsetting, and only slightly underperform state-of-the-art classifier-based\nmethods in the supervised setting. Code can be found at\nhttps://github.com/boschresearch/hierarchical_anomaly_detection.\n",
        "published": "2020",
        "authors": [
            "Robin Tibor Schirrmeister",
            "Yuxuan Zhou",
            "Tonio Ball",
            "Dan Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.12486v3",
        "title": "Locally Masked Convolution for Autoregressive Models",
        "abstract": "  High-dimensional generative models have many applications including image\ncompression, multimedia generation, anomaly detection and data completion.\nState-of-the-art estimators for natural images are autoregressive, decomposing\nthe joint distribution over pixels into a product of conditionals parameterized\nby a deep neural network, e.g. a convolutional neural network such as the\nPixelCNN. However, PixelCNNs only model a single decomposition of the joint,\nand only a single generation order is efficient. For tasks such as image\ncompletion, these models are unable to use much of the observed context. To\ngenerate data in arbitrary orders, we introduce LMConv: a simple modification\nto the standard 2D convolution that allows arbitrary masks to be applied to the\nweights at each location in the image. Using LMConv, we learn an ensemble of\ndistribution estimators that share parameters but differ in generation order,\nachieving improved performance on whole-image density estimation (2.89 bpd on\nunconditional CIFAR10), as well as globally coherent image completions. Our\ncode is available at https://ajayjain.github.io/lmconv.\n",
        "published": "2020",
        "authors": [
            "Ajay Jain",
            "Pieter Abbeel",
            "Deepak Pathak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.00970v2",
        "title": "MPLP: Learning a Message Passing Learning Protocol",
        "abstract": "  We present a novel method for learning the weights of an artificial neural\nnetwork - a Message Passing Learning Protocol (MPLP). In MPLP, we abstract\nevery operations occurring in ANNs as independent agents. Each agent is\nresponsible for ingesting incoming multidimensional messages from other agents,\nupdating its internal state, and generating multidimensional messages to be\npassed on to neighbouring agents. We demonstrate the viability of MPLP as\nopposed to traditional gradient-based approaches on simple feed-forward neural\nnetworks, and present a framework capable of generalizing to non-traditional\nneural network architectures. MPLP is meta learned using end-to-end\ngradient-based meta-optimisation. We further discuss the observed properties of\nMPLP and hypothesize its applicability on various fields of deep learning.\n",
        "published": "2020",
        "authors": [
            "Ettore Randazzo",
            "Eyvind Niklasson",
            "Alexander Mordvintsev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.01356v1",
        "title": "Decoder-free Robustness Disentanglement without (Additional) Supervision",
        "abstract": "  Adversarial Training (AT) is proposed to alleviate the adversarial\nvulnerability of machine learning models by extracting only robust features\nfrom the input, which, however, inevitably leads to severe accuracy reduction\nas it discards the non-robust yet useful features. This motivates us to\npreserve both robust and non-robust features and separate them with\ndisentangled representation learning. Our proposed Adversarial Asymmetric\nTraining (AAT) algorithm can reliably disentangle robust and non-robust\nrepresentations without additional supervision on robustness. Empirical results\nshow our method does not only successfully preserve accuracy by combining two\nrepresentations, but also achieve much better disentanglement than previous\nwork.\n",
        "published": "2020",
        "authors": [
            "Yifei Wang",
            "Dan Peng",
            "Furui Liu",
            "Zhenguo Li",
            "Zhitang Chen",
            "Jiansheng Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.01388v2",
        "title": "Learn Faster and Forget Slower via Fast and Stable Task Adaptation",
        "abstract": "  Training Deep Neural Networks (DNNs) is still highly time-consuming and\ncompute-intensive. It has been shown that adapting a pretrained model may\nsignificantly accelerate this process. With a focus on classification, we show\nthat current fine-tuning techniques make the pretrained models catastrophically\nforget the transferred knowledge even before anything about the new task is\nlearned. Such rapid knowledge loss undermines the merits of transfer learning\nand may result in a much slower convergence rate compared to when the maximum\namount of knowledge is exploited. We investigate the source of this problem\nfrom different perspectives and to alleviate it, introduce Fast And Stable\nTask-adaptation (FAST), an easy to apply fine-tuning algorithm. The paper\nprovides a novel geometric perspective on how the loss landscape of source and\ntarget tasks are linked in different transfer learning strategies. We\nempirically show that compared to prevailing fine-tuning practices, FAST learns\nthe target task faster and forgets the source task slower.\n",
        "published": "2020",
        "authors": [
            "Farshid Varno",
            "Lucas May Petry",
            "Lisa Di Jorio",
            "Stan Matwin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.01419v2",
        "title": "Persistent Neurons",
        "abstract": "  Neural networks (NN)-based learning algorithms are strongly affected by the\nchoices of initialization and data distribution. Different optimization\nstrategies have been proposed for improving the learning trajectory and finding\na better optima. However, designing improved optimization strategies is a\ndifficult task under the conventional landscape view. Here, we propose\npersistent neurons, a trajectory-based strategy that optimizes the learning\ntask using information from previous converged solutions. More precisely, we\nutilize the end of trajectories and let the parameters explore new landscapes\nby penalizing the model from converging to the previous solutions under the\nsame initialization. Persistent neurons can be regarded as a stochastic\ngradient method with informed bias where individual updates are corrupted by\ndeterministic error terms. Specifically, we show that persistent neurons, under\ncertain data distribution, is able to converge to more optimal solutions while\ninitializations under popular framework find bad local minima. We further\ndemonstrate that persistent neurons helps improve the model's performance under\nboth good and poor initializations. We evaluate the full and partial persistent\nmodel and show it can be used to boost the performance on a range of NN\nstructures, such as AlexNet and residual neural network (ResNet).\n",
        "published": "2020",
        "authors": [
            "Yimeng Min"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.01807v2",
        "title": "Continuously Indexed Domain Adaptation",
        "abstract": "  Existing domain adaptation focuses on transferring knowledge between domains\nwith categorical indices (e.g., between datasets A and B). However, many tasks\ninvolve continuously indexed domains. For example, in medical applications, one\noften needs to transfer disease analysis and prediction across patients of\ndifferent ages, where age acts as a continuous domain index. Such tasks are\nchallenging for prior domain adaptation methods since they ignore the\nunderlying relation among domains. In this paper, we propose the first method\nfor continuously indexed domain adaptation. Our approach combines traditional\nadversarial adaptation with a novel discriminator that models the\nencoding-conditioned domain index distribution. Our theoretical analysis\ndemonstrates the value of leveraging the domain index to generate invariant\nfeatures across a continuous range of domains. Our empirical results show that\nour approach outperforms the state-of-the-art domain adaption methods on both\nsynthetic and real-world medical datasets.\n",
        "published": "2020",
        "authors": [
            "Hao Wang",
            "Hao He",
            "Dina Katabi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.05123v1",
        "title": "Improving Adversarial Robustness by Enforcing Local and Global\n  Compactness",
        "abstract": "  The fact that deep neural networks are susceptible to crafted perturbations\nseverely impacts the use of deep learning in certain domains of application.\nAmong many developed defense models against such attacks, adversarial training\nemerges as the most successful method that consistently resists a wide range of\nattacks. In this work, based on an observation from a previous study that the\nrepresentations of a clean data example and its adversarial examples become\nmore divergent in higher layers of a deep neural net, we propose the Adversary\nDivergence Reduction Network which enforces local/global compactness and the\nclustering assumption over an intermediate layer of a deep neural network. We\nconduct comprehensive experiments to understand the isolating behavior of each\ncomponent (i.e., local/global compactness and the clustering assumption) and\ncompare our proposed model with state-of-the-art adversarial training methods.\nThe experimental results demonstrate that augmenting adversarial training with\nour proposed components can further improve the robustness of the network,\nleading to higher unperturbed and adversarial predictive performances.\n",
        "published": "2020",
        "authors": [
            "Anh Bui",
            "Trung Le",
            "He Zhao",
            "Paul Montague",
            "Olivier deVel",
            "Tamas Abraham",
            "Dinh Phung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.09200v2",
        "title": "Neural Networks with Recurrent Generative Feedback",
        "abstract": "  Neural networks are vulnerable to input perturbations such as additive noise\nand adversarial attacks. In contrast, human perception is much more robust to\nsuch perturbations. The Bayesian brain hypothesis states that human brains use\nan internal generative model to update the posterior beliefs of the sensory\ninput. This mechanism can be interpreted as a form of self-consistency between\nthe maximum a posteriori (MAP) estimation of an internal generative model and\nthe external environment. Inspired by such hypothesis, we enforce\nself-consistency in neural networks by incorporating generative recurrent\nfeedback. We instantiate this design on convolutional neural networks (CNNs).\nThe proposed framework, termed Convolutional Neural Networks with Feedback\n(CNN-F), introduces a generative feedback with latent variables to existing CNN\narchitectures, where consistent predictions are made through alternating MAP\ninference under a Bayesian framework. In the experiments, CNN-F shows\nconsiderably improved adversarial robustness over conventional feedforward CNNs\non standard benchmarks.\n",
        "published": "2020",
        "authors": [
            "Yujia Huang",
            "James Gornet",
            "Sihui Dai",
            "Zhiding Yu",
            "Tan Nguyen",
            "Doris Y. Tsao",
            "Anima Anandkumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.10143v1",
        "title": "Contextualizing Enhances Gradient Based Meta Learning",
        "abstract": "  Meta learning methods have found success when applied to few shot\nclassification problems, in which they quickly adapt to a small number of\nlabeled examples. Prototypical representations, each representing a particular\nclass, have been of particular importance in this setting, as they provide a\ncompact form to convey information learned from the labeled examples. However,\nthese prototypes are just one method of representing this information, and they\nare narrow in their scope and ability to classify unseen examples. We propose\nthe implementation of contextualizers, which are generalizable prototypes that\nadapt to given examples and play a larger role in classification for\ngradient-based models. We demonstrate how to equip meta learning methods with\ncontextualizers and show that their use can significantly boost performance on\na range of few shot learning datasets. We also present figures of merit\ndemonstrating the potential benefits of contextualizers, along with analysis of\nhow models make use of them. Our approach is particularly apt for low-data\nenvironments where it is difficult to update parameters without overfitting.\nOur implementation and instructions to reproduce the experiments are available\nat https://github.com/naveace/proto-context.\n",
        "published": "2020",
        "authors": [
            "Evan Vogelbaum",
            "Rumen Dangovski",
            "Li Jing",
            "Marin Solja\u010di\u0107"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.12499v1",
        "title": "Adma: A Flexible Loss Function for Neural Networks",
        "abstract": "  Highly increased interest in Artificial Neural Networks (ANNs) have resulted\nin impressively wide-ranging improvements in its structure. In this work, we\ncome up with the idea that instead of static plugins that the currently\navailable loss functions are, they should by default be flexible in nature. A\nflexible loss function can be a more insightful navigator for neural networks\nleading to higher convergence rates and therefore reaching the optimum accuracy\nmore quickly. The insights to help decide the degree of flexibility can be\nderived from the complexity of ANNs, the data distribution, selection of\nhyper-parameters and so on. In the wake of this, we introduce a novel flexible\nloss function for neural networks. The function is shown to characterize a\nrange of fundamentally unique properties from which, much of the properties of\nother loss functions are only a subset and varying the flexibility parameter in\nthe function allows it to emulate the loss curves and the learning behavior of\nprevalent static loss functions. The extensive experimentation performed with\nthe loss function demonstrates that it is able to give state-of-the-art\nperformance on selected data sets. Thus, in all the idea of flexibility itself\nand the proposed function built upon it carry the potential to open to a new\ninteresting chapter in deep learning research.\n",
        "published": "2020",
        "authors": [
            "Aditya Shrivastava"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.16149v1",
        "title": "HMCNAS: Neural Architecture Search using Hidden Markov Chains and\n  Bayesian Optimization",
        "abstract": "  Neural Architecture Search has achieved state-of-the-art performance in a\nvariety of tasks, out-performing human-designed networks. However, many\nassumptions, that require human definition, related with the problems being\nsolved or the models generated are still needed: final model architectures,\nnumber of layers to be sampled, forced operations, small search spaces, which\nultimately contributes to having models with higher performances at the cost of\ninducing bias into the system. In this paper, we propose HMCNAS, which is\ncomposed of two novel components: i) a method that leverages information about\nhuman-designed models to autonomously generate a complex search space, and ii)\nan Evolutionary Algorithm with Bayesian Optimization that is capable of\ngenerating competitive CNNs from scratch, without relying on human-defined\nparameters or small search spaces. The experimental results show that the\nproposed approach results in competitive architectures obtained in a very short\ntime. HMCNAS provides a step towards generalizing NAS, by providing a way to\ncreate competitive models, without requiring any human knowledge about the\nspecific task.\n",
        "published": "2020",
        "authors": [
            "Vasco Lopes",
            "Lu\u00eds A. Alexandre"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.06099v1",
        "title": "Similarity Based Stratified Splitting: an approach to train better\n  classifiers",
        "abstract": "  We propose a Similarity-Based Stratified Splitting (SBSS) technique, which\nuses both the output and input space information to split the data. The splits\nare generated using similarity functions among samples to place similar samples\nin different splits. This approach allows for a better representation of the\ndata in the training phase. This strategy leads to a more realistic performance\nestimation when used in real-world applications. We evaluate our proposal in\ntwenty-two benchmark datasets with classifiers such as Multi-Layer Perceptron,\nSupport Vector Machine, Random Forest and K-Nearest Neighbors, and five\nsimilarity functions Cityblock, Chebyshev, Cosine, Correlation, and Euclidean.\nAccording to the Wilcoxon Sign-Rank test, our approach consistently\noutperformed ordinary stratified 10-fold cross-validation in 75\\% of the\nassessed scenarios.\n",
        "published": "2020",
        "authors": [
            "Felipe Farias",
            "Teresa Ludermir",
            "Carmelo Bastos-Filho"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.00245v1",
        "title": "The Bayesian Method of Tensor Networks",
        "abstract": "  Bayesian learning is a powerful learning framework which combines the\nexternal information of the data (background information) with the internal\ninformation (training data) in a logically consistent way in inference and\nprediction. By Bayes rule, the external information (prior distribution) and\nthe internal information (training data likelihood) are combined coherently,\nand the posterior distribution and the posterior predictive (marginal)\ndistribution obtained by Bayes rule summarize the total information needed in\nthe inference and prediction, respectively. In this paper, we study the\nBayesian framework of the Tensor Network from two perspective. First, we\nintroduce the prior distribution to the weights in the Tensor Network and\npredict the labels of the new observations by the posterior predictive\n(marginal) distribution. Since the intractability of the parameter integral in\nthe normalization constant computation, we approximate the posterior predictive\ndistribution by Laplace approximation and obtain the out-product approximation\nof the hessian matrix of the posterior distribution of the Tensor Network\nmodel. Second, to estimate the parameters of the stationary mode, we propose a\nstable initialization trick to accelerate the inference process by which the\nTensor Network can converge to the stationary path more efficiently and stably\nwith gradient descent method. We verify our work on the MNIST, Phishing Website\nand Breast Cancer data set. We study the Bayesian properties of the Bayesian\nTensor Network by visualizing the parameters of the model and the decision\nboundaries in the two dimensional synthetic data set. For a application\npurpose, our work can reduce the overfitting and improve the performance of\nnormal Tensor Network model.\n",
        "published": "2021",
        "authors": [
            "Erdong Guo",
            "David Draper"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.00972v2",
        "title": "Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet",
        "abstract": "  Video sequences contain rich dynamic patterns, such as dynamic texture\npatterns that exhibit stationarity in the temporal domain, and action patterns\nthat are non-stationary in either spatial or temporal domain. We show that a\nspatial-temporal generative ConvNet can be used to model and synthesize dynamic\npatterns. The model defines a probability distribution on the video sequence,\nand the log probability is defined by a spatial-temporal ConvNet that consists\nof multiple layers of spatial-temporal filters to capture spatial-temporal\npatterns of different scales. The model can be learned from the training video\nsequences by an \"analysis by synthesis\" learning algorithm that iterates the\nfollowing two steps. Step 1 synthesizes video sequences from the currently\nlearned model. Step 2 then updates the model parameters based on the difference\nbetween the synthesized video sequences and the observed training sequences. We\nshow that the learning algorithm can synthesize realistic dynamic patterns.\n",
        "published": "2016",
        "authors": [
            "Jianwen Xie",
            "Song-Chun Zhu",
            "Ying Nian Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.04393v3",
        "title": "Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural\n  Networks",
        "abstract": "  Taking inspiration from biological evolution, we explore the idea of \"Can\ndeep neural networks evolve naturally over successive generations into highly\nefficient deep neural networks?\" by introducing the notion of synthesizing new\nhighly efficient, yet powerful deep neural networks over successive generations\nvia an evolutionary process from ancestor deep neural networks. The\narchitectural traits of ancestor deep neural networks are encoded using\nsynaptic probability models, which can be viewed as the `DNA' of these\nnetworks. New descendant networks with differing network architectures are\nsynthesized based on these synaptic probability models from the ancestor\nnetworks and computational environmental factor models, in a random manner to\nmimic heredity, natural selection, and random mutation. These offspring\nnetworks are then trained into fully functional networks, like one would train\na newborn, and have more efficient, more diverse network architectures than\ntheir ancestor networks, while achieving powerful modeling capabilities.\nExperimental results for the task of visual saliency demonstrated that the\nsynthesized `evolved' offspring networks can achieve state-of-the-art\nperformance while having network architectures that are significantly more\nefficient (with a staggering $\\sim$48-fold decrease in synapses by the fourth\ngeneration) compared to the original ancestor network.\n",
        "published": "2016",
        "authors": [
            "Mohammad Javad Shafiee",
            "Akshaya Mishra",
            "Alexander Wong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.08571v4",
        "title": "Alternating Back-Propagation for Generator Network",
        "abstract": "  This paper proposes an alternating back-propagation algorithm for learning\nthe generator network model. The model is a non-linear generalization of factor\nanalysis. In this model, the mapping from the continuous latent factors to the\nobserved signal is parametrized by a convolutional neural network. The\nalternating back-propagation algorithm iterates the following two steps: (1)\nInferential back-propagation, which infers the latent factors by Langevin\ndynamics or gradient descent. (2) Learning back-propagation, which updates the\nparameters given the inferred latent factors by gradient descent. The gradient\ncomputations in both steps are powered by back-propagation, and they share most\nof their code in common. We show that the alternating back-propagation\nalgorithm can learn realistic generator models of natural images, video\nsequences, and sounds. Moreover, it can also be used to learn from incomplete\nor indirect training data.\n",
        "published": "2016",
        "authors": [
            "Tian Han",
            "Yang Lu",
            "Song-Chun Zhu",
            "Ying Nian Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1612.04052v1",
        "title": "Theory and Tools for the Conversion of Analog to Spiking Convolutional\n  Neural Networks",
        "abstract": "  Deep convolutional neural networks (CNNs) have shown great potential for\nnumerous real-world machine learning applications, but performing inference in\nlarge CNNs in real-time remains a challenge. We have previously demonstrated\nthat traditional CNNs can be converted into deep spiking neural networks\n(SNNs), which exhibit similar accuracy while reducing both latency and\ncomputational load as a consequence of their data-driven, event-based style of\ncomputing. Here we provide a novel theory that explains why this conversion is\nsuccessful, and derive from it several new tools to convert a larger and more\npowerful class of deep networks into SNNs. We identify the main sources of\napproximation errors in previous conversion methods, and propose simple\nmechanisms to fix these issues. Furthermore, we develop spiking implementations\nof common CNN operations such as max-pooling, softmax, and batch-normalization,\nwhich allow almost loss-less conversion of arbitrary CNN architectures into the\nspiking domain. Empirical evaluation of different network architectures on the\nMNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.\n",
        "published": "2016",
        "authors": [
            "Bodo Rueckauer",
            "Iulia-Alexandra Lungu",
            "Yuhuang Hu",
            "Michael Pfeiffer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1612.04357v4",
        "title": "Stacked Generative Adversarial Networks",
        "abstract": "  In this paper, we propose a novel generative model named Stacked Generative\nAdversarial Networks (SGAN), which is trained to invert the hierarchical\nrepresentations of a bottom-up discriminative network. Our model consists of a\ntop-down stack of GANs, each learned to generate lower-level representations\nconditioned on higher-level representations. A representation discriminator is\nintroduced at each feature hierarchy to encourage the representation manifold\nof the generator to align with that of the bottom-up discriminative network,\nleveraging the powerful discriminative representations to guide the generative\nmodel. In addition, we introduce a conditional loss that encourages the use of\nconditional information from the layer above, and a novel entropy loss that\nmaximizes a variational lower bound on the conditional entropy of generator\noutputs. We first train each stack independently, and then train the whole\nmodel end-to-end. Unlike the original GAN that uses a single noise vector to\nrepresent all the variations, our SGAN decomposes variations into multiple\nlevels and gradually resolves uncertainties in the top-down generative process.\nBased on visual inspection, Inception scores and visual Turing test, we\ndemonstrate that SGAN is able to generate images of much higher quality than\nGANs without stacking.\n",
        "published": "2016",
        "authors": [
            "Xun Huang",
            "Yixuan Li",
            "Omid Poursaeed",
            "John Hopcroft",
            "Serge Belongie"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1708.02735v1",
        "title": "Gaussian Prototypical Networks for Few-Shot Learning on Omniglot",
        "abstract": "  We propose a novel architecture for $k$-shot classification on the Omniglot\ndataset. Building on prototypical networks, we extend their architecture to\nwhat we call Gaussian prototypical networks. Prototypical networks learn a map\nbetween images and embedding vectors, and use their clustering for\nclassification. In our model, a part of the encoder output is interpreted as a\nconfidence region estimate about the embedding point, and expressed as a\nGaussian covariance matrix. Our network then constructs a direction and class\ndependent distance metric on the embedding space, using uncertainties of\nindividual data points as weights. We show that Gaussian prototypical networks\nare a preferred architecture over vanilla prototypical networks with an\nequivalent number of parameters. We report state-of-the-art performance in\n1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot\n5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset.\nWe explore artificially down-sampling a fraction of images in the training set,\nwhich improves our performance even further. We therefore hypothesize that\nGaussian prototypical networks might perform better in less homogeneous,\nnoisier datasets, which are commonplace in real world applications.\n",
        "published": "2017",
        "authors": [
            "Stanislav Fort"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1708.07120v3",
        "title": "Super-Convergence: Very Fast Training of Neural Networks Using Large\n  Learning Rates",
        "abstract": "  In this paper, we describe a phenomenon, which we named \"super-convergence\",\nwhere neural networks can be trained an order of magnitude faster than with\nstandard training methods. The existence of super-convergence is relevant to\nunderstanding why deep networks generalize well. One of the key elements of\nsuper-convergence is training with one learning rate cycle and a large maximum\nlearning rate. A primary insight that allows super-convergence training is that\nlarge learning rates regularize the training, hence requiring a reduction of\nall other forms of regularization in order to preserve an optimal\nregularization balance. We also derive a simplification of the Hessian Free\noptimization method to compute an estimate of the optimal learning rate.\nExperiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet\ndatasets, and resnet, wide-resnet, densenet, and inception architectures. In\naddition, we show that super-convergence provides a greater boost in\nperformance relative to standard training when the amount of labeled training\ndata is limited. The architectures and code to replicate the figures in this\npaper are available at github.com/lnsmith54/super-convergence. See\nhttp://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of\nsuper-convergence to win the DAWNBench challenge (see\nhttps://dawn.cs.stanford.edu/benchmark/).\n",
        "published": "2017",
        "authors": [
            "Leslie N. Smith",
            "Nicholay Topin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.08177v1",
        "title": "Progressive Learning for Systematic Design of Large Neural Networks",
        "abstract": "  We develop an algorithm for systematic design of a large artificial neural\nnetwork using a progression property. We find that some non-linear functions,\nsuch as the rectifier linear unit and its derivatives, hold the property. The\nsystematic design addresses the choice of network size and regularization of\nparameters. The number of nodes and layers in network increases in progression\nwith the objective of consistently reducing an appropriate cost. Each layer is\noptimized at a time, where appropriate parameters are learned using convex\noptimization. Regularization parameters for convex optimization do not need a\nsignificant manual effort for tuning. We also use random instances for some\nweight matrices, and that helps to reduce the number of parameters we learn.\nThe developed network is expected to show good generalization power due to\nappropriate regularization and use of random weights in the layers. This\nexpectation is verified by extensive experiments for classification and\nregression problems, using standard databases.\n",
        "published": "2017",
        "authors": [
            "Saikat Chatterjee",
            "Alireza M. Javid",
            "Mostafa Sadeghi",
            "Partha P. Mitra",
            "Mikael Skoglund"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.01653v2",
        "title": "Review of Deep Learning",
        "abstract": "  In recent years, China, the United States and other countries, Google and\nother high-tech companies have increased investment in artificial intelligence.\nDeep learning is one of the current artificial intelligence research's key\nareas. This paper analyzes and summarizes the latest progress and future\nresearch directions of deep learning. Firstly, three basic models of deep\nlearning are outlined, including multilayer perceptrons, convolutional neural\nnetworks, and recurrent neural networks. On this basis, we further analyze the\nemerging new models of convolution neural networks and recurrent neural\nnetworks. This paper then summarizes deep learning's applications in many areas\nof artificial intelligence, including speech processing, computer vision,\nnatural language processing and so on. Finally, this paper discusses the\nexisting problems of deep learning and gives the corresponding possible\nsolutions.\n",
        "published": "2018",
        "authors": [
            "Rong Zhang",
            "Weiping Li",
            "Tong Mo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.09859v1",
        "title": "Competitive Learning Enriches Learning Representation and Accelerates\n  the Fine-tuning of CNNs",
        "abstract": "  In this study, we propose the integration of competitive learning into\nconvolutional neural networks (CNNs) to improve the representation learning and\nefficiency of fine-tuning. Conventional CNNs use back propagation learning, and\nit enables powerful representation learning by a discrimination task. However,\nit requires huge amount of labeled data, and acquisition of labeled data is\nmuch harder than that of unlabeled data. Thus, efficient use of unlabeled data\nis getting crucial for DNNs. To address the problem, we introduce unsupervised\ncompetitive learning into the convolutional layer, and utilize unlabeled data\nfor effective representation learning. The results of validation experiments\nusing a toy model demonstrated that strong representation learning effectively\nextracted bases of images into convolutional filters using unlabeled data, and\naccelerated the speed of the fine-tuning of subsequent supervised back\npropagation learning. The leverage was more apparent when the number of filters\nwas sufficiently large, and, in such a case, the error rate steeply decreased\nin the initial phase of fine-tuning. Thus, the proposed method enlarged the\nnumber of filters in CNNs, and enabled a more detailed and generalized\nrepresentation. It could provide a possibility of not only deep but broad\nneural networks.\n",
        "published": "2018",
        "authors": [
            "Takashi Shinozaki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.08340v3",
        "title": "Reducing Parameter Space for Neural Network Training",
        "abstract": "  For neural networks (NNs) with rectified linear unit (ReLU) or binary\nactivation functions, we show that their training can be accomplished in a\nreduced parameter space. Specifically, the weights in each neuron can be\ntrained on the unit sphere, as opposed to the entire space, and the threshold\ncan be trained in a bounded interval, as opposed to the real line. We show that\nthe NNs in the reduced parameter space are mathematically equivalent to the\nstandard NNs with parameters in the whole space. The reduced parameter space\nshall facilitate the optimization procedure for the network training, as the\nsearch space becomes (much) smaller. We demonstrate the improved training\nperformance using numerical examples.\n",
        "published": "2018",
        "authors": [
            "Tong Qin",
            "Ling Zhou",
            "Dongbin Xiu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.08709v2",
        "title": "A Simple Cache Model for Image Recognition",
        "abstract": "  Training large-scale image recognition models is computationally expensive.\nThis raises the question of whether there might be simple ways to improve the\ntest performance of an already trained model without having to re-train or\nfine-tune it with new data. Here, we show that, surprisingly, this is indeed\npossible. The key observation we make is that the layers of a deep network\nclose to the output layer contain independent, easily extractable\nclass-relevant information that is not contained in the output layer itself. We\npropose to extract this extra class-relevant information using a simple\nkey-value cache memory to improve the classification performance of the model\nat test time. Our cache memory is directly inspired by a similar cache model\npreviously proposed for language modeling (Grave et al., 2017). This cache\ncomponent does not require any training or fine-tuning; it can be applied to\nany pre-trained model and, by properly setting only two hyper-parameters, leads\nto significant improvements in its classification performance. Improvements are\nobserved across several architectures and datasets. In the cache component,\nusing features extracted from layers close to the output (but not from the\noutput layer itself) as keys leads to the largest improvements. Concatenating\nfeatures from multiple layers to form keys can further improve performance over\nusing single-layer features as keys. The cache component also has a\nregularizing effect, a simple consequence of which is that it substantially\nincreases the robustness of models against adversarial attacks.\n",
        "published": "2018",
        "authors": [
            "A. Emin Orhan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.10002v5",
        "title": "Learning to Propagate Labels: Transductive Propagation Network for\n  Few-shot Learning",
        "abstract": "  The goal of few-shot learning is to learn a classifier that generalizes well\neven when trained with a limited number of training instances per class. The\nrecently introduced meta-learning approaches tackle this problem by learning a\ngeneric classifier across a large number of multiclass classification tasks and\ngeneralizing the model to a new task. Yet, even with such meta-learning, the\nlow-data problem in the novel classification task still remains. In this paper,\nwe propose Transductive Propagation Network (TPN), a novel meta-learning\nframework for transductive inference that classifies the entire test set at\nonce to alleviate the low-data problem. Specifically, we propose to learn to\npropagate labels from labeled instances to unlabeled test instances, by\nlearning a graph construction module that exploits the manifold structure in\nthe data. TPN jointly learns both the parameters of feature embedding and the\ngraph construction in an end-to-end manner. We validate TPN on multiple\nbenchmark datasets, on which it largely outperforms existing few-shot learning\napproaches and achieves the state-of-the-art results.\n",
        "published": "2018",
        "authors": [
            "Yanbin Liu",
            "Juho Lee",
            "Minseop Park",
            "Saehoon Kim",
            "Eunho Yang",
            "Sung Ju Hwang",
            "Yi Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.12024v1",
        "title": "Privacy Aware Offloading of Deep Neural Networks",
        "abstract": "  Deep neural networks require large amounts of resources which makes them hard\nto use on resource constrained devices such as Internet-of-things devices.\nOffloading the computations to the cloud can circumvent these constraints but\nintroduces a privacy risk since the operator of the cloud is not necessarily\ntrustworthy. We propose a technique that obfuscates the data before sending it\nto the remote computation node. The obfuscated data is unintelligible for a\nhuman eavesdropper but can still be classified with a high accuracy by a neural\nnetwork trained on unobfuscated images.\n",
        "published": "2018",
        "authors": [
            "Sam Leroux",
            "Tim Verbelen",
            "Pieter Simoens",
            "Bart Dhoedt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.12152v5",
        "title": "Robustness May Be at Odds with Accuracy",
        "abstract": "  We show that there may exist an inherent tension between the goal of\nadversarial robustness and that of standard generalization. Specifically,\ntraining robust models may not only be more resource-consuming, but also lead\nto a reduction of standard accuracy. We demonstrate that this trade-off between\nthe standard accuracy of a model and its robustness to adversarial\nperturbations provably exists in a fairly simple and natural setting. These\nfindings also corroborate a similar phenomenon observed empirically in more\ncomplex settings. Further, we argue that this phenomenon is a consequence of\nrobust classifiers learning fundamentally different feature representations\nthan standard classifiers. These differences, in particular, seem to result in\nunexpected benefits: the representations learned by robust models tend to align\nbetter with salient data characteristics and human perception.\n",
        "published": "2018",
        "authors": [
            "Dimitris Tsipras",
            "Shibani Santurkar",
            "Logan Engstrom",
            "Alexander Turner",
            "Aleksander Madry"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.00783v1",
        "title": "The Quest for the Golden Activation Function",
        "abstract": "  Deep Neural Networks have been shown to be beneficial for a variety of tasks,\nin particular allowing for end-to-end learning and reducing the requirement for\nmanual design decisions. However, still many parameters have to be chosen in\nadvance, also raising the need to optimize them. One important, but often\nignored system parameter is the selection of a proper activation function.\nThus, in this paper we target to demonstrate the importance of activation\nfunctions in general and show that for different tasks different activation\nfunctions might be meaningful. To avoid the manual design or selection of\nactivation functions, we build on the idea of genetic algorithms to learn the\nbest activation function for a given task. In addition, we introduce two new\nactivation functions, ELiSH and HardELiSH, which can easily be incorporated in\nour framework. In this way, we demonstrate for three different image\nclassification benchmarks that different activation functions are learned, also\nshowing improved results compared to typically used baselines.\n",
        "published": "2018",
        "authors": [
            "Mina Basirat",
            "Peter M. Roth"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.07378v2",
        "title": "Progressive Weight Pruning of Deep Neural Networks using ADMM",
        "abstract": "  Deep neural networks (DNNs) although achieving human-level performance in\nmany domains, have very large model size that hinders their broader\napplications on edge computing devices. Extensive research work have been\nconducted on DNN model compression or pruning. However, most of the previous\nwork took heuristic approaches. This work proposes a progressive weight pruning\napproach based on ADMM (Alternating Direction Method of Multipliers), a\npowerful technique to deal with non-convex optimization problems with\npotentially combinatorial constraints. Motivated by dynamic programming, the\nproposed method reaches extremely high pruning rate by using partial prunings\nwith moderate pruning rates. Therefore, it resolves the accuracy degradation\nand long convergence time problems when pursuing extremely high pruning ratios.\nIt achieves up to 34 times pruning rate for ImageNet dataset and 167 times\npruning rate for MNIST dataset, significantly higher than those reached by the\nliterature work. Under the same number of epochs, the proposed method also\nachieves faster convergence and higher compression rates. The codes and pruned\nDNN models are released in the link bit.ly/2zxdlss\n",
        "published": "2018",
        "authors": [
            "Shaokai Ye",
            "Tianyun Zhang",
            "Kaiqi Zhang",
            "Jiayu Li",
            "Kaidi Xu",
            "Yunfei Yang",
            "Fuxun Yu",
            "Jian Tang",
            "Makan Fardad",
            "Sijia Liu",
            "Xiang Chen",
            "Xue Lin",
            "Yanzhi Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.10338v2",
        "title": "Strategies for Training Stain Invariant CNNs",
        "abstract": "  An important part of Digital Pathology is the analysis of multiple digitised\nwhole slide images from differently stained tissue sections. It is common\npractice to mount consecutive sections containing corresponding microscopic\nstructures on glass slides, and to stain them differently to highlight specific\ntissue components. These multiple staining modalities result in very different\nimages but include a significant amount of consistent image information. Deep\nlearning approaches have recently been proposed to analyse these images in\norder to automatically identify objects of interest for pathologists. These\nsupervised approaches require a vast amount of annotations, which are difficult\nand expensive to acquire---a problem that is multiplied with multiple\nstainings. This article presents several training strategies that make progress\ntowards stain invariant networks. By training the network on one commonly used\nstaining modality and applying it to images that include corresponding but\ndifferently stained tissue structures, the presented unsupervised strategies\ndemonstrate significant improvements over standard training strategies.\n",
        "published": "2018",
        "authors": [
            "Thomas Lampert",
            "Odyss\u00e9e Merveille",
            "Jessica Schmitz",
            "Germain Forestier",
            "Friedrich Feuerhake",
            "C\u00e9dric Wemmert"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.11955v2",
        "title": "On Exact Computation with an Infinitely Wide Neural Net",
        "abstract": "  How well does a classic deep net architecture like AlexNet or VGG19 classify\non a standard dataset such as CIFAR-10 when its width --- namely, number of\nchannels in convolutional layers, and number of nodes in fully-connected\ninternal layers --- is allowed to increase to infinity? Such questions have\ncome to the forefront in the quest to theoretically understand deep learning\nand its mysteries about optimization and generalization. They also connect deep\nlearning to notions such as Gaussian processes and kernels. A recent paper\n[Jacot et al., 2018] introduced the Neural Tangent Kernel (NTK) which captures\nthe behavior of fully-connected deep nets in the infinite width limit trained\nby gradient descent; this object was implicit in some other recent papers. An\nattraction of such ideas is that a pure kernel-based method is used to capture\nthe power of a fully-trained deep net of infinite width.\n  The current paper gives the first efficient exact algorithm for computing the\nextension of NTK to convolutional neural nets, which we call Convolutional NTK\n(CNTK), as well as an efficient GPU implementation of this algorithm. This\nresults in a significant new benchmark for the performance of a pure\nkernel-based method on CIFAR-10, being $10\\%$ higher than the methods reported\nin [Novak et al., 2019], and only $6\\%$ lower than the performance of the\ncorresponding finite deep net architecture (once batch normalization, etc. are\nturned off). Theoretically, we also give the first non-asymptotic proof showing\nthat a fully-trained sufficiently wide net is indeed equivalent to the kernel\nregression predictor using NTK.\n",
        "published": "2019",
        "authors": [
            "Sanjeev Arora",
            "Simon S. Du",
            "Wei Hu",
            "Zhiyuan Li",
            "Ruslan Salakhutdinov",
            "Ruosong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.01392v2",
        "title": "A Survey on Neural Architecture Search",
        "abstract": "  The growing interest in both the automation of machine learning and deep\nlearning has inevitably led to the development of a wide variety of automated\nmethods for neural architecture search. The choice of the network architecture\nhas proven to be critical, and many advances in deep learning spring from its\nimmediate improvements. However, deep learning techniques are computationally\nintensive and their application requires a high level of domain knowledge.\nTherefore, even partial automation of this process helps to make deep learning\nmore accessible to both researchers and practitioners. With this survey, we\nprovide a formalism which unifies and categorizes the landscape of existing\nmethods along with a detailed analysis that compares and contrasts the\ndifferent approaches. We achieve this via a comprehensive discussion of the\ncommonly adopted architecture search spaces and architecture optimization\nalgorithms based on principles of reinforcement learning and evolutionary\nalgorithms along with approaches that incorporate surrogate and one-shot\nmodels. Additionally, we address the new research directions which include\nconstrained and multi-objective architecture search as well as automated data\naugmentation, optimizer and activation function search.\n",
        "published": "2019",
        "authors": [
            "Martin Wistuba",
            "Ambrish Rawat",
            "Tejaswini Pedapati"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.03046v1",
        "title": "PiNet: A Permutation Invariant Graph Neural Network for Graph\n  Classification",
        "abstract": "  We propose an end-to-end deep learning learning model for graph\nclassification and representation learning that is invariant to permutation of\nthe nodes of the input graphs. We address the challenge of learning a fixed\nsize graph representation for graphs of varying dimensions through a\ndifferentiable node attention pooling mechanism. In addition to a theoretical\nproof of its invariance to permutation, we provide empirical evidence\ndemonstrating the statistically significant gain in accuracy when faced with an\nisomorphic graph classification task given only a small number of training\nexamples. We analyse the effect of four different matrices to facilitate the\nlocal message passing mechanism by which graph convolutions are performed vs. a\nmatrix parametrised by a learned parameter pair able to transition smoothly\nbetween the former. Finally, we show that our model achieves competitive\nclassification performance with existing techniques on a set of molecule\ndatasets.\n",
        "published": "2019",
        "authors": [
            "Peter Meltzer",
            "Marcelo Daniel Gutierrez Mallea",
            "Peter J. Bentley"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.04042v2",
        "title": "Prototype Propagation Networks (PPN) for Weakly-supervised Few-shot\n  Learning on Category Graph",
        "abstract": "  A variety of machine learning applications expect to achieve rapid learning\nfrom a limited number of labeled data. However, the success of most current\nmodels is the result of heavy training on big data. Meta-learning addresses\nthis problem by extracting common knowledge across different tasks that can be\nquickly adapted to new tasks. However, they do not fully explore\nweakly-supervised information, which is usually free or cheap to collect. In\nthis paper, we show that weakly-labeled data can significantly improve the\nperformance of meta-learning on few-shot classification. We propose prototype\npropagation network (PPN) trained on few-shot tasks together with data\nannotated by coarse-label. Given a category graph of the targeted fine-classes\nand some weakly-labeled coarse-classes, PPN learns an attention mechanism which\npropagates the prototype of one class to another on the graph, so that the\nK-nearest neighbor (KNN) classifier defined on the propagated prototypes\nresults in high accuracy across different few-shot tasks. The training tasks\nare generated by subgraph sampling, and the training objective is obtained by\naccumulating the level-wise classification loss on the subgraph. The resulting\ngraph of prototypes can be continually re-used and updated for new tasks and\nclasses. We also introduce two practical test/inference settings which differ\naccording to whether the test task can leverage any weakly-supervised\ninformation as in training. On two benchmarks, PPN significantly outperforms\nmost recent few-shot learning methods in different settings, even when they are\nalso allowed to train on weakly-labeled data.\n",
        "published": "2019",
        "authors": [
            "Lu Liu",
            "Tianyi Zhou",
            "Guodong Long",
            "Jing Jiang",
            "Lina Yao",
            "Chengqi Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.07320v3",
        "title": "EENA: Efficient Evolution of Neural Architecture",
        "abstract": "  Latest algorithms for automatic neural architecture search perform remarkable\nbut are basically directionless in search space and computational expensive in\ntraining of every intermediate architecture. In this paper, we propose a method\nfor efficient architecture search called EENA (Efficient Evolution of Neural\nArchitecture). Due to the elaborately designed mutation and crossover\noperations, the evolution process can be guided by the information have already\nbeen learned. Therefore, less computational effort will be required while the\nsearching and training time can be reduced significantly. On CIFAR-10\nclassification, EENA using minimal computational resources (0.65 GPU-days) can\ndesign highly effective neural architecture which achieves 2.56% test error\nwith 8.47M parameters. Furthermore, the best architecture discovered is also\ntransferable for CIFAR-100.\n",
        "published": "2019",
        "authors": [
            "Hui Zhu",
            "Zhulin An",
            "Chuanguang Yang",
            "Kaiqiang Xu",
            "Erhu Zhao",
            "Yongjun Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.09788v3",
        "title": "Multi-Sample Dropout for Accelerated Training and Better Generalization",
        "abstract": "  Dropout is a simple but efficient regularization technique for achieving\nbetter generalization of deep neural networks (DNNs); hence it is widely used\nin tasks based on DNNs. During training, dropout randomly discards a portion of\nthe neurons to avoid overfitting. This paper presents an enhanced dropout\ntechnique, which we call multi-sample dropout, for both accelerating training\nand improving generalization over the original dropout. The original dropout\ncreates a randomly selected subset (called a dropout sample) from the input in\neach training iteration while the multi-sample dropout creates multiple dropout\nsamples. The loss is calculated for each sample, and then the sample losses are\naveraged to obtain the final loss. This technique can be easily implemented by\nduplicating a part of the network after the dropout layer while sharing the\nweights among the duplicated fully connected layers. Experimental results using\nimage classification tasks including ImageNet, CIFAR-10, and CIFAR-100 showed\nthat multi-sample dropout accelerates training. Moreover, the networks trained\nusing multi-sample dropout achieved lower error rates compared to networks\ntrained with the original dropout. The additional computation cost due to the\nduplicated operations is not significant for deep convolutional networks\nbecause most of the computation time is consumed in the convolution layers\nbefore the dropout layer, which are not duplicated.\n",
        "published": "2019",
        "authors": [
            "Hiroshi Inoue"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.11528v3",
        "title": "Improved Training Speed, Accuracy, and Data Utilization Through Loss\n  Function Optimization",
        "abstract": "  As the complexity of neural network models has grown, it has become\nincreasingly important to optimize their design automatically through\nmetalearning. Methods for discovering hyperparameters, topologies, and learning\nrate schedules have lead to significant increases in performance. This paper\nshows that loss functions can be optimized with metalearning as well, and\nresult in similar improvements. The method, Genetic Loss-function Optimization\n(GLO), discovers loss functions de novo, and optimizes them for a target task.\nLeveraging techniques from genetic programming, GLO builds loss functions\nhierarchically from a set of operators and leaf nodes. These functions are\nrepeatedly recombined and mutated to find an optimal structure, and then a\ncovariance-matrix adaptation evolutionary strategy (CMA-ES) is used to find\noptimal coefficients. Networks trained with GLO loss functions are found to\noutperform the standard cross-entropy loss on standard image classification\ntasks. Training with these new loss functions requires fewer steps, results in\nlower test error, and allows for smaller datasets to be used. Loss-function\noptimization thus provides a new dimension of metalearning, and constitutes an\nimportant step towards AutoML.\n",
        "published": "2019",
        "authors": [
            "Santiago Gonzalez",
            "Risto Miikkulainen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.11926v4",
        "title": "Network Deconvolution",
        "abstract": "  Convolution is a central operation in Convolutional Neural Networks (CNNs),\nwhich applies a kernel to overlapping regions shifted across the image.\nHowever, because of the strong correlations in real-world image data,\nconvolutional kernels are in effect re-learning redundant data. In this work,\nwe show that this redundancy has made neural network training challenging, and\npropose network deconvolution, a procedure which optimally removes pixel-wise\nand channel-wise correlations before the data is fed into each layer. Network\ndeconvolution can be efficiently calculated at a fraction of the computational\ncost of a convolution layer. We also show that the deconvolution filters in the\nfirst layer of the network resemble the center-surround structure found in\nbiological neurons in the visual regions of the brain. Filtering with such\nkernels results in a sparse representation, a desired property that has been\nmissing in the training of neural networks. Learning from the sparse\nrepresentation promotes faster convergence and superior results without the use\nof batch normalization. We apply our network deconvolution operation to 10\nmodern neural network models by replacing batch normalization within each.\nExtensive experiments show that the network deconvolution operation is able to\ndeliver performance improvement in all cases on the CIFAR-10, CIFAR-100, MNIST,\nFashion-MNIST, Cityscapes, and ImageNet datasets.\n",
        "published": "2019",
        "authors": [
            "Chengxi Ye",
            "Matthew Evanusa",
            "Hua He",
            "Anton Mitrokhin",
            "Tom Goldstein",
            "James A. Yorke",
            "Cornelia Ferm\u00fcller",
            "Yiannis Aloimonos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.12019v5",
        "title": "Unified Probabilistic Deep Continual Learning through Generative Replay\n  and Open Set Recognition",
        "abstract": "  Modern deep neural networks are well known to be brittle in the face of\nunknown data instances and recognition of the latter remains a challenge.\nAlthough it is inevitable for continual-learning systems to encounter such\nunseen concepts, the corresponding literature appears to nonetheless focus\nprimarily on alleviating catastrophic interference with learned\nrepresentations. In this work, we introduce a probabilistic approach that\nconnects these perspectives based on variational inference in a single deep\nautoencoder model. Specifically, we propose to bound the approximate posterior\nby fitting regions of high density on the basis of correctly classified data\npoints. These bounds are shown to serve a dual purpose: unseen unknown\nout-of-distribution data can be distinguished from already trained known tasks\ntowards robust application. Simultaneously, to retain already acquired\nknowledge, a generative replay process can be narrowed to strictly\nin-distribution samples, in order to significantly alleviate catastrophic\ninterference.\n",
        "published": "2019",
        "authors": [
            "Martin Mundt",
            "Iuliia Pliushch",
            "Sagnik Majumder",
            "Yongwon Hong",
            "Visvanathan Ramesh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.12506v3",
        "title": "Are Disentangled Representations Helpful for Abstract Visual Reasoning?",
        "abstract": "  A disentangled representation encodes information about the salient factors\nof variation in the data independently. Although it is often argued that this\nrepresentational format is useful in learning to solve many real-world\ndown-stream tasks, there is little empirical evidence that supports this claim.\nIn this paper, we conduct a large-scale study that investigates whether\ndisentangled representations are more suitable for abstract reasoning tasks.\nUsing two new tasks similar to Raven's Progressive Matrices, we evaluate the\nusefulness of the representations learned by 360 state-of-the-art unsupervised\ndisentanglement models. Based on these representations, we train 3600 abstract\nreasoning models and observe that disentangled representations do in fact lead\nto better down-stream performance. In particular, they enable quicker learning\nusing fewer samples.\n",
        "published": "2019",
        "authors": [
            "Sjoerd van Steenkiste",
            "Francesco Locatello",
            "J\u00fcrgen Schmidhuber",
            "Olivier Bachem"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.00945v2",
        "title": "Adversarial Robustness as a Prior for Learned Representations",
        "abstract": "  An important goal in deep learning is to learn versatile, high-level feature\nrepresentations of input data. However, standard networks' representations seem\nto possess shortcomings that, as we illustrate, prevent them from fully\nrealizing this goal. In this work, we show that robust optimization can be\nre-cast as a tool for enforcing priors on the features learned by deep neural\nnetworks. It turns out that representations learned by robust models address\nthe aforementioned shortcomings and make significant progress towards learning\na high-level encoding of inputs. In particular, these representations are\napproximately invertible, while allowing for direct visualization and\nmanipulation of salient input features. More broadly, our results indicate\nadversarial robustness as a promising avenue for improving learned\nrepresentations. Our code and models for reproducing these results is available\nat https://git.io/robust-reps .\n",
        "published": "2019",
        "authors": [
            "Logan Engstrom",
            "Andrew Ilyas",
            "Shibani Santurkar",
            "Dimitris Tsipras",
            "Brandon Tran",
            "Aleksander Madry"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.02909v5",
        "title": "AutoGrow: Automatic Layer Growing in Deep Convolutional Networks",
        "abstract": "  Depth is a key component of Deep Neural Networks (DNNs), however, designing\ndepth is heuristic and requires many human efforts. We propose AutoGrow to\nautomate depth discovery in DNNs: starting from a shallow seed architecture,\nAutoGrow grows new layers if the growth improves the accuracy; otherwise, stops\ngrowing and thus discovers the depth. We propose robust growing and stopping\npolicies to generalize to different network architectures and datasets. Our\nexperiments show that by applying the same policy to different network\narchitectures, AutoGrow can always discover near-optimal depth on various\ndatasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For\nexample, in terms of accuracy-computation trade-off, AutoGrow discovers a\nbetter depth combination in ResNets than human experts. Our AutoGrow is\nefficient. It discovers depth within similar time of training a single DNN. Our\ncode is available at https://github.com/wenwei202/autogrow.\n",
        "published": "2019",
        "authors": [
            "Wei Wen",
            "Feng Yan",
            "Yiran Chen",
            "Hai Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.06818v2",
        "title": "Stacked Capsule Autoencoders",
        "abstract": "  Objects are composed of a set of geometrically organized parts. We introduce\nan unsupervised capsule autoencoder (SCAE), which explicitly uses geometric\nrelationships between parts to reason about objects. Since these relationships\ndo not depend on the viewpoint, our model is robust to viewpoint changes. SCAE\nconsists of two stages. In the first stage, the model predicts presences and\nposes of part templates directly from the image and tries to reconstruct the\nimage by appropriately arranging the templates. In the second stage, SCAE\npredicts parameters of a few object capsules, which are then used to\nreconstruct part poses. Inference in this model is amortized and performed by\noff-the-shelf neural encoders, unlike in previous capsule networks. We find\nthat object capsule presences are highly informative of the object class, which\nleads to state-of-the-art results for unsupervised classification on SVHN (55%)\nand MNIST (98.7%). The code is available at\nhttps://github.com/google-research/google-research/tree/master/stacked_capsule_autoencoders\n",
        "published": "2019",
        "authors": [
            "Adam R. Kosiorek",
            "Sara Sabour",
            "Yee Whye Teh",
            "Geoffrey E. Hinton"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.08416v2",
        "title": "Improving the robustness of ImageNet classifiers using elements of human\n  visual cognition",
        "abstract": "  We investigate the robustness properties of image recognition models equipped\nwith two features inspired by human vision, an explicit episodic memory and a\nshape bias, at the ImageNet scale. As reported in previous work, we show that\nan explicit episodic memory improves the robustness of image recognition models\nagainst small-norm adversarial perturbations under some threat models. It does\nnot, however, improve the robustness against more natural, and typically\nlarger, perturbations. Learning more robust features during training appears to\nbe necessary for robustness in this second sense. We show that features derived\nfrom a model that was encouraged to learn global, shape-based representations\n(Geirhos et al., 2019) do not only improve the robustness against natural\nperturbations, but when used in conjunction with an episodic memory, they also\nprovide additional robustness against adversarial perturbations. Finally, we\naddress three important design choices for the episodic memory: memory size,\ndimensionality of the memories and the retrieval method. We show that to make\nthe episodic memory more compact, it is preferable to reduce the number of\nmemories by clustering them, instead of reducing their dimensionality.\n",
        "published": "2019",
        "authors": [
            "A. Emin Orhan",
            "Brenden M. Lake"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.09453v2",
        "title": "Image Synthesis with a Single (Robust) Classifier",
        "abstract": "  We show that the basic classification framework alone can be used to tackle\nsome of the most challenging tasks in image synthesis. In contrast to other\nstate-of-the-art approaches, the toolkit we develop is rather minimal: it uses\na single, off-the-shelf classifier for all these tasks. The crux of our\napproach is that we train this classifier to be adversarially robust. It turns\nout that adversarial robustness is precisely what we need to directly\nmanipulate salient features of the input. Overall, our findings demonstrate the\nutility of robustness in the broader machine learning context. Code and models\nfor our experiments can be found at https://git.io/robust-apps.\n",
        "published": "2019",
        "authors": [
            "Shibani Santurkar",
            "Dimitris Tsipras",
            "Brandon Tran",
            "Andrew Ilyas",
            "Logan Engstrom",
            "Aleksander Madry"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.00052v3",
        "title": "Learning Digital Circuits: A Journey Through Weight Invariant\n  Self-Pruning Neural Networks",
        "abstract": "  Recently, in the paper \"Weight Agnostic Neural Networks\" Gaier & Ha utilized\narchitecture search to find networks where the topology completely encodes the\nknowledge. However, architecture search in topology space is expensive. We use\nthe existing framework of binarized networks to find performant topologies by\nconstraining the weights to be either, zero or one. We show that such\ntopologies achieve performance similar to standard networks while pruning more\nthan 99% weights. We further demonstrate that these topologies can perform\ntasks using constant weights without any explicit tuning. Finally, we discover\nthat in our setup each neuron acts like a NOR gate, virtually learning a\ndigital circuit. We demonstrate the efficacy of our approach on computer vision\ndatasets.\n",
        "published": "2019",
        "authors": [
            "Amey Agrawal",
            "Rohit Karlupia"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.02562v1",
        "title": "TFCheck : A TensorFlow Library for Detecting Training Issues in Neural\n  Network Programs",
        "abstract": "  The increasing inclusion of Machine Learning (ML) models in safety critical\nsystems like autonomous cars have led to the development of multiple\nmodel-based ML testing techniques. One common denominator of these testing\ntechniques is their assumption that training programs are adequate and\nbug-free. These techniques only focus on assessing the performance of the\nconstructed model using manually labeled data or automatically generated data.\nHowever, their assumptions about the training program are not always true as\ntraining programs can contain inconsistencies and bugs. In this paper, we\nexamine training issues in ML programs and propose a catalog of verification\nroutines that can be used to detect the identified issues, automatically. We\nimplemented the routines in a Tensorflow-based library named TFCheck. Using\nTFCheck, practitioners can detect the aforementioned issues automatically. To\nassess the effectiveness of TFCheck, we conducted a case study with real-world,\nmutants, and synthetic training programs. Results show that TFCheck can\nsuccessfully detect training issues in ML code implementations.\n",
        "published": "2019",
        "authors": [
            "Houssem Ben Braiek",
            "Foutse Khomh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.05631v1",
        "title": "Sparse Deep Neural Network Graph Challenge",
        "abstract": "  The MIT/IEEE/Amazon GraphChallenge.org encourages community approaches to\ndeveloping new solutions for analyzing graphs and sparse data. Sparse AI\nanalytics present unique scalability difficulties. The proposed Sparse Deep\nNeural Network (DNN) Challenge draws upon prior challenges from machine\nlearning, high performance computing, and visual analytics to create a\nchallenge that is reflective of emerging sparse AI systems. The Sparse DNN\nChallenge is based on a mathematically well-defined DNN inference computation\nand can be implemented in any programming environment. Sparse DNN inference is\namenable to both vertex-centric implementations and array-based implementations\n(e.g., using the GraphBLAS.org standard). The computations are simple enough\nthat performance predictions can be made based on simple computing hardware\nmodels. The input data sets are derived from the MNIST handwritten letters. The\nsurrounding I/O and verification provide the context for each sparse DNN\ninference that allows rigorous definition of both the input and the output.\nFurthermore, since the proposed sparse DNN challenge is scalable in both\nproblem size and hardware, it can be used to measure and quantitatively compare\na wide range of present day and future systems. Reference implementations have\nbeen implemented and their serial and parallel performance have been measured.\nSpecifications, data, and software are publicly available at GraphChallenge.org\n",
        "published": "2019",
        "authors": [
            "Jeremy Kepner",
            "Simon Alford",
            "Vijay Gadepally",
            "Michael Jones",
            "Lauren Milechin",
            "Ryan Robinett",
            "Sid Samsi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.09432v2",
        "title": "Genetic Neural Architecture Search for automatic assessment of human\n  sperm images",
        "abstract": "  Male infertility is a disease which affects approximately 7% of men. Sperm\nmorphology analysis (SMA) is one of the main diagnosis methods for this\nproblem. Manual SMA is an inexact, subjective, non-reproducible, and hard to\nteach process. As a result, in this paper, we introduce a novel automatic SMA\nbased on a neural architecture search algorithm termed Genetic Neural\nArchitecture Search (GeNAS). For this purpose, we used a collection of images\ncalled MHSMA dataset contains 1,540 sperm images which have been collected from\n235 patients with infertility problems. GeNAS is a genetic algorithm that acts\nas a meta-controller which explores the constrained search space of plain\nconvolutional neural network architectures. Every individual of the genetic\nalgorithm is a convolutional neural network trained to predict morphological\ndeformities in different segments of human sperm (head, vacuole, and acrosome),\nand its fitness is calculated by a novel proposed method named GeNAS-WF\nespecially designed for noisy, low resolution, and imbalanced datasets. Also, a\nhashing method is used to save each trained neural architecture fitness, so we\ncould reuse them during fitness evaluation and speed up the algorithm. Besides,\nin terms of running time and computation power, our proposed architecture\nsearch method is far more efficient than most of the other existing neural\narchitecture search algorithms. Additionally, other proposed methods have been\nevaluated on balanced datasets, whereas GeNAS is built specifically for noisy,\nlow quality, and imbalanced datasets which are common in the field of medical\nimaging. In our experiments, the best neural architecture found by GeNAS has\nreached an accuracy of 91.66%, 77.33%, and 77.66% in the vacuole, head, and\nacrosome abnormality detection, respectively. In comparison to other proposed\nalgorithms for MHSMA dataset, GeNAS achieved state-of-the-art results.\n",
        "published": "2019",
        "authors": [
            "Erfan Miahi",
            "Seyed Abolghasem Mirroshandel",
            "Alexis Nasr"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.00809v1",
        "title": "Enhanced Convolutional Neural Tangent Kernels",
        "abstract": "  Recent research shows that for training with $\\ell_2$ loss, convolutional\nneural networks (CNNs) whose width (number of channels in convolutional layers)\ngoes to infinity correspond to regression with respect to the CNN Gaussian\nProcess kernel (CNN-GP) if only the last layer is trained, and correspond to\nregression with respect to the Convolutional Neural Tangent Kernel (CNTK) if\nall layers are trained. An exact algorithm to compute CNTK (Arora et al., 2019)\nyielded the finding that classification accuracy of CNTK on CIFAR-10 is within\n6-7% of that of that of the corresponding CNN architecture (best figure being\naround 78%) which is interesting performance for a fixed kernel. Here we show\nhow to significantly enhance the performance of these kernels using two ideas.\n(1) Modifying the kernel using a new operation called Local Average Pooling\n(LAP) which preserves efficient computability of the kernel and inherits the\nspirit of standard data augmentation using pixel shifts. Earlier papers were\nunable to incorporate naive data augmentation because of the quadratic training\ncost of kernel regression. This idea is inspired by Global Average Pooling\n(GAP), which we show for CNN-GP and CNTK is equivalent to full translation data\naugmentation. (2) Representing the input image using a pre-processing technique\nproposed by Coates et al. (2011), which uses a single convolutional layer\ncomposed of random image patches. On CIFAR-10, the resulting kernel, CNN-GP\nwith LAP and horizontal flip data augmentation, achieves 89% accuracy, matching\nthe performance of AlexNet (Krizhevsky et al., 2012). Note that this is the\nbest such result we know of for a classifier that is not a trained neural\nnetwork. Similar improvements are obtained for Fashion-MNIST.\n",
        "published": "2019",
        "authors": [
            "Zhiyuan Li",
            "Ruosong Wang",
            "Dingli Yu",
            "Simon S. Du",
            "Wei Hu",
            "Ruslan Salakhutdinov",
            "Sanjeev Arora"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.05266v3",
        "title": "Learning Non-Parametric Invariances from Data with Permanent Random\n  Connectomes",
        "abstract": "  One of the fundamental problems in supervised classification and in machine\nlearning in general, is the modelling of non-parametric invariances that exist\nin data. Most prior art has focused on enforcing priors in the form of\ninvariances to parametric nuisance transformations that are expected to be\npresent in data. Learning non-parametric invariances directly from data remains\nan important open problem. In this paper, we introduce a new architectural\nlayer for convolutional networks which is capable of learning general\ninvariances from data itself. This layer can learn invariance to non-parametric\ntransformations and interestingly, motivates and incorporates permanent random\nconnectomes, thereby being called Permanent Random Connectome Non-Parametric\nTransformation Networks (PRC-NPTN). PRC-NPTN networks are initialized with\nrandom connections (not just weights) which are a small subset of the\nconnections in a fully connected convolution layer. Importantly, these\nconnections in PRC-NPTNs once initialized remain permanent throughout training\nand testing. Permanent random connectomes make these architectures loosely more\nbiologically plausible than many other mainstream network architectures which\nrequire highly ordered structures. We motivate randomly initialized connections\nas a simple method to learn invariance from data itself while invoking\ninvariance towards multiple nuisance transformations simultaneously. We find\nthat these randomly initialized permanent connections have positive effects on\ngeneralization, outperform much larger ConvNet baselines and the recently\nproposed Non-Parametric Transformation Network (NPTN) on benchmarks that\nenforce learning invariances from the data itself.\n",
        "published": "2019",
        "authors": [
            "Dipan K. Pal",
            "Akshay Chawla",
            "Marios Savvides"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.05916v1",
        "title": "Adversarial Margin Maximization Networks",
        "abstract": "  The tremendous recent success of deep neural networks (DNNs) has sparked a\nsurge of interest in understanding their predictive ability. Unlike the human\nvisual system which is able to generalize robustly and learn with little\nsupervision, DNNs normally require a massive amount of data to learn new\nconcepts. In addition, research works also show that DNNs are vulnerable to\nadversarial examples-maliciously generated images which seem perceptually\nsimilar to the natural ones but are actually formed to fool learning models,\nwhich means the models have problem generalizing to unseen data with certain\ntype of distortions. In this paper, we analyze the generalization ability of\nDNNs comprehensively and attempt to improve it from a geometric point of view.\nWe propose adversarial margin maximization (AMM), a learning-based\nregularization which exploits an adversarial perturbation as a proxy. It\nencourages a large margin in the input space, just like the support vector\nmachines. With a differentiable formulation of the perturbation, we train the\nregularized DNNs simply through back-propagation in an end-to-end manner.\nExperimental results on various datasets (including MNIST, CIFAR-10/100, SVHN\nand ImageNet) and different DNN architectures demonstrate the superiority of\nour method over previous state-of-the-arts. Code and models for reproducing our\nresults will be made publicly available.\n",
        "published": "2019",
        "authors": [
            "Ziang Yan",
            "Yiwen Guo",
            "Changshui Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.07446v1",
        "title": "NAIS: Neural Architecture and Implementation Search and its Applications\n  in Autonomous Driving",
        "abstract": "  The rapidly growing demands for powerful AI algorithms in many application\ndomains have motivated massive investment in both high-quality deep neural\nnetwork (DNN) models and high-efficiency implementations. In this position\npaper, we argue that a simultaneous DNN/implementation co-design methodology,\nnamed Neural Architecture and Implementation Search (NAIS), deserves more\nresearch attention to boost the development productivity and efficiency of both\nDNN models and implementation optimization. We propose a stylized design\nmethodology that can drastically cut down the search cost while preserving the\nquality of the end solution.As an illustration, we discuss this\nDNN/implementation methodology in the context of both FPGAs and GPUs. We take\nautonomous driving as a key use case as it is one of the most demanding areas\nfor high quality AI algorithms and accelerators. We discuss how such a\nco-design methodology can impact the autonomous driving industry significantly.\nWe identify several research opportunities in this exciting domain.\n",
        "published": "2019",
        "authors": [
            "Cong Hao",
            "Yao Chen",
            "Xinheng Liu",
            "Atif Sarwari",
            "Daryl Sew",
            "Ashutosh Dhar",
            "Bryan Wu",
            "Dongdong Fu",
            "Jinjun Xiong",
            "Wen-mei Hwu",
            "Junli Gu",
            "Deming Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.12675v1",
        "title": "Continuous Dropout",
        "abstract": "  Dropout has been proven to be an effective algorithm for training robust deep\nnetworks because of its ability to prevent overfitting by avoiding the\nco-adaptation of feature detectors. Current explanations of dropout include\nbagging, naive Bayes, regularization, and sex in evolution. According to the\nactivation patterns of neurons in the human brain, when faced with different\nsituations, the firing rates of neurons are random and continuous, not binary\nas current dropout does. Inspired by this phenomenon, we extend the traditional\nbinary dropout to continuous dropout. On the one hand, continuous dropout is\nconsiderably closer to the activation characteristics of neurons in the human\nbrain than traditional binary dropout. On the other hand, we demonstrate that\ncontinuous dropout has the property of avoiding the co-adaptation of feature\ndetectors, which suggests that we can extract more independent feature\ndetectors for model averaging in the test stage. We introduce the proposed\ncontinuous dropout to a feedforward neural network and comprehensively compare\nit with binary dropout, adaptive dropout, and DropConnect on MNIST, CIFAR-10,\nSVHN, NORB, and ILSVRC-12. Thorough experiments demonstrate that our method\nperforms better in preventing the co-adaptation of feature detectors and\nimproves test performance. The code is available at:\nhttps://github.com/jasonustc/caffe-multigpu/tree/dropout.\n",
        "published": "2019",
        "authors": [
            "Xu Shen",
            "Xinmei Tian",
            "Tongliang Liu",
            "Fang Xu",
            "Dacheng Tao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.02949v2",
        "title": "Activation Density driven Energy-Efficient Pruning in Training",
        "abstract": "  Neural network pruning with suitable retraining can yield networks with\nconsiderably fewer parameters than the original with comparable degrees of\naccuracy. Typical pruning methods require large, fully trained networks as a\nstarting point from which they perform a time-intensive iterative pruning and\nretraining procedure to regain the original accuracy. We propose a novel\npruning method that prunes a network real-time during training, reducing the\noverall training time to achieve an efficient compressed network. We introduce\nan activation density based analysis to identify the optimal relative sizing or\ncompression for each layer of the network. Our method is architecture agnostic,\nallowing it to be employed on a wide variety of systems. For VGG-19 and\nResNet18 on CIFAR-10, CIFAR-100, and TinyImageNet, we obtain exceedingly sparse\nnetworks (up to $200 \\times$ reduction in parameters and over $60 \\times$\nreduction in inference compute operations in the best case) with accuracy\ncomparable to the baseline network. By reducing the network size periodically\nduring training, we achieve total training times that are shorter than those of\npreviously proposed pruning methods. Furthermore, training compressed networks\nat different epochs with our proposed method yields considerable reduction in\ntraining compute complexity ($1.6\\times$ to $3.2\\times$ lower) at near\niso-accuracy as compared to a baseline network trained entirely from scratch.\n",
        "published": "2020",
        "authors": [
            "Timothy Foldy-Porto",
            "Yeshwanth Venkatesha",
            "Priyadarshini Panda"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.04289v2",
        "title": "To Share or Not To Share: A Comprehensive Appraisal of Weight-Sharing",
        "abstract": "  Weight-sharing (WS) has recently emerged as a paradigm to accelerate the\nautomated search for efficient neural architectures, a process dubbed Neural\nArchitecture Search (NAS). Although very appealing, this framework is not\nwithout drawbacks and several works have started to question its capabilities\non small hand-crafted benchmarks. In this paper, we take advantage of the\n\\nasbench dataset to challenge the efficiency of WS on a representative search\nspace. By comparing a SOTA WS approach to a plain random search we show that,\ndespite decent correlations between evaluations using weight-sharing and\nstandalone ones, WS is only rarely significantly helpful to NAS. In particular\nwe highlight the impact of the search space itself on the benefits.\n",
        "published": "2020",
        "authors": [
            "Alo\u00efs Pourchot",
            "Alexis Ducarouge",
            "Olivier Sigaud"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.04688v2",
        "title": "fastai: A Layered API for Deep Learning",
        "abstract": "  fastai is a deep learning library which provides practitioners with\nhigh-level components that can quickly and easily provide state-of-the-art\nresults in standard deep learning domains, and provides researchers with\nlow-level components that can be mixed and matched to build new approaches. It\naims to do both things without substantial compromises in ease of use,\nflexibility, or performance. This is possible thanks to a carefully layered\narchitecture, which expresses common underlying patterns of many deep learning\nand data processing techniques in terms of decoupled abstractions. These\nabstractions can be expressed concisely and clearly by leveraging the dynamism\nof the underlying Python language and the flexibility of the PyTorch library.\nfastai includes: a new type dispatch system for Python along with a semantic\ntype hierarchy for tensors; a GPU-optimized computer vision library which can\nbe extended in pure Python; an optimizer which refactors out the common\nfunctionality of modern optimizers into two basic pieces, allowing optimization\nalgorithms to be implemented in 4-5 lines of code; a novel 2-way callback\nsystem that can access any part of the data, model, or optimizer and change it\nat any point during training; a new data block API; and much more. We have used\nthis library to successfully create a complete deep learning course, which we\nwere able to write more quickly than using previous approaches, and the code\nwas more clear. The library is already in wide use in research, industry, and\nteaching. NB: This paper covers fastai v2, which is currently in pre-release at\nhttp://dev.fast.ai/\n",
        "published": "2020",
        "authors": [
            "Jeremy Howard",
            "Sylvain Gugger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.07224v2",
        "title": "Evolutionary Optimization of Deep Learning Activation Functions",
        "abstract": "  The choice of activation function can have a large effect on the performance\nof a neural network. While there have been some attempts to hand-engineer novel\nactivation functions, the Rectified Linear Unit (ReLU) remains the most\ncommonly-used in practice. This paper shows that evolutionary algorithms can\ndiscover novel activation functions that outperform ReLU. A tree-based search\nspace of candidate activation functions is defined and explored with mutation,\ncrossover, and exhaustive search. Experiments on training wide residual\nnetworks on the CIFAR-10 and CIFAR-100 image datasets show that this approach\nis effective. Replacing ReLU with evolved activation functions results in\nstatistically significant increases in network accuracy. Optimal performance is\nachieved when evolution is allowed to customize activation functions to a\nparticular task; however, these novel activation functions are shown to\ngeneralize, achieving high performance across tasks. Evolutionary optimization\nof activation functions is therefore a promising new dimension of metalearning\nin neural networks.\n",
        "published": "2020",
        "authors": [
            "Garrett Bingham",
            "William Macke",
            "Risto Miikkulainen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.08111v3",
        "title": "Hierarchical Quantized Autoencoders",
        "abstract": "  Despite progress in training neural networks for lossy image compression,\ncurrent approaches fail to maintain both perceptual quality and abstract\nfeatures at very low bitrates. Encouraged by recent success in learning\ndiscrete representations with Vector Quantized Variational Autoencoders\n(VQ-VAEs), we motivate the use of a hierarchy of VQ-VAEs to attain high factors\nof compression. We show that the combination of stochastic quantization and\nhierarchical latent structure aids likelihood-based image compression. This\nleads us to introduce a novel objective for training hierarchical VQ-VAEs. Our\nresulting scheme produces a Markovian series of latent variables that\nreconstruct images of high-perceptual quality which retain semantically\nmeaningful features. We provide qualitative and quantitative evaluations on the\nCelebA and MNIST datasets.\n",
        "published": "2020",
        "authors": [
            "Will Williams",
            "Sam Ringer",
            "Tom Ash",
            "John Hughes",
            "David MacLeod",
            "Jamie Dougherty"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.08118v5",
        "title": "Randomized Smoothing of All Shapes and Sizes",
        "abstract": "  Randomized smoothing is the current state-of-the-art defense with provable\nrobustness against $\\ell_2$ adversarial attacks. Many works have devised new\nrandomized smoothing schemes for other metrics, such as $\\ell_1$ or\n$\\ell_\\infty$; however, substantial effort was needed to derive such new\nguarantees. This begs the question: can we find a general theory for randomized\nsmoothing?\n  We propose a novel framework for devising and analyzing randomized smoothing\nschemes, and validate its effectiveness in practice. Our theoretical\ncontributions are: (1) we show that for an appropriate notion of \"optimal\", the\noptimal smoothing distributions for any \"nice\" norms have level sets given by\nthe norm's *Wulff Crystal*; (2) we propose two novel and complementary methods\nfor deriving provably robust radii for any smoothing distribution; and, (3) we\nshow fundamental limits to current randomized smoothing techniques via the\ntheory of *Banach space cotypes*. By combining (1) and (2), we significantly\nimprove the state-of-the-art certified accuracy in $\\ell_1$ on standard\ndatasets. Meanwhile, we show using (3) that with only label statistics under\nrandom input perturbations, randomized smoothing cannot achieve nontrivial\ncertified accuracy against perturbations of $\\ell_p$-norm $\\Omega(\\min(1,\nd^{\\frac{1}{p} - \\frac{1}{2}}))$, when the input dimension $d$ is large. We\nprovide code in github.com/tonyduan/rs4a.\n",
        "published": "2020",
        "authors": [
            "Greg Yang",
            "Tony Duan",
            "J. Edward Hu",
            "Hadi Salman",
            "Ilya Razenshteyn",
            "Jerry Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.09571v2",
        "title": "Learning to Continually Learn",
        "abstract": "  Continual lifelong learning requires an agent or model to learn many\nsequentially ordered tasks, building on previous knowledge without\ncatastrophically forgetting it. Much work has gone towards preventing the\ndefault tendency of machine learning models to catastrophically forget, yet\nvirtually all such work involves manually-designed solutions to the problem. We\ninstead advocate meta-learning a solution to catastrophic forgetting, allowing\nAI to learn to continually learn. Inspired by neuromodulatory processes in the\nbrain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It\ndifferentiates through a sequential learning process to meta-learn an\nactivation-gating function that enables context-dependent selective activation\nwithin a deep neural network. Specifically, a neuromodulatory (NM) neural\nnetwork gates the forward pass of another (otherwise normal) neural network\ncalled the prediction learning network (PLN). The NM network also thus\nindirectly controls selective plasticity (i.e. the backward pass of) the PLN.\nANML enables continual learning without catastrophic forgetting at scale: it\nproduces state-of-the-art continual learning performance, sequentially learning\nas many as 600 classes (over 9,000 SGD updates).\n",
        "published": "2020",
        "authors": [
            "Shawn Beaulieu",
            "Lapo Frati",
            "Thomas Miconi",
            "Joel Lehman",
            "Kenneth O. Stanley",
            "Jeff Clune",
            "Nick Cheney"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.09815v3",
        "title": "Neuron Shapley: Discovering the Responsible Neurons",
        "abstract": "  We develop Neuron Shapley as a new framework to quantify the contribution of\nindividual neurons to the prediction and performance of a deep network. By\naccounting for interactions across neurons, Neuron Shapley is more effective in\nidentifying important filters compared to common approaches based on activation\npatterns. Interestingly, removing just 30 filters with the highest Shapley\nscores effectively destroys the prediction accuracy of Inception-v3 on\nImageNet. Visualization of these few critical filters provides insights into\nhow the network functions. Neuron Shapley is a flexible framework and can be\napplied to identify responsible neurons in many tasks. We illustrate additional\napplications of identifying filters that are responsible for biased prediction\nin facial recognition and filters that are vulnerable to adversarial attacks.\nRemoving these filters is a quick way to repair models. Enabling all these\napplications is a new multi-arm bandit algorithm that we developed to\nefficiently estimate Neuron Shapley values.\n",
        "published": "2020",
        "authors": [
            "Amirata Ghorbani",
            "James Zou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.09958v2",
        "title": "Gradual Channel Pruning while Training using Feature Relevance Scores\n  for Convolutional Neural Networks",
        "abstract": "  The enormous inference cost of deep neural networks can be scaled down by\nnetwork compression. Pruning is one of the predominant approaches used for deep\nnetwork compression. However, existing pruning techniques have one or more of\nthe following limitations: 1) Additional energy cost on top of the compute\nheavy training stage due to pruning and fine-tuning stages, 2) Layer-wise\npruning based on the statistics of a particular, ignoring the effect of error\npropagation in the network, 3) Lack of an efficient estimate for determining\nthe important channels globally, 4) Unstructured pruning requires specialized\nhardware for effective use. To address all the above issues, we present a\nsimple-yet-effective gradual channel pruning while training methodology using a\nnovel data-driven metric referred to as feature relevance score. The proposed\ntechnique gets rid of the additional retraining cycles by pruning the least\nimportant channels in a structured fashion at fixed intervals during the actual\ntraining phase. Feature relevance scores help in efficiently evaluating the\ncontribution of each channel towards the discriminative power of the network.\nWe demonstrate the effectiveness of the proposed methodology on architectures\nsuch as VGG and ResNet using datasets such as CIFAR-10, CIFAR-100 and ImageNet,\nand successfully achieve significant model compression while trading off less\nthan $1\\%$ accuracy. Notably on CIFAR-10 dataset trained on ResNet-110, our\napproach achieves $2.4\\times$ compression and a $56\\%$ reduction in FLOPs with\nan accuracy drop of $0.01\\%$ compared to the unpruned network.\n",
        "published": "2020",
        "authors": [
            "Sai Aparna Aketi",
            "Sourjya Roy",
            "Anand Raghunathan",
            "Kaushik Roy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.11318v5",
        "title": "Can we have it all? On the Trade-off between Spatial and Adversarial\n  Robustness of Neural Networks",
        "abstract": "  (Non-)robustness of neural networks to small, adversarial pixel-wise\nperturbations, and as more recently shown, to even random spatial\ntransformations (e.g., translations, rotations) entreats both theoretical and\nempirical understanding. Spatial robustness to random translations and\nrotations is commonly attained via equivariant models (e.g., StdCNNs, GCNNs)\nand training augmentation, whereas adversarial robustness is typically achieved\nby adversarial training. In this paper, we prove a quantitative trade-off\nbetween spatial and adversarial robustness in a simple statistical setting. We\ncomplement this empirically by showing that: (a) as the spatial robustness of\nequivariant models improves by training augmentation with progressively larger\ntransformations, their adversarial robustness worsens progressively, and (b) as\nthe state-of-the-art robust models are adversarially trained with progressively\nlarger pixel-wise perturbations, their spatial robustness drops progressively.\nTowards achieving pareto-optimality in this trade-off, we propose a method\nbased on curriculum learning that trains gradually on more difficult\nperturbations (both spatial and adversarial) to improve spatial and adversarial\nrobustness simultaneously.\n",
        "published": "2020",
        "authors": [
            "Sandesh Kamath",
            "Amit Deshpande",
            "K V Subrahmanyam",
            "Vineeth N Balasubramanian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.04153v1",
        "title": "A Hybrid Method for Training Convolutional Neural Networks",
        "abstract": "  Artificial Intelligence algorithms have been steadily increasing in\npopularity and usage. Deep Learning, allows neural networks to be trained using\nhuge datasets and also removes the need for human extracted features, as it\nautomates the feature learning process. In the hearth of training deep neural\nnetworks, such as Convolutional Neural Networks, we find backpropagation, that\nby computing the gradient of the loss function with respect to the weights of\nthe network for a given input, it allows the weights of the network to be\nadjusted to better perform in the given task. In this paper, we propose a\nhybrid method that uses both backpropagation and evolutionary strategies to\ntrain Convolutional Neural Networks, where the evolutionary strategies are used\nto help to avoid local minimas and fine-tune the weights, so that the network\nachieves higher accuracy results. We show that the proposed hybrid method is\ncapable of improving upon regular training in the task of image classification\nin CIFAR-10, where a VGG16 model was used and the final test results increased\n0.61%, in average, when compared to using only backpropagation.\n",
        "published": "2020",
        "authors": [
            "Vasco Lopes",
            "Paulo Fazendeiro"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.05930v3",
        "title": "Localized convolutional neural networks for geospatial wind forecasting",
        "abstract": "  Convolutional Neural Networks (CNN) possess many positive qualities when it\ncomes to spatial raster data. Translation invariance enables CNNs to detect\nfeatures regardless of their position in the scene. However, in some domains,\nlike geospatial, not all locations are exactly equal. In this work, we propose\nlocalized convolutional neural networks that enable convolutional architectures\nto learn local features in addition to the global ones. We investigate their\ninstantiations in the form of learnable inputs, local weights, and a more\ngeneral form. They can be added to any convolutional layers, easily end-to-end\ntrained, introduce minimal additional complexity, and let CNNs retain most of\ntheir benefits to the extent that they are needed. In this work we address\nspatio-temporal prediction: test the effectiveness of our methods on a\nsynthetic benchmark dataset and tackle three real-world wind prediction\ndatasets. For one of them, we propose a method to spatially order the unordered\ndata. We compare the recent state-of-the-art spatio-temporal prediction models\non the same data. Models that use convolutional layers can be and are extended\nwith our localizations. In all these cases our extensions improve the results,\nand thus often the state-of-the-art. We share all the code at a public\nrepository.\n",
        "published": "2020",
        "authors": [
            "Arnas Uselis",
            "Mantas Luko\u0161evi\u010dius",
            "Lukas Stasytis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.10851v1",
        "title": "Conditionally Deep Hybrid Neural Networks Across Edge and Cloud",
        "abstract": "  The pervasiveness of \"Internet-of-Things\" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\n",
        "published": "2020",
        "authors": [
            "Yinghan Long",
            "Indranil Chakraborty",
            "Kaushik Roy"
        ]
    }
]