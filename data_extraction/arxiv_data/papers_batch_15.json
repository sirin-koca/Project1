[
    {
        "id": "http://arxiv.org/abs/2205.12013v3",
        "title": "Naive Few-Shot Learning: Uncovering the fluid intelligence of machines",
        "abstract": "  In this paper, we aimed to help bridge the gap between human fluid\nintelligence - the ability to solve novel tasks without prior training - and\nthe performance of deep neural networks, which typically require extensive\nprior training. An essential cognitive component for solving intelligence\ntests, which in humans are used to measure fluid intelligence, is the ability\nto identify regularities in sequences. This motivated us to construct a\nbenchmark task, which we term \\textit{sequence consistency evaluation} (SCE),\nwhose solution requires the ability to identify regularities in sequences.\nGiven the proven capabilities of deep networks, their ability to solve such\ntasks after extensive training is expected. Surprisingly, however, we show that\nnaive (randomly initialized) deep learning models that are trained on a\n\\textit{single} SCE with a \\textit{single} optimization step can still solve\nnon-trivial versions of the task relatively well. We extend our findings to\nsolve, without any prior training, real-world anomaly detection tasks in the\nvisual and auditory modalities. These results demonstrate the fluid-intelligent\ncomputational capabilities of deep networks. We discuss the implications of our\nwork for constructing fluid-intelligent machines.\n",
        "published": "2022",
        "authors": [
            "Tomer Barak",
            "Yonatan Loewenstein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.12755v6",
        "title": "An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale\n  Multitask Learning Systems",
        "abstract": "  Multitask learning assumes that models capable of learning from multiple\ntasks can achieve better quality and efficiency via knowledge transfer, a key\nfeature of human learning. Though, state of the art ML models rely on high\ncustomization for each task and leverage size and data scale rather than\nscaling the number of tasks. Also, continual learning, that adds the temporal\naspect to multitask, is often focused to the study of common pitfalls such as\ncatastrophic forgetting instead of being studied at a large scale as a critical\ncomponent to build the next generation artificial intelligence.We propose an\nevolutionary method capable of generating large scale multitask models that\nsupport the dynamic addition of new tasks. The generated multitask models are\nsparsely activated and integrates a task-based routing that guarantees bounded\ncompute cost and fewer added parameters per task as the model expands.The\nproposed method relies on a knowledge compartmentalization technique to achieve\nimmunity against catastrophic forgetting and other common pitfalls such as\ngradient interference and negative transfer. We demonstrate empirically that\nthe proposed method can jointly solve and achieve competitive results on\n69public image classification tasks, for example improving the state of the art\non a competitive benchmark such as cifar10 by achieving a 15% relative error\nreduction compared to the best model trained on public data.\n",
        "published": "2022",
        "authors": [
            "Andrea Gesmundo",
            "Jeff Dean"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.03429v2",
        "title": "Generating Long Videos of Dynamic Scenes",
        "abstract": "  We present a video generation model that accurately reproduces object motion,\nchanges in camera viewpoint, and new content that arises over time. Existing\nvideo generation methods often fail to produce new content as a function of\ntime while maintaining consistencies expected in real environments, such as\nplausible dynamics and object persistence. A common failure case is for content\nto never change due to over-reliance on inductive biases to provide temporal\nconsistency, such as a single latent code that dictates content for the entire\nvideo. On the other extreme, without long-term consistency, generated videos\nmay morph unrealistically between different scenes. To address these\nlimitations, we prioritize the time axis by redesigning the temporal latent\nrepresentation and learning long-term consistency from data by training on\nlonger videos. To this end, we leverage a two-phase training strategy, where we\nseparately train using longer videos at a low resolution and shorter videos at\na high resolution. To evaluate the capabilities of our model, we introduce two\nnew benchmark datasets with explicit focus on long-term temporal dynamics.\n",
        "published": "2022",
        "authors": [
            "Tim Brooks",
            "Janne Hellsten",
            "Miika Aittala",
            "Ting-Chun Wang",
            "Timo Aila",
            "Jaakko Lehtinen",
            "Ming-Yu Liu",
            "Alexei A. Efros",
            "Tero Karras"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.04016v1",
        "title": "SYNERgy between SYNaptic consolidation and Experience Replay for general\n  continual learning",
        "abstract": "  Continual learning (CL) in the brain is facilitated by a complex set of\nmechanisms. This includes the interplay of multiple memory systems for\nconsolidating information as posited by the complementary learning systems\n(CLS) theory and synaptic consolidation for protecting the acquired knowledge\nfrom erasure. Thus, we propose a general CL method that creates a synergy\nbetween SYNaptic consolidation and dual memory Experience Replay (SYNERgy). Our\nmethod maintains a semantic memory that accumulates and consolidates\ninformation across the tasks and interacts with the episodic memory for\neffective replay. It further employs synaptic consolidation by tracking the\nimportance of parameters during the training trajectory and anchoring them to\nthe consolidated parameters in the semantic memory. To the best of our\nknowledge, our study is the first to employ dual memory experience replay in\nconjunction with synaptic consolidation that is suitable for general CL whereby\nthe network does not utilize task boundaries or task labels during training or\ninference. Our evaluation on various challenging CL scenarios and\ncharacteristics analyses demonstrate the efficacy of incorporating both\nsynaptic consolidation and CLS theory in enabling effective CL in DNNs.\n",
        "published": "2022",
        "authors": [
            "Fahad Sarfraz",
            "Elahe Arani",
            "Bahram Zonooz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.05365v1",
        "title": "Object Detection, Recognition, Deep Learning, and the Universal Law of\n  Generalization",
        "abstract": "  Object detection and recognition are fundamental functions underlying the\nsuccess of species. Because the appearance of an object exhibits a large\nvariability, the brain has to group these different stimuli under the same\nobject identity, a process of generalization. Does the process of\ngeneralization follow some general principles or is it an ad-hoc\n\"bag-of-tricks\"? The Universal Law of Generalization provided evidence that\ngeneralization follows similar properties across a variety of species and\ntasks. Here we test the hypothesis that the internal representations underlying\ngeneralization reflect the natural properties of object detection and\nrecognition in our environment rather than the specifics of the system solving\nthese problems. By training a deep-neural-network with images of \"clear\" and\n\"camouflaged\" animals, we found that with a proper choice of category\nprototypes, the generalization functions are monotone decreasing, similar to\nthe generalization functions of biological systems. Our findings support the\nhypothesis of the study.\n",
        "published": "2022",
        "authors": [
            "Faris B. Rustom",
            "Haluk \u00d6\u011fmen",
            "Arash Yazdanbakhsh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.08138v2",
        "title": "Lessons learned from the NeurIPS 2021 MetaDL challenge: Backbone\n  fine-tuning without episodic meta-learning dominates for few-shot learning\n  image classification",
        "abstract": "  Although deep neural networks are capable of achieving performance superior\nto humans on various tasks, they are notorious for requiring large amounts of\ndata and computing resources, restricting their success to domains where such\nresources are available. Metalearning methods can address this problem by\ntransferring knowledge from related tasks, thus reducing the amount of data and\ncomputing resources needed to learn new tasks. We organize the MetaDL\ncompetition series, which provide opportunities for research groups all over\nthe world to create and experimentally assess new meta-(deep)learning solutions\nfor real problems. In this paper, authored collaboratively between the\ncompetition organizers and the top-ranked participants, we describe the design\nof the competition, the datasets, the best experimental results, as well as the\ntop-ranked methods in the NeurIPS 2021 challenge, which attracted 15 active\nteams who made it to the final phase (by outperforming the baseline), making\nover 100 code submissions during the feedback phase. The solutions of the top\nparticipants have been open-sourced. The lessons learned include that learning\ngood representations is essential for effective transfer learning.\n",
        "published": "2022",
        "authors": [
            "Adrian El Baz",
            "Ihsan Ullah",
            "Edesio Alcoba\u00e7a",
            "Andr\u00e9 C. P. L. F. Carvalho",
            "Hong Chen",
            "Fabio Ferreira",
            "Henry Gouk",
            "Chaoyu Guan",
            "Isabelle Guyon",
            "Timothy Hospedales",
            "Shell Hu",
            "Mike Huisman",
            "Frank Hutter",
            "Zhengying Liu",
            "Felix Mohr",
            "Ekrem \u00d6zt\u00fcrk",
            "Jan N. van Rijn",
            "Haozhe Sun",
            "Xin Wang",
            "Wenwu Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.13498v2",
        "title": "Auditing Visualizations: Transparency Methods Struggle to Detect\n  Anomalous Behavior",
        "abstract": "  Model visualizations provide information that outputs alone might miss. But\ncan we trust that model visualizations reflect model behavior? For instance,\ncan they diagnose abnormal behavior such as planted backdoors or\noverregularization? To evaluate visualization methods, we test whether they\nassign different visualizations to anomalously trained models and normal\nmodels. We find that while existing methods can detect models with starkly\nanomalous behavior, they struggle to identify more subtle anomalies. Moreover,\nthey often fail to recognize the inputs that induce anomalous behavior, e.g.\nimages containing a spurious cue. These results reveal blind spots and\nlimitations of some popular model visualizations. By introducing a novel\nevaluation framework for visualizations, our work paves the way for developing\nmore reliable model transparency methods in the future.\n",
        "published": "2022",
        "authors": [
            "Jean-Stanislas Denain",
            "Jacob Steinhardt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.00787v3",
        "title": "Object Representations as Fixed Points: Training Iterative Refinement\n  Algorithms with Implicit Differentiation",
        "abstract": "  Iterative refinement -- start with a random guess, then iteratively improve\nthe guess -- is a useful paradigm for representation learning because it offers\na way to break symmetries among equally plausible explanations for the data.\nThis property enables the application of such methods to infer representations\nof sets of entities, such as objects in physical scenes, structurally\nresembling clustering algorithms in latent space. However, most prior works\ndifferentiate through the unrolled refinement process, which can make\noptimization challenging. We observe that such methods can be made\ndifferentiable by means of the implicit function theorem, and develop an\nimplicit differentiation approach that improves the stability and tractability\nof training by decoupling the forward and backward passes. This connection\nenables us to apply advances in optimizing implicit layers to not only improve\nthe optimization of the slot attention module in SLATE, a state-of-the-art\nmethod for learning entity representations, but do so with constant space and\ntime complexity in backpropagation and only one additional line of code.\n",
        "published": "2022",
        "authors": [
            "Michael Chang",
            "Thomas L. Griffiths",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.05135v2",
        "title": "FreeREA: Training-Free Evolution-based Architecture Search",
        "abstract": "  In the last decade, most research in Machine Learning contributed to the\nimprovement of existing models, with the aim of increasing the performance of\nneural networks for the solution of a variety of different tasks. However, such\nadvancements often come at the cost of an increase of model memory and\ncomputational requirements. This represents a significant limitation for the\ndeployability of research output in realistic settings, where the cost, the\nenergy consumption, and the complexity of the framework play a crucial role. To\nsolve this issue, the designer should search for models that maximise the\nperformance while limiting its footprint. Typical approaches to reach this goal\nrely either on manual procedures, which cannot guarantee the optimality of the\nfinal design, or upon Neural Architecture Search algorithms to automatise the\nprocess, at the expenses of extremely high computational time. This paper\nprovides a solution for the fast identification of a neural network that\nmaximises the model accuracy while preserving size and computational\nconstraints typical of tiny devices. Our approach, named FreeREA, is a custom\ncell-based evolution NAS algorithm that exploits an optimised combination of\ntraining-free metrics to rank architectures during the search, thus without\nneed of model training. Our experiments, carried out on the common benchmarks\nNAS-Bench-101 and NATS-Bench, demonstrate that i) FreeREA is a fast, efficient,\nand effective search method for models automatic design; ii) it outperforms\nState of the Art training-based and training-free techniques in all the\ndatasets and benchmarks considered, and iii) it can easily generalise to\nconstrained scenarios, representing a competitive solution for fast Neural\nArchitecture Search in generic constrained applications. The code is available\nat \\url{https://github.com/NiccoloCavagnero/FreeREA}.\n",
        "published": "2022",
        "authors": [
            "Niccol\u00f2 Cavagnero",
            "Luca Robbiano",
            "Barbara Caputo",
            "Giuseppe Averta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.12534v3",
        "title": "Trainability Preserving Neural Pruning",
        "abstract": "  Many recent works have shown trainability plays a central role in neural\nnetwork pruning -- unattended broken trainability can lead to severe\nunder-performance and unintentionally amplify the effect of retraining learning\nrate, resulting in biased (or even misinterpreted) benchmark results. This\npaper introduces trainability preserving pruning (TPP), a scalable method to\npreserve network trainability against pruning, aiming for improved pruning\nperformance and being more robust to retraining hyper-parameters (e.g.,\nlearning rate). Specifically, we propose to penalize the gram matrix of\nconvolutional filters to decorrelate the pruned filters from the retained\nfilters. In addition to the convolutional layers, per the spirit of preserving\nthe trainability of the whole network, we also propose to regularize the batch\nnormalization parameters (scale and bias). Empirical studies on linear MLP\nnetworks show that TPP can perform on par with the oracle trainability recovery\nscheme. On nonlinear ConvNets (ResNet56/VGG19) on CIFAR10/100, TPP outperforms\nthe other counterpart approaches by an obvious margin. Moreover, results on\nImageNet-1K with ResNets suggest that TPP consistently performs more favorably\nagainst other top-performing structured pruning approaches. Code:\nhttps://github.com/MingSun-Tse/TPP.\n",
        "published": "2022",
        "authors": [
            "Huan Wang",
            "Yun Fu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.01204v2",
        "title": "Making a Spiking Net Work: Robust brain-like unsupervised machine\n  learning",
        "abstract": "  The surge in interest in Artificial Intelligence (AI) over the past decade\nhas been driven almost exclusively by advances in Artificial Neural Networks\n(ANNs). While ANNs set state-of-the-art performance for many previously\nintractable problems, the use of global gradient descent necessitates large\ndatasets and computational resources for training, potentially limiting their\nscalability for real-world domains. Spiking Neural Networks (SNNs) are an\nalternative to ANNs that use more brain-like artificial neurons and can use\nlocal unsupervised learning to rapidly discover sparse recognizable features in\nthe input data. SNNs, however, struggle with dynamical stability and have\nfailed to match the accuracy of ANNs. Here we show how an SNN can overcome many\nof the shortcomings that have been identified in the literature, including\noffering a principled solution to the dynamical \"vanishing spike problem\", to\noutperform all existing shallow SNNs and equal the performance of an ANN. It\naccomplishes this while using unsupervised learning with unlabeled data and\nonly 1/50th of the training epochs (labeled data is used only for a simple\nlinear readout layer). This result makes SNNs a viable new method for fast,\naccurate, efficient, explainable, and re-deployable machine learning with\nunlabeled data.\n",
        "published": "2022",
        "authors": [
            "Peter G. Stratton",
            "Andrew Wabnitz",
            "Chip Essam",
            "Allen Cheung",
            "Tara J. Hamilton"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.06475v1",
        "title": "Guided Evolutionary Neural Architecture Search With Efficient\n  Performance Estimation",
        "abstract": "  Neural Architecture Search (NAS) methods have been successfully applied to\nimage tasks with excellent results. However, NAS methods are often complex and\ntend to converge to local minima as soon as generated architectures seem to\nyield good results. This paper proposes GEA, a novel approach for guided NAS.\nGEA guides the evolution by exploring the search space by generating and\nevaluating several architectures in each generation at initialisation stage\nusing a zero-proxy estimator, where only the highest-scoring architecture is\ntrained and kept for the next generation. Subsequently, GEA continuously\nextracts knowledge about the search space without increased complexity by\ngenerating several off-springs from an existing architecture at each\ngeneration. More, GEA forces exploitation of the most performant architectures\nby descendant generation while simultaneously driving exploration through\nparent mutation and favouring younger architectures to the detriment of older\nones. Experimental results demonstrate the effectiveness of the proposed\nmethod, and extensive ablation studies evaluate the importance of different\nparameters. Results show that GEA achieves state-of-the-art results on all data\nsets of NAS-Bench-101, NAS-Bench-201 and TransNAS-Bench-101 benchmarks.\n",
        "published": "2022",
        "authors": [
            "Vasco Lopes",
            "Miguel Santos",
            "Bruno Degardin",
            "Lu\u00eds A. Alexandre"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.14686v1",
        "title": "NeurIPS'22 Cross-Domain MetaDL competition: Design and baseline results",
        "abstract": "  We present the design and baseline results for a new challenge in the\nChaLearn meta-learning series, accepted at NeurIPS'22, focusing on\n\"cross-domain\" meta-learning. Meta-learning aims to leverage experience gained\nfrom previous tasks to solve new tasks efficiently (i.e., with better\nperformance, little training data, and/or modest computational resources).\nWhile previous challenges in the series focused on within-domain few-shot\nlearning problems, with the aim of learning efficiently N-way k-shot tasks\n(i.e., N class classification problems with k training examples), this\ncompetition challenges the participants to solve \"any-way\" and \"any-shot\"\nproblems drawn from various domains (healthcare, ecology, biology,\nmanufacturing, and others), chosen for their humanitarian and societal impact.\nTo that end, we created Meta-Album, a meta-dataset of 40 image classification\ndatasets from 10 domains, from which we carve out tasks with any number of\n\"ways\" (within the range 2-20) and any number of \"shots\" (within the range\n1-20). The competition is with code submission, fully blind-tested on the\nCodaLab challenge platform. The code of the winners will be open-sourced,\nenabling the deployment of automated machine learning solutions for few-shot\nimage classification across several domains.\n",
        "published": "2022",
        "authors": [
            "Dustin Carri\u00f3n-Ojeda",
            "Hong Chen",
            "Adrian El Baz",
            "Sergio Escalera",
            "Chaoyu Guan",
            "Isabelle Guyon",
            "Ihsan Ullah",
            "Xin Wang",
            "Wenwu Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.04362v2",
        "title": "EDeNN: Event Decay Neural Networks for low latency vision",
        "abstract": "  Despite the success of neural networks in computer vision tasks, digital\n'neurons' are a very loose approximation of biological neurons. Today's\nlearning approaches are designed to function on digital devices with digital\ndata representations such as image frames. In contrast, biological vision\nsystems are generally much more capable and efficient than state-of-the-art\ndigital computer vision algorithms. Event cameras are an emerging sensor\ntechnology which imitates biological vision with asynchronously firing pixels,\neschewing the concept of the image frame. To leverage modern learning\ntechniques, many event-based algorithms are forced to accumulate events back to\nimage frames, somewhat squandering the advantages of event cameras.\n  We follow the opposite paradigm and develop a new type of neural network\nwhich operates closer to the original event data stream. We demonstrate\nstate-of-the-art performance in angular velocity regression and competitive\noptical flow estimation, while avoiding difficulties related to training SNN.\nFurthermore, the processing latency of our proposed approach is less than 1/10\nany other implementation, while continuous inference increases this improvement\nby another order of magnitude.\n",
        "published": "2022",
        "authors": [
            "Celyn Walters",
            "Simon Hadfield"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.06119v4",
        "title": "APTx: better activation function than MISH, SWISH, and ReLU's variants\n  used in deep learning",
        "abstract": "  Activation Functions introduce non-linearity in the deep neural networks.\nThis nonlinearity helps the neural networks learn faster and efficiently from\nthe dataset. In deep learning, many activation functions are developed and used\nbased on the type of problem statement. ReLU's variants, SWISH, and MISH are\ngoto activation functions. MISH function is considered having similar or even\nbetter performance than SWISH, and much better than ReLU. In this paper, we\npropose an activation function named APTx which behaves similar to MISH, but\nrequires lesser mathematical operations to compute. The lesser computational\nrequirements of APTx does speed up the model training, and thus also reduces\nthe hardware requirement for the deep learning model.\n",
        "published": "2022",
        "authors": [
            "Ravin Kumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.06399v1",
        "title": "A Survey on Evolutionary Computation for Computer Vision and Image\n  Analysis: Past, Present, and Future Trends",
        "abstract": "  Computer vision (CV) is a big and important field in artificial intelligence\ncovering a wide range of applications. Image analysis is a major task in CV\naiming to extract, analyse and understand the visual content of images.\nHowever, image-related tasks are very challenging due to many factors, e.g.,\nhigh variations across images, high dimensionality, domain expertise\nrequirement, and image distortions. Evolutionary computation (EC) approaches\nhave been widely used for image analysis with significant achievement. However,\nthere is no comprehensive survey of existing EC approaches to image analysis.\nTo fill this gap, this paper provides a comprehensive survey covering all\nessential EC approaches to important image analysis tasks including edge\ndetection, image segmentation, image feature analysis, image classification,\nobject detection, and others. This survey aims to provide a better\nunderstanding of evolutionary computer vision (ECV) by discussing the\ncontributions of different approaches and exploring how and why EC is used for\nCV and image analysis. The applications, challenges, issues, and trends\nassociated to this research field are also discussed and summarised to provide\nfurther guidelines and opportunities for future research.\n",
        "published": "2022",
        "authors": [
            "Ying Bi",
            "Bing Xue",
            "Pablo Mesejo",
            "Stefano Cagnoni",
            "Mengjie Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.07326v3",
        "title": "A Continual Development Methodology for Large-scale Multitask Dynamic ML\n  Systems",
        "abstract": "  The traditional Machine Learning (ML) methodology requires to fragment the\ndevelopment and experimental process into disconnected iterations whose\nfeedback is used to guide design or tuning choices. This methodology has\nmultiple efficiency and scalability disadvantages, such as leading to spend\nsignificant resources into the creation of multiple trial models that do not\ncontribute to the final solution.The presented work is based on the intuition\nthat defining ML models as modular and extensible artefacts allows to introduce\na novel ML development methodology enabling the integration of multiple design\nand evaluation iterations into the continuous enrichment of a single unbounded\nintelligent system. We define a novel method for the generation of dynamic\nmultitask ML models as a sequence of extensions and generalizations. We first\nanalyze the capabilities of the proposed method by using the standard ML\nempirical evaluation methodology. Finally, we propose a novel continuous\ndevelopment methodology that allows to dynamically extend a pre-existing\nmultitask large-scale ML system while analyzing the properties of the proposed\nmethod extensions. This results in the generation of an ML model capable of\njointly solving 124 image classification tasks achieving state of the art\nquality with improved size and compute cost.\n",
        "published": "2022",
        "authors": [
            "Andrea Gesmundo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.13233v1",
        "title": "Genetic Programming-Based Evolutionary Deep Learning for Data-Efficient\n  Image Classification",
        "abstract": "  Data-efficient image classification is a challenging task that aims to solve\nimage classification using small training data. Neural network-based deep\nlearning methods are effective for image classification, but they typically\nrequire large-scale training data and have major limitations such as requiring\nexpertise to design network architectures and having poor interpretability.\nEvolutionary deep learning is a recent hot topic that combines evolutionary\ncomputation with deep learning. However, most evolutionary deep learning\nmethods focus on evolving architectures of neural networks, which still suffer\nfrom limitations such as poor interpretability. To address this, this paper\nproposes a new genetic programming-based evolutionary deep learning approach to\ndata-efficient image classification. The new approach can automatically evolve\nvariable-length models using many important operators from both image and\nclassification domains. It can learn different types of image features from\ncolour or gray-scale images, and construct effective and diverse ensembles for\nimage classification. A flexible multi-layer representation enables the new\napproach to automatically construct shallow or deep models/trees for different\ntasks and perform effective transformations on the input data via multiple\ninternal nodes. The new approach is applied to solve five image classification\ntasks with different training set sizes. The results show that it achieves\nbetter performance in most cases than deep learning methods for data-efficient\nimage classification. A deep analysis shows that the new approach has good\nconvergence and evolves models with high interpretability, different\nlengths/sizes/shapes, and good transferability.\n",
        "published": "2022",
        "authors": [
            "Ying Bi",
            "Bing Xue",
            "Mengjie Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.13400v2",
        "title": "Activation Learning by Local Competitions",
        "abstract": "  Despite its great success, backpropagation has certain limitations that\nnecessitate the investigation of new learning methods. In this study, we\npresent a biologically plausible local learning rule that improves upon Hebb's\nwell-known proposal and discovers unsupervised features by local competitions\namong neurons. This simple learning rule enables the creation of a forward\nlearning paradigm called activation learning, in which the output activation\n(sum of the squared output) of the neural network estimates the likelihood of\nthe input patterns, or \"learn more, activate more\" in simpler terms. For\nclassification on a few small classical datasets, activation learning performs\ncomparably to backpropagation using a fully connected network, and outperforms\nbackpropagation when there are fewer training samples or unpredictable\ndisturbances. Additionally, the same trained network can be used for a variety\nof tasks, including image generation and completion. Activation learning also\nachieves state-of-the-art performance on several real-world datasets for\nanomaly detection. This new learning paradigm, which has the potential to unify\nsupervised, unsupervised, and semi-supervised learning and is reasonably more\nresistant to adversarial attacks, deserves in-depth investigation.\n",
        "published": "2022",
        "authors": [
            "Hongchao Zhou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.11114v1",
        "title": "Pruning by Active Attention Manipulation",
        "abstract": "  Filter pruning of a CNN is typically achieved by applying discrete masks on\nthe CNN's filter weights or activation maps, post-training. Here, we present a\nnew filter-importance-scoring concept named pruning by active attention\nmanipulation (PAAM), that sparsifies the CNN's set of filters through a\nparticular attention mechanism, during-training. PAAM learns analog filter\nscores from the filter weights by optimizing a cost function regularized by an\nadditive term in the scores. As the filters are not independent, we use\nattention to dynamically learn their correlations. Moreover, by training the\npruning scores of all layers simultaneously, PAAM can account for layer\ninter-dependencies, which is essential to finding a performant sparse\nsub-network. PAAM can also train and generate a pruned network from scratch in\na straightforward, one-stage training process without requiring a pre-trained\nnetwork. Finally, PAAM does not need layer-specific hyperparameters and\npre-defined layer budgets, since it can implicitly determine the appropriate\nnumber of filters in each layer. Our experimental results on different network\narchitectures suggest that PAAM outperforms state-of-the-art structured-pruning\nmethods (SOTA). On CIFAR-10 dataset, without requiring a pre-trained baseline\nnetwork, we obtain 1.02% and 1.19% accuracy gain and 52.3% and 54% parameters\nreduction, on ResNet56 and ResNet110, respectively. Similarly, on the ImageNet\ndataset, PAAM achieves 1.06% accuracy gain while pruning 51.1% of the\nparameters on ResNet50. For Cifar-10, this is better than the SOTA with a\nmargin of 9.5% and 6.6%, respectively, and on ImageNet with a margin of 11%.\n",
        "published": "2022",
        "authors": [
            "Zahra Babaiee",
            "Lucas Liebenwein",
            "Ramin Hasani",
            "Daniela Rus",
            "Radu Grosu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.14499v1",
        "title": "Deep neuroevolution for limited, heterogeneous data: proof-of-concept\n  application to Neuroblastoma brain metastasis using a small virtual pooled\n  image collection",
        "abstract": "  Artificial intelligence (AI) in radiology has made great strides in recent\nyears, but many hurdles remain. Overfitting and lack of generalizability\nrepresent important ongoing challenges hindering accurate and dependable\nclinical deployment. If AI algorithms can avoid overfitting and achieve true\ngeneralizability, they can go from the research realm to the forefront of\nclinical work. Recently, small data AI approaches such as deep neuroevolution\n(DNE) have avoided overfitting small training sets. We seek to address both\noverfitting and generalizability by applying DNE to a virtually pooled data set\nconsisting of images from various institutions. Our use case is classifying\nneuroblastoma brain metastases on MRI. Neuroblastoma is well-suited for our\ngoals because it is a rare cancer. Hence, studying this pediatric disease\nrequires a small data approach. As a tertiary care center, the neuroblastoma\nimages in our local Picture Archiving and Communication System (PACS) are\nlargely from outside institutions. These multi-institutional images provide a\nheterogeneous data set that can simulate real world clinical deployment. As in\nprior DNE work, we used a small training set, consisting of 30 normal and 30\nmetastasis-containing post-contrast MRI brain scans, with 37% outside images.\nThe testing set was enriched with 83% outside images. DNE converged to a\ntesting set accuracy of 97%. Hence, the algorithm was able to predict image\nclass with near-perfect accuracy on a testing set that simulates real-world\ndata. Hence, the work described here represents a considerable contribution\ntoward clinically feasible AI.\n",
        "published": "2022",
        "authors": [
            "Subhanik Purkayastha",
            "Hrithwik Shalu",
            "David Gutman",
            "Shakeel Modak",
            "Ellen Basu",
            "Brian Kushner",
            "Kim Kramer",
            "Sofia Haque",
            "Joseph Stember"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.06735v1",
        "title": "POPNASv3: a Pareto-Optimal Neural Architecture Search Solution for Image\n  and Time Series Classification",
        "abstract": "  The automated machine learning (AutoML) field has become increasingly\nrelevant in recent years. These algorithms can develop models without the need\nfor expert knowledge, facilitating the application of machine learning\ntechniques in the industry. Neural Architecture Search (NAS) exploits deep\nlearning techniques to autonomously produce neural network architectures whose\nresults rival the state-of-the-art models hand-crafted by AI experts. However,\nthis approach requires significant computational resources and hardware\ninvestments, making it less appealing for real-usage applications. This article\npresents the third version of Pareto-Optimal Progressive Neural Architecture\nSearch (POPNASv3), a new sequential model-based optimization NAS algorithm\ntargeting different hardware environments and multiple classification tasks.\nOur method is able to find competitive architectures within large search\nspaces, while keeping a flexible structure and data processing pipeline to\nadapt to different tasks. The algorithm employs Pareto optimality to reduce the\nnumber of architectures sampled during the search, drastically improving the\ntime efficiency without loss in accuracy. The experiments performed on images\nand time series classification datasets provide evidence that POPNASv3 can\nexplore a large set of assorted operators and converge to optimal architectures\nsuited for the type of data provided under different scenarios.\n",
        "published": "2022",
        "authors": [
            "Andrea Falanti",
            "Eugenio Lomurno",
            "Danilo Ardagna",
            "Matteo Matteucci"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.00620v1",
        "title": "Dynamically Modular and Sparse General Continual Learning",
        "abstract": "  Real-world applications often require learning continuously from a stream of\ndata under ever-changing conditions. When trying to learn from such\nnon-stationary data, deep neural networks (DNNs) undergo catastrophic\nforgetting of previously learned information. Among the common approaches to\navoid catastrophic forgetting, rehearsal-based methods have proven effective.\nHowever, they are still prone to forgetting due to task-interference as all\nparameters respond to all tasks. To counter this, we take inspiration from\nsparse coding in the brain and introduce dynamic modularity and sparsity\n(Dynamos) for rehearsal-based general continual learning. In this setup, the\nDNN learns to respond to stimuli by activating relevant subsets of neurons. We\ndemonstrate the effectiveness of Dynamos on multiple datasets under challenging\ncontinual learning evaluation protocols. Finally, we show that our method\nlearns representations that are modular and specialized, while maintaining\nreusability by activating subsets of neurons with overlaps corresponding to the\nsimilarity of stimuli.\n",
        "published": "2023",
        "authors": [
            "Arnav Varma",
            "Elahe Arani",
            "Bahram Zonooz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.02464v1",
        "title": "Architect, Regularize and Replay (ARR): a Flexible Hybrid Approach for\n  Continual Learning",
        "abstract": "  In recent years we have witnessed a renewed interest in machine learning\nmethodologies, especially for deep representation learning, that could overcome\nbasic i.i.d. assumptions and tackle non-stationary environments subject to\nvarious distributional shifts or sample selection biases. Within this context,\nseveral computational approaches based on architectural priors, regularizers\nand replay policies have been proposed with different degrees of success\ndepending on the specific scenario in which they were developed and assessed.\nHowever, designing comprehensive hybrid solutions that can flexibly and\ngenerally be applied with tunable efficiency-effectiveness trade-offs still\nseems a distant goal. In this paper, we propose \"Architect, Regularize and\nReplay\" (ARR), an hybrid generalization of the renowned AR1 algorithm and its\nvariants, that can achieve state-of-the-art results in classic scenarios (e.g.\nclass-incremental learning) but also generalize to arbitrary data streams\ngenerated from real-world datasets such as CIFAR-100, CORe50 and ImageNet-1000.\n",
        "published": "2023",
        "authors": [
            "Vincenzo Lomonaco",
            "Lorenzo Pellegrini",
            "Gabriele Graffieti",
            "Davide Maltoni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.05058v1",
        "title": "Sparse Coding in a Dual Memory System for Lifelong Learning",
        "abstract": "  Efficient continual learning in humans is enabled by a rich set of\nneurophysiological mechanisms and interactions between multiple memory systems.\nThe brain efficiently encodes information in non-overlapping sparse codes,\nwhich facilitates the learning of new associations faster with controlled\ninterference with previous associations. To mimic sparse coding in DNNs, we\nenforce activation sparsity along with a dropout mechanism which encourages the\nmodel to activate similar units for semantically similar inputs and have less\noverlap with activation patterns of semantically dissimilar inputs. This\nprovides us with an efficient mechanism for balancing the reusability and\ninterference of features, depending on the similarity of classes across tasks.\nFurthermore, we employ sparse coding in a multiple-memory replay mechanism. Our\nmethod maintains an additional long-term semantic memory that aggregates and\nconsolidates information encoded in the synaptic weights of the working model.\nOur extensive evaluation and characteristics analysis show that equipped with\nthese biologically inspired mechanisms, the model can further mitigate\nforgetting.\n",
        "published": "2022",
        "authors": [
            "Fahad Sarfraz",
            "Elahe Arani",
            "Bahram Zonooz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.01888v1",
        "title": "Enhancing Once-For-All: A Study on Parallel Blocks, Skip Connections and\n  Early Exits",
        "abstract": "  The use of Neural Architecture Search (NAS) techniques to automate the design\nof neural networks has become increasingly popular in recent years. The\nproliferation of devices with different hardware characteristics using such\nneural networks, as well as the need to reduce the power consumption for their\nsearch, has led to the realisation of Once-For-All (OFA), an eco-friendly\nalgorithm characterised by the ability to generate easily adaptable models\nthrough a single learning process. In order to improve this paradigm and\ndevelop high-performance yet eco-friendly NAS techniques, this paper presents\nOFAv2, the extension of OFA aimed at improving its performance while\nmaintaining the same ecological advantage. The algorithm is improved from an\narchitectural point of view by including early exits, parallel blocks and dense\nskip connections. The training process is extended by two new phases called\nElastic Level and Elastic Height. A new Knowledge Distillation technique is\npresented to handle multi-output networks, and finally a new strategy for\ndynamic teacher network selection is proposed. These modifications allow OFAv2\nto improve its accuracy performance on the Tiny ImageNet dataset by up to\n12.07% compared to the original version of OFA, while maintaining the algorithm\nflexibility and advantages.\n",
        "published": "2023",
        "authors": [
            "Simone Sarti",
            "Eugenio Lomurno",
            "Andrea Falanti",
            "Matteo Matteucci"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.13019v1",
        "title": "A Unified Framework for Soft Threshold Pruning",
        "abstract": "  Soft threshold pruning is among the cutting-edge pruning methods with\nstate-of-the-art performance. However, previous methods either perform aimless\nsearching on the threshold scheduler or simply set the threshold trainable,\nlacking theoretical explanation from a unified perspective. In this work, we\nreformulate soft threshold pruning as an implicit optimization problem solved\nusing the Iterative Shrinkage-Thresholding Algorithm (ISTA), a classic method\nfrom the fields of sparse recovery and compressed sensing. Under this\ntheoretical framework, all threshold tuning strategies proposed in previous\nstudies of soft threshold pruning are concluded as different styles of tuning\n$L_1$-regularization term. We further derive an optimal threshold scheduler\nthrough an in-depth study of threshold scheduling based on our framework. This\nscheduler keeps $L_1$-regularization coefficient stable, implying a\ntime-invariant objective function from the perspective of optimization. In\nprinciple, the derived pruning algorithm could sparsify any mathematical model\ntrained via SGD. We conduct extensive experiments and verify its\nstate-of-the-art performance on both Artificial Neural Networks (ResNet-50 and\nMobileNet-V1) and Spiking Neural Networks (SEW ResNet-18) on ImageNet datasets.\nOn the basis of this framework, we derive a family of pruning methods,\nincluding sparsify-during-training, early pruning, and pruning at\ninitialization. The code is available at https://github.com/Yanqi-Chen/LATS.\n",
        "published": "2023",
        "authors": [
            "Yanqi Chen",
            "Zhengyu Ma",
            "Wei Fang",
            "Xiawu Zheng",
            "Zhaofei Yu",
            "Yonghong Tian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.01584v2",
        "title": "Evolutionary Augmentation Policy Optimization for Self-supervised\n  Learning",
        "abstract": "  Self-supervised Learning (SSL) is a machine learning algorithm for\npretraining Deep Neural Networks (DNNs) without requiring manually labeled\ndata. The central idea of this learning technique is based on an auxiliary\nstage aka pretext task in which labeled data are created automatically through\ndata augmentation and exploited for pretraining the DNN. However, the effect of\neach pretext task is not well studied or compared in the literature. In this\npaper, we study the contribution of augmentation operators on the performance\nof self supervised learning algorithms in a constrained settings. We propose an\nevolutionary search method for optimization of data augmentation pipeline in\npretext tasks and measure the impact of augmentation operators in several SOTA\nSSL algorithms. By encoding different combination of augmentation operators in\nchromosomes we seek the optimal augmentation policies through an evolutionary\noptimization mechanism. We further introduce methods for analyzing and\nexplaining the performance of optimized SSL algorithms. Our results indicate\nthat our proposed method can find solutions that outperform the accuracy of\nclassification of SSL algorithms which confirms the influence of augmentation\npolicy choice on the overall performance of SSL algorithms. We also compare\noptimal SSL solutions found by our evolutionary search mechanism and show the\neffect of batch size in the pretext task on two visual datasets.\n",
        "published": "2023",
        "authors": [
            "Noah Barrett",
            "Zahra Sadeghi",
            "Stan Matwin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.02034v2",
        "title": "Linear CNNs Discover the Statistical Structure of the Dataset Using Only\n  the Most Dominant Frequencies",
        "abstract": "  We here present a stepping stone towards a deeper understanding of\nconvolutional neural networks (CNNs) in the form of a theory of learning in\nlinear CNNs. Through analyzing the gradient descent equations, we discover that\nthe evolution of the network during training is determined by the interplay\nbetween the dataset structure and the convolutional network structure. We show\nthat linear CNNs discover the statistical structure of the dataset with\nnon-linear, ordered, stage-like transitions, and that the speed of discovery\nchanges depending on the relationship between the dataset and the convolutional\nnetwork structure. Moreover, we find that this interplay lies at the heart of\nwhat we call the ``dominant frequency bias'', where linear CNNs arrive at these\ndiscoveries using only the dominant frequencies of the different structural\nparts present in the dataset. We furthermore provide experiments that show how\nour theory relates to deep, non-linear CNNs used in practice. Our findings shed\nnew light on the inner working of CNNs, and can help explain their shortcut\nlearning and their tendency to rely on texture instead of shape.\n",
        "published": "2023",
        "authors": [
            "Hannah Pinson",
            "Joeri Lenaerts",
            "Vincent Ginis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.02655v1",
        "title": "On Modifying a Neural Network's Perception",
        "abstract": "  Artificial neural networks have proven to be extremely useful models that\nhave allowed for multiple recent breakthroughs in the field of Artificial\nIntelligence and many others. However, they are typically regarded as black\nboxes, given how difficult it is for humans to interpret how these models reach\ntheir results. In this work, we propose a method which allows one to modify\nwhat an artificial neural network is perceiving regarding specific\nhuman-defined concepts, enabling the generation of hypothetical scenarios that\ncould help understand and even debug the neural network model. Through\nempirical evaluation, in a synthetic dataset and in the ImageNet dataset, we\ntest the proposed method on different models, assessing whether the performed\nmanipulations are well interpreted by the models, and analyzing how they react\nto them.\n",
        "published": "2023",
        "authors": [
            "Manuel de Sousa Ribeiro",
            "Jo\u00e3o Leite"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.13683v1",
        "title": "OFA$^2$: A Multi-Objective Perspective for the Once-for-All Neural\n  Architecture Search",
        "abstract": "  Once-for-All (OFA) is a Neural Architecture Search (NAS) framework designed\nto address the problem of searching efficient architectures for devices with\ndifferent resources constraints by decoupling the training and the searching\nstages. The computationally expensive process of training the OFA neural\nnetwork is done only once, and then it is possible to perform multiple searches\nfor subnetworks extracted from this trained network according to each\ndeployment scenario. In this work we aim to give one step further in the search\nfor efficiency by explicitly conceiving the search stage as a multi-objective\noptimization problem. A Pareto frontier is then populated with efficient, and\nalready trained, neural architectures exhibiting distinct trade-offs among the\nconflicting objectives. This could be achieved by using any multi-objective\nevolutionary algorithm during the search stage, such as NSGA-II and SMS-EMOA.\nIn other words, the neural network is trained once, the searching for\nsubnetworks considering different hardware constraints is also done one single\ntime, and then the user can choose a suitable neural network according to each\ndeployment scenario. The conjugation of OFA and an explicit algorithm for\nmulti-objective optimization opens the possibility of a posteriori\ndecision-making in NAS, after sampling efficient subnetworks which are a very\ngood approximation of the Pareto frontier, given that those subnetworks are\nalready trained and ready to use. The source code and the final search\nalgorithm will be released at https://github.com/ito-rafael/once-for-all-2\n",
        "published": "2023",
        "authors": [
            "Rafael C. Ito",
            "Fernando J. Von Zuben"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.15888v1",
        "title": "Projected Latent Distillation for Data-Agnostic Consolidation in\n  Distributed Continual Learning",
        "abstract": "  Distributed learning on the edge often comprises self-centered devices (SCD)\nwhich learn local tasks independently and are unwilling to contribute to the\nperformance of other SDCs. How do we achieve forward transfer at zero cost for\nthe single SCDs? We formalize this problem as a Distributed Continual Learning\nscenario, where SCD adapt to local tasks and a CL model consolidates the\nknowledge from the resulting stream of models without looking at the SCD's\nprivate data. Unfortunately, current CL methods are not directly applicable to\nthis scenario. We propose Data-Agnostic Consolidation (DAC), a novel double\nknowledge distillation method that consolidates the stream of SC models without\nusing the original data. DAC performs distillation in the latent space via a\nnovel Projected Latent Distillation loss. Experimental results show that DAC\nenables forward transfer between SCDs and reaches state-of-the-art accuracy on\nSplit CIFAR100, CORe50 and Split TinyImageNet, both in reharsal-free and\ndistributed CL scenarios. Somewhat surprisingly, even a single\nout-of-distribution image is sufficient as the only source of data during\nconsolidation.\n",
        "published": "2023",
        "authors": [
            "Antonio Carta",
            "Andrea Cossu",
            "Vincenzo Lomonaco",
            "Davide Bacciu",
            "Joost van de Weijer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.06738v1",
        "title": "A Study of Biologically Plausible Neural Network: The Role and\n  Interactions of Brain-Inspired Mechanisms in Continual Learning",
        "abstract": "  Humans excel at continually acquiring, consolidating, and retaining\ninformation from an ever-changing environment, whereas artificial neural\nnetworks (ANNs) exhibit catastrophic forgetting. There are considerable\ndifferences in the complexity of synapses, the processing of information, and\nthe learning mechanisms in biological neural networks and their artificial\ncounterparts, which may explain the mismatch in performance. We consider a\nbiologically plausible framework that constitutes separate populations of\nexclusively excitatory and inhibitory neurons that adhere to Dale's principle,\nand the excitatory pyramidal neurons are augmented with dendritic-like\nstructures for context-dependent processing of stimuli. We then conduct a\ncomprehensive study on the role and interactions of different mechanisms\ninspired by the brain, including sparse non-overlapping representations,\nHebbian learning, synaptic consolidation, and replay of past activations that\naccompanied the learning event. Our study suggests that the employing of\nmultiple complementary mechanisms in a biologically plausible architecture,\nsimilar to the brain, may be effective in enabling continual learning in ANNs.\n",
        "published": "2023",
        "authors": [
            "Fahad Sarfraz",
            "Elahe Arani",
            "Bahram Zonooz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.10191v1",
        "title": "Efficient Uncertainty Estimation in Spiking Neural Networks via\n  MC-dropout",
        "abstract": "  Spiking neural networks (SNNs) have gained attention as models of sparse and\nevent-driven communication of biological neurons, and as such have shown\nincreasing promise for energy-efficient applications in neuromorphic hardware.\nAs with classical artificial neural networks (ANNs), predictive uncertainties\nare important for decision making in high-stakes applications, such as\nautonomous vehicles, medical diagnosis, and high frequency trading. Yet,\ndiscussion of uncertainty estimation in SNNs is limited, and approaches for\nuncertainty estimation in artificial neural networks (ANNs) are not directly\napplicable to SNNs. Here, we propose an efficient Monte Carlo(MC)-dropout based\napproach for uncertainty estimation in SNNs. Our approach exploits the\ntime-step mechanism of SNNs to enable MC-dropout in a computationally efficient\nmanner, without introducing significant overheads during training and inference\nwhile demonstrating high accuracy and uncertainty quality.\n",
        "published": "2023",
        "authors": [
            "Tao Sun",
            "Bojian Yin",
            "Sander Bohte"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.00441v1",
        "title": "Multi-Task Structural Learning using Local Task Similarity induced\n  Neuron Creation and Removal",
        "abstract": "  Multi-task learning has the potential to improve generalization by maximizing\npositive transfer between tasks while reducing task interference. Fully\nachieving this potential is hindered by manually designed architectures that\nremain static throughout training. On the contrary, learning in the brain\noccurs through structural changes that are in tandem with changes in synaptic\nstrength. Thus, we propose \\textit{Multi-Task Structural Learning (MTSL)} that\nsimultaneously learns the multi-task architecture and its parameters. MTSL\nbegins with an identical single-task network for each task and alternates\nbetween a task-learning phase and a structural-learning phase. In the task\nlearning phase, each network specializes in the corresponding task. In each of\nthe structural learning phases, starting from the earliest layer, locally\nsimilar task layers first transfer their knowledge to a newly created group\nlayer before being removed. MTSL then uses the group layer in place of the\ncorresponding removed task layers and moves on to the next layers. Our\nempirical results show that MTSL achieves competitive generalization with\nvarious baselines and improves robustness to out-of-distribution data.\n",
        "published": "2023",
        "authors": [
            "Naresh Kumar Gurulingan",
            "Bahram Zonooz",
            "Elahe Arani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.12073v2",
        "title": "GELU Activation Function in Deep Learning: A Comprehensive Mathematical\n  Analysis and Performance",
        "abstract": "  Selecting the most suitable activation function is a critical factor in the\neffectiveness of deep learning models, as it influences their learning\ncapacity, stability, and computational efficiency. In recent years, the\nGaussian Error Linear Unit (GELU) activation function has emerged as a dominant\nmethod, surpassing traditional functions such as the Rectified Linear Unit\n(ReLU) in various applications. This study presents a rigorous mathematical\ninvestigation of the GELU activation function, exploring its differentiability,\nboundedness, stationarity, and smoothness properties in detail. Additionally,\nwe conduct an extensive experimental comparison of the GELU function against a\nbroad range of alternative activation functions, utilizing a residual\nconvolutional network trained on the CIFAR-10, CIFAR-100, and STL-10 datasets\nas the empirical testbed. Our results demonstrate the superior performance of\nGELU compared to other activation functions, establishing its suitability for a\nwide range of deep learning applications. This comprehensive study contributes\nto a more profound understanding of the underlying mathematical properties of\nGELU and provides valuable insights for practitioners aiming to select\nactivation functions that optimally align with their specific objectives and\nconstraints in deep learning.\n",
        "published": "2023",
        "authors": [
            "Minhyeok Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.16347v1",
        "title": "Prompt Evolution for Generative AI: A Classifier-Guided Approach",
        "abstract": "  Synthesis of digital artifacts conditioned on user prompts has become an\nimportant paradigm facilitating an explosion of use cases with generative AI.\nHowever, such models often fail to connect the generated outputs and desired\ntarget concepts/preferences implied by the prompts. Current research addressing\nthis limitation has largely focused on enhancing the prompts before output\ngeneration or improving the model's performance up front. In contrast, this\npaper conceptualizes prompt evolution, imparting evolutionary selection\npressure and variation during the generative process to produce multiple\noutputs that satisfy the target concepts/preferences better. We propose a\nmulti-objective instantiation of this broader idea that uses a multi-label\nimage classifier-guided approach. The predicted labels from the classifiers\nserve as multiple objectives to optimize, with the aim of producing diversified\nimages that meet user preferences. A novelty of our evolutionary algorithm is\nthat the pre-trained generative model gives us implicit mutation operations,\nleveraging the model's stochastic generative capability to automate the\ncreation of Pareto-optimized images more faithful to user preferences.\n",
        "published": "2023",
        "authors": [
            "Melvin Wong",
            "Yew-Soon Ong",
            "Abhishek Gupta",
            "Kavitesh K. Bali",
            "Caishun Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.03798v1",
        "title": "CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using\n  Latent Variable Evolution",
        "abstract": "  Models leveraging both visual and textual data such as Contrastive\nLanguage-Image Pre-training (CLIP), are increasingly gaining importance. In\nthis work, we show that despite their versatility, such models are vulnerable\nto what we refer to as fooling master images. Fooling master images are capable\nof maximizing the confidence score of a CLIP model for a significant number of\nwidely varying prompts, while being unrecognizable for humans. We demonstrate\nhow fooling master images can be mined by searching the latent space of\ngenerative models by means of an evolution strategy or stochastic gradient\ndescent. We investigate the properties of the mined fooling master images, and\nfind that images trained on a small number of image captions potentially\ngeneralize to a much larger number of semantically related captions. Further,\nwe evaluate two possible mitigation strategies and find that vulnerability to\nfooling master examples is closely related to a modality gap in contrastive\npre-trained multi-modal networks. From the perspective of vulnerability to\noff-manifold attacks, we therefore argue for the mitigation of modality gaps in\nCLIP and related multi-modal approaches. Source code and mined CLIPMasterPrints\nare available at https://github.com/matfrei/CLIPMasterPrints.\n",
        "published": "2023",
        "authors": [
            "Matthias Freiberger",
            "Peter Kun",
            "Anders Sundnes L\u00f8vlie",
            "Sebastian Risi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.16235v1",
        "title": "Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A\n  Survey",
        "abstract": "  For a long time, biology and neuroscience fields have been a great source of\ninspiration for computer scientists, towards the development of Artificial\nIntelligence (AI) technologies. This survey aims at providing a comprehensive\nreview of recent biologically-inspired approaches for AI. After introducing the\nmain principles of computation and synaptic plasticity in biological neurons,\nwe provide a thorough presentation of Spiking Neural Network (SNN) models, and\nwe highlight the main challenges related to SNN training, where traditional\nbackprop-based optimization is not directly applicable. Therefore, we discuss\nrecent bio-inspired training methods, which pose themselves as alternatives to\nbackprop, both for traditional and spiking networks. Bio-Inspired Deep Learning\n(BIDL) approaches towards advancing the computational capabilities and\nbiological plausibility of current models.\n",
        "published": "2023",
        "authors": [
            "Gabriele Lagani",
            "Fabrizio Falchi",
            "Claudio Gennaro",
            "Giuseppe Amato"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.16236v1",
        "title": "Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning:\n  A Survey",
        "abstract": "  Recently emerged technologies based on Deep Learning (DL) achieved\noutstanding results on a variety of tasks in the field of Artificial\nIntelligence (AI). However, these encounter several challenges related to\nrobustness to adversarial inputs, ecological impact, and the necessity of huge\namounts of training data. In response, researchers are focusing more and more\ninterest on biologically grounded mechanisms, which are appealing due to the\nimpressive capabilities exhibited by biological brains. This survey explores a\nrange of these biologically inspired models of synaptic plasticity, their\napplication in DL scenarios, and the connections with models of plasticity in\nSpiking Neural Networks (SNNs). Overall, Bio-Inspired Deep Learning (BIDL)\nrepresents an exciting research direction, aiming at advancing not only our\ncurrent technologies but also our understanding of intelligence.\n",
        "published": "2023",
        "authors": [
            "Gabriele Lagani",
            "Fabrizio Falchi",
            "Claudio Gennaro",
            "Giuseppe Amato"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.06046v1",
        "title": "BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise",
        "abstract": "  The negative impact of label noise is well studied in classical supervised\nlearning yet remains an open research question in meta-learning. Meta-learners\naim to adapt to unseen learning tasks by learning a good initial model in\nmeta-training and consecutively fine-tuning it according to new tasks during\nmeta-testing. In this paper, we present the first extensive analysis of the\nimpact of varying levels of label noise on the performance of state-of-the-art\nmeta-learners, specifically gradient-based $N$-way $K$-shot learners. We show\nthat the accuracy of Reptile, iMAML, and foMAML drops by up to 42% on the\nOmniglot and CifarFS datasets when meta-training is affected by label noise. To\nstrengthen the resilience against label noise, we propose two sampling\ntechniques, namely manifold (Man) and batch manifold (BatMan), which transform\nthe noisy supervised learners into semi-supervised ones to increase the utility\nof noisy labels. We first construct manifold samples of $N$-way\n$2$-contrastive-shot tasks through augmentation, learning the embedding via a\ncontrastive loss in meta-training, and then perform classification through\nzeroing on the embedding in meta-testing. We show that our approach can\neffectively mitigate the impact of meta-training label noise. Even with 60%\nwrong labels \\batman and \\man can limit the meta-testing accuracy drop to\n${2.5}$, ${9.4}$, ${1.1}$ percent points, respectively, with existing\nmeta-learners across the Omniglot, CifarFS, and MiniImagenet datasets.\n",
        "published": "2023",
        "authors": [
            "Jeroen M. Galjaard",
            "Robert Birke",
            "Juan Perez",
            "Lydia Y. Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.17042v1",
        "title": "StochGradAdam: Accelerating Neural Networks Training with Stochastic\n  Gradient Sampling",
        "abstract": "  In the rapidly advancing domain of deep learning optimization, this paper\nunveils the StochGradAdam optimizer, a novel adaptation of the well-regarded\nAdam algorithm. Central to StochGradAdam is its gradient sampling technique.\nThis method not only ensures stable convergence but also leverages the\nadvantages of selective gradient consideration, fostering robust training by\npotentially mitigating the effects of noisy or outlier data and enhancing the\nexploration of the loss landscape for more dependable convergence. In both\nimage classification and segmentation tasks, StochGradAdam has demonstrated\nsuperior performance compared to the traditional Adam optimizer. By judiciously\nsampling a subset of gradients at each iteration, the optimizer is optimized\nfor managing intricate models. The paper provides a comprehensive exploration\nof StochGradAdam's methodology, from its mathematical foundations to bias\ncorrection strategies, heralding a promising advancement in deep learning\ntraining techniques.\n",
        "published": "2023",
        "authors": [
            "Juyoung Yun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.01617v1",
        "title": "Look-Ahead Selective Plasticity for Continual Learning of Visual Tasks",
        "abstract": "  Contrastive representation learning has emerged as a promising technique for\ncontinual learning as it can learn representations that are robust to\ncatastrophic forgetting and generalize well to unseen future tasks. Previous\nwork in continual learning has addressed forgetting by using previous task data\nand trained models. Inspired by event models created and updated in the brain,\nwe propose a new mechanism that takes place during task boundaries, i.e., when\none task finishes and another starts. By observing the redundancy-inducing\nability of contrastive loss on the output of a neural network, our method\nleverages the first few samples of the new task to identify and retain\nparameters contributing most to the transfer ability of the neural network,\nfreeing up the remaining parts of the network to learn new features. We\nevaluate the proposed methods on benchmark computer vision datasets including\nCIFAR10 and TinyImagenet and demonstrate state-of-the-art performance in the\ntask-incremental, class-incremental, and domain-incremental continual learning\nscenarios.\n",
        "published": "2023",
        "authors": [
            "Rouzbeh Meshkinnejad",
            "Jie Mei",
            "Daniel Lizotte",
            "Yalda Mohsenzadeh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.07485v1",
        "title": "EvoFed: Leveraging Evolutionary Strategies for Communication-Efficient\n  Federated Learning",
        "abstract": "  Federated Learning (FL) is a decentralized machine learning paradigm that\nenables collaborative model training across dispersed nodes without having to\nforce individual nodes to share data. However, its broad adoption is hindered\nby the high communication costs of transmitting a large number of model\nparameters. This paper presents EvoFed, a novel approach that integrates\nEvolutionary Strategies (ES) with FL to address these challenges. EvoFed\nemploys a concept of 'fitness-based information sharing', deviating\nsignificantly from the conventional model-based FL. Rather than exchanging the\nactual updated model parameters, each node transmits a distance-based\nsimilarity measure between the locally updated model and each member of the\nnoise-perturbed model population. Each node, as well as the server, generates\nan identical population set of perturbed models in a completely synchronized\nfashion using the same random seeds. With properly chosen noise variance and\npopulation size, perturbed models can be combined to closely reflect the actual\nmodel updated using the local dataset, allowing the transmitted similarity\nmeasures (or fitness values) to carry nearly the complete information about the\nmodel parameters. As the population size is typically much smaller than the\nnumber of model parameters, the savings in communication load is large. The\nserver aggregates these fitness values and is able to update the global model.\nThis global fitness vector is then disseminated back to the nodes, each of\nwhich applies the same update to be synchronized to the global model. Our\nanalysis shows that EvoFed converges, and our experimental results validate\nthat at the cost of increased local processing loads, EvoFed achieves\nperformance comparable to FedAvg while reducing overall communication\nrequirements drastically in various practical settings.\n",
        "published": "2023",
        "authors": [
            "Mohammad Mahdi Rahimi",
            "Hasnain Irshad Bhatti",
            "Younghyun Park",
            "Humaira Kousar",
            "Jaekyun Moon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.13664v1",
        "title": "Sample as You Infer: Predictive Coding With Langevin Dynamics",
        "abstract": "  We present a novel algorithm for parameter learning in generic deep\ngenerative models that builds upon the predictive coding (PC) framework of\ncomputational neuroscience. Our approach modifies the standard PC algorithm to\nbring performance on-par and exceeding that obtained from standard variational\nauto-encoder (VAE) training. By injecting Gaussian noise into the PC inference\nprocedure we re-envision it as an overdamped Langevin sampling, which\nfacilitates optimisation with respect to a tight evidence lower bound (ELBO).\nWe improve the resultant encoder-free training method by incorporating an\nencoder network to provide an amortised warm-start to our Langevin sampling and\ntest three different objectives for doing so. Finally, to increase robustness\nto the sampling step size and reduce sensitivity to curvature, we validate a\nlightweight and easily computable form of preconditioning, inspired by Riemann\nManifold Langevin and adaptive optimizers from the SGD literature. We compare\nagainst VAEs by training like-for-like generative models using our technique\nagainst those trained with standard reparameterisation-trick-based ELBOs. We\nobserve our method out-performs or matches performance across a number of\nmetrics, including sample quality, while converging in a fraction of the number\nof SGD training iterations.\n",
        "published": "2023",
        "authors": [
            "Umais Zahid",
            "Qinghai Guo",
            "Zafeirios Fountas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.16141v1",
        "title": "Brain-Inspired Efficient Pruning: Exploiting Criticality in Spiking\n  Neural Networks",
        "abstract": "  Spiking Neural Networks (SNNs) have been an attractive option for deployment\non devices with limited computing resources and lower power consumption because\nof the event-driven computing characteristic. As such devices have limited\ncomputing and storage resources, pruning for SNNs has been widely focused\nrecently. However, the binary and non-differentiable property of spike signals\nmake pruning deep SNNs challenging, so existing methods require high time\noverhead to make pruning decisions. In this paper, inspired by critical brain\nhypothesis in neuroscience, we design a regeneration mechanism based on\ncriticality to efficiently obtain the critical pruned networks. Firstly, we\npropose a low-cost metric for the criticality of pruning structures. Then we\nre-rank the pruned structures after pruning and regenerate those with higher\ncriticality. We evaluate our method using VGG-16 and ResNet-19 for both\nunstructured pruning and structured pruning. Our method achieves higher\nperformance compared to current state-of-the-art (SOTA) method with the same\ntime overhead. We also achieve comparable performances (even better on VGG-16)\ncompared to the SOTA method with 11.3x and 15.5x acceleration. Moreover, we\ninvestigate underlying mechanism of our method and find that it efficiently\nselects potential structures, learns the consistent feature representations and\nreduces the overfitting during the recovery phase.\n",
        "published": "2023",
        "authors": [
            "Shuo Chen",
            "Boxiao Liu",
            "Haihang You"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.02843v1",
        "title": "Are Vision Transformers More Data Hungry Than Newborn Visual Systems?",
        "abstract": "  Vision transformers (ViTs) are top performing models on many computer vision\nbenchmarks and can accurately predict human behavior on object recognition\ntasks. However, researchers question the value of using ViTs as models of\nbiological learning because ViTs are thought to be more data hungry than\nbrains, with ViTs requiring more training data to reach similar levels of\nperformance. To test this assumption, we directly compared the learning\nabilities of ViTs and animals, by performing parallel controlled rearing\nexperiments on ViTs and newborn chicks. We first raised chicks in impoverished\nvisual environments containing a single object, then simulated the training\ndata available in those environments by building virtual animal chambers in a\nvideo game engine. We recorded the first-person images acquired by agents\nmoving through the virtual chambers and used those images to train self\nsupervised ViTs that leverage time as a teaching signal, akin to biological\nvisual systems. When ViTs were trained through the eyes of newborn chicks, the\nViTs solved the same view invariant object recognition tasks as the chicks.\nThus, ViTs were not more data hungry than newborn visual systems: both learned\nview invariant object representations in impoverished visual environments. The\nflexible and generic attention based learning mechanism in ViTs combined with\nthe embodied data streams available to newborn animals appears sufficient to\ndrive the development of animal-like object recognition.\n",
        "published": "2023",
        "authors": [
            "Lalit Pandey",
            "Samantha M. W. Wood",
            "Justin N. Wood"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.05721v1",
        "title": "Learning Spatially-Continuous Fiber Orientation Functions",
        "abstract": "  Our understanding of the human connectome is fundamentally limited by the\nresolution of diffusion MR images. Reconstructing a connectome's constituent\nneural pathways with tractography requires following a continuous field of\nfiber directions. Typically, this field is found with simple trilinear\ninterpolation in low-resolution, noisy diffusion MRIs. However, trilinear\ninterpolation struggles following fine-scale changes in low-quality data.\nRecent deep learning methods in super-resolving diffusion MRIs have focused on\nupsampling to a fixed spatial grid, but this does not satisfy tractography's\nneed for a continuous field. In this work, we propose FENRI, a novel method\nthat learns spatially-continuous fiber orientation density functions from\nlow-resolution diffusion-weighted images. To quantify FENRI's capabilities in\ntractography, we also introduce an expanded simulated dataset built for\nevaluating deep-learning tractography models. We demonstrate that FENRI\naccurately predicts high-resolution fiber orientations from realistic\nlow-quality data, and that FENRI-based tractography offers improved streamline\nreconstruction over the current use of trilinear interpolation.\n",
        "published": "2023",
        "authors": [
            "Tyler Spears",
            "P. Thomas Fletcher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.07466v1",
        "title": "Efficient Object Detection in Autonomous Driving using Spiking Neural\n  Networks: Performance, Energy Consumption Analysis, and Insights into\n  Open-set Object Discovery",
        "abstract": "  Besides performance, efficiency is a key design driver of technologies\nsupporting vehicular perception. Indeed, a well-balanced trade-off between\nperformance and energy consumption is crucial for the sustainability of\nautonomous vehicles. In this context, the diversity of real-world contexts in\nwhich autonomous vehicles can operate motivates the need for empowering\nperception models with the capability to detect, characterize and identify\nnewly appearing objects by themselves. In this manuscript we elaborate on this\nthreefold conundrum (performance, efficiency and open-world learning) for\nobject detection modeling tasks over image data collected from vehicular\nscenarios. Specifically, we show that well-performing and efficient models can\nbe realized by virtue of Spiking Neural Networks (SNNs), reaching competitive\nlevels of detection performance when compared to their non-spiking counterparts\nat dramatic energy consumption savings (up to 85%) and a slightly improved\nrobustness against image noise. Our experiments herein offered also expose\nqualitatively the complexity of detecting new objects based on the preliminary\nresults of a simple approach to discriminate potential object proposals in the\ncaptured image.\n",
        "published": "2023",
        "authors": [
            "Aitor Martinez Seras",
            "Javier Del Ser",
            "Pablo Garcia-Bringas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.16792v1",
        "title": "RL-LOGO: Deep Reinforcement Learning Localization for Logo Recognition",
        "abstract": "  This paper proposes a novel logo image recognition approach incorporating a\nlocalization technique based on reinforcement learning. Logo recognition is an\nimage classification task identifying a brand in an image. As the size and\nposition of a logo vary widely from image to image, it is necessary to\ndetermine its position for accurate recognition. However, because there is no\nannotation for the position coordinates, it is impossible to train and infer\nthe location of the logo in the image. Therefore, we propose a deep\nreinforcement learning localization method for logo recognition (RL-LOGO). It\nutilizes deep reinforcement learning to identify a logo region in images\nwithout annotations of the positions, thereby improving classification\naccuracy. We demonstrated a significant improvement in accuracy compared with\nexisting methods in several published benchmarks. Specifically, we achieved an\n18-point accuracy improvement over competitive methods on the complex dataset\nLogo-2K+. This demonstrates that the proposed method is a promising approach to\nlogo recognition in real-world applications.\n",
        "published": "2023",
        "authors": [
            "Masato Fujitake"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.08623v1",
        "title": "Wake-Sleep Consolidated Learning",
        "abstract": "  We propose Wake-Sleep Consolidated Learning (WSCL), a learning strategy\nleveraging Complementary Learning System theory and the wake-sleep phases of\nthe human brain to improve the performance of deep neural networks for visual\nclassification tasks in continual learning settings. Our method learns\ncontinually via the synchronization between distinct wake and sleep phases.\nDuring the wake phase, the model is exposed to sensory input and adapts its\nrepresentations, ensuring stability through a dynamic parameter freezing\nmechanism and storing episodic memories in a short-term temporary memory\n(similarly to what happens in the hippocampus). During the sleep phase, the\ntraining process is split into NREM and REM stages. In the NREM stage, the\nmodel's synaptic weights are consolidated using replayed samples from the\nshort-term and long-term memory and the synaptic plasticity mechanism is\nactivated, strengthening important connections and weakening unimportant ones.\nIn the REM stage, the model is exposed to previously-unseen realistic visual\nsensory experience, and the dreaming process is activated, which enables the\nmodel to explore the potential feature space, thus preparing synapses to future\nknowledge. We evaluate the effectiveness of our approach on three benchmark\ndatasets: CIFAR-10, Tiny-ImageNet and FG-ImageNet. In all cases, our method\noutperforms the baselines and prior work, yielding a significant performance\ngain on continual visual classification tasks. Furthermore, we demonstrate the\nusefulness of all processing stages and the importance of dreaming to enable\npositive forward transfer.\n",
        "published": "2023",
        "authors": [
            "Amelia Sorrenti",
            "Giovanni Bellitto",
            "Federica Proietto Salanitri",
            "Matteo Pennisi",
            "Simone Palazzo",
            "Concetto Spampinato"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.06400v2",
        "title": "Learning a bidirectional mapping between human whole-body motion and\n  natural language using deep recurrent neural networks",
        "abstract": "  Linking human whole-body motion and natural language is of great interest for\nthe generation of semantic representations of observed human behaviors as well\nas for the generation of robot behaviors based on natural language input. While\nthere has been a large body of research in this area, most approaches that\nexist today require a symbolic representation of motions (e.g. in the form of\nmotion primitives), which have to be defined a-priori or require complex\nsegmentation algorithms. In contrast, recent advances in the field of neural\nnetworks and especially deep learning have demonstrated that sub-symbolic\nrepresentations that can be learned end-to-end usually outperform more\ntraditional approaches, for applications such as machine translation. In this\npaper we propose a generative model that learns a bidirectional mapping between\nhuman whole-body motion and natural language using deep recurrent neural\nnetworks (RNNs) and sequence-to-sequence learning. Our approach does not\nrequire any segmentation or manual feature engineering and learns a distributed\nrepresentation, which is shared for all motions and descriptions. We evaluate\nour approach on 2,846 human whole-body motions and 6,187 natural language\ndescriptions thereof from the KIT Motion-Language Dataset. Our results clearly\ndemonstrate the effectiveness of the proposed model: We show that our model\ngenerates a wide variety of realistic motions only from descriptions thereof in\nform of a single sentence. Conversely, our model is also capable of generating\ncorrect and detailed natural language descriptions from human motions.\n",
        "published": "2017",
        "authors": [
            "Matthias Plappert",
            "Christian Mandery",
            "Tamim Asfour"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.00697v3",
        "title": "Translating Natural Language Instructions for Behavioral Robot\n  Navigation with a Multi-Head Attention Mechanism",
        "abstract": "  We propose a multi-head attention mechanism as a blending layer in a neural\nnetwork model that translates natural language to a high level behavioral\nlanguage for indoor robot navigation. We follow the framework established by\n(Zang et al., 2018a) that proposes the use of a navigation graph as a knowledge\nbase for the task. Our results show significant performance gains when\ntranslating instructions on previously unseen environments, therefore,\nimproving the generalization capabilities of the model.\n",
        "published": "2020",
        "authors": [
            "Patricio Cerda-Mardini",
            "Vladimir Araujo",
            "Alvaro Soto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1612.07182v2",
        "title": "Multi-Agent Cooperation and the Emergence of (Natural) Language",
        "abstract": "  The current mainstream approach to train natural language systems is to\nexpose them to large amounts of text. This passive learning is problematic if\nwe are interested in developing interactive machines, such as conversational\nagents. We propose a framework for language learning that relies on multi-agent\ncommunication. We study this learning in the context of referential games. In\nthese games, a sender and a receiver see a pair of images. The sender is told\none of them is the target and is allowed to send a message from a fixed,\narbitrary vocabulary to the receiver. The receiver must rely on this message to\nidentify the target. Thus, the agents develop their own language interactively\nout of the need to communicate. We show that two networks with simple\nconfigurations are able to learn to coordinate in the referential game. We\nfurther explore how to make changes to the game environment to cause the \"word\nmeanings\" induced in the game to better reflect intuitive semantic properties\nof the images. In addition, we present a simple strategy for grounding the\nagents' code into natural language. Both of these are necessary steps towards\ndeveloping machines that are able to communicate with humans productively.\n",
        "published": "2016",
        "authors": [
            "Angeliki Lazaridou",
            "Alexander Peysakhovich",
            "Marco Baroni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1702.07492v1",
        "title": "Robot gains Social Intelligence through Multimodal Deep Reinforcement\n  Learning",
        "abstract": "  For robots to coexist with humans in a social world like ours, it is crucial\nthat they possess human-like social interaction skills. Programming a robot to\npossess such skills is a challenging task. In this paper, we propose a\nMultimodal Deep Q-Network (MDQN) to enable a robot to learn human-like\ninteraction skills through a trial and error method. This paper aims to develop\na robot that gathers data during its interaction with a human and learns human\ninteraction behaviour from the high-dimensional sensory information using\nend-to-end reinforcement learning. This paper demonstrates that the robot was\nable to learn basic interaction skills successfully, after 14 days of\ninteracting with people.\n",
        "published": "2017",
        "authors": [
            "Ahmed Hussain Qureshi",
            "Yutaka Nakamura",
            "Yuichiro Yoshikawa",
            "Hiroshi Ishiguro"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1702.08626v1",
        "title": "Show, Attend and Interact: Perceivable Human-Robot Social Interaction\n  through Neural Attention Q-Network",
        "abstract": "  For a safe, natural and effective human-robot social interaction, it is\nessential to develop a system that allows a robot to demonstrate the\nperceivable responsive behaviors to complex human behaviors. We introduce the\nMultimodal Deep Attention Recurrent Q-Network using which the robot exhibits\nhuman-like social interaction skills after 14 days of interacting with people\nin an uncontrolled real world. Each and every day during the 14 days, the\nsystem gathered robot interaction experiences with people through a\nhit-and-trial method and then trained the MDARQN on these experiences using\nend-to-end reinforcement learning approach. The results of interaction based\nlearning indicate that the robot has learned to respond to complex human\nbehaviors in a perceivable and socially acceptable manner.\n",
        "published": "2017",
        "authors": [
            "Ahmed Hussain Qureshi",
            "Yutaka Nakamura",
            "Yuichiro Yoshikawa",
            "Hiroshi Ishiguro"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.14491v1",
        "title": "Training Lightweight CNNs for Human-Nanodrone Proximity Interaction from\n  Small Datasets using Background Randomization",
        "abstract": "  We consider the task of visually estimating the pose of a human from images\nacquired by a nearby nano-drone; in this context, we propose a data\naugmentation approach based on synthetic background substitution to learn a\nlightweight CNN model from a small real-world training set. Experimental\nresults on data from two different labs proves that the approach improves\ngeneralization to unseen environments.\n",
        "published": "2021",
        "authors": [
            "Marco Ferri",
            "Dario Mantegazza",
            "Elia Cereda",
            "Nicky Zimmerman",
            "Luca M. Gambardella",
            "Daniele Palossi",
            "J\u00e9r\u00f4me Guzzi",
            "Alessandro Giusti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.14706v2",
        "title": "Sensing Anomalies as Potential Hazards: Datasets and Benchmarks",
        "abstract": "  We consider the problem of detecting, in the visual sensing data stream of an\nautonomous mobile robot, semantic patterns that are unusual (i.e., anomalous)\nwith respect to the robot's previous experience in similar environments. These\nanomalies might indicate unforeseen hazards and, in scenarios where failure is\ncostly, can be used to trigger an avoidance behavior. We contribute three novel\nimage-based datasets acquired in robot exploration scenarios, comprising a\ntotal of more than 200k labeled frames, spanning various types of anomalies. On\nthese datasets, we study the performance of an anomaly detection approach based\non autoencoders operating at different scales.\n",
        "published": "2021",
        "authors": [
            "Dario Mantegazza",
            "Carlos Redondo",
            "Fran Espada",
            "Luca M. Gambardella",
            "Alessandro Giusti",
            "J\u00e9r\u00f4me Guzzi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.08546v4",
        "title": "Towards Non-I.I.D. and Invisible Data with FedNAS: Federated Deep\n  Learning via Neural Architecture Search",
        "abstract": "  Federated Learning (FL) has been proved to be an effective learning framework\nwhen data cannot be centralized due to privacy, communication costs, and\nregulatory restrictions. When training deep learning models under an FL\nsetting, people employ the predefined model architecture discovered in the\ncentralized environment. However, this predefined architecture may not be the\noptimal choice because it may not fit data with non-identical and independent\ndistribution (non-IID). Thus, we advocate automating federated learning\n(AutoFL) to improve model accuracy and reduce the manual design effort. We\nspecifically study AutoFL via Neural Architecture Search (NAS), which can\nautomate the design process. We propose a Federated NAS (FedNAS) algorithm to\nhelp scattered workers collaboratively searching for a better architecture with\nhigher accuracy. We also build a system based on FedNAS. Our experiments on\nnon-IID dataset show that the architecture searched by FedNAS can outperform\nthe manually predefined architecture.\n",
        "published": "2020",
        "authors": [
            "Chaoyang He",
            "Murali Annavaram",
            "Salman Avestimehr"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1407.0577v1",
        "title": "Systematic Derivation of Behaviour Characterisations in Evolutionary\n  Robotics",
        "abstract": "  Evolutionary techniques driven by behavioural diversity, such as novelty\nsearch, have shown significant potential in evolutionary robotics. These\ntechniques rely on priorly specified behaviour characterisations to estimate\nthe similarity between individuals. Characterisations are typically defined in\nan ad hoc manner based on the experimenter's intuition and knowledge about the\ntask. Alternatively, generic characterisations based on the sensor-effector\nvalues of the agents are used. In this paper, we propose a novel approach that\nallows for systematic derivation of behaviour characterisations for\nevolutionary robotics, based on a formal description of the agents and their\nenvironment. Systematically derived behaviour characterisations (SDBCs) go\nbeyond generic characterisations in that they can contain task-specific\nfeatures related to the internal state of the agents, environmental features,\nand relations between them. We evaluate SDBCs with novelty search in three\nsimulated collective robotics tasks. Our results show that SDBCs yield a\nperformance comparable to the task-specific characterisations, in terms of both\nsolution quality and behaviour space exploration.\n",
        "published": "2014",
        "authors": [
            "Jorge Gomes",
            "Pedro Mariano",
            "Anders Lyhne Christensen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.04882v1",
        "title": "A Neuro-inspired Theory of Joint Human-Swarm Interaction",
        "abstract": "  Human-swarm interaction (HSI) is an active research challenge in the realms\nof swarm robotics and human-factors engineering. Here we apply a cognitive\nsystems engineering perspective and introduce a neuro-inspired joint systems\ntheory of HSI. The mindset defines predictions for adaptive, robust and\nscalable HSI dynamics and therefore has the potential to inform human-swarm\nloop design.\n",
        "published": "2020",
        "authors": [
            "Jonas D. Hasbach",
            "Maren Bennewitz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.10519v4",
        "title": "Three Dimensional Route Planning for Multiple Unmanned Aerial Vehicles\n  using Salp Swarm Algorithm",
        "abstract": "  Route planning for multiple Unmanned Aerial Vehicles (UAVs) is a series of\ntranslation and rotational steps from a given start location to the destination\ngoal location. The goal of the route planning problem is to determine the most\noptimal route avoiding any collisions with the obstacles present in the\nenvironment. Route planning is an NP-hard optimization problem. In this paper,\na newly proposed Salp Swarm Algorithm (SSA) is used, and its performance is\ncompared with deterministic and other Nature-Inspired Algorithms (NIAs). The\nresults illustrate that SSA outperforms all the other meta-heuristic algorithms\nin route planning for multiple UAVs in a 3D environment. The proposed approach\nimproves the average cost and overall time by 1.25% and 6.035% respectively\nwhen compared to recently reported data. Route planning is involved in many\nreal-life applications like robot navigation, self-driving car, autonomous UAV\nfor search and rescue operations in dangerous ground-zero situations, civilian\nsurveillance, military combat and even commercial services like package\ndelivery by drones.\n",
        "published": "2019",
        "authors": [
            "Priyansh Saxena",
            "Ram Kishan Dewangan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.03975v1",
        "title": "Self-organizing nest migration dynamics synthesis for ant colony systems",
        "abstract": "  In this study, we synthesize a novel dynamical approach for ant colonies\nenabling them to migrate to new nest sites in a self-organizing fashion. In\nother words, we realize ant colony migration as a self-organizing\nphenotype-level collective behavior. For this purpose, we first segment the\nedges of the graph of ants' pathways. Then, each segment, attributed to its own\npheromone profile, may host an ant. So, multiple ants may occupy an edge at the\nsame time. Thanks to this segment-wise edge formulation, ants have more\nselection options in the course of their pathway determination, thereby\nincreasing the diversity of their colony's emergent behaviors. In light of the\ncontinuous pheromone dynamics of segments, each edge owns a spatio-temporal\npiece-wise continuous pheromone profile in which both deposit and evaporation\nprocesses are unified. The passive dynamics of the proposed migration mechanism\nis sufficiently rich so that an ant colony can migrate to the vicinity of a new\nnest site in a self-organizing manner without any external supervision. In\nparticular, we perform extensive simulations to test our migration dynamics\napplied to a colony including 500 ants traversing a pathway graph comprising\n200 nodes and 4000 edges which are segmented based on various resolutions. The\nobtained results exhibit the effectiveness of our strategy.\n",
        "published": "2022",
        "authors": [
            "Matin Macktoobian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.02994v1",
        "title": "Evolution of Collective Decision-Making Mechanisms for Collective\n  Perception",
        "abstract": "  Autonomous robot swarms must be able to make fast and accurate collective\ndecisions, but speed and accuracy are known to be conflicting goals. While\ncollective decision-making is widely studied in swarm robotics research, only\nfew works on using methods of evolutionary computation to generate collective\ndecision-making mechanisms exist. These works use task-specific fitness\nfunctions rewarding the accomplishment of the respective collective\ndecision-making task. But task-independent rewards, such as for prediction\nerror minimization, may promote the emergence of diverse and innovative\nsolutions. We evolve collective decision-making mechanisms using a\ntask-specific fitness function rewarding correct robot opinions, a\ntask-independent reward for prediction accuracy, and a hybrid fitness function\ncombining the two previous. In our simulations, we use the collective\nperception scenario, that is, robots must collectively determine which of two\nenvironmental features is more frequent. We show that evolution successfully\noptimizes fitness in all three scenarios, but that only the task-specific\nfitness function and the hybrid fitness function lead to the emergence of\ncollective decision-making behaviors. In benchmark experiments, we show the\ncompetitiveness of the evolved decision-making mechanisms to the voter model\nand the majority rule and analyze the scalability of the decision-making\nmechanisms with problem difficulty.\n",
        "published": "2023",
        "authors": [
            "Tanja Katharina Kaiser",
            "Tristan Potten",
            "Heiko Hamann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.16606v1",
        "title": "Evolutionary Swarm Robotics: Dynamic Subgoal-Based Path Formation and\n  Task Allocation for Exploration and Navigation in Unknown Environments",
        "abstract": "  This research paper addresses the challenges of exploration and navigation in\nunknown environments from an evolutionary swarm robotics perspective. Path\nformation plays a crucial role in enabling cooperative swarm robots to\naccomplish these tasks. The paper presents a method called the sub-goal-based\npath formation, which establishes a path between two different locations by\nexploiting visually connected sub-goals. Simulation experiments conducted in\nthe Argos simulator demonstrate the successful formation of paths in the\nmajority of trials.\n  Furthermore, the paper tackles the problem of inter-collision (traffic) among\na large number of robots engaged in path formation, which negatively impacts\nthe performance of the sub-goal-based method. To mitigate this issue, a task\nallocation strategy is proposed, leveraging local communication protocols and\nlight signal-based communication. The strategy evaluates the distance between\npoints and determines the required number of robots for the path formation\ntask, reducing unwanted exploration and traffic congestion. The performance of\nthe sub-goal-based path formation and task allocation strategy is evaluated by\ncomparing path length, time, and resource reduction against the A* algorithm.\nThe simulation experiments demonstrate promising results, showcasing the\nscalability, robustness, and fault tolerance characteristics of the proposed\napproach.\n",
        "published": "2023",
        "authors": [
            "Lavanya Ratnabala",
            "Robinroy Peter",
            "E. Y. A. Charles"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.07879v1",
        "title": "Using Sensory Time-cue to enable Unsupervised Multimodal Meta-learning",
        "abstract": "  As data from IoT (Internet of Things) sensors become ubiquitous,\nstate-of-the-art machine learning algorithms face many challenges on directly\nusing sensor data. To overcome these challenges, methods must be designed to\nlearn directly from sensors without manual annotations. This paper introduces\nSensory Time-cue for Unsupervised Meta-learning (STUM). Different from\ntraditional learning approaches that either heavily depend on labels or on\ntime-independent feature extraction assumptions, such as Gaussian distribution\nfeatures, the STUM system uses time relation of inputs to guide the feature\nspace formation within and across modalities. The fact that STUM learns from a\nvariety of small tasks may put this method in the camp of Meta-Learning.\nDifferent from existing Meta-Learning approaches, STUM learning tasks are\ncomposed within and across multiple modalities based on time-cue co-exist with\nthe IoT streaming data. In an audiovisual learning example, because consecutive\nvisual frames usually comprise the same object, this approach provides a unique\nway to organize features from the same object together. The same method can\nalso organize visual object features with the object's spoken-name features\ntogether if the spoken name is presented with the object at about the same\ntime. This cross-modality feature organization may further help the\norganization of visual features that belong to similar objects but acquired at\ndifferent location and time. Promising results are achieved through\nevaluations.\n",
        "published": "2020",
        "authors": [
            "Qiong Liu",
            "Yanxia Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.13578v1",
        "title": "Test-Time Training for Deformable Multi-Scale Image Registration",
        "abstract": "  Registration is a fundamental task in medical robotics and is often a crucial\nstep for many downstream tasks such as motion analysis, intra-operative\ntracking and image segmentation. Popular registration methods such as ANTs and\nNiftyReg optimize objective functions for each pair of images from scratch,\nwhich are time-consuming for 3D and sequential images with complex\ndeformations. Recently, deep learning-based registration approaches such as\nVoxelMorph have been emerging and achieve competitive performance. In this\nwork, we construct a test-time training for deep deformable image registration\nto improve the generalization ability of conventional learning-based\nregistration model. We design multi-scale deep networks to consecutively model\nthe residual deformations, which is effective for high variational\ndeformations. Extensive experiments validate the effectiveness of multi-scale\ndeep registration with test-time training based on Dice coefficient for image\nsegmentation and mean square error (MSE), normalized local cross-correlation\n(NLCC) for tissue dense tracking tasks. Two videos are in\nhttps://www.youtube.com/watch?v=NvLrCaqCiAE and\nhttps://www.youtube.com/watch?v=pEA6ZmtTNuQ\n",
        "published": "2021",
        "authors": [
            "Wentao Zhu",
            "Yufang Huang",
            "Daguang Xu",
            "Zhen Qian",
            "Wei Fan",
            "Xiaohui Xie"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.14392v1",
        "title": "FEDORA: Flying Event Dataset fOr Reactive behAvior",
        "abstract": "  The ability of living organisms to perform complex high speed manoeuvers in\nflight with a very small number of neurons and an incredibly low failure rate\nhighlights the efficacy of these resource-constrained biological systems.\nEvent-driven hardware has emerged, in recent years, as a promising avenue for\nimplementing complex vision tasks in resource-constrained environments.\nVision-based autonomous navigation and obstacle avoidance consists of several\nindependent but related tasks such as optical flow estimation, depth\nestimation, Simultaneous Localization and Mapping (SLAM), object detection, and\nrecognition. To ensure coherence between these tasks, it is imperative that\nthey be trained on a single dataset. However, most existing datasets provide\nonly a selected subset of the required data. This makes inter-network coherence\ndifficult to achieve. Another limitation of existing datasets is the limited\ntemporal resolution they provide. To address these limitations, we present\nFEDORA, a first-of-its-kind fully synthetic dataset for vision-based tasks,\nwith ground truths for depth, pose, ego-motion, and optical flow. FEDORA is the\nfirst dataset to provide optical flow at three different frequencies - 10Hz,\n25Hz, and 50Hz\n",
        "published": "2023",
        "authors": [
            "Amogh Joshi",
            "Adarsh Kosta",
            "Wachirawit Ponghiran",
            "Manish Nagaraj",
            "Kaushik Roy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1410.8326v1",
        "title": "Towards Learning Object Affordance Priors from Technical Texts",
        "abstract": "  Everyday activities performed by artificial assistants can potentially be\nexecuted naively and dangerously given their lack of common sense knowledge.\nThis paper presents conceptual work towards obtaining prior knowledge on the\nusual modality (passive or active) of any given entity, and their affordance\nestimates, by extracting high-confidence ability modality semantic relations (X\ncan Y relationship) from non-figurative texts, by analyzing co-occurrence of\ngrammatical instances of subjects and verbs, and verbs and objects. The\ndiscussion includes an outline of the concept, potential and limitations, and\npossible feature and learning framework adoption.\n",
        "published": "2014",
        "authors": [
            "Nicholas H. Kirk"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1706.07230v2",
        "title": "Gated-Attention Architectures for Task-Oriented Language Grounding",
        "abstract": "  To perform tasks specified by natural language instructions, autonomous\nagents need to extract semantically meaningful representations of language and\nmap it to visual elements and actions in the environment. This problem is\ncalled task-oriented language grounding. We propose an end-to-end trainable\nneural architecture for task-oriented language grounding in 3D environments\nwhich assumes no prior linguistic or perceptual knowledge and requires only raw\npixels from the environment and the natural language instruction as input. The\nproposed model combines the image and text representations using a\nGated-Attention mechanism and learns a policy to execute the natural language\ninstruction using standard reinforcement and imitation learning methods. We\nshow the effectiveness of the proposed model on unseen instructions as well as\nunseen maps, both quantitatively and qualitatively. We also introduce a novel\nenvironment based on a 3D game engine to simulate the challenges of\ntask-oriented language grounding over a rich set of instructions and\nenvironment states.\n",
        "published": "2017",
        "authors": [
            "Devendra Singh Chaplot",
            "Kanthashree Mysore Sathyendra",
            "Rama Kumar Pasumarthi",
            "Dheeraj Rajagopal",
            "Ruslan Salakhutdinov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.03804v2",
        "title": "Source-Target Inference Models for Spatial Instruction Understanding",
        "abstract": "  Models that can execute natural language instructions for situated robotic\ntasks such as assembly and navigation have several useful applications in\nhomes, offices, and remote scenarios. We study the semantics of\nspatially-referred configuration and arrangement instructions, based on the\nchallenging Bisk-2016 blank-labeled block dataset. This task involves finding a\nsource block and moving it to the target position (mentioned via a reference\nblock and offset), where the blocks have no names or colors and are just\nreferred to via spatial location features. We present novel models for the\nsubtasks of source block classification and target position regression, based\non joint-loss language and spatial-world representation learning, as well as\nCNN-based and dual attention models to compute the alignment between the world\nblocks and the instruction phrases. For target position prediction, we compare\ntwo inference approaches: annealed sampling via policy gradient versus\nexpectation inference via supervised regression. Our models achieve the new\nstate-of-the-art on this task, with an improvement of 47% on source block\naccuracy and 22% on target position distance.\n",
        "published": "2017",
        "authors": [
            "Hao Tan",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.03481v3",
        "title": "Improved and Scalable Online Learning of Spatial Concepts and Language\n  Models with Mapping",
        "abstract": "  We propose a novel online learning algorithm, called SpCoSLAM 2.0, for\nspatial concepts and lexical acquisition with high accuracy and scalability.\nPreviously, we proposed SpCoSLAM as an online learning algorithm based on\nunsupervised Bayesian probabilistic model that integrates multimodal place\ncategorization, lexical acquisition, and SLAM. However, our original algorithm\nhad limited estimation accuracy owing to the influence of the early stages of\nlearning, and increased computational complexity with added training data.\nTherefore, we introduce techniques such as fixed-lag rejuvenation to reduce the\ncalculation time while maintaining an accuracy higher than that of the original\nalgorithm. The results show that, in terms of estimation accuracy, the proposed\nalgorithm exceeds the original algorithm and is comparable to batch learning.\nIn addition, the calculation time of the proposed algorithm does not depend on\nthe amount of training data and becomes constant for each step of the scalable\nalgorithm. Our approach will contribute to the realization of long-term spatial\nlanguage interactions between humans and robots.\n",
        "published": "2018",
        "authors": [
            "Akira Taniguchi",
            "Yoshinobu Hagiwara",
            "Tadahiro Taniguchi",
            "Tetsunari Inamura"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.10423v1",
        "title": "Learning how to learn: an adaptive dialogue agent for incrementally\n  learning visually grounded word meanings",
        "abstract": "  We present an optimised multi-modal dialogue agent for interactive learning\nof visually grounded word meanings from a human tutor, trained on real\nhuman-human tutoring data. Within a life-long interactive learning period, the\nagent, trained using Reinforcement Learning (RL), must be able to handle\nnatural conversations with human users and achieve good learning performance\n(accuracy) while minimising human effort in the learning process. We train and\nevaluate this system in interaction with a simulated human tutor, which is\nbuilt on the BURCHAK corpus -- a Human-Human Dialogue dataset for the visual\nlearning task. The results show that: 1) The learned policy can coherently\ninteract with the simulated user to achieve the goal of the task (i.e. learning\nvisual attributes of objects, e.g. colour and shape); and 2) it finds a better\ntrade-off between classifier accuracy and tutoring costs than hand-crafted\nrule-based policies, including ones with dynamic policies.\n",
        "published": "2017",
        "authors": [
            "Yanchao Yu",
            "Arash Eshghi",
            "Oliver Lemon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.10426v1",
        "title": "Training an adaptive dialogue policy for interactive learning of\n  visually grounded word meanings",
        "abstract": "  We present a multi-modal dialogue system for interactive learning of\nperceptually grounded word meanings from a human tutor. The system integrates\nan incremental, semantic parsing/generation framework - Dynamic Syntax and Type\nTheory with Records (DS-TTR) - with a set of visual classifiers that are\nlearned throughout the interaction and which ground the meaning representations\nthat it produces. We use this system in interaction with a simulated human\ntutor to study the effects of different dialogue policies and capabilities on\nthe accuracy of learned meanings, learning rates, and efforts/costs to the\ntutor. We show that the overall performance of the learning agent is affected\nby (1) who takes initiative in the dialogues; (2) the ability to express/use\ntheir confidence level about visual attributes; and (3) the ability to process\nelliptical and incrementally constructed dialogue turns. Ultimately, we train\nan adaptive dialogue policy which optimises the trade-off between classifier\naccuracy and tutoring costs.\n",
        "published": "2017",
        "authors": [
            "Yanchao Yu",
            "Arash Eshghi",
            "Oliver Lemon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.10431v1",
        "title": "The BURCHAK corpus: a Challenge Data Set for Interactive Learning of\n  Visually Grounded Word Meanings",
        "abstract": "  We motivate and describe a new freely available human-human dialogue dataset\nfor interactive learning of visually grounded word meanings through ostensive\ndefinition by a tutor to a learner. The data has been collected using a novel,\ncharacter-by-character variant of the DiET chat tool (Healey et al., 2003;\nMills and Healey, submitted) with a novel task, where a Learner needs to learn\ninvented visual attribute words (such as \" burchak \" for square) from a tutor.\nAs such, the text-based interactions closely resemble face-to-face conversation\nand thus contain many of the linguistic phenomena encountered in natural,\nspontaneous dialogue. These include self-and other-correction, mid-sentence\ncontinuations, interruptions, overlaps, fillers, and hedges. We also present a\ngeneric n-gram framework for building user (i.e. tutor) simulations from this\ntype of incremental data, which is freely available to researchers. We show\nthat the simulations produce outputs that are similar to the original data\n(e.g. 78% turn match similarity). Finally, we train and evaluate a\nReinforcement Learning dialogue control agent for learning visually grounded\nword meanings, trained from the BURCHAK corpus. The learned policy shows\ncomparable performance to a rule-based system built previously.\n",
        "published": "2017",
        "authors": [
            "Yanchao Yu",
            "Arash Eshghi",
            "Gregory Mills",
            "Oliver Joseph Lemon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.08309v1",
        "title": "Prospection: Interpretable Plans From Language By Predicting the Future",
        "abstract": "  High-level human instructions often correspond to behaviors with multiple\nimplicit steps. In order for robots to be useful in the real world, they must\nbe able to to reason over both motions and intermediate goals implied by human\ninstructions. In this work, we propose a framework for learning representations\nthat convert from a natural-language command to a sequence of intermediate\ngoals for execution on a robot. A key feature of this framework is prospection,\ntraining an agent not just to correctly execute the prescribed command, but to\npredict a horizon of consequences of an action before taking it. We demonstrate\nthe fidelity of plans generated by our framework when interpreting real,\ncrowd-sourced natural language commands for a robot in simulated scenes.\n",
        "published": "2019",
        "authors": [
            "Chris Paxton",
            "Yonatan Bisk",
            "Jesse Thomason",
            "Arunkumar Byravan",
            "Dieter Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.09243v1",
        "title": "Inferring Compact Representations for Efficient Natural Language\n  Understanding of Robot Instructions",
        "abstract": "  The speed and accuracy with which robots are able to interpret natural\nlanguage is fundamental to realizing effective human-robot interaction. A great\ndeal of attention has been paid to developing models and approximate inference\nalgorithms that improve the efficiency of language understanding. However,\nexisting methods still attempt to reason over a representation of the\nenvironment that is flat and unnecessarily detailed, which limits scalability.\nAn open problem is then to develop methods capable of producing the most\ncompact environment model sufficient for accurate and efficient natural\nlanguage understanding. We propose a model that leverages environment-related\ninformation encoded within instructions to identify the subset of observations\nand perceptual classifiers necessary to perceive a succinct,\ninstruction-specific environment representation. The framework uses three\nprobabilistic graphical models trained from a corpus of annotated instructions\nto infer salient scene semantics, perceptual classifiers, and grounded symbols.\nExperimental results on two robots operating in different environments\ndemonstrate that by exploiting the content and the structure of the\ninstructions, our method learns compact environment representations that\nsignificantly improve the efficiency of natural language symbol grounding.\n",
        "published": "2019",
        "authors": [
            "Siddharth Patki",
            "Andrea F. Daniele",
            "Matthew R. Walter",
            "Thomas M. Howard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.04304v1",
        "title": "Unsupervised Online Grounding of Natural Language during Human-Robot\n  Interactions",
        "abstract": "  Allowing humans to communicate through natural language with robots requires\nconnections between words and percepts. The process of creating these\nconnections is called symbol grounding and has been studied for nearly three\ndecades. Although many studies have been conducted, not many considered\ngrounding of synonyms and the employed algorithms either work only offline or\nin a supervised manner. In this paper, a cross-situational learning based\ngrounding framework is proposed that allows grounding of words and phrases\nthrough corresponding percepts without human supervision and online, i.e. it\ndoes not require any explicit training phase, but instead updates the obtained\nmappings for every new encountered situation. The proposed framework is\nevaluated through an interaction experiment between a human tutor and a robot,\nand compared to an existing unsupervised grounding framework. The results show\nthat the proposed framework is able to ground words through their corresponding\npercepts online and in an unsupervised manner, while outperforming the baseline\nframework.\n",
        "published": "2020",
        "authors": [
            "Oliver Roesler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.05150v2",
        "title": "Safe Reinforcement Learning with Natural Language Constraints",
        "abstract": "  While safe reinforcement learning (RL) holds great promise for many practical\napplications like robotics or autonomous cars, current approaches require\nspecifying constraints in mathematical form. Such specifications demand domain\nexpertise, limiting the adoption of safe RL. In this paper, we propose learning\nto interpret natural language constraints for safe RL. To this end, we first\nintroduce HazardWorld, a new multi-task benchmark that requires an agent to\noptimize reward while not violating constraints specified in free-form text. We\nthen develop an agent with a modular architecture that can interpret and adhere\nto such textual constraints while learning new tasks. Our model consists of (1)\na constraint interpreter that encodes textual constraints into spatial and\ntemporal representations of forbidden states, and (2) a policy network that\nuses these representations to produce a policy achieving minimal constraint\nviolations during training. Across different domains in HazardWorld, we show\nthat our method achieves higher rewards (up to11x) and fewer constraint\nviolations (by 1.8x) compared to existing approaches. However, in terms of\nabsolute performance, HazardWorld still poses significant challenges for agents\nto learn efficiently, motivating the need for future work.\n",
        "published": "2020",
        "authors": [
            "Tsung-Yen Yang",
            "Michael Hu",
            "Yinlam Chow",
            "Peter J. Ramadge",
            "Karthik Narasimhan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.05190v1",
        "title": "Learning Adaptive Language Interfaces through Decomposition",
        "abstract": "  Our goal is to create an interactive natural language interface that\nefficiently and reliably learns from users to complete tasks in simulated\nrobotics settings. We introduce a neural semantic parsing system that learns\nnew high-level abstractions through decomposition: users interactively teach\nthe system by breaking down high-level utterances describing novel behavior\ninto low-level steps that it can understand. Unfortunately, existing methods\neither rely on grammars which parse sentences with limited flexibility, or\nneural sequence-to-sequence models that do not learn efficiently or reliably\nfrom individual examples. Our approach bridges this gap, demonstrating the\nflexibility of modern neural systems, as well as the one-shot reliable\ngeneralization of grammar-based methods. Our crowdsourced interactive\nexperiments suggest that over time, users complete complex tasks more\nefficiently while using our system by leveraging what they just taught. At the\nsame time, getting users to trust the system enough to be incentivized to teach\nhigh-level utterances is still an ongoing challenge. We end with a discussion\nof some of the obstacles we need to overcome to fully realize the potential of\nthe interactive paradigm.\n",
        "published": "2020",
        "authors": [
            "Siddharth Karamcheti",
            "Dorsa Sadigh",
            "Percy Liang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.07140v4",
        "title": "Natural Language Specification of Reinforcement Learning Policies\n  through Differentiable Decision Trees",
        "abstract": "  Human-AI policy specification is a novel procedure we define in which humans\ncan collaboratively warm-start a robot's reinforcement learning policy. This\nprocedure is comprised of two steps; (1) Policy Specification, i.e. humans\nspecifying the behavior they would like their companion robot to accomplish,\nand (2) Policy Optimization, i.e. the robot applying reinforcement learning to\nimprove the initial policy. Existing approaches to enabling collaborative\npolicy specification are often unintelligible black-box methods, and are not\ncatered towards making the autonomous system accessible to a novice end-user.\nIn this paper, we develop a novel collaborative framework to allow humans to\ninitialize and interpret an autonomous agent's behavior. Through our framework,\nwe enable humans to specify an initial behavior model via unstructured, natural\nlanguage (NL), which we convert to lexical decision trees. Next, we leverage\nthese translated specifications, to warm-start reinforcement learning and allow\nthe agent to further optimize these potentially suboptimal policies. Our\napproach warm-starts an RL agent by utilizing non-expert natural language\nspecifications without incurring the additional domain exploration costs. We\nvalidate our approach by showing that our model is able to produce >80%\ntranslation accuracy, and that policies initialized by a human can match the\nperformance of relevant RL baselines in two domains.\n",
        "published": "2021",
        "authors": [
            "Pradyumna Tambwekar",
            "Andrew Silva",
            "Nakul Gopalan",
            "Matthew Gombolay"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.14593v1",
        "title": "Neural Variational Learning for Grounded Language Acquisition",
        "abstract": "  We propose a learning system in which language is grounded in visual percepts\nwithout specific pre-defined categories of terms. We present a unified\ngenerative method to acquire a shared semantic/visual embedding that enables\nthe learning of language about a wide range of real-world objects. We evaluate\nthe efficacy of this learning by predicting the semantics of objects and\ncomparing the performance with neural and non-neural inputs. We show that this\ngenerative approach exhibits promising results in language grounding without\npre-specifying visual categories under low resource settings. Our experiments\ndemonstrate that this approach is generalizable to multilingual, highly varied\ndatasets.\n",
        "published": "2021",
        "authors": [
            "Nisha Pillai",
            "Cynthia Matuszek",
            "Francis Ferraro"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1610.03164v1",
        "title": "Navigational Instruction Generation as Inverse Reinforcement Learning\n  with Neural Machine Translation",
        "abstract": "  Modern robotics applications that involve human-robot interaction require\nrobots to be able to communicate with humans seamlessly and effectively.\nNatural language provides a flexible and efficient medium through which robots\ncan exchange information with their human partners. Significant advancements\nhave been made in developing robots capable of interpreting free-form\ninstructions, but less attention has been devoted to endowing robots with the\nability to generate natural language. We propose a navigational guide model\nthat enables robots to generate natural language instructions that allow humans\nto navigate a priori unknown environments. We first decide which information to\nshare with the user according to their preferences, using a policy trained from\nhuman demonstrations via inverse reinforcement learning. We then \"translate\"\nthis information into a natural language instruction using a neural\nsequence-to-sequence model that learns to generate free-form instructions from\nnatural language corpora. We evaluate our method on a benchmark route\ninstruction dataset and achieve a BLEU score of 72.18% when compared to\nhuman-generated reference instructions. We additionally conduct navigation\nexperiments with human participants that demonstrate that our method generates\ninstructions that people follow as accurately and easily as those produced by\nhumans.\n",
        "published": "2016",
        "authors": [
            "Andrea F. Daniele",
            "Mohit Bansal",
            "Matthew R. Walter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.06150v1",
        "title": "FollowNet: Robot Navigation by Following Natural Language Directions\n  with Deep Reinforcement Learning",
        "abstract": "  Understanding and following directions provided by humans can enable robots\nto navigate effectively in unknown situations. We present FollowNet, an\nend-to-end differentiable neural architecture for learning multi-modal\nnavigation policies. FollowNet maps natural language instructions as well as\nvisual and depth inputs to locomotion primitives. FollowNet processes\ninstructions using an attention mechanism conditioned on its visual and depth\ninput to focus on the relevant parts of the command while performing the\nnavigation task. Deep reinforcement learning (RL) a sparse reward learns\nsimultaneously the state representation, the attention function, and control\npolicies. We evaluate our agent on a dataset of complex natural language\ndirections that guide the agent through a rich and realistic dataset of\nsimulated homes. We show that the FollowNet agent learns to execute previously\nunseen instructions described with a similar vocabulary, and successfully\nnavigates along paths not encountered during training. The agent shows 30%\nimprovement over a baseline model without the attention mechanism, with 52%\nsuccess rate at novel instructions.\n",
        "published": "2018",
        "authors": [
            "Pararth Shah",
            "Marek Fiser",
            "Aleksandra Faust",
            "J. Chase Kew",
            "Dilek Hakkani-Tur"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.08329v2",
        "title": "Guided Feature Transformation (GFT): A Neural Language Grounding Module\n  for Embodied Agents",
        "abstract": "  Recently there has been a rising interest in training agents, embodied in\nvirtual environments, to perform language-directed tasks by deep reinforcement\nlearning. In this paper, we propose a simple but effective neural language\ngrounding module for embodied agents that can be trained end to end from\nscratch taking raw pixels, unstructured linguistic commands, and sparse rewards\nas the inputs. We model the language grounding process as a language-guided\ntransformation of visual features, where latent sentence embeddings are used as\nthe transformation matrices. In several language-directed navigation tasks that\nfeature challenging partial observability and require simple reasoning, our\nmodule significantly outperforms the state of the art. We also release\nXWorld3D, an easy-to-customize 3D environment that can potentially be modified\nto evaluate a variety of embodied agents.\n",
        "published": "2018",
        "authors": [
            "Haonan Yu",
            "Xiaochen Lian",
            "Haichao Zhang",
            "Wei Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.05825v2",
        "title": "ELLA: Exploration through Learned Language Abstraction",
        "abstract": "  Building agents capable of understanding language instructions is critical to\neffective and robust human-AI collaboration. Recent work focuses on training\nthese agents via reinforcement learning in environments with synthetic\nlanguage; however, instructions often define long-horizon, sparse-reward tasks,\nand learning policies requires many episodes of experience. We introduce ELLA:\nExploration through Learned Language Abstraction, a reward shaping approach\ngeared towards boosting sample efficiency in sparse reward environments by\ncorrelating high-level instructions with simpler low-level constituents. ELLA\nhas two key elements: 1) A termination classifier that identifies when agents\ncomplete low-level instructions, and 2) A relevance classifier that correlates\nlow-level instructions with success on high-level tasks. We learn the\ntermination classifier offline from pairs of instructions and terminal states.\nNotably, in departure from prior work in language and abstraction, we learn the\nrelevance classifier online, without relying on an explicit decomposition of\nhigh-level instructions to low-level instructions. On a suite of complex BabyAI\nenvironments with varying instruction complexities and reward sparsity, ELLA\nshows gains in sample efficiency relative to language-based shaping and\ntraditional RL methods.\n",
        "published": "2021",
        "authors": [
            "Suvir Mirchandani",
            "Siddharth Karamcheti",
            "Dorsa Sadigh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.08521v1",
        "title": "Embodying Pre-Trained Word Embeddings Through Robot Actions",
        "abstract": "  We propose a promising neural network model with which to acquire a grounded\nrepresentation of robot actions and the linguistic descriptions thereof.\nProperly responding to various linguistic expressions, including polysemous\nwords, is an important ability for robots that interact with people via\nlinguistic dialogue. Previous studies have shown that robots can use words that\nare not included in the action-description paired datasets by using pre-trained\nword embeddings. However, the word embeddings trained under the distributional\nhypothesis are not grounded, as they are derived purely from a text corpus. In\nthis letter, we transform the pre-trained word embeddings to embodied ones by\nusing the robot's sensory-motor experiences. We extend a bidirectional\ntranslation model for actions and descriptions by incorporating non-linear\nlayers that retrofit the word embeddings. By training the retrofit layer and\nthe bidirectional translation model alternately, our proposed model is able to\ntransform the pre-trained word embeddings to adapt to a paired\naction-description dataset. Our results demonstrate that the embeddings of\nsynonyms form a semantic cluster by reflecting the experiences (actions and\nenvironments) of a robot. These embeddings allow the robot to properly generate\nactions from unseen words that are not paired with actions in a dataset.\n",
        "published": "2021",
        "authors": [
            "Minori Toyoda",
            "Kanata Suzuki",
            "Hiroki Mori",
            "Yoshihiko Hayashi",
            "Tetsuya Ogata"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.09741v2",
        "title": "Trajectory Prediction with Linguistic Representations",
        "abstract": "  Language allows humans to build mental models that interpret what is\nhappening around them resulting in more accurate long-term predictions. We\npresent a novel trajectory prediction model that uses linguistic intermediate\nrepresentations to forecast trajectories, and is trained using trajectory\nsamples with partially-annotated captions. The model learns the meaning of each\nof the words without direct per-word supervision. At inference time, it\ngenerates a linguistic description of trajectories which captures maneuvers and\ninteractions over an extended time interval. This generated description is used\nto refine predictions of the trajectories of multiple agents. We train and\nvalidate our model on the Argoverse dataset, and demonstrate improved accuracy\nresults in trajectory prediction. In addition, our model is more interpretable:\nit presents part of its reasoning in plain language as captions, which can aid\nmodel development and can aid in building confidence in the model before\ndeploying it.\n",
        "published": "2021",
        "authors": [
            "Yen-Ling Kuo",
            "Xin Huang",
            "Andrei Barbu",
            "Stephen G. McGill",
            "Boris Katz",
            "John J. Leonard",
            "Guy Rosman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.10963v1",
        "title": "Neuro-Symbolic Reinforcement Learning with First-Order Logic",
        "abstract": "  Deep reinforcement learning (RL) methods often require many trials before\nconvergence, and no direct interpretability of trained policies is provided. In\norder to achieve fast convergence and interpretability for the policy in RL, we\npropose a novel RL method for text-based games with a recent neuro-symbolic\nframework called Logical Neural Network, which can learn symbolic and\ninterpretable rules in their differentiable network. The method is first to\nextract first-order logical facts from text observation and external word\nmeaning network (ConceptNet), then train a policy in the network with directly\ninterpretable logical operators. Our experimental results show RL training with\nthe proposed method converges significantly faster than other state-of-the-art\nneuro-symbolic methods in a TextWorld benchmark.\n",
        "published": "2021",
        "authors": [
            "Daiki Kimura",
            "Masaki Ono",
            "Subhajit Chaudhury",
            "Ryosuke Kohita",
            "Akifumi Wachi",
            "Don Joven Agravante",
            "Michiaki Tatsubori",
            "Asim Munawar",
            "Alexander Gray"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.10973v1",
        "title": "LOA: Logical Optimal Actions for Text-based Interaction Games",
        "abstract": "  We present Logical Optimal Actions (LOA), an action decision architecture of\nreinforcement learning applications with a neuro-symbolic framework which is a\ncombination of neural network and symbolic knowledge acquisition approach for\nnatural language interaction games. The demonstration for LOA experiments\nconsists of a web-based interactive platform for text-based games and\nvisualization for acquired knowledge for improving interpretability for trained\nrules. This demonstration also provides a comparison module with other\nneuro-symbolic approaches as well as non-symbolic state-of-the-art agent models\non the same text-based games. Our LOA also provides open-sourced implementation\nin Python for the reinforcement learning environment to facilitate an\nexperiment for studying neuro-symbolic agents. Code: https://github.com/ibm/loa\n",
        "published": "2021",
        "authors": [
            "Daiki Kimura",
            "Subhajit Chaudhury",
            "Masaki Ono",
            "Michiaki Tatsubori",
            "Don Joven Agravante",
            "Asim Munawar",
            "Akifumi Wachi",
            "Ryosuke Kohita",
            "Alexander Gray"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.13758v1",
        "title": "Bridging the Gap: Using Deep Acoustic Representations to Learn Grounded\n  Language from Percepts and Raw Speech",
        "abstract": "  Learning to understand grounded language, which connects natural language to\npercepts, is a critical research area. Prior work in grounded language\nacquisition has focused primarily on textual inputs. In this work we\ndemonstrate the feasibility of performing grounded language acquisition on\npaired visual percepts and raw speech inputs. This will allow interactions in\nwhich language about novel tasks and environments is learned from end users,\nreducing dependence on textual inputs and potentially mitigating the effects of\ndemographic bias found in widely available speech recognition systems. We\nleverage recent work in self-supervised speech representation models and show\nthat learned representations of speech can make language grounding systems more\ninclusive towards specific groups while maintaining or even increasing general\nperformance.\n",
        "published": "2021",
        "authors": [
            "Gaoussou Youssouf Kebe",
            "Luke E. Richards",
            "Edward Raff",
            "Francis Ferraro",
            "Cynthia Matuszek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.00054v3",
        "title": "LISA: Learning Interpretable Skill Abstractions from Language",
        "abstract": "  Learning policies that effectively utilize language instructions in complex,\nmulti-task environments is an important problem in sequential decision-making.\nWhile it is possible to condition on the entire language instruction directly,\nsuch an approach could suffer from generalization issues. In our work, we\npropose \\emph{Learning Interpretable Skill Abstractions (LISA)}, a hierarchical\nimitation learning framework that can learn diverse, interpretable primitive\nbehaviors or skills from language-conditioned demonstrations to better\ngeneralize to unseen instructions. LISA uses vector quantization to learn\ndiscrete skill codes that are highly correlated with language instructions and\nthe behavior of the learned policy. In navigation and robotic manipulation\nenvironments, LISA outperforms a strong non-hierarchical Decision Transformer\nbaseline in the low data regime and is able to compose learned skills to solve\ntasks containing unseen long-range instructions. Our method demonstrates a more\nnatural way to condition on language in sequential decision-making problems and\nachieve interpretable and controllable behavior with the learned skills.\n",
        "published": "2022",
        "authors": [
            "Divyansh Garg",
            "Skanda Vaidyanath",
            "Kuno Kim",
            "Jiaming Song",
            "Stefano Ermon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.04218v2",
        "title": "Learning Bidirectional Translation between Descriptions and Actions with\n  Small Paired Data",
        "abstract": "  This study achieved bidirectional translation between descriptions and\nactions using small paired data from different modalities. The ability to\nmutually generate descriptions and actions is essential for robots to\ncollaborate with humans in their daily lives, which generally requires a large\ndataset that maintains comprehensive pairs of both modality data. However, a\npaired dataset is expensive to construct and difficult to collect. To address\nthis issue, this study proposes a two-stage training method for bidirectional\ntranslation. In the proposed method, we train recurrent autoencoders (RAEs) for\ndescriptions and actions with a large amount of non-paired data. Then, we\nfinetune the entire model to bind their intermediate representations using\nsmall paired data. Because the data used for pre-training do not require\npairing, behavior-only data or a large language corpus can be used. We\nexperimentally evaluated our method using a paired dataset consisting of\nmotion-captured actions and descriptions. The results showed that our method\nperformed well, even when the amount of paired data to train was small. The\nvisualization of the intermediate representations of each RAE showed that\nsimilar actions were encoded in a clustered position and the corresponding\nfeature vectors were well aligned.\n",
        "published": "2022",
        "authors": [
            "Minori Toyoda",
            "Kanata Suzuki",
            "Yoshihiko Hayashi",
            "Tetsuya Ogata"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.06175v3",
        "title": "A Generalist Agent",
        "abstract": "  Inspired by progress in large-scale language modeling, we apply a similar\napproach towards building a single generalist agent beyond the realm of text\noutputs. The agent, which we refer to as Gato, works as a multi-modal,\nmulti-task, multi-embodiment generalist policy. The same network with the same\nweights can play Atari, caption images, chat, stack blocks with a real robot\narm and much more, deciding based on its context whether to output text, joint\ntorques, button presses, or other tokens. In this report we describe the model\nand the data, and document the current capabilities of Gato.\n",
        "published": "2022",
        "authors": [
            "Scott Reed",
            "Konrad Zolna",
            "Emilio Parisotto",
            "Sergio Gomez Colmenarejo",
            "Alexander Novikov",
            "Gabriel Barth-Maron",
            "Mai Gimenez",
            "Yury Sulsky",
            "Jackie Kay",
            "Jost Tobias Springenberg",
            "Tom Eccles",
            "Jake Bruce",
            "Ali Razavi",
            "Ashley Edwards",
            "Nicolas Heess",
            "Yutian Chen",
            "Raia Hadsell",
            "Oriol Vinyals",
            "Mahyar Bordbar",
            "Nando de Freitas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.04429v2",
        "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language,\n  Vision, and Action",
        "abstract": "  Goal-conditioned policies for robotic navigation can be trained on large,\nunannotated datasets, providing for good generalization to real-world settings.\nHowever, particularly in vision-based settings where specifying goals requires\nan image, this makes for an unnatural interface. Language provides a more\nconvenient modality for communication with robots, but contemporary methods\ntypically require expensive supervision, in the form of trajectories annotated\nwith language descriptions. We present a system, LM-Nav, for robotic navigation\nthat enjoys the benefits of training on unannotated large datasets of\ntrajectories, while still providing a high-level interface to the user. Instead\nof utilizing a labeled instruction following dataset, we show that such a\nsystem can be constructed entirely out of pre-trained models for navigation\n(ViNG), image-language association (CLIP), and language modeling (GPT-3),\nwithout requiring any fine-tuning or language-annotated robot data. We\ninstantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon\nnavigation through complex, outdoor environments from natural language\ninstructions. For videos of our experiments, code release, and an interactive\nColab notebook that runs in your browser, please check out our project page\nhttps://sites.google.com/view/lmnav\n",
        "published": "2022",
        "authors": [
            "Dhruv Shah",
            "Blazej Osinski",
            "Brian Ichter",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.00465v3",
        "title": "On Grounded Planning for Embodied Tasks with Language Models",
        "abstract": "  Language models (LMs) have demonstrated their capability in possessing\ncommonsense knowledge of the physical world, a crucial aspect of performing\ntasks in everyday life. However, it remains unclear **whether LMs have the\ncapacity to generate grounded, executable plans for embodied tasks.** This is a\nchallenging task as LMs lack the ability to perceive the environment through\nvision and feedback from the physical environment. In this paper, we address\nthis important research question and present the first investigation into the\ntopic. Our novel problem formulation, named **G-PlanET**, inputs a high-level\ngoal and a data table about objects in a specific environment, and then outputs\na step-by-step actionable plan for a robotic agent to follow. To facilitate the\nstudy, we establish an **evaluation protocol** and design a dedicated metric to\nassess the quality of the plans. Our experiments demonstrate that the use of\ntables for encoding the environment and an iterative decoding strategy can\nsignificantly enhance the LMs' ability in grounded planning. Our analysis also\nreveals interesting and non-trivial findings.\n",
        "published": "2022",
        "authors": [
            "Bill Yuchen Lin",
            "Chengsong Huang",
            "Qian Liu",
            "Wenda Gu",
            "Sam Sommerer",
            "Xiang Ren"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.11302v1",
        "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language\n  Models",
        "abstract": "  Task planning can require defining myriad domain knowledge about the world in\nwhich a robot needs to act. To ameliorate that effort, large language models\n(LLMs) can be used to score potential next actions during task planning, and\neven generate action sequences directly, given an instruction in natural\nlanguage with no additional domain information. However, such methods either\nrequire enumerating all possible next steps for scoring, or generate free-form\ntext that may contain actions not possible on a given robot in its current\ncontext. We present a programmatic LLM prompt structure that enables plan\ngeneration functional across situated environments, robot capabilities, and\ntasks. Our key insight is to prompt the LLM with program-like specifications of\nthe available actions and objects in an environment, as well as with example\nprograms that can be executed. We make concrete recommendations about prompt\nstructure and generation constraints through ablation experiments, demonstrate\nstate of the art success rates in VirtualHome household tasks, and deploy our\nmethod on a physical robot arm for tabletop tasks. Website at\nprogprompt.github.io\n",
        "published": "2022",
        "authors": [
            "Ishika Singh",
            "Valts Blukis",
            "Arsalan Mousavian",
            "Ankit Goyal",
            "Danfei Xu",
            "Jonathan Tremblay",
            "Dieter Fox",
            "Jesse Thomason",
            "Animesh Garg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.09935v2",
        "title": "CAPE: Corrective Actions from Precondition Errors using Large Language\n  Models",
        "abstract": "  Extracting commonsense knowledge from a large language model (LLM) offers a\npath to designing intelligent robots. Existing approaches that leverage LLMs\nfor planning are unable to recover when an action fails and often resort to\nretrying failed actions, without resolving the error's underlying cause.\n  We propose a novel approach (CAPE) that attempts to propose corrective\nactions to resolve precondition errors during planning. CAPE improves the\nquality of generated plans by leveraging few-shot reasoning from action\npreconditions. Our approach enables embodied agents to execute more tasks than\nbaseline methods while ensuring semantic correctness and minimizing\nre-prompting. In VirtualHome, CAPE generates executable plans while improving a\nhuman-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our\nimprovements transfer to a Boston Dynamics Spot robot initialized with a set of\nskills (specified in language) and associated preconditions, where CAPE\nimproves the correctness metric of the executed task plans by 76.49% compared\nto SayCan. Our approach enables the robot to follow natural language commands\nand robustly recover from failures, which baseline approaches largely cannot\nresolve or address inefficiently.\n",
        "published": "2022",
        "authors": [
            "Shreyas Sundara Raman",
            "Vanya Cohen",
            "David Paulius",
            "Ifrah Idrees",
            "Eric Rosen",
            "Ray Mooney",
            "Stefanie Tellex"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.09922v1",
        "title": "Learning to Summarize and Answer Questions about a Virtual Robot's Past\n  Actions",
        "abstract": "  When robots perform long action sequences, users will want to easily and\nreliably find out what they have done. We therefore demonstrate the task of\nlearning to summarize and answer questions about a robot agent's past actions\nusing natural language alone. A single system with a large language model at\nits core is trained to both summarize and answer questions about action\nsequences given ego-centric video frames of a virtual robot and a question\nprompt. To enable training of question answering, we develop a method to\nautomatically generate English-language questions and answers about objects,\nactions, and the temporal order in which actions occurred during episodes of\nrobot action in the virtual environment. Training one model to both summarize\nand answer questions enables zero-shot transfer of representations of objects\nlearned through question answering to improved action summarization. %\ninvolving objects not seen in training to summarize.\n",
        "published": "2023",
        "authors": [
            "Chad DeChant",
            "Iretiayo Akinola",
            "Daniel Bauer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.11489v2",
        "title": "Text2Reward: Automated Dense Reward Function Generation for\n  Reinforcement Learning",
        "abstract": "  Designing reward functions is a longstanding challenge in reinforcement\nlearning (RL); it requires specialized knowledge or domain data, leading to\nhigh costs for development. To address this, we introduce Text2Reward, a\ndata-free framework that automates the generation of dense reward functions\nbased on large language models (LLMs). Given a goal described in natural\nlanguage, Text2Reward generates dense reward functions as an executable program\ngrounded in a compact representation of the environment. Unlike inverse RL and\nrecent work that uses LLMs to write sparse reward codes, Text2Reward produces\ninterpretable, free-form dense reward codes that cover a wide range of tasks,\nutilize existing packages, and allow iterative refinement with human feedback.\nWe evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2,\nMetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17\nmanipulation tasks, policies trained with generated reward codes achieve\nsimilar or better task success rates and convergence speed than expert-written\nreward codes. For locomotion tasks, our method learns six novel locomotion\nbehaviors with a success rate exceeding 94%. Furthermore, we show that the\npolicies trained in the simulator with our method can be deployed in the real\nworld. Finally, Text2Reward further improves the policies by refining their\nreward functions with human feedback. Video results are available at\nhttps://text-to-reward.github.io\n",
        "published": "2023",
        "authors": [
            "Tianbao Xie",
            "Siheng Zhao",
            "Chen Henry Wu",
            "Yitao Liu",
            "Qian Luo",
            "Victor Zhong",
            "Yanchao Yang",
            "Tao Yu"
        ]
    }
]