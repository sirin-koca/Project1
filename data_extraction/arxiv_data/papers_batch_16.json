[
    {
        "id": "http://arxiv.org/abs/2310.03779v1",
        "title": "HandMeThat: Human-Robot Communication in Physical and Social\n  Environments",
        "abstract": "  We introduce HandMeThat, a benchmark for a holistic evaluation of instruction\nunderstanding and following in physical and social environments. While previous\ndatasets primarily focused on language grounding and planning, HandMeThat\nconsiders the resolution of human instructions with ambiguities based on the\nphysical (object states and relations) and social (human actions and goals)\ninformation. HandMeThat contains 10,000 episodes of human-robot interactions.\nIn each episode, the robot first observes a trajectory of human actions towards\nher internal goal. Next, the robot receives a human instruction and should take\nactions to accomplish the subgoal set through the instruction. In this paper,\nwe present a textual interface for our benchmark, where the robot interacts\nwith a virtual environment through textual commands. We evaluate several\nbaseline models on HandMeThat, and show that both offline and online\nreinforcement learning algorithms perform poorly on HandMeThat, suggesting\nsignificant room for future work on physical and social human-robot\ncommunications and interactions.\n",
        "published": "2023",
        "authors": [
            "Yanming Wan",
            "Jiayuan Mao",
            "Joshua B. Tenenbaum"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.08582v1",
        "title": "Tree-Planner: Efficient Close-loop Task Planning with Large Language\n  Models",
        "abstract": "  This paper studies close-loop task planning, which refers to the process of\ngenerating a sequence of skills (a plan) to accomplish a specific goal while\nadapting the plan based on real-time observations. Recently, prompting Large\nLanguage Models (LLMs) to generate actions iteratively has become a prevalent\nparadigm due to its superior performance and user-friendliness. However, this\nparadigm is plagued by two inefficiencies: high token consumption and redundant\nerror correction, both of which hinder its scalability for large-scale testing\nand applications. To address these issues, we propose Tree-Planner, which\nreframes task planning with LLMs into three distinct phases: plan sampling,\naction tree construction, and grounded deciding. Tree-Planner starts by using\nan LLM to sample a set of potential plans before execution, followed by the\naggregation of them to form an action tree. Finally, the LLM performs a\ntop-down decision-making process on the tree, taking into account real-time\nenvironmental information. Experiments show that Tree-Planner achieves\nstate-of-the-art performance while maintaining high efficiency. By decomposing\nLLM queries into a single plan-sampling call and multiple grounded-deciding\ncalls, a considerable part of the prompt are less likely to be repeatedly\nconsumed. As a result, token consumption is reduced by 92.2% compared to the\npreviously best-performing model. Additionally, by enabling backtracking on the\naction tree as needed, the correction process becomes more flexible, leading to\na 40.5% decrease in error corrections. Project page:\nhttps://tree-planner.github.io/\n",
        "published": "2023",
        "authors": [
            "Mengkang Hu",
            "Yao Mu",
            "Xinmiao Yu",
            "Mingyu Ding",
            "Shiguang Wu",
            "Wenqi Shao",
            "Qiguang Chen",
            "Bin Wang",
            "Yu Qiao",
            "Ping Luo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.10103v1",
        "title": "Navigation with Large Language Models: Semantic Guesswork as a Heuristic\n  for Planning",
        "abstract": "  Navigation in unfamiliar environments presents a major challenge for robots:\nwhile mapping and planning techniques can be used to build up a representation\nof the world, quickly discovering a path to a desired goal in unfamiliar\nsettings with such methods often requires lengthy mapping and exploration.\nHumans can rapidly navigate new environments, particularly indoor environments\nthat are laid out logically, by leveraging semantics -- e.g., a kitchen often\nadjoins a living room, an exit sign indicates the way out, and so forth.\nLanguage models can provide robots with such knowledge, but directly using\nlanguage models to instruct a robot how to reach some destination can also be\nimpractical: while language models might produce a narrative about how to reach\nsome goal, because they are not grounded in real-world observations, this\nnarrative might be arbitrarily wrong. Therefore, in this paper we study how the\n``semantic guesswork'' produced by language models can be utilized as a guiding\nheuristic for planning algorithms. Our method, Language Frontier Guide (LFG),\nuses the language model to bias exploration of novel real-world environments by\nincorporating the semantic knowledge stored in language models as a search\nheuristic for planning with either topological or metric maps. We evaluate LFG\nin challenging real-world environments and simulated benchmarks, outperforming\nuninformed exploration and other ways of using language models.\n",
        "published": "2023",
        "authors": [
            "Dhruv Shah",
            "Michael Equi",
            "Blazej Osinski",
            "Fei Xia",
            "Brian Ichter",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.15127v2",
        "title": "Open-Ended Instructable Embodied Agents with Memory-Augmented Large\n  Language Models",
        "abstract": "  Pre-trained and frozen large language models (LLMs) can effectively map\nsimple scene rearrangement instructions to programs over a robot's visuomotor\nfunctions through appropriate few-shot example prompting. To parse open-domain\nnatural language and adapt to a user's idiosyncratic procedures, not known\nduring prompt engineering time, fixed prompts fall short. In this paper, we\nintroduce HELPER, an embodied agent equipped with an external memory of\nlanguage-program pairs that parses free-form human-robot dialogue into action\nprograms through retrieval-augmented LLM prompting: relevant memories are\nretrieved based on the current dialogue, instruction, correction, or VLM\ndescription, and used as in-context prompt examples for LLM querying. The\nmemory is expanded during deployment to include pairs of user's language and\naction plans, to assist future inferences and personalize them to the user's\nlanguage and routines. HELPER sets a new state-of-the-art in the TEACh\nbenchmark in both Execution from Dialog History (EDH) and Trajectory from\nDialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for\nTfD. Our models, code, and video results can be found in our project's website:\nhttps://helper-agent-llm.github.io.\n",
        "published": "2023",
        "authors": [
            "Gabriel Sarch",
            "Yue Wu",
            "Michael J. Tarr",
            "Katerina Fragkiadaki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.04474v2",
        "title": "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator",
        "abstract": "  Code provides a general syntactic structure to build complex programs and\nperform precise computations when paired with a code interpreter - we\nhypothesize that language models (LMs) can leverage code-writing to improve\nChain of Thought reasoning not only for logic and arithmetic tasks, but also\nfor semantic ones (and in particular, those that are a mix of both). For\nexample, consider prompting an LM to write code that counts the number of times\nit detects sarcasm in an essay: the LM may struggle to write an implementation\nfor \"detect_sarcasm(string)\" that can be executed by the interpreter (handling\nthe edge cases would be insurmountable). However, LMs may still produce a valid\nsolution if they not only write code, but also selectively \"emulate\" the\ninterpreter by generating the expected output of \"detect_sarcasm(string)\" and\nother lines of code that cannot be executed. In this work, we propose Chain of\nCode (CoC), a simple yet surprisingly effective extension that improves LM\ncode-driven reasoning. The key idea is to encourage LMs to format semantic\nsub-tasks in a program as flexible pseudocode that the interpreter can\nexplicitly catch undefined behaviors and hand off to simulate with an LM (as an\n\"LMulator\"). Experiments demonstrate that Chain of Code outperforms Chain of\nThought and other baselines across a variety of benchmarks; on BIG-Bench Hard,\nChain of Code achieves 84%, a gain of 12% over Chain of Thought. CoC scales\nwell with large and small models alike, and broadens the scope of reasoning\nquestions that LMs can correctly answer by \"thinking in code\". Project webpage:\nhttps://chain-of-code.github.io.\n",
        "published": "2023",
        "authors": [
            "Chengshu Li",
            "Jacky Liang",
            "Andy Zeng",
            "Xinyun Chen",
            "Karol Hausman",
            "Dorsa Sadigh",
            "Sergey Levine",
            "Li Fei-Fei",
            "Fei Xia",
            "Brian Ichter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.11337v1",
        "title": "Commonsense Reasoning for Identifying and Understanding the Implicit\n  Need of Help and Synthesizing Assistive Actions",
        "abstract": "  Human-Robot Interaction (HRI) is an emerging subfield of service robotics.\nWhile most existing approaches rely on explicit signals (i.e. voice, gesture)\nto engage, current literature is lacking solutions to address implicit user\nneeds. In this paper, we present an architecture to (a) detect user implicit\nneed of help and (b) generate a set of assistive actions without prior\nlearning. Task (a) will be performed using state-of-the-art solutions for Scene\nGraph Generation coupled to the use of commonsense knowledge; whereas, task (b)\nwill be performed using additional commonsense knowledge as well as a sentiment\nanalysis on graph structure. Finally, we propose an evaluation of our solution\nusing established benchmarks (e.g. ActionGenome dataset) along with human\nexperiments. The main motivation of our approach is the embedding of the\nperception-decision-action loop in a single architecture.\n",
        "published": "2022",
        "authors": [
            "Ma\u00eblic Neau",
            "Paulo Santos",
            "Anne-Gwenn Bosser",
            "Nathan Beu",
            "C\u00e9dric Buche"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1309.1501v3",
        "title": "Improvements to deep convolutional neural networks for LVCSR",
        "abstract": "  Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural\nNetworks (DNN), as they are able to better reduce spectral variation in the\ninput signal. This has also been confirmed experimentally, with CNNs showing\nimprovements in word error rate (WER) between 4-12% relative compared to DNNs\nacross a variety of LVCSR tasks. In this paper, we describe different methods\nto further improve CNN performance. First, we conduct a deep analysis comparing\nlimited weight sharing and full weight sharing with state-of-the-art features.\nSecond, we apply various pooling strategies that have shown improvements in\ncomputer vision to an LVCSR speech task. Third, we introduce a method to\neffectively incorporate speaker adaptation, namely fMLLR, into log-mel\nfeatures. Fourth, we introduce an effective strategy to use dropout during\nHessian-free sequence training. We find that with these improvements,\nparticularly with fMLLR and dropout, we are able to achieve an additional 2-3%\nrelative improvement in WER on a 50-hour Broadcast News task over our previous\nbest CNN baseline. On a larger 400-hour BN task, we find an additional 4-5%\nrelative improvement over our previous best CNN baseline.\n",
        "published": "2013",
        "authors": [
            "Tara N. Sainath",
            "Brian Kingsbury",
            "Abdel-rahman Mohamed",
            "George E. Dahl",
            "George Saon",
            "Hagen Soltau",
            "Tomas Beran",
            "Aleksandr Y. Aravkin",
            "Bhuvana Ramabhadran"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1309.1508v3",
        "title": "Accelerating Hessian-free optimization for deep neural networks by\n  implicit preconditioning and sampling",
        "abstract": "  Hessian-free training has become a popular parallel second or- der\noptimization technique for Deep Neural Network training. This study aims at\nspeeding up Hessian-free training, both by means of decreasing the amount of\ndata used for training, as well as through reduction of the number of Krylov\nsubspace solver iterations used for implicit estimation of the Hessian. In this\npaper, we develop an L-BFGS based preconditioning scheme that avoids the need\nto access the Hessian explicitly. Since L-BFGS cannot be regarded as a\nfixed-point iteration, we further propose the employment of flexible Krylov\nsubspace solvers that retain the desired theoretical convergence guarantees of\ntheir conventional counterparts. Second, we propose a new sampling algorithm,\nwhich geometrically increases the amount of data utilized for gradient and\nKrylov subspace iteration calculations. On a 50-hr English Broadcast News task,\nwe find that these methodologies provide roughly a 1.5x speed-up, whereas, on a\n300-hr Switchboard task, these techniques provide over a 2.3x speedup, with no\nloss in WER. These results suggest that even further speed-up is expected, as\nproblems scale and complexity grows.\n",
        "published": "2013",
        "authors": [
            "Tara N. Sainath",
            "Lior Horesh",
            "Brian Kingsbury",
            "Aleksandr Y. Aravkin",
            "Bhuvana Ramabhadran"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1409.2944v2",
        "title": "Collaborative Deep Learning for Recommender Systems",
        "abstract": "  Collaborative filtering (CF) is a successful approach commonly used by many\nrecommender systems. Conventional CF-based methods use the ratings given to\nitems by users as the sole source of information for learning to make\nrecommendation. However, the ratings are often very sparse in many\napplications, causing CF-based methods to degrade significantly in their\nrecommendation performance. To address this sparsity problem, auxiliary\ninformation such as item content information may be utilized. Collaborative\ntopic regression (CTR) is an appealing recent method taking this approach which\ntightly couples the two components that learn from two different sources of\ninformation. Nevertheless, the latent representation learned by CTR may not be\nvery effective when the auxiliary information is very sparse. To address this\nproblem, we generalize recent advances in deep learning from i.i.d. input to\nnon-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian\nmodel called collaborative deep learning (CDL), which jointly performs deep\nrepresentation learning for the content information and collaborative filtering\nfor the ratings (feedback) matrix. Extensive experiments on three real-world\ndatasets from different domains show that CDL can significantly advance the\nstate of the art.\n",
        "published": "2014",
        "authors": [
            "Hao Wang",
            "Naiyan Wang",
            "Dit-Yan Yeung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.01416v1",
        "title": "Language as a matrix product state",
        "abstract": "  We propose a statistical model for natural language that begins by\nconsidering language as a monoid, then representing it in complex matrices with\na compatible translation invariant probability measure. We interpret the\nprobability measure as arising via the Born rule from a translation invariant\nmatrix product state.\n",
        "published": "2017",
        "authors": [
            "Vasily Pestun",
            "John Terilla",
            "Yiannis Vlassopoulos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.06887v2",
        "title": "Can a Fruit Fly Learn Word Embeddings?",
        "abstract": "  The mushroom body of the fruit fly brain is one of the best studied systems\nin neuroscience. At its core it consists of a population of Kenyon cells, which\nreceive inputs from multiple sensory modalities. These cells are inhibited by\nthe anterior paired lateral neuron, thus creating a sparse high dimensional\nrepresentation of the inputs. In this work we study a mathematical\nformalization of this network motif and apply it to learning the correlational\nstructure between words and their context in a corpus of unstructured text, a\ncommon natural language processing (NLP) task. We show that this network can\nlearn semantic representations of words and can generate both static and\ncontext-dependent word embeddings. Unlike conventional methods (e.g., BERT,\nGloVe) that use dense representations for word embedding, our algorithm encodes\nsemantic meaning of words and their context in the form of sparse binary hash\ncodes. The quality of the learned representations is evaluated on word\nsimilarity analysis, word-sense disambiguation, and document classification. It\nis shown that not only can the fruit fly network motif achieve performance\ncomparable to existing methods in NLP, but, additionally, it uses only a\nfraction of the computational resources (shorter training time and smaller\nmemory footprint).\n",
        "published": "2021",
        "authors": [
            "Yuchen Liang",
            "Chaitanya K. Ryali",
            "Benjamin Hoover",
            "Leopold Grinberg",
            "Saket Navlakha",
            "Mohammed J. Zaki",
            "Dmitry Krotov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.07298v1",
        "title": "Explaining Predictions of Non-Linear Classifiers in NLP",
        "abstract": "  Layer-wise relevance propagation (LRP) is a recently proposed technique for\nexplaining predictions of complex non-linear classifiers in terms of input\nvariables. In this paper, we apply LRP for the first time to natural language\nprocessing (NLP). More precisely, we use it to explain the predictions of a\nconvolutional neural network (CNN) trained on a topic categorization task. Our\nanalysis highlights which words are relevant for a specific prediction of the\nCNN. We compare our technique to standard sensitivity analysis, both\nqualitatively and quantitatively, using a \"word deleting\" perturbation\nexperiment, a PCA analysis, and various visualizations. All experiments\nvalidate the suitability of LRP for explaining the CNN predictions, which is\nalso in line with results reported in recent image classification studies.\n",
        "published": "2016",
        "authors": [
            "Leila Arras",
            "Franziska Horn",
            "Gr\u00e9goire Montavon",
            "Klaus-Robert M\u00fcller",
            "Wojciech Samek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.10248v2",
        "title": "Tensor network language model",
        "abstract": "  We propose a new statistical model suitable for machine learning of systems\nwith long distance correlations such as natural languages. The model is based\non directed acyclic graph decorated by multi-linear tensor maps in the vertices\nand vector spaces in the edges, called tensor network. Such tensor networks\nhave been previously employed for effective numerical computation of the\nrenormalization group flow on the space of effective quantum field theories and\nlattice models of statistical mechanics. We provide explicit algebro-geometric\nanalysis of the parameter moduli space for tree graphs, discuss model\nproperties and applications such as statistical translation.\n",
        "published": "2017",
        "authors": [
            "Vasily Pestun",
            "Yiannis Vlassopoulos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.07382v3",
        "title": "Analysis of Railway Accidents' Narratives Using Deep Learning",
        "abstract": "  Automatic understanding of domain specific texts in order to extract useful\nrelationships for later use is a non-trivial task. One such relationship would\nbe between railroad accidents' causes and their correspondent descriptions in\nreports. From 2001 to 2016 rail accidents in the U.S. cost more than $4.6B.\nRailroads involved in accidents are required to submit an accident report to\nthe Federal Railroad Administration (FRA). These reports contain a variety of\nfixed field entries including primary cause of the accidents (a coded variable\nwith 389 values) as well as a narrative field which is a short text description\nof the accident. Although these narratives provide more information than a\nfixed field entry, the terminologies used in these reports are not easy to\nunderstand by a non-expert reader. Therefore, providing an assisting method to\nfill in the primary cause from such domain specific texts(narratives) would\nhelp to label the accidents with more accuracy. Another important question for\ntransportation safety is whether the reported accident cause is consistent with\nnarrative description. To address these questions, we applied deep learning\nmethods together with powerful word embeddings such as Word2Vec and GloVe to\nclassify accident cause values for the primary cause field using the text in\nthe narratives. The results show that such approaches can both accurately\nclassify accident causes based on report narratives and find important\ninconsistencies in accident reporting.\n",
        "published": "2018",
        "authors": [
            "Mojtaba Heidarysafa",
            "Kamran Kowsari",
            "Laura E. Barnes",
            "Donald E. Brown"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.12315v1",
        "title": "Stable Parallel Training of Wasserstein Conditional Generative\n  Adversarial Neural Networks",
        "abstract": "  We propose a stable, parallel approach to train Wasserstein Conditional\nGenerative Adversarial Neural Networks (W-CGANs) under the constraint of a\nfixed computational budget. Differently from previous distributed GANs training\ntechniques, our approach avoids inter-process communications, reduces the risk\nof mode collapse and enhances scalability by using multiple generators, each\none of them concurrently trained on a single data label. The use of the\nWasserstein metric also reduces the risk of cycling by stabilizing the training\nof each generator. We illustrate the approach on the CIFAR10, CIFAR100, and\nImageNet1k datasets, three standard benchmark image datasets, maintaining the\noriginal resolution of the images for each dataset. Performance is assessed in\nterms of scalability and final accuracy within a limited fixed computational\ntime and computational resources. To measure accuracy, we use the inception\nscore, the Frechet inception distance, and image quality. An improvement in\ninception score and Frechet inception distance is shown in comparison to\nprevious results obtained by performing the parallel approach on deep\nconvolutional conditional generative adversarial neural networks (DC-CGANs) as\nwell as an improvement of image quality of the new images created by the GANs\napproach. Weak scaling is attained on both datasets using up to 2,000 NVIDIA\nV100 GPUs on the OLCF supercomputer Summit.\n",
        "published": "2022",
        "authors": [
            "Massimiliano Lupo Pasini",
            "Junqi Yin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.10476v1",
        "title": "Research Directions in Democratizing Innovation through Design\n  Automation, One-Click Manufacturing Services and Intelligent Machines",
        "abstract": "  The digitalization of manufacturing has created opportunities for consumers\nto customize products that fit their individualized needs which in turn would\ndrive demand for manufacturing services. However, this pull-based manufacturing\nsystem production of extremely low quantity and limitless variety for products\nis expensive to implement. New emerging technology in design automation driven\nby data-driven computational design, manufacturing-as-a-service marketplaces\nand digitally enabled micro-factories holds promise towards democratization of\ninnovation. In this paper, scientific, technology and infrastructure challenges\nare identified and if solved, the impact of these emerging technologies on\nproduct innovation and future factory organization is discussed.\n",
        "published": "2019",
        "authors": [
            "Binil Starly",
            "Atin Angrish",
            "Paul Cohen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.13170v4",
        "title": "AdaBest: Minimizing Client Drift in Federated Learning via Adaptive Bias\n  Estimation",
        "abstract": "  In Federated Learning (FL), a number of clients or devices collaborate to\ntrain a model without sharing their data. Models are optimized locally at each\nclient and further communicated to a central hub for aggregation. While FL is\nan appealing decentralized training paradigm, heterogeneity among data from\ndifferent clients can cause the local optimization to drift away from the\nglobal objective. In order to estimate and therefore remove this drift,\nvariance reduction techniques have been incorporated into FL optimization\nrecently. However, these approaches inaccurately estimate the clients' drift\nand ultimately fail to remove it properly. In this work, we propose an adaptive\nalgorithm that accurately estimates drift across clients. In comparison to\nprevious works, our approach necessitates less storage and communication\nbandwidth, as well as lower compute costs. Additionally, our proposed\nmethodology induces stability by constraining the norm of estimates for client\ndrift, making it more practical for large scale FL. Experimental findings\ndemonstrate that the proposed algorithm converges significantly faster and\nachieves higher accuracy than the baselines across various FL benchmarks.\n",
        "published": "2022",
        "authors": [
            "Farshid Varno",
            "Marzie Saghayi",
            "Laya Rafiee Sevyeri",
            "Sharut Gupta",
            "Stan Matwin",
            "Mohammad Havaei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1003.5956v2",
        "title": "Unbiased Offline Evaluation of Contextual-bandit-based News Article\n  Recommendation Algorithms",
        "abstract": "  Contextual bandit algorithms have become popular for online recommendation\nsystems such as Digg, Yahoo! Buzz, and news recommendation in general.\n\\emph{Offline} evaluation of the effectiveness of new algorithms in these\napplications is critical for protecting online user experiences but very\nchallenging due to their \"partial-label\" nature. Common practice is to create a\nsimulator which simulates the online environment for the problem at hand and\nthen run an algorithm against this simulator. However, creating simulator\nitself is often difficult and modeling bias is usually unavoidably introduced.\nIn this paper, we introduce a \\emph{replay} methodology for contextual bandit\nalgorithm evaluation. Different from simulator-based approaches, our method is\ncompletely data-driven and very easy to adapt to different applications. More\nimportantly, our method can provide provably unbiased evaluations. Our\nempirical results on a large-scale news article recommendation dataset\ncollected from Yahoo! Front Page conform well with our theoretical results.\nFurthermore, comparisons between our offline replay and online bucket\nevaluation of several contextual bandit algorithms show accuracy and\neffectiveness of our offline evaluation method.\n",
        "published": "2010",
        "authors": [
            "Lihong Li",
            "Wei Chu",
            "John Langford",
            "Xuanhui Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.05992v1",
        "title": "Improved GQ-CNN: Deep Learning Model for Planning Robust Grasps",
        "abstract": "  Recent developments in the field of robot grasping have shown great\nimprovements in the grasp success rates when dealing with unknown objects. In\nthis work we improve on one of the most promising approaches, the Grasp Quality\nConvolutional Neural Network (GQ-CNN) trained on the DexNet 2.0 dataset. We\npropose a new architecture for the GQ-CNN and describe practical improvements\nthat increase the model validation accuracy from 92.2% to 95.8% and from 85.9%\nto 88.0% on respectively image-wise and object-wise training and validation\nsplits.\n",
        "published": "2018",
        "authors": [
            "Maciej Ja\u015bkowski",
            "Jakub \u015awi\u0105tkowski",
            "Micha\u0142 Zaj\u0105c",
            "Maciej Klimek",
            "Jarek Potiuk",
            "Piotr Rybicki",
            "Piotr Polatowski",
            "Przemys\u0142aw Walczyk",
            "Kacper Nowicki",
            "Marek Cygan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.03719v3",
        "title": "DeepMoTIon: Learning to Navigate Like Humans",
        "abstract": "  We present a novel human-aware navigation approach, where the robot learns to\nmimic humans to navigate safely in crowds. The presented model, referred to as\nDeepMoTIon, is trained with pedestrian surveillance data to predict human\nvelocity in the environment. The robot processes LiDAR scans via the trained\nnetwork to navigate to the target location. We conduct extensive experiments to\nassess the components of our network and prove their necessity to imitate\nhumans. Our experiments show that DeepMoTIion outperforms all the benchmarks in\nterms of human imitation, achieving a 24% reduction in time series-based path\ndeviation over the next best approach. In addition, while many other approaches\noften failed to reach the target, our method reached the target in 100% of the\ntest cases while complying with social norms and ensuring human safety.\n",
        "published": "2018",
        "authors": [
            "Mahmoud Hamandi",
            "Mike D'Arcy",
            "Pooyan Fazli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.11503v1",
        "title": "Contrasting Exploration in Parameter and Action Space: A Zeroth-Order\n  Optimization Perspective",
        "abstract": "  Black-box optimizers that explore in parameter space have often been shown to\noutperform more sophisticated action space exploration methods developed\nspecifically for the reinforcement learning problem. We examine these black-box\nmethods closely to identify situations in which they are worse than action\nspace exploration methods and those in which they are superior. Through simple\ntheoretical analyses, we prove that complexity of exploration in parameter\nspace depends on the dimensionality of parameter space, while complexity of\nexploration in action space depends on both the dimensionality of action space\nand horizon length. This is also demonstrated empirically by comparing simple\nexploration methods on several model problems, including Contextual Bandit,\nLinear Regression and Reinforcement Learning in continuous control.\n",
        "published": "2019",
        "authors": [
            "Anirudh Vemula",
            "Wen Sun",
            "J. Andrew Bagnell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.04072v1",
        "title": "Implications of Human Irrationality for Reinforcement Learning",
        "abstract": "  Recent work in the behavioural sciences has begun to overturn the long-held\nbelief that human decision making is irrational, suboptimal and subject to\nbiases. This turn to the rational suggests that human decision making may be a\nbetter source of ideas for constraining how machine learning problems are\ndefined than would otherwise be the case. One promising idea concerns human\ndecision making that is dependent on apparently irrelevant aspects of the\nchoice context. Previous work has shown that by taking into account choice\ncontext and making relational observations, people can maximize expected value.\nOther work has shown that Partially observable Markov decision processes\n(POMDPs) are a useful way to formulate human-like decision problems. Here, we\npropose a novel POMDP model for contextual choice tasks and show that, despite\nthe apparent irrationalities, a reinforcement learner can take advantage of the\nway that humans make decisions. We suggest that human irrationalities may offer\na productive source of inspiration for improving the design of AI architectures\nand machine learning methods.\n",
        "published": "2020",
        "authors": [
            "Haiyang Chen",
            "Hyung Jin Chang",
            "Andrew Howes"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.01298v1",
        "title": "Beyond Tabula-Rasa: a Modular Reinforcement Learning Approach for\n  Physically Embedded 3D Sokoban",
        "abstract": "  Intelligent robots need to achieve abstract objectives using concrete,\nspatiotemporally complex sensory information and motor control. Tabula rasa\ndeep reinforcement learning (RL) has tackled demanding tasks in terms of either\nvisual, abstract, or physical reasoning, but solving these jointly remains a\nformidable challenge. One recent, unsolved benchmark task that integrates these\nchallenges is Mujoban, where a robot needs to arrange 3D warehouses generated\nfrom 2D Sokoban puzzles. We explore whether integrated tasks like Mujoban can\nbe solved by composing RL modules together in a sense-plan-act hierarchy, where\nmodules have well-defined roles similarly to classic robot architectures.\nUnlike classic architectures that are typically model-based, we use only\nmodel-free modules trained with RL or supervised learning. We find that our\nmodular RL approach dramatically outperforms the state-of-the-art monolithic RL\nagent on Mujoban. Further, learned modules can be reused when, e.g., using a\ndifferent robot platform to solve the same task. Together our results give\nstrong evidence for the importance of research into modular RL designs. Project\nwebsite: https://sites.google.com/view/modular-rl/\n",
        "published": "2020",
        "authors": [
            "Peter Karkus",
            "Mehdi Mirza",
            "Arthur Guez",
            "Andrew Jaegle",
            "Timothy Lillicrap",
            "Lars Buesing",
            "Nicolas Heess",
            "Theophane Weber"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.14497v2",
        "title": "Conservative Safety Critics for Exploration",
        "abstract": "  Safe exploration presents a major challenge in reinforcement learning (RL):\nwhen active data collection requires deploying partially trained policies, we\nmust ensure that these policies avoid catastrophically unsafe regions, while\nstill enabling trial and error learning. In this paper, we target the problem\nof safe exploration in RL by learning a conservative safety estimate of\nenvironment states through a critic, and provably upper bound the likelihood of\ncatastrophic failures at every training iteration. We theoretically\ncharacterize the tradeoff between safety and policy improvement, show that the\nsafety constraints are likely to be satisfied with high probability during\ntraining, derive provable convergence guarantees for our approach, which is no\nworse asymptotically than standard RL, and demonstrate the efficacy of the\nproposed approach on a suite of challenging navigation, manipulation, and\nlocomotion tasks. Empirically, we show that the proposed approach can achieve\ncompetitive task performance while incurring significantly lower catastrophic\nfailure rates during training than prior methods. Videos are at this url\nhttps://sites.google.com/view/conservative-safety-critics/home\n",
        "published": "2020",
        "authors": [
            "Homanga Bharadhwaj",
            "Aviral Kumar",
            "Nicholas Rhinehart",
            "Sergey Levine",
            "Florian Shkurti",
            "Animesh Garg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.02422v3",
        "title": "Simulator Predictive Control: Using Learned Task Representations and MPC\n  for Zero-Shot Generalization and Sequencing",
        "abstract": "  Simulation-to-real transfer is an important strategy for making reinforcement\nlearning practical with real robots. Successful sim-to-real transfer systems\nhave difficulty producing policies which generalize across tasks, despite\ntraining for thousands of hours equivalent real robot time. To address this\nshortcoming, we present a novel approach to efficiently learning new robotic\nskills directly on a real robot, based on model-predictive control (MPC) and an\nalgorithm for learning task representations. In short, we show how to reuse the\nsimulation from the pre-training step of sim-to-real methods as a tool for\nforesight, allowing the sim-to-real policy adapt to unseen tasks. Rather than\nend-to-end learning policies for single tasks and attempting to transfer them,\nwe first use simulation to simultaneously learn (1) a continuous\nparameterization (i.e. a skill embedding or latent) of task-appropriate\nprimitive skills, and (2) a single policy for these skills which is conditioned\non this representation. We then directly transfer our multi-skill policy to a\nreal robot, and actuate the robot by choosing sequences of skill latents which\nactuate the policy, with each latent corresponding to a pre-learned primitive\nskill controller. We complete unseen tasks by choosing new sequences of skill\nlatents to control the robot using MPC, where our MPC model is composed of the\npre-trained skill policy executed in the simulation environment, run in\nparallel with the real robot. We discuss the background and principles of our\nmethod, detail its practical implementation, and evaluate its performance by\nusing our method to train a real Sawyer Robot to achieve motion tasks such as\ndrawing and block pushing.\n",
        "published": "2018",
        "authors": [
            "Zhanpeng He",
            "Ryan Julian",
            "Eric Heiden",
            "Hejia Zhang",
            "Stefan Schaal",
            "Joseph J. Lim",
            "Gaurav Sukhatme",
            "Karol Hausman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.11177v1",
        "title": "Learning sparse relational transition models",
        "abstract": "  We present a representation for describing transition models in complex\nuncertain domains using relational rules. For any action, a rule selects a set\nof relevant objects and computes a distribution over properties of just those\nobjects in the resulting state given their properties in the previous state. An\niterative greedy algorithm is used to construct a set of deictic references\nthat determine which objects are relevant in any given state. Feed-forward\nneural networks are used to learn the transition distribution on the relevant\nobjects' properties. This strategy is demonstrated to be both more versatile\nand more sample efficient than learning a monolithic transition model in a\nsimulated domain in which a robot pushes stacks of objects on a cluttered\ntable.\n",
        "published": "2018",
        "authors": [
            "Victoria Xia",
            "Zi Wang",
            "Leslie Pack Kaelbling"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.10681v2",
        "title": "Composing Task-Agnostic Policies with Deep Reinforcement Learning",
        "abstract": "  The composition of elementary behaviors to solve challenging transfer\nlearning problems is one of the key elements in building intelligent machines.\nTo date, there has been plenty of work on learning task-specific policies or\nskills but almost no focus on composing necessary, task-agnostic skills to find\na solution to new problems. In this paper, we propose a novel deep\nreinforcement learning-based skill transfer and composition method that takes\nthe agent's primitive policies to solve unseen tasks. We evaluate our method in\ndifficult cases where training policy through standard reinforcement learning\n(RL) or even hierarchical RL is either not feasible or exhibits high sample\ncomplexity. We show that our method not only transfers skills to new problem\nsettings but also solves the challenging environments requiring both task\nplanning and motion control with high data efficiency.\n",
        "published": "2019",
        "authors": [
            "Ahmed H. Qureshi",
            "Jacob J. Johnson",
            "Yuzhe Qin",
            "Taylor Henderson",
            "Byron Boots",
            "Michael C. Yip"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.11602v1",
        "title": "Differentiable Algorithm Networks for Composable Robot Learning",
        "abstract": "  This paper introduces the Differentiable Algorithm Network (DAN), a\ncomposable architecture for robot learning systems. A DAN is composed of neural\nnetwork modules, each encoding a differentiable robot algorithm and an\nassociated model; and it is trained end-to-end from data. DAN combines the\nstrengths of model-driven modular system design and data-driven end-to-end\nlearning. The algorithms and models act as structural assumptions to reduce the\ndata requirements for learning; end-to-end learning allows the modules to adapt\nto one another and compensate for imperfect models and algorithms, in order to\nachieve the best overall system performance. We illustrate the DAN methodology\nthrough a case study on a simulated robot system, which learns to navigate in\ncomplex 3-D environments with only local visual observations and an image of a\npartially correct 2-D floor map.\n",
        "published": "2019",
        "authors": [
            "Peter Karkus",
            "Xiao Ma",
            "David Hsu",
            "Leslie Pack Kaelbling",
            "Wee Sun Lee",
            "Tomas Lozano-Perez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.09205v1",
        "title": "Continual Reinforcement Learning with Diversity Exploration and\n  Adversarial Self-Correction",
        "abstract": "  Deep reinforcement learning has made significant progress in the field of\ncontinuous control, such as physical control and autonomous driving. However,\nit is challenging for a reinforcement model to learn a policy for each task\nsequentially due to catastrophic forgetting. Specifically, the model would\nforget knowledge it learned in the past when trained on a new task. We consider\nthis challenge from two perspectives: i) acquiring task-specific skills is\ndifficult since task information and rewards are not highly related; ii)\nlearning knowledge from previous experience is difficult in continuous control\ndomains. In this paper, we introduce an end-to-end framework namely Continual\nDiversity Adversarial Network (CDAN). We first develop an unsupervised\ndiversity exploration method to learn task-specific skills using an\nunsupervised objective. Then, we propose an adversarial self-correction\nmechanism to learn knowledge by exploiting past experience. The two learning\nprocedures are presumably reciprocal. To evaluate the proposed method, we\npropose a new continuous reinforcement learning environment named Continual Ant\nMaze (CAM) and a new metric termed Normalized Shorten Distance (NSD). The\nexperimental results confirm the effectiveness of diversity exploration and\nself-correction. It is worthwhile noting that our final result outperforms\nbaseline by 18.35% in terms of NSD, and 0.61 according to the average reward.\n",
        "published": "2019",
        "authors": [
            "Fengda Zhu",
            "Xiaojun Chang",
            "Runhao Zeng",
            "Mingkui Tan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.06833v1",
        "title": "Improved Exploration through Latent Trajectory Optimization in Deep\n  Deterministic Policy Gradient",
        "abstract": "  Model-free reinforcement learning algorithms such as Deep Deterministic\nPolicy Gradient (DDPG) often require additional exploration strategies,\nespecially if the actor is of deterministic nature. This work evaluates the use\nof model-based trajectory optimization methods used for exploration in Deep\nDeterministic Policy Gradient when trained on a latent image embedding. In\naddition, an extension of DDPG is derived using a value function as critic,\nmaking use of a learned deep dynamics model to compute the policy gradient.\nThis approach leads to a symbiotic relationship between the deep reinforcement\nlearning algorithm and the latent trajectory optimizer. The trajectory\noptimizer benefits from the critic learned by the RL algorithm and the latter\nfrom the enhanced exploration generated by the planner. The developed methods\nare evaluated on two continuous control tasks, one in simulation and one in the\nreal world. In particular, a Baxter robot is trained to perform an insertion\ntask, while only receiving sparse rewards and images as observations from the\nenvironment.\n",
        "published": "2019",
        "authors": [
            "Kevin Sebastian Luck",
            "Mel Vecerik",
            "Simon Stepputtis",
            "Heni Ben Amor",
            "Jonathan Scholz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.08444v1",
        "title": "MANGA: Method Agnostic Neural-policy Generalization and Adaptation",
        "abstract": "  In this paper we target the problem of transferring policies across multiple\nenvironments with different dynamics parameters and motor noise variations, by\nintroducing a framework that decouples the processes of policy learning and\nsystem identification. Efficiently transferring learned policies to an unknown\nenvironment with changes in dynamics configurations in the presence of motor\nnoise is very important for operating robots in the real world, and our work is\na novel attempt in that direction. We introduce MANGA: Method Agnostic\nNeural-policy Generalization and Adaptation, that trains dynamics conditioned\npolicies and efficiently learns to estimate the dynamics parameters of the\nenvironment given off-policy state-transition rollouts in the environment. Our\nscheme is agnostic to the type of training method used - both reinforcement\nlearning (RL) and imitation learning (IL) strategies can be used. We\ndemonstrate the effectiveness of our approach by experimenting with four\ndifferent MuJoCo agents and comparing against previously proposed transfer\nbaselines.\n",
        "published": "2019",
        "authors": [
            "Homanga Bharadhwaj",
            "Shoichiro Yamaguchi",
            "Shin-ichi Maeda"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1307.0813v2",
        "title": "Multi-Task Policy Search",
        "abstract": "  Learning policies that generalize across multiple tasks is an important and\nchallenging research topic in reinforcement learning and robotics. Training\nindividual policies for every single potential task is often impractical,\nespecially for continuous task variations, requiring more principled approaches\nto share and transfer knowledge among similar tasks. We present a novel\napproach for learning a nonlinear feedback policy that generalizes across\nmultiple tasks. The key idea is to define a parametrized policy as a function\nof both the state and the task, which allows learning a single policy that\ngeneralizes across multiple known and unknown tasks. Applications of our novel\napproach to reinforcement and imitation learning in real-robot experiments are\nshown.\n",
        "published": "2013",
        "authors": [
            "Marc Peter Deisenroth",
            "Peter Englert",
            "Jan Peters",
            "Dieter Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1605.09674v4",
        "title": "VIME: Variational Information Maximizing Exploration",
        "abstract": "  Scalable and effective exploration remains a key challenge in reinforcement\nlearning (RL). While there are methods with optimality guarantees in the\nsetting of discrete state and action spaces, these methods cannot be applied in\nhigh-dimensional deep RL scenarios. As such, most contemporary RL relies on\nsimple heuristics such as epsilon-greedy exploration or adding Gaussian noise\nto the controls. This paper introduces Variational Information Maximizing\nExploration (VIME), an exploration strategy based on maximization of\ninformation gain about the agent's belief of environment dynamics. We propose a\npractical implementation, using variational inference in Bayesian neural\nnetworks which efficiently handles continuous state and action spaces. VIME\nmodifies the MDP reward function, and can be applied with several different\nunderlying RL algorithms. We demonstrate that VIME achieves significantly\nbetter performance compared to heuristic exploration methods across a variety\nof continuous control tasks and algorithms, including tasks with very sparse\nrewards.\n",
        "published": "2016",
        "authors": [
            "Rein Houthooft",
            "Xi Chen",
            "Yan Duan",
            "John Schulman",
            "Filip De Turck",
            "Pieter Abbeel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1609.07560v1",
        "title": "Informative Planning and Online Learning with Sparse Gaussian Processes",
        "abstract": "  A big challenge in environmental monitoring is the spatiotemporal variation\nof the phenomena to be observed. To enable persistent sensing and estimation in\nsuch a setting, it is beneficial to have a time-varying underlying\nenvironmental model. Here we present a planning and learning method that\nenables an autonomous marine vehicle to perform persistent ocean monitoring\ntasks by learning and refining an environmental model. To alleviate the\ncomputational bottleneck caused by large-scale data accumulated, we propose a\nframework that iterates between a planning component aimed at collecting the\nmost information-rich data, and a sparse Gaussian Process learning component\nwhere the environmental model and hyperparameters are learned online by taking\nadvantage of only a subset of data that provides the greatest contribution. Our\nsimulations with ground-truth ocean data shows that the proposed method is both\naccurate and efficient.\n",
        "published": "2016",
        "authors": [
            "Kai-Chieh Ma",
            "Lantao Liu",
            "Gaurav S. Sukhatme"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.04070v2",
        "title": "Prediction and Control with Temporal Segment Models",
        "abstract": "  We introduce a method for learning the dynamics of complex nonlinear systems\nbased on deep generative models over temporal segments of states and actions.\nUnlike dynamics models that operate over individual discrete timesteps, we\nlearn the distribution over future state trajectories conditioned on past\nstate, past action, and planned future action trajectories, as well as a latent\nprior over action trajectories. Our approach is based on convolutional\nautoregressive models and variational autoencoders. It makes stable and\naccurate predictions over long horizons for complex, stochastic systems,\neffectively expressing uncertainty and modeling the effects of collisions,\nsensory noise, and action delays. The learned dynamics model and action prior\ncan be used for end-to-end, fully differentiable trajectory optimization and\nmodel-based policy optimization, which we use to evaluate the performance and\nsample-efficiency of our method.\n",
        "published": "2017",
        "authors": [
            "Nikhil Mishra",
            "Pieter Abbeel",
            "Igor Mordatch"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.09310v2",
        "title": "Adaptive Simulation-based Training of AI Decision-makers using Bayesian\n  Optimization",
        "abstract": "  This work studies how an AI-controlled dog-fighting agent with tunable\ndecision-making parameters can learn to optimize performance against an\nintelligent adversary, as measured by a stochastic objective function evaluated\non simulated combat engagements. Gaussian process Bayesian optimization (GPBO)\ntechniques are developed to automatically learn global Gaussian Process (GP)\nsurrogate models, which provide statistical performance predictions in both\nexplored and unexplored areas of the parameter space. This allows a learning\nengine to sample full-combat simulations at parameter values that are most\nlikely to optimize performance and also provide highly informative data points\nfor improving future predictions. However, standard GPBO methods do not provide\na reliable surrogate model for the highly volatile objective functions found in\naerial combat, and thus do not reliably identify global maxima. These issues\nare addressed by novel Repeat Sampling (RS) and Hybrid Repeat/Multi-point\nSampling (HRMS) techniques. Simulation studies show that HRMS improves the\naccuracy of GP surrogate models, allowing AI decision-makers to more accurately\npredict performance and efficiently tune parameters.\n",
        "published": "2017",
        "authors": [
            "Brett W. Israelsen",
            "Nisar Ahmed",
            "Kenneth Center",
            "Roderick Green",
            "Winston Bennett Jr"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.07479v1",
        "title": "Teaching a Machine to Read Maps with Deep Reinforcement Learning",
        "abstract": "  The ability to use a 2D map to navigate a complex 3D environment is quite\nremarkable, and even difficult for many humans. Localization and navigation is\nalso an important problem in domains such as robotics, and has recently become\na focus of the deep reinforcement learning community. In this paper we teach a\nreinforcement learning agent to read a map in order to find the shortest way\nout of a random maze it has never seen before. Our system combines several\nstate-of-the-art methods such as A3C and incorporates novel elements such as a\nrecurrent localization cell. Our agent learns to localize itself based on 3D\nfirst person images and an approximate orientation angle. The agent generalizes\nwell to bigger mazes, showing that it learned useful localization and\nnavigation capabilities.\n",
        "published": "2017",
        "authors": [
            "Gino Brunner",
            "Oliver Richter",
            "Yuyi Wang",
            "Roger Wattenhofer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.04765v1",
        "title": "Progressive Reinforcement Learning with Distillation for Multi-Skilled\n  Motion Control",
        "abstract": "  Deep reinforcement learning has demonstrated increasing capabilities for\ncontinuous control problems, including agents that can move with skill and\nagility through their environment. An open problem in this setting is that of\ndeveloping good strategies for integrating or merging policies for multiple\nskills, where each individual skill is a specialist in a specific skill and its\nassociated state distribution. We extend policy distillation methods to the\ncontinuous action setting and leverage this technique to combine expert\npolicies, as evaluated in the domain of simulated bipedal locomotion across\ndifferent classes of terrain. We also introduce an input injection method for\naugmenting an existing policy network to exploit new input features. Lastly,\nour method uses transfer learning to assist in the efficient acquisition of new\nskills. The combination of these methods allows a policy to be incrementally\naugmented with new skills. We compare our progressive learning and integration\nvia distillation (PLAID) method against three alternative baselines.\n",
        "published": "2018",
        "authors": [
            "Glen Berseth",
            "Cheng Xie",
            "Paul Cernek",
            "Michiel Van de Panne"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.08013v2",
        "title": "Intrinsic Motivation and Mental Replay enable Efficient Online\n  Adaptation in Stochastic Recurrent Networks",
        "abstract": "  Autonomous robots need to interact with unknown, unstructured and changing\nenvironments, constantly facing novel challenges. Therefore, continuous online\nadaptation for lifelong-learning and the need of sample-efficient mechanisms to\nadapt to changes in the environment, the constraints, the tasks, or the robot\nitself are crucial. In this work, we propose a novel framework for\nprobabilistic online motion planning with online adaptation based on a\nbio-inspired stochastic recurrent neural network. By using learning signals\nwhich mimic the intrinsic motivation signalcognitive dissonance in addition\nwith a mental replay strategy to intensify experiences, the stochastic\nrecurrent network can learn from few physical interactions and adapts to novel\nenvironments in seconds. We evaluate our online planning and adaptation\nframework on an anthropomorphic KUKA LWR arm. The rapid online adaptation is\nshown by learning unknown workspace constraints sample-efficiently from few\nphysical interactions while following given way points.\n",
        "published": "2018",
        "authors": [
            "Daniel Tanneberg",
            "Jan Peters",
            "Elmar Rueckert"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.10463v1",
        "title": "DiGrad: Multi-Task Reinforcement Learning with Shared Actions",
        "abstract": "  Most reinforcement learning algorithms are inefficient for learning multiple\ntasks in complex robotic systems, where different tasks share a set of actions.\nIn such environments a compound policy may be learnt with shared neural network\nparameters, which performs multiple tasks concurrently. However such compound\npolicy may get biased towards a task or the gradients from different tasks\nnegate each other, making the learning unstable and sometimes less data\nefficient. In this paper, we propose a new approach for simultaneous training\nof multiple tasks sharing a set of common actions in continuous action spaces,\nwhich we call as DiGrad (Differential Policy Gradient). The proposed framework\nis based on differential policy gradients and can accommodate multi-task\nlearning in a single actor-critic network. We also propose a simple heuristic\nin the differential policy gradient update to further improve the learning. The\nproposed architecture was tested on 8 link planar manipulator and 27 degrees of\nfreedom(DoF) Humanoid for learning multi-goal reachability tasks for 3 and 2\nend effectors respectively. We show that our approach supports efficient\nmulti-task learning in complex robotic systems, outperforming related methods\nin continuous action spaces.\n",
        "published": "2018",
        "authors": [
            "Parijat Dewangan",
            "S Phaniteja",
            "K Madhava Krishna",
            "Abhishek Sarkar",
            "Balaraman Ravindran"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.00196v1",
        "title": "Learning Flexible and Reusable Locomotion Primitives for a Microrobot",
        "abstract": "  The design of gaits for robot locomotion can be a daunting process which\nrequires significant expert knowledge and engineering. This process is even\nmore challenging for robots that do not have an accurate physical model, such\nas compliant or micro-scale robots. Data-driven gait optimization provides an\nautomated alternative to analytical gait design. In this paper, we propose a\nnovel approach to efficiently learn a wide range of locomotion tasks with\nwalking robots. This approach formalizes locomotion as a contextual policy\nsearch task to collect data, and subsequently uses that data to learn\nmulti-objective locomotion primitives that can be used for planning. As a\nproof-of-concept we consider a simulated hexapod modeled after a recently\ndeveloped microrobot, and we thoroughly evaluate the performance of this\nmicrorobot on different tasks and gaits. Our results validate the proposed\ncontroller and learning scheme on single and multi-objective locomotion tasks.\nMoreover, the experimental simulations show that without any prior knowledge\nabout the robot used (e.g., dynamics model), our approach is capable of\nlearning locomotion primitives within 250 trials and subsequently using them to\nsuccessfully navigate through a maze.\n",
        "published": "2018",
        "authors": [
            "Brian Yang",
            "Grant Wang",
            "Roberto Calandra",
            "Daniel Contreras",
            "Sergey Levine",
            "Kristofer Pister"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.06773v1",
        "title": "Composable Deep Reinforcement Learning for Robotic Manipulation",
        "abstract": "  Model-free deep reinforcement learning has been shown to exhibit good\nperformance in domains ranging from video games to simulated robotic\nmanipulation and locomotion. However, model-free methods are known to perform\npoorly when the interaction time with the environment is limited, as is the\ncase for most real-world robotic tasks. In this paper, we study how maximum\nentropy policies trained using soft Q-learning can be applied to real-world\nrobotic manipulation. The application of this method to real-world manipulation\nis facilitated by two important features of soft Q-learning. First, soft\nQ-learning can learn multimodal exploration strategies by learning policies\nrepresented by expressive energy-based models. Second, we show that policies\nlearned with soft Q-learning can be composed to create new policies, and that\nthe optimality of the resulting policy can be bounded in terms of the\ndivergence between the composed policies. This compositionality provides an\nespecially valuable tool for real-world manipulation, where constructing new\npolicies by composing existing skills can provide a large gain in efficiency\nover training from scratch. Our experimental evaluation demonstrates that soft\nQ-learning is substantially more sample efficient than prior model-free deep\nreinforcement learning methods, and that compositionality can be performed for\nboth simulated and real-world tasks.\n",
        "published": "2018",
        "authors": [
            "Tuomas Haarnoja",
            "Vitchyr Pong",
            "Aurick Zhou",
            "Murtaza Dalal",
            "Pieter Abbeel",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.07067v1",
        "title": "Setting up a Reinforcement Learning Task with a Real-World Robot",
        "abstract": "  Reinforcement learning is a promising approach to developing hard-to-engineer\nadaptive solutions for complex and diverse robotic tasks. However, learning\nwith real-world robots is often unreliable and difficult, which resulted in\ntheir low adoption in reinforcement learning research. This difficulty is\nworsened by the lack of guidelines for setting up learning tasks with robots.\nIn this work, we develop a learning task with a UR5 robotic arm to bring to\nlight some key elements of a task setup and study their contributions to the\nchallenges with robots. We find that learning performance can be highly\nsensitive to the setup, and thus oversights and omissions in setup details can\nmake effective learning, reproducibility, and fair comparison hard. Our study\nsuggests some mitigating steps to help future experimenters avoid difficulties\nand pitfalls. We show that highly reliable and repeatable experiments can be\nperformed in our setup, indicating the possibility of reinforcement learning\nresearch extensively based on real-world robots.\n",
        "published": "2018",
        "authors": [
            "A. Rupam Mahmood",
            "Dmytro Korenkevych",
            "Brent J. Komer",
            "James Bergstra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.01267v4",
        "title": "Internal Model from Observations for Reward Shaping",
        "abstract": "  Reinforcement learning methods require careful design involving a reward\nfunction to obtain the desired action policy for a given task. In the absence\nof hand-crafted reward functions, prior work on the topic has proposed several\nmethods for reward estimation by using expert state trajectories and action\npairs. However, there are cases where complete or good action information\ncannot be obtained from expert demonstrations. We propose a novel reinforcement\nlearning method in which the agent learns an internal model of observation on\nthe basis of expert-demonstrated state trajectories to estimate rewards without\ncompletely learning the dynamics of the external environment from state-action\npairs. The internal model is obtained in the form of a predictive model for the\ngiven expert state distribution. During reinforcement learning, the agent\npredicts the reward as a function of the difference between the actual state\nand the state predicted by the internal model. We conducted multiple\nexperiments in environments of varying complexity, including the Super Mario\nBros and Flappy Bird games. We show our method successfully trains good\npolicies directly from expert game-play videos.\n",
        "published": "2018",
        "authors": [
            "Daiki Kimura",
            "Subhajit Chaudhury",
            "Ryuki Tachibana",
            "Sakyasingha Dasgupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.06003v1",
        "title": "On Machine Learning and Structure for Mobile Robots",
        "abstract": "  Due to recent advances - compute, data, models - the role of learning in\nautonomous systems has expanded significantly, rendering new applications\npossible for the first time. While some of the most significant benefits are\nobtained in the perception modules of the software stack, other aspects\ncontinue to rely on known manual procedures based on prior knowledge on\ngeometry, dynamics, kinematics etc. Nonetheless, learning gains relevance in\nthese modules when data collection and curation become easier than manual rule\ndesign. Building on this coarse and broad survey of current research, the final\nsections aim to provide insights into future potentials and challenges as well\nas the necessity of structure in current practical applications.\n",
        "published": "2018",
        "authors": [
            "Markus Wulfmeier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.06408v1",
        "title": "Gated Path Planning Networks",
        "abstract": "  Value Iteration Networks (VINs) are effective differentiable path planning\nmodules that can be used by agents to perform navigation while still\nmaintaining end-to-end differentiability of the entire architecture. Despite\ntheir effectiveness, they suffer from several disadvantages including training\ninstability, random seed sensitivity, and other optimization problems. In this\nwork, we reframe VINs as recurrent-convolutional networks which demonstrates\nthat VINs couple recurrent convolutions with an unconventional max-pooling\nactivation. From this perspective, we argue that standard gated recurrent\nupdate equations could potentially alleviate the optimization issues plaguing\nVIN. The resulting architecture, which we call the Gated Path Planning Network,\nis shown to empirically outperform VIN on a variety of metrics such as learning\nspeed, hyperparameter sensitivity, iteration count, and even generalization.\nFurthermore, we show that this performance gap is consistent across different\nmaze transition types, maze sizes and even show success on a challenging 3D\nenvironment, where the planner is only provided with first-person RGB images.\n",
        "published": "2018",
        "authors": [
            "Lisa Lee",
            "Emilio Parisotto",
            "Devendra Singh Chaplot",
            "Eric Xing",
            "Ruslan Salakhutdinov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.09444v1",
        "title": "A Transferable Pedestrian Motion Prediction Model for Intersections with\n  Different Geometries",
        "abstract": "  This paper presents a novel framework for accurate pedestrian intent\nprediction at intersections. Given some prior knowledge of the curbside\ngeometry, the presented framework can accurately predict pedestrian\ntrajectories, even in new intersections that it has not been trained on. This\nis achieved by making use of the contravariant components of trajectories in\nthe curbside coordinate system, which ensures that the transformation of\ntrajectories across intersections is affine, regardless of the curbside\ngeometry. Our method is based on the Augmented Semi Nonnegative Sparse Coding\n(ASNSC) formulation and we use that as a baseline to show improvement in\nprediction performance on real pedestrian datasets collected at two\nintersections in Cambridge, with distinctly different curbside and crosswalk\ngeometries. We demonstrate a 7.2% improvement in prediction accuracy in the\ncase of same train and test intersections. Furthermore, we show a comparable\nprediction performance of TASNSC when trained and tested in different\nintersections with the baseline, trained and tested on the same intersection.\n",
        "published": "2018",
        "authors": [
            "Nikita Jaipuria",
            "Golnaz Habibi",
            "Jonathan P. How"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.00412v2",
        "title": "Learning to Drive in a Day",
        "abstract": "  We demonstrate the first application of deep reinforcement learning to\nautonomous driving. From randomly initialised parameters, our model is able to\nlearn a policy for lane following in a handful of training episodes using a\nsingle monocular image as input. We provide a general and easy to obtain\nreward: the distance travelled by the vehicle without the safety driver taking\ncontrol. We use a continuous, model-free deep reinforcement learning algorithm,\nwith all exploration and optimisation performed on-vehicle. This demonstrates a\nnew framework for autonomous driving which moves away from reliance on defined\nlogical rules, mapping, and direct supervision. We discuss the challenges and\nopportunities to scale this approach to a broader range of autonomous driving\ntasks.\n",
        "published": "2018",
        "authors": [
            "Alex Kendall",
            "Jeffrey Hawke",
            "David Janz",
            "Przemyslaw Mazur",
            "Daniele Reda",
            "John-Mark Allen",
            "Vinh-Dieu Lam",
            "Alex Bewley",
            "Amar Shah"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.01970v1",
        "title": "Arcades: A deep model for adaptive decision making in voice controlled\n  smart-home",
        "abstract": "  In a voice-controlled smart-home, a controller must respond not only to\nuser's requests but also according to the interaction context. This paper\ndescribes Arcades, a system which uses deep reinforcement learning to extract\ncontext from a graphical representation of home automation system and to update\ncontinuously its behavior to the user's one. This system is robust to changes\nin the environment (sensor breakdown or addition) through its graphical\nrepresentation (scale well) and the reinforcement mechanism (adapt well). The\nexperiments on realistic data demonstrate that this method promises to reach\nlong life context-aware control of smart-home.\n",
        "published": "2018",
        "authors": [
            "Alexis Brenon",
            "Fran\u00e7ois Portet",
            "Michel Vacher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.02303v5",
        "title": "A survey on policy search algorithms for learning robot controllers in a\n  handful of trials",
        "abstract": "  Most policy search algorithms require thousands of training episodes to find\nan effective policy, which is often infeasible with a physical robot. This\nsurvey article focuses on the extreme other end of the spectrum: how can a\nrobot adapt with only a handful of trials (a dozen) and a few minutes? By\nanalogy with the word \"big-data\", we refer to this challenge as \"micro-data\nreinforcement learning\". We show that a first strategy is to leverage prior\nknowledge on the policy structure (e.g., dynamic movement primitives), on the\npolicy parameters (e.g., demonstrations), or on the dynamics (e.g.,\nsimulators). A second strategy is to create data-driven surrogate models of the\nexpected reward (e.g., Bayesian optimization) or the dynamical model (e.g.,\nmodel-based policy search), so that the policy optimizer queries the model\ninstead of the real system. Overall, all successful micro-data algorithms\ncombine these two strategies by varying the kind of model and prior knowledge.\nThe current scientific challenges essentially revolve around scaling up to\ncomplex robots (e.g., humanoids), designing generic priors, and optimizing the\ncomputing time.\n",
        "published": "2018",
        "authors": [
            "Konstantinos Chatzilygeroudis",
            "Vassilis Vassiliades",
            "Freek Stulp",
            "Sylvain Calinon",
            "Jean-Baptiste Mouret"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.01848v3",
        "title": "Plan Online, Learn Offline: Efficient Learning and Exploration via\n  Model-Based Control",
        "abstract": "  We propose a plan online and learn offline (POLO) framework for the setting\nwhere an agent, with an internal model, needs to continually act and learn in\nthe world. Our work builds on the synergistic relationship between local\nmodel-based control, global value function learning, and exploration. We study\nhow local trajectory optimization can cope with approximation errors in the\nvalue function, and can stabilize and accelerate value function learning.\nConversely, we also study how approximate value functions can help reduce the\nplanning horizon and allow for better policies beyond local solutions. Finally,\nwe also demonstrate how trajectory optimization can be used to perform\ntemporally coordinated exploration in conjunction with estimating uncertainty\nin value function approximation. This exploration is critical for fast and\nstable learning of the value function. Combining these components enable\nsolutions to complex simulated control tasks, like humanoid locomotion and\ndexterous in-hand manipulation, in the equivalent of a few minutes of\nexperience in the real world.\n",
        "published": "2018",
        "authors": [
            "Kendall Lowrey",
            "Aravind Rajeswaran",
            "Sham Kakade",
            "Emanuel Todorov",
            "Igor Mordatch"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.02945v3",
        "title": "Behavioural Repertoire via Generative Adversarial Policy Networks",
        "abstract": "  Learning algorithms are enabling robots to solve increasingly challenging\nreal-world tasks. These approaches often rely on demonstrations and reproduce\nthe behavior shown. Unexpected changes in the environment may require using\ndifferent behaviors to achieve the same effect, for instance to reach and grasp\nan object in changing clutter. An emerging paradigm addressing this robustness\nissue is to learn a diverse set of successful behaviors for a given task, from\nwhich a robot can select the most suitable policy when faced with a new\nenvironment. In this paper, we explore a novel realization of this vision by\nlearning a generative model over policies. Rather than learning a single\npolicy, or a small fixed repertoire, our generative model for policies\ncompactly encodes an unbounded number of policies and allows novel controller\nvariants to be sampled. Leveraging our generative policy network, a robot can\nsample novel behaviors until it finds one that works for a new environment. We\ndemonstrate this idea with an application of robust ball-throwing in the\npresence of obstacles. We show that this approach achieves a greater diversity\nof behaviors than an existing evolutionary approach, while maintaining good\nefficacy of sampled behaviors, allowing a Baxter robot to hit targets more\noften when ball throwing in the presence of obstacles.\n",
        "published": "2018",
        "authors": [
            "Marija Jegorova",
            "St\u00e9phane Doncieux",
            "Timothy Hospedales"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.10097v1",
        "title": "Planning in Dynamic Environments with Conditional Autoregressive Models",
        "abstract": "  We demonstrate the use of conditional autoregressive generative models (van\nden Oord et al., 2016a) over a discrete latent space (van den Oord et al.,\n2017b) for forward planning with MCTS. In order to test this method, we\nintroduce a new environment featuring varying difficulty levels, along with\nmoving goals and obstacles. The combination of high-quality frame generation\nand classical planning approaches nearly matches true environment performance\nfor our task, demonstrating the usefulness of this method for model-based\nplanning in dynamic environments.\n",
        "published": "2018",
        "authors": [
            "Johanna Hansen",
            "Kyle Kastner",
            "Aaron Courville",
            "Gregory Dudek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.00117v2",
        "title": "An Active Learning Framework for Efficient Robust Policy Search",
        "abstract": "  Robust Policy Search is the problem of learning policies that do not degrade\nin performance when subject to unseen environment model parameters. It is\nparticularly relevant for transferring policies learned in a simulation\nenvironment to the real world. Several existing approaches involve sampling\nlarge batches of trajectories which reflect the differences in various possible\nenvironments, and then selecting some subset of these to learn robust policies,\nsuch as the ones that result in the worst performance. We propose an active\nlearning based framework, EffAcTS, to selectively choose model parameters for\nthis purpose so as to collect only as much data as necessary to select such a\nsubset. We apply this framework using Linear Bandits, and experimentally\nvalidate the gains in sample efficiency and the performance of our approach on\nstandard continuous control tasks. We also present a Multi-Task Learning\nperspective to the problem of Robust Policy Search, and draw connections from\nour proposed framework to existing work on Multi-Task Learning.\n",
        "published": "2019",
        "authors": [
            "Sai Kiran Narayanaswami",
            "Nandan Sudarsanam",
            "Balaraman Ravindran"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.03327v1",
        "title": "A New Tensioning Method using Deep Reinforcement Learning for Surgical\n  Pattern Cutting",
        "abstract": "  Surgeons normally need surgical scissors and tissue grippers to cut through a\ndeformable surgical tissue. The cutting accuracy depends on the skills to\nmanipulate these two tools. Such skills are part of basic surgical skills\ntraining as in the Fundamentals of Laparoscopic Surgery. The gripper is used to\npinch a point on the surgical sheet and pull the tissue to a certain direction\nto maintain the tension while the scissors cut through a trajectory. As the\nsurgical materials are deformable, it requires a comprehensive tensioning\npolicy to yield appropriate tensioning direction at each step of the cutting\nprocess. Automating a tensioning policy for a given cutting trajectory will\nsupport not only the human surgeons but also the surgical robots to improve the\ncutting accuracy and reliability. This paper presents a multiple pinch point\napproach to modelling an autonomous tensioning planner based on a deep\nreinforcement learning algorithm. Experiments on a simulator show that the\nproposed method is superior to existing methods in terms of both performance\nand robustness.\n",
        "published": "2019",
        "authors": [
            "Thanh Thi Nguyen",
            "Ngoc Duy Nguyen",
            "Fernando Bello",
            "Saeid Nahavandi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.05101v1",
        "title": "ReNeg and Backseat Driver: Learning from Demonstration with Continuous\n  Human Feedback",
        "abstract": "  In autonomous vehicle (AV) control, allowing mistakes can be quite dangerous\nand costly in the real world. For this reason we investigate methods of\ntraining an AV without allowing the agent to explore and instead having a human\nexplorer collect the data. Supervised learning has been explored for AV\ncontrol, but it encounters the issue of the covariate shift. That is, training\ndata collected from an optimal demonstration consists only of the states\ninduced by the optimal control policy, but at runtime, the trained agent may\nencounter a vastly different state distribution with little relevant training\ndata. To mitigate this issue, we have our human explorer make sub-optimal\ndecisions. In order to have our agent not replicate these sub-optimal\ndecisions, supervised learning requires that we either erase these actions, or\nreplace these action with the correct action. Erasing is wasteful and replacing\nis difficult, since it is not easy to know the correct action without driving.\nWe propose an alternate framework that includes continuous scalar feedback for\neach action, marking which actions we should replicate, which we should avoid,\nand how sure we are. Our framework learns continuous control from sub-optimal\ndemonstration and evaluative feedback collected before training. We find that a\nhuman demonstrator can explore sub-optimal states in a safe manner, while still\ngetting enough gradation to benefit learning. The collection method for data\nand feedback we call \"Backseat Driver.\" We call the more general learning\nframework ReNeg, since it learns a regression from states to actions given\nnegative as well as positive examples. We empirically validate several models\nin the ReNeg framework, testing on lane-following with limited data. We find\nthat the best solution is a generalization of mean-squared error and\noutperforms supervised learning on the positive examples alone.\n",
        "published": "2019",
        "authors": [
            "Jacob Beck",
            "Zoe Papakipos",
            "Michael Littman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.06803v1",
        "title": "Active Learning with Gaussian Processes for High Throughput Phenotyping",
        "abstract": "  A looming question that must be solved before robotic plant phenotyping\ncapabilities can have significant impact to crop improvement programs is\nscalability. High Throughput Phenotyping (HTP) uses robotic technologies to\nanalyze crops in order to determine species with favorable traits, however, the\ncurrent practices rely on exhaustive coverage and data collection from the\nentire crop field being monitored under the breeding experiment. This works\nwell in relatively small agricultural fields but can not be scaled to the\nlarger ones, thus limiting the progress of genetics research. In this work, we\npropose an active learning algorithm to enable an autonomous system to collect\nthe most informative samples in order to accurately learn the distribution of\nphenotypes in the field with the help of a Gaussian Process model. We\ndemonstrate the superior performance of our proposed algorithm compared to the\ncurrent practices on sorghum phenotype data collection.\n",
        "published": "2019",
        "authors": [
            "Sumit Kumar",
            "Wenhao Luo",
            "George Kantor",
            "Katia Sycara"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.00089v2",
        "title": "Safe, Efficient, and Comfortable Velocity Control based on Reinforcement\n  Learning for Autonomous Driving",
        "abstract": "  A model used for velocity control during car following was proposed based on\ndeep reinforcement learning (RL). To fulfil the multi-objectives of car\nfollowing, a reward function reflecting driving safety, efficiency, and comfort\nwas constructed. With the reward function, the RL agent learns to control\nvehicle speed in a fashion that maximizes cumulative rewards, through trials\nand errors in the simulation environment. A total of 1,341 car-following events\nextracted from the Next Generation Simulation (NGSIM) dataset were used to\ntrain the model. Car-following behavior produced by the model were compared\nwith that observed in the empirical NGSIM data, to demonstrate the model's\nability to follow a lead vehicle safely, efficiently, and comfortably. Results\nshow that the model demonstrates the capability of safe, efficient, and\ncomfortable velocity control in that it 1) has small percentages (8\\%) of\ndangerous minimum time to collision values (\\textless\\ 5s) than human drivers\nin the NGSIM data (35\\%); 2) can maintain efficient and safe headways in the\nrange of 1s to 2s; and 3) can follow the lead vehicle comfortably with smooth\nacceleration. The results indicate that reinforcement learning methods could\ncontribute to the development of autonomous driving systems.\n",
        "published": "2019",
        "authors": [
            "Meixin Zhu",
            "Yinhai Wang",
            "Ziyuan Pu",
            "Jingyun Hu",
            "Xuesong Wang",
            "Ruimin Ke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.01240v1",
        "title": "PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos",
        "abstract": "  Previously, the exploding gradient problem has been explained to be central\nin deep learning and model-based reinforcement learning, because it causes\nnumerical issues and instability in optimization. Our experiments in\nmodel-based reinforcement learning imply that the problem is not just a\nnumerical issue, but it may be caused by a fundamental chaos-like nature of\nlong chains of nonlinear computations. Not only do the magnitudes of the\ngradients become large, the direction of the gradients becomes essentially\nrandom. We show that reparameterization gradients suffer from the problem,\nwhile likelihood ratio gradients are robust. Using our insights, we develop a\nmodel-based policy search framework, Probabilistic Inference for Particle-Based\nPolicy Search (PIPPS), which is easily extensible, and allows for almost\narbitrary models and policies, while simultaneously matching the performance of\nprevious data-efficient learning algorithms. Finally, we invent the total\npropagation algorithm, which efficiently computes a union over all pathwise\nderivative depths during a single backwards pass, automatically giving greater\nweight to estimators with lower variance, sometimes improving over\nreparameterization gradients by $10^6$ times.\n",
        "published": "2019",
        "authors": [
            "Paavo Parmas",
            "Carl Edward Rasmussen",
            "Jan Peters",
            "Kenji Doya"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.01909v1",
        "title": "Adaptive Stress Testing for Autonomous Vehicles",
        "abstract": "  This paper presents a method for testing the decision making systems of\nautonomous vehicles. Our approach involves perturbing stochastic elements in\nthe vehicle's environment until the vehicle is involved in a collision. Instead\nof applying direct Monte Carlo sampling to find collision scenarios, we\nformulate the problem as a Markov decision process and use reinforcement\nlearning algorithms to find the most likely failure scenarios. This paper\npresents Monte Carlo Tree Search (MCTS) and Deep Reinforcement Learning (DRL)\nsolutions that can scale to large environments. We show that DRL can find more\nlikely failure scenarios than MCTS with fewer calls to the simulator. A\nsimulation scenario involving a vehicle approaching a crosswalk is used to\nvalidate the framework. Our proposed approach is very general and can be easily\napplied to other scenarios given the appropriate models of the vehicle and the\nenvironment.\n",
        "published": "2019",
        "authors": [
            "Mark Koren",
            "Saud Alsaif",
            "Ritchie Lee",
            "Mykel J. Kochenderfer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.00399v1",
        "title": "Safe Reinforcement Learning on Autonomous Vehicles",
        "abstract": "  There have been numerous advances in reinforcement learning, but the\ntypically unconstrained exploration of the learning process prevents the\nadoption of these methods in many safety critical applications. Recent work in\nsafe reinforcement learning uses idealized models to achieve their guarantees,\nbut these models do not easily accommodate the stochasticity or\nhigh-dimensionality of real world systems. We investigate how prediction\nprovides a general and intuitive framework to constraint exploration, and show\nhow it can be used to safely learn intersection handling behaviors on an\nautonomous vehicle.\n",
        "published": "2019",
        "authors": [
            "David Isele",
            "Alireza Nakhaei",
            "Kikuo Fujimura"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.00528v1",
        "title": "Augmenting learning using symmetry in a biologically-inspired domain",
        "abstract": "  Invariances to translation, rotation and other spatial transformations are a\nhallmark of the laws of motion, and have widespread use in the natural sciences\nto reduce the dimensionality of systems of equations. In supervised learning,\nsuch as in image classification tasks, rotation, translation and scale\ninvariances are used to augment training datasets. In this work, we use data\naugmentation in a similar way, exploiting symmetry in the quadruped domain of\nthe DeepMind control suite (Tassa et al. 2018) to add to the trajectories\nexperienced by the actor in the actor-critic algorithm of Abdolmaleki et al.\n(2018). In a data-limited regime, the agent using a set of experiences\naugmented through symmetry is able to learn faster. Our approach can be used to\ninject knowledge of invariances in the domain and task to augment learning in\nrobots, and more generally, to speed up learning in realistic robotics\napplications.\n",
        "published": "2019",
        "authors": [
            "Shruti Mishra",
            "Abbas Abdolmaleki",
            "Arthur Guez",
            "Piotr Trochim",
            "Doina Precup"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.01077v2",
        "title": "Task-Relevant Adversarial Imitation Learning",
        "abstract": "  We show that a critical vulnerability in adversarial imitation is the\ntendency of discriminator networks to learn spurious associations between\nvisual features and expert labels. When the discriminator focuses on\ntask-irrelevant features, it does not provide an informative reward signal,\nleading to poor task performance. We analyze this problem in detail and propose\na solution that outperforms standard Generative Adversarial Imitation Learning\n(GAIL). Our proposed method, Task-Relevant Adversarial Imitation Learning\n(TRAIL), uses constrained discriminator optimization to learn informative\nrewards. In comprehensive experiments, we show that TRAIL can solve challenging\nrobotic manipulation tasks from pixels by imitating human operators without\naccess to any task rewards, and clearly outperforms comparable baseline\nimitation agents, including those trained via behaviour cloning and\nconventional GAIL.\n",
        "published": "2019",
        "authors": [
            "Konrad Zolna",
            "Scott Reed",
            "Alexander Novikov",
            "Sergio Gomez Colmenarejo",
            "David Budden",
            "Serkan Cabi",
            "Misha Denil",
            "Nando de Freitas",
            "Ziyu Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.01240v1",
        "title": "Deep Reinforcement Learning for Single-Shot Diagnosis and Adaptation in\n  Damaged Robots",
        "abstract": "  Robotics has proved to be an indispensable tool in many industrial as well as\nsocial applications, such as warehouse automation, manufacturing, disaster\nrobotics, etc. In most of these scenarios, damage to the agent while\naccomplishing mission-critical tasks can result in failure. To enable robotic\nadaptation in such situations, the agent needs to adopt policies which are\nrobust to a diverse set of damages and must do so with minimum computational\ncomplexity. We thus propose a damage aware control architecture which diagnoses\nthe damage prior to gait selection while also incorporating domain\nrandomization in the damage space for learning a robust policy. To implement\ndamage awareness, we have used a Long Short Term Memory based supervised\nlearning network which diagnoses the damage and predicts the type of damage.\nThe main novelty of this approach is that only a single policy is trained to\nadapt against a wide variety of damages and the diagnosis is done in a single\ntrial at the time of damage.\n",
        "published": "2019",
        "authors": [
            "Shresth Verma",
            "Haritha S. Nair",
            "Gaurav Agarwal",
            "Joydip Dhar",
            "Anupam Shukla"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.01741v3",
        "title": "Improving Sample Efficiency in Model-Free Reinforcement Learning from\n  Images",
        "abstract": "  Training an agent to solve control tasks directly from high-dimensional\nimages with model-free reinforcement learning (RL) has proven difficult. A\npromising approach is to learn a latent representation together with the\ncontrol policy. However, fitting a high-capacity encoder using a scarce reward\nsignal is sample inefficient and leads to poor performance. Prior work has\nshown that auxiliary losses, such as image reconstruction, can aid efficient\nrepresentation learning. However, incorporating reconstruction loss into an\noff-policy learning algorithm often leads to training instability. We explore\nthe underlying reasons and identify variational autoencoders, used by previous\ninvestigations, as the cause of the divergence. Following these findings, we\npropose effective techniques to improve training stability. This results in a\nsimple approach capable of matching state-of-the-art model-free and model-based\nalgorithms on MuJoCo control tasks. Furthermore, our approach demonstrates\nrobustness to observational noise, surpassing existing approaches in this\nsetting. Code, results, and videos are anonymously available at\nhttps://sites.google.com/view/sac-ae/home.\n",
        "published": "2019",
        "authors": [
            "Denis Yarats",
            "Amy Zhang",
            "Ilya Kostrikov",
            "Brandon Amos",
            "Joelle Pineau",
            "Rob Fergus"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.02835v1",
        "title": "A Learnable Safety Measure",
        "abstract": "  Failures are challenging for learning to control physical systems since they\nrisk damage, time-consuming resets, and often provide little gradient\ninformation. Adding safety constraints to exploration typically requires a lot\nof prior knowledge and domain expertise. We present a safety measure which\nimplicitly captures how the system dynamics relate to a set of failure states.\nNot only can this measure be used as a safety function, but also to directly\ncompute the set of safe state-action pairs. Further, we show a model-free\napproach to learn this measure by active sampling using Gaussian processes.\nWhile safety can only be guaranteed after learning the safety measure, we show\nthat failures can already be greatly reduced by using the estimated measure\nduring learning.\n",
        "published": "2019",
        "authors": [
            "Steve Heim",
            "Alexander von Rohr",
            "Sebastian Trimpe",
            "Alexander Badri-Spr\u00f6witz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.04417v4",
        "title": "Imitation Learning from Observations by Minimizing Inverse Dynamics\n  Disagreement",
        "abstract": "  This paper studies Learning from Observations (LfO) for imitation learning\nwith access to state-only demonstrations. In contrast to Learning from\nDemonstration (LfD) that involves both action and state supervision, LfO is\nmore practical in leveraging previously inapplicable resources (e.g. videos),\nyet more challenging due to the incomplete expert guidance. In this paper, we\ninvestigate LfO and its difference with LfD in both theoretical and practical\nperspectives. We first prove that the gap between LfD and LfO actually lies in\nthe disagreement of inverse dynamics models between the imitator and the\nexpert, if following the modeling approach of GAIL. More importantly, the upper\nbound of this gap is revealed by a negative causal entropy which can be\nminimized in a model-free way. We term our method as\nInverse-Dynamics-Disagreement-Minimization (IDDM) which enhances the\nconventional LfO method through further bridging the gap to LfD. Considerable\nempirical results on challenging benchmarks indicate that our method attains\nconsistent improvements over other LfO counterparts.\n",
        "published": "2019",
        "authors": [
            "Chao Yang",
            "Xiaojian Ma",
            "Wenbing Huang",
            "Fuchun Sun",
            "Huaping Liu",
            "Junzhou Huang",
            "Chuang Gan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.04729v1",
        "title": "Efficient Intrinsically Motivated Robotic Grasping with\n  Learning-Adaptive Imagination in Latent Space",
        "abstract": "  Combining model-based and model-free deep reinforcement learning has shown\ngreat promise for improving sample efficiency on complex control tasks while\nstill retaining high performance. Incorporating imagination is a recent effort\nin this direction inspired by human mental simulation of motor behavior. We\npropose a learning-adaptive imagination approach which, unlike previous\napproaches, takes into account the reliability of the learned dynamics model\nused for imagining the future. Our approach learns an ensemble of disjoint\nlocal dynamics models in latent space and derives an intrinsic reward based on\nlearning progress, motivating the controller to take actions leading to data\nthat improves the models. The learned models are used to generate imagined\nexperiences, augmenting the training set of real experiences. We evaluate our\napproach on learning vision-based robotic grasping and show that it\nsignificantly improves sample efficiency and achieves near-optimal performance\nin a sparse reward environment.\n",
        "published": "2019",
        "authors": [
            "Muhammad Burhan Hafez",
            "Cornelius Weber",
            "Matthias Kerzel",
            "Stefan Wermter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.07294v4",
        "title": "Reinforcement Learning for Robotic Manipulation using Simulated\n  Locomotion Demonstrations",
        "abstract": "  Mastering robotic manipulation skills through reinforcement learning (RL)\ntypically requires the design of shaped reward functions. Recent developments\nin this area have demonstrated that using sparse rewards, i.e. rewarding the\nagent only when the task has been successfully completed, can lead to better\npolicies. However, state-action space exploration is more difficult in this\ncase. Recent RL approaches to learning with sparse rewards have leveraged\nhigh-quality human demonstrations for the task, but these can be costly, time\nconsuming or even impossible to obtain. In this paper, we propose a novel and\neffective approach that does not require human demonstrations. We observe that\nevery robotic manipulation task could be seen as involving a locomotion task\nfrom the perspective of the object being manipulated, i.e. the object could\nlearn how to reach a target state on its own. In order to exploit this idea, we\nintroduce a framework whereby an object locomotion policy is initially obtained\nusing a realistic physics simulator. This policy is then used to generate\nauxiliary rewards, called simulated locomotion demonstration rewards (SLDRs),\nwhich enable us to learn the robot manipulation policy. The proposed approach\nhas been evaluated on 13 tasks of increasing complexity, and can achieve higher\nsuccess rate and faster learning rates compared to alternative algorithms.\nSLDRs are especially beneficial for tasks like multi-object stacking and\nnon-rigid object manipulation.\n",
        "published": "2019",
        "authors": [
            "Ozsel Kilinc",
            "Giovanni Montana"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.08639v4",
        "title": "OffWorld Gym: open-access physical robotics environment for real-world\n  reinforcement learning benchmark and research",
        "abstract": "  Success stories of applied machine learning can be traced back to the\ndatasets and environments that were put forward as challenges for the\ncommunity. The challenge that the community sets as a benchmark is usually the\nchallenge that the community eventually solves. The ultimate challenge of\nreinforcement learning research is to train real agents to operate in the real\nenvironment, but until now there has not been a common real-world RL benchmark.\nIn this work, we present a prototype real-world environment from OffWorld Gym\n-- a collection of real-world environments for reinforcement learning in\nrobotics with free public remote access. Close integration into existing\necosystem allows the community to start using OffWorld Gym without any prior\nexperience in robotics and takes away the burden of managing a physical\nrobotics system, abstracting it under a familiar API. We introduce a navigation\ntask, where a robot has to reach a visual beacon on an uneven terrain using\nonly the camera input and provide baseline results in both the real environment\nand the simulated replica. To start training, visit https://gym.offworld.ai\n",
        "published": "2019",
        "authors": [
            "Ashish Kumar",
            "Toby Buckley",
            "John B. Lanier",
            "Qiaozhi Wang",
            "Alicia Kavelaars",
            "Ilya Kuzovkin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.10897v2",
        "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta\n  Reinforcement Learning",
        "abstract": "  Meta-reinforcement learning algorithms can enable robots to acquire new\nskills much more quickly, by leveraging prior experience to learn how to learn.\nHowever, much of the current research on meta-reinforcement learning focuses on\ntask distributions that are very narrow. For example, a commonly used\nmeta-reinforcement learning benchmark uses different running velocities for a\nsimulated robot as different tasks. When policies are meta-trained on such\nnarrow task distributions, they cannot possibly generalize to more quickly\nacquire entirely new tasks. Therefore, if the aim of these methods is to enable\nfaster acquisition of entirely new behaviors, we must evaluate them on task\ndistributions that are sufficiently broad to enable generalization to new\nbehaviors. In this paper, we propose an open-source simulated benchmark for\nmeta-reinforcement learning and multi-task learning consisting of 50 distinct\nrobotic manipulation tasks. Our aim is to make it possible to develop\nalgorithms that generalize to accelerate the acquisition of entirely new,\nheld-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and\nmulti-task learning algorithms on these tasks. Surprisingly, while each task\nand its variations (e.g., with different object positions) can be learned with\nreasonable success, these algorithms struggle to learn with multiple tasks at\nthe same time, even with as few as ten distinct training tasks. Our analysis\nand open-source environments pave the way for future research in multi-task\nlearning and meta-learning that can enable meaningful generalization, thereby\nunlocking the full potential of these methods.\n",
        "published": "2019",
        "authors": [
            "Tianhe Yu",
            "Deirdre Quillen",
            "Zhanpeng He",
            "Ryan Julian",
            "Avnish Narayan",
            "Hayden Shively",
            "Adithya Bellathur",
            "Karol Hausman",
            "Chelsea Finn",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.12453v1",
        "title": "Asynchronous Methods for Model-Based Reinforcement Learning",
        "abstract": "  Significant progress has been made in the area of model-based reinforcement\nlearning. State-of-the-art algorithms are now able to match the asymptotic\nperformance of model-free methods while being significantly more data\nefficient. However, this success has come at a price: state-of-the-art\nmodel-based methods require significant computation interleaved with data\ncollection, resulting in run times that take days, even if the amount of agent\ninteraction might be just hours or even minutes. When considering the goal of\nlearning in real-time on real robots, this means these state-of-the-art\nmodel-based algorithms still remain impractical. In this work, we propose an\nasynchronous framework for model-based reinforcement learning methods that\nbrings down the run time of these algorithms to be just the data collection\ntime. We evaluate our asynchronous framework on a range of standard MuJoCo\nbenchmarks. We also evaluate our asynchronous framework on three real-world\nrobotic manipulation tasks. We show how asynchronous learning not only speeds\nup learning w.r.t wall-clock time through parallelization, but also further\nreduces the sample complexity of model-based approaches by means of improving\nthe exploration and by means of effectively avoiding the policy overfitting to\nthe deficiencies of learned dynamics models.\n",
        "published": "2019",
        "authors": [
            "Yunzhi Zhang",
            "Ignasi Clavera",
            "Boren Tsai",
            "Pieter Abbeel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.13726v1",
        "title": "Safe Exploration for Interactive Machine Learning",
        "abstract": "  In Interactive Machine Learning (IML), we iteratively make decisions and\nobtain noisy observations of an unknown function. While IML methods, e.g.,\nBayesian optimization and active learning, have been successful in\napplications, on real-world systems they must provably avoid unsafe decisions.\nTo this end, safe IML algorithms must carefully learn about a priori unknown\nconstraints without making unsafe decisions. Existing algorithms for this\nproblem learn about the safety of all decisions to ensure convergence. This is\nsample-inefficient, as it explores decisions that are not relevant for the\noriginal IML objective. In this paper, we introduce a novel framework that\nrenders any existing unsafe IML algorithm safe. Our method works as an add-on\nthat takes suggested decisions as input and exploits regularity assumptions in\nterms of a Gaussian process prior in order to efficiently learn about their\nsafety. As a result, we only explore the safe set when necessary for the IML\nproblem. We apply our framework to safe Bayesian optimization and to safe\nexploration in deterministic Markov Decision Processes (MDP), which have been\nanalyzed separately before. Our method outperforms other algorithms\nempirically.\n",
        "published": "2019",
        "authors": [
            "Matteo Turchetta",
            "Felix Berkenkamp",
            "Andreas Krause"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.14033v2",
        "title": "Plan Arithmetic: Compositional Plan Vectors for Multi-Task Control",
        "abstract": "  Autonomous agents situated in real-world environments must be able to master\nlarge repertoires of skills. While a single short skill can be learned quickly,\nit would be impractical to learn every task independently. Instead, the agent\nshould share knowledge across behaviors such that each task can be learned\nefficiently, and such that the resulting model can generalize to new tasks,\nespecially ones that are compositions or subsets of tasks seen previously. A\npolicy conditioned on a goal or demonstration has the potential to share\nknowledge between tasks if it sees enough diversity of inputs. However, these\nmethods may not generalize to a more complex task at test time. We introduce\ncompositional plan vectors (CPVs) to enable a policy to perform compositions of\ntasks without additional supervision. CPVs represent trajectories as the sum of\nthe subtasks within them. We show that CPVs can be learned within a one-shot\nimitation learning framework without any additional supervision or information\nabout task hierarchy, and enable a demonstration-conditioned policy to\ngeneralize to tasks that sequence twice as many skills as the tasks seen during\ntraining.\n  Analogously to embeddings such as word2vec in NLP, CPVs can also support\nsimple arithmetic operations -- for example, we can add the CPVs for two\ndifferent tasks to command an agent to compose both tasks, without any\nadditional training.\n",
        "published": "2019",
        "authors": [
            "Coline Devin",
            "Daniel Geng",
            "Pieter Abbeel",
            "Trevor Darrell",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.00530v1",
        "title": "Learning Sparse Rewarded Tasks from Sub-Optimal Demonstrations",
        "abstract": "  Model-free deep reinforcement learning (RL) has demonstrated its superiority\non many complex sequential decision-making problems. However, heavy dependence\non dense rewards and high sample-complexity impedes the wide adoption of these\nmethods in real-world scenarios. On the other hand, imitation learning (IL)\nlearns effectively in sparse-rewarded tasks by leveraging the existing expert\ndemonstrations. In practice, collecting a sufficient amount of expert\ndemonstrations can be prohibitively expensive, and the quality of\ndemonstrations typically limits the performance of the learning policy. In this\nwork, we propose Self-Adaptive Imitation Learning (SAIL) that can achieve\n(near) optimal performance given only a limited number of sub-optimal\ndemonstrations for highly challenging sparse reward tasks. SAIL bridges the\nadvantages of IL and RL to reduce the sample complexity substantially, by\neffectively exploiting sup-optimal demonstrations and efficiently exploring the\nenvironment to surpass the demonstrated performance. Extensive empirical\nresults show that not only does SAIL significantly improve the\nsample-efficiency but also leads to much better final performance across\ndifferent continuous control tasks, comparing to the state-of-the-art.\n",
        "published": "2020",
        "authors": [
            "Zhuangdi Zhu",
            "Kaixiang Lin",
            "Bo Dai",
            "Jiayu Zhou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.06089v4",
        "title": "Thinking While Moving: Deep Reinforcement Learning with Concurrent\n  Control",
        "abstract": "  We study reinforcement learning in settings where sampling an action from the\npolicy must be done concurrently with the time evolution of the controlled\nsystem, such as when a robot must decide on the next action while still\nperforming the previous action. Much like a person or an animal, the robot must\nthink and move at the same time, deciding on its next action before the\nprevious one has completed. In order to develop an algorithmic framework for\nsuch concurrent control problems, we start with a continuous-time formulation\nof the Bellman equations, and then discretize them in a way that is aware of\nsystem delays. We instantiate this new class of approximate dynamic programming\nmethods via a simple architectural extension to existing value-based deep\nreinforcement learning algorithms. We evaluate our methods on simulated\nbenchmark tasks and a large-scale robotic grasping task where the robot must\n\"think while moving\".\n",
        "published": "2020",
        "authors": [
            "Ted Xiao",
            "Eric Jang",
            "Dmitry Kalashnikov",
            "Sergey Levine",
            "Julian Ibarz",
            "Karol Hausman",
            "Alexander Herzog"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.07804v2",
        "title": "A Game Theoretic Framework for Model Based Reinforcement Learning",
        "abstract": "  Model-based reinforcement learning (MBRL) has recently gained immense\ninterest due to its potential for sample efficiency and ability to incorporate\noff-policy data. However, designing stable and efficient MBRL algorithms using\nrich function approximators have remained challenging. To help expose the\npractical challenges in MBRL and simplify algorithm design from the lens of\nabstraction, we develop a new framework that casts MBRL as a game between: (1)\na policy player, which attempts to maximize rewards under the learned model;\n(2) a model player, which attempts to fit the real-world data collected by the\npolicy player. For algorithm development, we construct a Stackelberg game\nbetween the two players, and show that it can be solved with approximate\nbi-level optimization. This gives rise to two natural families of algorithms\nfor MBRL based on which player is chosen as the leader in the Stackelberg game.\nTogether, they encapsulate, unify, and generalize many previous MBRL\nalgorithms. Furthermore, our framework is consistent with and provides a clear\nbasis for heuristics known to be important in practice from prior works.\nFinally, through experiments we validate that our proposed algorithms are\nhighly sample efficient, match the asymptotic performance of model-free policy\ngradient, and scale gracefully to high-dimensional tasks like dexterous hand\nmanipulation. Additional details and code can be obtained from the project page\nat https://sites.google.com/view/mbrl-game\n",
        "published": "2020",
        "authors": [
            "Aravind Rajeswaran",
            "Igor Mordatch",
            "Vikash Kumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.08648v1",
        "title": "Modeling Survival in model-based Reinforcement Learning",
        "abstract": "  Although recent model-free reinforcement learning algorithms have been shown\nto be capable of mastering complicated decision-making tasks, the sample\ncomplexity of these methods has remained a hurdle to utilizing them in many\nreal-world applications. In this regard, model-based reinforcement learning\nproposes some remedies. Yet, inherently, model-based methods are more\ncomputationally expensive and susceptible to sub-optimality. One reason is that\nmodel-generated data are always less accurate than real data, and this often\nleads to inaccurate transition and reward function models. With the aim to\nmitigate this problem, this work presents the notion of survival by discussing\ncases in which the agent's goal is to survive and its analogy to maximizing the\nexpected rewards. To that end, a substitute model for the reward function\napproximator is introduced that learns to avoid terminal states rather than to\nmaximize accumulated rewards from safe states. Focusing on terminal states, as\na small fraction of state-space, reduces the training effort drastically. Next,\na model-based reinforcement learning method is proposed (Survive) to train an\nagent to avoid dangerous states through a safety map model built upon temporal\ncredit assignment in the vicinity of terminal states. Finally, the performance\nof the presented algorithm is investigated, along with a comparison between the\nproposed and current methods.\n",
        "published": "2020",
        "authors": [
            "Saeed Moazami",
            "Peggy Doerschuk"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.08763v1",
        "title": "Model-Predictive Control via Cross-Entropy and Gradient-Based\n  Optimization",
        "abstract": "  Recent works in high-dimensional model-predictive control and model-based\nreinforcement learning with learned dynamics and reward models have resorted to\npopulation-based optimization methods, such as the Cross-Entropy Method (CEM),\nfor planning a sequence of actions. To decide on an action to take, CEM\nconducts a search for the action sequence with the highest return according to\nthe dynamics model and reward. Action sequences are typically randomly sampled\nfrom an unconditional Gaussian distribution and evaluated on the environment.\nThis distribution is iteratively updated towards action sequences with higher\nreturns. However, this planning method can be very inefficient, especially for\nhigh-dimensional action spaces. An alternative line of approaches optimize\naction sequences directly via gradient descent, but are prone to local optima.\nWe propose a method to solve this planning problem by interleaving CEM and\ngradient descent steps in optimizing the action sequence. Our experiments show\nfaster convergence of the proposed hybrid approach, even for high-dimensional\naction spaces, avoidance of local minima, and better or equal performance to\nCEM. Code accompanying the paper is available here\nhttps://github.com/homangab/gradcem.\n",
        "published": "2020",
        "authors": [
            "Homanga Bharadhwaj",
            "Kevin Xie",
            "Florian Shkurti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.08830v3",
        "title": "Improving Robot Dual-System Motor Learning with Intrinsically Motivated\n  Meta-Control and Latent-Space Experience Imagination",
        "abstract": "  Combining model-based and model-free learning systems has been shown to\nimprove the sample efficiency of learning to perform complex robotic tasks.\nHowever, dual-system approaches fail to consider the reliability of the learned\nmodel when it is applied to make multiple-step predictions, resulting in a\ncompounding of prediction errors and performance degradation. In this paper, we\npresent a novel dual-system motor learning approach where a meta-controller\narbitrates online between model-based and model-free decisions based on an\nestimate of the local reliability of the learned model. The reliability\nestimate is used in computing an intrinsic feedback signal, encouraging actions\nthat lead to data that improves the model. Our approach also integrates\narbitration with imagination where a learned latent-space model generates\nimagined experiences, based on its local reliability, to be used as additional\ntraining data. We evaluate our approach against baseline and state-of-the-art\nmethods on learning vision-based robotic grasping in simulation and real world.\nThe results show that our approach outperforms the compared methods and learns\nnear-optimal grasping policies in dense- and sparse-reward environments.\n",
        "published": "2020",
        "authors": [
            "Muhammad Burhan Hafez",
            "Cornelius Weber",
            "Matthias Kerzel",
            "Stefan Wermter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.11667v1",
        "title": "PBCS : Efficient Exploration and Exploitation Using a Synergy between\n  Reinforcement Learning and Motion Planning",
        "abstract": "  The exploration-exploitation trade-off is at the heart of reinforcement\nlearning (RL). However, most continuous control benchmarks used in recent RL\nresearch only require local exploration. This led to the development of\nalgorithms that have basic exploration capabilities, and behave poorly in\nbenchmarks that require more versatile exploration. For instance, as\ndemonstrated in our empirical study, state-of-the-art RL algorithms such as\nDDPG and TD3 are unable to steer a point mass in even small 2D mazes. In this\npaper, we propose a new algorithm called \"Plan, Backplay, Chain Skills\" (PBCS)\nthat combines motion planning and reinforcement learning to solve hard\nexploration environments. In a first phase, a motion planning algorithm is used\nto find a single good trajectory, then an RL algorithm is trained using a\ncurriculum derived from the trajectory, by combining a variant of the Backplay\nalgorithm and skill chaining. We show that this method outperforms\nstate-of-the-art RL algorithms in 2D maze environments of various sizes, and is\nable to improve on the trajectory obtained by the motion planning phase.\n",
        "published": "2020",
        "authors": [
            "Guillaume Matheron",
            "Nicolas Perrin",
            "Olivier Sigaud"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.12873v1",
        "title": "Maximum Entropy Multi-Task Inverse RL",
        "abstract": "  Multi-task IRL allows for the possibility that the expert could be switching\nbetween multiple ways of solving the same problem, or interleaving\ndemonstrations of multiple tasks. The learner aims to learn the multiple reward\nfunctions that guide these ways of solving the problem. We present a new method\nfor multi-task IRL that generalizes the well-known maximum entropy approach to\nIRL by combining it with the Dirichlet process based clustering of the observed\ninput. This yields a single nonlinear optimization problem, called MaxEnt\nMulti-task IRL, which can be solved using the Lagrangian relaxation and\ngradient descent methods. We evaluate MaxEnt Multi-task IRL in simulation on\nthe robotic task of sorting onions on a processing line where the expert\nutilizes multiple ways of detecting and removing blemished onions. The method\nis able to learn the underlying reward functions to a high level of accuracy\nand it improves on the previous approaches to multi-task IRL.\n",
        "published": "2020",
        "authors": [
            "Saurabh Arora",
            "Bikramjit Banerjee",
            "Prashant Doshi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.06890v1",
        "title": "Gaussian Process Planning with Lipschitz Continuous Reward Functions:\n  Towards Unifying Bayesian Optimization, Active Learning, and Beyond",
        "abstract": "  This paper presents a novel nonmyopic adaptive Gaussian process planning\n(GPP) framework endowed with a general class of Lipschitz continuous reward\nfunctions that can unify some active learning/sensing and Bayesian optimization\ncriteria and offer practitioners some flexibility to specify their desired\nchoices for defining new tasks/problems. In particular, it utilizes a\nprincipled Bayesian sequential decision problem framework for jointly and\nnaturally optimizing the exploration-exploitation trade-off. In general, the\nresulting induced GPP policy cannot be derived exactly due to an uncountable\nset of candidate observations. A key contribution of our work here thus lies in\nexploiting the Lipschitz continuity of the reward functions to solve for a\nnonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real\ntime, we further propose an asymptotically optimal, branch-and-bound anytime\nvariant of epsilon-GPP with performance guarantee. We empirically demonstrate\nthe effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian\noptimization and an energy harvesting task.\n",
        "published": "2015",
        "authors": [
            "Chun Kai Ling",
            "Kian Hsiang Low",
            "Patrick Jaillet"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.00921v1",
        "title": "Active Learning within Constrained Environments through Imitation of an\n  Expert Questioner",
        "abstract": "  Active learning agents typically employ a query selection algorithm which\nsolely considers the agent's learning objectives. However, this may be\ninsufficient in more realistic human domains. This work uses imitation learning\nto enable an agent in a constrained environment to concurrently reason about\nboth its internal learning goals and environmental constraints externally\nimposed, all within its objective function. Experiments are conducted on a\nconcept learning task to test generalization of the proposed algorithm to\ndifferent environmental conditions and analyze how time and resource\nconstraints impact efficacy of solving the learning problem. Our findings show\nthe environmentally-aware learning agent is able to statistically outperform\nall other active learners explored under most of the constrained conditions. A\nkey implication is adaptation for active learning agents to more realistic\nhuman environments, where constraints are often externally imposed on the\nlearner.\n",
        "published": "2019",
        "authors": [
            "Kalesha Bullard",
            "Yannick Schroecker",
            "Sonia Chernova"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.02057v1",
        "title": "Benchmarking Model-Based Reinforcement Learning",
        "abstract": "  Model-based reinforcement learning (MBRL) is widely seen as having the\npotential to be significantly more sample efficient than model-free RL.\nHowever, research in model-based RL has not been very standardized. It is\nfairly common for authors to experiment with self-designed environments, and\nthere are several separate lines of research, which are sometimes\nclosed-sourced or not reproducible. Accordingly, it is an open question how\nthese various existing MBRL algorithms perform relative to each other. To\nfacilitate research in MBRL, in this paper we gather a wide collection of MBRL\nalgorithms and propose over 18 benchmarking environments specially designed for\nMBRL. We benchmark these algorithms with unified problem settings, including\nnoisy environments. Beyond cataloguing performance, we explore and unify the\nunderlying algorithmic differences across MBRL algorithms. We characterize\nthree key research challenges for future MBRL research: the dynamics\nbottleneck, the planning horizon dilemma, and the early-termination dilemma.\nFinally, to maximally facilitate future research on MBRL, we open-source our\nbenchmark in http://www.cs.toronto.edu/~tingwuwang/mbrl.html.\n",
        "published": "2019",
        "authors": [
            "Tingwu Wang",
            "Xuchan Bao",
            "Ignasi Clavera",
            "Jerrick Hoang",
            "Yeming Wen",
            "Eric Langlois",
            "Shunshi Zhang",
            "Guodong Zhang",
            "Pieter Abbeel",
            "Jimmy Ba"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.03627v2",
        "title": "Vision-based Navigation Using Deep Reinforcement Learning",
        "abstract": "  Deep reinforcement learning (RL) has been successfully applied to a variety\nof game-like environments. However, the application of deep RL to visual\nnavigation with realistic environments is a challenging task. We propose a\nnovel learning architecture capable of navigating an agent, e.g. a mobile\nrobot, to a target given by an image. To achieve this, we have extended the\nbatched A2C algorithm with auxiliary tasks designed to improve visual\nnavigation performance. We propose three additional auxiliary tasks: predicting\nthe segmentation of the observation image and of the target image and\npredicting the depth-map. These tasks enable the use of supervised learning to\npre-train a large part of the network and to reduce the number of training\nsteps substantially. The training performance has been further improved by\nincreasing the environment complexity gradually over time. An efficient neural\nnetwork structure is proposed, which is capable of learning for multiple\ntargets in multiple environments. Our method navigates in continuous state\nspaces and on the AI2-THOR environment simulator outperforms state-of-the-art\ngoal-oriented visual navigation methods from the literature.\n",
        "published": "2019",
        "authors": [
            "Jon\u00e1\u0161 Kulh\u00e1nek",
            "Erik Derner",
            "Tim de Bruin",
            "Robert Babu\u0161ka"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.05348v1",
        "title": "From Crystallized Adaptivity to Fluid Adaptivity in Deep Reinforcement\n  Learning -- Insights from Biological Systems on Adaptive Flexibility",
        "abstract": "  Recent developments in machine-learning algorithms have led to impressive\nperformance increases in many traditional application scenarios of artificial\nintelligence research. In the area of deep reinforcement learning, deep\nlearning functional architectures are combined with incremental learning\nschemes for sequential tasks that include interaction-based, but often delayed\nfeedback. Despite their impressive successes, modern machine-learning\napproaches, including deep reinforcement learning, still perform weakly when\ncompared to flexibly adaptive biological systems in certain naturally occurring\nscenarios. Such scenarios include transfers to environments different than the\nones in which the training took place or environments that dynamically change,\nboth of which are often mastered by biological systems through a capability\nthat we here term \"fluid adaptivity\" to contrast it from the much slower\nadaptivity (\"crystallized adaptivity\") of the prior learning from which the\nbehavior emerged. In this article, we derive and discuss research strategies,\nbased on analyzes of fluid adaptivity in biological systems and its neuronal\nmodeling, that might aid in equipping future artificially intelligent systems\nwith capabilities of fluid adaptivity more similar to those seen in some\nbiologically intelligent systems. A key component of this research strategy is\nthe dynamization of the problem space itself and the implementation of this\ndynamization by suitably designed flexibly interacting modules.\n",
        "published": "2019",
        "authors": [
            "Malte Schilling",
            "Helge Ritter",
            "Frank W. Ohl"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.05751v2",
        "title": "Examining the Use of Temporal-Difference Incremental Delta-Bar-Delta for\n  Real-World Predictive Knowledge Architectures",
        "abstract": "  Predictions and predictive knowledge have seen recent success in improving\nnot only robot control but also other applications ranging from industrial\nprocess control to rehabilitation. A property that makes these predictive\napproaches well suited for robotics is that they can be learned online and\nincrementally through interaction with the environment. However, a remaining\nchallenge for many prediction-learning approaches is an appropriate choice of\nprediction-learning parameters, especially parameters that control the\nmagnitude of a learning machine's updates to its predictions (the learning rate\nor step size). To begin to address this challenge, we examine the use of online\nstep-size adaptation using a sensor-rich robotic arm. Our method of choice,\nTemporal-Difference Incremental Delta-Bar-Delta (TIDBD), learns and adapts step\nsizes on a feature level; importantly, TIDBD allows step-size tuning and\nrepresentation learning to occur at the same time. We show that TIDBD is a\npractical alternative for classic Temporal-Difference (TD) learning via an\nextensive parameter search. Both approaches perform comparably in terms of\npredicting future aspects of a robotic data stream. Furthermore, the use of a\nstep-size adaptation method like TIDBD appears to allow a system to\nautomatically detect and characterize common sensor failures in a robotic\napplication. Together, these results promise to improve the ability of robotic\ndevices to learn from interactions with their environments in a robust way,\nproviding key capabilities for autonomous agents and robots.\n",
        "published": "2019",
        "authors": [
            "Johannes G\u00fcnther",
            "Nadia M. Ady",
            "Alex Kearney",
            "Michael R. Dawson",
            "Patrick M. Pilarski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.02926v1",
        "title": "Probabilistic Prediction of Interactive Driving Behavior via\n  Hierarchical Inverse Reinforcement Learning",
        "abstract": "  Autonomous vehicles (AVs) are on the road. To safely and efficiently interact\nwith other road participants, AVs have to accurately predict the behavior of\nsurrounding vehicles and plan accordingly. Such prediction should be\nprobabilistic, to address the uncertainties in human behavior. Such prediction\nshould also be interactive, since the distribution over all possible\ntrajectories of the predicted vehicle depends not only on historical\ninformation, but also on future plans of other vehicles that interact with it.\nTo achieve such interaction-aware predictions, we propose a probabilistic\nprediction approach based on hierarchical inverse reinforcement learning (IRL).\nFirst, we explicitly consider the hierarchical trajectory-generation process of\nhuman drivers involving both discrete and continuous driving decisions. Based\non this, the distribution over all future trajectories of the predicted vehicle\nis formulated as a mixture of distributions partitioned by the discrete\ndecisions. Then we apply IRL hierarchically to learn the distributions from\nreal human demonstrations. A case study for the ramp-merging driving scenario\nis provided. The quantitative results show that the proposed approach can\naccurately predict both the discrete driving decisions such as yield or pass as\nwell as the continuous trajectories.\n",
        "published": "2018",
        "authors": [
            "Liting Sun",
            "Wei Zhan",
            "Masayoshi Tomizuka"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.06404v3",
        "title": "Adversarial Imitation via Variational Inverse Reinforcement Learning",
        "abstract": "  We consider a problem of learning the reward and policy from expert examples\nunder unknown dynamics. Our proposed method builds on the framework of\ngenerative adversarial networks and introduces the empowerment-regularized\nmaximum-entropy inverse reinforcement learning to learn near-optimal rewards\nand policies. Empowerment-based regularization prevents the policy from\noverfitting to expert demonstrations, which advantageously leads to more\ngeneralized behaviors that result in learning near-optimal rewards. Our method\nsimultaneously learns empowerment through variational information maximization\nalong with the reward and policy under the adversarial learning formulation. We\nevaluate our approach on various high-dimensional complex control tasks. We\nalso test our learned rewards in challenging transfer learning problems where\ntraining and testing environments are made to be different from each other in\nterms of dynamics or structure. The results show that our proposed method not\nonly learns near-optimal rewards and policies that are matching expert behavior\nbut also performs significantly better than state-of-the-art inverse\nreinforcement learning algorithms.\n",
        "published": "2018",
        "authors": [
            "Ahmed H. Qureshi",
            "Byron Boots",
            "Michael C. Yip"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.07731v1",
        "title": "Benchmarking Reinforcement Learning Algorithms on Real-World Robots",
        "abstract": "  Through many recent successes in simulation, model-free reinforcement\nlearning has emerged as a promising approach to solving continuous control\nrobotic tasks. The research community is now able to reproduce, analyze and\nbuild quickly on these results due to open source implementations of learning\nalgorithms and simulated benchmark tasks. To carry forward these successes to\nreal-world applications, it is crucial to withhold utilizing the unique\nadvantages of simulations that do not transfer to the real world and experiment\ndirectly with physical robots. However, reinforcement learning research with\nphysical robots faces substantial resistance due to the lack of benchmark tasks\nand supporting source code. In this work, we introduce several reinforcement\nlearning tasks with multiple commercially available robots that present varying\nlevels of learning difficulty, setup, and repeatability. On these tasks, we\ntest the learning performance of off-the-shelf implementations of four\nreinforcement learning algorithms and analyze sensitivity to their\nhyper-parameters to determine their readiness for applications in various\nreal-world tasks. Our results show that with a careful setup of the task\ninterface and computations, some of these implementations can be readily\napplicable to physical robots. We find that state-of-the-art learning\nalgorithms are highly sensitive to their hyper-parameters and their relative\nordering does not transfer across tasks, indicating the necessity of re-tuning\nthem for each task for best performance. On the other hand, the best\nhyper-parameter configuration from one task may often result in effective\nlearning on held-out tasks even with different robots, providing a reasonable\ndefault. We make the benchmark tasks publicly available to enhance\nreproducibility in real-world reinforcement learning.\n",
        "published": "2018",
        "authors": [
            "A. Rupam Mahmood",
            "Dmytro Korenkevych",
            "Gautham Vasan",
            "William Ma",
            "James Bergstra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.08923v1",
        "title": "Target Transfer Q-Learning and Its Convergence Analysis",
        "abstract": "  Q-learning is one of the most popular methods in Reinforcement Learning (RL).\nTransfer Learning aims to utilize the learned knowledge from source tasks to\nhelp new tasks to improve the sample complexity of the new tasks. Considering\nthat data collection in RL is both more time and cost consuming and Q-learning\nconverges slowly comparing to supervised learning, different kinds of transfer\nRL algorithms are designed. However, most of them are heuristic with no\ntheoretical guarantee of the convergence rate. Therefore, it is important for\nus to clearly understand when and how will transfer learning help RL method and\nprovide the theoretical guarantee for the improvement of the sample complexity.\nIn this paper, we propose to transfer the Q-function learned in the source task\nto the target of the Q-learning in the new task when certain safe conditions\nare satisfied. We call this new transfer Q-learning method target transfer\nQ-Learning. The safe conditions are necessary to avoid the harm to the new\ntasks and thus ensure the convergence of the algorithm. We study the\nconvergence rate of the target transfer Q-learning. We prove that if the two\ntasks are similar with respect to the MDPs, the optimal Q-functions in the\nsource and new RL tasks are similar which means the error of the transferred\ntarget Q-function in new MDP is small. Also, the convergence rate analysis\nshows that the target transfer Q-Learning will converge faster than Q-learning\nif the error of the transferred target Q-function is smaller than the current\nQ-function in the new task. Based on our theoretical results, we design the\nsafe condition as the Bellman error of the transferred target Q-function is\nless than the current Q-function. Our experiments are consistent with our\ntheoretical founding and verified the effectiveness of our proposed target\ntransfer Q-learning method.\n",
        "published": "2018",
        "authors": [
            "Yue Wang",
            "Qi Meng",
            "Wei Cheng",
            "Yuting Liug",
            "Zhi-Ming Ma",
            "Tie-Yan Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.10253v3",
        "title": "Scaling simulation-to-real transfer by learning composable robot skills",
        "abstract": "  We present a novel solution to the problem of simulation-to-real transfer,\nwhich builds on recent advances in robot skill decomposition. Rather than\nfocusing on minimizing the simulation-reality gap, we learn a set of diverse\npolicies that are parameterized in a way that makes them easily reusable. This\ndiversity and parameterization of low-level skills allows us to find a\ntransferable policy that is able to use combinations and variations of\ndifferent skills to solve more complex, high-level tasks. In particular, we\nfirst use simulation to jointly learn a policy for a set of low-level skills,\nand a \"skill embedding\" parameterization which can be used to compose them.\nLater, we learn high-level policies which actuate the low-level policies via\nthis skill embedding parameterization. The high-level policies encode how and\nwhen to reuse the low-level skills together to achieve specific high-level\ntasks. Importantly, our method learns to control a real robot in joint-space to\nachieve these high-level tasks with little or no on-robot time, despite the\nfact that the low-level policies may not be perfectly transferable from\nsimulation to real, and that the low-level skills were not trained on any\nexamples of high-level tasks. We illustrate the principles of our method using\ninformative simulation experiments. We then verify its usefulness for real\nrobotics problems by learning, transferring, and composing free-space and\ncontact motion skills on a Sawyer robot using only joint-space control. We\nexperiment with several techniques for composing pre-learned skills, and find\nthat our method allows us to use both learning-based approaches and efficient\nsearch-based planning to achieve high-level tasks using only pre-learned\nskills.\n",
        "published": "2018",
        "authors": [
            "Ryan Julian",
            "Eric Heiden",
            "Zhanpeng He",
            "Hejia Zhang",
            "Stefan Schaal",
            "Joseph J. Lim",
            "Gaurav Sukhatme",
            "Karol Hausman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.05905v2",
        "title": "Soft Actor-Critic Algorithms and Applications",
        "abstract": "  Model-free deep reinforcement learning (RL) algorithms have been successfully\napplied to a range of challenging sequential decision making and control tasks.\nHowever, these methods typically suffer from two major challenges: high sample\ncomplexity and brittleness to hyperparameters. Both of these challenges limit\nthe applicability of such methods to real-world domains. In this paper, we\ndescribe Soft Actor-Critic (SAC), our recently introduced off-policy\nactor-critic algorithm based on the maximum entropy RL framework. In this\nframework, the actor aims to simultaneously maximize expected return and\nentropy. That is, to succeed at the task while acting as randomly as possible.\nWe extend SAC to incorporate a number of modifications that accelerate training\nand improve stability with respect to the hyperparameters, including a\nconstrained formulation that automatically tunes the temperature\nhyperparameter. We systematically evaluate SAC on a range of benchmark tasks,\nas well as real-world challenging tasks such as locomotion for a quadrupedal\nrobot and robotic manipulation with a dexterous hand. With these improvements,\nSAC achieves state-of-the-art performance, outperforming prior on-policy and\noff-policy methods in sample-efficiency and asymptotic performance.\nFurthermore, we demonstrate that, in contrast to other off-policy algorithms,\nour approach is very stable, achieving similar performance across different\nrandom seeds. These results suggest that SAC is a promising candidate for\nlearning in real-world robotics tasks.\n",
        "published": "2018",
        "authors": [
            "Tuomas Haarnoja",
            "Aurick Zhou",
            "Kristian Hartikainen",
            "George Tucker",
            "Sehoon Ha",
            "Jie Tan",
            "Vikash Kumar",
            "Henry Zhu",
            "Abhishek Gupta",
            "Pieter Abbeel",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.07671v2",
        "title": "Deep Online Learning via Meta-Learning: Continual Adaptation for\n  Model-Based RL",
        "abstract": "  Humans and animals can learn complex predictive models that allow them to\naccurately and reliably reason about real-world phenomena, and they can adapt\nsuch models extremely quickly in the face of unexpected changes. Deep neural\nnetwork models allow us to represent very complex functions, but lack this\ncapacity for rapid online adaptation. The goal in this paper is to develop a\nmethod for continual online learning from an incoming stream of data, using\ndeep neural network models. We formulate an online learning procedure that uses\nstochastic gradient descent to update model parameters, and an expectation\nmaximization algorithm with a Chinese restaurant process prior to develop and\nmaintain a mixture of models to handle non-stationary task distributions. This\nallows for all models to be adapted as necessary, with new models instantiated\nfor task changes and old models recalled when previously seen tasks are\nencountered again. Furthermore, we observe that meta-learning can be used to\nmeta-train a model such that this direct online adaptation with SGD is\neffective, which is otherwise not the case for large function approximators. In\nthis work, we apply our meta-learning for online learning (MOLe) approach to\nmodel-based reinforcement learning, where adapting the predictive model is\ncritical for control; we demonstrate that MOLe outperforms alternative prior\nmethods, and enables effective continuous adaptation in non-stationary task\ndistributions such as varying terrains, motor failures, and unexpected\ndisturbances.\n",
        "published": "2018",
        "authors": [
            "Anusha Nagabandi",
            "Chelsea Finn",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.11103v3",
        "title": "Learning to Walk via Deep Reinforcement Learning",
        "abstract": "  Deep reinforcement learning (deep RL) holds the promise of automating the\nacquisition of complex controllers that can map sensory inputs directly to\nlow-level actions. In the domain of robotic locomotion, deep RL could enable\nlearning locomotion skills with minimal engineering and without an explicit\nmodel of the robot dynamics. Unfortunately, applying deep RL to real-world\nrobotic tasks is exceptionally difficult, primarily due to poor sample\ncomplexity and sensitivity to hyperparameters. While hyperparameters can be\neasily tuned in simulated domains, tuning may be prohibitively expensive on\nphysical systems, such as legged robots, that can be damaged through extensive\ntrial-and-error learning. In this paper, we propose a sample-efficient deep RL\nalgorithm based on maximum entropy RL that requires minimal per-task tuning and\nonly a modest number of trials to learn neural network policies. We apply this\nmethod to learning walking gaits on a real-world Minitaur robot. Our method can\nacquire a stable gait from scratch directly in the real world in about two\nhours, without relying on any model or simulation, and the resulting policy is\nrobust to moderate variations in the environment. We further show that our\nalgorithm achieves state-of-the-art performance on simulated benchmarks with a\nsingle set of hyperparameters. Videos of training and the learned policy can be\nfound on the project website.\n",
        "published": "2018",
        "authors": [
            "Tuomas Haarnoja",
            "Sehoon Ha",
            "Aurick Zhou",
            "Jie Tan",
            "George Tucker",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.01063v1",
        "title": "NoRML: No-Reward Meta Learning",
        "abstract": "  Efficiently adapting to new environments and changes in dynamics is critical\nfor agents to successfully operate in the real world. Reinforcement learning\n(RL) based approaches typically rely on external reward feedback for\nadaptation. However, in many scenarios this reward signal might not be readily\navailable for the target task, or the difference between the environments can\nbe implicit and only observable from the dynamics. To this end, we introduce a\nmethod that allows for self-adaptation of learned policies: No-Reward Meta\nLearning (NoRML). NoRML extends Model Agnostic Meta Learning (MAML) for RL and\nuses observable dynamics of the environment instead of an explicit reward\nfunction in MAML's finetune step. Our method has a more expressive update step\nthan MAML, while maintaining MAML's gradient based foundation. Additionally, in\norder to allow more targeted exploration, we implement an extension to MAML\nthat effectively disconnects the meta-policy parameters from the fine-tuned\npolicies' parameters. We first study our method on a number of synthetic\ncontrol problems and then validate our method on common benchmark environments,\nshowing that NoRML outperforms MAML when the dynamics change between tasks.\n",
        "published": "2019",
        "authors": [
            "Yuxiang Yang",
            "Ken Caluwaerts",
            "Atil Iscen",
            "Jie Tan",
            "Chelsea Finn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.01669v1",
        "title": "Deep Active Localization",
        "abstract": "  Active localization is the problem of generating robot actions that allow it\nto maximally disambiguate its pose within a reference map. Traditional\napproaches to this use an information-theoretic criterion for action selection\nand hand-crafted perceptual models. In this work we propose an end-to-end\ndifferentiable method for learning to take informative actions that is\ntrainable entirely in simulation and then transferable to real robot hardware\nwith zero refinement. The system is composed of two modules: a convolutional\nneural network for perception, and a deep reinforcement learned planning\nmodule. We introduce a multi-scale approach to the learned perceptual model\nsince the accuracy needed to perform action selection with reinforcement\nlearning is much less than the accuracy needed for robot control. We\ndemonstrate that the resulting system outperforms using the traditional\napproach for either perception or planning. We also demonstrate our approaches\nrobustness to different map configurations and other nuisance parameters\nthrough the use of domain randomization in training. The code is also\ncompatible with the OpenAI gym framework, as well as the Gazebo simulator.\n",
        "published": "2019",
        "authors": [
            "Sai Krishna",
            "Keehong Seo",
            "Dhaivat Bhatt",
            "Vincent Mai",
            "Krishna Murthy",
            "Liam Paull"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.03698v4",
        "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning",
        "abstract": "  Autonomous agents that must exhibit flexible and broad capabilities will need\nto be equipped with large repertoires of skills. Defining each skill with a\nmanually-designed reward function limits this repertoire and imposes a manual\nengineering burden. Self-supervised agents that set their own goals can\nautomate this process, but designing appropriate goal setting objectives can be\ndifficult, and often involves heuristic design decisions. In this paper, we\npropose a formal exploration objective for goal-reaching policies that\nmaximizes state coverage. We show that this objective is equivalent to\nmaximizing goal reaching performance together with the entropy of the goal\ndistribution, where goals correspond to full state observations. To instantiate\nthis principle, we present an algorithm called Skew-Fit for learning a\nmaximum-entropy goal distributions. We prove that, under regularity conditions,\nSkew-Fit converges to a uniform distribution over the set of valid states, even\nwhen we do not know this set beforehand. Our experiments show that combining\nSkew-Fit for learning goal distributions with existing goal-reaching methods\noutperforms a variety of prior methods on open-sourced visual goal-reaching\ntasks. Moreover, we demonstrate that Skew-Fit enables a real-world robot to\nlearn to open a door, entirely from scratch, from pixels, and without any\nmanually-designed reward function.\n",
        "published": "2019",
        "authors": [
            "Vitchyr H. Pong",
            "Murtaza Dalal",
            "Steven Lin",
            "Ashvin Nair",
            "Shikhar Bahl",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.07400v2",
        "title": "Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically\n  Motivated Exploration",
        "abstract": "  Exploration in sparse reward reinforcement learning remains an open\nchallenge. Many state-of-the-art methods use intrinsic motivation to complement\nthe sparse extrinsic reward signal, giving the agent more opportunities to\nreceive feedback during exploration. Commonly these signals are added as bonus\nrewards, which results in a mixture policy that neither conducts exploration\nnor task fulfillment resolutely. In this paper, we instead learn separate\nintrinsic and extrinsic task policies and schedule between these different\ndrives to accelerate exploration and stabilize learning. Moreover, we introduce\na new type of intrinsic reward denoted as successor feature control (SFC),\nwhich is general and not task-specific. It takes into account statistics over\ncomplete trajectories and thus differs from previous methods that only use\nlocal information to evaluate intrinsic motivation. We evaluate our proposed\nscheduled intrinsic drive (SID) agent using three different environments with\npure visual inputs: VizDoom, DeepMind Lab and DeepMind Control Suite. The\nresults show a substantially improved exploration efficiency with SFC and the\nhierarchical usage of the intrinsic drives. A video of our experimental results\ncan be found at https://youtu.be/b0MbY3lUlEI.\n",
        "published": "2019",
        "authors": [
            "Jingwei Zhang",
            "Niklas Wetzel",
            "Nicolai Dorka",
            "Joschka Boedecker",
            "Wolfram Burgard"
        ]
    }
]