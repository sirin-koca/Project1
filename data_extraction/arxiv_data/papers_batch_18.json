[
    {
        "id": "http://arxiv.org/abs/2303.05501v2",
        "title": "PDSketch: Integrated Planning Domain Programming and Learning",
        "abstract": "  This paper studies a model learning and online planning approach towards\nbuilding flexible and general robots. Specifically, we investigate how to\nexploit the locality and sparsity structures in the underlying environmental\ntransition model to improve model generalization, data-efficiency, and\nruntime-efficiency. We present a new domain definition language, named\nPDSketch. It allows users to flexibly define high-level structures in the\ntransition models, such as object and feature dependencies, in a way similar to\nhow programmers use TensorFlow or PyTorch to specify kernel sizes and hidden\ndimensions of a convolutional neural network. The details of the transition\nmodel will be filled in by trainable neural networks. Based on the defined\nstructures and learned parameters, PDSketch automatically generates\ndomain-independent planning heuristics without additional training. The derived\nheuristics accelerate the performance-time planning for novel goals.\n",
        "published": "2023",
        "authors": [
            "Jiayuan Mao",
            "Tom\u00e1s Lozano-P\u00e9rez",
            "Joshua B. Tenenbaum",
            "Leslie Pack Kaelbling"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.02473v1",
        "title": "A Survey of Imitation Learning: Algorithms, Recent Developments, and\n  Challenges",
        "abstract": "  In recent years, the development of robotics and artificial intelligence (AI)\nsystems has been nothing short of remarkable. As these systems continue to\nevolve, they are being utilized in increasingly complex and unstructured\nenvironments, such as autonomous driving, aerial robotics, and natural language\nprocessing. As a consequence, programming their behaviors manually or defining\ntheir behavior through reward functions (as done in reinforcement learning\n(RL)) has become exceedingly difficult. This is because such environments\nrequire a high degree of flexibility and adaptability, making it challenging to\nspecify an optimal set of rules or reward signals that can account for all\npossible situations. In such environments, learning from an expert's behavior\nthrough imitation is often more appealing. This is where imitation learning\n(IL) comes into play - a process where desired behavior is learned by imitating\nan expert's behavior, which is provided through demonstrations.\n  This paper aims to provide an introduction to IL and an overview of its\nunderlying assumptions and approaches. It also offers a detailed description of\nrecent advances and emerging areas of research in the field. Additionally, the\npaper discusses how researchers have addressed common challenges associated\nwith IL and provides potential directions for future research. Overall, the\ngoal of the paper is to provide a comprehensive guide to the growing field of\nIL in robotics and AI.\n",
        "published": "2023",
        "authors": [
            "Maryam Zare",
            "Parham M. Kebria",
            "Abbas Khosravi",
            "Saeid Nahavandi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.01939v1",
        "title": "Foundations for Transfer in Reinforcement Learning: A Taxonomy of\n  Knowledge Modalities",
        "abstract": "  Contemporary artificial intelligence systems exhibit rapidly growing\nabilities accompanied by the growth of required resources, expansive datasets\nand corresponding investments into computing infrastructure. Although earlier\nsuccesses predominantly focus on constrained settings, recent strides in\nfundamental research and applications aspire to create increasingly general\nsystems. This evolving landscape presents a dual panorama of opportunities and\nchallenges in refining the generalisation and transfer of knowledge - the\nextraction from existing sources and adaptation as a comprehensive foundation\nfor tackling new problems. Within the domain of reinforcement learning (RL),\nthe representation of knowledge manifests through various modalities, including\ndynamics and reward models, value functions, policies, and the original data.\nThis taxonomy systematically targets these modalities and frames its discussion\nbased on their inherent properties and alignment with different objectives and\nmechanisms for transfer. Where possible, we aim to provide coarse guidance\ndelineating approaches which address requirements such as limiting environment\ninteractions, maximising computational efficiency, and enhancing generalisation\nacross varying axes of change. Finally, we analyse reasons contributing to the\nprevalence or scarcity of specific forms of transfer, the inherent potential\nbehind pushing these frontiers, and underscore the significance of\ntransitioning from designed to learned transfer.\n",
        "published": "2023",
        "authors": [
            "Markus Wulfmeier",
            "Arunkumar Byravan",
            "Sarah Bechtle",
            "Karol Hausman",
            "Nicolas Heess"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.13724v1",
        "title": "Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots",
        "abstract": "  We present Habitat 3.0: a simulation platform for studying collaborative\nhuman-robot tasks in home environments. Habitat 3.0 offers contributions across\nthree dimensions: (1) Accurate humanoid simulation: addressing challenges in\nmodeling complex deformable bodies and diversity in appearance and motion, all\nwhile ensuring high simulation speed. (2) Human-in-the-loop infrastructure:\nenabling real human interaction with simulated robots via mouse/keyboard or a\nVR interface, facilitating evaluation of robot policies with human input. (3)\nCollaborative tasks: studying two collaborative tasks, Social Navigation and\nSocial Rearrangement. Social Navigation investigates a robot's ability to\nlocate and follow humanoid avatars in unseen environments, whereas Social\nRearrangement addresses collaboration between a humanoid and robot while\nrearranging a scene. These contributions allow us to study end-to-end learned\nand heuristic baselines for human-robot collaboration in-depth, as well as\nevaluate them with humans in the loop. Our experiments demonstrate that learned\nrobot policies lead to efficient task completion when collaborating with unseen\nhumanoid agents and human partners that might exhibit behaviors that the robot\nhas not seen before. Additionally, we observe emergent behaviors during\ncollaborative task execution, such as the robot yielding space when obstructing\na humanoid agent, thereby allowing the effective completion of the task by the\nhumanoid agent. Furthermore, our experiments using the human-in-the-loop tool\ndemonstrate that our automated evaluation with humanoids can provide an\nindication of the relative ordering of different policies when evaluated with\nreal human collaborators. Habitat 3.0 unlocks interesting new features in\nsimulators for Embodied AI, and we hope it paves the way for a new frontier of\nembodied human-AI interaction capabilities.\n",
        "published": "2023",
        "authors": [
            "Xavier Puig",
            "Eric Undersander",
            "Andrew Szot",
            "Mikael Dallaire Cote",
            "Tsung-Yen Yang",
            "Ruslan Partsey",
            "Ruta Desai",
            "Alexander William Clegg",
            "Michal Hlavac",
            "So Yeon Min",
            "Vladim\u00edr Vondru\u0161",
            "Theophile Gervet",
            "Vincent-Pierre Berges",
            "John M. Turner",
            "Oleksandr Maksymets",
            "Zsolt Kira",
            "Mrinal Kalakrishnan",
            "Jitendra Malik",
            "Devendra Singh Chaplot",
            "Unnat Jain",
            "Dhruv Batra",
            "Akshara Rai",
            "Roozbeh Mottaghi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.6749v1",
        "title": "SENNS: Sparse Extraction Neural NetworkS for Feature Extraction",
        "abstract": "  By drawing on ideas from optimisation theory, artificial neural networks\n(ANN), graph embeddings and sparse representations, I develop a novel\ntechnique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at\naddressing the feature extraction problem. The proposed method uses (preferably\ndeep) ANNs for projecting input attribute vectors to an output space wherein\npairwise distances are maximized for vectors belonging to different classes,\nbut minimized for those belonging to the same class, while simultaneously\nenforcing sparsity on the ANN outputs. The vectors that result from the\nprojection can then be used as features in any classifier of choice.\nMathematically, I formulate the proposed method as the minimisation of an\nobjective function which can be interpreted, in the ANN output space, as a\nnegative factor of the sum of the squares of the pair-wise distances between\noutput vectors belonging to different classes, added to a positive factor of\nthe sum of squares of the pair-wise distances between output vectors belonging\nto the same classes, plus sparsity and weight decay terms. To derive an\nalgorithm for minimizing the objective function via gradient descent, I use the\nmulti-variate version of the chain rule to obtain the partial derivatives of\nthe function with respect to ANN weights and biases, and find that each of the\nrequired partial derivatives can be expressed as a sum of six terms. As it\nturns out, four of those six terms can be computed using the standard back\npropagation algorithm; the fifth can be computed via a slight modification of\nthe standard backpropagation algorithm; while the sixth one can be computed via\nsimple arithmetic. Finally, I propose experiments on the ARABASE Arabic corpora\nof digits and letters, the CMU PIE database of faces, the MNIST digits\ndatabase, and other standard machine learning databases.\n",
        "published": "2014",
        "authors": [
            "Abdulrahman Oladipupo Ibraheem"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.11204v1",
        "title": "Classification using Hyperdimensional Computing: A Review",
        "abstract": "  Hyperdimensional (HD) computing is built upon its unique data type referred\nto as hypervectors. The dimension of these hypervectors is typically in the\nrange of tens of thousands. Proposed to solve cognitive tasks, HD computing\naims at calculating similarity among its data. Data transformation is realized\nby three operations, including addition, multiplication and permutation. Its\nultra-wide data representation introduces redundancy against noise. Since\ninformation is evenly distributed over every bit of the hypervectors, HD\ncomputing is inherently robust. Additionally, due to the nature of those three\noperations, HD computing leads to fast learning ability, high energy efficiency\nand acceptable accuracy in learning and classification tasks. This paper\nintroduces the background of HD computing, and reviews the data representation,\ndata transformation, and similarity measurement. The orthogonality in high\ndimensions presents opportunities for flexible computing. To balance the\ntradeoff between accuracy and efficiency, strategies include but are not\nlimited to encoding, retraining, binarization and hardware acceleration.\nEvaluations indicate that HD computing shows great potential in addressing\nproblems using data in the form of letters, signals and images. HD computing\nespecially shows significant promise to replace machine learning algorithms as\na light-weight classifier in the field of internet of things (IoTs).\n",
        "published": "2020",
        "authors": [
            "Lulu Ge",
            "Keshab K. Parhi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.01111v1",
        "title": "Use of Artificial Intelligence to Analyse Risk in Legal Documents for a\n  Better Decision Support",
        "abstract": "  Assessing risk for voluminous legal documents such as request for proposal;\ncontracts is tedious and error prone. We have developed \"risk-o-meter\", a\nframework, based on machine learning and natural language processing to review\nand assess risks of any legal document. Our framework uses Paragraph Vector, an\nunsupervised model to generate vector representation of text. This enables the\nframework to learn contextual relations of legal terms and generate sensible\ncontext aware embedding. The framework then feeds the vector space into a\nsupervised classification algorithm to predict whether a paragraph belongs to a\nper-defined risk category or not. The framework thus extracts risk prone\nparagraphs. This technique efficiently overcomes the limitations of\nkeyword-based search. We have achieved an accuracy of 91% for the risk category\nhaving the largest training dataset. This framework will help organizations\noptimize effort to identify risk from large document base with minimal human\nintervention and thus will help to have risk mitigated sustainable growth. Its\nmachine learning capability makes it scalable to uncover relevant information\nfrom any type of document apart from legal documents, provided the library is\nper-populated and rich.\n",
        "published": "2019",
        "authors": [
            "Dipankar Chakrabarti",
            "Neelam Patodia",
            "Udayan Bhattacharya",
            "Indranil Mitra",
            "Satyaki Roy",
            "Jayanta Mandi",
            "Nandini Roy",
            "Prasun Nandy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.06477v3",
        "title": "Learning Reasoning Strategies in End-to-End Differentiable Proving",
        "abstract": "  Attempts to render deep learning models interpretable, data-efficient, and\nrobust have seen some success through hybridisation with rule-based systems,\nfor example, in Neural Theorem Provers (NTPs). These neuro-symbolic models can\ninduce interpretable rules and learn representations from data via\nback-propagation, while providing logical explanations for their predictions.\nHowever, they are restricted by their computational complexity, as they need to\nconsider all possible proof paths for explaining a goal, thus rendering them\nunfit for large-scale applications. We present Conditional Theorem Provers\n(CTPs), an extension to NTPs that learns an optimal rule selection strategy via\ngradient-based optimisation. We show that CTPs are scalable and yield\nstate-of-the-art results on the CLUTRR dataset, which tests systematic\ngeneralisation of neural models by learning to reason over smaller graphs and\nevaluating on larger ones. Finally, CTPs show better link prediction results on\nstandard benchmarks in comparison with other neural-symbolic models, while\nbeing explainable. All source code and datasets are available online, at\nhttps://github.com/uclnlp/ctp.\n",
        "published": "2020",
        "authors": [
            "Pasquale Minervini",
            "Sebastian Riedel",
            "Pontus Stenetorp",
            "Edward Grefenstette",
            "Tim Rockt\u00e4schel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.02837v1",
        "title": "aschern at SemEval-2020 Task 11: It Takes Three to Tango: RoBERTa, CRF,\n  and Transfer Learning",
        "abstract": "  We describe our system for SemEval-2020 Task 11 on Detection of Propaganda\nTechniques in News Articles. We developed ensemble models using RoBERTa-based\nneural architectures, additional CRF layers, transfer learning between the two\nsubtasks, and advanced post-processing to handle the multi-label nature of the\ntask, the consistency between nested spans, repetitions, and labels from\nsimilar spans in training. We achieved sizable improvements over baseline\nfine-tuned RoBERTa models, and the official evaluation ranked our system 3rd\n(almost tied with the 2nd) out of 36 teams on the span identification subtask\nwith an F1 score of 0.491, and 2nd (almost tied with the 1st) out of 31 teams\non the technique classification subtask with an F1 score of 0.62.\n",
        "published": "2020",
        "authors": [
            "Anton Chernyavskiy",
            "Dmitry Ilvovsky",
            "Preslav Nakov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.01693v2",
        "title": "DLGNet-Task: An End-to-end Neural Network Framework for Modeling\n  Multi-turn Multi-domain Task-Oriented Dialogue",
        "abstract": "  Task oriented dialogue (TOD) requires the complex interleaving of a number of\nindividually controllable components with strong guarantees for explainability\nand verifiability. This has made it difficult to adopt the multi-turn\nmulti-domain dialogue generation capabilities of streamlined end-to-end\nopen-domain dialogue systems. In this paper, we present a new framework,\nDLGNet-Task, a unified task-oriented dialogue system which employs\nautoregressive transformer networks such as DLGNet and GPT-2/3 to complete user\ntasks in multi-turn multi-domain conversations. Our framework enjoys the\ncontrollable, verifiable, and explainable outputs of modular approaches, and\nthe low development, deployment and maintenance cost of end-to-end systems.\nTreating open-domain system components as additional TOD system modules allows\nDLGNet-Task to learn the joint distribution of the inputs and outputs of all\nthe functional blocks of existing modular approaches such as, natural language\nunderstanding (NLU), state tracking, action policy, as well as natural language\ngeneration (NLG). Rather than training the modules individually, as is common\nin real-world systems, we trained them jointly with appropriate module\nseparations. When evaluated on the MultiWOZ2.1 dataset, DLGNet-Task shows\ncomparable performance to the existing state-of-the-art approaches.\nFurthermore, using DLGNet-Task in conversational AI systems reduces the level\nof effort required for developing, deploying, and maintaining intelligent\nassistants at scale.\n",
        "published": "2020",
        "authors": [
            "Oluwatobi O. Olabiyi",
            "Prarthana Bhattarai",
            "C. Bayan Bruss",
            "Zachary Kulis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.02590v3",
        "title": "Reliability Testing for Natural Language Processing Systems",
        "abstract": "  Questions of fairness, robustness, and transparency are paramount to address\nbefore deploying NLP systems. Central to these concerns is the question of\nreliability: Can NLP systems reliably treat different demographics fairly and\nfunction correctly in diverse and noisy environments? To address this, we argue\nfor the need for reliability testing and contextualize it among existing work\non improving accountability. We show how adversarial attacks can be reframed\nfor this goal, via a framework for developing reliability tests. We argue that\nreliability testing -- with an emphasis on interdisciplinary collaboration --\nwill enable rigorous and targeted testing, and aid in the enactment and\nenforcement of industry standards.\n",
        "published": "2021",
        "authors": [
            "Samson Tan",
            "Shafiq Joty",
            "Kathy Baxter",
            "Araz Taeihagh",
            "Gregory A. Bennett",
            "Min-Yen Kan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1612.02741v4",
        "title": "Coupling Distributed and Symbolic Execution for Natural Language Queries",
        "abstract": "  Building neural networks to query a knowledge base (a table) with natural\nlanguage is an emerging research topic in deep learning. An executor for table\nquerying typically requires multiple steps of execution because queries may\nhave complicated structures. In previous studies, researchers have developed\neither fully distributed executors or symbolic executors for table querying. A\ndistributed executor can be trained in an end-to-end fashion, but is weak in\nterms of execution efficiency and explicit interpretability. A symbolic\nexecutor is efficient in execution, but is very difficult to train especially\nat initial stages. In this paper, we propose to couple distributed and symbolic\nexecution for natural language queries, where the symbolic executor is\npretrained with the distributed executor's intermediate execution results in a\nstep-by-step fashion. Experiments show that our approach significantly\noutperforms both distributed and symbolic executors, exhibiting high accuracy,\nhigh learning efficiency, high execution efficiency, and high interpretability.\n",
        "published": "2016",
        "authors": [
            "Lili Mou",
            "Zhengdong Lu",
            "Hang Li",
            "Zhi Jin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.00254v1",
        "title": "Joint Bootstrapping Machines for High Confidence Relation Extraction",
        "abstract": "  Semi-supervised bootstrapping techniques for relationship extraction from\ntext iteratively expand a set of initial seed instances. Due to the lack of\nlabeled data, a key challenge in bootstrapping is semantic drift: if a false\npositive instance is added during an iteration, then all following iterations\nare contaminated. We introduce BREX, a new bootstrapping method that protects\nagainst such contamination by highly effective confidence assessment. This is\nachieved by using entity and template seeds jointly (as opposed to just one as\nin previous work), by expanding entities and templates in parallel and in a\nmutually constraining fashion in each iteration and by introducing\nhigherquality similarity measures for templates. Experimental results show that\nBREX achieves an F1 that is 0.13 (0.87 vs. 0.74) better than the state of the\nart for four relationships.\n",
        "published": "2018",
        "authors": [
            "Pankaj Gupta",
            "Benjamin Roth",
            "Hinrich Sch\u00fctze"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.06490v2",
        "title": "Active$^2$ Learning: Actively reducing redundancies in Active Learning\n  methods for Sequence Tagging and Machine Translation",
        "abstract": "  While deep learning is a powerful tool for natural language processing (NLP)\nproblems, successful solutions to these problems rely heavily on large amounts\nof annotated samples. However, manually annotating data is expensive and\ntime-consuming. Active Learning (AL) strategies reduce the need for huge\nvolumes of labeled data by iteratively selecting a small number of examples for\nmanual annotation based on their estimated utility in training the given model.\nIn this paper, we argue that since AL strategies choose examples independently,\nthey may potentially select similar examples, all of which may not contribute\nsignificantly to the learning process. Our proposed approach,\nActive$\\mathbf{^2}$ Learning (A$\\mathbf{^2}$L), actively adapts to the deep\nlearning model being trained to eliminate further such redundant examples\nchosen by an AL strategy. We show that A$\\mathbf{^2}$L is widely applicable by\nusing it in conjunction with several different AL strategies and NLP tasks. We\nempirically demonstrate that the proposed approach is further able to reduce\nthe data requirements of state-of-the-art AL strategies by an absolute\npercentage reduction of $\\approx\\mathbf{3-25\\%}$ on multiple NLP tasks while\nachieving the same performance with no additional computation overhead.\n",
        "published": "2021",
        "authors": [
            "Rishi Hazra",
            "Parag Dutta",
            "Shubham Gupta",
            "Mohammed Abdul Qaathir",
            "Ambedkar Dukkipati"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.07423v2",
        "title": "The Role of Context in Detecting Previously Fact-Checked Claims",
        "abstract": "  Recent years have seen the proliferation of disinformation and fake news\nonline. Traditional approaches to mitigate these issues is to use manual or\nautomatic fact-checking. Recently, another approach has emerged: checking\nwhether the input claim has previously been fact-checked, which can be done\nautomatically, and thus fast, while also offering credibility and\nexplainability, thanks to the human fact-checking and explanations in the\nassociated fact-checking article. Here, we focus on claims made in a political\ndebate and we study the impact of modeling the context of the claim: both on\nthe source side, i.e., in the debate, as well as on the target side, i.e., in\nthe fact-checking explanation document. We do this by modeling the local\ncontext, the global context, as well as by means of co-reference resolution,\nand multi-hop reasoning over the sentences of the document describing the\nfact-checked claim. The experimental results show that each of these represents\na valuable information source, but that modeling the source-side context is\nmost important, and can yield 10+ points of absolute improvement over a\nstate-of-the-art model.\n",
        "published": "2021",
        "authors": [
            "Shaden Shaar",
            "Firoj Alam",
            "Giovanni Da San Martino",
            "Preslav Nakov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.13006v1",
        "title": "RuleBert: Teaching Soft Rules to Pre-trained Language Models",
        "abstract": "  While pre-trained language models (PLMs) are the go-to solution to tackle\nmany natural language processing problems, they are still very limited in their\nability to capture and to use common-sense knowledge. In fact, even if\ninformation is available in the form of approximate (soft) logical rules, it is\nnot clear how to transfer it to a PLM in order to improve its performance for\ndeductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how\nto reason with soft Horn rules. We introduce a classification task where, given\nfacts and soft rules, the PLM should return a prediction with a probability for\na given hypothesis. We release the first dataset for this task, and we propose\na revised loss function that enables the PLM to learn how to predict precise\nprobabilities for the task. Our evaluation results show that the resulting\nfine-tuned models achieve very high performance, even on logical rules that\nwere unseen at training. Moreover, we demonstrate that logical notions\nexpressed by the rules are transferred to the fine-tuned model, yielding\nstate-of-the-art results on external datasets.\n",
        "published": "2021",
        "authors": [
            "Mohammed Saeed",
            "Naser Ahmadi",
            "Preslav Nakov",
            "Paolo Papotti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.15728v1",
        "title": "Deep Learning for Bias Detection: From Inception to Deployment",
        "abstract": "  To create a more inclusive workplace, enterprises are actively investing in\nidentifying and eliminating unconscious bias (e.g., gender, race, age,\ndisability, elitism and religion) across their various functions. We propose a\ndeep learning model with a transfer learning based language model to learn from\nmanually tagged documents for automatically identifying bias in enterprise\ncontent. We first pretrain a deep learning-based language-model using\nWikipedia, then fine tune the model with a large unlabelled data set related\nwith various types of enterprise content. Finally, a linear layer followed by\nsoftmax layer is added at the end of the language model and the model is\ntrained on a labelled bias dataset consisting of enterprise content. The\ntrained model is thoroughly evaluated on independent datasets to ensure a\ngeneral application. We present the proposed method and its deployment detail\nin a real-world application.\n",
        "published": "2021",
        "authors": [
            "Md Abul Bashar",
            "Richi Nayak",
            "Anjor Kothare",
            "Vishal Sharma",
            "Kesavan Kandadai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.14232v1",
        "title": "Long-range and hierarchical language predictions in brains and\n  algorithms",
        "abstract": "  Deep learning has recently made remarkable progress in natural language\nprocessing. Yet, the resulting algorithms remain far from competing with the\nlanguage abilities of the human brain. Predictive coding theory offers a\npotential explanation to this discrepancy: while deep language algorithms are\noptimized to predict adjacent words, the human brain would be tuned to make\nlong-range and hierarchical predictions. To test this hypothesis, we analyze\nthe fMRI brain signals of 304 subjects each listening to 70min of short\nstories. After confirming that the activations of deep language algorithms\nlinearly map onto those of the brain, we show that enhancing these models with\nlong-range forecast representations improves their brain-mapping. The results\nfurther reveal a hierarchy of predictions in the brain, whereby the\nfronto-parietal cortices forecast more abstract and more distant\nrepresentations than the temporal cortices. Overall, this study strengthens\npredictive coding theory and suggests a critical role of long-range and\nhierarchical predictions in natural language processing.\n",
        "published": "2021",
        "authors": [
            "Charlotte Caucheteux",
            "Alexandre Gramfort",
            "Jean-Remi King"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.02095v1",
        "title": "Intelligent Trading Systems: A Sentiment-Aware Reinforcement Learning\n  Approach",
        "abstract": "  The feasibility of making profitable trades on a single asset on stock\nexchanges based on patterns identification has long attracted researchers.\nReinforcement Learning (RL) and Natural Language Processing have gained\nnotoriety in these single-asset trading tasks, but only a few works have\nexplored their combination. Moreover, some issues are still not addressed, such\nas extracting market sentiment momentum through the explicit capture of\nsentiment features that reflect the market condition over time and assessing\nthe consistency and stability of RL results in different situations. Filling\nthis gap, we propose the Sentiment-Aware RL (SentARL) intelligent trading\nsystem that improves profit stability by leveraging market mood through an\nadaptive amount of past sentiment features drawn from textual news. We\nevaluated SentARL across twenty assets, two transaction costs, and five\ndifferent periods and initializations to show its consistent effectiveness\nagainst baselines. Subsequently, this thorough assessment allowed us to\nidentify the boundary between news coverage and market sentiment regarding the\ncorrelation of price-time series above which SentARL's effectiveness is\noutstanding.\n",
        "published": "2021",
        "authors": [
            "Francisco Caio Lima Paiva",
            "Leonardo Kanashiro Felizardo",
            "Reinaldo Augusto da Costa Bianchi",
            "Anna Helena Reali Costa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.04120v1",
        "title": "BASPRO: a balanced script producer for speech corpus collection based on\n  the genetic algorithm",
        "abstract": "  The performance of speech-processing models is heavily influenced by the\nspeech corpus that is used for training and evaluation. In this study, we\npropose BAlanced Script PROducer (BASPRO) system, which can automatically\nconstruct a phonetically balanced and rich set of Chinese sentences for\ncollecting Mandarin Chinese speech data. First, we used pretrained natural\nlanguage processing systems to extract ten-character candidate sentences from a\nlarge corpus of Chinese news texts. Then, we applied a genetic algorithm-based\nmethod to select 20 phonetically balanced sentence sets, each containing 20\nsentences, from the candidate sentences. Using BASPRO, we obtained a recording\nscript called TMNews, which contains 400 ten-character sentences. TMNews covers\n84% of the syllables used in the real world. Moreover, the syllable\ndistribution has 0.96 cosine similarity to the real-world syllable\ndistribution. We converted the script into a speech corpus using two\ntext-to-speech systems. Using the designed speech corpus, we tested the\nperformances of speech enhancement (SE) and automatic speech recognition (ASR),\nwhich are one of the most important regression- and classification-based speech\nprocessing tasks, respectively. The experimental results show that the SE and\nASR models trained on the designed speech corpus outperform their counterparts\ntrained on a randomly composed speech corpus.\n",
        "published": "2022",
        "authors": [
            "Yu-Wen Chen",
            "Hsin-Min Wang",
            "Yu Tsao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.11244v2",
        "title": "A Parameter-Efficient Learning Approach to Arabic Dialect Identification\n  with Pre-Trained General-Purpose Speech Model",
        "abstract": "  In this work, we explore Parameter-Efficient-Learning (PEL) techniques to\nrepurpose a General-Purpose-Speech (GSM) model for Arabic dialect\nidentification (ADI). Specifically, we investigate different setups to\nincorporate trainable features into a multi-layer encoder-decoder GSM\nformulation under frozen pre-trained settings. Our architecture includes\nresidual adapter and model reprogramming (input-prompting). We design a\ntoken-level label mapping to condition the GSM for Arabic Dialect\nIdentification (ADI). This is challenging due to the high variation in\nvocabulary and pronunciation among the numerous regional dialects. We achieve\nnew state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We\nfurther reduce the training budgets with the PEL method, which performs within\n1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable\nparameters. Our study demonstrates how to identify Arabic dialects using a\nsmall dataset and limited computation with open source code and pre-trained\nmodels.\n",
        "published": "2023",
        "authors": [
            "Srijith Radhakrishnan",
            "Chao-Han Huck Yang",
            "Sumeer Ahmad Khan",
            "Narsis A. Kiani",
            "David Gomez-Cabrero",
            "Jesper N. Tegner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.06440v4",
        "title": "No Train No Gain: Revisiting Efficient Training Algorithms For\n  Transformer-based Language Models",
        "abstract": "  The computation necessary for training Transformer-based language models has\nskyrocketed in recent years. This trend has motivated research on efficient\ntraining algorithms designed to improve training, validation, and downstream\nperformance faster than standard training. In this work, we revisit three\ncategories of such algorithms: dynamic architectures (layer stacking, layer\ndropping), batch selection (selective backprop, RHO loss), and efficient\noptimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed\ncomputation budget using such methods, we find that their training, validation,\nand downstream gains vanish compared to a baseline with a fully-decayed\nlearning rate. We define an evaluation protocol that enables computation to be\ndone on arbitrary machines by mapping all computation time to a reference\nmachine which we call reference system time. We discuss the limitations of our\nproposed protocol and release our code to encourage rigorous research in\nefficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.\n",
        "published": "2023",
        "authors": [
            "Jean Kaddour",
            "Oscar Key",
            "Piotr Nawrot",
            "Pasquale Minervini",
            "Matt J. Kusner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.09397v1",
        "title": "Do Large GPT Models Discover Moral Dimensions in Language\n  Representations? A Topological Study Of Sentence Embeddings",
        "abstract": "  As Large Language Models are deployed within Artificial Intelligence systems,\nthat are increasingly integrated with human society, it becomes more important\nthan ever to study their internal structures. Higher level abilities of LLMs\nsuch as GPT-3.5 emerge in large part due to informative language\nrepresentations they induce from raw text data during pre-training on trillions\nof words. These embeddings exist in vector spaces of several thousand\ndimensions, and their processing involves mapping between multiple vector\nspaces, with total number of parameters on the order of trillions. Furthermore,\nthese language representations are induced by gradient optimization, resulting\nin a black box system that is hard to interpret. In this paper, we take a look\nat the topological structure of neuronal activity in the \"brain\" of Chat-GPT's\nfoundation language model, and analyze it with respect to a metric representing\nthe notion of fairness. We develop a novel approach to visualize GPT's moral\ndimensions. We first compute a fairness metric, inspired by social psychology\nliterature, to identify factors that typically influence fairness assessments\nin humans, such as legitimacy, need, and responsibility. Subsequently, we\nsummarize the manifold's shape using a lower-dimensional simplicial complex,\nwhose topology is derived from this metric. We color it with a heat map\nassociated with this fairness metric, producing human-readable visualizations\nof the high-dimensional sentence manifold. Our results show that sentence\nembeddings based on GPT-3.5 can be decomposed into two submanifolds\ncorresponding to fair and unfair moral judgments. This indicates that GPT-based\nlanguage models develop a moral dimension within their representation spaces\nand induce an understanding of fairness during their training process.\n",
        "published": "2023",
        "authors": [
            "Stephen Fitz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1510.08983v2",
        "title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition",
        "abstract": "  In this paper, we extend the deep long short-term memory (DLSTM) recurrent\nneural networks by introducing gated direct connections between memory cells in\nadjacent layers. These direct links, called highway connections, enable\nunimpeded information flow across different layers and thus alleviate the\ngradient vanishing problem when building deeper LSTMs. We further introduce the\nlatency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole\nhistory while keeping the latency under control. Efficient algorithms are\nproposed to train these novel networks using both frame and sequence\ndiscriminative criteria. Experiments on the AMI distant speech recognition\n(DSR) task indicate that we can train deeper LSTMs and achieve better\nimprovement from sequence training with highway LSTMs (HLSTMs). Our novel model\nobtains $43.9/47.7\\%$ WER on AMI (SDM) dev and eval sets, outperforming all\nprevious works. It beats the strong DNN and DLSTM baselines with $15.7\\%$ and\n$5.3\\%$ relative improvement respectively.\n",
        "published": "2015",
        "authors": [
            "Yu Zhang",
            "Guoguo Chen",
            "Dong Yu",
            "Kaisheng Yao",
            "Sanjeev Khudanpur",
            "James Glass"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.05374v2",
        "title": "Twin Regularization for online speech recognition",
        "abstract": "  Online speech recognition is crucial for developing natural human-machine\ninterfaces. This modality, however, is significantly more challenging than\noff-line ASR, since real-time/low-latency constraints inevitably hinder the use\nof future information, that is known to be very helpful to perform robust\npredictions. A popular solution to mitigate this issue consists of feeding\nneural acoustic models with context windows that gather some future frames.\nThis introduces a latency which depends on the number of employed look-ahead\nfeatures. This paper explores a different approach, based on estimating the\nfuture rather than waiting for it. Our technique encourages the hidden\nrepresentations of a unidirectional recurrent network to embed some useful\ninformation about the future. Inspired by a recently proposed technique called\nTwin Networks, we add a regularization term that forces forward hidden states\nto be as close as possible to cotemporal backward ones, computed by a \"twin\"\nneural network running backwards in time. The experiments, conducted on a\nnumber of datasets, recurrent architectures, input features, and acoustic\nconditions, have shown the effectiveness of this approach. One important\nadvantage is that our method does not introduce any additional computation at\ntest time if compared to standard unidirectional recurrent networks.\n",
        "published": "2018",
        "authors": [
            "Mirco Ravanelli",
            "Dmitriy Serdyuk",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.04364v1",
        "title": "It's Morphin' Time! Combating Linguistic Discrimination with\n  Inflectional Perturbations",
        "abstract": "  Training on only perfect Standard English corpora predisposes pre-trained\nneural networks to discriminate against minorities from non-standard linguistic\nbackgrounds (e.g., African American Vernacular English, Colloquial Singapore\nEnglish, etc.). We perturb the inflectional morphology of words to craft\nplausible and semantically similar adversarial examples that expose these\nbiases in popular NLP models, e.g., BERT and Transformer, and show that\nadversarially fine-tuning them for a single epoch significantly improves\nrobustness without sacrificing performance on clean data.\n",
        "published": "2020",
        "authors": [
            "Samson Tan",
            "Shafiq Joty",
            "Min-Yen Kan",
            "Richard Socher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.09593v3",
        "title": "Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots",
        "abstract": "  Multilingual models have demonstrated impressive cross-lingual transfer\nperformance. However, test sets like XNLI are monolingual at the example level.\nIn multilingual communities, it is common for polyglots to code-mix when\nconversing with each other. Inspired by this phenomenon, we present two strong\nblack-box adversarial attacks (one word-level, one phrase-level) for\nmultilingual models that push their ability to handle code-mixed sentences to\nthe limit. The former uses bilingual dictionaries to propose perturbations and\ntranslations of the clean example for sense disambiguation. The latter directly\naligns the clean example with its translations before extracting phrases as\nperturbations. Our phrase-level attack has a success rate of 89.75% against\nXLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI.\nFinally, we propose an efficient adversarial training scheme that trains in the\nsame number of steps as the original model and show that it improves model\naccuracy.\n",
        "published": "2021",
        "authors": [
            "Samson Tan",
            "Shafiq Joty"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.03552v1",
        "title": "Cross-lingual Offensive Language Identification for Low Resource\n  Languages: The Case of Marathi",
        "abstract": "  The widespread presence of offensive language on social media motivated the\ndevelopment of systems capable of recognizing such content automatically. Apart\nfrom a few notable exceptions, most research on automatic offensive language\nidentification has dealt with English. To address this shortcoming, we\nintroduce MOLD, the Marathi Offensive Language Dataset. MOLD is the first\ndataset of its kind compiled for Marathi, thus opening a new domain for\nresearch in low-resource Indo-Aryan languages. We present results from several\nmachine learning experiments on this dataset, including zero-short and other\ntransfer learning experiments on state-of-the-art cross-lingual transformers\nfrom existing data in Bengali, English, and Hindi.\n",
        "published": "2021",
        "authors": [
            "Saurabh Gaikwad",
            "Tharindu Ranasinghe",
            "Marcos Zampieri",
            "Christopher M. Homan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.02220v2",
        "title": "Fast Contextual Adaptation with Neural Associative Memory for On-Device\n  Personalized Speech Recognition",
        "abstract": "  Fast contextual adaptation has shown to be effective in improving Automatic\nSpeech Recognition (ASR) of rare words and when combined with an on-device\npersonalized training, it can yield an even better recognition result. However,\nthe traditional re-scoring approaches based on an external language model is\nprone to diverge during the personalized training. In this work, we introduce a\nmodel-based end-to-end contextual adaptation approach that is decoder-agnostic\nand amenable to on-device personalization. Our on-device simulation experiments\ndemonstrate that the proposed approach outperforms the traditional re-scoring\ntechnique by 12% relative WER and 15.7% entity mention specific F1-score in a\ncontinues personalization scenario.\n",
        "published": "2021",
        "authors": [
            "Tsendsuren Munkhdalai",
            "Khe Chai Sim",
            "Angad Chandorkar",
            "Fan Gao",
            "Mason Chua",
            "Trevor Strohman",
            "Fran\u00e7oise Beaufays"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.15725v1",
        "title": "Batch-Softmax Contrastive Loss for Pairwise Sentence Scoring Tasks",
        "abstract": "  The use of contrastive loss for representation learning has become prominent\nin computer vision, and it is now getting attention in Natural Language\nProcessing (NLP). Here, we explore the idea of using a batch-softmax\ncontrastive loss when fine-tuning large-scale pre-trained transformer models to\nlearn better task-specific sentence embeddings for pairwise sentence scoring\ntasks. We introduce and study a number of variations in the calculation of the\nloss as well as in the overall training procedure; in particular, we find that\ndata shuffling can be quite important. Our experimental results show sizable\nimprovements on a number of datasets and pairwise sentence scoring tasks\nincluding classification, ranking, and regression. Finally, we offer detailed\nanalysis and discussion, which should be useful for researchers aiming to\nexplore the utility of contrastive loss in NLP.\n",
        "published": "2021",
        "authors": [
            "Anton Chernyavskiy",
            "Dmitry Ilvovsky",
            "Pavel Kalinin",
            "Preslav Nakov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.02443v1",
        "title": "An Exploratory Literature Study on Sharing and Energy Use of Language\n  Models for Source Code",
        "abstract": "  Large language models trained on source code can support a variety of\nsoftware development tasks, such as code recommendation and program repair.\nLarge amounts of data for training such models benefit the models' performance.\nHowever, the size of the data and models results in long training times and\nhigh energy consumption. While publishing source code allows for replicability,\nusers need to repeat the expensive training process if models are not shared.\nThe main goal of the study is to investigate if publications that trained\nlanguage models for software engineering (SE) tasks share source code and\ntrained artifacts. The second goal is to analyze the transparency on training\nenergy usage. We perform a snowballing-based literature search to find\npublications on language models for source code, and analyze their reusability\nfrom a sustainability standpoint.\n  From 494 unique publications, we identified 293 relevant publications that\nuse language models to address code-related tasks. Among them, 27% (79 out of\n293) make artifacts available for reuse. This can be in the form of tools or\nIDE plugins designed for specific tasks or task-agnostic models that can be\nfine-tuned for a variety of downstream tasks. Moreover, we collect insights on\nthe hardware used for model training, as well as training time, which together\ndetermine the energy consumption of the development process. We find that there\nare deficiencies in the sharing of information and artifacts for current\nstudies on source code models for software engineering tasks, with 40% of the\nsurveyed papers not sharing source code or trained artifacts. We recommend the\nsharing of source code as well as trained artifacts, to enable sustainable\nreproducibility. Moreover, comprehensive information on training times and\nhardware configurations should be shared for transparency on a model's carbon\nfootprint.\n",
        "published": "2023",
        "authors": [
            "Max Hort",
            "Anastasiia Grishina",
            "Leon Moonen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.02283v3",
        "title": "Generation and Comprehension of Unambiguous Object Descriptions",
        "abstract": "  We propose a method that can generate an unambiguous description (known as a\nreferring expression) of a specific object or region in an image, and which can\nalso comprehend or interpret such an expression to infer which object is being\ndescribed. We show that our method outperforms previous methods that generate\ndescriptions of objects without taking into account other potentially ambiguous\nobjects in the scene. Our model is inspired by recent successes of deep\nlearning methods for image captioning, but while image captioning is difficult\nto evaluate, our task allows for easy objective evaluation. We also present a\nnew large-scale dataset for referring expressions, based on MS-COCO. We have\nreleased the dataset and a toolbox for visualization and evaluation, see\nhttps://github.com/mjhucla/Google_Refexp_toolbox\n",
        "published": "2015",
        "authors": [
            "Junhua Mao",
            "Jonathan Huang",
            "Alexander Toshev",
            "Oana Camburu",
            "Alan Yuille",
            "Kevin Murphy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.03409v2",
        "title": "Transferable Representation Learning in Vision-and-Language Navigation",
        "abstract": "  Vision-and-Language Navigation (VLN) tasks such as Room-to-Room (R2R) require\nmachine agents to interpret natural language instructions and learn to act in\nvisually realistic environments to achieve navigation goals. The overall task\nrequires competence in several perception problems: successful agents combine\nspatio-temporal, vision and language understanding to produce appropriate\naction sequences. Our approach adapts pre-trained vision and language\nrepresentations to relevant in-domain tasks making them more effective for VLN.\nSpecifically, the representations are adapted to solve both a cross-modal\nsequence alignment and sequence coherence task. In the sequence alignment task,\nthe model determines whether an instruction corresponds to a sequence of visual\nframes. In the sequence coherence task, the model determines whether the\nperceptual sequences are predictive sequentially in the instruction-conditioned\nlatent space. By transferring the domain-adapted representations, we improve\ncompetitive agents in R2R as measured by the success rate weighted by path\nlength (SPL) metric.\n",
        "published": "2019",
        "authors": [
            "Haoshuo Huang",
            "Vihan Jain",
            "Harsh Mehta",
            "Alexander Ku",
            "Gabriel Magalhaes",
            "Jason Baldridge",
            "Eugene Ie"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.12083v1",
        "title": "Language-Conditioned Imitation Learning for Robot Manipulation Tasks",
        "abstract": "  Imitation learning is a popular approach for teaching motor skills to robots.\nHowever, most approaches focus on extracting policy parameters from execution\ntraces alone (i.e., motion trajectories and perceptual data). No adequate\ncommunication channel exists between the human expert and the robot to describe\ncritical aspects of the task, such as the properties of the target object or\nthe intended shape of the motion. Motivated by insights into the human teaching\nprocess, we introduce a method for incorporating unstructured natural language\ninto imitation learning. At training time, the expert can provide\ndemonstrations along with verbal descriptions in order to describe the\nunderlying intent (e.g., \"go to the large green bowl\"). The training process\nthen interrelates these two modalities to encode the correlations between\nlanguage, perception, and motion. The resulting language-conditioned visuomotor\npolicies can be conditioned at runtime on new human commands and instructions,\nwhich allows for more fine-grained control over the trained policies while also\nreducing situational ambiguity. We demonstrate in a set of simulation\nexperiments how our approach can learn language-conditioned manipulation\npolicies for a seven-degree-of-freedom robot arm and compare the results to a\nvariety of alternative methods.\n",
        "published": "2020",
        "authors": [
            "Simon Stepputtis",
            "Joseph Campbell",
            "Mariano Phielipp",
            "Stefan Lee",
            "Chitta Baral",
            "Heni Ben Amor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1607.03827v2",
        "title": "The KIT Motion-Language Dataset",
        "abstract": "  Linking human motion and natural language is of great interest for the\ngeneration of semantic representations of human activities as well as for the\ngeneration of robot activities based on natural language input. However, while\nthere have been years of research in this area, no standardized and openly\navailable dataset exists to support the development and evaluation of such\nsystems. We therefore propose the KIT Motion-Language Dataset, which is large,\nopen, and extensible. We aggregate data from multiple motion capture databases\nand include them in our dataset using a unified representation that is\nindependent of the capture system or marker set, making it easy to work with\nthe data regardless of its origin. To obtain motion annotations in natural\nlanguage, we apply a crowd-sourcing approach and a web-based tool that was\nspecifically build for this purpose, the Motion Annotation Tool. We thoroughly\ndocument the annotation process itself and discuss gamification methods that we\nused to keep annotators motivated. We further propose a novel method,\nperplexity-based selection, which systematically selects motions for further\nannotation that are either under-represented in our dataset or that have\nerroneous annotations. We show that our method mitigates the two aforementioned\nproblems and ensures a systematic annotation process. We provide an in-depth\nanalysis of the structure and contents of our resulting dataset, which, as of\nOctober 10, 2016, contains 3911 motions with a total duration of 11.23 hours\nand 6278 annotations in natural language that contain 52,903 words. We believe\nthis makes our dataset an excellent choice that enables more transparent and\ncomparable research in this important area.\n",
        "published": "2016",
        "authors": [
            "Matthias Plappert",
            "Christian Mandery",
            "Tamim Asfour"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.11744v1",
        "title": "Imitation Learning of Robot Policies by Combining Language, Vision and\n  Demonstration",
        "abstract": "  In this work we propose a novel end-to-end imitation learning approach which\ncombines natural language, vision, and motion information to produce an\nabstract representation of a task, which in turn is used to synthesize specific\nmotion controllers at run-time. This multimodal approach enables generalization\nto a wide variety of environmental conditions and allows an end-user to direct\na robot policy through verbal communication. We empirically validate our\napproach with an extensive set of simulations and show that it achieves a high\ntask success rate over a variety of conditions while remaining amenable to\nprobabilistic interpretability.\n",
        "published": "2019",
        "authors": [
            "Simon Stepputtis",
            "Joseph Campbell",
            "Mariano Phielipp",
            "Chitta Baral",
            "Heni Ben Amor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.10638v2",
        "title": "Towards Learning a Generic Agent for Vision-and-Language Navigation via\n  Pre-training",
        "abstract": "  Learning to navigate in a visual environment following natural-language\ninstructions is a challenging task, because the multimodal inputs to the agent\nare highly variable, and the training data on a new task is often limited. In\nthis paper, we present the first pre-training and fine-tuning paradigm for\nvision-and-language navigation (VLN) tasks. By training on a large amount of\nimage-text-action triplets in a self-supervised learning manner, the\npre-trained model provides generic representations of visual environments and\nlanguage instructions. It can be easily used as a drop-in for existing VLN\nframeworks, leading to the proposed agent called Prevalent. It learns more\neffectively in new tasks and generalizes better in a previously unseen\nenvironment. The performance is validated on three VLN tasks. On the\nRoom-to-Room benchmark, our model improves the state-of-the-art from 47% to 51%\non success rate weighted by path length. Further, the learned representation is\ntransferable to other VLN tasks. On two recent tasks, vision-and-dialog\nnavigation and \"Help, Anna!\" the proposed Prevalent leads to significant\nimprovement over existing methods, achieving a new state of the art.\n",
        "published": "2020",
        "authors": [
            "Weituo Hao",
            "Chunyuan Li",
            "Xiujun Li",
            "Lawrence Carin",
            "Jianfeng Gao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.12098v1",
        "title": "CLIPort: What and Where Pathways for Robotic Manipulation",
        "abstract": "  How can we imbue robots with the ability to manipulate objects precisely but\nalso to reason about them in terms of abstract concepts? Recent works in\nmanipulation have shown that end-to-end networks can learn dexterous skills\nthat require precise spatial reasoning, but these methods often fail to\ngeneralize to new goals or quickly learn transferable concepts across tasks. In\nparallel, there has been great progress in learning generalizable semantic\nrepresentations for vision and language by training on large-scale internet\ndata, however these representations lack the spatial understanding necessary\nfor fine-grained manipulation. To this end, we propose a framework that\ncombines the best of both worlds: a two-stream architecture with semantic and\nspatial pathways for vision-based manipulation. Specifically, we present\nCLIPort, a language-conditioned imitation-learning agent that combines the\nbroad semantic understanding (what) of CLIP [1] with the spatial precision\n(where) of Transporter [2]. Our end-to-end framework is capable of solving a\nvariety of language-specified tabletop tasks from packing unseen objects to\nfolding cloths, all without any explicit representations of object poses,\ninstance segmentations, memory, symbolic states, or syntactic structures.\nExperiments in simulated and real-world settings show that our approach is data\nefficient in few-shot settings and generalizes effectively to seen and unseen\nsemantic concepts. We even learn one multi-task policy for 10 simulated and 9\nreal-world tasks that is better or comparable to single-task policies.\n",
        "published": "2021",
        "authors": [
            "Mohit Shridhar",
            "Lucas Manuelli",
            "Dieter Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.05135v3",
        "title": "Signs of Language: Embodied Sign Language Fingerspelling Acquisition\n  from Demonstrations for Human-Robot Interaction",
        "abstract": "  Learning fine-grained movements is a challenging topic in robotics,\nparticularly in the context of robotic hands. One specific instance of this\nchallenge is the acquisition of fingerspelling sign language in robots. In this\npaper, we propose an approach for learning dexterous motor imitation from video\nexamples without additional information. To achieve this, we first build a URDF\nmodel of a robotic hand with a single actuator for each joint. We then leverage\npre-trained deep vision models to extract the 3D pose of the hand from RGB\nvideos. Next, using state-of-the-art reinforcement learning algorithms for\nmotion imitation (namely, proximal policy optimization and soft actor-critic),\nwe train a policy to reproduce the movement extracted from the demonstrations.\nWe identify the optimal set of hyperparameters for imitation based on a\nreference motion. Finally, we demonstrate the generalizability of our approach\nby testing it on six different tasks, corresponding to fingerspelled letters.\nOur results show that our approach is able to successfully imitate these\nfine-grained movements without additional information, highlighting its\npotential for real-world applications in robotics.\n",
        "published": "2022",
        "authors": [
            "Federico Tavella",
            "Aphrodite Galata",
            "Angelo Cangelosi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.05629v2",
        "title": "Leveraging Large (Visual) Language Models for Robot 3D Scene\n  Understanding",
        "abstract": "  Abstract semantic 3D scene understanding is a problem of critical importance\nin robotics. As robots still lack the common-sense knowledge about household\nobjects and locations of an average human, we investigate the use of\npre-trained language models to impart common sense for scene understanding. We\nintroduce and compare a wide range of scene classification paradigms that\nleverage language only (zero-shot, embedding-based, and structured-language) or\nvision and language (zero-shot and fine-tuned). We find that the best\napproaches in both categories yield $\\sim 70\\%$ room classification accuracy,\nexceeding the performance of pure-vision and graph classifiers. We also find\nsuch methods demonstrate notable generalization and transfer capabilities\nstemming from their use of language.\n",
        "published": "2022",
        "authors": [
            "William Chen",
            "Siyi Hu",
            "Rajat Talak",
            "Luca Carlone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.03112v3",
        "title": "A New Path: Scaling Vision-and-Language Navigation with Synthetic\n  Instructions and Imitation Learning",
        "abstract": "  Recent studies in Vision-and-Language Navigation (VLN) train RL agents to\nexecute natural-language navigation instructions in photorealistic\nenvironments, as a step towards robots that can follow human instructions.\nHowever, given the scarcity of human instruction data and limited diversity in\nthe training environments, these agents still struggle with complex language\ngrounding and spatial language understanding. Pretraining on large text and\nimage-text datasets from the web has been extensively explored but the\nimprovements are limited. We investigate large-scale augmentation with\nsynthetic instructions. We take 500+ indoor environments captured in\ndensely-sampled 360 degree panoramas, construct navigation trajectories through\nthese panoramas, and generate a visually-grounded instruction for each\ntrajectory using Marky, a high-quality multilingual navigation instruction\ngenerator. We also synthesize image observations from novel viewpoints using an\nimage-to-image GAN. The resulting dataset of 4.2M instruction-trajectory pairs\nis two orders of magnitude larger than existing human-annotated datasets, and\ncontains a wider variety of environments and viewpoints. To efficiently\nleverage data at this scale, we train a simple transformer agent with imitation\nlearning. On the challenging RxR dataset, our approach outperforms all existing\nRL agents, improving the state-of-the-art NDTW from 71.1 to 79.1 in seen\nenvironments, and from 64.6 to 66.8 in unseen test environments. Our work\npoints to a new path to improving instruction-following agents, emphasizing\nlarge-scale imitation learning and the development of synthetic instruction\ngeneration capabilities.\n",
        "published": "2022",
        "authors": [
            "Aishwarya Kamath",
            "Peter Anderson",
            "Su Wang",
            "Jing Yu Koh",
            "Alexander Ku",
            "Austin Waters",
            "Yinfei Yang",
            "Jason Baldridge",
            "Zarana Parekh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.15818v1",
        "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic\n  Control",
        "abstract": "  We study how vision-language models trained on Internet-scale data can be\nincorporated directly into end-to-end robotic control to boost generalization\nand enable emergent semantic reasoning. Our goal is to enable a single\nend-to-end trained model to both learn to map robot observations to actions and\nenjoy the benefits of large-scale pretraining on language and vision-language\ndata from the web. To this end, we propose to co-fine-tune state-of-the-art\nvision-language models on both robotic trajectory data and Internet-scale\nvision-language tasks, such as visual question answering. In contrast to other\napproaches, we propose a simple, general recipe to achieve this goal: in order\nto fit both natural language responses and robotic actions into the same\nformat, we express the actions as text tokens and incorporate them directly\ninto the training set of the model in the same way as natural language tokens.\nWe refer to such category of models as vision-language-action models (VLA) and\ninstantiate an example of such a model, which we call RT-2. Our extensive\nevaluation (6k evaluation trials) shows that our approach leads to performant\nrobotic policies and enables RT-2 to obtain a range of emergent capabilities\nfrom Internet-scale training. This includes significantly improved\ngeneralization to novel objects, the ability to interpret commands not present\nin the robot training data (such as placing an object onto a particular number\nor icon), and the ability to perform rudimentary reasoning in response to user\ncommands (such as picking up the smallest or largest object, or the one closest\nto another object). We further show that incorporating chain of thought\nreasoning allows RT-2 to perform multi-stage semantic reasoning, for example\nfiguring out which object to pick up for use as an improvised hammer (a rock),\nor which type of drink is best suited for someone who is tired (an energy\ndrink).\n",
        "published": "2023",
        "authors": [
            "Anthony Brohan",
            "Noah Brown",
            "Justice Carbajal",
            "Yevgen Chebotar",
            "Xi Chen",
            "Krzysztof Choromanski",
            "Tianli Ding",
            "Danny Driess",
            "Avinava Dubey",
            "Chelsea Finn",
            "Pete Florence",
            "Chuyuan Fu",
            "Montse Gonzalez Arenas",
            "Keerthana Gopalakrishnan",
            "Kehang Han",
            "Karol Hausman",
            "Alexander Herzog",
            "Jasmine Hsu",
            "Brian Ichter",
            "Alex Irpan",
            "Nikhil Joshi",
            "Ryan Julian",
            "Dmitry Kalashnikov",
            "Yuheng Kuang",
            "Isabel Leal",
            "Lisa Lee",
            "Tsang-Wei Edward Lee",
            "Sergey Levine",
            "Yao Lu",
            "Henryk Michalewski",
            "Igor Mordatch",
            "Karl Pertsch",
            "Kanishka Rao",
            "Krista Reymann",
            "Michael Ryoo",
            "Grecia Salazar",
            "Pannag Sanketi",
            "Pierre Sermanet",
            "Jaspiar Singh",
            "Anikait Singh",
            "Radu Soricut",
            "Huong Tran",
            "Vincent Vanhoucke",
            "Quan Vuong",
            "Ayzaan Wahid",
            "Stefan Welker",
            "Paul Wohlhart",
            "Jialin Wu",
            "Fei Xia",
            "Ted Xiao",
            "Peng Xu",
            "Sichun Xu",
            "Tianhe Yu",
            "Brianna Zitkovich"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.01361v1",
        "title": "GenSim: Generating Robotic Simulation Tasks via Large Language Models",
        "abstract": "  Collecting large amounts of real-world interaction data to train general\nrobotic policies is often prohibitively expensive, thus motivating the use of\nsimulation data. However, existing methods for data generation have generally\nfocused on scene-level diversity (e.g., object instances and poses) rather than\ntask-level diversity, due to the human effort required to come up with and\nverify novel tasks. This has made it challenging for policies trained on\nsimulation data to demonstrate significant task-level generalization. In this\npaper, we propose to automatically generate rich simulation environments and\nexpert demonstrations by exploiting a large language models' (LLM) grounding\nand coding ability. Our approach, dubbed GenSim, has two modes: goal-directed\ngeneration, wherein a target task is given to the LLM and the LLM proposes a\ntask curriculum to solve the target task, and exploratory generation, wherein\nthe LLM bootstraps from previous tasks and iteratively proposes novel tasks\nthat would be helpful in solving more complex tasks. We use GPT4 to expand the\nexisting benchmark by ten times to over 100 tasks, on which we conduct\nsupervised finetuning and evaluate several LLMs including finetuned GPTs and\nCode Llama on code generation for robotic simulation tasks. Furthermore, we\nobserve that LLMs-generated simulation programs can enhance task-level\ngeneralization significantly when used for multitask policy training. We\nfurther find that with minimal sim-to-real adaptation, the multitask policies\npretrained on GPT4-generated simulation tasks exhibit stronger transfer to\nunseen long-horizon tasks in the real world and outperform baselines by 25%.\nSee the project website (https://liruiw.github.io/gensim) for code, demos, and\nvideos.\n",
        "published": "2023",
        "authors": [
            "Lirui Wang",
            "Yiyang Ling",
            "Zhecheng Yuan",
            "Mohit Shridhar",
            "Chen Bao",
            "Yuzhe Qin",
            "Bailin Wang",
            "Huazhe Xu",
            "Xiaolong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.02264v1",
        "title": "Generalizable Long-Horizon Manipulations with Large Language Models",
        "abstract": "  This work introduces a framework harnessing the capabilities of Large\nLanguage Models (LLMs) to generate primitive task conditions for generalizable\nlong-horizon manipulations with novel objects and unseen tasks. These task\nconditions serve as guides for the generation and adjustment of Dynamic\nMovement Primitives (DMP) trajectories for long-horizon task execution. We\nfurther create a challenging robotic manipulation task suite based on Pybullet\nfor long-horizon task evaluation. Extensive experiments in both simulated and\nreal-world environments demonstrate the effectiveness of our framework on both\nfamiliar tasks involving new objects and novel but related tasks, highlighting\nthe potential of LLMs in enhancing robotic system versatility and adaptability.\nProject website: https://object814.github.io/Task-Condition-With-LLM/\n",
        "published": "2023",
        "authors": [
            "Haoyu Zhou",
            "Mingyu Ding",
            "Weikun Peng",
            "Masayoshi Tomizuka",
            "Lin Shao",
            "Chuang Gan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.07334v1",
        "title": "Relative Distributed Formation and Obstacle Avoidance with Multi-agent\n  Reinforcement Learning",
        "abstract": "  Multi-agent formation as well as obstacle avoidance is one of the most\nactively studied topics in the field of multi-agent systems. Although some\nclassic controllers like model predictive control (MPC) and fuzzy control\nachieve a certain measure of success, most of them require precise global\ninformation which is not accessible in harsh environments. On the other hand,\nsome reinforcement learning (RL) based approaches adopt the leader-follower\nstructure to organize different agents' behaviors, which sacrifices the\ncollaboration between agents thus suffering from bottlenecks in maneuverability\nand robustness. In this paper, we propose a distributed formation and obstacle\navoidance method based on multi-agent reinforcement learning (MARL). Agents in\nour system only utilize local and relative information to make decisions and\ncontrol themselves distributively. Agent in the multi-agent system will\nreorganize themselves into a new topology quickly in case that any of them is\ndisconnected. Our method achieves better performance regarding formation error,\nformation convergence rate and on-par success rate of obstacle avoidance\ncompared with baselines (both classic control methods and another RL-based\nmethod). The feasibility of our method is verified by both simulation and\nhardware implementation with Ackermann-steering vehicles.\n",
        "published": "2021",
        "authors": [
            "Yuzi Yan",
            "Xiaoxiang Li",
            "Xinyou Qiu",
            "Jiantao Qiu",
            "Jian Wang",
            "Yu Wang",
            "Yuan Shen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.09537v1",
        "title": "QuadSwarm: A Modular Multi-Quadrotor Simulator for Deep Reinforcement\n  Learning with Direct Thrust Control",
        "abstract": "  Reinforcement learning (RL) has shown promise in creating robust policies for\nrobotics tasks. However, contemporary RL algorithms are data-hungry, often\nrequiring billions of environment transitions to train successful policies.\nThis necessitates the use of fast and highly-parallelizable simulators. In\naddition to speed, such simulators need to model the physics of the robots and\ntheir interaction with the environment to a level acceptable for transferring\npolicies learned in simulation to reality. We present QuadSwarm, a fast,\nreliable simulator for research in single and multi-robot RL for quadrotors\nthat addresses both issues. QuadSwarm, with fast forward-dynamics propagation\ndecoupled from rendering, is designed to be highly parallelizable such that\nthroughput scales linearly with additional compute. It provides multiple\ncomponents tailored toward multi-robot RL, including diverse training\nscenarios, and provides domain randomization to facilitate the development and\nsim2real transfer of multi-quadrotor control policies. Initial experiments\nsuggest that QuadSwarm achieves over 48,500 simulation samples per second (SPS)\non a single quadrotor and over 62,000 SPS on eight quadrotors on a 16-core CPU.\nThe code can be found in https://github.com/Zhehui-Huang/quad-swarm-rl.\n",
        "published": "2023",
        "authors": [
            "Zhehui Huang",
            "Sumeet Batra",
            "Tao Chen",
            "Rahul Krupani",
            "Tushar Kumar",
            "Artem Molchanov",
            "Aleksei Petrenko",
            "James A. Preiss",
            "Zhaojing Yang",
            "Gaurav S. Sukhatme"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.08112v1",
        "title": "Optimizing Industrial HVAC Systems with Hierarchical Reinforcement\n  Learning",
        "abstract": "  Reinforcement learning (RL) techniques have been developed to optimize\nindustrial cooling systems, offering substantial energy savings compared to\ntraditional heuristic policies. A major challenge in industrial control\ninvolves learning behaviors that are feasible in the real world due to\nmachinery constraints. For example, certain actions can only be executed every\nfew hours while other actions can be taken more frequently. Without extensive\nreward engineering and experimentation, an RL agent may not learn realistic\noperation of machinery. To address this, we use hierarchical reinforcement\nlearning with multiple agents that control subsets of actions according to\ntheir operation time scales. Our hierarchical approach achieves energy savings\nover existing baselines while maintaining constraints such as operating\nchillers within safe bounds in a simulated HVAC control environment.\n",
        "published": "2022",
        "authors": [
            "William Wong",
            "Praneet Dutta",
            "Octavian Voicu",
            "Yuri Chervonyi",
            "Cosmin Paduraru",
            "Jerry Luo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.13132v1",
        "title": "Hierarchical Needs-driven Agent Learning Systems: From Deep\n  Reinforcement Learning To Diverse Strategies",
        "abstract": "  The needs describe the necessities for a system to survive and evolve, which\narouses an agent to action toward a goal, giving purpose and direction to\nbehavior. Based on Maslow hierarchy of needs, an agent needs to satisfy a\ncertain amount of needs at the current level as a condition to arise at the\nnext stage -- upgrade and evolution. Especially, Deep Reinforcement Learning\n(DAL) can help AI agents (like robots) organize and optimize their behaviors\nand strategies to develop diverse Strategies based on their current state and\nneeds (expected utilities or rewards). This paper introduces the new\nhierarchical needs-driven Learning systems based on DAL and investigates the\nimplementation in the single-robot with a novel approach termed Bayesian Soft\nActor-Critic (BSAC). Then, we extend this topic to the Multi-Agent systems\n(MAS), discussing the potential research fields and directions.\n",
        "published": "2023",
        "authors": [
            "Qin Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.16641v1",
        "title": "A Hierarchical Game-Theoretic Decision-Making for Cooperative\n  Multi-Agent Systems Under the Presence of Adversarial Agents",
        "abstract": "  Underlying relationships among Multi-Agent Systems (MAS) in hazardous\nscenarios can be represented as Game-theoretic models. This paper proposes a\nnew hierarchical network-based model called Game-theoretic Utility Tree (GUT),\nwhich decomposes high-level strategies into executable low-level actions for\ncooperative MAS decisions. It combines with a new payoff measure based on agent\nneeds for real-time strategy games. We present an Explore game domain, where we\nmeasure the performance of MAS achieving tasks from the perspective of\nbalancing the success probability and system costs. We evaluate the GUT\napproach against state-of-the-art methods that greedily rely on rewards of the\ncomposite actions. Conclusive results on extensive numerical simulations\nindicate that GUT can organize more complex relationships among MAS\ncooperation, helping the group achieve challenging tasks with lower costs and\nhigher winning rates. Furthermore, we demonstrated the applicability of the GUT\nusing the simulator-hardware testbed - Robotarium. The performances verified\nthe effectiveness of the GUT in the real robot application and validated that\nthe GUT could effectively organize MAS cooperation strategies, helping the\ngroup with fewer advantages achieve higher performance.\n",
        "published": "2023",
        "authors": [
            "Qin Yang",
            "Ramviyas Parasuraman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.05893v1",
        "title": "Learning to Team-Based Navigation: A Review of Deep Reinforcement\n  Learning Techniques for Multi-Agent Pathfinding",
        "abstract": "  Multi-agent pathfinding (MAPF) is a critical field in many large-scale\nrobotic applications, often being the fundamental step in multi-agent systems.\nThe increasing complexity of MAPF in complex and crowded environments, however,\ncritically diminishes the effectiveness of existing solutions. In contrast to\nother studies that have either presented a general overview of the recent\nadvancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL)\nwithin multi-agent system settings independently, our work presented in this\nreview paper focuses on highlighting the integration of DRL-based approaches in\nMAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions\nby addressing the lack of unified evaluation metrics and providing\ncomprehensive clarification on these metrics. Finally, our paper discusses the\npotential of model-based DRL as a promising future direction and provides its\nrequired foundational understanding to address current challenges in MAPF. Our\nobjective is to assist readers in gaining insight into the current research\ndirection, providing unified metrics for comparing different MAPF algorithms\nand expanding their knowledge of model-based DRL to address the existing\nchallenges in MAPF.\n",
        "published": "2023",
        "authors": [
            "Jaehoon Chung",
            "Jamil Fayyad",
            "Younes Al Younes",
            "Homayoun Najjaran"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.00063v1",
        "title": "Safe multi-agent motion planning under uncertainty for drones using\n  filtered reinforcement learning",
        "abstract": "  We consider the problem of safe multi-agent motion planning for drones in\nuncertain, cluttered workspaces. For this problem, we present a tractable\nmotion planner that builds upon the strengths of reinforcement learning and\nconstrained-control-based trajectory planning. First, we use single-agent\nreinforcement learning to learn motion plans from data that reach the target\nbut may not be collision-free. Next, we use a convex optimization, chance\nconstraints, and set-based methods for constrained control to ensure safety,\ndespite the uncertainty in the workspace, agent motion, and sensing. The\nproposed approach can handle state and control constraints on the agents, and\nenforce collision avoidance among themselves and with static obstacles in the\nworkspace with high probability. The proposed approach yields a safe, real-time\nimplementable, multi-agent motion planner that is simpler to train than methods\nbased solely on learning. Numerical simulations and experiments show the\nefficacy of the approach.\n",
        "published": "2023",
        "authors": [
            "Sleiman Safaoui",
            "Abraham P. Vinod",
            "Ankush Chakrabarty",
            "Rien Quirynen",
            "Nobuyuki Yoshikawa",
            "Stefano Di Cairano"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.09461v1",
        "title": "Modelling resource allocation in uncertain system environment through\n  deep reinforcement learning",
        "abstract": "  Reinforcement Learning has applications in field of mechatronics, robotics,\nand other resource-constrained control system. Problem of resource allocation\nis primarily solved using traditional predefined techniques and modern deep\nlearning methods. The drawback of predefined and most deep learning methods for\nresource allocation is failing to meet the requirements in cases of uncertain\nsystem environment. We can approach problem of resource allocation in uncertain\nsystem environment alongside following certain criteria using deep\nreinforcement learning. Also, reinforcement learning has ability for adapting\nto new uncertain environment for prolonged period of time. The paper provides a\ndetailed comparative analysis on various deep reinforcement learning methods by\napplying different components to modify architecture of reinforcement learning\nwith use of noisy layers, prioritized replay, bagging, duelling networks, and\nother related combination to obtain improvement in terms of performance and\nreduction of computational cost. The paper identifies problem of resource\nallocation in uncertain environment could be effectively solved using Noisy\nBagging duelling double deep Q network achieving efficiency of 97.7% by\nmaximizing reward with significant exploration in given simulated environment\nfor resource allocation.\n",
        "published": "2021",
        "authors": [
            "Neel Gandhi",
            "Shakti Mishra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.08857v1",
        "title": "SEIHAI: A Sample-efficient Hierarchical AI for the MineRL Competition",
        "abstract": "  The MineRL competition is designed for the development of reinforcement\nlearning and imitation learning algorithms that can efficiently leverage human\ndemonstrations to drastically reduce the number of environment interactions\nneeded to solve the complex \\emph{ObtainDiamond} task with sparse rewards. To\naddress the challenge, in this paper, we present \\textbf{SEIHAI}, a\n\\textbf{S}ample-\\textbf{e}ff\\textbf{i}cient \\textbf{H}ierarchical \\textbf{AI},\nthat fully takes advantage of the human demonstrations and the task structure.\nSpecifically, we split the task into several sequentially dependent subtasks,\nand train a suitable agent for each subtask using reinforcement learning and\nimitation learning. We further design a scheduler to select different agents\nfor different subtasks automatically. SEIHAI takes the first place in the\npreliminary and final of the NeurIPS-2020 MineRL competition.\n",
        "published": "2021",
        "authors": [
            "Hangyu Mao",
            "Chao Wang",
            "Xiaotian Hao",
            "Yihuan Mao",
            "Yiming Lu",
            "Chengjie Wu",
            "Jianye Hao",
            "Dong Li",
            "Pingzhong Tang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.15030v2",
        "title": "Solving Disjunctive Temporal Networks with Uncertainty under Restricted\n  Time-Based Controllability using Tree Search and Graph Neural Networks",
        "abstract": "  Planning under uncertainty is an area of interest in artificial intelligence.\nWe present a novel approach based on tree search and graph machine learning for\nthe scheduling problem known as Disjunctive Temporal Networks with Uncertainty\n(DTNU). Dynamic Controllability (DC) of DTNUs seeks a reactive scheduling\nstrategy to satisfy temporal constraints in response to uncontrollable action\ndurations. We introduce new semantics for reactive scheduling: Time-based\nDynamic Controllability (TDC) and a restricted subset of TDC, R-TDC. We design\na tree search algorithm to determine whether or not a DTNU is R-TDC. Moreover,\nwe leverage a graph neural network as a heuristic for tree search guidance.\nFinally, we conduct experiments on a known benchmark on which we show R-TDC to\nretain significant completeness with regard to DC, while being faster to prove.\nThis results in the tree search processing fifty percent more DTNU problems in\nR-TDC than the state-of-the-art DC solver does in DC with the same time budget.\nWe also observe that graph neural network search guidance leads to substantial\nperformance gains on benchmarks of more complex DTNUs, with up to eleven times\nmore problems solved than the baseline tree search.\n",
        "published": "2022",
        "authors": [
            "Kevin Osanlou",
            "Jeremy Frank",
            "Andrei Bursuc",
            "Tristan Cazenave",
            "Eric Jacopin",
            "Christophe Guettier",
            "J. Benton"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.04193v1",
        "title": "A Strategy-Oriented Bayesian Soft Actor-Critic Model",
        "abstract": "  Adopting reasonable strategies is challenging but crucial for an intelligent\nagent with limited resources working in hazardous, unstructured, and dynamic\nenvironments to improve the system's utility, decrease the overall cost, and\nincrease mission success probability. This paper proposes a novel hierarchical\nstrategy decomposition approach based on the Bayesian chain rule to separate an\nintricate policy into several simple sub-policies and organize their\nrelationships as Bayesian strategy networks (BSN). We integrate this approach\ninto the state-of-the-art DRL method -- soft actor-critic (SAC) and build the\ncorresponding Bayesian soft actor-critic (BSAC) model by organizing several\nsub-policies as a joint policy. We compare the proposed BSAC method with the\nSAC and other state-of-the-art approaches such as TD3, DDPG, and PPO on the\nstandard continuous control benchmarks -- Hopper-v2, Walker2d-v2, and\nHumanoid-v2 -- in MuJoCo with the OpenAI Gym environment. The results\ndemonstrate that the promising potential of the BSAC method significantly\nimproves training efficiency.\n",
        "published": "2023",
        "authors": [
            "Qin Yang",
            "Ramviyas Parasuraman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1701.01329v1",
        "title": "Generating Focussed Molecule Libraries for Drug Discovery with Recurrent\n  Neural Networks",
        "abstract": "  In de novo drug design, computational strategies are used to generate novel\nmolecules with good affinity to the desired biological target. In this work, we\nshow that recurrent neural networks can be trained as generative models for\nmolecular structures, similar to statistical language models in natural\nlanguage processing. We demonstrate that the properties of the generated\nmolecules correlate very well with the properties of the molecules used to\ntrain the model. In order to enrich libraries with molecules active towards a\ngiven biological target, we propose to fine-tune the model with small sets of\nmolecules, which are known to be active against that target.\n  Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test\nmolecules that medicinal chemists designed, whereas against Plasmodium\nfalciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled\nwith a scoring function, our model can perform the complete de novo drug design\ncycle to generate large sets of novel molecules for drug discovery.\n",
        "published": "2017",
        "authors": [
            "Marwin H. S. Segler",
            "Thierry Kogej",
            "Christian Tyrchan",
            "Mark P. Waller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.02114v4",
        "title": "Bounding and Counting Linear Regions of Deep Neural Networks",
        "abstract": "  We investigate the complexity of deep neural networks (DNN) that represent\npiecewise linear (PWL) functions. In particular, we study the number of linear\nregions, i.e. pieces, that a PWL function represented by a DNN can attain, both\ntheoretically and empirically. We present (i) tighter upper and lower bounds\nfor the maximum number of linear regions on rectifier networks, which are exact\nfor inputs of dimension one; (ii) a first upper bound for multi-layer maxout\nnetworks; and (iii) a first method to perform exact enumeration or counting of\nthe number of regions by modeling the DNN with a mixed-integer linear\nformulation. These bounds come from leveraging the dimension of the space\ndefining each linear region. The results also indicate that a deep rectifier\nnetwork can only have more linear regions than every shallow counterpart with\nsame number of neurons if that number exceeds the dimension of the input.\n",
        "published": "2017",
        "authors": [
            "Thiago Serra",
            "Christian Tjandraatmadja",
            "Srikumar Ramalingam"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.03143v1",
        "title": "Comparing heterogeneous entities using artificial neural networks of\n  trainable weighted structural components and machine-learned activation\n  functions",
        "abstract": "  To compare entities of differing types and structural components, the\nartificial neural network paradigm was used to cross-compare structural\ncomponents between heterogeneous documents. Trainable weighted structural\ncomponents were input into machine-learned activation functions of the neurons.\nThe model was used for matching news articles and videos, where the inputs and\nactivation functions respectively consisted of term vectors and cosine\nsimilarity measures between the weighted structural components. The model was\ntested with different weights, achieving as high as 59.2% accuracy for matching\nvideos to news articles. A mobile application user interface for recommending\nrelated videos for news articles was developed to demonstrate consumer value,\nincluding its potential usefulness for cross-selling products from unrelated\ncategories.\n",
        "published": "2018",
        "authors": [
            "Artit Wangperawong",
            "Kettip Kriangchaivech",
            "Austin Lanari",
            "Supui Lam",
            "Panthong Wangperawong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.06672v1",
        "title": "Detecting Irregular Patterns in IoT Streaming Data for Fall Detection",
        "abstract": "  Detecting patterns in real time streaming data has been an interesting and\nchallenging data analytics problem. With the proliferation of a variety of\nsensor devices, real-time analytics of data from the Internet of Things (IoT)\nto learn regular and irregular patterns has become an important machine\nlearning problem to enable predictive analytics for automated notification and\ndecision support. In this work, we address the problem of learning an irregular\nhuman activity pattern, fall, from streaming IoT data from wearable sensors. We\npresent a deep neural network model for detecting fall based on accelerometer\ndata giving 98.75 percent accuracy using an online physical activity monitoring\ndataset called \"MobiAct\", which was published by Vavoulas et al. The initial\nmodel was developed using IBM Watson studio and then later transferred and\ndeployed on IBM Cloud with the streaming analytics service supported by IBM\nStreams for monitoring real-time IoT data. We also present the systems\narchitecture of the real-time fall detection framework that we intend to use\nwith mbientlabs wearable health monitoring sensors for real time patient\nmonitoring at retirement homes or rehabilitation clinics.\n",
        "published": "2018",
        "authors": [
            "Sazia Mahfuz",
            "Haruna Isah",
            "Farhana Zulkernine",
            "Peter Nicholls"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.00016v1",
        "title": "Network Parameter Learning Using Nonlinear Transforms, Local\n  Representation Goals and Local Propagation Constraints",
        "abstract": "  In this paper, we introduce a novel concept for learning of the parameters in\na neural network. Our idea is grounded on modeling a learning problem that\naddresses a trade-off between (i) satisfying local objectives at each node and\n(ii) achieving desired data propagation through the network under (iii) local\npropagation constraints. We consider two types of nonlinear transforms which\ndescribe the network representations. One of the nonlinear transforms serves as\nactivation function. The other one enables a locally adjusted, deviation\ncorrective components to be included in the update of the network weights in\norder to enable attaining target specific representations at the last network\nnode. Our learning principle not only provides insight into the understanding\nand the interpretation of the learning dynamics, but it offers theoretical\nguarantees over decoupled and parallel parameter estimation strategy that\nenables learning in synchronous and asynchronous mode. Numerical experiments\nvalidate the potential of our approach on image recognition task. The\npreliminary results show advantages in comparison to the state-of-the-art\nmethods, w.r.t. the learning time and the network size while having competitive\nrecognition accuracy.\n",
        "published": "2019",
        "authors": [
            "Dimche Kostadinov",
            "Behrooz Razdehi",
            "Slava Voloshynovskiy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.13490v4",
        "title": "A Neurocomputational Account of Flexible Goal-directed Cognition and\n  Consciousness: The Goal-Aligning Representation Internal Manipulation Theory\n  (GARIM)",
        "abstract": "  Goal-directed manipulation of representations is a key element of human\nflexible behaviour, while consciousness is often related to several aspects of\nhigher-order cognition and human flexibility. Currently these two phenomena are\nonly partially integrated (e.g., see Neurorepresentationalism) and this (a)\nlimits our understanding of neuro-computational processes that lead conscious\nstates to produce flexible goal-directed behaviours, (b) prevents a\ncomputational formalisation of conscious goal-directed manipulations of\nrepresentations occurring in the brain, and (c) inhibits the exploitation of\nthis knowledge for modelling and technological purposes. Addressing these\nissues, here we extend our `three-component theory of flexible cognition' by\nproposing the `Goal-Aligning Representations Internal Manipulation' (GARIM)\ntheory of conscious and flexible goal-directed cognition. The central idea of\nthe theory is that conscious states support the active manipulation of\ngoal-relevant internal representations (e.g., of world states, objects, and\naction sequences) to make them more aligned with the pursued goals. This leads\nto the generation of the knowledge which is necessary to face novel\nsituations/goals, thus increasing the flexibility of goal-directed behaviours.\nThe GARIM theory integrates key aspects of the main theories of consciousness\ninto the functional neuro-computational framework of goal-directed behaviour.\nMoreover, it takes into account the subjective sensation of agency that\naccompanies conscious goal-directed processes (`GARIM agency'). The proposal\nhas also implications for experimental studies on consciousness and clinical\naspects of conscious goal-directed behaviour. Finally, the GARIM theory benefit\ntechnological fields such as autonomous robotics and machine learning (e.g.,\nthe manipulation process may describe the operations performed by systems based\non transformers).\n",
        "published": "2019",
        "authors": [
            "Giovanni Granato",
            "Gianluca Baldassarre"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.08467v2",
        "title": "GoTube: Scalable Stochastic Verification of Continuous-Depth Models",
        "abstract": "  We introduce a new stochastic verification algorithm that formally quantifies\nthe behavioral robustness of any time-continuous process formulated as a\ncontinuous-depth model. Our algorithm solves a set of global optimization (Go)\nproblems over a given time horizon to construct a tight enclosure (Tube) of the\nset of all process executions starting from a ball of initial states. We call\nour algorithm GoTube. Through its construction, GoTube ensures that the\nbounding tube is conservative up to a desired probability and up to a desired\ntightness. GoTube is implemented in JAX and optimized to scale to complex\ncontinuous-depth neural network models. Compared to advanced reachability\nanalysis tools for time-continuous neural networks, GoTube does not accumulate\noverapproximation errors between time steps and avoids the infamous wrapping\neffect inherent in symbolic techniques. We show that GoTube substantially\noutperforms state-of-the-art verification tools in terms of the size of the\ninitial ball, speed, time-horizon, task completion, and scalability on a large\nset of experiments. GoTube is stable and sets the state-of-the-art in terms of\nits ability to scale to time horizons well beyond what has been previously\npossible.\n",
        "published": "2021",
        "authors": [
            "Sophie Gruenbacher",
            "Mathias Lechner",
            "Ramin Hasani",
            "Daniela Rus",
            "Thomas A. Henzinger",
            "Scott Smolka",
            "Radu Grosu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1611.04488v6",
        "title": "Generative Models and Model Criticism via Optimized Maximum Mean\n  Discrepancy",
        "abstract": "  We propose a method to optimize the representation and distinguishability of\nsamples from two probability distributions, by maximizing the estimated power\nof a statistical test based on the maximum mean discrepancy (MMD). This\noptimized MMD is applied to the setting of unsupervised learning by generative\nadversarial networks (GAN), in which a model attempts to generate realistic\nsamples, and a discriminator attempts to tell these apart from data samples. In\nthis context, the MMD may be used in two roles: first, as a discriminator,\neither directly on the samples, or on features of the samples. Second, the MMD\ncan be used to evaluate the performance of a generative model, by testing the\nmodel's samples against a reference data set. In the latter role, the optimized\nMMD is particularly helpful, as it gives an interpretable indication of how the\nmodel and data distributions differ, even in cases where individual model\nsamples are not easily distinguished either by eye or by classifier.\n",
        "published": "2016",
        "authors": [
            "Danica J. Sutherland",
            "Hsiao-Yu Tung",
            "Heiko Strathmann",
            "Soumyajit De",
            "Aaditya Ramdas",
            "Alex Smola",
            "Arthur Gretton"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.00076v3",
        "title": "News Session-Based Recommendations using Deep Neural Networks",
        "abstract": "  News recommender systems are aimed to personalize users experiences and help\nthem to discover relevant articles from a large and dynamic search space.\nTherefore, news domain is a challenging scenario for recommendations, due to\nits sparse user profiling, fast growing number of items, accelerated item's\nvalue decay, and users preferences dynamic shift. Some promising results have\nbeen recently achieved by the usage of Deep Learning techniques on Recommender\nSystems, specially for item's feature extraction and for session-based\nrecommendations with Recurrent Neural Networks. In this paper, it is proposed\nan instantiation of the CHAMELEON -- a Deep Learning Meta-Architecture for News\nRecommender Systems. This architecture is composed of two modules, the first\nresponsible to learn news articles representations, based on their text and\nmetadata, and the second module aimed to provide session-based recommendations\nusing Recurrent Neural Networks. The recommendation task addressed in this work\nis next-item prediction for users sessions: \"what is the next most likely\narticle a user might read in a session?\" Users sessions context is leveraged by\nthe architecture to provide additional information in such extreme cold-start\nscenario of news recommendation. Users' behavior and item features are both\nmerged in an hybrid recommendation approach. A temporal offline evaluation\nmethod is also proposed as a complementary contribution, for a more realistic\nevaluation of such task, considering dynamic factors that affect global\nreadership interests like popularity, recency, and seasonality. Experiments\nwith an extensive number of session-based recommendation methods were performed\nand the proposed instantiation of CHAMELEON meta-architecture obtained a\nsignificant relative improvement in top-n accuracy and ranking metrics (10% on\nHit Rate and 13% on MRR) over the best benchmark methods.\n",
        "published": "2018",
        "authors": [
            "Gabriel de Souza P. Moreira",
            "Felipe Ferreira",
            "Adilson Marques da Cunha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.07961v2",
        "title": "Guiding Inferences in Connection Tableau by Recurrent Neural Networks",
        "abstract": "  We present a dataset and experiments on applying recurrent neural networks\n(RNNs) for guiding clause selection in the connection tableau proof calculus.\nThe RNN encodes a sequence of literals from the current branch of the partial\nproof tree to a hidden vector state; using it, the system selects a clause for\nextending the proof tree. The training data and learning setup are described,\nand the results are discussed and compared with state of the art using gradient\nboosted trees. Additionally, we perform a conjecturing experiment in which the\nRNN does not just select an existing clause, but completely constructs the next\ntableau goal.\n",
        "published": "2019",
        "authors": [
            "Bartosz Piotrowski",
            "Josef Urban"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.09264v2",
        "title": "Visualizing Representational Dynamics with Multidimensional Scaling\n  Alignment",
        "abstract": "  Representational similarity analysis (RSA) has been shown to be an effective\nframework to characterize brain-activity profiles and deep neural network\nactivations as representational geometry by computing the pairwise distances of\nthe response patterns as a representational dissimilarity matrix (RDM).\nHowever, how to properly analyze and visualize the representational geometry as\ndynamics over the time course from stimulus onset to offset is not well\nunderstood. In this work, we formulated the pipeline to understand\nrepresentational dynamics with RDM movies and Procrustes-aligned\nMultidimensional Scaling (pMDS), and applied it to neural recording of monkey\nIT cortex. Our results suggest that the the multidimensional scaling alignment\ncan genuinely capture the dynamics of the category-specific representation\nspaces with multiple visualization possibilities, and that object\ncategorization may be hierarchical, multi-staged, and oscillatory (or\nrecurrent).\n",
        "published": "2019",
        "authors": [
            "Baihan Lin",
            "Marieke Mur",
            "Tim Kietzmann",
            "Nikolaus Kriegeskorte"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.02982v2",
        "title": "DRLViz: Understanding Decisions and Memory in Deep Reinforcement\n  Learning",
        "abstract": "  We present DRLViz, a visual analytics interface to interpret the internal\nmemory of an agent (e.g. a robot) trained using deep reinforcement learning.\nThis memory is composed of large temporal vectors updated when the agent moves\nin an environment and is not trivial to understand due to the number of\ndimensions, dependencies to past vectors, spatial/temporal correlations, and\nco-correlation between dimensions. It is often referred to as a black box as\nonly inputs (images) and outputs (actions) are intelligible for humans. Using\nDRLViz, experts are assisted to interpret decisions using memory reduction\ninteractions, and to investigate the role of parts of the memory when errors\nhave been made (e.g. wrong direction). We report on DRLViz applied in the\ncontext of video games simulators (ViZDoom) for a navigation scenario with item\ngathering tasks. We also report on experts evaluation using DRLViz, and\napplicability of DRLViz to other scenarios and navigation problems beyond\nsimulation games, as well as its contribution to black box models\ninterpretability and explainability in the field of visual analytics.\n",
        "published": "2019",
        "authors": [
            "Theo Jaunet",
            "Romain Vuillemot",
            "Christian Wolf"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.08874v3",
        "title": "Local and Global Explanations of Agent Behavior: Integrating Strategy\n  Summaries with Saliency Maps",
        "abstract": "  With advances in reinforcement learning (RL), agents are now being developed\nin high-stakes application domains such as healthcare and transportation.\nExplaining the behavior of these agents is challenging, as the environments in\nwhich they act have large state spaces, and their decision-making can be\naffected by delayed rewards, making it difficult to analyze their behavior. To\naddress this problem, several approaches have been developed. Some approaches\nattempt to convey the $\\textit{global}$ behavior of the agent, describing the\nactions it takes in different states. Other approaches devised $\\textit{local}$\nexplanations which provide information regarding the agent's decision-making in\na particular state. In this paper, we combine global and local explanation\nmethods, and evaluate their joint and separate contributions, providing (to the\nbest of our knowledge) the first user study of combined local and global\nexplanations for RL agents. Specifically, we augment strategy summaries that\nextract important trajectories of states from simulations of the agent with\nsaliency maps which show what information the agent attends to. Our results\nshow that the choice of what states to include in the summary (global\ninformation) strongly affects people's understanding of agents: participants\nshown summaries that included important states significantly outperformed\nparticipants who were presented with agent behavior in a randomly set of chosen\nworld-states. We find mixed results with respect to augmenting demonstrations\nwith saliency maps (local information), as the addition of saliency maps did\nnot significantly improve performance in most cases. However, we do find some\nevidence that saliency maps can help users better understand what information\nthe agent relies on in its decision making, suggesting avenues for future work\nthat can further improve explanations of RL agents.\n",
        "published": "2020",
        "authors": [
            "Tobias Huber",
            "Katharina Weitz",
            "Elisabeth Andr\u00e9",
            "Ofra Amir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.12570v4",
        "title": "AntBO: Towards Real-World Automated Antibody Design with Combinatorial\n  Bayesian Optimisation",
        "abstract": "  Antibodies are canonically Y-shaped multimeric proteins capable of highly\nspecific molecular recognition. The CDRH3 region located at the tip of variable\nchains of an antibody dominates antigen-binding specificity. Therefore, it is a\npriority to design optimal antigen-specific CDRH3 regions to develop\ntherapeutic antibodies. However, the combinatorial nature of CDRH3 sequence\nspace makes it impossible to search for an optimal binding sequence\nexhaustively and efficiently using computational approaches. Here, we present\n\\texttt{AntBO}: a combinatorial Bayesian optimisation framework enabling\nefficient \\textit{in silico} design of the CDRH3 region. Ideally, antibodies\nare expected to have high target specificity and developability. We introduce a\nCDRH3 trust region that restricts the search to sequences with favourable\ndevelopability scores to achieve this goal. For benchmarking, \\texttt{AntBO}\nuses the \\texttt{Absolut!} software suite as a black-box oracle to score the\ntarget specificity and affinity of designed antibodies \\textit{in silico} in an\nunconstrained fashion~\\citep{robert2021one}. The experiments performed for\n$159$ discretised antigens used in \\texttt{Absolut!} demonstrate the benefit of\n\\texttt{AntBO} in designing CDRH3 regions with diverse biophysical properties.\nIn under $200$ calls to black-box oracle, \\texttt{AntBO} can suggest antibody\nsequences that outperform the best binding sequence drawn from 6.9 million\nexperimentally obtained CDRH3s and a commonly used genetic algorithm baseline.\nAdditionally, \\texttt{AntBO} finds very-high affinity CDRH3 sequences in only\n38 protein designs whilst requiring no domain knowledge. We conclude\n\\texttt{AntBO} brings automated antibody design methods closer to what is\npractically viable for in vitro experimentation.\n",
        "published": "2022",
        "authors": [
            "Asif Khan",
            "Alexander I. Cowen-Rivers",
            "Antoine Grosnit",
            "Derrick-Goh-Xin Deik",
            "Philippe A. Robert",
            "Victor Greiff",
            "Eva Smorodina",
            "Puneet Rawat",
            "Kamil Dreczkowski",
            "Rahmad Akbar",
            "Rasul Tutunov",
            "Dany Bou-Ammar",
            "Jun Wang",
            "Amos Storkey",
            "Haitham Bou-Ammar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.09250v1",
        "title": "Symmetry-Based Representations for Artificial and Biological General\n  Intelligence",
        "abstract": "  Biological intelligence is remarkable in its ability to produce complex\nbehaviour in many diverse situations through data efficient, generalisable and\ntransferable skill acquisition. It is believed that learning \"good\" sensory\nrepresentations is important for enabling this, however there is little\nagreement as to what a good representation should look like. In this review\narticle we are going to argue that symmetry transformations are a fundamental\nprinciple that can guide our search for what makes a good representation. The\nidea that there exist transformations (symmetries) that affect some aspects of\nthe system but not others, and their relationship to conserved quantities has\nbecome central in modern physics, resulting in a more unified theoretical\nframework and even ability to predict the existence of new particles. Recently,\nsymmetries have started to gain prominence in machine learning too, resulting\nin more data efficient and generalisable algorithms that can mimic some of the\ncomplex behaviours produced by biological intelligence. Finally, first\ndemonstrations of the importance of symmetry transformations for representation\nlearning in the brain are starting to arise in neuroscience. Taken together,\nthe overwhelming positive effect that symmetries bring to these disciplines\nsuggest that they may be an important general framework that determines the\nstructure of the universe, constrains the nature of natural tasks and\nconsequently shapes both biological and artificial intelligence.\n",
        "published": "2022",
        "authors": [
            "Irina Higgins",
            "S\u00e9bastien Racani\u00e8re",
            "Danilo Rezende"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.11889v1",
        "title": "Insights From the NeurIPS 2021 NetHack Challenge",
        "abstract": "  In this report, we summarize the takeaways from the first NeurIPS 2021\nNetHack Challenge. Participants were tasked with developing a program or agent\nthat can win (i.e., 'ascend' in) the popular dungeon-crawler game of NetHack by\ninteracting with the NetHack Learning Environment (NLE), a scalable,\nprocedurally generated, and challenging Gym environment for reinforcement\nlearning (RL). The challenge showcased community-driven progress in AI with\nmany diverse approaches significantly beating the previously best results on\nNetHack. Furthermore, it served as a direct comparison between neural (e.g.,\ndeep RL) and symbolic AI, as well as hybrid systems, demonstrating that on\nNetHack symbolic bots currently outperform deep RL by a large margin. Lastly,\nno agent got close to winning the game, illustrating NetHack's suitability as a\nlong-term benchmark for AI research.\n",
        "published": "2022",
        "authors": [
            "Eric Hambro",
            "Sharada Mohanty",
            "Dmitrii Babaev",
            "Minwoo Byeon",
            "Dipam Chakraborty",
            "Edward Grefenstette",
            "Minqi Jiang",
            "Daejin Jo",
            "Anssi Kanervisto",
            "Jongmin Kim",
            "Sungwoong Kim",
            "Robert Kirk",
            "Vitaly Kurin",
            "Heinrich K\u00fcttler",
            "Taehwon Kwon",
            "Donghoon Lee",
            "Vegard Mella",
            "Nantas Nardelli",
            "Ivan Nazarov",
            "Nikita Ovsov",
            "Jack Parker-Holder",
            "Roberta Raileanu",
            "Karolis Ramanauskas",
            "Tim Rockt\u00e4schel",
            "Danielle Rothermel",
            "Mikayel Samvelyan",
            "Dmitry Sorokin",
            "Maciej Sypetkowski",
            "Micha\u0142 Sypetkowski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.13913v3",
        "title": "SpeqNets: Sparsity-aware Permutation-equivariant Graph Networks",
        "abstract": "  While (message-passing) graph neural networks have clear limitations in\napproximating permutation-equivariant functions over graphs or general\nrelational data, more expressive, higher-order graph neural networks do not\nscale to large graphs. They either operate on $k$-order tensors or consider all\n$k$-node subgraphs, implying an exponential dependence on $k$ in memory\nrequirements, and do not adapt to the sparsity of the graph. By introducing new\nheuristics for the graph isomorphism problem, we devise a class of universal,\npermutation-equivariant graph networks, which, unlike previous architectures,\noffer a fine-grained control between expressivity and scalability and adapt to\nthe sparsity of the graph. These architectures lead to vastly reduced\ncomputation times compared to standard higher-order graph networks in the\nsupervised node- and graph-level classification and regression regime while\nsignificantly improving over standard graph neural network and graph kernel\narchitectures in terms of predictive performance.\n",
        "published": "2022",
        "authors": [
            "Christopher Morris",
            "Gaurav Rattan",
            "Sandra Kiefer",
            "Siamak Ravanbakhsh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.11168v3",
        "title": "Ordered Subgraph Aggregation Networks",
        "abstract": "  Numerous subgraph-enhanced graph neural networks (GNNs) have emerged\nrecently, provably boosting the expressive power of standard (message-passing)\nGNNs. However, there is a limited understanding of how these approaches relate\nto each other and to the Weisfeiler-Leman hierarchy. Moreover, current\napproaches either use all subgraphs of a given size, sample them uniformly at\nrandom, or use hand-crafted heuristics instead of learning to select subgraphs\nin a data-driven manner. Here, we offer a unified way to study such\narchitectures by introducing a theoretical framework and extending the known\nexpressivity results of subgraph-enhanced GNNs. Concretely, we show that\nincreasing subgraph size always increases the expressive power and develop a\nbetter understanding of their limitations by relating them to the established\n$k\\text{-}\\mathsf{WL}$ hierarchy. In addition, we explore different approaches\nfor learning to sample subgraphs using recent methods for backpropagating\nthrough complex discrete probability distributions. Empirically, we study the\npredictive performance of different subgraph-enhanced GNNs, showing that our\ndata-driven architectures increase prediction accuracy on standard benchmark\ndatasets compared to non-data-driven subgraph-enhanced graph neural networks\nwhile reducing computation time.\n",
        "published": "2022",
        "authors": [
            "Chendi Qian",
            "Gaurav Rattan",
            "Floris Geerts",
            "Christopher Morris",
            "Mathias Niepert"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.08549v3",
        "title": "Automatic Emergency Dust-Free solution on-board International Space\n  Station with Bi-GRU (AED-ISS)",
        "abstract": "  With a rising attention for the issue of PM2.5 or PM0.3, particulate matters\nhave become not only a potential threat to both the environment and human, but\nalso a harming existence to instruments onboard International Space Station\n(ISS). Our team is aiming to relate various concentration of particulate\nmatters to magnetic fields, humidity, acceleration, temperature, pressure and\nCO2 concentration. Our goal is to establish an early warning system (EWS),\nwhich is able to forecast the levels of particulate matters and provides ample\nreaction time for astronauts to protect their instruments in some experiments\nor increase the accuracy of the measurements; In addition, the constructed\nmodel can be further developed into a prototype of a remote-sensing smoke alarm\nfor applications related to fires. In this article, we will implement the\nBi-GRU (Bidirectional Gated Recurrent Unit) algorithms that collect data for\npast 90 minutes and predict the levels of particulates which over 2.5\nmicrometer per 0.1 liter for the next 1 minute, which is classified as an early\nwarning\n",
        "published": "2022",
        "authors": [
            "Po-Han Hou",
            "Wei-Chih Lin",
            "Hong-Chun Hou",
            "Yu-Hao Huang",
            "Jih-Hong Shue"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.16512v2",
        "title": "From Complexity to Clarity: Analytical Expressions of Deep Neural\n  Network Weights via Clifford's Geometric Algebra and Convexity",
        "abstract": "  In this paper, we introduce a novel analysis of neural networks based on\ngeometric (Clifford) algebra and convex optimization. We show that optimal\nweights of deep ReLU neural networks are given by the wedge product of training\nsamples when trained with standard regularized loss. Furthermore, the training\nproblem reduces to convex optimization over wedge product features, which\nencode the geometric structure of the training dataset. This structure is given\nin terms of signed volumes of triangles and parallelotopes generated by data\nvectors. The convex problem finds a small subset of samples via $\\ell_1$\nregularization to discover only relevant wedge product features. Our analysis\nprovides a novel perspective on the inner workings of deep neural networks and\nsheds light on the role of the hidden layers.\n",
        "published": "2023",
        "authors": [
            "Mert Pilanci"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.10603v1",
        "title": "Exploring the Power of Graph Neural Networks in Solving Linear\n  Optimization Problems",
        "abstract": "  Recently, machine learning, particularly message-passing graph neural\nnetworks (MPNNs), has gained traction in enhancing exact optimization\nalgorithms. For example, MPNNs speed up solving mixed-integer optimization\nproblems by imitating computational intensive heuristics like strong branching,\nwhich entails solving multiple linear optimization problems (LPs). Despite the\nempirical success, the reasons behind MPNNs' effectiveness in emulating linear\noptimization remain largely unclear. Here, we show that MPNNs can simulate\nstandard interior-point methods for LPs, explaining their practical success.\nFurthermore, we highlight how MPNNs can serve as a lightweight proxy for\nsolving LPs, adapting to a given problem instance distribution. Empirically, we\nshow that MPNNs solve LP relaxations of standard combinatorial optimization\nproblems close to optimality, often surpassing conventional solvers and\ncompeting approaches in solving time.\n",
        "published": "2023",
        "authors": [
            "Chendi Qian",
            "Didier Ch\u00e9telat",
            "Christopher Morris"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.03654v1",
        "title": "Efficient Inverse Design Optimization through Multi-fidelity\n  Simulations, Machine Learning, and Search Space Reduction Strategies",
        "abstract": "  This paper introduces a methodology designed to augment the inverse design\noptimization process in scenarios constrained by limited compute, through the\nstrategic synergy of multi-fidelity evaluations, machine learning models, and\noptimization algorithms. The proposed methodology is analyzed on two distinct\nengineering inverse design problems: airfoil inverse design and the scalar\nfield reconstruction problem. It leverages a machine learning model trained\nwith low-fidelity simulation data, in each optimization cycle, thereby\nproficiently predicting a target variable and discerning whether a\nhigh-fidelity simulation is necessitated, which notably conserves computational\nresources. Additionally, the machine learning model is strategically deployed\nprior to optimization to reduce the search space, thereby further accelerating\nconvergence toward the optimal solution. The methodology has been employed to\nenhance two optimization algorithms, namely Differential Evolution and Particle\nSwarm Optimization. Comparative analyses illustrate performance improvements\nacross both algorithms. Notably, this method is adeptly adaptable across any\ninverse design application, facilitating a harmonious synergy between a\nrepresentative low-fidelity machine learning model, and high-fidelity\nsimulation, and can be seamlessly applied across any variety of\npopulation-based optimization algorithms.\n",
        "published": "2023",
        "authors": [
            "Luka Grbcic",
            "Juliane M\u00fcller",
            "Wibe Albert de Jong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.17216v1",
        "title": "SparseProp: Efficient Event-Based Simulation and Training of Sparse\n  Recurrent Spiking Neural Networks",
        "abstract": "  Spiking Neural Networks (SNNs) are biologically-inspired models that are\ncapable of processing information in streams of action potentials. However,\nsimulating and training SNNs is computationally expensive due to the need to\nsolve large systems of coupled differential equations. In this paper, we\nintroduce SparseProp, a novel event-based algorithm for simulating and training\nsparse SNNs. Our algorithm reduces the computational cost of both the forward\nand backward pass operations from O(N) to O(log(N)) per network spike, thereby\nenabling numerically exact simulations of large spiking networks and their\nefficient training using backpropagation through time. By leveraging the\nsparsity of the network, SparseProp eliminates the need to iterate through all\nneurons at each spike, employing efficient state updates instead. We\ndemonstrate the efficacy of SparseProp across several classical\nintegrate-and-fire neuron models, including a simulation of a sparse SNN with\none million LIF neurons. This results in a speed-up exceeding four orders of\nmagnitude relative to previous event-based implementations. Our work provides\nan efficient and exact solution for training large-scale spiking neural\nnetworks and opens up new possibilities for building more sophisticated\nbrain-inspired models.\n",
        "published": "2023",
        "authors": [
            "Rainer Engelken"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.08167v2",
        "title": "On The Robustness of a Neural Network",
        "abstract": "  With the development of neural networks based machine learning and their\nusage in mission critical applications, voices are rising against the\n\\textit{black box} aspect of neural networks as it becomes crucial to\nunderstand their limits and capabilities. With the rise of neuromorphic\nhardware, it is even more critical to understand how a neural network, as a\ndistributed system, tolerates the failures of its computing nodes, neurons, and\nits communication channels, synapses. Experimentally assessing the robustness\nof neural networks involves the quixotic venture of testing all the possible\nfailures, on all the possible inputs, which ultimately hits a combinatorial\nexplosion for the first, and the impossibility to gather all the possible\ninputs for the second.\n  In this paper, we prove an upper bound on the expected error of the output\nwhen a subset of neurons crashes. This bound involves dependencies on the\nnetwork parameters that can be seen as being too pessimistic in the average\ncase. It involves a polynomial dependency on the Lipschitz coefficient of the\nneurons activation function, and an exponential dependency on the depth of the\nlayer where a failure occurs. We back up our theoretical results with\nexperiments illustrating the extent to which our prediction matches the\ndependencies between the network parameters and robustness. Our results show\nthat the robustness of neural networks to the average crash can be estimated\nwithout the need to neither test the network on all failure configurations, nor\naccess the training set used to train the network, both of which are\npractically impossible requirements.\n",
        "published": "2017",
        "authors": [
            "El Mahdi El Mhamdi",
            "Rachid Guerraoui",
            "Sebastien Rouault"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.05136v5",
        "title": "Deep Rewiring: Training very sparse deep networks",
        "abstract": "  Neuromorphic hardware tends to pose limits on the connectivity of deep\nnetworks that one can run on them. But also generic hardware and software\nimplementations of deep learning run more efficiently for sparse networks.\nSeveral methods exist for pruning connections of a neural network after it was\ntrained without connectivity constraints. We present an algorithm, DEEP R, that\nenables us to train directly a sparsely connected neural network. DEEP R\nautomatically rewires the network during supervised training so that\nconnections are there where they are most needed for the task, while its total\nnumber is all the time strictly bounded. We demonstrate that DEEP R can be used\nto train very sparse feedforward and recurrent neural networks on standard\nbenchmark tasks with just a minor loss in performance. DEEP R is based on a\nrigorous theoretical foundation that views rewiring as stochastic sampling of\nnetwork configurations from a posterior.\n",
        "published": "2017",
        "authors": [
            "Guillaume Bellec",
            "David Kappel",
            "Wolfgang Maass",
            "Robert Legenstein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.03710v1",
        "title": "Recurrent Auto-Encoder Model for Large-Scale Industrial Sensor Signal\n  Analysis",
        "abstract": "  Recurrent auto-encoder model summarises sequential data through an encoder\nstructure into a fixed-length vector and then reconstructs the original\nsequence through the decoder structure. The summarised vector can be used to\nrepresent time series features. In this paper, we propose relaxing the\ndimensionality of the decoder output so that it performs partial\nreconstruction. The fixed-length vector therefore represents features in the\nselected dimensions only. In addition, we propose using rolling fixed window\napproach to generate training samples from unbounded time series data. The\nchange of time series features over time can be summarised as a smooth\ntrajectory path. The fixed-length vectors are further analysed using additional\nvisualisation and unsupervised clustering techniques. The proposed method can\nbe applied in large-scale industrial processes for sensors signal analysis\npurpose, where clusters of the vector representations can reflect the operating\nstates of the industrial system.\n",
        "published": "2018",
        "authors": [
            "Timothy Wong",
            "Zhiyuan Luo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.05870v1",
        "title": "ZhuSuan: A Library for Bayesian Deep Learning",
        "abstract": "  In this paper we introduce ZhuSuan, a python probabilistic programming\nlibrary for Bayesian deep learning, which conjoins the complimentary advantages\nof Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike\nexisting deep learning libraries, which are mainly designed for deterministic\nneural networks and supervised tasks, ZhuSuan is featured for its deep root\ninto Bayesian inference, thus supporting various kinds of probabilistic models,\nincluding both the traditional hierarchical Bayesian models and recent deep\ngenerative models. We use running examples to illustrate the probabilistic\nprogramming on ZhuSuan, including Bayesian logistic regression, variational\nauto-encoders, deep sigmoid belief networks and Bayesian recurrent neural\nnetworks.\n",
        "published": "2017",
        "authors": [
            "Jiaxin Shi",
            "Jianfei Chen",
            "Jun Zhu",
            "Shengyang Sun",
            "Yucen Luo",
            "Yihong Gu",
            "Yuhao Zhou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.07695v4",
        "title": "Multitask Learning on Graph Neural Networks: Learning Multiple Graph\n  Centrality Measures with a Unified Network",
        "abstract": "  The application of deep learning to symbolic domains remains an active\nresearch endeavour. Graph neural networks (GNN), consisting of trained neural\nmodules which can be arranged in different topologies at run time, are sound\nalternatives to tackle relational problems which lend themselves to graph\nrepresentations. In this paper, we show that GNNs are capable of multitask\nlearning, which can be naturally enforced by training the model to refine a\nsingle set of multidimensional embeddings $\\in \\mathbb{R}^d$ and decode them\ninto multiple outputs by connecting MLPs at the end of the pipeline. We\ndemonstrate the multitask learning capability of the model in the relevant\nrelational problem of estimating network centrality measures, focusing\nprimarily on producing rankings based on these measures, i.e. is vertex $v_1$\nmore central than vertex $v_2$ given centrality $c$?. We then show that a GNN\ncan be trained to develop a \\emph{lingua franca} of vertex embeddings from\nwhich all relevant information about any of the trained centrality measures can\nbe decoded. The proposed model achieves $89\\%$ accuracy on a test dataset of\nrandom instances with up to 128 vertices and is shown to generalise to larger\nproblem sizes. The model is also shown to obtain reasonable accuracy on a\ndataset of real world instances with up to 4k vertices, vastly surpassing the\nsizes of the largest instances with which the model was trained ($n=128$).\nFinally, we believe that our contributions attest to the potential of GNNs in\nsymbolic domains in general and in relational learning in particular.\n",
        "published": "2018",
        "authors": [
            "Pedro H. C. Avelar",
            "Henrique Lemos",
            "Marcelo O. R. Prates",
            "Luis Lamb"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.02079v1",
        "title": "Optimizing Software Effort Estimation Models Using Firefly Algorithm",
        "abstract": "  Software development effort estimation is considered a fundamental task for\nsoftware development life cycle as well as for managing project cost, time and\nquality. Therefore, accurate estimation is a substantial factor in projects\nsuccess and reducing the risks. In recent years, software effort estimation has\nreceived a considerable amount of attention from researchers and became a\nchallenge for software industry. In the last two decades, many researchers and\npractitioners proposed statistical and machine learning-based models for\nsoftware effort estimation. In this work, Firefly Algorithm is proposed as a\nmetaheuristic optimization method for optimizing the parameters of three\nCOCOMO-based models. These models include the basic COCOMO model and other two\nmodels proposed in the literature as extensions of the basic COCOMO model. The\ndeveloped estimation models are evaluated using different evaluation metrics.\nExperimental results show high accuracy and significant error minimization of\nFirefly Algorithm over other metaheuristic optimization algorithms including\nGenetic Algorithms and Particle Swarm Optimization.\n",
        "published": "2019",
        "authors": [
            "Nazeeh Ghatasheh",
            "Hossam Faris",
            "Ibrahim Aljarah",
            "Rizik M. H. Al-Sayyed"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1611.07422v1",
        "title": "Deep Learning Approximation for Stochastic Control Problems",
        "abstract": "  Many real world stochastic control problems suffer from the \"curse of\ndimensionality\". To overcome this difficulty, we develop a deep learning\napproach that directly solves high-dimensional stochastic control problems\nbased on Monte-Carlo sampling. We approximate the time-dependent controls as\nfeedforward neural networks and stack these networks together through model\ndynamics. The objective function for the control problem plays the role of the\nloss function for the deep neural network. We test this approach using examples\nfrom the areas of optimal trading and energy storage. Our results suggest that\nthe algorithm presented here achieves satisfactory accuracy and at the same\ntime, can handle rather high dimensional problems.\n",
        "published": "2016",
        "authors": [
            "Jiequn Han",
            "Weinan E"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.07916v1",
        "title": "Deep-learning of Parametric Partial Differential Equations from Sparse\n  and Noisy Data",
        "abstract": "  Data-driven methods have recently made great progress in the discovery of\npartial differential equations (PDEs) from spatial-temporal data. However,\nseveral challenges remain to be solved, including sparse noisy data, incomplete\ncandidate library, and spatially- or temporally-varying coefficients. In this\nwork, a new framework, which combines neural network, genetic algorithm and\nadaptive methods, is put forward to address all of these challenges\nsimultaneously. In the framework, a trained neural network is utilized to\ncalculate derivatives and generate a large amount of meta-data, which solves\nthe problem of sparse noisy data. Next, genetic algorithm is utilized to\ndiscover the form of PDEs and corresponding coefficients with an incomplete\ncandidate library. Finally, a two-step adaptive method is introduced to\ndiscover parametric PDEs with spatially- or temporally-varying coefficients. In\nthis method, the structure of a parametric PDE is first discovered, and then\nthe general form of varying coefficients is identified. The proposed algorithm\nis tested on the Burgers equation, the convection-diffusion equation, the wave\nequation, and the KdV equation. The results demonstrate that this method is\nrobust to sparse and noisy data, and is able to discover parametric PDEs with\nan incomplete candidate library.\n",
        "published": "2020",
        "authors": [
            "Hao Xu",
            "Dongxiao Zhang",
            "Junsheng Zeng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.06373v1",
        "title": "Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct\n  Feedback Alignment",
        "abstract": "  The scaling hypothesis motivates the expansion of models past trillions of\nparameters as a path towards better performance. Recent significant\ndevelopments, such as GPT-3, have been driven by this conjecture. However, as\nmodels scale-up, training them efficiently with backpropagation becomes\ndifficult. Because model, pipeline, and data parallelism distribute parameters\nand gradients over compute nodes, communication is challenging to orchestrate:\nthis is a bottleneck to further scaling. In this work, we argue that\nalternative training methods can mitigate these issues, and can inform the\ndesign of extreme-scale training hardware. Indeed, using a synaptically\nasymmetric method with a parallelizable backward pass, such as Direct Feedback\nAlignement, communication needs are drastically reduced. We present a photonic\naccelerator for Direct Feedback Alignment, able to compute random projections\nwith trillions of parameters. We demonstrate our system on benchmark tasks,\nusing both fully-connected and graph convolutional networks. Our hardware is\nthe first architecture-agnostic photonic co-processor for training neural\nnetworks. This is a significant step towards building scalable hardware, able\nto go beyond backpropagation, and opening new avenues for deep learning.\n",
        "published": "2020",
        "authors": [
            "Julien Launay",
            "Iacopo Poli",
            "Kilian M\u00fcller",
            "Gustave Pariente",
            "Igor Carron",
            "Laurent Daudet",
            "Florent Krzakala",
            "Sylvain Gigan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1206.6462v1",
        "title": "Learning Object Arrangements in 3D Scenes using Human Context",
        "abstract": "  We consider the problem of learning object arrangements in a 3D scene. The\nkey idea here is to learn how objects relate to human poses based on their\naffordances, ease of use and reachability. In contrast to modeling\nobject-object relationships, modeling human-object relationships scales\nlinearly in the number of objects. We design appropriate density functions\nbased on 3D spatial features to capture this. We learn the distribution of\nhuman poses in a scene using a variant of the Dirichlet process mixture model\nthat allows sharing of the density function parameters across the same object\ntypes. Then we can reason about arrangements of the objects in the room based\non these meaningful human poses. In our extensive experiments on 20 different\nrooms with a total of 47 objects, our algorithm predicted correct placements\nwith an average error of 1.6 meters from ground truth. In arranging five real\nscenes, it received a score of 4.3/5 compared to 3.7 for the best baseline\nmethod.\n",
        "published": "2012",
        "authors": [
            "Yun Jiang",
            "Marcus Lim",
            "Ashutosh Saxena"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1303.5913v1",
        "title": "A Diffusion Process on Riemannian Manifold for Visual Tracking",
        "abstract": "  Robust visual tracking for long video sequences is a research area that has\nmany important applications. The main challenges include how the target image\ncan be modeled and how this model can be updated. In this paper, we model the\ntarget using a covariance descriptor, as this descriptor is robust to problems\nsuch as pixel-pixel misalignment, pose and illumination changes, that commonly\noccur in visual tracking. We model the changes in the template using a\ngenerative process. We introduce a new dynamical model for the template update\nusing a random walk on the Riemannian manifold where the covariance descriptors\nlie in. This is done using log-transformed space of the manifold to free the\nconstraints imposed inherently by positive semidefinite matrices. Modeling\ntemplate variations and poses kinetics together in the state space enables us\nto jointly quantify the uncertainties relating to the kinematic states and the\ntemplate in a principled way. Finally, the sequential inference of the\nposterior distribution of the kinematic states and the template is done using a\nparticle filter. Our results shows that this principled approach can be robust\nto changes in illumination, poses and spatial affine transformation. In the\nexperiments, our method outperformed the current state-of-the-art algorithm -\nthe incremental Principal Component Analysis method, particularly when a target\nunderwent fast poses changes and also maintained a comparable performance in\nstable target tracking cases.\n",
        "published": "2013",
        "authors": [
            "Marcus Chen",
            "Cham Tat Jen",
            "Pang Sze Kim",
            "Alvina Goh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1605.05045v3",
        "title": "Incremental Robot Learning of New Objects with Fixed Update Time",
        "abstract": "  We consider object recognition in the context of lifelong learning, where a\nrobotic agent learns to discriminate between a growing number of object classes\nas it accumulates experience about the environment. We propose an incremental\nvariant of the Regularized Least Squares for Classification (RLSC) algorithm,\nand exploit its structure to seamlessly add new classes to the learned model.\nThe presented algorithm addresses the problem of having an unbalanced\nproportion of training examples per class, which occurs when new objects are\npresented to the system for the first time.\n  We evaluate our algorithm on both a machine learning benchmark dataset and\ntwo challenging object recognition tasks in a robotic setting. Empirical\nevidence shows that our approach achieves comparable or higher classification\nperformance than its batch counterpart when classes are unbalanced, while being\nsignificantly faster.\n",
        "published": "2016",
        "authors": [
            "Raffaello Camoriano",
            "Giulia Pasquale",
            "Carlo Ciliberto",
            "Lorenzo Natale",
            "Lorenzo Rosasco",
            "Giorgio Metta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.09386v2",
        "title": "A Systematic Comparison of Deep Learning Architectures in an Autonomous\n  Vehicle",
        "abstract": "  Self-driving technology is advancing rapidly --- albeit with significant\nchallenges and limitations. This progress is largely due to recent developments\nin deep learning algorithms. To date, however, there has been no systematic\ncomparison of how different deep learning architectures perform at such tasks,\nor an attempt to determine a correlation between classification performance and\nperformance in an actual vehicle, a potentially critical factor in developing\nself-driving systems. Here, we introduce the first controlled comparison of\nmultiple deep-learning architectures in an end-to-end autonomous driving task\nacross multiple testing conditions. We compared performance, under identical\ndriving conditions, across seven architectures including a fully-connected\nnetwork, a simple 2 layer CNN, AlexNet, VGG-16, Inception-V3, ResNet, and an\nLSTM by assessing the number of laps each model was able to successfully\ncomplete without crashing while traversing an indoor racetrack. We compared\nperformance across models when the conditions exactly matched those in training\nas well as when the local environment and track were configured differently and\nobjects that were not included in the training dataset were placed on the track\nin various positions. In addition, we considered performance using several\ndifferent data types for training and testing including single grayscale and\ncolor frames, and multiple grayscale frames stacked together in sequence. With\nthe exception of a fully-connected network, all models performed reasonably\nwell (around or above 80\\%) and most very well (~95\\%) on at least one input\ntype but with considerable variation across models and inputs. Overall,\nAlexNet, operating on single color frames as input, achieved the best level of\nperformance (100\\% success rate in phase one and 55\\% in phase two) while\nVGG-16 performed well most consistently across image types.\n",
        "published": "2018",
        "authors": [
            "Michael Teti",
            "William Edward Hahn",
            "Shawn Martin",
            "Christopher Teti",
            "Elan Barenholtz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.09266v1",
        "title": "Learning Task-Oriented Grasping for Tool Manipulation from Simulated\n  Self-Supervision",
        "abstract": "  Tool manipulation is vital for facilitating robots to complete challenging\ntask goals. It requires reasoning about the desired effect of the task and thus\nproperly grasping and manipulating the tool to achieve the task. Task-agnostic\ngrasping optimizes for grasp robustness while ignoring crucial task-specific\nconstraints. In this paper, we propose the Task-Oriented Grasping Network\n(TOG-Net) to jointly optimize both task-oriented grasping of a tool and the\nmanipulation policy for that tool. The training process of the model is based\non large-scale simulated self-supervision with procedurally generated tool\nobjects. We perform both simulated and real-world experiments on two tool-based\nmanipulation tasks: sweeping and hammering. Our model achieves overall 71.1%\ntask success rate for sweeping and 80.0% task success rate for hammering.\nSupplementary material is available at: bit.ly/task-oriented-grasp\n",
        "published": "2018",
        "authors": [
            "Kuan Fang",
            "Yuke Zhu",
            "Animesh Garg",
            "Andrey Kurenkov",
            "Viraj Mehta",
            "Li Fei-Fei",
            "Silvio Savarese"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.04742v2",
        "title": "Visual Reinforcement Learning with Imagined Goals",
        "abstract": "  For an autonomous agent to fulfill a wide range of user-specified goals at\ntest time, it must be able to learn broadly applicable and general-purpose\nskill repertoires. Furthermore, to provide the requisite level of generality,\nthese skills must handle raw sensory input such as images. In this paper, we\npropose an algorithm that acquires such general-purpose skills by combining\nunsupervised representation learning and reinforcement learning of\ngoal-conditioned policies. Since the particular goals that might be required at\ntest-time are not known in advance, the agent performs a self-supervised\n\"practice\" phase where it imagines goals and attempts to achieve them. We learn\na visual representation with three distinct purposes: sampling goals for\nself-supervised practice, providing a structured transformation of raw sensory\ninputs, and computing a reward signal for goal reaching. We also propose a\nretroactive goal relabeling scheme to further improve the sample-efficiency of\nour method. Our off-policy algorithm is efficient enough to learn policies that\noperate on raw image observations and goals for a real-world robotic system,\nand substantially outperforms prior techniques.\n",
        "published": "2018",
        "authors": [
            "Ashvin Nair",
            "Vitchyr Pong",
            "Murtaza Dalal",
            "Shikhar Bahl",
            "Steven Lin",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.05597v1",
        "title": "Deep Learning for Semantic Segmentation on Minimal Hardware",
        "abstract": "  Deep learning has revolutionised many fields, but it is still challenging to\ntransfer its success to small mobile robots with minimal hardware.\nSpecifically, some work has been done to this effect in the RoboCup humanoid\nfootball domain, but results that are performant and efficient and still\ngenerally applicable outside of this domain are lacking. We propose an approach\nconceptually different from those taken previously. It is based on semantic\nsegmentation and does achieve these desired properties. In detail, it is being\nable to process full VGA images in real-time on a low-power mobile processor.\nIt can further handle multiple image dimensions without retraining, it does not\nrequire specific domain knowledge for achieving a high frame rate and it is\napplicable on a minimal mobile hardware.\n",
        "published": "2018",
        "authors": [
            "Sander G. van Dijk",
            "Marcus M. Scheunemann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.08241v1",
        "title": "NAVREN-RL: Learning to fly in real environment via end-to-end deep\n  reinforcement learning using monocular images",
        "abstract": "  We present NAVREN-RL, an approach to NAVigate an unmanned aerial vehicle in\nan indoor Real ENvironment via end-to-end reinforcement learning RL. A suitable\nreward function is designed keeping in mind the cost and weight constraints for\nmicro drone with minimum number of sensing modalities. Collection of small\nnumber of expert data and knowledge based data aggregation is integrated into\nthe RL process to aid convergence. Experimentation is carried out on a Parrot\nAR drone in different indoor arenas and the results are compared with other\nbaseline technologies. We demonstrate how the drone successfully avoids\nobstacles and navigates across different arenas.\n",
        "published": "2018",
        "authors": [
            "Malik Aqeel Anwar",
            "Arijit Raychowdhury"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.05599v1",
        "title": "Virtual-to-Real-World Transfer Learning for Robots on Wilderness Trails",
        "abstract": "  Robots hold promise in many scenarios involving outdoor use, such as\nsearch-and-rescue, wildlife management, and collecting data to improve\nenvironment, climate, and weather forecasting. However, autonomous navigation\nof outdoor trails remains a challenging problem. Recent work has sought to\naddress this issue using deep learning. Although this approach has achieved\nstate-of-the-art results, the deep learning paradigm may be limited due to a\nreliance on large amounts of annotated training data. Collecting and curating\ntraining datasets may not be feasible or practical in many situations,\nespecially as trail conditions may change due to seasonal weather variations,\nstorms, and natural erosion. In this paper, we explore an approach to address\nthis issue through virtual-to-real-world transfer learning using a variety of\ndeep learning models trained to classify the direction of a trail in an image.\nOur approach utilizes synthetic data gathered from virtual environments for\nmodel training, bypassing the need to collect a large amount of real images of\nthe outdoors. We validate our approach in three main ways. First, we\ndemonstrate that our models achieve classification accuracies upwards of 95% on\nour synthetic data set. Next, we utilize our classification models in the\ncontrol system of a simulated robot to demonstrate feasibility. Finally, we\nevaluate our models on real-world trail data and demonstrate the potential of\nvirtual-to-real-world transfer learning.\n",
        "published": "2019",
        "authors": [
            "Michael L. Iuzzolino",
            "Michael E. Walker",
            "Daniel Szafir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.09270v1",
        "title": "Challenges in Designing Datasets and Validation for Autonomous Driving",
        "abstract": "  Autonomous driving is getting a lot of attention in the last decade and will\nbe the hot topic at least until the first successful certification of a car\nwith Level 5 autonomy. There are many public datasets in the academic\ncommunity. However, they are far away from what a robust industrial production\nsystem needs. There is a large gap between academic and industrial setting and\na substantial way from a research prototype, built on public datasets, to a\ndeployable solution which is a challenging task. In this paper, we focus on bad\npractices that often happen in the autonomous driving from an industrial\ndeployment perspective. Data design deserves at least the same amount of\nattention as the model design. There is very little attention paid to these\nissues in the scientific community, and we hope this paper encourages better\nformalization of dataset design. More specifically, we focus on the datasets\ndesign and validation scheme for autonomous driving, where we would like to\nhighlight the common problems, wrong assumptions, and steps towards avoiding\nthem, as well as some open problems.\n",
        "published": "2019",
        "authors": [
            "Michal Uricar",
            "David Hurych",
            "Pavel Krizek",
            "Senthil Yogamani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.03466v2",
        "title": "Hierarchical Multi-task Deep Neural Network Architecture for End-to-End\n  Driving",
        "abstract": "  A novel hierarchical Deep Neural Network (DNN) model is presented to address\nthe task of end-to-end driving. The model consists of a master classifier\nnetwork which determines the driving task required from an input stereo image\nand directs said image to one of a set of subservient network regression models\nthat perform inference and output a steering command. These subservient\nnetworks are designed and trained for a specific driving task: straightaway,\nswerve maneuver, tight turn, gradual turn, and chicane. Using this modular\nnetwork strategy allows for two primary advantages: an overall reduction in the\namount of data required to train the complete system, and for model tailoring\nwhere more complex models can be used for more challenging tasks while\nsimplified networks can handle more mundane tasks. It is this latter facet of\nthe model that makes the approach attractive to a number of applications beyond\nthe current vehicle steering strategy.\n",
        "published": "2019",
        "authors": [
            "Jose Solomon",
            "Francois Charette"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.03589v2",
        "title": "NeurAll: Towards a Unified Model for Visual Perception in Automated\n  Driving",
        "abstract": "  Convolutional Neural Networks (CNNs) are successfully used for the important\nautomotive visual perception tasks including object recognition, motion and\ndepth estimation, visual SLAM, etc. However, these tasks are typically\nindependently explored and modeled. In this paper, we propose a joint\nmulti-task network design for learning several tasks simultaneously. Our main\nmotivation is the computational efficiency achieved by sharing the expensive\ninitial convolutional layers between all tasks. Indeed, the main bottleneck in\nautomated driving systems is the limited processing power available on\ndeployment hardware. There is also some evidence for other benefits in\nimproving accuracy for some tasks and easing development effort. It also offers\nscalability to add more tasks leveraging existing features and achieving better\ngeneralization. We survey various CNN based solutions for visual perception\ntasks in automated driving. Then we propose a unified CNN model for the\nimportant tasks and discuss several advanced optimization and architecture\ndesign techniques to improve the baseline model. The paper is partly review and\npartly positional with demonstration of several preliminary results promising\nfor future research. We first demonstrate results of multi-stream learning and\nauxiliary learning which are important ingredients to scale to a large\nmulti-task model. Finally, we implement a two-stream three-task network which\nperforms better in many cases compared to their corresponding single-task\nmodels, while maintaining network size.\n",
        "published": "2019",
        "authors": [
            "Ganesh Sistu",
            "Isabelle Leang",
            "Sumanth Chennupati",
            "Ciaran Hughes",
            "Stefan Milz",
            "Senthil Yogamani",
            "Samir Rawashdeh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.05400v1",
        "title": "Data-Driven Vehicle Trajectory Forecasting",
        "abstract": "  An active area of research is to increase the safety of self-driving\nvehicles. Although safety cannot be guarenteed completely, the capability of a\nvehicle to predict the future trajectories of its surrounding vehicles could\nhelp ensure this notion of safety to a greater deal. We cast the trajectory\nforecast problem in a multi-time step forecasting problem and develop a\nConvolutional Neural Network based approach to learn from trajectory sequences\ngenerated from completely raw dataset in real-time. Results show improvement\nover baselines.\n",
        "published": "2019",
        "authors": [
            "Shayan Jawed",
            "Eya Boumaiza",
            "Josif Grabocka",
            "Lars Schmidt-Thieme"
        ]
    }
]