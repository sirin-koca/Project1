[
    {
        "id": "http://arxiv.org/abs/1902.05542v1",
        "title": "Unsupervised Visuomotor Control through Distributional Planning Networks",
        "abstract": "  While reinforcement learning (RL) has the potential to enable robots to\nautonomously acquire a wide range of skills, in practice, RL usually requires\nmanual, per-task engineering of reward functions, especially in real world\nsettings where aspects of the environment needed to compute progress are not\ndirectly accessible. To enable robots to autonomously learn skills, we instead\nconsider the problem of reinforcement learning without access to rewards. We\naim to learn an unsupervised embedding space under which the robot can measure\nprogress towards a goal for itself. Our approach explicitly optimizes for a\nmetric space under which action sequences that reach a particular state are\noptimal when the goal is the final state reached. This enables learning\neffective and control-centric representations that lead to more autonomous\nreinforcement learning algorithms. Our experiments on three simulated\nenvironments and two real-world manipulation problems show that our method can\nlearn effective goal metrics from unlabeled interaction, and use the learned\ngoal metrics for autonomous reinforcement learning.\n",
        "published": "2019",
        "authors": [
            "Tianhe Yu",
            "Gleb Shevchuk",
            "Dorsa Sadigh",
            "Chelsea Finn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.01907v1",
        "title": "Attacking Vision-based Perception in End-to-End Autonomous Driving\n  Models",
        "abstract": "  Recent advances in machine learning, especially techniques such as deep\nneural networks, are enabling a range of emerging applications. One such\nexample is autonomous driving, which often relies on deep learning for\nperception. However, deep learning-based perception has been shown to be\nvulnerable to a host of subtle adversarial manipulations of images.\nNevertheless, the vast majority of such demonstrations focus on perception that\nis disembodied from end-to-end control. We present novel end-to-end attacks on\nautonomous driving in simulation, using simple physically realizable attacks:\nthe painting of black lines on the road. These attacks target deep neural\nnetwork models for end-to-end autonomous driving control. A systematic\ninvestigation shows that such attacks are easy to engineer, and we describe\nscenarios (e.g., right turns) in which they are highly effective. We define\nseveral objective functions that quantify the success of an attack and develop\ntechniques based on Bayesian Optimization to efficiently traverse the search\nspace of higher dimensional attacks. Additionally, we define a novel class of\nhijacking attacks, where painted lines on the road cause the driver-less car to\nfollow a target path. Through the use of network deconvolution, we provide\ninsights into the successful attacks, which appear to work by mimicking\nactivations of entirely different scenarios. Our code is available at\nhttps://github.com/xz-group/AdverseDrive\n",
        "published": "2019",
        "authors": [
            "Adith Boloor",
            "Karthik Garimella",
            "Xin He",
            "Christopher Gill",
            "Yevgeniy Vorobeychik",
            "Xuan Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.05449v1",
        "title": "MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for\n  Behavior Prediction",
        "abstract": "  Predicting human behavior is a difficult and crucial task required for motion\nplanning. It is challenging in large part due to the highly uncertain and\nmulti-modal set of possible outcomes in real-world domains such as autonomous\ndriving. Beyond single MAP trajectory prediction, obtaining an accurate\nprobability distribution of the future is an area of active interest. We\npresent MultiPath, which leverages a fixed set of future state-sequence anchors\nthat correspond to modes of the trajectory distribution. At inference, our\nmodel predicts a discrete distribution over the anchors and, for each anchor,\nregresses offsets from anchor waypoints along with uncertainties, yielding a\nGaussian mixture at each time step. Our model is efficient, requiring only one\nforward inference pass to obtain multi-modal future distributions, and the\noutput is parametric, allowing compact communication and analytical\nprobabilistic queries. We show on several datasets that our model achieves more\naccurate predictions, and compared to sampling baselines, does so with an order\nof magnitude fewer trajectories.\n",
        "published": "2019",
        "authors": [
            "Yuning Chai",
            "Benjamin Sapp",
            "Mayank Bansal",
            "Dragomir Anguelov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.10190v2",
        "title": "Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic\n  Reinforcement Learning",
        "abstract": "  One of the great promises of robot learning systems is that they will be able\nto learn from their mistakes and continuously adapt to ever-changing\nenvironments. Despite this potential, most of the robot learning systems today\nare deployed as a fixed policy and they are not being adapted after their\ndeployment. Can we efficiently adapt previously learned behaviors to new\nenvironments, objects and percepts in the real world? In this paper, we present\na method and empirical evidence towards a robot learning framework that\nfacilitates continuous adaption. In particular, we demonstrate how to adapt\nvision-based robotic manipulation policies to new variations by fine-tuning via\noff-policy reinforcement learning, including changes in background, object\nshape and appearance, lighting conditions, and robot morphology. Further, this\nadaptation uses less than 0.2% of the data necessary to learn the task from\nscratch. We find that our approach of adapting pre-trained policies leads to\nsubstantial performance gains over the course of fine-tuning, and that\npre-training via RL is essential: training from scratch or adapting from\nsupervised ImageNet features are both unsuccessful with such small amounts of\ndata. We also find that these positive results hold in a limited continual\nlearning setting, in which we repeatedly fine-tune a single lineage of policies\nusing data from a succession of new tasks. Our empirical conclusions are\nconsistently supported by experiments on simulated manipulation tasks, and by\n52 unique fine-tuning experiments on a real robotic grasping system pre-trained\non 580,000 grasps.\n",
        "published": "2020",
        "authors": [
            "Ryan Julian",
            "Benjamin Swanson",
            "Gaurav S. Sukhatme",
            "Sergey Levine",
            "Chelsea Finn",
            "Karol Hausman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.00749v1",
        "title": "Deep Multi-Task Learning for Anomalous Driving Detection Using CAN Bus\n  Scalar Sensor Data",
        "abstract": "  Corner cases are the main bottlenecks when applying Artificial Intelligence\n(AI) systems to safety-critical applications. An AI system should be\nintelligent enough to detect such situations so that system developers can\nprepare for subsequent planning. In this paper, we propose semi-supervised\nanomaly detection considering the imbalance of normal situations. In\nparticular, driving data consists of multiple positive/normal situations (e.g.,\nright turn, going straight), some of which (e.g., U-turn) could be as rare as\nanomalous situations. Existing machine learning based anomaly detection\napproaches do not fare sufficiently well when applied to such imbalanced data.\nIn this paper, we present a novel multi-task learning based approach that\nleverages domain-knowledge (maneuver labels) for anomaly detection in driving\ndata. We evaluate the proposed approach both quantitatively and qualitatively\non 150 hours of real-world driving data and show improved performance over\nbaseline approaches.\n",
        "published": "2019",
        "authors": [
            "Vidyasagar Sadhu",
            "Teruhisa Misu",
            "Dario Pompili"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.05269v1",
        "title": "Influence of Pointing on Learning to Count: A Neuro-Robotics Model",
        "abstract": "  In this paper a neuro-robotics model capable of counting using gestures is\nintroduced. The contribution of gestures to learning to count is tested with\nvarious model and training conditions. Two studies were presented in this\narticle. In the first, we combine different modalities of the robot's neural\nnetwork, in the second, a novel training procedure for it is proposed. The\nmodel is trained with pointing data from an iCub robot simulator. The behaviour\nof the model is in line with that of human children in terms of performance\nchange depending on gesture production.\n",
        "published": "2019",
        "authors": [
            "Leszek Pecyna",
            "Angelo Cangelosi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.05270v2",
        "title": "A Deep Neural Network for Finger Counting and Numerosity Estimation",
        "abstract": "  In this paper, we present neuro-robotics models with a deep artificial neural\nnetwork capable of generating finger counting positions and number estimation.\nWe first train the model in an unsupervised manner where each layer is treated\nas a Restricted Boltzmann Machine or an autoencoder. Such a model is further\ntrained in a supervised way. This type of pre-training is tested on our\nbaseline model and two methods of pre-training are compared. The network is\nextended to produce finger counting positions. The performance in number\nestimation of such an extended model is evaluated. We test the hypothesis if\nthe subitizing process can be obtained by one single model used also for\nestimation of higher numerosities. The results confirm the importance of\nunsupervised training in our enumeration task and show some similarities to\nhuman behaviour in the case of subitizing.\n",
        "published": "2019",
        "authors": [
            "Leszek Pecyna",
            "Angelo Cangelosi",
            "Alessandro Di Nuovo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.10388v2",
        "title": "Higher-Order Function Networks for Learning Composable 3D Object\n  Representations",
        "abstract": "  We present a new approach to 3D object representation where a neural network\nencodes the geometry of an object directly into the weights and biases of a\nsecond 'mapping' network. This mapping network can be used to reconstruct an\nobject by applying its encoded transformation to points randomly sampled from a\nsimple geometric space, such as the unit sphere. We study the effectiveness of\nour method through various experiments on subsets of the ShapeNet dataset. We\nfind that the proposed approach can reconstruct encoded objects with accuracy\nequal to or exceeding state-of-the-art methods with orders of magnitude fewer\nparameters. Our smallest mapping network has only about 7000 parameters and\nshows reconstruction quality on par with state-of-the-art object decoder\narchitectures with millions of parameters. Further experiments on feature\nmixing through the composition of learned functions show that the encoding\ncaptures a meaningful subspace of objects.\n",
        "published": "2019",
        "authors": [
            "Eric Mitchell",
            "Selim Engin",
            "Volkan Isler",
            "Daniel D Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.11025v1",
        "title": "Towards Generalizing Sensorimotor Control Across Weather Conditions",
        "abstract": "  The ability of deep learning models to generalize well across different\nscenarios depends primarily on the quality and quantity of annotated data.\nLabeling large amounts of data for all possible scenarios that a model may\nencounter would not be feasible; if even possible. We propose a framework to\ndeal with limited labeled training data and demonstrate it on the application\nof vision-based vehicle control. We show how limited steering angle data\navailable for only one condition can be transferred to multiple different\nweather scenarios. This is done by leveraging unlabeled images in a\nteacher-student learning paradigm complemented with an image-to-image\ntranslation network. The translation network transfers the images to a new\ndomain, whereas the teacher provides soft supervised targets to train the\nstudent on this domain. Furthermore, we demonstrate how utilization of\nauxiliary networks can reduce the size of a model at inference time, without\naffecting the accuracy. The experiments show that our approach generalizes well\nacross multiple different weather conditions using only ground truth labels\nfrom one domain.\n",
        "published": "2019",
        "authors": [
            "Qadeer Khan",
            "Patrick Wenzel",
            "Daniel Cremers",
            "Laura Leal-Taix\u00e9"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.00219v3",
        "title": "Deep Kinematic Models for Kinematically Feasible Vehicle Trajectory\n  Predictions",
        "abstract": "  Self-driving vehicles (SDVs) hold great potential for improving traffic\nsafety and are poised to positively affect the quality of life of millions of\npeople. To unlock this potential one of the critical aspects of the autonomous\ntechnology is understanding and predicting future movement of vehicles\nsurrounding the SDV. This work presents a deep-learning-based method for\nkinematically feasible motion prediction of such traffic actors. Previous work\ndid not explicitly encode vehicle kinematics and instead relied on the models\nto learn the constraints directly from the data, potentially resulting in\nkinematically infeasible, suboptimal trajectory predictions. To address this\nissue we propose a method that seamlessly combines ideas from the AI with\nphysically grounded vehicle motion models. In this way we employ best of the\nboth worlds, coupling powerful learning models with strong feasibility\nguarantees for their outputs. The proposed approach is general, being\napplicable to any type of learning method. Extensive experiments using deep\nconvnets on real-world data strongly indicate its benefits, outperforming the\nexisting state-of-the-art.\n",
        "published": "2019",
        "authors": [
            "Henggang Cui",
            "Thi Nguyen",
            "Fang-Chieh Chou",
            "Tsung-Han Lin",
            "Jeff Schneider",
            "David Bradley",
            "Nemanja Djuric"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.03918v3",
        "title": "DynaNet: Neural Kalman Dynamical Model for Motion Estimation and\n  Prediction",
        "abstract": "  Dynamical models estimate and predict the temporal evolution of physical\nsystems. State Space Models (SSMs) in particular represent the system dynamics\nwith many desirable properties, such as being able to model uncertainty in both\nthe model and measurements, and optimal (in the Bayesian sense) recursive\nformulations e.g. the Kalman Filter. However, they require significant domain\nknowledge to derive the parametric form and considerable hand-tuning to\ncorrectly set all the parameters. Data driven techniques e.g. Recurrent Neural\nNetworks have emerged as compelling alternatives to SSMs with wide success\nacross a number of challenging tasks, in part due to their ability to extract\nrelevant features from rich inputs. They however lack interpretability and\nrobustness to unseen conditions. In this work, we present DynaNet, a hybrid\ndeep learning and time-varying state-space model which can be trained\nend-to-end. Our neural Kalman dynamical model allows us to exploit the relative\nmerits of each approach. We demonstrate state-of-the-art estimation and\nprediction on a number of physically challenging tasks, including visual\nodometry, sensor fusion for visual-inertial navigation and pendulum control. In\naddition we show how DynaNet can indicate failures through investigation of\nproperties such as the rate of innovation (Kalman Gain).\n",
        "published": "2019",
        "authors": [
            "Changhao Chen",
            "Chris Xiaoxuan Lu",
            "Bing Wang",
            "Niki Trigoni",
            "Andrew Markham"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.09031v1",
        "title": "Generic Tracking and Probabilistic Prediction Framework and Its\n  Application in Autonomous Driving",
        "abstract": "  Accurately tracking and predicting behaviors of surrounding objects are key\nprerequisites for intelligent systems such as autonomous vehicles to achieve\nsafe and high-quality decision making and motion planning. However, there still\nremain challenges for multi-target tracking due to object number fluctuation\nand occlusion. To overcome these challenges, we propose a constrained mixture\nsequential Monte Carlo (CMSMC) method in which a mixture representation is\nincorporated in the estimated posterior distribution to maintain\nmulti-modality. Multiple targets can be tracked simultaneously within a unified\nframework without explicit data association between observations and tracking\ntargets. The framework can incorporate an arbitrary prediction model as the\nimplicit proposal distribution of the CMSMC method. An example in this paper is\na learning-based model for hierarchical time-series prediction, which consists\nof a behavior recognition module and a state evolution module. Both modules in\nthe proposed model are generic and flexible so as to be applied to a class of\ntime-series prediction problems where behaviors can be separated into different\nlevels. Finally, the proposed framework is applied to a numerical case study as\nwell as a task of on-road vehicle tracking, behavior recognition, and\nprediction in highway scenarios. Instead of only focusing on forecasting\ntrajectory of a single entity, we jointly predict continuous motions for\ninteractive entities simultaneously. The proposed approaches are evaluated from\nmultiple aspects, which demonstrate great potential for intelligent vehicular\nsystems and traffic surveillance systems.\n",
        "published": "2019",
        "authors": [
            "Jiachen Li",
            "Wei Zhan",
            "Yeping Hu",
            "Masayoshi Tomizuka"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.10237v1",
        "title": "Generic Vehicle Tracking Framework Capable of Handling Occlusions Based\n  on Modified Mixture Particle Filter",
        "abstract": "  Accurate and robust tracking of surrounding road participants plays an\nimportant role in autonomous driving. However, there is usually no prior\nknowledge of the number of tracking targets due to object emergence, object\ndisappearance and false alarms. To overcome this challenge, we propose a\ngeneric vehicle tracking framework based on modified mixture particle filter,\nwhich can make the number of tracking targets adaptive to real-time\nobservations and track all the vehicles within sensor range simultaneously in a\nuniform architecture without explicit data association. Each object corresponds\nto a mixture component whose distribution is non-parametric and approximated by\nparticle hypotheses. Most tracking approaches employ vehicle kinematic models\nas the prediction model. However, it is hard for these models to make proper\npredictions when sensor measurements are lost or become low quality due to\npartial or complete occlusions. Moreover, these models are incapable of\nforecasting sudden maneuvers. To address these problems, we propose to\nincorporate learning-based behavioral models instead of pure vehicle kinematic\nmodels to realize prediction in the prior update of recursive Bayesian state\nestimation. Two typical driving scenarios including lane keeping and lane\nchange are demonstrated to verify the effectiveness and accuracy of the\nproposed framework as well as the advantages of employing learning-based\nmodels.\n",
        "published": "2018",
        "authors": [
            "Jiachen Li",
            "Wei Zhan",
            "Masayoshi Tomizuka"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.10732v2",
        "title": "Multimodal Trajectory Predictions for Autonomous Driving using Deep\n  Convolutional Networks",
        "abstract": "  Autonomous driving presents one of the largest problems that the robotics and\nartificial intelligence communities are facing at the moment, both in terms of\ndifficulty and potential societal impact. Self-driving vehicles (SDVs) are\nexpected to prevent road accidents and save millions of lives while improving\nthe livelihood and life quality of many more. However, despite large interest\nand a number of industry players working in the autonomous domain, there still\nremains more to be done in order to develop a system capable of operating at a\nlevel comparable to best human drivers. One reason for this is high uncertainty\nof traffic behavior and large number of situations that an SDV may encounter on\nthe roads, making it very difficult to create a fully generalizable system. To\nensure safe and efficient operations, an autonomous vehicle is required to\naccount for this uncertainty and to anticipate a multitude of possible\nbehaviors of traffic actors in its surrounding. We address this critical\nproblem and present a method to predict multiple possible trajectories of\nactors while also estimating their probabilities. The method encodes each\nactor's surrounding context into a raster image, used as input by deep\nconvolutional networks to automatically derive relevant features for the task.\nFollowing extensive offline evaluation and comparison to state-of-the-art\nbaselines, the method was successfully tested on SDVs in closed-course tests.\n",
        "published": "2018",
        "authors": [
            "Henggang Cui",
            "Vladan Radosavljevic",
            "Fang-Chieh Chou",
            "Tsung-Han Lin",
            "Thi Nguyen",
            "Tzu-Kuo Huang",
            "Jeff Schneider",
            "Nemanja Djuric"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.07567v1",
        "title": "Generative One-Shot Learning (GOL): A Semi-Parametric Approach to\n  One-Shot Learning in Autonomous Vision",
        "abstract": "  Highly Autonomous Driving (HAD) systems rely on deep neural networks for the\nvisual perception of the driving environment. Such networks are trained on\nlarge manually annotated databases. In this work, a semi-parametric approach to\none-shot learning is proposed, with the aim of bypassing the manual annotation\nstep required for training perceptions systems used in autonomous driving. The\nproposed generative framework, coined Generative One-Shot Learning (GOL), takes\nas input single one-shot objects, or generic patterns, and a small set of\nso-called regularization samples used to drive the generative process. New\nsynthetic data is generated as Pareto optimal solutions from one-shot objects\nusing a set of generalization functions built into a generalization generator.\nGOL has been evaluated on environment perception challenges encountered in\nautonomous vision.\n",
        "published": "2018",
        "authors": [
            "Sorin Grigorescu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.09395v3",
        "title": "Multi-Step Prediction of Occupancy Grid Maps with Recurrent Neural\n  Networks",
        "abstract": "  We investigate the multi-step prediction of the drivable space, represented\nby Occupancy Grid Maps (OGMs), for autonomous vehicles. Our motivation is that\naccurate multi-step prediction of the drivable space can efficiently improve\npath planning and navigation resulting in safe, comfortable and optimum paths\nin autonomous driving. We train a variety of Recurrent Neural Network (RNN)\nbased architectures on the OGM sequences from the KITTI dataset. The results\ndemonstrate significant improvement of the prediction accuracy using our\nproposed difference learning method, incorporating motion related features,\nover the state of the art. We remove the egomotion from the OGM sequences by\ntransforming them into a common frame. Although in the transformed sequences\nthe KITTI dataset is heavily biased toward static objects, by learning the\ndifference between subsequent OGMs, our proposed method provides accurate\nprediction over both the static and moving objects.\n",
        "published": "2018",
        "authors": [
            "Nima Mohajerin",
            "Mohsen Rohani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.03878v1",
        "title": "Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks",
        "abstract": "  Many robotic applications require the agent to perform long-horizon tasks in\npartially observable environments. In such applications, decision making at any\nstep can depend on observations received far in the past. Hence, being able to\nproperly memorize and utilize the long-term history is crucial. In this work,\nwe propose a novel memory-based policy, named Scene Memory Transformer (SMT).\nThe proposed policy embeds and adds each observation to a memory and uses the\nattention mechanism to exploit spatio-temporal dependencies. This model is\ngeneric and can be efficiently trained with reinforcement learning over long\nepisodes. On a range of visual navigation tasks, SMT demonstrates superior\nperformance to existing reactive and memory-based policies by a margin.\n",
        "published": "2019",
        "authors": [
            "Kuan Fang",
            "Alexander Toshev",
            "Li Fei-Fei",
            "Silvio Savarese"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.11027v5",
        "title": "nuScenes: A multimodal dataset for autonomous driving",
        "abstract": "  Robust detection and tracking of objects is crucial for the deployment of\nautonomous vehicle technology. Image based benchmark datasets have driven\ndevelopment in computer vision tasks such as object detection, tracking and\nsegmentation of agents in the environment. Most autonomous vehicles, however,\ncarry a combination of cameras and range sensors such as lidar and radar. As\nmachine learning based methods for detection and tracking become more\nprevalent, there is a need to train and evaluate such methods on datasets\ncontaining range sensor data along with images. In this work we present\nnuTonomy scenes (nuScenes), the first dataset to carry the full autonomous\nvehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree\nfield of view. nuScenes comprises 1000 scenes, each 20s long and fully\nannotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as\nmany annotations and 100x as many images as the pioneering KITTI dataset. We\ndefine novel 3D detection and tracking metrics. We also provide careful dataset\nanalysis as well as baselines for lidar and image based detection and tracking.\nData, development kit and more information are available online.\n",
        "published": "2019",
        "authors": [
            "Holger Caesar",
            "Varun Bankiti",
            "Alex H. Lang",
            "Sourabh Vora",
            "Venice Erin Liong",
            "Qiang Xu",
            "Anush Krishnan",
            "Yu Pan",
            "Giancarlo Baldan",
            "Oscar Beijbom"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.00438v1",
        "title": "RST-MODNet: Real-time Spatio-temporal Moving Object Detection for\n  Autonomous Driving",
        "abstract": "  Moving Object Detection (MOD) is a critical task for autonomous vehicles as\nmoving objects represent higher collision risk than static ones. The trajectory\nof the ego-vehicle is planned based on the future states of detected moving\nobjects. It is quite challenging as the ego-motion has to be modelled and\ncompensated to be able to understand the motion of the surrounding objects. In\nthis work, we propose a real-time end-to-end CNN architecture for MOD utilizing\nspatio-temporal context to improve robustness. We construct a novel time-aware\narchitecture exploiting temporal motion information embedded within sequential\nimages in addition to explicit motion maps using optical flow images.We\ndemonstrate the impact of our algorithm on KITTI dataset where we obtain an\nimprovement of 8% relative to the baselines. We compare our algorithm with\nstate-of-the-art methods and achieve competitive results on KITTI-Motion\ndataset in terms of accuracy at three times better run-time. The proposed\nalgorithm runs at 23 fps on a standard desktop GPU targeting deployment on\nembedded platforms.\n",
        "published": "2019",
        "authors": [
            "Mohamed Ramzy",
            "Hazem Rashed",
            "Ahmad El Sallab",
            "Senthil Yogamani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.04527v2",
        "title": "Learning Pose Estimation for UAV Autonomous Navigation andLanding Using\n  Visual-Inertial Sensor Data",
        "abstract": "  In this work, we propose a new learning approach for autonomous navigation\nand landing of an Unmanned-Aerial-Vehicle (UAV). We develop a multimodal fusion\nof deep neural architectures for visual-inertial odometry. We train the model\nin an end-to-end fashion to estimate the current vehicle pose from streams of\nvisual and inertial measurements. We first evaluate the accuracy of our\nestimation by comparing the prediction of the model to traditional algorithms\non the publicly available EuRoC MAV dataset. The results illustrate a $25 \\%$\nimprovement in estimation accuracy over the baseline. Finally, we integrate the\narchitecture in the closed-loop flight control system of Airsim - a plugin\nsimulator for Unreal Engine - and we provide simulation results for autonomous\nnavigation and landing.\n",
        "published": "2019",
        "authors": [
            "Francesca Baldini",
            "Animashree Anandkumar",
            "Richard M. Murray"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.05440v1",
        "title": "Self-Driving Car Steering Angle Prediction Based on Image Recognition",
        "abstract": "  Self-driving vehicles have expanded dramatically over the last few years.\nUdacity has release a dataset containing, among other data, a set of images\nwith the steering angle captured during driving. The Udacity challenge aimed to\npredict steering angle based on only the provided images. We explore two\ndifferent models to perform high quality prediction of steering angles based on\nimages using different deep learning techniques including Transfer Learning, 3D\nCNN, LSTM and ResNet. If the Udacity challenge was still ongoing, both of our\nmodels would have placed in the top ten of all entries.\n",
        "published": "2019",
        "authors": [
            "Shuyang Du",
            "Haoli Guo",
            "Andrew Simpson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.11676v2",
        "title": "Deep Learning-based Vehicle Behaviour Prediction For Autonomous Driving\n  Applications: A Review",
        "abstract": "  Behaviour prediction function of an autonomous vehicle predicts the future\nstates of the nearby vehicles based on the current and past observations of the\nsurrounding environment. This helps enhance their awareness of the imminent\nhazards. However, conventional behaviour prediction solutions are applicable in\nsimple driving scenarios that require short prediction horizons. Most recently,\ndeep learning-based approaches have become popular due to their superior\nperformance in more complex environments compared to the conventional\napproaches. Motivated by this increased popularity, we provide a comprehensive\nreview of the state-of-the-art of deep learning-based approaches for vehicle\nbehaviour prediction in this paper. We firstly give an overview of the generic\nproblem of vehicle behaviour prediction and discuss its challenges, followed by\nclassification and review of the most recent deep learning-based solutions\nbased on three criteria: input representation, output type, and prediction\nmethod. The paper also discusses the performance of several well-known\nsolutions, identifies the research gaps in the literature and outlines\npotential new research directions.\n",
        "published": "2019",
        "authors": [
            "Sajjad Mozaffari",
            "Omar Y. Al-Jarrah",
            "Mehrdad Dianati",
            "Paul Jennings",
            "Alexandros Mouzakitis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.06782v4",
        "title": "Gradient Surgery for Multi-Task Learning",
        "abstract": "  While deep learning and deep reinforcement learning (RL) systems have\ndemonstrated impressive results in domains such as image classification, game\nplaying, and robotic control, data efficiency remains a major challenge.\nMulti-task learning has emerged as a promising approach for sharing structure\nacross multiple tasks to enable more efficient learning. However, the\nmulti-task setting presents a number of optimization challenges, making it\ndifficult to realize large efficiency gains compared to learning tasks\nindependently. The reasons why multi-task learning is so challenging compared\nto single-task learning are not fully understood. In this work, we identify a\nset of three conditions of the multi-task optimization landscape that cause\ndetrimental gradient interference, and develop a simple yet general approach\nfor avoiding such interference between task gradients. We propose a form of\ngradient surgery that projects a task's gradient onto the normal plane of the\ngradient of any other task that has a conflicting gradient. On a series of\nchallenging multi-task supervised and multi-task RL problems, this approach\nleads to substantial gains in efficiency and performance. Further, it is\nmodel-agnostic and can be combined with previously-proposed multi-task\narchitectures for enhanced performance.\n",
        "published": "2020",
        "authors": [
            "Tianhe Yu",
            "Saurabh Kumar",
            "Abhishek Gupta",
            "Sergey Levine",
            "Karol Hausman",
            "Chelsea Finn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.02641v1",
        "title": "Dimensionality Reduction and Motion Clustering during Activities of\n  Daily Living: 3, 4, and 7 Degree-of-Freedom Arm Movements",
        "abstract": "  The wide variety of motions performed by the human arm during daily tasks\nmakes it desirable to find representative subsets to reduce the dimensionality\nof these movements for a variety of applications, including the design and\ncontrol of robotic and prosthetic devices. This paper presents a novel method\nand the results of an extensive human subjects study to obtain representative\narm joint angle trajectories that span naturalistic motions during Activities\nof Daily Living (ADLs). In particular, we seek to identify sets of useful\nmotion trajectories of the upper limb that are functions of a single variable,\nallowing, for instance, an entire prosthetic or robotic arm to be controlled\nwith a single input from a user, along with a means to select between motions\nfor different tasks. Data driven approaches are used to obtain clusters as well\nas representative motion averages for the full-arm 7 degree of freedom (DOF),\nelbow-wrist 4 DOF, and wrist-only 3 DOF motions. The proposed method makes use\nof well-known techniques such as dynamic time warping (DTW) to obtain a\ndivergence measure between motion segments, DTW barycenter averaging (DBA) to\nobtain averages, Ward's distance criterion to build hierarchical trees,\nbatch-DTW to simultaneously align multiple motion data, and functional\nprincipal component analysis (fPCA) to evaluate cluster variability. The\nclusters that emerge associate various recorded motions into primarily hand\nstart and end location for the full-arm system, motion direction for the\nwrist-only system, and an intermediate between the two qualities for the\nelbow-wrist system. The proposed clustering methodology is justified by\ncomparing results against alternative approaches.\n",
        "published": "2020",
        "authors": [
            "Yuri Gloumakov",
            "Adam J. Spiers",
            "Aaron M. Dollar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.05436v1",
        "title": "Learning Predictive Representations for Deformable Objects Using\n  Contrastive Estimation",
        "abstract": "  Using visual model-based learning for deformable object manipulation is\nchallenging due to difficulties in learning plannable visual representations\nalong with complex dynamic models. In this work, we propose a new learning\nframework that jointly optimizes both the visual representation model and the\ndynamics model using contrastive estimation. Using simulation data collected by\nrandomly perturbing deformable objects on a table, we learn latent dynamics\nmodels for these objects in an offline fashion. Then, using the learned models,\nwe use simple model-based planning to solve challenging deformable object\nmanipulation tasks such as spreading ropes and cloths. Experimentally, we show\nsubstantial improvements in performance over standard model-based learning\ntechniques across our rope and cloth manipulation suite. Finally, we transfer\nour visual manipulation policies trained on data purely collected in simulation\nto a real PR2 robot through domain randomization.\n",
        "published": "2020",
        "authors": [
            "Wilson Yan",
            "Ashwin Vangipuram",
            "Pieter Abbeel",
            "Lerrel Pinto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.08756v1",
        "title": "Deep Neural Network Perception Models and Robust Autonomous Driving\n  Systems",
        "abstract": "  This paper analyzes the robustness of deep learning models in autonomous\ndriving applications and discusses the practical solutions to address that.\n",
        "published": "2020",
        "authors": [
            "Mohammad Javad Shafiee",
            "Ahmadreza Jeddi",
            "Amir Nazemi",
            "Paul Fieguth",
            "Alexander Wong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.04767v2",
        "title": "Motion Prediction using Trajectory Sets and Self-Driving Domain\n  Knowledge",
        "abstract": "  Predicting the future motion of vehicles has been studied using various\ntechniques, including stochastic policies, generative models, and regression.\nRecent work has shown that classification over a trajectory set, which\napproximates possible motions, achieves state-of-the-art performance and avoids\nissues like mode collapse. However, map information and the physical\nrelationships between nearby trajectories is not fully exploited in this\nformulation. We build on classification-based approaches to motion prediction\nby adding an auxiliary loss that penalizes off-road predictions. This auxiliary\nloss can easily be pretrained using only map information (e.g., off-road area),\nwhich significantly improves performance on small datasets. We also investigate\nweighted cross-entropy losses to capture spatial-temporal relationships among\ntrajectories. Our final contribution is a detailed comparison of classification\nand ordinal regression on two public self-driving datasets.\n",
        "published": "2020",
        "authors": [
            "Freddy A. Boulton",
            "Elena Corina Grigore",
            "Eric M. Wolff"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.08470v2",
        "title": "Towards Incorporating Contextual Knowledge into the Prediction of\n  Driving Behavior",
        "abstract": "  Predicting the behavior of surrounding traffic participants is crucial for\nadvanced driver assistance systems and autonomous driving. Most researchers\nhowever do not consider contextual knowledge when predicting vehicle motion.\nExtending former studies, we investigate how predictions are affected by\nexternal conditions. To do so, we categorize different kinds of contextual\ninformation and provide a carefully chosen definition as well as examples for\nexternal conditions. More precisely, we investigate how a state-of-the-art\napproach for lateral motion prediction is influenced by one selected external\ncondition, namely the traffic density. Our investigations demonstrate that this\nkind of information is highly relevant in order to improve the performance of\nprediction algorithms. Therefore, this study constitutes the first step towards\nthe integration of such information into automated vehicles. Moreover, our\nmotion prediction approach is evaluated based on the public highD data set\nshowing a maneuver prediction performance with areas under the ROC curve above\n97% and a median lateral prediction error of only 0.18m on a prediction horizon\nof 5s.\n",
        "published": "2020",
        "authors": [
            "Florian Wirthm\u00fcller",
            "Julian Schlechtriemen",
            "Jochen Hipp",
            "Manfred Reichert"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.11631v1",
        "title": "Estimating Model Uncertainty of Neural Networks in Sparse Information\n  Form",
        "abstract": "  We present a sparse representation of model uncertainty for Deep Neural\nNetworks (DNNs) where the parameter posterior is approximated with an inverse\nformulation of the Multivariate Normal Distribution (MND), also known as the\ninformation form. The key insight of our work is that the information matrix,\ni.e. the inverse of the covariance matrix tends to be sparse in its spectrum.\nTherefore, dimensionality reduction techniques such as low rank approximations\n(LRA) can be effectively exploited. To achieve this, we develop a novel\nsparsification algorithm and derive a cost-effective analytical sampler. As a\nresult, we show that the information form can be scalably applied to represent\nmodel uncertainty in DNNs. Our exhaustive theoretical analysis and empirical\nevaluations on various benchmarks show the competitiveness of our approach over\nthe current methods.\n",
        "published": "2020",
        "authors": [
            "Jongseok Lee",
            "Matthias Humt",
            "Jianxiang Feng",
            "Rudolph Triebel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.03514v1",
        "title": "Imitation Learning Approach for AI Driving Olympics Trained on\n  Real-world and Simulation Data Simultaneously",
        "abstract": "  In this paper, we describe our winning approach to solving the Lane Following\nChallenge at the AI Driving Olympics Competition through imitation learning on\na mixed set of simulation and real-world data. AI Driving Olympics is a\ntwo-stage competition: at stage one, algorithms compete in a simulated\nenvironment with the best ones advancing to a real-world final. One of the main\nproblems that participants encounter during the competition is that algorithms\ntrained for the best performance in simulated environments do not hold up in a\nreal-world environment and vice versa. Classic control algorithms also do not\ntranslate well between tasks since most of them have to be tuned to specific\ndriving conditions such as lighting, road type, camera position, etc. To\novercome this problem, we employed the imitation learning algorithm and trained\nit on a dataset collected from sources both from simulation and real-world,\nforcing our model to perform equally well in all environments.\n",
        "published": "2020",
        "authors": [
            "Mikita Sazanovich",
            "Konstantin Chaika",
            "Kirill Krinkin",
            "Aleksei Shpilman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.04309v3",
        "title": "Self-Supervised Policy Adaptation during Deployment",
        "abstract": "  In most real world scenarios, a policy trained by reinforcement learning in\none environment needs to be deployed in another, potentially quite different\nenvironment. However, generalization across different environments is known to\nbe hard. A natural solution would be to keep training after deployment in the\nnew environment, but this cannot be done if the new environment offers no\nreward signal. Our work explores the use of self-supervision to allow the\npolicy to continue training after deployment without using any rewards. While\nprevious methods explicitly anticipate changes in the new environment, we\nassume no prior knowledge of those changes yet still obtain significant\nimprovements. Empirical evaluations are performed on diverse simulation\nenvironments from DeepMind Control suite and ViZDoom, as well as real robotic\nmanipulation tasks in continuously changing environments, taking observations\nfrom an uncalibrated camera. Our method improves generalization in 31 out of 36\nenvironments across various tasks and outperforms domain randomization on a\nmajority of environments.\n",
        "published": "2020",
        "authors": [
            "Nicklas Hansen",
            "Rishabh Jangir",
            "Yu Sun",
            "Guillem Aleny\u00e0",
            "Pieter Abbeel",
            "Alexei A. Efros",
            "Lerrel Pinto",
            "Xiaolong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.06965v1",
        "title": "Automated Synthetic-to-Real Generalization",
        "abstract": "  Models trained on synthetic images often face degraded generalization to real\ndata. As a convention, these models are often initialized with ImageNet\npre-trained representation. Yet the role of ImageNet knowledge is seldom\ndiscussed despite common practices that leverage this knowledge to maintain the\ngeneralization ability. An example is the careful hand-tuning of early stopping\nand layer-wise learning rates, which is shown to improve synthetic-to-real\ngeneralization but is also laborious and heuristic. In this work, we explicitly\nencourage the synthetically trained model to maintain similar representations\nwith the ImageNet pre-trained model, and propose a \\textit{learning-to-optimize\n(L2O)} strategy to automate the selection of layer-wise learning rates. We\ndemonstrate that the proposed framework can significantly improve the\nsynthetic-to-real generalization performance without seeing and training on\nreal data, while also benefiting downstream tasks such as domain adaptation.\nCode is available at: https://github.com/NVlabs/ASG.\n",
        "published": "2020",
        "authors": [
            "Wuyang Chen",
            "Zhiding Yu",
            "Zhangyang Wang",
            "Anima Anandkumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.12036v1",
        "title": "Implicit Latent Variable Model for Scene-Consistent Motion Forecasting",
        "abstract": "  In order to plan a safe maneuver an autonomous vehicle must accurately\nperceive its environment, and understand the interactions among traffic\nparticipants. In this paper, we aim to learn scene-consistent motion forecasts\nof complex urban traffic directly from sensor data. In particular, we propose\nto characterize the joint distribution over future trajectories via an implicit\nlatent variable model. We model the scene as an interaction graph and employ\npowerful graph neural networks to learn a distributed latent representation of\nthe scene. Coupled with a deterministic decoder, we obtain trajectory samples\nthat are consistent across traffic participants, achieving state-of-the-art\nresults in motion forecasting and interaction understanding. Last but not\nleast, we demonstrate that our motion forecasts result in safer and more\ncomfortable motion planning.\n",
        "published": "2020",
        "authors": [
            "Sergio Casas",
            "Cole Gulino",
            "Simon Suo",
            "Katie Luo",
            "Renjie Liao",
            "Raquel Urtasun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.12284v2",
        "title": "learn2learn: A Library for Meta-Learning Research",
        "abstract": "  Meta-learning researchers face two fundamental issues in their empirical\nwork: prototyping and reproducibility. Researchers are prone to make mistakes\nwhen prototyping new algorithms and tasks because modern meta-learning methods\nrely on unconventional functionalities of machine learning frameworks. In turn,\nreproducing existing results becomes a tedious endeavour -- a situation\nexacerbated by the lack of standardized implementations and benchmarks. As a\nresult, researchers spend inordinate amounts of time on implementing software\nrather than understanding and developing new ideas.\n  This manuscript introduces learn2learn, a library for meta-learning research\nfocused on solving those prototyping and reproducibility issues. learn2learn\nprovides low-level routines common across a wide-range of meta-learning\ntechniques (e.g. meta-descent, meta-reinforcement learning, few-shot learning),\nand builds standardized interfaces to algorithms and benchmarks on top of them.\nIn releasing learn2learn under a free and open source license, we hope to\nfoster a community around standardized software for meta-learning research.\n",
        "published": "2020",
        "authors": [
            "S\u00e9bastien M. R. Arnold",
            "Praateek Mahajan",
            "Debajyoti Datta",
            "Ian Bunner",
            "Konstantinos Saitas Zarkias"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.09006v3",
        "title": "KINet: Unsupervised Forward Models for Robotic Pushing Manipulation",
        "abstract": "  Object-centric representation is an essential abstraction for forward\nprediction. Most existing forward models learn this representation through\nextensive supervision (e.g., object class and bounding box) although such\nground-truth information is not readily accessible in reality. To address this,\nwe introduce KINet (Keypoint Interaction Network) -- an end-to-end unsupervised\nframework to reason about object interactions based on a keypoint\nrepresentation. Using visual observations, our model learns to associate\nobjects with keypoint coordinates and discovers a graph representation of the\nsystem as a set of keypoint embeddings and their relations. It then learns an\naction-conditioned forward model using contrastive estimation to predict future\nkeypoint states. By learning to perform physical reasoning in the keypoint\nspace, our model automatically generalizes to scenarios with a different number\nof objects, novel backgrounds, and unseen object geometries. Experiments\ndemonstrate the effectiveness of our model in accurately performing forward\nprediction and learning plannable object-centric representations for downstream\nrobotic pushing manipulation tasks.\n",
        "published": "2022",
        "authors": [
            "Alireza Rezazadeh",
            "Changhyun Choi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.05512v1",
        "title": "The Feeling of Success: Does Touch Sensing Help Predict Grasp Outcomes?",
        "abstract": "  A successful grasp requires careful balancing of the contact forces. Deducing\nwhether a particular grasp will be successful from indirect measurements, such\nas vision, is therefore quite challenging, and direct sensing of contacts\nthrough touch sensing provides an appealing avenue toward more successful and\nconsistent robotic grasping. However, in order to fully evaluate the value of\ntouch sensing for grasp outcome prediction, we must understand how touch\nsensing can influence outcome prediction accuracy when combined with other\nmodalities. Doing so using conventional model-based techniques is exceptionally\ndifficult. In this work, we investigate the question of whether touch sensing\naids in predicting grasp outcomes within a multimodal sensing framework that\ncombines vision and touch. To that end, we collected more than 9,000 grasping\ntrials using a two-finger gripper equipped with GelSight high-resolution\ntactile sensors on each finger, and evaluated visuo-tactile deep neural network\nmodels to directly predict grasp outcomes from either modality individually,\nand from both modalities together. Our experimental results indicate that\nincorporating tactile readings substantially improve grasping performance.\n",
        "published": "2017",
        "authors": [
            "Roberto Calandra",
            "Andrew Owens",
            "Manu Upadhyaya",
            "Wenzhen Yuan",
            "Justin Lin",
            "Edward H. Adelson",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.05819v3",
        "title": "Uncertainty-aware Short-term Motion Prediction of Traffic Actors for\n  Autonomous Driving",
        "abstract": "  We address one of the crucial aspects necessary for safe and efficient\noperations of autonomous vehicles, namely predicting future state of traffic\nactors in the autonomous vehicle's surroundings. We introduce a deep\nlearning-based approach that takes into account a current world state and\nproduces raster images of each actor's vicinity. The rasters are then used as\ninputs to deep convolutional models to infer future movement of actors while\nalso accounting for and capturing inherent uncertainty of the prediction task.\nExtensive experiments on real-world data strongly suggest benefits of the\nproposed approach. Moreover, following completion of the offline tests the\nsystem was successfully tested onboard self-driving vehicles.\n",
        "published": "2018",
        "authors": [
            "Nemanja Djuric",
            "Vladan Radosavljevic",
            "Henggang Cui",
            "Thi Nguyen",
            "Fang-Chieh Chou",
            "Tsung-Han Lin",
            "Nitin Singh",
            "Jeff Schneider"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.02076v1",
        "title": "Transferring Physical Motion Between Domains for Neural Inertial\n  Tracking",
        "abstract": "  Inertial information processing plays a pivotal role in ego-motion awareness\nfor mobile agents, as inertial measurements are entirely egocentric and not\nenvironment dependent. However, they are affected greatly by changes in sensor\nplacement/orientation or motion dynamics, and it is infeasible to collect\nlabelled data from every domain. To overcome the challenges of domain\nadaptation on long sensory sequences, we propose a novel framework that\nextracts domain-invariant features of raw sequences from arbitrary domains, and\ntransforms to new domains without any paired data. Through the experiments, we\ndemonstrate that it is able to efficiently and effectively convert the raw\nsequence from a new unlabelled target domain into an accurate inertial\ntrajectory, benefiting from the physical motion knowledge transferred from the\nlabelled source domain. We also conduct real-world experiments to show our\nframework can reconstruct physically meaningful trajectories from raw IMU\nmeasurements obtained with a standard mobile phone in various attachments.\n",
        "published": "2018",
        "authors": [
            "Changhao Chen",
            "Yishu Miao",
            "Chris Xiaoxuan Lu",
            "Phil Blunsom",
            "Andrew Markham",
            "Niki Trigoni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.05394v1",
        "title": "Sequential Learning of Movement Prediction in Dynamic Environments using\n  LSTM Autoencoder",
        "abstract": "  Predicting movement of objects while the action of learning agent interacts\nwith the dynamics of the scene still remains a key challenge in robotics. We\npropose a multi-layer Long Short Term Memory (LSTM) autoendocer network that\npredicts future frames for a robot navigating in a dynamic environment with\nmoving obstacles. The autoencoder network is composed of a state and action\nconditioned decoder network that reconstructs the future frames of video,\nconditioned on the action taken by the agent. The input image frames are first\ntransformed into low dimensional feature vectors with a pre-trained encoder\nnetwork and then reconstructed with the LSTM autoencoder network to generate\nthe future frames. A virtual environment, based on the OpenAi-Gym framework for\nrobotics, is used to gather training data and test the proposed network. The\ninitial experiments show promising results indicating that these predicted\nframes can be used by an appropriate reinforcement learning framework in future\nto navigate around dynamic obstacles.\n",
        "published": "2018",
        "authors": [
            "Meenakshi Sarkar",
            "Debasish Ghose"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.07854v2",
        "title": "End-to-End Robotic Reinforcement Learning without Reward Engineering",
        "abstract": "  The combination of deep neural network models and reinforcement learning\nalgorithms can make it possible to learn policies for robotic behaviors that\ndirectly read in raw sensory inputs, such as camera images, effectively\nsubsuming both estimation and control into one model. However, real-world\napplications of reinforcement learning must specify the goal of the task by\nmeans of a manually programmed reward function, which in practice requires\neither designing the very same perception pipeline that end-to-end\nreinforcement learning promises to avoid, or else instrumenting the environment\nwith additional sensors to determine if the task has been performed\nsuccessfully. In this paper, we propose an approach for removing the need for\nmanual engineering of reward specifications by enabling a robot to learn from a\nmodest number of examples of successful outcomes, followed by actively\nsolicited queries, where the robot shows the user a state and asks for a label\nto determine whether that state represents successful completion of the task.\nWhile requesting labels for every single state would amount to asking the user\nto manually provide the reward signal, our method requires labels for only a\ntiny fraction of the states seen during training, making it an efficient and\npractical approach for learning skills without manually engineered rewards. We\nevaluate our method on real-world robotic manipulation tasks where the\nobservations consist of images viewed by the robot's camera. In our\nexperiments, our method effectively learns to arrange objects, place books, and\ndrape cloth, directly from images and without any manually specified reward\nfunctions, and with only 1-4 hours of interaction with the real world.\n",
        "published": "2019",
        "authors": [
            "Avi Singh",
            "Larry Yang",
            "Kristian Hartikainen",
            "Chelsea Finn",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.09007v2",
        "title": "DeepLocalization: Landmark-based Self-Localization with Deep Neural\n  Networks",
        "abstract": "  We address the problem of vehicle self-localization from multi-modal sensor\ninformation and a reference map. The map is generated off-line by extracting\nlandmarks from the vehicle's field of view, while the measurements are\ncollected similarly on the fly. Our goal is to determine the autonomous\nvehicle's pose from the landmark measurements and map landmarks. To learn this\nmapping, we propose DeepLocalization, a deep neural network that regresses the\nvehicle's translation and rotation parameters from unordered and dynamic input\nlandmarks. The proposed network architecture is robust to changes of the\ndynamic environment and can cope with a small number of extracted landmarks.\nDuring the training process we rely on synthetically generated ground-truth. In\nour experiments, we evaluate two inference approaches in real-world scenarios.\nWe show that DeepLocalization can be combined with regular GPS signals and\nfiltering algorithms such as the extended Kalman filter. Our approach achieves\nstate-of-the-art accuracy and is about ten times faster than the related work.\n",
        "published": "2019",
        "authors": [
            "Nico Engel",
            "Stefan Hoermann",
            "Markus Horn",
            "Vasileios Belagiannis",
            "Klaus Dietmayer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.10261v1",
        "title": "Improving benchmarks for autonomous vehicles testing using synthetically\n  generated images",
        "abstract": "  Nowadays autonomous technologies are a very heavily explored area and\nparticularly computer vision as the main component of vehicle perception. The\nquality of the whole vision system based on neural networks relies on the\ndataset it was trained on. It is extremely difficult to find traffic sign\ndatasets from most of the counties of the world. Meaning autonomous vehicle\nfrom the USA will not be able to drive though Lithuania recognizing all road\nsigns on the way. In this paper, we propose a solution on how to update model\nusing a small dataset from the country vehicle will be used in. It is important\nto mention that is not panacea, rather small upgrade which can boost autonomous\ncar development in countries with limited data access. We achieved about 10\npercent quality raise and expect even better results during future experiments.\n",
        "published": "2019",
        "authors": [
            "Aleksander Lukashou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.03708v1",
        "title": "Machine Vision in the Context of Robotics: A Systematic Literature\n  Review",
        "abstract": "  Machine vision is critical to robotics due to a wide range of applications\nwhich rely on input from visual sensors such as autonomous mobile robots and\nsmart production systems. To create the smart homes and systems of tomorrow, an\noverview about current challenges in the research field would be of use to\nidentify further possible directions, created in a systematic and reproducible\nmanner. In this work a systematic literature review was conducted covering\nresearch from the last 10 years. We screened 172 papers from four databases and\nselected 52 relevant papers. While robustness and computation time were\nimproved greatly, occlusion and lighting variance are still the biggest\nproblems faced. From the number of recent publications, we conclude that the\nobserved field is of relevance and interest to the research community. Further\nchallenges arise in many areas of the field.\n",
        "published": "2019",
        "authors": [
            "Javad Ghofrani",
            "Robert Kirschne",
            "Daniel Rossburg",
            "Dirk Reichelt",
            "Tom Dimter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.00825v1",
        "title": "Self-supervised Body Image Acquisition Using a Deep Neural Network for\n  Sensorimotor Prediction",
        "abstract": "  This work investigates how a naive agent can acquire its own body image in a\nself-supervised way, based on the predictability of its sensorimotor\nexperience. Our working hypothesis is that, due to its temporal stability, an\nagent's body produces more consistent sensory experiences than the environment,\nwhich exhibits a greater variability. Given its motor experience, an agent can\nthus reliably predict what appearance its body should have. This intrinsic\npredictability can be used to automatically isolate the body image from the\nrest of the environment. We propose a two-branches deconvolutional neural\nnetwork to predict the visual sensory state associated with an input motor\nstate, as well as the prediction error associated with this input. We train the\nnetwork on a dataset of first-person images collected with a simulated Pepper\nrobot, and show how the network outputs can be used to automatically isolate\nits visible arm from the rest of the environment. Finally, the quality of the\nbody image produced by the network is evaluated.\n",
        "published": "2019",
        "authors": [
            "Alban Laflaqui\u00e8re",
            "Verena V. Hafner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.11886v1",
        "title": "Traffic Light Recognition Using Deep Learning and Prior Maps for\n  Autonomous Cars",
        "abstract": "  Autonomous terrestrial vehicles must be capable of perceiving traffic lights\nand recognizing their current states to share the streets with human drivers.\nMost of the time, human drivers can easily identify the relevant traffic\nlights. To deal with this issue, a common solution for autonomous cars is to\nintegrate recognition with prior maps. However, additional solution is required\nfor the detection and recognition of the traffic light. Deep learning\ntechniques have showed great performance and power of generalization including\ntraffic related problems. Motivated by the advances in deep learning, some\nrecent works leveraged some state-of-the-art deep detectors to locate (and\nfurther recognize) traffic lights from 2D camera images. However, none of them\ncombine the power of the deep learning-based detectors with prior maps to\nrecognize the state of the relevant traffic lights. Based on that, this work\nproposes to integrate the power of deep learning-based detection with the prior\nmaps used by our car platform IARA (acronym for Intelligent Autonomous Robotic\nAutomobile) to recognize the relevant traffic lights of predefined routes. The\nprocess is divided in two phases: an offline phase for map construction and\ntraffic lights annotation; and an online phase for traffic light recognition\nand identification of the relevant ones. The proposed system was evaluated on\nfive test cases (routes) in the city of Vit\\'oria, each case being composed of\na video sequence and a prior map with the relevant traffic lights for the\nroute. Results showed that the proposed technique is able to correctly identify\nthe relevant traffic light along the trajectory.\n",
        "published": "2019",
        "authors": [
            "Lucas C. Possatti",
            "R\u00e2nik Guidolini",
            "Vinicius B. Cardoso",
            "Rodrigo F. Berriel",
            "Thiago M. Paix\u00e3o",
            "Claudine Badue",
            "Alberto F. De Souza",
            "Thiago Oliveira-Santos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.05659v1",
        "title": "Estimating Fingertip Forces, Torques, and Local Curvatures from\n  Fingernail Images",
        "abstract": "  The study of dexterous manipulation has provided important insights in humans\nsensorimotor control as well as inspiration for manipulation strategies in\nrobotic hands. Previous work focused on experimental environment with\nrestrictions. Here we describe a method using the deformation and color\ndistribution of the fingernail and its surrounding skin, to estimate the\nfingertip forces, torques and contact surface curvatures for various objects,\nincluding the shape and material of the contact surfaces and the weight of the\nobjects. The proposed method circumvents limitations associated with sensorized\nobjects, gloves or fixed contact surface type. In addition, compared with\nprevious single finger estimation in an experimental environment, we extend the\napproach to multiple finger force estimation, which can be used for\napplications such as human grasping analysis. Four algorithms are used, c.q.,\nGaussian process (GP), Convolutional Neural Networks (CNN), Neural Networks\nwith Fast Dropout (NN-FD) and Recurrent Neural Networks with Fast Dropout\n(RNN-FD), to model a mapping from images to the corresponding labels. The\nresults further show that the proposed method has high accuracy to predict\nforce, torque and contact surface.\n",
        "published": "2019",
        "authors": [
            "Nutan Chen",
            "G\u00f6ran Westling",
            "Benoni B. Edin",
            "Patrick van der Smagt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.12950v1",
        "title": "Towards Object Detection from Motion",
        "abstract": "  We present a novel approach to weakly supervised object detection. Instead of\nannotated images, our method only requires two short videos to learn to detect\na new object: 1) a video of a moving object and 2) one or more \"negative\"\nvideos of the scene without the object. The key idea of our algorithm is to\ntrain the object detector to produce physically plausible object motion when\napplied to the first video and to not detect anything in the second video. With\nthis approach, our method learns to locate objects without any object location\nannotations. Once the model is trained, it performs object detection on single\nimages. We evaluate our method in three robotics settings that afford learning\nobjects from motion: observing moving objects, watching demonstrations of\nobject manipulation, and physically interacting with objects (see a video\nsummary at https://youtu.be/BH0Hv3zZG_4).\n",
        "published": "2019",
        "authors": [
            "Rico Jonschkowski",
            "Austin Stone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.13377v2",
        "title": "Lane Attention: Predicting Vehicles' Moving Trajectories by Learning\n  Their Attention over Lanes",
        "abstract": "  Accurately forecasting the future movements of surrounding vehicles is\nessential for safe and efficient operations of autonomous driving cars. This\ntask is difficult because a vehicle's moving trajectory is greatly determined\nby its driver's intention, which is often hard to estimate. By leveraging\nattention mechanisms along with long short-term memory (LSTM) networks, this\nwork learns the relation between a driver's intention and the vehicle's\nchanging positions relative to road infrastructures, and uses it to guide the\nprediction. Different from other state-of-the-art solutions, our work treats\nthe on-road lanes as non-Euclidean structures, unfolds the vehicle's moving\nhistory to form a spatio-temporal graph, and uses methods from Graph Neural\nNetworks to solve the problem. Not only is our approach a pioneering attempt in\nusing non-Euclidean methods to process static environmental features around a\npredicted object, our model also outperforms other state-of-the-art models in\nseveral metrics. The practicability and interpretability analysis of the model\nshows great potential for large-scale deployment in various autonomous driving\nsystems in addition to our own.\n",
        "published": "2019",
        "authors": [
            "Jiacheng Pan",
            "Hongyi Sun",
            "Kecheng Xu",
            "Yifei Jiang",
            "Xiangquan Xiao",
            "Jiangtao Hu",
            "Jinghao Miao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.13561v4",
        "title": "Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis",
        "abstract": "  In this paper we explore the richness of information captured by the latent\nspace of a vision-based generative model. The model combines unsupervised\ngenerative learning with a task-based performance predictor to learn and to\nexploit task-relevant object affordances given visual observations from a\nreaching task, involving a scenario and a stick-like tool. While the learned\nembedding of the generative model captures factors of variation in 3D tool\ngeometry (e.g. length, width, and shape), the performance predictor identifies\nsub-manifolds of the embedding that correlate with task success. Within a\nvariety of scenarios, we demonstrate that traversing the latent space via\nbackpropagation from the performance predictor allows us to imagine tools\nappropriate for the task at hand. Our results indicate that affordances-like\nthe utility for reaching-are encoded along smooth trajectories in latent space.\nAccessing these emergent affordances by considering only high-level performance\ncriteria (such as task success) enables an agent to manipulate tool geometries\nin a targeted and deliberate way.\n",
        "published": "2019",
        "authors": [
            "Yizhe Wu",
            "Sudhanshu Kasewa",
            "Oliver Groth",
            "Sasha Salter",
            "Li Sun",
            "Oiwi Parker Jones",
            "Ingmar Posner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.04971v3",
        "title": "Deep Variational Semi-Supervised Novelty Detection",
        "abstract": "  In anomaly detection (AD), one seeks to identify whether a test sample is\nabnormal, given a data set of normal samples. A recent and promising approach\nto AD relies on deep generative models, such as variational autoencoders\n(VAEs), for unsupervised learning of the normal data distribution. In\nsemi-supervised AD (SSAD), the data also includes a small sample of labeled\nanomalies. In this work, we propose two variational methods for training VAEs\nfor SSAD. The intuitive idea in both methods is to train the encoder to\n`separate' between latent vectors for normal and outlier data. We show that\nthis idea can be derived from principled probabilistic formulations of the\nproblem, and propose simple and effective algorithms. Our methods can be\napplied to various data types, as we demonstrate on SSAD datasets ranging from\nnatural images to astronomy and medicine, can be combined with any VAE model\narchitecture, and are naturally compatible with ensembling. When comparing to\nstate-of-the-art SSAD methods that are not specific to particular data types,\nwe obtain marked improvement in outlier detection.\n",
        "published": "2019",
        "authors": [
            "Tal Daniel",
            "Thanard Kurutach",
            "Aviv Tamar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.06487v2",
        "title": "OpenLORIS-Object: A Robotic Vision Dataset and Benchmark for Lifelong\n  Deep Learning",
        "abstract": "  The recent breakthroughs in computer vision have benefited from the\navailability of large representative datasets (e.g. ImageNet and COCO) for\ntraining. Yet, robotic vision poses unique challenges for applying visual\nalgorithms developed from these standard computer vision datasets due to their\nimplicit assumption over non-varying distributions for a fixed set of tasks.\nFully retraining models each time a new task becomes available is infeasible\ndue to computational, storage and sometimes privacy issues, while na\\\"{i}ve\nincremental strategies have been shown to suffer from catastrophic forgetting.\nIt is crucial for the robots to operate continuously under open-set and\ndetrimental conditions with adaptive visual perceptual systems, where lifelong\nlearning is a fundamental capability. However, very few datasets and benchmarks\nare available to evaluate and compare emerging techniques. To fill this gap, we\nprovide a new lifelong robotic vision dataset (\"OpenLORIS-Object\") collected\nvia RGB-D cameras. The dataset embeds the challenges faced by a robot in the\nreal-life application and provides new benchmarks for validating lifelong\nobject recognition algorithms. Moreover, we have provided a testbed of $9$\nstate-of-the-art lifelong learning algorithms. Each of them involves $48$ tasks\nwith $4$ evaluation metrics over the OpenLORIS-Object dataset. The results\ndemonstrate that the object recognition task in the ever-changing difficulty\nenvironments is far from being solved and the bottlenecks are at the\nforward/backward transfer designs. Our dataset and benchmark are publicly\navailable at at\n\\href{https://lifelong-robotic-vision.github.io/dataset/object}{\\underline{https://lifelong-robotic-vision.github.io/dataset/object}}.\n",
        "published": "2019",
        "authors": [
            "Qi She",
            "Fan Feng",
            "Xinyue Hao",
            "Qihan Yang",
            "Chuanlin Lan",
            "Vincenzo Lomonaco",
            "Xuesong Shi",
            "Zhengwei Wang",
            "Yao Guo",
            "Yimin Zhang",
            "Fei Qiao",
            "Rosa H. M. Chan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.09676v1",
        "title": "Third-Person Visual Imitation Learning via Decoupled Hierarchical\n  Controller",
        "abstract": "  We study a generalized setup for learning from demonstration to build an\nagent that can manipulate novel objects in unseen scenarios by looking at only\na single video of human demonstration from a third-person perspective. To\naccomplish this goal, our agent should not only learn to understand the intent\nof the demonstrated third-person video in its context but also perform the\nintended task in its environment configuration. Our central insight is to\nenforce this structure explicitly during learning by decoupling what to achieve\n(intended task) from how to perform it (controller). We propose a hierarchical\nsetup where a high-level module learns to generate a series of first-person\nsub-goals conditioned on the third-person video demonstration, and a low-level\ncontroller predicts the actions to achieve those sub-goals. Our agent acts from\nraw image observations without any access to the full state information. We\nshow results on a real robotic platform using Baxter for the manipulation tasks\nof pouring and placing objects in a box. Project video and code are at\nhttps://pathak22.github.io/hierarchical-imitation/\n",
        "published": "2019",
        "authors": [
            "Pratyusha Sharma",
            "Deepak Pathak",
            "Abhinav Gupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.03735v1",
        "title": "Real-Time Object Detection and Recognition on Low-Compute Humanoid\n  Robots using Deep Learning",
        "abstract": "  We envision that in the near future, humanoid robots would share home space\nand assist us in our daily and routine activities through object manipulations.\nOne of the fundamental technologies that need to be developed for robots is to\nenable them to detect objects and recognize them for effective manipulations\nand take real-time decisions involving those objects. In this paper, we\ndescribe a novel architecture that enables multiple low-compute NAO robots to\nperform real-time detection, recognition and localization of objects in its\ncamera view and take programmable actions based on the detected objects. The\nproposed algorithm for object detection and localization is an empirical\nmodification of YOLOv3, based on indoor experiments in multiple scenarios, with\na smaller weight size and lesser computational requirements. Quantization of\nthe weights and re-adjusting filter sizes and layer arrangements for\nconvolutions improved the inference time for low-resolution images from the\nrobot s camera feed. YOLOv3 was chosen after a comparative study of bounding\nbox algorithms was performed with an objective to choose one that strikes the\nperfect balance among information retention, low inference time and high\naccuracy for real-time object detection and localization. The architecture also\ncomprises of an effective end-to-end pipeline to feed the real-time frames from\nthe camera feed to the neural net and use its results for guiding the robot\nwith customizable actions corresponding to the detected class labels.\n",
        "published": "2020",
        "authors": [
            "Sayantan Chatterjee",
            "Faheem H. Zunjani",
            "Souvik Sen",
            "Gora C. Nandi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.01698v2",
        "title": "How to Train Your Energy-Based Model for Regression",
        "abstract": "  Energy-based models (EBMs) have become increasingly popular within computer\nvision in recent years. While they are commonly employed for generative image\nmodeling, recent work has applied EBMs also for regression tasks, achieving\nstate-of-the-art performance on object detection and visual tracking. Training\nEBMs is however known to be challenging. While a variety of different\ntechniques have been explored for generative modeling, the application of EBMs\nto regression is not a well-studied problem. How EBMs should be trained for\nbest possible regression performance is thus currently unclear. We therefore\naccept the task of providing the first detailed study of this problem. To that\nend, we propose a simple yet highly effective extension of noise contrastive\nestimation, and carefully compare its performance to six popular methods from\nliterature on the tasks of 1D regression and object detection. The results of\nthis comparison suggest that our training method should be considered the go-to\napproach. We also apply our method to the visual tracking task, achieving\nstate-of-the-art performance on five datasets. Notably, our tracker achieves\n63.7% AUC on LaSOT and 78.7% Success on TrackingNet. Code is available at\nhttps://github.com/fregu856/ebms_regression.\n",
        "published": "2020",
        "authors": [
            "Fredrik K. Gustafsson",
            "Martin Danelljan",
            "Radu Timofte",
            "Thomas B. Sch\u00f6n"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.04450v2",
        "title": "Map-Adaptive Goal-Based Trajectory Prediction",
        "abstract": "  We present a new method for multi-modal, long-term vehicle trajectory\nprediction. Our approach relies on using lane centerlines captured in rich maps\nof the environment to generate a set of proposed goal paths for each vehicle.\nUsing these paths -- which are generated at run time and therefore dynamically\nadapt to the scene -- as spatial anchors, we predict a set of goal-based\ntrajectories along with a categorical distribution over the goals. This\napproach allows us to directly model the goal-directed behavior of traffic\nactors, which unlocks the potential for more accurate long-term prediction. Our\nexperimental results on both a large-scale internal driving dataset and on the\npublic nuScenes dataset show that our model outperforms state-of-the-art\napproaches for vehicle trajectory prediction over a 6-second horizon. We also\nempirically demonstrate that our model is better able to generalize to road\nscenes from a completely new city than existing methods.\n",
        "published": "2020",
        "authors": [
            "Lingyao Zhang",
            "Po-Hsun Su",
            "Jerrick Hoang",
            "Galen Clark Haynes",
            "Micol Marchetti-Bowick"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.05147v1",
        "title": "Practical Cross-modal Manifold Alignment for Grounded Language",
        "abstract": "  We propose a cross-modality manifold alignment procedure that leverages\ntriplet loss to jointly learn consistent, multi-modal embeddings of\nlanguage-based concepts of real-world items. Our approach learns these\nembeddings by sampling triples of anchor, positive, and negative data points\nfrom RGB-depth images and their natural language descriptions. We show that our\napproach can benefit from, but does not require, post-processing steps such as\nProcrustes analysis, in contrast to some of our baselines which require it for\nreasonable performance. We demonstrate the effectiveness of our approach on two\ndatasets commonly used to develop robotic-based grounded language learning\nsystems, where our approach outperforms four baselines, including a\nstate-of-the-art approach, across five evaluation metrics.\n",
        "published": "2020",
        "authors": [
            "Andre T. Nguyen",
            "Luke E. Richards",
            "Gaoussou Youssouf Kebe",
            "Edward Raff",
            "Kasra Darvish",
            "Frank Ferraro",
            "Cynthia Matuszek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.12916v1",
        "title": "Interaction-Based Trajectory Prediction Over a Hybrid Traffic Graph",
        "abstract": "  Behavior prediction of traffic actors is an essential component of any\nreal-world self-driving system. Actors' long-term behaviors tend to be governed\nby their interactions with other actors or traffic elements (traffic lights,\nstop signs) in the scene. To capture this highly complex structure of\ninteractions, we propose to use a hybrid graph whose nodes represent both the\ntraffic actors as well as the static and dynamic traffic elements present in\nthe scene. The different modes of temporal interaction (e.g., stopping and\ngoing) among actors and traffic elements are explicitly modeled by graph edges.\nThis explicit reasoning about discrete interaction types not only helps in\npredicting future motion, but also enhances the interpretability of the model,\nwhich is important for safety-critical applications such as autonomous driving.\nWe predict actors' trajectories and interaction types using a graph neural\nnetwork, which is trained in a semi-supervised manner. We show that our\nproposed model, TrafficGraphNet, achieves state-of-the-art trajectory\nprediction accuracy while maintaining a high level of interpretability.\n",
        "published": "2020",
        "authors": [
            "Sumit Kumar",
            "Yiming Gu",
            "Jerrick Hoang",
            "Galen Clark Haynes",
            "Micol Marchetti-Bowick"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.04634v2",
        "title": "Accurate 3D Object Detection using Energy-Based Models",
        "abstract": "  Accurate 3D object detection (3DOD) is crucial for safe navigation of complex\nenvironments by autonomous robots. Regressing accurate 3D bounding boxes in\ncluttered environments based on sparse LiDAR data is however a highly\nchallenging problem. We address this task by exploring recent advances in\nconditional energy-based models (EBMs) for probabilistic regression. While\nmethods employing EBMs for regression have demonstrated impressive performance\non 2D object detection in images, these techniques are not directly applicable\nto 3D bounding boxes. In this work, we therefore design a differentiable\npooling operator for 3D bounding boxes, serving as the core module of our EBM\nnetwork. We further integrate this general approach into the state-of-the-art\n3D object detector SA-SSD. On the KITTI dataset, our proposed approach\nconsistently outperforms the SA-SSD baseline across all 3DOD metrics,\ndemonstrating the potential of EBM-based regression for highly accurate 3DOD.\nCode is available at https://github.com/fregu856/ebms_3dod.\n",
        "published": "2020",
        "authors": [
            "Fredrik K. Gustafsson",
            "Martin Danelljan",
            "Thomas B. Sch\u00f6n"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.02988v1",
        "title": "PRISM: Probabilistic Real-Time Inference in Spatial World Models",
        "abstract": "  We introduce PRISM, a method for real-time filtering in a probabilistic\ngenerative model of agent motion and visual perception. Previous approaches\neither lack uncertainty estimates for the map and agent state, do not run in\nreal-time, do not have a dense scene representation or do not model agent\ndynamics. Our solution reconciles all of these aspects. We start from a\npredefined state-space model which combines differentiable rendering and 6-DoF\ndynamics. Probabilistic inference in this model amounts to simultaneous\nlocalisation and mapping (SLAM) and is intractable. We use a series of\napproximations to Bayesian inference to arrive at probabilistic map and state\nestimates. We take advantage of well-established methods and closed-form\nupdates, preserving accuracy and enabling real-time capability. The proposed\nsolution runs at 10Hz real-time and is similarly accurate to state-of-the-art\nSLAM in small to medium-sized indoor environments, with high-speed UAV and\nhandheld camera agents (Blackbird, EuRoC and TUM-RGBD).\n",
        "published": "2022",
        "authors": [
            "Atanas Mirchev",
            "Baris Kayalibay",
            "Ahmed Agha",
            "Patrick van der Smagt",
            "Daniel Cremers",
            "Justin Bayer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.08576v1",
        "title": "Learning to Act from Actionless Videos through Dense Correspondences",
        "abstract": "  In this work, we present an approach to construct a video-based robot policy\ncapable of reliably executing diverse tasks across different robots and\nenvironments from few video demonstrations without using any action\nannotations. Our method leverages images as a task-agnostic representation,\nencoding both the state and action information, and text as a general\nrepresentation for specifying robot goals. By synthesizing videos that\n``hallucinate'' robot executing actions and in combination with dense\ncorrespondences between frames, our approach can infer the closed-formed action\nto execute to an environment without the need of any explicit action labels.\nThis unique capability allows us to train the policy solely based on RGB videos\nand deploy learned policies to various robotic tasks. We demonstrate the\nefficacy of our approach in learning policies on table-top manipulation and\nnavigation tasks. Additionally, we contribute an open-source framework for\nefficient video modeling, enabling the training of high-fidelity policy models\nwith four GPUs within a single day.\n",
        "published": "2023",
        "authors": [
            "Po-Chen Ko",
            "Jiayuan Mao",
            "Yilun Du",
            "Shao-Hua Sun",
            "Joshua B. Tenenbaum"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.03017v5",
        "title": "Learning Visual Reasoning Without Strong Priors",
        "abstract": "  Achieving artificial visual reasoning - the ability to answer image-related\nquestions which require a multi-step, high-level process - is an important step\ntowards artificial general intelligence. This multi-modal task requires\nlearning a question-dependent, structured reasoning process over images from\nlanguage. Standard deep learning approaches tend to exploit biases in the data\nrather than learn this underlying structure, while leading methods learn to\nvisually reason successfully but are hand-crafted for reasoning. We show that a\ngeneral-purpose, Conditional Batch Normalization approach achieves\nstate-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4%\nerror rate. We outperform the next best end-to-end method (4.5%) and even\nmethods that use extra supervision (3.1%). We probe our model to shed light on\nhow it reasons, showing it has learned a question-dependent, multi-step\nprocess. Previous work has operated under the assumption that visual reasoning\ncalls for a specialized architecture, but we show that a general architecture\nwith proper conditioning can learn to visually reason effectively.\n",
        "published": "2017",
        "authors": [
            "Ethan Perez",
            "Harm de Vries",
            "Florian Strub",
            "Vincent Dumoulin",
            "Aaron Courville"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.09457v1",
        "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using\n  Corpus-level Constraints",
        "abstract": "  Language is increasingly being used to define rich visual recognition\nproblems with supporting image collections sourced from the web. Structured\nprediction models are used in these tasks to take advantage of correlations\nbetween co-occurring labels and visual input but risk inadvertently encoding\nsocial biases found in web corpora. In this work, we study data and models\nassociated with multilabel object classification and visual semantic role\nlabeling. We find that (a) datasets for these tasks contain significant gender\nbias and (b) models trained on these datasets further amplify existing bias.\nFor example, the activity cooking is over 33% more likely to involve females\nthan males in a training set, and a trained model further amplifies the\ndisparity to 68% at test time. We propose to inject corpus-level constraints\nfor calibrating existing structured prediction models and design an algorithm\nbased on Lagrangian relaxation for collective inference. Our method results in\nalmost no performance loss for the underlying recognition task but decreases\nthe magnitude of bias amplification by 47.5% and 40.5% for multilabel\nclassification and visual semantic role labeling, respectively.\n",
        "published": "2017",
        "authors": [
            "Jieyu Zhao",
            "Tianlu Wang",
            "Mark Yatskar",
            "Vicente Ordonez",
            "Kai-Wei Chang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.06821v3",
        "title": "Acquiring Common Sense Spatial Knowledge through Implicit Spatial\n  Templates",
        "abstract": "  Spatial understanding is a fundamental problem with wide-reaching real-world\napplications. The representation of spatial knowledge is often modeled with\nspatial templates, i.e., regions of acceptability of two objects under an\nexplicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with\nprior work that restricts spatial templates to explicit spatial prepositions\n(e.g., \"glass on table\"), here we extend this concept to implicit spatial\nlanguage, i.e., those relationships (generally actions) for which the spatial\narrangement of the objects is only implicitly implied (e.g., \"man riding\nhorse\"). In contrast with explicit relationships, predicting spatial\narrangements from implicit spatial language requires significant common sense\nspatial understanding. Here, we introduce the task of predicting spatial\ntemplates for two objects under a relationship, which can be seen as a spatial\nquestion-answering task with a (2D) continuous output (\"where is the man w.r.t.\na horse when the man is walking the horse?\"). We present two simple\nneural-based models that leverage annotated images and structured text to learn\nthis task. The good performance of these models reveals that spatial locations\nare to a large extent predictable from implicit spatial language. Crucially,\nthe models attain similar performance in a challenging generalized setting,\nwhere the object-relation-object combinations (e.g.,\"man walking dog\") have\nnever been seen before. Next, we go one step further by presenting the models\nwith unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging\nword embeddings enables the models to output accurate spatial predictions,\nproving that the models acquire solid common sense spatial knowledge allowing\nfor such generalization.\n",
        "published": "2017",
        "authors": [
            "Guillem Collell",
            "Luc Van Gool",
            "Marie-Francine Moens"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.07871v2",
        "title": "FiLM: Visual Reasoning with a General Conditioning Layer",
        "abstract": "  We introduce a general-purpose conditioning method for neural networks called\nFiLM: Feature-wise Linear Modulation. FiLM layers influence neural network\ncomputation via a simple, feature-wise affine transformation based on\nconditioning information. We show that FiLM layers are highly effective for\nvisual reasoning - answering image-related questions which require a\nmulti-step, high-level process - a task which has proven difficult for standard\ndeep learning methods that do not explicitly model reasoning. Specifically, we\nshow on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error\nfor the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are\nrobust to ablations and architectural modifications, and 4) generalize well to\nchallenging, new data from few examples or even zero-shot.\n",
        "published": "2017",
        "authors": [
            "Ethan Perez",
            "Florian Strub",
            "Harm de Vries",
            "Vincent Dumoulin",
            "Aaron Courville"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.08150v1",
        "title": "Data-Driven Dynamic Decision Models",
        "abstract": "  This article outlines a method for automatically generating models of dynamic\ndecision-making that both have strong predictive power and are interpretable in\nhuman terms. This is useful for designing empirically grounded agent-based\nsimulations and for gaining direct insight into observed dynamic processes. We\nuse an efficient model representation and a genetic algorithm-based estimation\nprocess to generate simple approximations that explain most of the structure of\ncomplex stochastic processes. This method, implemented in C++ and R, scales\nwell to large data sets. We apply our methods to empirical data from human\nsubjects game experiments and international relations. We also demonstrate the\nmethod's ability to recover known data-generating processes by simulating data\nwith agent-based models and correctly deriving the underlying decision models\nfor multiple agent models and degrees of stochasticity.\n",
        "published": "2016",
        "authors": [
            "John J. Nay",
            "Jonathan M. Gilligan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.07496v1",
        "title": "Is a Picture Worth Ten Thousand Words in a Review Dataset?",
        "abstract": "  While textual reviews have become prominent in many recommendation-based\nsystems, automated frameworks to provide relevant visual cues against text\nreviews where pictures are not available is a new form of task confronted by\ndata mining and machine learning researchers. Suggestions of pictures that are\nrelevant to the content of a review could significantly benefit the users by\nincreasing the effectiveness of a review. We propose a deep learning-based\nframework to automatically: (1) tag the images available in a review dataset,\n(2) generate a caption for each image that does not have one, and (3) enhance\neach review by recommending relevant images that might not be uploaded by the\ncorresponding reviewer. We evaluate the proposed framework using the Yelp\nChallenge Dataset. While a subset of the images in this particular dataset are\ncorrectly captioned, the majority of the pictures do not have any associated\ntext. Moreover, there is no mapping between reviews and images. Each image has\na corresponding business-tag where the picture was taken, though. The overall\ndata setting and unavailability of crucial pieces required for a mapping make\nthe problem of recommending images for reviews a major challenge. Qualitative\nand quantitative evaluations indicate that our proposed framework provides high\nquality enhancements through automatic captioning, tagging, and recommendation\nfor mapping reviews and images.\n",
        "published": "2016",
        "authors": [
            "Roberto Camacho Barranco",
            "Laura M. Rodriguez",
            "Rebecca Urbina",
            "M. Shahriar Hossain"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.01262v1",
        "title": "SABER: Data-Driven Motion Planner for Autonomously Navigating\n  Heterogeneous Robots",
        "abstract": "  We present an end-to-end online motion planning framework that uses a\ndata-driven approach to navigate a heterogeneous robot team towards a global\ngoal while avoiding obstacles in uncertain environments. First, we use\nstochastic model predictive control (SMPC) to calculate control inputs that\nsatisfy robot dynamics, and consider uncertainty during obstacle avoidance with\nchance constraints. Second, recurrent neural networks are used to provide a\nquick estimate of future state uncertainty considered in the SMPC finite-time\nhorizon solution, which are trained on uncertainty outputs of various\nsimultaneous localization and mapping algorithms. When two or more robots are\nin communication range, these uncertainties are then updated using a\ndistributed Kalman filtering approach. Lastly, a Deep Q-learning agent is\nemployed to serve as a high-level path planner, providing the SMPC with target\npositions that move the robots towards a desired global goal. Our complete\nmethods are demonstrated on a ground and aerial robot simultaneously (code\navailable at: https://github.com/AlexS28/SABER).\n",
        "published": "2021",
        "authors": [
            "Alexander Schperberg",
            "Stephanie Tsuei",
            "Stefano Soatto",
            "Dennis Hong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.05733v2",
        "title": "Choosing Smartly: Adaptive Multimodal Fusion for Object Detection in\n  Changing Environments",
        "abstract": "  Object detection is an essential task for autonomous robots operating in\ndynamic and changing environments. A robot should be able to detect objects in\nthe presence of sensor noise that can be induced by changing lighting\nconditions for cameras and false depth readings for range sensors, especially\nRGB-D cameras. To tackle these challenges, we propose a novel adaptive fusion\napproach for object detection that learns weighting the predictions of\ndifferent sensor modalities in an online manner. Our approach is based on a\nmixture of convolutional neural network (CNN) experts and incorporates multiple\nmodalities including appearance, depth and motion. We test our method in\nextensive robot experiments, in which we detect people in a combined indoor and\noutdoor scenario from RGB-D data, and we demonstrate that our method can adapt\nto harsh lighting changes and severe camera motion blur. Furthermore, we\npresent a new RGB-D dataset for people detection in mixed in- and outdoor\nenvironments, recorded with a mobile robot. Code, pretrained models and dataset\nare available at http://adaptivefusion.cs.uni-freiburg.de\n",
        "published": "2017",
        "authors": [
            "Oier Mees",
            "Andreas Eitel",
            "Wolfram Burgard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.08740v2",
        "title": "Speeding-up Object Detection Training for Robotics with FALKON",
        "abstract": "  Latest deep learning methods for object detection provide remarkable\nperformance, but have limits when used in robotic applications. One of the most\nrelevant issues is the long training time, which is due to the large size and\nimbalance of the associated training sets, characterized by few positive and a\nlarge number of negative examples (i.e. background). Proposed approaches are\nbased on end-to-end learning by back-propagation [22] or kernel methods trained\nwith Hard Negatives Mining on top of deep features [8]. These solutions are\neffective, but prohibitively slow for on-line applications. In this paper we\npropose a novel pipeline for object detection that overcomes this problem and\nprovides comparable performance, with a 60x training speedup. Our pipeline\ncombines (i) the Region Proposal Network and the deep feature extractor from\n[22] to efficiently select candidate RoIs and encode them into powerful\nrepresentations, with (ii) the FALKON [23] algorithm, a novel kernel-based\nmethod that allows fast training on large scale problems (millions of points).\nWe address the size and imbalance of training data by exploiting the stochastic\nsubsampling intrinsic into the method and a novel, fast, bootstrapping\napproach. We assess the effectiveness of the approach on a standard Computer\nVision dataset (PASCAL VOC 2007 [5]) and demonstrate its applicability to a\nreal robotic scenario with the iCubWorld Transformations [18] dataset.\n",
        "published": "2018",
        "authors": [
            "Elisa Maiettini",
            "Giulia Pasquale",
            "Lorenzo Rosasco",
            "Lorenzo Natale"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.08894v2",
        "title": "ClusterNet: 3D Instance Segmentation in RGB-D Images",
        "abstract": "  We propose a method for instance-level segmentation that uses RGB-D data as\ninput and provides detailed information about the location, geometry and number\nof individual objects in the scene. This level of understanding is fundamental\nfor autonomous robots. It enables safe and robust decision-making under the\nlarge uncertainty of the real-world. In our model, we propose to use the first\nand second order moments of the object occupancy function to represent an\nobject instance. We train an hourglass Deep Neural Network (DNN) where each\npixel in the output votes for the 3D position of the corresponding object\ncenter and for the object's size and pose. The final instance segmentation is\nachieved through clustering in the space of moments. The object-centric\ntraining loss is defined on the output of the clustering. Our method\noutperforms the state-of-the-art instance segmentation method on our\nsynthesized dataset. We show that our method generalizes well on real-world\ndata achieving visually better segmentation results.\n",
        "published": "2018",
        "authors": [
            "Lin Shao",
            "Ye Tian",
            "Jeannette Bohg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.05155v1",
        "title": "Learning to Explore using Active Neural SLAM",
        "abstract": "  This work presents a modular and hierarchical approach to learn policies for\nexploring 3D environments, called `Active Neural SLAM'. Our approach leverages\nthe strengths of both classical and learning-based methods, by using analytical\npath planners with learned SLAM module, and global and local policies. The use\nof learning provides flexibility with respect to input modalities (in the SLAM\nmodule), leverages structural regularities of the world (in global policies),\nand provides robustness to errors in state estimation (in local policies). Such\nuse of learning within each module retains its benefits, while at the same\ntime, hierarchical decomposition and modular training allow us to sidestep the\nhigh sample complexities associated with training end-to-end policies. Our\nexperiments in visually and physically realistic simulated 3D environments\ndemonstrate the effectiveness of our approach over past learning and\ngeometry-based approaches. The proposed model can also be easily transferred to\nthe PointGoal task and was the winning entry of the CVPR 2019 Habitat PointGoal\nNavigation Challenge.\n",
        "published": "2020",
        "authors": [
            "Devendra Singh Chaplot",
            "Dhiraj Gandhi",
            "Saurabh Gupta",
            "Abhinav Gupta",
            "Ruslan Salakhutdinov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.02249v2",
        "title": "Uncertainty-Aware Learning from Demonstration using Mixture Density\n  Networks with Sampling-Free Variance Modeling",
        "abstract": "  In this paper, we propose an uncertainty-aware learning from demonstration\nmethod by presenting a novel uncertainty estimation method utilizing a mixture\ndensity network appropriate for modeling complex and noisy human behaviors. The\nproposed uncertainty acquisition can be done with a single forward path without\nMonte Carlo sampling and is suitable for real-time robotics applications. The\nproperties of the proposed uncertainty measure are analyzed through three\ndifferent synthetic examples, absence of data, heavy measurement noise, and\ncomposition of functions scenarios. We show that each case can be distinguished\nusing the proposed uncertainty measure and presented an uncertainty-aware\nlearn- ing from demonstration method of an autonomous driving using this\nproperty. The proposed uncertainty-aware learning from demonstration method\noutperforms other compared methods in terms of safety using a complex\nreal-world driving dataset.\n",
        "published": "2017",
        "authors": [
            "Sungjoon Choi",
            "Kyungjae Lee",
            "Sungbin Lim",
            "Songhwai Oh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.08960v2",
        "title": "Short-Term Prediction and Multi-Camera Fusion on Semantic Grids",
        "abstract": "  An environment representation (ER) is a substantial part of every autonomous\nsystem. It introduces a common interface between perception and other system\ncomponents, such as decision making, and allows downstream algorithms to deal\nwith abstracted data without knowledge of the used sensor. In this work, we\npropose and evaluate a novel architecture that generates an egocentric,\ngrid-based, predictive, and semantically-interpretable ER. In particular, we\nprovide a proof of concept for the spatio-temporal fusion of multiple camera\nsequences and short-term prediction in such an ER. Our design utilizes a strong\nsemantic segmentation network together with depth and egomotion estimates to\nfirst extract semantic information from multiple camera streams and then\ntransform these separately into egocentric temporally-aligned bird's-eye view\ngrids. A deep encoder-decoder network is trained to fuse a stack of these grids\ninto a unified semantic grid representation and to predict the dynamics of its\nsurrounding. We evaluate this representation on real-world sequences of the\nCityscapes dataset and show that our architecture can make accurate predictions\nin complex sensor fusion scenarios and significantly outperforms a model-driven\nbaseline in a category-based evaluation.\n",
        "published": "2019",
        "authors": [
            "Lukas Hoyer",
            "Patrick Kesper",
            "Anna Khoreva",
            "Volker Fischer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.06740v2",
        "title": "Measuring Visual Generalization in Continuous Control from Pixels",
        "abstract": "  Self-supervised learning and data augmentation have significantly reduced the\nperformance gap between state and image-based reinforcement learning agents in\ncontinuous control tasks. However, it is still unclear whether current\ntechniques can face a variety of visual conditions required by real-world\nenvironments. We propose a challenging benchmark that tests agents' visual\ngeneralization by adding graphical variety to existing continuous control\ndomains. Our empirical analysis shows that current methods struggle to\ngeneralize across a diverse set of visual changes, and we examine the specific\nfactors of variation that make these tasks difficult. We find that data\naugmentation techniques outperform self-supervised learning approaches and that\nmore significant image transformations provide better visual generalization\n\\footnote{The benchmark and our augmented actor-critic implementation are\nopen-sourced @ https://github.com/QData/dmc_remastered)\n",
        "published": "2020",
        "authors": [
            "Jake Grigsby",
            "Yanjun Qi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.10324v3",
        "title": "VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning",
        "abstract": "  We propose VRL3, a powerful data-driven framework with a simple design for\nsolving challenging visual deep reinforcement learning (DRL) tasks. We analyze\na number of major obstacles in taking a data-driven approach, and present a\nsuite of design principles, novel findings, and critical insights about\ndata-driven visual DRL. Our framework has three stages: in stage 1, we leverage\nnon-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations;\nin stage 2, we use offline RL data (e.g. a limited number of expert\ndemonstrations) to convert the task-agnostic representations into more powerful\ntask-specific representations; in stage 3, we fine-tune the agent with online\nRL. On a set of challenging hand manipulation tasks with sparse reward and\nrealistic visual inputs, compared to the previous SOTA, VRL3 achieves an\naverage of 780% better sample efficiency. And on the hardest task, VRL3 is\n1220% more sample efficient (2440% when using a wider encoder) and solves the\ntask with only 10% of the computation. These significant results clearly\ndemonstrate the great potential of data-driven deep reinforcement learning.\n",
        "published": "2022",
        "authors": [
            "Che Wang",
            "Xufang Luo",
            "Keith Ross",
            "Dongsheng Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.11092v1",
        "title": "ReorientBot: Learning Object Reorientation for Specific-Posed Placement",
        "abstract": "  Robots need the capability of placing objects in arbitrary, specific poses to\nrearrange the world and achieve various valuable tasks. Object reorientation\nplays a crucial role in this as objects may not initially be oriented such that\nthe robot can grasp and then immediately place them in a specific goal pose. In\nthis work, we present a vision-based manipulation system, ReorientBot, which\nconsists of 1) visual scene understanding with pose estimation and volumetric\nreconstruction using an onboard RGB-D camera; 2) learned waypoint selection for\nsuccessful and efficient motion generation for reorientation; 3) traditional\nmotion planning to generate a collision-free trajectory from the selected\nwaypoints. We evaluate our method using the YCB objects in both simulation and\nthe real world, achieving 93% overall success, 81% improvement in success rate,\nand 22% improvement in execution time compared to a heuristic approach. We\ndemonstrate extended multi-object rearrangement showing the general capability\nof the system.\n",
        "published": "2022",
        "authors": [
            "Kentaro Wada",
            "Stephen James",
            "Andrew J. Davison"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.05268v1",
        "title": "Self-Supervised Visual Planning with Temporal Skip Connections",
        "abstract": "  In order to autonomously learn wide repertoires of complex skills, robots\nmust be able to learn from their own autonomously collected data, without human\nsupervision. One learning signal that is always available for autonomously\ncollected data is prediction: if a robot can learn to predict the future, it\ncan use this predictive model to take actions to produce desired outcomes, such\nas moving an object to a particular location. However, in complex open-world\nscenarios, designing a representation for prediction is difficult. In this\nwork, we instead aim to enable self-supervised robotic learning through direct\nvideo prediction: instead of attempting to design a good representation, we\ndirectly predict what the robot will see next, and then use this model to\nachieve desired goals. A key challenge in video prediction for robotic\nmanipulation is handling complex spatial arrangements such as occlusions. To\nthat end, we introduce a video prediction model that can keep track of objects\nthrough occlusion by incorporating temporal skip-connections. Together with a\nnovel planning criterion and action space formulation, we demonstrate that this\nmodel substantially outperforms prior work on video prediction-based control.\nOur results show manipulation of objects not seen during training, handling\nmultiple objects, and pushing objects around obstructions. These results\nrepresent a significant advance in the range and complexity of skills that can\nbe performed entirely with self-supervised robotic learning.\n",
        "published": "2017",
        "authors": [
            "Frederik Ebert",
            "Chelsea Finn",
            "Alex X. Lee",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.01849v1",
        "title": "SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation",
        "abstract": "  Recent techniques in self-supervised monocular depth estimation are\napproaching the performance of supervised methods, but operate in low\nresolution only. We show that high resolution is key towards high-fidelity\nself-supervised monocular depth prediction. Inspired by recent deep learning\nmethods for Single-Image Super-Resolution, we propose a sub-pixel convolutional\nlayer extension for depth super-resolution that accurately synthesizes\nhigh-resolution disparities from their corresponding low-resolution\nconvolutional features. In addition, we introduce a differentiable\nflip-augmentation layer that accurately fuses predictions from the image and\nits horizontally flipped version, reducing the effect of left and right shadow\nregions generated in the disparity map due to occlusions. Both contributions\nprovide significant performance gains over the state-of-the-art in\nself-supervised depth and pose estimation on the public KITTI benchmark. A\nvideo of our approach can be found at https://youtu.be/jKNgBeBMx0I.\n",
        "published": "2018",
        "authors": [
            "Sudeep Pillai",
            "Rares Ambrus",
            "Adrien Gaidon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.08405v3",
        "title": "Event-based Vision: A Survey",
        "abstract": "  Event cameras are bio-inspired sensors that differ from conventional frame\ncameras: Instead of capturing images at a fixed rate, they asynchronously\nmeasure per-pixel brightness changes, and output a stream of events that encode\nthe time, location and sign of the brightness changes. Event cameras offer\nattractive properties compared to traditional cameras: high temporal resolution\n(in the order of microseconds), very high dynamic range (140 dB vs. 60 dB), low\npower consumption, and high pixel bandwidth (on the order of kHz) resulting in\nreduced motion blur. Hence, event cameras have a large potential for robotics\nand computer vision in challenging scenarios for traditional cameras, such as\nlow-latency, high speed, and high dynamic range. However, novel methods are\nrequired to process the unconventional output of these sensors in order to\nunlock their potential. This paper provides a comprehensive overview of the\nemerging field of event-based vision, with a focus on the applications and the\nalgorithms developed to unlock the outstanding properties of event cameras. We\npresent event cameras from their working principle, the actual sensors that are\navailable and the tasks that they have been used for, from low-level vision\n(feature detection and tracking, optic flow, etc.) to high-level vision\n(reconstruction, segmentation, recognition). We also discuss the techniques\ndeveloped to process events, including learning-based techniques, as well as\nspecialized processors for these novel sensors, such as spiking neural\nnetworks. Additionally, we highlight the challenges that remain to be tackled\nand the opportunities that lie ahead in the search for a more efficient,\nbio-inspired way for machines to perceive and interact with the world.\n",
        "published": "2019",
        "authors": [
            "Guillermo Gallego",
            "Tobi Delbruck",
            "Garrick Orchard",
            "Chiara Bartolozzi",
            "Brian Taba",
            "Andrea Censi",
            "Stefan Leutenegger",
            "Andrew Davison",
            "Joerg Conradt",
            "Kostas Daniilidis",
            "Davide Scaramuzza"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.03173v2",
        "title": "Ego-Pose Estimation and Forecasting as Real-Time PD Control",
        "abstract": "  We propose the use of a proportional-derivative (PD) control based policy\nlearned via reinforcement learning (RL) to estimate and forecast 3D human pose\nfrom egocentric videos. The method learns directly from unsegmented egocentric\nvideos and motion capture data consisting of various complex human motions\n(e.g., crouching, hopping, bending, and motion transitions). We propose a\nvideo-conditioned recurrent control technique to forecast physically-valid and\nstable future motions of arbitrary length. We also introduce a value function\nbased fail-safe mechanism which enables our method to run as a single pass\nalgorithm over the video data. Experiments with both controlled and in-the-wild\ndata show that our approach outperforms previous art in both quantitative\nmetrics and visual quality of the motions, and is also robust enough to\ntransfer directly to real-world scenarios. Additionally, our time analysis\nshows that the combined use of our pose estimation and forecasting can run at\n30 FPS, making it suitable for real-time applications.\n",
        "published": "2019",
        "authors": [
            "Ye Yuan",
            "Kris Kitani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.13072v1",
        "title": "Regression Planning Networks",
        "abstract": "  Recent learning-to-plan methods have shown promising results on planning\ndirectly from observation space. Yet, their ability to plan for long-horizon\ntasks is limited by the accuracy of the prediction model. On the other hand,\nclassical symbolic planners show remarkable capabilities in solving\nlong-horizon tasks, but they require predefined symbolic rules and symbolic\nstates, restricting their real-world applicability. In this work, we combine\nthe benefits of these two paradigms and propose a learning-to-plan method that\ncan directly generate a long-term symbolic plan conditioned on high-dimensional\nobservations. We borrow the idea of regression (backward) planning from\nclassical planning literature and introduce Regression Planning Networks (RPN),\na neural network architecture that plans backward starting at a task goal and\ngenerates a sequence of intermediate goals that reaches the current\nobservation. We show that our model not only inherits many favorable traits\nfrom symbolic planning, e.g., the ability to solve previously unseen tasks but\nalso can learn from visual inputs in an end-to-end manner. We evaluate the\ncapabilities of RPN in a grid world environment and a simulated 3D kitchen\nenvironment featuring complex visual scenes and long task horizons, and show\nthat it achieves near-optimal performance in completely new task instances.\n",
        "published": "2019",
        "authors": [
            "Danfei Xu",
            "Roberto Mart\u00edn-Mart\u00edn",
            "De-An Huang",
            "Yuke Zhu",
            "Silvio Savarese",
            "Li Fei-Fei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.03301v1",
        "title": "Sensors, Safety Models and A System-Level Approach to Safe and Scalable\n  Automated Vehicles",
        "abstract": "  When considering the accuracy of sensors in an automated vehicle (AV), it is\nnot sufficient to evaluate the performance of any given sensor in isolation.\nRather, the performance of any individual sensor must be considered in the\ncontext of the overall system design. Techniques like redundancy and different\nsensing modalities can reduce the chances of a sensing failure. Additionally,\nthe use of safety models is essential to understanding whether any particular\nsensing failure is relevant. Only when the entire system design is taken into\naccount can one properly understand the meaning of safety-relevant sensing\nfailures in an AV. In this paper, we will consider what should actually\nconstitute a sensing failure, how safety models play an important role in\nmitigating potential failures, how a system-level approach to safety will\ndeliver a safe and scalable AV, and what an acceptable sensing failure rate\nshould be considering the full picture of an AV's architecture.\n",
        "published": "2020",
        "authors": [
            "Jack Weast"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.12259v2",
        "title": "YolactEdge: Real-time Instance Segmentation on the Edge",
        "abstract": "  We propose YolactEdge, the first competitive instance segmentation approach\nthat runs on small edge devices at real-time speeds. Specifically, YolactEdge\nruns at up to 30.8 FPS on a Jetson AGX Xavier (and 172.7 FPS on an RTX 2080 Ti)\nwith a ResNet-101 backbone on 550x550 resolution images. To achieve this, we\nmake two improvements to the state-of-the-art image-based real-time method\nYOLACT: (1) applying TensorRT optimization while carefully trading off speed\nand accuracy, and (2) a novel feature warping module to exploit temporal\nredundancy in videos. Experiments on the YouTube VIS and MS COCO datasets\ndemonstrate that YolactEdge produces a 3-5x speed up over existing real-time\nmethods while producing competitive mask and box detection accuracy. We also\nconduct ablation studies to dissect our design choices and modules. Code and\nmodels are available at https://github.com/haotian-liu/yolact_edge.\n",
        "published": "2020",
        "authors": [
            "Haotian Liu",
            "Rafael A. Rivera Soto",
            "Fanyi Xiao",
            "Yong Jae Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.09942v1",
        "title": "Machine Vision based Sample-Tube Localization for Mars Sample Return",
        "abstract": "  A potential Mars Sample Return (MSR) architecture is being jointly studied by\nNASA and ESA. As currently envisioned, the MSR campaign consists of a series of\n3 missions: sample cache, fetch and return to Earth. In this paper, we focus on\nthe fetch part of the MSR, and more specifically the problem of autonomously\ndetecting and localizing sample tubes deposited on the Martian surface. Towards\nthis end, we study two machine-vision based approaches: First, a\ngeometry-driven approach based on template matching that uses hard-coded\nfilters and a 3D shape model of the tube; and second, a data-driven approach\nbased on convolutional neural networks (CNNs) and learned features.\nFurthermore, we present a large benchmark dataset of sample-tube images,\ncollected in representative outdoor environments and annotated with ground\ntruth segmentation masks and locations. The dataset was acquired systematically\nacross different terrain, illumination conditions and dust-coverage; and\nbenchmarking was performed to study the feasibility of each approach, their\nrelative strengths and weaknesses, and robustness in the presence of adverse\nenvironmental conditions.\n",
        "published": "2021",
        "authors": [
            "Shreyansh Daftry",
            "Barry Ridge",
            "William Seto",
            "Tu-Hoa Pham",
            "Peter Ilhardt",
            "Gerard Maggiolino",
            "Mark Van der Merwe",
            "Alex Brinkman",
            "John Mayo",
            "Eric Kulczyski",
            "Renaud Detry"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.05969v3",
        "title": "Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation",
        "abstract": "  We propose a method for object-aware 3D egocentric pose estimation that\ntightly integrates kinematics modeling, dynamics modeling, and scene object\ninformation. Unlike prior kinematics or dynamics-based approaches where the two\ncomponents are used disjointly, we synergize the two approaches via\ndynamics-regulated training. At each timestep, a kinematic model is used to\nprovide a target pose using video evidence and simulation state. Then, a\nprelearned dynamics model attempts to mimic the kinematic pose in a physics\nsimulator. By comparing the pose instructed by the kinematic model against the\npose generated by the dynamics model, we can use their misalignment to further\nimprove the kinematic model. By factoring in the 6DoF pose of objects (e.g.,\nchairs, boxes) in the scene, we demonstrate for the first time, the ability to\nestimate physically-plausible 3D human-object interactions using a single\nwearable camera. We evaluate our egocentric pose estimation method in both\ncontrolled laboratory settings and real-world scenarios.\n",
        "published": "2021",
        "authors": [
            "Zhengyi Luo",
            "Ryo Hachiuma",
            "Ye Yuan",
            "Kris Kitani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.00210v2",
        "title": "Mastering Atari Games with Limited Data",
        "abstract": "  Reinforcement learning has achieved great success in many applications.\nHowever, sample efficiency remains a key challenge, with prominent methods\nrequiring millions (or even billions) of environment steps to train. Recently,\nthere has been significant progress in sample efficient image-based RL\nalgorithms; however, consistent human-level performance on the Atari game\nbenchmark remains an elusive goal. We propose a sample efficient model-based\nvisual RL algorithm built on MuZero, which we name EfficientZero. Our method\nachieves 194.3% mean human performance and 109.0% median performance on the\nAtari 100k benchmark with only two hours of real-time game experience and\noutperforms the state SAC in some tasks on the DMControl 100k benchmark. This\nis the first time an algorithm achieves super-human performance on Atari games\nwith such little data. EfficientZero's performance is also close to DQN's\nperformance at 200 million frames while we consume 500 times less data.\nEfficientZero's low sample complexity and high performance can bring RL closer\nto real-world applicability. We implement our algorithm in an\neasy-to-understand manner and it is available at\nhttps://github.com/YeWR/EfficientZero. We hope it will accelerate the research\nof MCTS-based RL algorithms in the wider community.\n",
        "published": "2021",
        "authors": [
            "Weirui Ye",
            "Shaohuai Liu",
            "Thanard Kurutach",
            "Pieter Abbeel",
            "Yang Gao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.01674v1",
        "title": "Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged\n  Robots",
        "abstract": "  Legged locomotion is commonly studied and expressed as a discrete set of gait\npatterns, like walk, trot, gallop, which are usually treated as given and\npre-programmed in legged robots for efficient locomotion at different speeds.\nHowever, fixing a set of pre-programmed gaits limits the generality of\nlocomotion. Recent animal motor studies show that these conventional gaits are\nonly prevalent in ideal flat terrain conditions while real-world locomotion is\nunstructured and more like bouts of intermittent steps. What principles could\nlead to both structured and unstructured patterns across mammals and how to\nsynthesize them in robots? In this work, we take an analysis-by-synthesis\napproach and learn to move by minimizing mechanical energy. We demonstrate that\nlearning to minimize energy consumption plays a key role in the emergence of\nnatural locomotion gaits at different speeds in real quadruped robots. The\nemergent gaits are structured in ideal terrains and look similar to that of\nhorses and sheep. The same approach leads to unstructured gaits in rough\nterrains which is consistent with the findings in animal motor control. We\nvalidate our hypothesis in both simulation and real hardware across natural\nterrains. Videos at https://energy-locomotion.github.io\n",
        "published": "2021",
        "authors": [
            "Zipeng Fu",
            "Ashish Kumar",
            "Jitendra Malik",
            "Deepak Pathak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.14693v3",
        "title": "SAGCI-System: Towards Sample-Efficient, Generalizable, Compositional,\n  and Incremental Robot Learning",
        "abstract": "  Building general-purpose robots to perform a diverse range of tasks in a\nlarge variety of environments in the physical world at the human level is\nextremely challenging. It requires the robot learning to be sample-efficient,\ngeneralizable, compositional, and incremental. In this work, we introduce a\nsystematic learning framework called SAGCI-system towards achieving these above\nfour requirements. Our system first takes the raw point clouds gathered by the\ncamera mounted on the robot's wrist as the inputs and produces initial modeling\nof the surrounding environment represented as a file of Unified Robot\nDescription Format (URDF). Our system adopts a learning-augmented\ndifferentiable simulation that loads the URDF. The robot then utilizes the\ninteractive perception to interact with the environment to online verify and\nmodify the URDF. Leveraging the differentiable simulation, we propose a\nmodel-based learning algorithm combining object-centric and robot-centric\nstages to efficiently produce policies to accomplish manipulation tasks. We\napply our system to perform articulated object manipulation tasks, both in the\nsimulation and the real world. Extensive experiments demonstrate the\neffectiveness of our proposed learning framework. Supplemental materials and\nvideos are available on https://sites.google.com/view/egci.\n",
        "published": "2021",
        "authors": [
            "Jun Lv",
            "Qiaojun Yu",
            "Lin Shao",
            "Wenhai Liu",
            "Wenqiang Xu",
            "Cewu Lu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.05124v1",
        "title": "Neural Descriptor Fields: SE(3)-Equivariant Object Representations for\n  Manipulation",
        "abstract": "  We present Neural Descriptor Fields (NDFs), an object representation that\nencodes both points and relative poses between an object and a target (such as\na robot gripper or a rack used for hanging) via category-level descriptors. We\nemploy this representation for object manipulation, where given a task\ndemonstration, we want to repeat the same task on a new object instance from\nthe same category. We propose to achieve this objective by searching (via\noptimization) for the pose whose descriptor matches that observed in the\ndemonstration. NDFs are conveniently trained in a self-supervised fashion via a\n3D auto-encoding task that does not rely on expert-labeled keypoints. Further,\nNDFs are SE(3)-equivariant, guaranteeing performance that generalizes across\nall possible 3D object translations and rotations. We demonstrate learning of\nmanipulation tasks from few (5-10) demonstrations both in simulation and on a\nreal robot. Our performance generalizes across both object instances and 6-DoF\nobject poses, and significantly outperforms a recent baseline that relies on 2D\ndescriptors. Project website: https://yilundu.github.io/ndf/.\n",
        "published": "2021",
        "authors": [
            "Anthony Simeonov",
            "Yilun Du",
            "Andrea Tagliasacchi",
            "Joshua B. Tenenbaum",
            "Alberto Rodriguez",
            "Pulkit Agrawal",
            "Vincent Sitzmann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.07662v4",
        "title": "What's in the Black Box? The False Negative Mechanisms Inside Object\n  Detectors",
        "abstract": "  In object detection, false negatives arise when a detector fails to detect a\ntarget object. To understand why object detectors produce false negatives, we\nidentify five 'false negative mechanisms', where each mechanism describes how a\nspecific component inside the detector architecture failed. Focusing on\ntwo-stage and one-stage anchor-box object detector architectures, we introduce\na framework for quantifying these false negative mechanisms. Using this\nframework, we investigate why Faster R-CNN and RetinaNet fail to detect objects\nin benchmark vision datasets and robotics datasets. We show that a detector's\nfalse negative mechanisms differ significantly between computer vision\nbenchmark datasets and robotics deployment scenarios. This has implications for\nthe translation of object detectors developed for benchmark datasets to\nrobotics applications. Code is publicly available at\nhttps://github.com/csiro-robotics/fn_mechanisms\n",
        "published": "2022",
        "authors": [
            "Dimity Miller",
            "Peyman Moghadam",
            "Mark Cox",
            "Matt Wildie",
            "Raja Jurdak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.13590v1",
        "title": "Computer Vision for Road Imaging and Pothole Detection: A\n  State-of-the-Art Review of Systems and Algorithms",
        "abstract": "  Computer vision algorithms have been prevalently utilized for 3-D road\nimaging and pothole detection for over two decades. Nonetheless, there is a\nlack of systematic survey articles on state-of-the-art (SoTA) computer vision\ntechniques, especially deep learning models, developed to tackle these\nproblems. This article first introduces the sensing systems employed for 2-D\nand 3-D road data acquisition, including camera(s), laser scanners, and\nMicrosoft Kinect. Afterward, it thoroughly and comprehensively reviews the SoTA\ncomputer vision algorithms, including (1) classical 2-D image processing, (2)\n3-D point cloud modeling and segmentation, and (3) machine/deep learning,\ndeveloped for road pothole detection. This article also discusses the existing\nchallenges and future development trends of computer vision-based road pothole\ndetection approaches: classical 2-D image processing-based and 3-D point cloud\nmodeling and segmentation-based approaches have already become history; and\nConvolutional neural networks (CNNs) have demonstrated compelling road pothole\ndetection results and are promising to break the bottleneck with the future\nadvances in self/un-supervised learning for multi-modal semantic segmentation.\nWe believe that this survey can serve as practical guidance for developing the\nnext-generation road condition assessment systems.\n",
        "published": "2022",
        "authors": [
            "Nachuan Ma",
            "Jiahe Fan",
            "Wenshuo Wang",
            "Jin Wu",
            "Yu Jiang",
            "Lihua Xie",
            "Rui Fan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.13678v1",
        "title": "Unified Simulation, Perception, and Generation of Human Behavior",
        "abstract": "  Understanding and modeling human behavior is fundamental to almost any\ncomputer vision and robotics applications that involve humans. In this thesis,\nwe take a holistic approach to human behavior modeling and tackle its three\nessential aspects -- simulation, perception, and generation. Throughout the\nthesis, we show how the three aspects are deeply connected and how utilizing\nand improving one aspect can greatly benefit the other aspects. We also discuss\nthe lessons learned and our vision for what is next for human behavior\nmodeling.\n",
        "published": "2022",
        "authors": [
            "Ye Yuan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.02111v2",
        "title": "Improved Orientation Estimation and Detection with Hybrid Object\n  Detection Networks for Automotive Radar",
        "abstract": "  This paper presents novel hybrid architectures that combine grid- and\npoint-based processing to improve the detection performance and orientation\nestimation of radar-based object detection networks. Purely grid-based\ndetection models operate on a bird's-eye-view (BEV) projection of the input\npoint cloud. These approaches suffer from a loss of detailed information\nthrough the discrete grid resolution. This applies in particular to radar\nobject detection, where relatively coarse grid resolutions are commonly used to\naccount for the sparsity of radar point clouds. In contrast, point-based models\nare not affected by this problem as they process point clouds without\ndiscretization. However, they generally exhibit worse detection performances\nthan grid-based methods.\n  We show that a point-based model can extract neighborhood features,\nleveraging the exact relative positions of points, before grid rendering. This\nhas significant benefits for a subsequent grid-based convolutional detection\nbackbone. In experiments on the public nuScenes dataset our hybrid architecture\nachieves improvements in terms of detection performance (19.7% higher mAP for\ncar class than next-best radar-only submission) and orientation estimates\n(11.5% relative orientation improvement) over networks from previous\nliterature.\n",
        "published": "2022",
        "authors": [
            "Michael Ulrich",
            "Sascha Braun",
            "Daniel K\u00f6hler",
            "Daniel Niederl\u00f6hner",
            "Florian Faion",
            "Claudius Gl\u00e4ser",
            "Holger Blume"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.00401v2",
        "title": "Autonomous Intraluminal Navigation of a Soft Robot using\n  Deep-Learning-based Visual Servoing",
        "abstract": "  Navigation inside luminal organs is an arduous task that requires\nnon-intuitive coordination between the movement of the operator's hand and the\ninformation obtained from the endoscopic video. The development of tools to\nautomate certain tasks could alleviate the physical and mental load of doctors\nduring interventions, allowing them to focus on diagnosis and decision-making\ntasks. In this paper, we present a synergic solution for intraluminal\nnavigation consisting of a 3D printed endoscopic soft robot that can move\nsafely inside luminal structures. Visual servoing, based on Convolutional\nNeural Networks (CNNs) is used to achieve the autonomous navigation task. The\nCNN is trained with phantoms and in-vivo data to segment the lumen, and a\nmodel-less approach is presented to control the movement in constrained\nenvironments. The proposed robot is validated in anatomical phantoms in\ndifferent path configurations. We analyze the movement of the robot using\ndifferent metrics such as task completion time, smoothness, error in the\nsteady-state, and mean and maximum error. We show that our method is suitable\nto navigate safely in hollow environments and conditions which are different\nthan the ones the network was originally trained on.\n",
        "published": "2022",
        "authors": [
            "Jorge F. Lazo",
            "Chun-Feng Lai",
            "Sara Moccia",
            "Benoit Rosa",
            "Michele Catellani",
            "Michel de Mathelin",
            "Giancarlo Ferrigno",
            "Paul Breedveld",
            "Jenny Dankelman",
            "Elena De Momi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.11079v2",
        "title": "Robot Active Neural Sensing and Planning in Unknown Cluttered\n  Environments",
        "abstract": "  Active sensing and planning in unknown, cluttered environments is an open\nchallenge for robots intending to provide home service, search and rescue,\nnarrow-passage inspection, and medical assistance. Although many active sensing\nmethods exist, they often consider open spaces, assume known settings, or\nmostly do not generalize to real-world scenarios. We present the active neural\nsensing approach that generates the kinematically feasible viewpoint sequences\nfor the robot manipulator with an in-hand camera to gather the minimum number\nof observations needed to reconstruct the underlying environment. Our framework\nactively collects the visual RGBD observations, aggregates them into scene\nrepresentation, and performs object shape inference to avoid unnecessary robot\ninteractions with the environment. We train our approach on synthetic data with\ndomain randomization and demonstrate its successful execution via sim-to-real\ntransfer in reconstructing narrow, covered, real-world cabinet environments\ncluttered with unknown objects. The natural cabinet scenarios impose\nsignificant challenges for robot motion and scene reconstruction due to\nsurrounding obstacles and low ambient lighting conditions. However, despite\nunfavorable settings, our method exhibits high performance compared to its\nbaselines in terms of various environment reconstruction metrics, including\nplanning speed, the number of viewpoints, and overall scene coverage.\n",
        "published": "2022",
        "authors": [
            "Hanwen Ren",
            "Ahmed H. Qureshi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.00767v2",
        "title": "Exploiting Proximity-Aware Tasks for Embodied Social Navigation",
        "abstract": "  Learning how to navigate among humans in an occluded and spatially\nconstrained indoor environment, is a key ability required to embodied agent to\nbe integrated into our society. In this paper, we propose an end-to-end\narchitecture that exploits Proximity-Aware Tasks (referred as to Risk and\nProximity Compass) to inject into a reinforcement learning navigation policy\nthe ability to infer common-sense social behaviors. To this end, our tasks\nexploit the notion of immediate and future dangers of collision. Furthermore,\nwe propose an evaluation protocol specifically designed for the Social\nNavigation Task in simulated environments. This is done to capture fine-grained\nfeatures and characteristics of the policy by analyzing the minimal unit of\nhuman-robot spatial interaction, called Encounter. We validate our approach on\nGibson4+ and Habitat-Matterport3D datasets.\n",
        "published": "2022",
        "authors": [
            "Enrico Cancelli",
            "Tommaso Campari",
            "Luciano Serafini",
            "Angel X. Chang",
            "Lamberto Ballan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.03844v1",
        "title": "Look Beyond Bias with Entropic Adversarial Data Augmentation",
        "abstract": "  Deep neural networks do not discriminate between spurious and causal\npatterns, and will only learn the most predictive ones while ignoring the\nothers. This shortcut learning behaviour is detrimental to a network's ability\nto generalize to an unknown test-time distribution in which the spurious\ncorrelations do not hold anymore. Debiasing methods were developed to make\nnetworks robust to such spurious biases but require to know in advance if a\ndataset is biased and make heavy use of minority counterexamples that do not\ndisplay the majority bias of their class. In this paper, we argue that such\nsamples should not be necessarily needed because the ''hidden'' causal\ninformation is often also contained in biased images. To study this idea, we\npropose 3 publicly released synthetic classification benchmarks, exhibiting\npredictive classification shortcuts, each of a different and challenging\nnature, without any minority samples acting as counterexamples. First, we\ninvestigate the effectiveness of several state-of-the-art strategies on our\nbenchmarks and show that they do not yield satisfying results on them. Then, we\npropose an architecture able to succeed on our benchmarks, despite their\nunusual properties, using an entropic adversarial data augmentation training\nscheme. An encoder-decoder architecture is tasked to produce images that are\nnot recognized by a classifier, by maximizing the conditional entropy of its\noutputs, and keep as much as possible of the initial content. A precise control\nof the information destroyed, via a disentangling process, enables us to remove\nthe shortcut and leave everything else intact. Furthermore, results competitive\nwith the state-of-the-art on the BAR dataset ensure the applicability of our\nmethod in real-life situations.\n",
        "published": "2023",
        "authors": [
            "Thomas Duboudin",
            "Emmanuel Dellandr\u00e9a",
            "Corentin Abgrall",
            "Gilles H\u00e9naff",
            "Liming Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.13166v3",
        "title": "ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object\n  Navigation",
        "abstract": "  The ability to accurately locate and navigate to a specific object is a\ncrucial capability for embodied agents that operate in the real world and\ninteract with objects to complete tasks. Such object navigation tasks usually\nrequire large-scale training in visual environments with labeled objects, which\ngeneralizes poorly to novel objects in unknown environments. In this work, we\npresent a novel zero-shot object navigation method, Exploration with Soft\nCommonsense constraints (ESC), that transfers commonsense knowledge in\npre-trained models to open-world object navigation without any navigation\nexperience nor any other training on the visual environments. First, ESC\nleverages a pre-trained vision and language model for open-world prompt-based\ngrounding and a pre-trained commonsense language model for room and object\nreasoning. Then ESC converts commonsense knowledge into navigation actions by\nmodeling it as soft logic predicates for efficient exploration. Extensive\nexperiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method\nimproves significantly over baselines, and achieves new state-of-the-art\nresults for zero-shot object navigation (e.g., 288% relative Success Rate\nimprovement than CoW on MP3D).\n",
        "published": "2023",
        "authors": [
            "Kaiwen Zhou",
            "Kaizhi Zheng",
            "Connor Pryor",
            "Yilin Shen",
            "Hongxia Jin",
            "Lise Getoor",
            "Xin Eric Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.06195v1",
        "title": "Exploring Navigation Maps for Learning-Based Motion Prediction",
        "abstract": "  The prediction of surrounding agents' motion is a key for safe autonomous\ndriving. In this paper, we explore navigation maps as an alternative to the\npredominant High Definition (HD) maps for learning-based motion prediction.\nNavigation maps provide topological and geometrical information on road-level,\nHD maps additionally have centimeter-accurate lane-level information. As a\nresult, HD maps are costly and time-consuming to obtain, while navigation maps\nwith near-global coverage are freely available. We describe an approach to\nintegrate navigation maps into learning-based motion prediction models. To\nexploit locally available HD maps during training, we additionally propose a\nmodel-agnostic method for knowledge distillation. In experiments on the\npublicly available Argoverse dataset with navigation maps obtained from\nOpenStreetMap, our approach shows a significant improvement over not using a\nmap at all. Combined with our method for knowledge distillation, we achieve\nresults that are close to the original HD map-reliant models. Our publicly\navailable navigation map API for Argoverse enables researchers to develop and\nevaluate their own approaches using navigation maps.\n",
        "published": "2023",
        "authors": [
            "Julian Schmidt",
            "Julian Jordan",
            "Franz Gritschneder",
            "Thomas Monninger",
            "Klaus Dietmayer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.07308v2",
        "title": "NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial\n  Understanding with Objects",
        "abstract": "  We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and\nillustrate how it supports object SLAM for consistent spatial understanding\nwith long-term scene changes. NeuSE is a set of latent object embeddings\ncreated from partial object observations. It serves as a compact point cloud\nsurrogate for complete object models, encoding full shape information while\ntransforming SE(3)-equivariantly in tandem with the object in the physical\nworld. With NeuSE, relative frame transforms can be directly derived from\ninferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape\nand pose characterization, can operate independently or in conjunction with\ntypical SLAM systems. It directly infers SE(3) camera pose constraints that are\ncompatible with general SLAM pose graph optimization, while also maintaining a\nlightweight object-centric map that adapts to real-world changes. Our approach\nis evaluated on synthetic and real-world sequences featuring changed objects\nand shows improved localization accuracy and change-aware mapping capability,\nwhen working either standalone or jointly with a common SLAM pipeline.\n",
        "published": "2023",
        "authors": [
            "Jiahui Fu",
            "Yilun Du",
            "Kurran Singh",
            "Joshua B. Tenenbaum",
            "John J. Leonard"
        ]
    }
]