[
    {
        "id": "http://arxiv.org/abs/1806.08874v1",
        "title": "The Foundations of Deep Learning with a Path Towards General\n  Intelligence",
        "abstract": "  Like any field of empirical science, AI may be approached axiomatically. We\nformulate requirements for a general-purpose, human-level AI system in terms of\npostulates. We review the methodology of deep learning, examining the explicit\nand tacit assumptions in deep learning research. Deep Learning methodology\nseeks to overcome limitations in traditional machine learning research as it\ncombines facets of model richness, generality, and practical applicability. The\nmethodology so far has produced outstanding results due to a productive synergy\nof function approximation, under plausible assumptions of irreducibility and\nthe efficiency of back-propagation family of algorithms. We examine these\nwinning traits of deep learning, and also observe the various known failure\nmodes of deep learning. We conclude by giving recommendations on how to extend\ndeep learning methodology to cover the postulates of general-purpose AI\nincluding modularity, and cognitive architecture. We also relate deep learning\nto advances in theoretical neuroscience research.\n",
        "published": "2018",
        "authors": [
            "Eray \u00d6zkural"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.10451v1",
        "title": "Adversarial Robustness of Deep Learning: Theory, Algorithms, and\n  Applications",
        "abstract": "  This tutorial aims to introduce the fundamentals of adversarial robustness of\ndeep learning, presenting a well-structured review of up-to-date techniques to\nassess the vulnerability of various types of deep learning models to\nadversarial examples. This tutorial will particularly highlight\nstate-of-the-art techniques in adversarial attacks and robustness verification\nof deep neural networks (DNNs). We will also introduce some effective\ncountermeasures to improve the robustness of deep learning models, with a\nparticular focus on adversarial training. We aim to provide a comprehensive\noverall picture about this emerging direction and enable the community to be\naware of the urgency and importance of designing robust deep learning models in\nsafety-critical data analytical applications, ultimately enabling the end-users\nto trust deep learning classifiers. We will also summarize potential research\ndirections concerning the adversarial robustness of deep learning, and its\npotential benefits to enable accountable and trustworthy deep learning-based\ndata analytical systems and applications.\n",
        "published": "2021",
        "authors": [
            "Wenjie Ruan",
            "Xinping Yi",
            "Xiaowei Huang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.03836v1",
        "title": "Topological Deep Learning: A Review of an Emerging Paradigm",
        "abstract": "  Topological data analysis (TDA) provides insight into data shape. The\nsummaries obtained by these methods are principled global descriptions of\nmulti-dimensional data whilst exhibiting stable properties such as robustness\nto deformation and noise. Such properties are desirable in deep learning\npipelines but they are typically obtained using non-TDA strategies. This is\npartly caused by the difficulty of combining TDA constructs (e.g. barcode and\npersistence diagrams) with current deep learning algorithms. Fortunately, we\nare now witnessing a growth of deep learning applications embracing\ntopologically-guided components. In this survey, we review the nascent field of\ntopological deep learning by first revisiting the core concepts of TDA. We then\nexplore how the use of TDA techniques has evolved over time to support deep\nlearning frameworks, and how they can be integrated into different aspects of\ndeep learning. Furthermore, we touch on TDA usage for analyzing existing deep\nmodels; deep topological analytics. Finally, we discuss the challenges and\nfuture prospects of topological deep learning.\n",
        "published": "2023",
        "authors": [
            "Ali Zia",
            "Abdelwahed Khamis",
            "James Nichols",
            "Zeeshan Hayder",
            "Vivien Rolland",
            "Lars Petersson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.18357v1",
        "title": "DeepSI: Interactive Deep Learning for Semantic Interaction",
        "abstract": "  In this paper, we design novel interactive deep learning methods to improve\nsemantic interactions in visual analytics applications. The ability of semantic\ninteraction to infer analysts' precise intents during sensemaking is dependent\non the quality of the underlying data representation. We propose the\n$\\text{DeepSI}_{\\text{finetune}}$ framework that integrates deep learning into\nthe human-in-the-loop interactive sensemaking pipeline, with two important\nproperties. First, deep learning extracts meaningful representations from raw\ndata, which improves semantic interaction inference. Second, semantic\ninteractions are exploited to fine-tune the deep learning representations,\nwhich then further improves semantic interaction inference. This feedback loop\nbetween human interaction and deep learning enables efficient learning of user-\nand task-specific representations. To evaluate the advantage of embedding the\ndeep learning within the semantic interaction loop, we compare\n$\\text{DeepSI}_{\\text{finetune}}$ against a state-of-the-art but more basic use\nof deep learning as only a feature extractor pre-processed outside of the\ninteractive loop. Results of two complementary studies, a human-centered\nqualitative case study and an algorithm-centered simulation-based quantitative\nexperiment, show that $\\text{DeepSI}_{\\text{finetune}}$ more accurately\ncaptures users' complex mental models with fewer interactions.\n",
        "published": "2023",
        "authors": [
            "Yali Bian",
            "Chris North"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1604.01662v4",
        "title": "A Survey on Bayesian Deep Learning",
        "abstract": "  A comprehensive artificial intelligence system needs to not only perceive the\nenvironment with different `senses' (e.g., seeing and hearing) but also infer\nthe world's conditional (or even causal) relations and corresponding\nuncertainty. The past decade has seen major advances in many perception tasks\nsuch as visual object recognition and speech recognition using deep learning\nmodels. For higher-level inference, however, probabilistic graphical models\nwith their Bayesian nature are still more powerful and flexible. In recent\nyears, Bayesian deep learning has emerged as a unified probabilistic framework\nto tightly integrate deep learning and Bayesian models. In this general\nframework, the perception of text or images using deep learning can boost the\nperformance of higher-level inference and in turn, the feedback from the\ninference process is able to enhance the perception of text or images. This\nsurvey provides a comprehensive introduction to Bayesian deep learning and\nreviews its recent applications on recommender systems, topic models, control,\netc. Besides, we also discuss the relationship and differences between Bayesian\ndeep learning and other related topics such as Bayesian treatment of neural\nnetworks. For a constantly updating project page, please refer to\nhttps://github.com/js05212/BayesianDeepLearning-Survey.\n",
        "published": "2016",
        "authors": [
            "Hao Wang",
            "Dit-Yan Yeung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.01423v1",
        "title": "Training Efficiency and Robustness in Deep Learning",
        "abstract": "  Deep Learning has revolutionized machine learning and artificial\nintelligence, achieving superhuman performance in several standard benchmarks.\nIt is well-known that deep learning models are inefficient to train; they learn\nby processing millions of training data multiple times and require powerful\ncomputational resources to process large batches of data in parallel at the\nsame time rather than sequentially. Deep learning models also have unexpected\nfailure modes; they can be fooled into misbehaviour, producing unexpectedly\nincorrect predictions.\n  In this thesis, we study approaches to improve the training efficiency and\nrobustness of deep learning models. In the context of learning visual-semantic\nembeddings, we find that prioritizing learning on more informative training\ndata increases convergence speed and improves generalization performance on\ntest data. We formalize a simple trick called hard negative mining as a\nmodification to the learning objective function with no computational overhead.\nNext, we seek improvements to optimization speed in general-purpose\noptimization methods in deep learning. We show that a redundancy-aware\nmodification to the sampling of training data improves the training speed and\ndevelops an efficient method for detecting the diversity of training signal,\nnamely, gradient clustering. Finally, we study adversarial robustness in deep\nlearning and approaches to achieve maximal adversarial robustness without\ntraining with additional data. For linear models, we prove guaranteed maximal\nrobustness achieved only by appropriate choice of the optimizer,\nregularization, or architecture.\n",
        "published": "2021",
        "authors": [
            "Fartash Faghri"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.01757v1",
        "title": "Predicting Mergers and Acquisitions using Graph-based Deep Learning",
        "abstract": "  The graph data structure is a staple in mathematics, yet graph-based machine\nlearning is a relatively green field within the domain of data science. Recent\nadvances in graph-based ML and open source implementations of relevant\nalgorithms are allowing researchers to apply methods created in academia to\nreal-world datasets. The goal of this project was to utilize a popular graph\nmachine learning framework, GraphSAGE, to predict mergers and acquisitions\n(M&A) of enterprise companies. The results were promising, as the model\npredicted with 81.79% accuracy on a validation dataset. Given the abundance of\ndata sources and algorithmic decision making within financial data science,\ngraph-based machine learning offers a performant, yet non-traditional approach\nto generating alpha.\n",
        "published": "2021",
        "authors": [
            "Keenan Venuti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.02726v3",
        "title": "Creativity and Machine Learning: A Survey",
        "abstract": "  There is a growing interest in the area of machine learning and creativity.\nThis survey presents an overview of the history and the state of the art of\ncomputational creativity theories, key machine learning techniques (including\ngenerative deep learning), and corresponding automatic evaluation methods.\nAfter presenting a critical discussion of the key contributions in this area,\nwe outline the current research challenges and emerging opportunities in this\nfield.\n",
        "published": "2021",
        "authors": [
            "Giorgio Franceschelli",
            "Mirco Musolesi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.09464v1",
        "title": "Explainable Artificial Intelligence and Machine Learning: A reality\n  rooted perspective",
        "abstract": "  We are used to the availability of big data generated in nearly all fields of\nscience as a consequence of technological progress. However, the analysis of\nsuch data possess vast challenges. One of these relates to the explainability\nof artificial intelligence (AI) or machine learning methods. Currently, many of\nsuch methods are non-transparent with respect to their working mechanism and\nfor this reason are called black box models, most notably deep learning\nmethods. However, it has been realized that this constitutes severe problems\nfor a number of fields including the health sciences and criminal justice and\narguments have been brought forward in favor of an explainable AI. In this\npaper, we do not assume the usual perspective presenting explainable AI as it\nshould be, but rather we provide a discussion what explainable AI can be. The\ndifference is that we do not present wishful thinking but reality grounded\nproperties in relation to a scientific theory beyond physics.\n",
        "published": "2020",
        "authors": [
            "Frank Emmert-Streib",
            "Olli Yli-Harja",
            "Matthias Dehmer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.14370v1",
        "title": "A Survey on the Role of Artificial Intelligence in the Prediction and\n  Diagnosis of Schizophrenia",
        "abstract": "  Machine learning is employed in healthcare to draw approximate conclusions\nregarding human diseases and mental health problems. Compared to older\ntraditional methods, it can help to analyze data more efficiently and produce\nbetter and more dependable results. Millions of people are affected by\nschizophrenia, which is a chronic mental disorder that can significantly impact\ntheir lives. Many machine learning algorithms have been developed to predict\nand prevent this disease, and they can potentially be implemented in the\ndiagnosis of individuals who have it. This survey aims to review papers that\nhave focused on the use of deep learning to detect and predict schizophrenia\nusing EEG signals, functional magnetic resonance imaging (fMRI), and diffusion\nmagnetic resonance imaging (dMRI). With our chosen search strategy, we assessed\nten publications from 2019 to 2022. All studies achieved successful predictions\nof more than 80%. This review provides summaries of the studies and compares\ntheir notable aspects. In the field of artificial intelligence (AI) and machine\nlearning (ML) for schizophrenia, significant advances have been made due to the\navailability of ML tools, and we are optimistic that this field will continue\nto grow.\n",
        "published": "2023",
        "authors": [
            "Narges Ramesh",
            "Yasmin Ghodsi",
            "Hamidreza Bolhasani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.04981v1",
        "title": "What AI can do for horse-racing ?",
        "abstract": "  Since the 1980s, machine learning has been widely used for horse-racing\npredictions, gradually expanding to where algorithms are now playing a huge\nrole in the betting market. Machine learning has changed the horse-racing\nbetting market over the last ten years, but main changes are still to come. The\nparadigm shift of neural networks (deep learning) may not only improve our\nability to simply predict the outcome of a race, but it will also certainly\nshake our entire way of thinking about horse-racing - and maybe more generally\nabout horses. Since 2012, deep learning provided more and more state-of-the-art\nresults in computer vision and now statistical learning or game theory. We\ndescribe how the convergence of the three machine learning fields (computer\nvision, statistical learning, and game theory) will be game-changers in the\nnext decade in our ability to predict and understand horse-racing. We consider\nthat horse-racing is a real world laboratory where we can work on the\nanimal-human interaction and build a non-anthropocentric Artificial\nIntelligence. We believe that this will lead us to understand the horses better\nand the interactions between animals and humans in general.\n",
        "published": "2022",
        "authors": [
            "Pierre Colle"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.08174v1",
        "title": "Towards understanding deep learning with the natural clustering prior",
        "abstract": "  The prior knowledge (a.k.a. priors) integrated into the design of a machine\nlearning system strongly influences its generalization abilities. In the\nspecific context of deep learning, some of these priors are poorly understood\nas they implicitly emerge from the successful heuristics and tentative\napproximations of biological brains involved in deep learning design. Through\nthe lens of supervised image classification problems, this thesis investigates\nthe implicit integration of a natural clustering prior composed of three\nstatements: (i) natural images exhibit a rich clustered structure, (ii) image\nclasses are composed of multiple clusters and (iii) each cluster contains\nexamples from a single class. The decomposition of classes into multiple\nclusters implies that supervised deep learning systems could benefit from\nunsupervised clustering to define appropriate decision boundaries. Hence, this\nthesis attempts to identify implicit clustering abilities, mechanisms and\nhyperparameters in deep learning systems and evaluate their relevance for\nexplaining the generalization abilities of these systems. We do so through an\nextensive empirical study of the training dynamics as well as the neuron- and\nlayer-level representations of deep neural networks. The resulting collection\nof experiments provides preliminary evidence for the relevance of the natural\nclustering prior for understanding deep learning.\n",
        "published": "2022",
        "authors": [
            "Simon Carbonnelle"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1501.04413v1",
        "title": "Statistical-mechanical analysis of pre-training and fine tuning in deep\n  learning",
        "abstract": "  In this paper, we present a statistical-mechanical analysis of deep learning.\nWe elucidate some of the essential components of deep learning---pre-training\nby unsupervised learning and fine tuning by supervised learning. We formulate\nthe extraction of features from the training data as a margin criterion in a\nhigh-dimensional feature-vector space. The self-organized classifier is then\nsupplied with small amounts of labelled data, as in deep learning. Although we\nemploy a simple single-layer perceptron model, rather than directly analyzing a\nmulti-layer neural network, we find a nontrivial phase transition that is\ndependent on the number of unlabelled data in the generalization error of the\nresultant classifier. In this sense, we evaluate the efficacy of the\nunsupervised learning component of deep learning. The analysis is performed by\nthe replica method, which is a sophisticated tool in statistical mechanics. We\nvalidate our result in the manner of deep learning, using a simple iterative\nalgorithm to learn the weight vector on the basis of belief propagation.\n",
        "published": "2015",
        "authors": [
            "Masayuki Ohzeki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.02282v2",
        "title": "Creativity of Deep Learning: Conceptualization and Assessment",
        "abstract": "  While the potential of deep learning(DL) for automating simple tasks is\nalready well explored, recent research started investigating the use of deep\nlearning for creative design, both for complete artifact creation and\nsupporting humans in the creation process. In this paper, we use insights from\ncomputational creativity to conceptualize and assess current applications of\ngenerative deep learning in creative domains identified in a literature review.\nWe highlight parallels between current systems and different models of human\ncreativity as well as their shortcomings. While deep learning yields results of\nhigh value, such as high quality images, their novelity is typically limited\ndue to multiple reasons such a being tied to a conceptual space defined by\ntraining data and humans. Current DL methods also do not allow for changes in\nthe internal problem representation and they lack the capability to identify\nconnections across highly different domains, both of which are seen as major\ndrivers of human creativity.\n",
        "published": "2020",
        "authors": [
            "Johannes Schneider",
            "Marcus Basalla"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.15754v1",
        "title": "Limitations of Deep Neural Networks: a discussion of G. Marcus' critical\n  appraisal of deep learning",
        "abstract": "  Deep neural networks have triggered a revolution in artificial intelligence,\nhaving been applied with great results in medical imaging, semi-autonomous\nvehicles, ecommerce, genetics research, speech recognition, particle physics,\nexperimental art, economic forecasting, environmental science, industrial\nmanufacturing, and a wide variety of applications in nearly every field. This\nsudden success, though, may have intoxicated the research community and blinded\nthem to the potential pitfalls of assigning deep learning a higher status than\nwarranted. Also, research directed at alleviating the weaknesses of deep\nlearning may seem less attractive to scientists and engineers, who focus on the\nlow-hanging fruit of finding more and more applications for deep learning\nmodels, thus letting short-term benefits hamper long-term scientific progress.\nGary Marcus wrote a paper entitled Deep Learning: A Critical Appraisal, and\nhere we discuss Marcus' core ideas, as well as attempt a general assessment of\nthe subject. This study examines some of the limitations of deep neural\nnetworks, with the intention of pointing towards potential paths for future\nresearch, and of clearing up some metaphysical misconceptions, held by numerous\nresearchers, that may misdirect them.\n",
        "published": "2020",
        "authors": [
            "Stefanos Tsimenidis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.16540v2",
        "title": "Mind the gap: Challenges of deep learning approaches to Theory of Mind",
        "abstract": "  Theory of Mind is an essential ability of humans to infer the mental states\nof others. Here we provide a coherent summary of the potential, current\nprogress, and problems of deep learning approaches to Theory of Mind. We\nhighlight that many current findings can be explained through shortcuts. These\nshortcuts arise because the tasks used to investigate Theory of Mind in deep\nlearning systems have been too narrow. Thus, we encourage researchers to\ninvestigate Theory of Mind in complex open-ended environments. Furthermore, to\ninspire future deep learning systems we provide a concise overview of prior\nwork done in humans. We further argue that when studying Theory of Mind with\ndeep learning, the research's main focus and contribution ought to be opening\nup the network's representations. We recommend researchers use tools from the\nfield of interpretability of AI to study the relationship between different\nnetwork components and aspects of Theory of Mind.\n",
        "published": "2022",
        "authors": [
            "Jaan Aru",
            "Aqeel Labash",
            "Oriol Corcoll",
            "Raul Vicente"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.07291v1",
        "title": "Adversarial Security Attacks and Perturbations on Machine Learning and\n  Deep Learning Methods",
        "abstract": "  The ever-growing big data and emerging artificial intelligence (AI) demand\nthe use of machine learning (ML) and deep learning (DL) methods. Cybersecurity\nalso benefits from ML and DL methods for various types of applications. These\nmethods however are susceptible to security attacks. The adversaries can\nexploit the training and testing data of the learning models or can explore the\nworkings of those models for launching advanced future attacks. The topic of\nadversarial security attacks and perturbations within the ML and DL domains is\na recent exploration and a great interest is expressed by the security\nresearchers and practitioners. The literature covers different adversarial\nsecurity attacks and perturbations on ML and DL methods and those have their\nown presentation styles and merits. A need to review and consolidate knowledge\nthat is comprehending of this increasingly focused and growing topic of\nresearch; however, is the current demand of the research communities. In this\nreview paper, we specifically aim to target new researchers in the\ncybersecurity domain who may seek to acquire some basic knowledge on the\nmachine learning and deep learning models and algorithms, as well as some of\nthe relevant adversarial security attacks and perturbations.\n",
        "published": "2019",
        "authors": [
            "Arif Siddiqi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.08094v1",
        "title": "Smart Grid: A Survey of Architectural Elements, Machine Learning and\n  Deep Learning Applications and Future Directions",
        "abstract": "  The Smart grid (SG), generally known as the next-generation power grid\nemerged as a replacement for ill-suited power systems in the 21st century. It\nis in-tegrated with advanced communication and computing capabilities, thus it\nis ex-pected to enhance the reliability and the efficiency of energy\ndistribution with minimum effects. With the massive infrastructure it holds and\nthe underlying communication network in the system, it introduced a large\nvolume of data that demands various techniques for proper analysis and decision\nmaking. Big data analytics, machine learning (ML), and deep learning (DL) plays\na key role when it comes to the analysis of this massive amount of data and\ngeneration of valuable insights. This paper explores and surveys the Smart grid\narchitectural elements, machine learning, and deep learning-based applications\nand approaches in the context of the Smart grid. In addition in terms of\nmachine learning-based data an-alytics, this paper highlights the limitations\nof the current research and highlights future directions as well.\n",
        "published": "2020",
        "authors": [
            "Navod Neranjan Thilakarathne",
            "Mohan Krishna Kagita",
            "Dr. Surekha Lanka",
            "Hussain Ahmad"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.09668v1",
        "title": "Deep Learning for Spatiotemporal Modeling of Urbanization",
        "abstract": "  Urbanization has a strong impact on the health and wellbeing of populations\nacross the world. Predictive spatial modeling of urbanization therefore can be\na useful tool for effective public health planning. Many spatial urbanization\nmodels have been developed using classic machine learning and numerical\nmodeling techniques. However, deep learning with its proven capacity to capture\ncomplex spatiotemporal phenomena has not been applied to urbanization modeling.\nHere we explore the capacity of deep spatial learning for the predictive\nmodeling of urbanization. We treat numerical geospatial data as images with\npixels and channels, and enrich the dataset by augmentation, in order to\nleverage the high capacity of deep learning. Our resulting model can generate\nend-to-end multi-variable urbanization predictions, and outperforms a\nstate-of-the-art classic machine learning urbanization model in preliminary\ncomparisons.\n",
        "published": "2021",
        "authors": [
            "Tang Li",
            "Jing Gao",
            "Xi Peng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.13292v1",
        "title": "Deep learning and machine learning for Malaria detection: overview,\n  challenges and future directions",
        "abstract": "  To have the greatest impact, public health initiatives must be made using\nevidence-based decision-making. Machine learning Algorithms are created to\ngather, store, process, and analyse data to provide knowledge and guide\ndecisions. A crucial part of any surveillance system is image analysis. The\ncommunities of computer vision and machine learning has ended up curious about\nit as of late. This study uses a variety of machine learning and image\nprocessing approaches to detect and forecast the malarial illness. In our\nresearch, we discovered the potential of deep learning techniques as smart\ntools with broader applicability for malaria detection, which benefits\nphysicians by assisting in the diagnosis of the condition. We examine the\ncommon confinements of deep learning for computer frameworks and organising,\ncounting need of preparing data, preparing overhead, realtime execution, and\nexplain ability, and uncover future inquire about bearings focusing on these\nrestrictions.\n",
        "published": "2022",
        "authors": [
            "Imen Jdey",
            "Ghazala Hcini",
            "Hela Ltifi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.12509v1",
        "title": "Machine Learning for Leaf Disease Classification: Data, Techniques and\n  Applications",
        "abstract": "  The growing demand for sustainable development brings a series of information\ntechnologies to help agriculture production. Especially, the emergence of\nmachine learning applications, a branch of artificial intelligence, has shown\nmultiple breakthroughs which can enhance and revolutionize plant pathology\napproaches. In recent years, machine learning has been adopted for leaf disease\nclassification in both academic research and industrial applications.\nTherefore, it is enormously beneficial for researchers, engineers, managers,\nand entrepreneurs to have a comprehensive view about the recent development of\nmachine learning technologies and applications for leaf disease detection. This\nstudy will provide a survey in different aspects of the topic including data,\ntechniques, and applications. The paper will start with publicly available\ndatasets. After that, we summarize common machine learning techniques,\nincluding traditional (shallow) learning, deep learning, and augmented\nlearning. Finally, we discuss related applications. This paper would provide\nuseful resources for future study and application of machine learning for smart\nagriculture in general and leaf disease classification in particular.\n",
        "published": "2023",
        "authors": [
            "Jianping Yao",
            "Son N. Tran",
            "Samantha Sawyer",
            "Saurabh Garg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1611.07422v1",
        "title": "Deep Learning Approximation for Stochastic Control Problems",
        "abstract": "  Many real world stochastic control problems suffer from the \"curse of\ndimensionality\". To overcome this difficulty, we develop a deep learning\napproach that directly solves high-dimensional stochastic control problems\nbased on Monte-Carlo sampling. We approximate the time-dependent controls as\nfeedforward neural networks and stack these networks together through model\ndynamics. The objective function for the control problem plays the role of the\nloss function for the deep neural network. We test this approach using examples\nfrom the areas of optimal trading and energy storage. Our results suggest that\nthe algorithm presented here achieves satisfactory accuracy and at the same\ntime, can handle rather high dimensional problems.\n",
        "published": "2016",
        "authors": [
            "Jiequn Han",
            "Weinan E"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.01810v1",
        "title": "Deep Learning Chromatic and Clique Numbers of Graphs",
        "abstract": "  Deep neural networks have been applied to a wide range of problems across\ndifferent application domains with great success. Recently, research into\ncombinatorial optimization problems in particular has generated much interest\nin the machine learning community. In this work, we develop deep learning\nmodels to predict the chromatic number and maximum clique size of graphs, both\nof which represent classical NP-complete combinatorial optimization problems\nencountered in graph theory. The neural networks are trained using the most\nbasic representation of the graph, the adjacency matrix, as opposed to\nundergoing complex domain-specific feature engineering. The experimental\nresults show that deep neural networks, and in particular convolutional neural\nnetworks, obtain strong performance on this problem.\n",
        "published": "2021",
        "authors": [
            "Jason Van Hulse",
            "Joshua S. Friedman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.03878v1",
        "title": "Histogram Layers for Synthetic Aperture Sonar Imagery",
        "abstract": "  Synthetic aperture sonar (SAS) imagery is crucial for several applications,\nincluding target recognition and environmental segmentation. Deep learning\nmodels have led to much success in SAS analysis; however, the features\nextracted by these approaches may not be suitable for capturing certain\ntextural information. To address this problem, we present a novel application\nof histogram layers on SAS imagery. The addition of histogram layer(s) within\nthe deep learning models improved performance by incorporating statistical\ntexture information on both synthetic and real-world datasets.\n",
        "published": "2022",
        "authors": [
            "Joshua Peeples",
            "Alina Zare",
            "Jeffrey Dale",
            "James Keller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1210.8353v1",
        "title": "Temporal Autoencoding Restricted Boltzmann Machine",
        "abstract": "  Much work has been done refining and characterizing the receptive fields\nlearned by deep learning algorithms. A lot of this work has focused on the\ndevelopment of Gabor-like filters learned when enforcing sparsity constraints\non a natural image dataset. Little work however has investigated how these\nfilters might expand to the temporal domain, namely through training on natural\nmovies. Here we investigate exactly this problem in established temporal deep\nlearning algorithms as well as a new learning paradigm suggested here, the\nTemporal Autoencoding Restricted Boltzmann Machine (TARBM).\n",
        "published": "2012",
        "authors": [
            "Chris H\u00e4usler",
            "Alex Susemihl"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.6614v4",
        "title": "In Search of the Real Inductive Bias: On the Role of Implicit\n  Regularization in Deep Learning",
        "abstract": "  We present experiments demonstrating that some other form of capacity\ncontrol, different from network size, plays a central role in learning\nmultilayer feed-forward networks. We argue, partially through analogy to matrix\nfactorization, that this is an inductive bias that can help shed light on deep\nlearning.\n",
        "published": "2014",
        "authors": [
            "Behnam Neyshabur",
            "Ryota Tomioka",
            "Nathan Srebro"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.10995v1",
        "title": "Protection against Cloning for Deep Learning",
        "abstract": "  The susceptibility of deep learning to adversarial attack can be understood\nin the framework of the Renormalisation Group (RG) and the vulnerability of a\nspecific network may be diagnosed provided the weights in each layer are known.\nAn adversary with access to the inputs and outputs could train a second network\nto clone these weights and, having identified a weakness, use them to compute\nthe perturbation of the input data which exploits it. However, the RG framework\nalso provides a means to poison the outputs of the network imperceptibly,\nwithout affecting their legitimate use, so as to prevent such cloning of its\nweights and thereby foil the generation of adversarial data.\n",
        "published": "2018",
        "authors": [
            "Richard Kenway"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.04704v2",
        "title": "Neural network models and deep learning - a primer for biologists",
        "abstract": "  Originally inspired by neurobiology, deep neural network models have become a\npowerful tool of machine learning and artificial intelligence, where they are\nused to approximate functions and dynamics by learning from examples. Here we\ngive a brief introduction to neural network models and deep learning for\nbiologists. We introduce feedforward and recurrent networks and explain the\nexpressive power of this modeling framework and the backpropagation algorithm\nfor setting the parameters. Finally, we consider how deep neural networks might\nhelp us understand the brain's computations.\n",
        "published": "2019",
        "authors": [
            "Nikolaus Kriegeskorte",
            "Tal Golan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.03363v1",
        "title": "Torchbearer: A Model Fitting Library for PyTorch",
        "abstract": "  We introduce torchbearer, a model fitting library for pytorch aimed at\nresearchers working on deep learning or differentiable programming. The\ntorchbearer library provides a high level metric and callback API that can be\nused for a wide range of applications. We also include a series of built in\ncallbacks that can be used for: model persistence, learning rate decay,\nlogging, data visualization and more. The extensive documentation includes an\nexample library for deep learning and dynamic programming problems and can be\nfound at http://torchbearer.readthedocs.io. The code is licensed under the MIT\nLicense and available at https://github.com/ecs-vlc/torchbearer.\n",
        "published": "2018",
        "authors": [
            "Ethan Harris",
            "Matthew Painter",
            "Jonathon Hare"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.09957v1",
        "title": "Activation Functions in Artificial Neural Networks: A Systematic\n  Overview",
        "abstract": "  Activation functions shape the outputs of artificial neurons and, therefore,\nare integral parts of neural networks in general and deep learning in\nparticular. Some activation functions, such as logistic and relu, have been\nused for many decades. But with deep learning becoming a mainstream research\ntopic, new activation functions have mushroomed, leading to confusion in both\ntheory and practice. This paper provides an analytic yet up-to-date overview of\npopular activation functions and their properties, which makes it a timely\nresource for anyone who studies or applies neural networks.\n",
        "published": "2021",
        "authors": [
            "Johannes Lederer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.01174v3",
        "title": "Generalization Error in Deep Learning",
        "abstract": "  Deep learning models have lately shown great performance in various fields\nsuch as computer vision, speech recognition, speech translation, and natural\nlanguage processing. However, alongside their state-of-the-art performance, it\nis still generally unclear what is the source of their generalization ability.\nThus, an important question is what makes deep neural networks able to\ngeneralize well from the training set to new data. In this article, we provide\nan overview of the existing theory and bounds for the characterization of the\ngeneralization error of deep neural networks, combining both classical and more\nrecent theoretical and empirical results.\n",
        "published": "2018",
        "authors": [
            "Daniel Jakubovitz",
            "Raja Giryes",
            "Miguel R. D. Rodrigues"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.03589v1",
        "title": "TimeGPT-1",
        "abstract": "  In this paper, we introduce TimeGPT, the first foundation model for time\nseries, capable of generating accurate predictions for diverse datasets not\nseen during training. We evaluate our pre-trained model against established\nstatistical, machine learning, and deep learning methods, demonstrating that\nTimeGPT zero-shot inference excels in performance, efficiency, and simplicity.\nOur study provides compelling evidence that insights from other domains of\nartificial intelligence can be effectively applied to time series analysis. We\nconclude that large-scale time series models offer an exciting opportunity to\ndemocratize access to precise predictions and reduce uncertainty by leveraging\nthe capabilities of contemporary advancements in deep learning.\n",
        "published": "2023",
        "authors": [
            "Azul Garza",
            "Max Mergenthaler-Canseco"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.07065v1",
        "title": "Non-approximability of constructive global $\\mathcal{L}^2$ minimizers by\n  gradient descent in Deep Learning",
        "abstract": "  We analyze geometric aspects of the gradient descent algorithm in Deep\nLearning (DL) networks. In particular, we prove that the globally minimizing\nweights and biases for the $\\mathcal{L}^2$ cost obtained constructively in\n[Chen-Munoz Ewald 2023] for underparametrized ReLU DL networks can generically\nnot be approximated via the gradient descent flow. We therefore conclude that\nthe method introduced in [Chen-Munoz Ewald 2023] is disjoint from the gradient\ndescent method.\n",
        "published": "2023",
        "authors": [
            "Thomas Chen",
            "Patricia Mu\u00f1oz Ewald"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.00780v1",
        "title": "Ambient Hidden Space of Generative Adversarial Networks",
        "abstract": "  Generative adversarial models are powerful tools to model structure in\ncomplex distributions for a variety of tasks. Current techniques for learning\ngenerative models require an access to samples which have high quality, and\nadvanced generative models are applied to generate samples from noisy training\ndata through ambient modules. However, the modules are only practical for the\noutput space of the generator, and their application in the hidden space is not\nwell studied. In this paper, we extend the ambient module to the hidden space\nof the generator, and provide the uniqueness condition and the corresponding\nstrategy for the ambient hidden generator in the adversarial training process.\nWe report the practicality of the proposed method on the benchmark dataset.\n",
        "published": "2018",
        "authors": [
            "Xinhan Di",
            "Pengqian Yu",
            "Meng Tian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.04690v1",
        "title": "AI in Telemedicine: An Appraisal on Deep Learning-Based Approaches to\n  Virtual Diagnostic Solutions (VDS)",
        "abstract": "  Advancements in Telemedicine as an approach to healthcare delivery have\nheralded a new dawn in modern Medicine. Its fast-paced development in our\ncontemporary society is credence to the advances in Artificial Intelligence and\nInformation Technology. This paper carries out a descriptive study to broadly\nexplore AI's implementations in healthcare delivery with a more holistic view\nof the usability of various Telemedical Innovations in enhancing Virtual\nDiagnostic Solutions (VDS). This research further explores notable developments\nin Deep Learning model optimizations for Virtual Diagnostic Solutions. A\nfurther research review on the prospects of Virtual Diagnostic Solutions (VDS)\nand foreseeable challenges was also highlighted. Conclusively, this research\ngives a general overview of Artificial Intelligence in Telemedicine with a\ncentral focus on Deep Learning-based approaches to Virtual Diagnostic\nSolutions.\n",
        "published": "2022",
        "authors": [
            "Ozioma Collins Oguine",
            "Kanyifeechukwu Jane Oguine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.05390v1",
        "title": "TensorLog: Deep Learning Meets Probabilistic DBs",
        "abstract": "  We present an implementation of a probabilistic first-order logic called\nTensorLog, in which classes of logical queries are compiled into differentiable\nfunctions in a neural-network infrastructure such as Tensorflow or Theano. This\nleads to a close integration of probabilistic logical reasoning with\ndeep-learning infrastructure: in particular, it enables high-performance deep\nlearning frameworks to be used for tuning the parameters of a probabilistic\nlogic. Experimental results show that TensorLog scales to problems involving\nhundreds of thousands of knowledge-base triples and tens of thousands of\nexamples.\n",
        "published": "2017",
        "authors": [
            "William W. Cohen",
            "Fan Yang",
            "Kathryn Rivard Mazaitis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1708.07902v3",
        "title": "Deep Learning for Video Game Playing",
        "abstract": "  In this article, we review recent Deep Learning advances in the context of\nhow they have been applied to play different types of video games such as\nfirst-person shooters, arcade games, and real-time strategy games. We analyze\nthe unique requirements that different game genres pose to a deep learning\nsystem and highlight important open challenges in the context of applying these\nmachine learning methods to video games, such as general game playing, dealing\nwith extremely large decision spaces and sparse rewards.\n",
        "published": "2017",
        "authors": [
            "Niels Justesen",
            "Philip Bontrager",
            "Julian Togelius",
            "Sebastian Risi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.12982v1",
        "title": "From Semantic Retrieval to Pairwise Ranking: Applying Deep Learning in\n  E-commerce Search",
        "abstract": "  We introduce deep learning models to the two most important stages in product\nsearch at JD.com, one of the largest e-commerce platforms in the world.\nSpecifically, we outline the design of a deep learning system that retrieves\nsemantically relevant items to a query within milliseconds, and a pairwise deep\nre-ranking system, which learns subtle user preferences. Compared to\ntraditional search systems, the proposed approaches are better at semantic\nretrieval and personalized ranking, achieving significant improvements.\n",
        "published": "2021",
        "authors": [
            "Rui Li",
            "Yunjiang Jiang",
            "Wenyun Yang",
            "Guoyu Tang",
            "Songlin Wang",
            "Chaoyi Ma",
            "Wei He",
            "Xi Xiong",
            "Yun Xiao",
            "Eric Yihong Zhao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.00737v1",
        "title": "Hardware-friendly Deep Learning by Network Quantization and Binarization",
        "abstract": "  Quantization is emerging as an efficient approach to promote\nhardware-friendly deep learning and run deep neural networks on\nresource-limited hardware. However, it still causes a significant decrease to\nthe network in accuracy. We summarize challenges of quantization into two\ncategories: Quantization for Diverse Architectures and Quantization on Complex\nScenes. Our studies focus mainly on applying quantization on various\narchitectures and scenes and pushing the limit of quantization to extremely\ncompress and accelerate networks. The comprehensive research on quantization\nwill achieve more powerful, more efficient, and more flexible hardware-friendly\ndeep learning, and make it better suited to more real-world applications.\n",
        "published": "2021",
        "authors": [
            "Haotong Qin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.11636v1",
        "title": "Forecasting of Non-Stationary Sales Time Series Using Deep Learning",
        "abstract": "  The paper describes the deep learning approach for forecasting non-stationary\ntime series with using time trend correction in a neural network model. Along\nwith the layers for predicting sales values, the neural network model includes\na subnetwork block for the prediction weight for a time trend term which is\nadded to a predicted sales value. The time trend term is considered as a\nproduct of the predicted weight value and normalized time value. The results\nshow that the forecasting accuracy can be essentially improved for\nnon-stationary sales with time trends using the trend correction block in the\ndeep learning model.\n",
        "published": "2022",
        "authors": [
            "Bohdan M. Pavlyshenko"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.11633v1",
        "title": "On a Built-in Conflict between Deep Learning and Systematic\n  Generalization",
        "abstract": "  In this paper, we hypothesize that internal function sharing is one of the\nreasons to weaken o.o.d. or systematic generalization in deep learning for\nclassification tasks. Under equivalent prediction, a model partitions an input\nspace into multiple parts separated by boundaries. The function sharing prefers\nto reuse boundaries, leading to fewer parts for new outputs, which conflicts\nwith systematic generalization. We show such phenomena in standard deep\nlearning models, such as fully connected, convolutional, residual networks,\nLSTMs, and (Vision) Transformers. We hope this study provides novel insights\ninto systematic generalization and forms a basis for new research directions.\n",
        "published": "2022",
        "authors": [
            "Yuanpeng Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.12027v1",
        "title": "Forecasting with Deep Learning",
        "abstract": "  This paper presents a method for time series forecasting with deep learning\nand its assessment on two datasets. The method starts with data preparation,\nfollowed by model training and evaluation. The final step is a visual\ninspection. Experimental work demonstrates that a single time series can be\nused to train deep learning networks if time series in a dataset contain\npatterns that repeat even with a certain variation. However, for less\nstructured time series such as stock market closing prices, the networks\nperform just like a baseline that repeats the last observed value. The\nimplementation of the method as well as the experiments are open-source.\n",
        "published": "2023",
        "authors": [
            "Gissel Velarde"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.08193v1",
        "title": "Applications of Deep Learning for Top-View Omnidirectional Imaging: A\n  Survey",
        "abstract": "  A large field-of-view fisheye camera allows for capturing a large area with\nminimal numbers of cameras when they are mounted on a high position facing\ndownwards. This top-view omnidirectional setup greatly reduces the work and\ncost for deployment compared to traditional solutions with multiple perspective\ncameras. In recent years, deep learning has been widely employed for vision\nrelated tasks, including for such omnidirectional settings. In this survey, we\nlook at the application of deep learning in combination with omnidirectional\ntop-view cameras, including the available datasets, human and object detection,\nhuman pose estimation, activity recognition and other miscellaneous\napplications.\n",
        "published": "2023",
        "authors": [
            "Jingrui Yu",
            "Ana Cecilia Perez Grassi",
            "Gangolf Hirtz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.04308v1",
        "title": "Breast cancer detection using artificial intelligence techniques: A\n  systematic literature review",
        "abstract": "  Cancer is one of the most dangerous diseases to humans, and yet no permanent\ncure has been developed for it. Breast cancer is one of the most common cancer\ntypes. According to the National Breast Cancer foundation, in 2020 alone, more\nthan 276,000 new cases of invasive breast cancer and more than 48,000\nnon-invasive cases were diagnosed in the US. To put these figures in\nperspective, 64% of these cases are diagnosed early in the disease's cycle,\ngiving patients a 99% chance of survival. Artificial intelligence and machine\nlearning have been used effectively in detection and treatment of several\ndangerous diseases, helping in early diagnosis and treatment, and thus\nincreasing the patient's chance of survival. Deep learning has been designed to\nanalyze the most important features affecting detection and treatment of\nserious diseases. For example, breast cancer can be detected using genes or\nhistopathological imaging. Analysis at the genetic level is very expensive, so\nhistopathological imaging is the most common approach used to detect breast\ncancer. In this research work, we systematically reviewed previous work done on\ndetection and treatment of breast cancer using genetic sequencing or\nhistopathological imaging with the help of deep learning and machine learning.\nWe also provide recommendations to researchers who will work in this field\n",
        "published": "2022",
        "authors": [
            "Ali Bou Nassif",
            "Manar Abu Talib",
            "Qassim Nasir",
            "Yaman Afadar",
            "Omar Elgendy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.09274v1",
        "title": "Deep Learning on Mobile Devices - A Review",
        "abstract": "  Recent breakthroughs in deep learning and artificial intelligence\ntechnologies have enabled numerous mobile applications. While traditional\ncomputation paradigms rely on mobile sensing and cloud computing, deep learning\nimplemented on mobile devices provides several advantages. These advantages\ninclude low communication bandwidth, small cloud computing resource cost, quick\nresponse time, and improved data privacy. Research and development of deep\nlearning on mobile and embedded devices has recently attracted much attention.\nThis paper provides a timely review of this fast-paced field to give the\nresearcher, engineer, practitioner, and graduate student a quick grasp on the\nrecent advancements of deep learning on mobile devices. In this paper, we\ndiscuss hardware architectures for mobile deep learning, including Field\nProgrammable Gate Arrays, Application Specific Integrated Circuit, and recent\nmobile Graphic Processing Units. We present Size, Weight, Area and Power\nconsiderations and their relation to algorithm optimizations, such as\nquantization, pruning, compression, and approximations that simplify\ncomputation while retaining performance accuracy. We cover existing systems and\ngive a state-of-the-industry review of TensorFlow, MXNet, Mobile AI Compute\nEngine, and Paddle-mobile deep learning platform. We discuss resources for\nmobile deep learning practitioners, including tools, libraries, models, and\nperformance benchmarks. We present applications of various mobile sensing\nmodalities to industries, ranging from robotics, healthcare and multi-media,\nbiometrics to autonomous drive and defense. We address the key deep learning\nchallenges to overcome, including low quality data, and small\ntraining/adaptation data sets. In addition, the review provides numerous\ncitations and links to existing code bases implementing various technologies.\n",
        "published": "2019",
        "authors": [
            "Yunbin Deng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.01942v1",
        "title": "Fault-Tolerant Deep Learning: A Hierarchical Perspective",
        "abstract": "  With the rapid advancements of deep learning in the past decade, it can be\nforeseen that deep learning will be continuously deployed in more and more\nsafety-critical applications such as autonomous driving and robotics. In this\ncontext, reliability turns out to be critical to the deployment of deep\nlearning in these applications and gradually becomes a first-class citizen\namong the major design metrics like performance and energy efficiency.\nNevertheless, the back-box deep learning models combined with the diverse\nunderlying hardware faults make resilient deep learning extremely challenging.\nIn this special session, we conduct a comprehensive survey of fault-tolerant\ndeep learning design approaches with a hierarchical perspective and investigate\nthese approaches from model layer, architecture layer, circuit layer, and cross\nlayer respectively.\n",
        "published": "2022",
        "authors": [
            "Cheng Liu",
            "Zhen Gao",
            "Siting Liu",
            "Xuefei Ning",
            "Huawei Li",
            "Xiaowei Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.08137v1",
        "title": "Achieve Optimal Adversarial Accuracy for Adversarial Deep Learning using\n  Stackelberg Game",
        "abstract": "  Adversarial deep learning is to train robust DNNs against adversarial\nattacks, which is one of the major research focuses of deep learning. Game\ntheory has been used to answer some of the basic questions about adversarial\ndeep learning such as the existence of a classifier with optimal robustness and\nthe existence of optimal adversarial samples for a given class of classifiers.\nIn most previous work, adversarial deep learning was formulated as a\nsimultaneous game and the strategy spaces are assumed to be certain probability\ndistributions in order for the Nash equilibrium to exist. But, this assumption\nis not applicable to the practical situation. In this paper, we give answers to\nthese basic questions for the practical case where the classifiers are DNNs\nwith a given structure, by formulating the adversarial deep learning as\nsequential games. The existence of Stackelberg equilibria for these games are\nproved. Furthermore, it is shown that the equilibrium DNN has the largest\nadversarial accuracy among all DNNs with the same structure, when\nCarlini-Wagner's margin loss is used. Trade-off between robustness and accuracy\nin adversarial deep learning is also studied from game theoretical aspect.\n",
        "published": "2022",
        "authors": [
            "Xiao-Shan Gao",
            "Shuang Liu",
            "Lijia Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.10550v1",
        "title": "Deep learning applied to EEG data with different montages using spatial\n  attention",
        "abstract": "  The ability of Deep Learning to process and extract relevant information in\ncomplex brain dynamics from raw EEG data has been demonstrated in various\nrecent works. Deep learning models, however, have also been shown to perform\nbest on large corpora of data. When processing EEG, a natural approach is to\ncombine EEG datasets from different experiments to train large deep-learning\nmodels. However, most EEG experiments use custom channel montages, requiring\nthe data to be transformed into a common space. Previous methods have used the\nraw EEG signal to extract features of interest and focused on using a common\nfeature space across EEG datasets. While this is a sensible approach, it\nunderexploits the potential richness of EEG raw data. Here, we explore using\nspatial attention applied to EEG electrode coordinates to perform channel\nharmonization of raw EEG data, allowing us to train deep learning on EEG data\nusing different montages. We test this model on a gender classification task.\nWe first show that spatial attention increases model performance. Then, we show\nthat a deep learning model trained on data using different channel montages\nperforms significantly better than deep learning models trained on fixed 23-\nand 128-channel data montages.\n",
        "published": "2023",
        "authors": [
            "Dung Truong",
            "Muhammad Abdullah Khalid",
            "Arnaud Delorme"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.11237v1",
        "title": "Emerging Threats in Deep Learning-Based Autonomous Driving: A\n  Comprehensive Survey",
        "abstract": "  Since the 2004 DARPA Grand Challenge, the autonomous driving technology has\nwitnessed nearly two decades of rapid development. Particularly, in recent\nyears, with the application of new sensors and deep learning technologies\nextending to the autonomous field, the development of autonomous driving\ntechnology has continued to make breakthroughs. Thus, many carmakers and\nhigh-tech giants dedicated to research and system development of autonomous\ndriving. However, as the foundation of autonomous driving, the deep learning\ntechnology faces many new security risks. The academic community has proposed\ndeep learning countermeasures against the adversarial examples and AI backdoor,\nand has introduced them into the autonomous driving field for verification.\nDeep learning security matters to autonomous driving system security, and then\nmatters to personal safety, which is an issue that deserves attention and\nresearch.This paper provides an summary of the concepts, developments and\nrecent research in deep learning security technologies in autonomous driving.\nFirstly, we briefly introduce the deep learning framework and pipeline in the\nautonomous driving system, which mainly include the deep learning technologies\nand algorithms commonly used in this field. Moreover, we focus on the potential\nsecurity threats of the deep learning based autonomous driving system in each\nfunctional layer in turn. We reviews the development of deep learning attack\ntechnologies to autonomous driving, investigates the State-of-the-Art\nalgorithms, and reveals the potential risks. At last, we provides an outlook on\ndeep learning security in the autonomous driving field and proposes\nrecommendations for building a safe and trustworthy autonomous driving system.\n",
        "published": "2022",
        "authors": [
            "Hui Cao",
            "Wenlong Zou",
            "Yinkun Wang",
            "Ting Song",
            "Mengjun Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.08371v1",
        "title": "The Quarks of Attention",
        "abstract": "  Attention plays a fundamental role in both natural and artificial\nintelligence systems. In deep learning, attention-based neural architectures,\nsuch as transformer architectures, are widely used to tackle problems in\nnatural language processing and beyond. Here we investigate the fundamental\nbuilding blocks of attention and their computational properties. Within the\nstandard model of deep learning, we classify all possible fundamental building\nblocks of attention in terms of their source, target, and computational\nmechanism. We identify and study three most important mechanisms: additive\nactivation attention, multiplicative output attention (output gating), and\nmultiplicative synaptic attention (synaptic gating). The gating mechanisms\ncorrespond to multiplicative extensions of the standard model and are used\nacross all current attention-based deep learning architectures. We study their\nfunctional properties and estimate the capacity of several attentional building\nblocks in the case of linear and polynomial threshold gates. Surprisingly,\nadditive activation attention plays a central role in the proofs of the lower\nbounds. Attention mechanisms reduce the depth of certain basic circuits and\nleverage the power of quadratic activations without incurring their full cost.\n",
        "published": "2022",
        "authors": [
            "Pierre Baldi",
            "Roman Vershynin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.10929v1",
        "title": "HAR-Net:Fusing Deep Representation and Hand-crafted Features for Human\n  Activity Recognition",
        "abstract": "  Wearable computing and context awareness are the focuses of study in the\nfield of artificial intelligence recently. One of the most appealing as well as\nchallenging applications is the Human Activity Recognition (HAR) utilizing\nsmart phones. Conventional HAR based on Support Vector Machine relies on\nsubjective manually extracted features. This approach is time and energy\nconsuming as well as immature in prediction due to the partial view toward\nwhich features to be extracted by human. With the rise of deep learning,\nartificial intelligence has been making progress toward being a mature\ntechnology. This paper proposes a new approach based on deep learning and\ntraditional feature engineering called HAR-Net to address the issue related to\nHAR. The study used the data collected by gyroscopes and acceleration sensors\nin android smart phones. The raw sensor data was put into the HAR-Net proposed.\nThe HAR-Net fusing the hand-crafted features and high-level features extracted\nfrom convolutional network to make prediction. The performance of the proposed\nmethod was proved to be 0.9% higher than the original MC-SVM approach. The\nexperimental results on the UCI dataset demonstrate that fusing the two kinds\nof features can make up for the shortage of traditional feature engineering and\ndeep learning techniques.\n",
        "published": "2018",
        "authors": [
            "Mingtao Dong",
            "Jindong Han"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.09748v1",
        "title": "A Deep Hybrid Model for Recommendation Systems",
        "abstract": "  Recommendation has been a long-standing problem in many areas ranging from\ne-commerce to social websites. Most current studies focus only on traditional\napproaches such as content-based or collaborative filtering while there are\nrelatively fewer studies in hybrid recommender systems. Due to the latest\nadvances of deep learning achieved in different fields including computer\nvision and natural language processing, deep learning has also gained much\nattention in Recommendation Systems. There are several studies that utilize ID\nembeddings of users and items to implement collaborative filtering with deep\nneural networks. However, such studies do not take advantage of other\ncategorical or continuous features of inputs. In this paper, we propose a new\ndeep neural network architecture which consists of not only ID embeddings but\nalso auxiliary information such as features of job postings and candidates for\njob recommendation system which is a reciprocal recommendation system.\nExperimental results on the dataset from a job-site show that the proposed\nmethod improves recommendation results over deep learning models utilizing ID\nembeddings.\n",
        "published": "2020",
        "authors": [
            "Muhammet cakir",
            "sule gunduz oguducu",
            "resul tugay"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.06119v4",
        "title": "APTx: better activation function than MISH, SWISH, and ReLU's variants\n  used in deep learning",
        "abstract": "  Activation Functions introduce non-linearity in the deep neural networks.\nThis nonlinearity helps the neural networks learn faster and efficiently from\nthe dataset. In deep learning, many activation functions are developed and used\nbased on the type of problem statement. ReLU's variants, SWISH, and MISH are\ngoto activation functions. MISH function is considered having similar or even\nbetter performance than SWISH, and much better than ReLU. In this paper, we\npropose an activation function named APTx which behaves similar to MISH, but\nrequires lesser mathematical operations to compute. The lesser computational\nrequirements of APTx does speed up the model training, and thus also reduces\nthe hardware requirement for the deep learning model.\n",
        "published": "2022",
        "authors": [
            "Ravin Kumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.03678v1",
        "title": "Machine Learning Automation Toolbox (MLaut)",
        "abstract": "  In this paper we present MLaut (Machine Learning AUtomation Toolbox) for the\npython data science ecosystem. MLaut automates large-scale evaluation and\nbenchmarking of machine learning algorithms on a large number of datasets.\nMLaut provides a high-level workflow interface to machine algorithm algorithms,\nimplements a local back-end to a database of dataset collections, trained\nalgorithms, and experimental results, and provides easy-to-use interfaces to\nthe scikit-learn and keras modelling libraries. Experiments are easy to set up\nwith default settings in a few lines of code, while remaining fully\ncustomizable to the level of hyper-parameter tuning, pipeline composition, or\ndeep learning architecture.\n  As a principal test case for MLaut, we conducted a large-scale supervised\nclassification study in order to benchmark the performance of a number of\nmachine learning algorithms - to our knowledge also the first larger-scale\nstudy on standard supervised learning data sets to include deep learning\nalgorithms. While corroborating a number of previous findings in literature, we\nfound (within the limitations of our study) that deep neural networks do not\nperform well on basic supervised learning, i.e., outside the more specialized,\nimage-, audio-, or text-based tasks.\n",
        "published": "2019",
        "authors": [
            "Viktor Kazakov",
            "Franz J. Kir\u00e1ly"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.07268v1",
        "title": "Adversarial Machine Learning Security Problems for 6G: mmWave Beam\n  Prediction Use-Case",
        "abstract": "  6G is the next generation for the communication systems. In recent years,\nmachine learning algorithms have been applied widely in various fields such as\nhealth, transportation, and the autonomous car. The predictive algorithms will\nbe used in 6G problems. With the rapid developments of deep learning\ntechniques, it is critical to take the security concern into account to apply\nthe algorithms. While machine learning offers significant advantages for 6G, AI\nmodels' security is ignored. Since it has many applications in the real world,\nsecurity is a vital part of the algorithms. This paper has proposed a\nmitigation method for adversarial attacks against proposed 6G machine learning\nmodels for the millimeter-wave (mmWave) beam prediction with adversarial\nlearning. The main idea behind adversarial attacks against machine learning\nmodels is to produce faulty results by manipulating trained deep learning\nmodels for 6G applications for mmWave beam prediction use case. We have also\npresented the adversarial learning mitigation method's performance for 6G\nsecurity in millimeter-wave beam prediction application with fast gradient sign\nmethod attack. The mean square errors of the defended model and undefended\nmodel are very close.\n",
        "published": "2021",
        "authors": [
            "Evren Catak",
            "Ferhat Ozgur Catak",
            "Arild Moldsvor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.09791v1",
        "title": "Bioinformatics and Medicine in the Era of Deep Learning",
        "abstract": "  Many of the current scientific advances in the life sciences have their\norigin in the intensive use of data for knowledge discovery. In no area this is\nso clear as in bioinformatics, led by technological breakthroughs in data\nacquisition technologies. It has been argued that bioinformatics could quickly\nbecome the field of research generating the largest data repositories, beating\nother data-intensive areas such as high-energy physics or astroinformatics.\nOver the last decade, deep learning has become a disruptive advance in machine\nlearning, giving new live to the long-standing connectionist paradigm in\nartificial intelligence. Deep learning methods are ideally suited to\nlarge-scale data and, therefore, they should be ideally suited to knowledge\ndiscovery in bioinformatics and biomedicine at large. In this brief paper, we\nreview key aspects of the application of deep learning in bioinformatics and\nmedicine, drawing from the themes covered by the contributions to an ESANN 2018\nspecial session devoted to this topic.\n",
        "published": "2018",
        "authors": [
            "Davide Bacciu",
            "Paulo J. G. Lisboa",
            "Jos\u00e9 D. Mart\u00edn",
            "Ruxandra Stoean",
            "Alfredo Vellido"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.08570v2",
        "title": "Deep Learning in Pharmacogenomics: From Gene Regulation to Patient\n  Stratification",
        "abstract": "  This Perspective provides examples of current and future applications of deep\nlearning in pharmacogenomics, including: (1) identification of novel regulatory\nvariants located in noncoding domains and their function as applied to\npharmacoepigenomics; (2) patient stratification from medical records; and (3)\nprediction of drugs, targets, and their interactions. Deep learning\nencapsulates a family of machine learning algorithms that over the last decade\nhas transformed many important subfields of artificial intelligence (AI) and\nhas demonstrated breakthrough performance improvements on a wide range of tasks\nin biomedicine. We anticipate that in the future deep learning will be widely\nused to predict personalized drug response and optimize medication selection\nand dosing, using knowledge extracted from large and complex molecular,\nepidemiological, clinical, and demographic datasets.\n",
        "published": "2018",
        "authors": [
            "Alexandr A. Kalinin",
            "Gerald A. Higgins",
            "Narathip Reamaroon",
            "S. M. Reza Soroushmehr",
            "Ari Allyn-Feuer",
            "Ivo D. Dinov",
            "Kayvan Najarian",
            "Brian D. Athey"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.09211v1",
        "title": "Analyzing the benefits of communication channels between deep learning\n  models",
        "abstract": "  As artificial intelligence systems spread to more diverse and larger tasks in\nmany domains, the machine learning algorithms, and in particular the deep\nlearning models and the databases required to train them are getting bigger\nthemselves. Some algorithms do allow for some scaling of large computations by\nleveraging data parallelism. However, they often require a large amount of data\nto be exchanged in order to ensure the shared knowledge throughout the compute\nnodes is accurate.\n  In this work, the effect of different levels of communications between deep\nlearning models is studied, in particular how it affects performance. The first\napproach studied looks at decentralizing the numerous computations that are\ndone in parallel in training procedures such as synchronous and asynchronous\nstochastic gradient descent. In this setting, a simplified communication that\nconsists of exchanging low bandwidth outputs between compute nodes can be\nbeneficial. In the following chapter, the communication protocol is slightly\nmodified to further include training instructions. Indeed, this is studied in a\nsimplified setup where a pre-trained model, analogous to a teacher, can\ncustomize a randomly initialized model's training procedure to accelerate\nlearning. Finally, a communication channel where two deep learning models can\nexchange a purposefully crafted language is explored while allowing for\ndifferent ways of optimizing that language.\n",
        "published": "2019",
        "authors": [
            "Philippe Lacaille"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.11477v1",
        "title": "Robust-MBFD: A Robust Deep Learning System for Motor Bearing Faults\n  Detection Using Multiple Deep Learning Training Strategies and A Novel Double\n  Loss Function",
        "abstract": "  This paper presents a comprehensive analysis of motor bearing fault detection\n(MBFD), which involves the task of identifying faults in a motor bearing based\non its vibration. To this end, we first propose and evaluate various machine\nlearning based systems for the MBFD task. Furthermore, we propose three deep\nlearning based systems for the MBFD task, each of which explores one of the\nfollowing training strategies: supervised learning, semi-supervised learning,\nand unsupervised learning. The proposed machine learning based systems and deep\nlearning based systems are evaluated, compared, and then they are used to\nidentify the best model for the MBFD task. We conducted extensive experiments\non various benchmark datasets of motor bearing faults, including those from the\nAmerican Society for Mechanical Failure Prevention Technology (MFPT), Case\nWestern Reserve University Bearing Center (CWRU), and the Condition Monitoring\nof Bearing Damage in Electromechanical Drive Systems from Paderborn University\n(PU). The experimental results on different datasets highlight two main\ncontributions of this study. First, we prove that deep learning based systems\nare more effective than machine learning based systems for the MBFD task.\nSecond, we achieve a robust and general deep learning based system with a novel\nloss function for the MBFD task on several benchmark datasets, demonstrating\nits potential for real-life MBFD applications.\n",
        "published": "2023",
        "authors": [
            "Khoa Tran",
            "Lam Pham",
            "Hai-Canh Vu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.00314v1",
        "title": "Adversarial-Aware Deep Learning System based on a Secondary Classical\n  Machine Learning Verification Approach",
        "abstract": "  Deep learning models have been used in creating various effective image\nclassification applications. However, they are vulnerable to adversarial\nattacks that seek to misguide the models into predicting incorrect classes. Our\nstudy of major adversarial attack models shows that they all specifically\ntarget and exploit the neural networking structures in their designs. This\nunderstanding makes us develop a hypothesis that most classical machine\nlearning models, such as Random Forest (RF), are immune to adversarial attack\nmodels because they do not rely on neural network design at all. Our\nexperimental study of classical machine learning models against popular\nadversarial attacks supports this hypothesis. Based on this hypothesis, we\npropose a new adversarial-aware deep learning system by using a classical\nmachine learning model as the secondary verification system to complement the\nprimary deep learning model in image classification. Although the secondary\nclassical machine learning model has less accurate output, it is only used for\nverification purposes, which does not impact the output accuracy of the primary\ndeep learning model, and at the same time, can effectively detect an\nadversarial attack when a clear mismatch occurs. Our experiments based on\nCIFAR-100 dataset show that our proposed approach outperforms current\nstate-of-the-art adversarial defense systems.\n",
        "published": "2023",
        "authors": [
            "Mohammed Alkhowaiter",
            "Hisham Kholidy",
            "Mnassar Alyami",
            "Abdulmajeed Alghamdi",
            "Cliff Zou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.00696v1",
        "title": "Graph Learning: A Survey",
        "abstract": "  Graphs are widely used as a popular representation of the network structure\nof connected data. Graph data can be found in a broad spectrum of application\ndomains such as social systems, ecosystems, biological networks, knowledge\ngraphs, and information systems. With the continuous penetration of artificial\nintelligence technologies, graph learning (i.e., machine learning on graphs) is\ngaining attention from both researchers and practitioners. Graph learning\nproves effective for many tasks, such as classification, link prediction, and\nmatching. Generally, graph learning methods extract relevant features of graphs\nby taking advantage of machine learning algorithms. In this survey, we present\na comprehensive overview on the state-of-the-art of graph learning. Special\nattention is paid to four categories of existing graph learning methods,\nincluding graph signal processing, matrix factorization, random walk, and deep\nlearning. Major models and algorithms under these categories are reviewed\nrespectively. We examine graph learning applications in areas such as text,\nimages, science, knowledge graphs, and combinatorial optimization. In addition,\nwe discuss several promising research directions in this field.\n",
        "published": "2021",
        "authors": [
            "Feng Xia",
            "Ke Sun",
            "Shuo Yu",
            "Abdul Aziz",
            "Liangtian Wan",
            "Shirui Pan",
            "Huan Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.06123v2",
        "title": "Adversarial Attacks and Defenses in Explainable Artificial Intelligence:\n  A Survey",
        "abstract": "  Explainable artificial intelligence (XAI) methods are portrayed as a remedy\nfor debugging and trusting statistical and deep learning models, as well as\ninterpreting their predictions. However, recent advances in adversarial machine\nlearning (AdvML) highlight the limitations and vulnerabilities of\nstate-of-the-art explanation methods, putting their security and\ntrustworthiness into question. The possibility of manipulating, fooling or\nfairwashing evidence of the model's reasoning has detrimental consequences when\napplied in high-stakes decision-making and knowledge discovery. This survey\nprovides a comprehensive overview of research concerning adversarial attacks on\nexplanations of machine learning models, as well as fairness metrics. We\nintroduce a unified notation and taxonomy of methods facilitating a common\nground for researchers and practitioners from the intersecting research fields\nof AdvML and XAI. We discuss how to defend against attacks and design robust\ninterpretation methods. We contribute a list of existing insecurities in XAI\nand outline the emerging research directions in adversarial XAI (AdvXAI).\nFuture work should address improving explanation methods and evaluation\nprotocols to take into account the reported safety issues.\n",
        "published": "2023",
        "authors": [
            "Hubert Baniecki",
            "Przemyslaw Biecek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.07641v3",
        "title": "Deceptive AI Explanations: Creation and Detection",
        "abstract": "  Artificial intelligence (AI) comes with great opportunities but can also pose\nsignificant risks. Automatically generated explanations for decisions can\nincrease transparency and foster trust, especially for systems based on\nautomated predictions by AI models. However, given, e.g., economic incentives\nto create dishonest AI, to what extent can we trust explanations? To address\nthis issue, our work investigates how AI models (i.e., deep learning, and\nexisting instruments to increase transparency regarding AI decisions) can be\nused to create and detect deceptive explanations. As an empirical evaluation,\nwe focus on text classification and alter the explanations generated by\nGradCAM, a well-established explanation technique in neural networks. Then, we\nevaluate the effect of deceptive explanations on users in an experiment with\n200 participants. Our findings confirm that deceptive explanations can indeed\nfool humans. However, one can deploy machine learning (ML) methods to detect\nseemingly minor deception attempts with accuracy exceeding 80% given sufficient\ndomain knowledge. Without domain knowledge, one can still infer inconsistencies\nin the explanations in an unsupervised manner, given basic knowledge of the\npredictive model under scrutiny.\n",
        "published": "2020",
        "authors": [
            "Johannes Schneider",
            "Christian Meske",
            "Michalis Vlachos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.09932v1",
        "title": "Federated Learning Meets Fairness and Differential Privacy",
        "abstract": "  Deep learning's unprecedented success raises several ethical concerns ranging\nfrom biased predictions to data privacy. Researchers tackle these issues by\nintroducing fairness metrics, or federated learning, or differential privacy. A\nfirst, this work presents an ethical federated learning model, incorporating\nall three measures simultaneously. Experiments on the Adult, Bank and Dutch\ndatasets highlight the resulting ``empirical interplay\" between accuracy,\nfairness, and privacy.\n",
        "published": "2021",
        "authors": [
            "Manisha Padala",
            "Sankarshan Damle",
            "Sujit Gujar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.01950v3",
        "title": "Torchattacks: A PyTorch Repository for Adversarial Attacks",
        "abstract": "  Torchattacks is a PyTorch library that contains adversarial attacks to\ngenerate adversarial examples and to verify the robustness of deep learning\nmodels. The code can be found at\nhttps://github.com/Harry24k/adversarial-attacks-pytorch.\n",
        "published": "2020",
        "authors": [
            "Hoki Kim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.06458v1",
        "title": "Deep Probabilistic Programming Languages: A Qualitative Study",
        "abstract": "  Deep probabilistic programming languages try to combine the advantages of\ndeep learning with those of probabilistic programming languages. If successful,\nthis would be a big step forward in machine learning and programming languages.\nUnfortunately, as of now, this new crop of languages is hard to use and\nunderstand. This paper addresses this problem directly by explaining deep\nprobabilistic programming languages and indirectly by characterizing their\ncurrent strengths and weaknesses.\n",
        "published": "2018",
        "authors": [
            "Guillaume Baudart",
            "Martin Hirzel",
            "Louis Mandel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.09588v1",
        "title": "What are Neural Networks made of?",
        "abstract": "  The success of Deep Learning methods is not well understood, though various\nattempts at explaining it have been made, typically centered on properties of\nstochastic gradient descent. Even less clear is why certain neural network\narchitectures perform better than others. We provide a potential opening with\nthe hypothesis that neural network training is a form of Genetic Programming.\n",
        "published": "2019",
        "authors": [
            "Rene Schaub"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.05760v1",
        "title": "Notes on Deep Learning Theory",
        "abstract": "  These are the notes for the lectures that I was giving during Fall 2020 at\nthe Moscow Institute of Physics and Technology (MIPT) and at the Yandex School\nof Data Analysis (YSDA). The notes cover some aspects of initialization, loss\nlandscape, generalization, and a neural tangent kernel theory. While many other\ntopics (e.g. expressivity, a mean-field theory, a double descent phenomenon)\nare missing in the current version, we plan to add them in future revisions.\n",
        "published": "2020",
        "authors": [
            "Eugene A. Golikov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.11805v2",
        "title": "Disaster Monitoring using Unmanned Aerial Vehicles and Deep Learning",
        "abstract": "  Monitoring of disasters is crucial for mitigating their effects on the\nenvironment and human population, and can be facilitated by the use of unmanned\naerial vehicles (UAV), equipped with camera sensors that produce aerial photos\nof the areas of interest. A modern technique for recognition of events based on\naerial photos is deep learning. In this paper, we present the state of the art\nwork related to the use of deep learning techniques for disaster\nidentification. We demonstrate the potential of this technique in identifying\ndisasters with high accuracy, by means of a relatively simple deep learning\nmodel. Based on a dataset of 544 images (containing disaster images such as\nfires, earthquakes, collapsed buildings, tsunami and flooding, as well as\nnon-disaster scenes), our results show an accuracy of 91% achieved, indicating\nthat deep learning, combined with UAV equipped with camera sensors, have the\npotential to predict disasters with high accuracy.\n",
        "published": "2018",
        "authors": [
            "Andreas Kamilaris",
            "Francesc X. Prenafeta-Bold\u00fa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.07101v3",
        "title": "Application of Deep Learning in Fundus Image Processing for Ophthalmic\n  Diagnosis -- A Review",
        "abstract": "  An overview of the applications of deep learning in ophthalmic diagnosis\nusing retinal fundus images is presented. We also review various retinal image\ndatasets that can be used for deep learning purposes. Applications of deep\nlearning for segmentation of optic disk, blood vessels and retinal layer as\nwell as detection of lesions are reviewed. Recent deep learning models for\nclassification of diseases such as age-related macular degeneration,\nglaucoma,diabetic macular edema and diabetic retinopathy are also reported.\n",
        "published": "2018",
        "authors": [
            "Sourya Sengupta",
            "Amitojdeep Singh",
            "Henry A. Leopold",
            "Tanmay Gulati",
            "Vasudevan Lakshminarayanan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.14146v1",
        "title": "A Survey on Deep Learning Techniques for Video Anomaly Detection",
        "abstract": "  Anomaly detection in videos is a problem that has been studied for more than\na decade. This area has piqued the interest of researchers due to its wide\napplicability. Because of this, there has been a wide array of approaches that\nhave been proposed throughout the years and these approaches range from\nstatistical-based approaches to machine learning-based approaches. Numerous\nsurveys have already been conducted on this area but this paper focuses on\nproviding an overview on the recent advances in the field of anomaly detection\nusing Deep Learning. Deep Learning has been applied successfully in many fields\nof artificial intelligence such as computer vision, natural language processing\nand more. This survey, however, focuses on how Deep Learning has improved and\nprovided more insights to the area of video anomaly detection. This paper\nprovides a categorization of the different Deep Learning approaches with\nrespect to their objectives. Additionally, it also discusses the commonly used\ndatasets along with the common evaluation metrics. Afterwards, a discussion\nsynthesizing all of the recent approaches is made to provide direction and\npossible areas for future research.\n",
        "published": "2020",
        "authors": [
            "Jessie James P. Suarez",
            "Prospero C. Naval Jr"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.05100v2",
        "title": "KL-divergence Based Deep Learning for Discrete Time Model",
        "abstract": "  Neural Network (Deep Learning) is a modern model in Artificial Intelligence\nand it has been exploited in Survival Analysis. Although several improvements\nhave been shown by previous works, training an excellent deep learning model\nrequires a huge amount of data, which may not hold in practice. To address this\nchallenge, we develop a Kullback-Leibler-based (KL) deep learning procedure to\nintegrate external survival prediction models with newly collected\ntime-to-event data. Time-dependent KL discrimination information is utilized to\nmeasure the discrepancy between the external and internal data. This is the\nfirst work considering using prior information to deal with short data problem\nin Survival Analysis for deep learning. Simulation and real data results show\nthat the proposed model achieves better performance and higher robustness\ncompared with previous works.\n",
        "published": "2022",
        "authors": [
            "Li Liu",
            "Xiangeng Fang",
            "Di Wang",
            "Weijing Tang",
            "Kevin He"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.00014v1",
        "title": "Inertial Navigation Meets Deep Learning: A Survey of Current Trends and\n  Future Directions",
        "abstract": "  Inertial sensing is used in many applications and platforms, ranging from\nday-to-day devices such as smartphones to very complex ones such as autonomous\nvehicles. In recent years, the development of machine learning and deep\nlearning techniques has increased significantly in the field of inertial\nsensing. This is due to the development of efficient computing hardware and the\naccessibility of publicly available sensor data. These data-driven approaches\nare used to empower model-based navigation and sensor fusion algorithms. This\npaper provides an in-depth review of those deep learning methods. We examine\nseparately, each vehicle operation domain including land, air, and sea. Each\ndomain is divided into pure inertial advances and improvements based on filter\nparameters learning. In addition, we review deep learning approaches for\ncalibrating and denoising inertial sensors. Throughout the paper, we discuss\nthese trends and future directions. We also provide statistics on the commonly\nused approaches to illustrate their efficiency and stimulate further research\nin deep learning embedded in inertial navigation and fusion.\n",
        "published": "2023",
        "authors": [
            "Nadav Cohen",
            "Itzik Klein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.04945v2",
        "title": "Auto deep learning for bioacoustic signals",
        "abstract": "  This study investigates the potential of automated deep learning to enhance\nthe accuracy and efficiency of multi-class classification of bird\nvocalizations, compared against traditional manually-designed deep learning\nmodels. Using the Western Mediterranean Wetland Birds dataset, we investigated\nthe use of AutoKeras, an automated machine learning framework, to automate\nneural architecture search and hyperparameter tuning. Comparative analysis\nvalidates our hypothesis that the AutoKeras-derived model consistently\noutperforms traditional models like MobileNet, ResNet50 and VGG16. Our approach\nand findings underscore the transformative potential of automated deep learning\nfor advancing bioacoustics research and models. In fact, the automated\ntechniques eliminate the need for manual feature engineering and model design\nwhile improving performance. This study illuminates best practices in sampling,\nevaluation and reporting to enhance reproducibility in this nascent field. All\nthe code used is available at https:\n//github.com/giuliotosato/AutoKeras-bioacustic\n  Keywords: AutoKeras; automated deep learning; audio classification; Wetlands\nBird dataset; comparative analysis; bioacoustics; validation dataset;\nmulti-class classification; spectrograms.\n",
        "published": "2023",
        "authors": [
            "Giulio Tosato",
            "Abdelrahman Shehata",
            "Joshua Janssen",
            "Kees Kamp",
            "Pramatya Jati",
            "Dan Stowell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.01596v1",
        "title": "Combination of Hyperband and Bayesian Optimization for Hyperparameter\n  Optimization in Deep Learning",
        "abstract": "  Deep learning has achieved impressive results on many problems. However, it\nrequires high degree of expertise or a lot of experience to tune well the\nhyperparameters, and such manual tuning process is likely to be biased.\nMoreover, it is not practical to try out as many different hyperparameter\nconfigurations in deep learning as in other machine learning scenarios, because\nevaluating each single hyperparameter configuration in deep learning would mean\ntraining a deep neural network, which usually takes quite long time. Hyperband\nalgorithm achieves state-of-the-art performance on various hyperparameter\noptimization problems in the field of deep learning. However, Hyperband\nalgorithm does not utilize history information of previous explored\nhyperparameter configurations, thus the solution found is suboptimal. We\npropose to combine Hyperband algorithm with Bayesian optimization (which does\nnot ignore history when sampling next trial configuration). Experimental\nresults show that our combination approach is superior to other hyperparameter\noptimization approaches including Hyperband algorithm.\n",
        "published": "2018",
        "authors": [
            "Jiazhuo Wang",
            "Jason Xu",
            "Xuejun Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.11059v1",
        "title": "LocalGLMnet: interpretable deep learning for tabular data",
        "abstract": "  Deep learning models have gained great popularity in statistical modeling\nbecause they lead to very competitive regression models, often outperforming\nclassical statistical models such as generalized linear models. The\ndisadvantage of deep learning models is that their solutions are difficult to\ninterpret and explain, and variable selection is not easily possible because\ndeep learning models solve feature engineering and variable selection\ninternally in a nontransparent way. Inspired by the appealing structure of\ngeneralized linear models, we propose a new network architecture that shares\nsimilar features as generalized linear models, but provides superior predictive\npower benefiting from the art of representation learning. This new architecture\nallows for variable selection of tabular data and for interpretation of the\ncalibrated deep learning model, in fact, our approach provides an additive\ndecomposition in the spirit of Shapley values and integrated gradients.\n",
        "published": "2021",
        "authors": [
            "Ronald Richman",
            "Mario V. W\u00fcthrich"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.07132v1",
        "title": "Improving Data Quality through Deep Learning and Statistical Models",
        "abstract": "  Traditional data quality control methods are based on users experience or\npreviously established business rules, and this limits performance in addition\nto being a very time consuming process with lower than desirable accuracy.\nUtilizing deep learning, we can leverage computing resources and advanced\ntechniques to overcome these challenges and provide greater value to users. In\nthis paper, we, the authors, first review relevant works and discuss machine\nlearning techniques, tools, and statistical quality models. Second, we offer a\ncreative data quality framework based on deep learning and statistical model\nalgorithm for identifying data quality. Third, we use data involving salary\nlevels from an open dataset published by the state of Arkansas to demonstrate\nhow to identify outlier data and how to improve data quality via deep learning.\nFinally, we discuss future work.\n",
        "published": "2018",
        "authors": [
            "Wei Dai",
            "Kenji Yoshigoe",
            "William Parsley"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.04549v1",
        "title": "Multimodal Deep Learning for Flaw Detection in Software Programs",
        "abstract": "  We explore the use of multiple deep learning models for detecting flaws in\nsoftware programs. Current, standard approaches for flaw detection rely on a\nsingle representation of a software program (e.g., source code or a program\nbinary). We illustrate that, by using techniques from multimodal deep learning,\nwe can simultaneously leverage multiple representations of software programs to\nimprove flaw detection over single representation analyses. Specifically, we\nadapt three deep learning models from the multimodal learning literature for\nuse in flaw detection and demonstrate how these models outperform traditional\ndeep learning models. We present results on detecting software flaws using the\nJuliet Test Suite and Linux Kernel.\n",
        "published": "2020",
        "authors": [
            "Scott Heidbrink",
            "Kathryn N. Rodhouse",
            "Daniel M. Dunlavy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.00653v2",
        "title": "Sparse Deep Learning: A New Framework Immune to Local Traps and\n  Miscalibration",
        "abstract": "  Deep learning has powered recent successes of artificial intelligence (AI).\nHowever, the deep neural network, as the basic model of deep learning, has\nsuffered from issues such as local traps and miscalibration. In this paper, we\nprovide a new framework for sparse deep learning, which has the above issues\naddressed in a coherent way. In particular, we lay down a theoretical\nfoundation for sparse deep learning and propose prior annealing algorithms for\nlearning sparse neural networks. The former has successfully tamed the sparse\ndeep neural network into the framework of statistical modeling, enabling\nprediction uncertainty correctly quantified. The latter can be asymptotically\nguaranteed to converge to the global optimum, enabling the validity of the\ndown-stream statistical inference. Numerical result indicates the superiority\nof the proposed method compared to the existing ones.\n",
        "published": "2021",
        "authors": [
            "Yan Sun",
            "Wenjun Xiong",
            "Faming Liang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.14545v2",
        "title": "Explainable Deep Learning: A Field Guide for the Uninitiated",
        "abstract": "  Deep neural networks (DNNs) have become a proven and indispensable machine\nlearning tool. As a black-box model, it remains difficult to diagnose what\naspects of the model's input drive the decisions of a DNN. In countless\nreal-world domains, from legislation and law enforcement to healthcare, such\ndiagnosis is essential to ensure that DNN decisions are driven by aspects\nappropriate in the context of its use. The development of methods and studies\nenabling the explanation of a DNN's decisions has thus blossomed into an\nactive, broad area of research. A practitioner wanting to study explainable\ndeep learning may be intimidated by the plethora of orthogonal directions the\nfield has taken. This complexity is further exacerbated by competing\ndefinitions of what it means ``to explain'' the actions of a DNN and to\nevaluate an approach's ``ability to explain''. This article offers a field\nguide to explore the space of explainable deep learning aimed at those\nuninitiated in the field. The field guide: i) Introduces three simple\ndimensions defining the space of foundational methods that contribute to\nexplainable deep learning, ii) discusses the evaluations for model\nexplanations, iii) places explainability in the context of other related deep\nlearning research areas, and iv) finally elaborates on user-oriented\nexplanation designing and potential future directions on explainable deep\nlearning. We hope the guide is used as an easy-to-digest starting point for\nthose just embarking on research in this field.\n",
        "published": "2020",
        "authors": [
            "Gabrielle Ras",
            "Ning Xie",
            "Marcel van Gerven",
            "Derek Doran"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.03559v1",
        "title": "Deep Learning Towards Mobile Applications",
        "abstract": "  Recent years have witnessed an explosive growth of mobile devices. Mobile\ndevices are permeating every aspect of our daily lives. With the increasing\nusage of mobile devices and intelligent applications, there is a soaring demand\nfor mobile applications with machine learning services. Inspired by the\ntremendous success achieved by deep learning in many machine learning tasks, it\nbecomes a natural trend to push deep learning towards mobile applications.\nHowever, there exist many challenges to realize deep learning in mobile\napplications, including the contradiction between the miniature nature of\nmobile devices and the resource requirement of deep neural networks, the\nprivacy and security concerns about individuals' data, and so on. To resolve\nthese challenges, during the past few years, great leaps have been made in this\narea. In this paper, we provide an overview of the current challenges and\nrepresentative achievements about pushing deep learning on mobile devices from\nthree aspects: training with mobile data, efficient inference on mobile\ndevices, and applications of mobile deep learning. The former two aspects cover\nthe primary tasks of deep learning. Then, we go through our two recent\napplications that apply the data collected by mobile devices to inferring mood\ndisturbance and user identification. Finally, we conclude this paper with the\ndiscussion of the future of this area.\n",
        "published": "2018",
        "authors": [
            "Ji Wang",
            "Bokai Cao",
            "Philip S. Yu",
            "Lichao Sun",
            "Weidong Bao",
            "Xiaomin Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.00008v1",
        "title": "Why is AI hard and Physics simple?",
        "abstract": "  We discuss why AI is hard and why physics is simple. We discuss how physical\nintuition and the approach of theoretical physics can be brought to bear on the\nfield of artificial intelligence and specifically machine learning. We suggest\nthat the underlying project of machine learning and the underlying project of\nphysics are strongly coupled through the principle of sparsity, and we call\nupon theoretical physicists to work on AI as physicists. As a first step in\nthat direction, we discuss an upcoming book on the principles of deep learning\ntheory that attempts to realize this approach.\n",
        "published": "2021",
        "authors": [
            "Daniel A. Roberts"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.07791v1",
        "title": "To Learn or Not to Learn: Deep Learning Assisted Wireless Modem Design",
        "abstract": "  Deep learning is driving a radical paradigm shift in wireless communications,\nall the way from the application layer down to the physical layer. Despite\nthis, there is an ongoing debate as to what additional values artificial\nintelligence (or machine learning) could bring to us, particularly on the\nphysical layer design; and what penalties there may have? These questions\nmotivate a fundamental rethinking of the wireless modem design in the\nartificial intelligence era. Through several physical-layer case studies, we\nargue for a significant role that machine learning could play, for instance in\nparallel error-control coding and decoding, channel equalization, interference\ncancellation, as well as multiuser and multiantenna detection. In addition, we\nwill also discuss the fundamental bottlenecks of machine learning as well as\ntheir potential solutions in this paper.\n",
        "published": "2019",
        "authors": [
            "S. Xue",
            "A. Li",
            "J. Wang",
            "N. Yi",
            "Y. Ma",
            "R. Tafazolli",
            "T. Dodgson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.08323v2",
        "title": "How to estimate carbon footprint when training deep learning models? A\n  guide and review",
        "abstract": "  Machine learning and deep learning models have become essential in the recent\nfast development of artificial intelligence in many sectors of the society. It\nis now widely acknowledge that the development of these models has an\nenvironmental cost that has been analyzed in many studies. Several online and\nsoftware tools have been developed to track energy consumption while training\nmachine learning models. In this paper, we propose a comprehensive introduction\nand comparison of these tools for AI practitioners wishing to start estimating\nthe environmental impact of their work. We review the specific vocabulary, the\ntechnical requirements for each tool. We compare the energy consumption\nestimated by each tool on two deep neural networks for image processing and on\ndifferent types of servers. From these experiments, we provide some advice for\nbetter choosing the right tool and infrastructure.\n",
        "published": "2023",
        "authors": [
            "Lucia Bouza Heguerte",
            "Aur\u00e9lie Bugeau",
            "Lo\u00efc Lannelongue"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.10535v1",
        "title": "Deep Learning vs. Gradient Boosting: Benchmarking state-of-the-art\n  machine learning algorithms for credit scoring",
        "abstract": "  Artificial intelligence (AI) and machine learning (ML) have become vital to\nremain competitive for financial services companies around the globe. The two\nmodels currently competing for the pole position in credit risk management are\ndeep learning (DL) and gradient boosting machines (GBM). This paper benchmarked\nthose two algorithms in the context of credit scoring using three distinct\ndatasets with different features to account for the reality that model\nchoice/power is often dependent on the underlying characteristics of the\ndataset. The experiment has shown that GBM tends to be more powerful than DL\nand has also the advantage of speed due to lower computational requirements.\nThis makes GBM the winner and choice for credit scoring. However, it was also\nshown that the outperformance of GBM is not always guaranteed and ultimately\nthe concrete problem scenario or dataset will determine the final model choice.\nOverall, based on this study both algorithms can be considered state-of-the-art\nfor binary classification tasks on structured datasets, while GBM should be the\ngo-to solution for most problem scenarios due to easier use, significantly\nfaster training time, and superior accuracy.\n",
        "published": "2022",
        "authors": [
            "Marc Schmitt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.07426v3",
        "title": "Generalization in Machine Learning via Analytical Learning Theory",
        "abstract": "  This paper introduces a novel measure-theoretic theory for machine learning\nthat does not require statistical assumptions. Based on this theory, a new\nregularization method in deep learning is derived and shown to outperform\nprevious methods in CIFAR-10, CIFAR-100, and SVHN. Moreover, the proposed\ntheory provides a theoretical basis for a family of practically successful\nregularization methods in deep learning. We discuss several consequences of our\nresults on one-shot learning, representation learning, deep learning, and\ncurriculum learning. Unlike statistical learning theory, the proposed learning\ntheory analyzes each problem instance individually via measure theory, rather\nthan a set of problem instances via statistics. As a result, it provides\ndifferent types of results and insights when compared to statistical learning\ntheory.\n",
        "published": "2018",
        "authors": [
            "Kenji Kawaguchi",
            "Yoshua Bengio",
            "Vikas Verma",
            "Leslie Pack Kaelbling"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.00712v1",
        "title": "Worth of knowledge in deep learning",
        "abstract": "  Knowledge constitutes the accumulated understanding and experience that\nhumans use to gain insight into the world. In deep learning, prior knowledge is\nessential for mitigating shortcomings of data-driven models, such as data\ndependence, generalization ability, and compliance with constraints. To enable\nefficient evaluation of the worth of knowledge, we present a framework inspired\nby interpretable machine learning. Through quantitative experiments, we assess\nthe influence of data volume and estimation range on the worth of knowledge.\nOur findings elucidate the complex relationship between data and knowledge,\nincluding dependence, synergistic, and substitution effects. Our model-agnostic\nframework can be applied to a variety of common network architectures,\nproviding a comprehensive understanding of the role of prior knowledge in deep\nlearning models. It can also be used to improve the performance of informed\nmachine learning, as well as distinguish improper prior knowledge.\n",
        "published": "2023",
        "authors": [
            "Hao Xu",
            "Yuntian Chen",
            "Dongxiao Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.14963v2",
        "title": "Deep Learning and Artificial General Intelligence: Still a Long Way to\n  Go",
        "abstract": "  In recent years, deep learning using neural network architecture, i.e. deep\nneural networks, has been on the frontier of computer science research. It has\neven lead to superhuman performance in some problems, e.g., in computer vision,\ngames and biology, and as a result the term deep learning revolution was\ncoined. The undisputed success and rapid growth of deep learning suggests that,\nin future, it might become an enabler for Artificial General Intelligence\n(AGI). In this article, we approach this statement critically showing five\nmajor reasons of why deep neural networks, as of the current state, are not\nready to be the technique of choice for reaching AGI.\n",
        "published": "2022",
        "authors": [
            "Maciej \u015awiechowski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1407.7417v1",
        "title": "'Almost Sure' Chaotic Properties of Machine Learning Methods",
        "abstract": "  It has been demonstrated earlier that universal computation is 'almost\nsurely' chaotic. Machine learning is a form of computational fixed point\niteration, iterating over the computable function space. We showcase some\nproperties of this iteration, and establish in general that the iteration is\n'almost surely' of chaotic nature. This theory explains the observation in the\ncounter intuitive properties of deep learning methods. This paper demonstrates\nthat these properties are going to be universal to any learning method.\n",
        "published": "2014",
        "authors": [
            "Nabarun Mondal",
            "Partha P. Ghosh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1706.05098v1",
        "title": "An Overview of Multi-Task Learning in Deep Neural Networks",
        "abstract": "  Multi-task learning (MTL) has led to successes in many applications of\nmachine learning, from natural language processing and speech recognition to\ncomputer vision and drug discovery. This article aims to give a general\noverview of MTL, particularly in deep neural networks. It introduces the two\nmost common methods for MTL in Deep Learning, gives an overview of the\nliterature, and discusses recent advances. In particular, it seeks to help ML\npractitioners apply MTL by shedding light on how MTL works and providing\nguidelines for choosing appropriate auxiliary tasks.\n",
        "published": "2017",
        "authors": [
            "Sebastian Ruder"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.00723v1",
        "title": "Deep Learning for Identifying Potential Conceptual Shifts for\n  Co-creative Drawing",
        "abstract": "  We present a system for identifying conceptual shifts between visual\ncategories, which will form the basis for a co-creative drawing system to help\nusers draw more creative sketches. The system recognizes human sketches and\nmatches them to structurally similar sketches from categories to which they do\nnot belong. This would allow a co-creative drawing system to produce an\nambiguous sketch that blends features from both categories.\n",
        "published": "2018",
        "authors": [
            "Pegah Karimi",
            "Nicholas Davis",
            "Kazjon Grace",
            "Mary Lou Maher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.03148v3",
        "title": "Generating Artificial Data for Private Deep Learning",
        "abstract": "  In this paper, we propose generating artificial data that retain statistical\nproperties of real data as the means of providing privacy with respect to the\noriginal dataset. We use generative adversarial network to draw\nprivacy-preserving artificial data samples and derive an empirical method to\nassess the risk of information disclosure in a differential-privacy-like way.\nOur experiments show that we are able to generate artificial data of high\nquality and successfully train and validate machine learning models on this\ndata while limiting potential privacy loss.\n",
        "published": "2018",
        "authors": [
            "Aleksei Triastcyn",
            "Boi Faltings"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.10040v1",
        "title": "Towards Aggregating Weighted Feature Attributions",
        "abstract": "  Current approaches for explaining machine learning models fall into two\ndistinct classes: antecedent event influence and value attribution. The former\nleverages training instances to describe how much influence a training point\nexerts on a test point, while the latter attempts to attribute value to the\nfeatures most pertinent to a given prediction. In this work, we discuss an\nalgorithm, AVA: Aggregate Valuation of Antecedents, that fuses these two\nexplanation classes to form a new approach to feature attribution that not only\nretrieves local explanations but also captures global patterns learned by a\nmodel. Our experimentation convincingly favors weighting and aggregating\nfeature attributions via AVA.\n",
        "published": "2019",
        "authors": [
            "Umang Bhatt",
            "Pradeep Ravikumar",
            "Jose M. F. Moura"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1611.00625v2",
        "title": "TorchCraft: a Library for Machine Learning Research on Real-Time\n  Strategy Games",
        "abstract": "  We present TorchCraft, a library that enables deep learning research on\nReal-Time Strategy (RTS) games such as StarCraft: Brood War, by making it\neasier to control these games from a machine learning framework, here Torch.\nThis white paper argues for using RTS games as a benchmark for AI research, and\ndescribes the design and components of TorchCraft.\n",
        "published": "2016",
        "authors": [
            "Gabriel Synnaeve",
            "Nantas Nardelli",
            "Alex Auvolat",
            "Soumith Chintala",
            "Timoth\u00e9e Lacroix",
            "Zeming Lin",
            "Florian Richoux",
            "Nicolas Usunier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.02716v1",
        "title": "Real-time regression analysis with deep convolutional neural networks",
        "abstract": "  We discuss the development of novel deep learning algorithms to enable\nreal-time regression analysis for time series data. We showcase the application\nof this new method with a timely case study, and then discuss the applicability\nof this approach to tackle similar challenges across science domains.\n",
        "published": "2018",
        "authors": [
            "E. A. Huerta",
            "Daniel George",
            "Zhizhen Zhao",
            "Gabrielle Allen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.15235v1",
        "title": "KNN, An Underestimated Model for Regional Rainfall Forecasting",
        "abstract": "  Regional rainfall forecasting is an important issue in hydrology and\nmeteorology. This paper aims to design an integrated tool by applying various\nmachine learning algorithms, especially the state-of-the-art deep learning\nalgorithms including Deep Neural Network, Wide Neural Network, Deep and Wide\nNeural Network, Reservoir Computing, Long Short Term Memory, Support Vector\nMachine, K-Nearest Neighbor for forecasting regional precipitations over\ndifferent catchments in Upstate New York. Through the experimental results and\nthe comparison among machine learning models including classification and\nregression, we find that KNN is an outstanding model over other models to\nhandle the uncertainty in the precipitation data. The data normalization\nmethods such as ZScore and MinMax are also evaluated and discussed.\n",
        "published": "2021",
        "authors": [
            "Ning Yu",
            "Timothy Haskins"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.05842v1",
        "title": "Causality in Neural Networks -- An Extended Abstract",
        "abstract": "  Causal reasoning is the main learning and explanation tool used by humans. AI\nsystems should possess causal reasoning capabilities to be deployed in the real\nworld with trust and reliability. Introducing the ideas of causality to machine\nlearning helps in providing better learning and explainable models.\nExplainability, causal disentanglement are some important aspects of any\nmachine learning model. Causal explanations are required to believe in a\nmodel's decision and causal disentanglement learning is important for transfer\nlearning applications. We exploit the ideas of causality to be used in deep\nlearning models to achieve better and causally explainable models that are\nuseful in fairness, disentangled representation, etc.\n",
        "published": "2021",
        "authors": [
            "Abbavaram Gowtham Reddy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.00938v1",
        "title": "Machine Learning Constructives and Local Searches for the Travelling\n  Salesman Problem",
        "abstract": "  The ML-Constructive heuristic is a recently presented method and the first\nhybrid method capable of scaling up to real scale traveling salesman problems.\nIt combines machine learning techniques and classic optimization techniques. In\nthis paper we present improvements to the computational weight of the original\ndeep learning model. In addition, as simpler models reduce the execution time,\nthe possibility of adding a local-search phase is explored to further improve\nperformance. Experimental results corroborate the quality of the proposed\nimprovements.\n",
        "published": "2021",
        "authors": [
            "Tommaso Vitali",
            "Umberto Junior Mele",
            "Luca Maria Gambardella",
            "Roberto Montemanni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.05593v1",
        "title": "Learning the Wrong Lessons: Inserting Trojans During Knowledge\n  Distillation",
        "abstract": "  In recent years, knowledge distillation has become a cornerstone of\nefficiently deployed machine learning, with labs and industries using knowledge\ndistillation to train models that are inexpensive and resource-optimized.\nTrojan attacks have contemporaneously gained significant prominence, revealing\nfundamental vulnerabilities in deep learning models. Given the widespread use\nof knowledge distillation, in this work we seek to exploit the unlabelled data\nknowledge distillation process to embed Trojans in a student model without\nintroducing conspicuous behavior in the teacher. We ultimately devise a Trojan\nattack that effectively reduces student accuracy, does not alter teacher\nperformance, and is efficiently constructible in practice.\n",
        "published": "2023",
        "authors": [
            "Leonard Tang",
            "Tom Shlomi",
            "Alexander Cai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.05757v2",
        "title": "A comparative study of non-deep learning, deep learning, and ensemble\n  learning methods for sunspot number prediction",
        "abstract": "  Solar activity has significant impacts on human activities and health. One\nmost commonly used measure of solar activity is the sunspot number. This paper\ncompares three important non-deep learning models, four popular deep learning\nmodels, and their five ensemble models in forecasting sunspot numbers. In\nparticular, we propose an ensemble model called XGBoost-DL, which uses XGBoost\nas a two-level nonlinear ensemble method to combine the deep learning models.\nOur XGBoost-DL achieves the best forecasting performance (RMSE = 25.70 and MAE\n= 19.82) in the comparison, outperforming the best non-deep learning model\nSARIMA (RMSE = 54.11 and MAE = 45.51), the best deep learning model Informer\n(RMSE = 29.90 and MAE = 22.35) and the NASA's forecast (RMSE = 48.38 and MAE =\n38.45). Our XGBoost-DL forecasts a peak sunspot number of 133.47 in May 2025\nfor Solar Cycle 25 and 164.62 in November 2035 for Solar Cycle 26, similar to\nbut later than the NASA's at 137.7 in October 2024 and 161.2 in December 2034.\nAn open-source Python package of our XGBoost-DL for the sunspot number\nprediction is available at https://github.com/yd1008/ts_ensemble_sunspot.\n",
        "published": "2022",
        "authors": [
            "Yuchen Dang",
            "Ziqi Chen",
            "Heng Li",
            "Hai Shu"
        ]
    }
]