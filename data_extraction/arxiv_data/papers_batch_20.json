[
    {
        "id": "http://arxiv.org/abs/2304.05856v1",
        "title": "RESET: Revisiting Trajectory Sets for Conditional Behavior Prediction",
        "abstract": "  It is desirable to predict the behavior of traffic participants conditioned\non different planned trajectories of the autonomous vehicle. This allows the\ndownstream planner to estimate the impact of its decisions. Recent approaches\nfor conditional behavior prediction rely on a regression decoder, meaning that\ncoordinates or polynomial coefficients are regressed. In this work we revisit\nset-based trajectory prediction, where the probability of each trajectory in a\npredefined trajectory set is determined by a classification model, and\nfirst-time employ it to the task of conditional behavior prediction. We propose\nRESET, which combines a new metric-driven algorithm for trajectory set\ngeneration with a graph-based encoder. For unconditional prediction, RESET\nachieves comparable performance to a regression-based approach. Due to the\nnature of set-based approaches, it has the advantageous property of being able\nto predict a flexible number of trajectories without influencing runtime or\ncomplexity. For conditional prediction, RESET achieves reasonable results with\nlate fusion of the planned trajectory, which was not observed for\nregression-based approaches before. This means that RESET is computationally\nlightweight to combine with a planner that proposes multiple future plans of\nthe autonomous vehicle, as large parts of the forward pass can be reused.\n",
        "published": "2023",
        "authors": [
            "Julian Schmidt",
            "Pascal Huissel",
            "Julian Wiederer",
            "Julian Jordan",
            "Vasileios Belagiannis",
            "Klaus Dietmayer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.04021v1",
        "title": "Energy-Based Models for Cross-Modal Localization using Convolutional\n  Transformers",
        "abstract": "  We present a novel framework using Energy-Based Models (EBMs) for localizing\na ground vehicle mounted with a range sensor against satellite imagery in the\nabsence of GPS. Lidar sensors have become ubiquitous on autonomous vehicles for\ndescribing its surrounding environment. Map priors are typically built using\nthe same sensor modality for localization purposes. However, these map building\nendeavors using range sensors are often expensive and time-consuming.\nAlternatively, we leverage the use of satellite images as map priors, which are\nwidely available, easily accessible, and provide comprehensive coverage. We\npropose a method using convolutional transformers that performs accurate\nmetric-level localization in a cross-modal manner, which is challenging due to\nthe drastic difference in appearance between the sparse range sensor readings\nand the rich satellite imagery. We train our model end-to-end and demonstrate\nour approach achieving higher accuracy than the state-of-the-art on KITTI,\nPandaset, and a custom dataset.\n",
        "published": "2023",
        "authors": [
            "Alan Wu",
            "Michael S. Ryoo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.14941v2",
        "title": "SIMMF: Semantics-aware Interactive Multiagent Motion Forecasting for\n  Autonomous Vehicle Driving",
        "abstract": "  Autonomous vehicles require motion forecasting of their surrounding\nmultiagents (pedestrians and vehicles) to make optimal decisions for\nnavigation. The existing methods focus on techniques to utilize the positions\nand velocities of these agents and fail to capture semantic information from\nthe scene. Moreover, to mitigate the increase in computational complexity\nassociated with the number of agents in the scene, some works leverage\nEuclidean distance to prune far-away agents. However, distance-based metric\nalone is insufficient to select relevant agents and accurately perform their\npredictions. To resolve these issues, we propose the Semantics-aware\nInteractive Multiagent Motion Forecasting (SIMMF) method to capture semantics\nalong with spatial information and optimally select relevant agents for motion\nprediction. Specifically, we achieve this by implementing a semantic-aware\nselection of relevant agents from the scene and passing them through an\nattention mechanism to extract global encodings. These encodings along with\nagents' local information, are passed through an encoder to obtain\ntime-dependent latent variables for a motion policy predicting the future\ntrajectories. Our results show that the proposed approach outperforms\nstate-of-the-art baselines and provides more accurate and scene-consistent\npredictions.\n",
        "published": "2023",
        "authors": [
            "Vidyaa Krishnan Nivash",
            "Ahmed H. Qureshi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.17529v1",
        "title": "Locking On: Leveraging Dynamic Vehicle-Imposed Motion Constraints to\n  Improve Visual Localization",
        "abstract": "  Most 6-DoF localization and SLAM systems use static landmarks but ignore\ndynamic objects because they cannot be usefully incorporated into a typical\npipeline. Where dynamic objects have been incorporated, typical approaches have\nattempted relatively sophisticated identification and localization of these\nobjects, limiting their robustness or general utility. In this research, we\npropose a middle ground, demonstrated in the context of autonomous vehicles,\nusing dynamic vehicles to provide limited pose constraint information in a\n6-DoF frame-by-frame PnP-RANSAC localization pipeline. We refine initial pose\nestimates with a motion model and propose a method for calculating the\npredicted quality of future pose estimates, triggered based on whether or not\nthe autonomous vehicle's motion is constrained by the relative frame-to-frame\nlocation of dynamic vehicles in the environment. Our approach detects and\nidentifies suitable dynamic vehicles to define these pose constraints to modify\na pose filter, resulting in improved recall across a range of localization\ntolerances from $0.25m$ to $5m$, compared to a state-of-the-art baseline single\nimage PnP method and its vanilla pose filtering. Our constraint detection\nsystem is active for approximately $35\\%$ of the time on the Ford AV dataset\nand localization is particularly improved when the constraint detection is\nactive.\n",
        "published": "2023",
        "authors": [
            "Stephen Hausler",
            "Sourav Garg",
            "Punarjay Chakravarty",
            "Shubham Shrivastava",
            "Ankit Vora",
            "Michael Milford"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.03175v1",
        "title": "Push Past Green: Learning to Look Behind Plant Foliage by Moving It",
        "abstract": "  Autonomous agriculture applications (e.g., inspection, phenotyping, plucking\nfruits) require manipulating the plant foliage to look behind the leaves and\nthe branches. Partial visibility, extreme clutter, thin structures, and unknown\ngeometry and dynamics for plants make such manipulation challenging. We tackle\nthese challenges through data-driven methods. We use self-supervision to train\nSRPNet, a neural network that predicts what space is revealed on execution of a\ncandidate action on a given plant. We use SRPNet with the cross-entropy method\nto predict actions that are effective at revealing space beneath plant foliage.\nFurthermore, as SRPNet does not just predict how much space is revealed but\nalso where it is revealed, we can execute a sequence of actions that\nincrementally reveal more and more space beneath the plant foliage. We\nexperiment with a synthetic (vines) and a real plant (Dracaena) on a physical\ntest-bed across 5 settings including 2 settings that test generalization to\nnovel plant configurations. Our experiments reveal the effectiveness of our\noverall method, PPG, over a competitive hand-crafted exploration method, and\nthe effectiveness of SRPNet over a hand-crafted dynamics model and relevant\nablations.\n",
        "published": "2023",
        "authors": [
            "Xiaoyu Zhang",
            "Saurabh Gupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.15320v1",
        "title": "Robust Visual Sim-to-Real Transfer for Robotic Manipulation",
        "abstract": "  Learning visuomotor policies in simulation is much safer and cheaper than in\nthe real world. However, due to discrepancies between the simulated and real\ndata, simulator-trained policies often fail when transferred to real robots.\nOne common approach to bridge the visual sim-to-real domain gap is domain\nrandomization (DR). While previous work mainly evaluates DR for disembodied\ntasks, such as pose estimation and object detection, here we systematically\nexplore visual domain randomization methods and benchmark them on a rich set of\nchallenging robotic manipulation tasks. In particular, we propose an off-line\nproxy task of cube localization to select DR parameters for texture\nrandomization, lighting randomization, variations of object colors and camera\nparameters. Notably, we demonstrate that DR parameters have similar impact on\nour off-line proxy task and on-line policies. We, hence, use off-line optimized\nDR parameters to train visuomotor policies in simulation and directly apply\nsuch policies to a real robot. Our approach achieves 93% success rate on\naverage when tested on a diverse set of challenging manipulation tasks.\nMoreover, we evaluate the robustness of policies to visual variations in real\nscenes and show that our simulator-trained policies outperform policies learned\nusing real but limited data. Code, simulation environment, real robot datasets\nand trained models are available at\nhttps://www.di.ens.fr/willow/research/robust_s2r/.\n",
        "published": "2023",
        "authors": [
            "Ricardo Garcia",
            "Robin Strudel",
            "Shizhe Chen",
            "Etienne Arlaud",
            "Ivan Laptev",
            "Cordelia Schmid"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.10707v1",
        "title": "Sensor Fusion by Spatial Encoding for Autonomous Driving",
        "abstract": "  Sensor fusion is critical to perception systems for task domains such as\nautonomous driving and robotics. Recently, the Transformer integrated with CNN\nhas demonstrated high performance in sensor fusion for various perception\ntasks. In this work, we introduce a method for fusing data from camera and\nLiDAR. By employing Transformer modules at multiple resolutions, proposed\nmethod effectively combines local and global contextual relationships. The\nperformance of the proposed method is validated by extensive experiments with\ntwo adversarial benchmarks with lengthy routes and high-density traffics. The\nproposed method outperforms previous approaches with the most challenging\nbenchmarks, achieving significantly higher driving and infraction scores.\nCompared with TransFuser, it achieves 8% and 19% improvement in driving scores\nfor the Longest6 and Town05 Long benchmarks, respectively.\n",
        "published": "2023",
        "authors": [
            "Quoc-Vinh Lai-Dang",
            "Jihui Lee",
            "Bumgeun Park",
            "Dongsoo Har"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.19797v2",
        "title": "DEFT: Dexterous Fine-Tuning for Real-World Hand Policies",
        "abstract": "  Dexterity is often seen as a cornerstone of complex manipulation. Humans are\nable to perform a host of skills with their hands, from making food to\noperating tools. In this paper, we investigate these challenges, especially in\nthe case of soft, deformable objects as well as complex, relatively\nlong-horizon tasks. However, learning such behaviors from scratch can be data\ninefficient. To circumvent this, we propose a novel approach, DEFT (DExterous\nFine-Tuning for Hand Policies), that leverages human-driven priors, which are\nexecuted directly in the real world. In order to improve upon these priors,\nDEFT involves an efficient online optimization procedure. With the integration\nof human-based learning and online fine-tuning, coupled with a soft robotic\nhand, DEFT demonstrates success across various tasks, establishing a robust,\ndata-efficient pathway toward general dexterous manipulation. Please see our\nwebsite at https://dexterous-finetuning.github.io for video results.\n",
        "published": "2023",
        "authors": [
            "Aditya Kannan",
            "Kenneth Shaw",
            "Shikhar Bahl",
            "Pragna Mannam",
            "Deepak Pathak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1509.07831v2",
        "title": "Deep Multimodal Embedding: Manipulating Novel Objects with Point-clouds,\n  Language and Trajectories",
        "abstract": "  A robot operating in a real-world environment needs to perform reasoning over\na variety of sensor modalities such as vision, language and motion\ntrajectories. However, it is extremely challenging to manually design features\nrelating such disparate modalities. In this work, we introduce an algorithm\nthat learns to embed point-cloud, natural language, and manipulation trajectory\ndata into a shared embedding space with a deep neural network. To learn\nsemantically meaningful spaces throughout our network, we use a loss-based\nmargin to bring embeddings of relevant pairs closer together while driving\nless-relevant cases from different modalities further apart. We use this both\nto pre-train its lower layers and fine-tune our final embedding space, leading\nto a more robust representation. We test our algorithm on the task of\nmanipulating novel objects and appliances based on prior experience with other\nobjects. On a large dataset, we achieve significant improvements in both\naccuracy and inference time over the previous state of the art. We also perform\nend-to-end experiments on a PR2 robot utilizing our learned embedding space.\n",
        "published": "2015",
        "authors": [
            "Jaeyong Sung",
            "Ian Lenz",
            "Ashutosh Saxena"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.02199v4",
        "title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning\n  and Large-Scale Data Collection",
        "abstract": "  We describe a learning-based approach to hand-eye coordination for robotic\ngrasping from monocular images. To learn hand-eye coordination for grasping, we\ntrained a large convolutional neural network to predict the probability that\ntask-space motion of the gripper will result in successful grasps, using only\nmonocular camera images and independently of camera calibration or the current\nrobot pose. This requires the network to observe the spatial relationship\nbetween the gripper and objects in the scene, thus learning hand-eye\ncoordination. We then use this network to servo the gripper in real time to\nachieve successful grasps. To train our network, we collected over 800,000\ngrasp attempts over the course of two months, using between 6 and 14 robotic\nmanipulators at any given time, with differences in camera placement and\nhardware. Our experimental evaluation demonstrates that our method achieves\neffective real-time control, can successfully grasp novel objects, and corrects\nmistakes by continuous servoing.\n",
        "published": "2016",
        "authors": [
            "Sergey Levine",
            "Peter Pastor",
            "Alex Krizhevsky",
            "Deirdre Quillen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1605.00164v2",
        "title": "Look-ahead before you leap: end-to-end active recognition by forecasting\n  the effect of motion",
        "abstract": "  Visual recognition systems mounted on autonomous moving agents face the\nchallenge of unconstrained data, but simultaneously have the opportunity to\nimprove their performance by moving to acquire new views of test data. In this\nwork, we first show how a recurrent neural network-based system may be trained\nto perform end-to-end learning of motion policies suited for this \"active\nrecognition\" setting. Further, we hypothesize that active vision requires an\nagent to have the capacity to reason about the effects of its motions on its\nview of the world. To verify this hypothesis, we attempt to induce this\ncapacity in our active recognition pipeline, by simultaneously learning to\nforecast the effects of the agent's motions on its internal representation of\nthe environment conditional on all past views. Results across two challenging\ndatasets confirm both that our end-to-end system successfully learns meaningful\npolicies for active category recognition, and that \"learning to look ahead\"\nfurther boosts recognition performance.\n",
        "published": "2016",
        "authors": [
            "Dinesh Jayaraman",
            "Kristen Grauman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1605.07157v4",
        "title": "Unsupervised Learning for Physical Interaction through Video Prediction",
        "abstract": "  A core challenge for an agent learning to interact with the world is to\npredict how its actions affect objects in its environment. Many existing\nmethods for learning the dynamics of physical interactions require labeled\nobject information. However, to scale real-world interaction learning to a\nvariety of scenes and objects, acquiring labeled data becomes increasingly\nimpractical. To learn about physical object motion without labels, we develop\nan action-conditioned video prediction model that explicitly models pixel\nmotion, by predicting a distribution over pixel motion from previous frames.\nBecause our model explicitly predicts motion, it is partially invariant to\nobject appearance, enabling it to generalize to previously unseen objects. To\nexplore video prediction for real-world interactive agents, we also introduce a\ndataset of 59,000 robot interactions involving pushing motions, including a\ntest set with novel objects. In this dataset, accurate prediction of videos\nconditioned on the robot's future actions amounts to learning a \"visual\nimagination\" of different futures based on different courses of action. Our\nexperiments show that our proposed method produces more accurate video\npredictions both quantitatively and qualitatively, when compared to prior\nmethods.\n",
        "published": "2016",
        "authors": [
            "Chelsea Finn",
            "Ian Goodfellow",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1608.01127v1",
        "title": "Autonomous Grounding of Visual Field Experience through Sensorimotor\n  Prediction",
        "abstract": "  In a developmental framework, autonomous robots need to explore the world and\nlearn how to interact with it. Without an a priori model of the system, this\nopens the challenging problem of having robots master their interface with the\nworld: how to perceive their environment using their sensors, and how to act in\nit using their motors. The sensorimotor approach of perception claims that a\nnaive agent can learn to master this interface by capturing regularities in the\nway its actions transform its sensory inputs. In this paper, we apply such an\napproach to the discovery and mastery of the visual field associated with a\nvisual sensor. A computational model is formalized and applied to a simulated\nsystem to illustrate the approach.\n",
        "published": "2016",
        "authors": [
            "Alban Laflaqui\u00e8re"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1609.09365v3",
        "title": "Deep Tracking on the Move: Learning to Track the World from a Moving\n  Vehicle using Recurrent Neural Networks",
        "abstract": "  This paper presents an end-to-end approach for tracking static and dynamic\nobjects for an autonomous vehicle driving through crowded urban environments.\nUnlike traditional approaches to tracking, this method is learned end-to-end,\nand is able to directly predict a full unoccluded occupancy grid map from raw\nlaser input data. Inspired by the recently presented DeepTracking approach\n[Ondruska, 2016], we employ a recurrent neural network (RNN) to capture the\ntemporal evolution of the state of the environment, and propose to use Spatial\nTransformer modules to exploit estimates of the egomotion of the vehicle. Our\nresults demonstrate the ability to track a range of objects, including cars,\nbuses, pedestrians, and cyclists through occlusion, from both moving and\nstationary platforms, using a single learned model. Experimental results\ndemonstrate that the model can also predict the future states of objects from\ncurrent inputs, with greater accuracy than previous work.\n",
        "published": "2016",
        "authors": [
            "Julie Dequaire",
            "Dushyant Rao",
            "Peter Ondruska",
            "Dominic Wang",
            "Ingmar Posner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1702.03920v3",
        "title": "Cognitive Mapping and Planning for Visual Navigation",
        "abstract": "  We introduce a neural architecture for navigation in novel environments. Our\nproposed architecture learns to map from first-person views and plans a\nsequence of actions towards goals in the environment. The Cognitive Mapper and\nPlanner (CMP) is based on two key ideas: a) a unified joint architecture for\nmapping and planning, such that the mapping is driven by the needs of the task,\nand b) a spatial memory with the ability to plan given an incomplete set of\nobservations about the world. CMP constructs a top-down belief map of the world\nand applies a differentiable neural net planner to produce the next action at\neach time step. The accumulated belief of the world enables the agent to track\nvisited regions of the environment. We train and test CMP on navigation\nproblems in simulation environments derived from scans of real world buildings.\nOur experiments demonstrate that CMP outperforms alternate learning-based\narchitectures, as well as, classical mapping and path planning approaches in\nmany cases. Furthermore, it naturally extends to semantically specified goals,\nsuch as 'going to a chair'. We also deploy CMP on physical robots in indoor\nenvironments, where it achieves reasonable performance, even though it is\ntrained entirely in simulation.\n",
        "published": "2017",
        "authors": [
            "Saurabh Gupta",
            "Varun Tolani",
            "James Davidson",
            "Sergey Levine",
            "Rahul Sukthankar",
            "Jitendra Malik"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.01040v2",
        "title": "Learning Robot Activities from First-Person Human Videos Using\n  Convolutional Future Regression",
        "abstract": "  We design a new approach that allows robot learning of new activities from\nunlabeled human example videos. Given videos of humans executing the same\nactivity from a human's viewpoint (i.e., first-person videos), our objective is\nto make the robot learn the temporal structure of the activity as its future\nregression network, and learn to transfer such model for its own motor\nexecution. We present a new deep learning model: We extend the state-of-the-art\nconvolutional object detection network for the representation/estimation of\nhuman hands in training videos, and newly introduce the concept of using a\nfully convolutional network to regress (i.e., predict) the intermediate scene\nrepresentation corresponding to the future frame (e.g., 1-2 seconds later).\nCombining these allows direct prediction of future locations of human hands and\nobjects, which enables the robot to infer the motor control plan using our\nmanipulation network. We experimentally confirm that our approach makes\nlearning of robot activities from unlabeled human interaction videos possible,\nand demonstrate that our robot is able to execute the learned collaborative\nactivities in real-time directly based on its camera input.\n",
        "published": "2017",
        "authors": [
            "Jangwon Lee",
            "Michael S. Ryoo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.03550v1",
        "title": "CORe50: a New Dataset and Benchmark for Continuous Object Recognition",
        "abstract": "  Continuous/Lifelong learning of high-dimensional data streams is a\nchallenging research problem. In fact, fully retraining models each time new\ndata become available is infeasible, due to computational and storage issues,\nwhile na\\\"ive incremental strategies have been shown to suffer from\ncatastrophic forgetting. In the context of real-world object recognition\napplications (e.g., robotic vision), where continuous learning is crucial, very\nfew datasets and benchmarks are available to evaluate and compare emerging\ntechniques. In this work we propose a new dataset and benchmark CORe50,\nspecifically designed for continuous object recognition, and introduce baseline\napproaches for different continuous learning scenarios.\n",
        "published": "2017",
        "authors": [
            "Vincenzo Lomonaco",
            "Davide Maltoni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.03167v1",
        "title": "RegNet: Multimodal Sensor Registration Using Deep Neural Networks",
        "abstract": "  In this paper, we present RegNet, the first deep convolutional neural network\n(CNN) to infer a 6 degrees of freedom (DOF) extrinsic calibration between\nmultimodal sensors, exemplified using a scanning LiDAR and a monocular camera.\nCompared to existing approaches, RegNet casts all three conventional\ncalibration steps (feature extraction, feature matching and global regression)\ninto a single real-time capable CNN. Our method does not require any human\ninteraction and bridges the gap between classical offline and target-less\nonline calibration approaches as it provides both a stable initial estimation\nas well as a continuous online correction of the extrinsic parameters. During\ntraining we randomly decalibrate our system in order to train RegNet to infer\nthe correspondence between projected depth measurements and RGB image and\nfinally regress the extrinsic calibration. Additionally, with an iterative\nexecution of multiple CNNs, that are trained on different magnitudes of\ndecalibration, our approach compares favorably to state-of-the-art methods in\nterms of a mean calibration error of 0.28 degrees for the rotational and 6 cm\nfor the translation components even for large decalibrations up to 1.5 m and 20\ndegrees.\n",
        "published": "2017",
        "authors": [
            "Nick Schneider",
            "Florian Piewak",
            "Christoph Stiller",
            "Uwe Franke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.00267v2",
        "title": "Acquiring Target Stacking Skills by Goal-Parameterized Deep\n  Reinforcement Learning",
        "abstract": "  Understanding physical phenomena is a key component of human intelligence and\nenables physical interaction with previously unseen environments. In this\npaper, we study how an artificial agent can autonomously acquire this intuition\nthrough interaction with the environment. We created a synthetic block stacking\nenvironment with physics simulation in which the agent can learn a policy\nend-to-end through trial and error. Thereby, we bypass to explicitly model\nphysical knowledge within the policy. We are specifically interested in tasks\nthat require the agent to reach a given goal state that may be different for\nevery new trial. To this end, we propose a deep reinforcement learning\nframework that learns policies which are parametrized by a goal. We validated\nthe model on a toy example navigating in a grid world with different target\npositions and in a block stacking task with different target structures of the\nfinal tower. In contrast to prior work, our policies show better generalization\nacross different goals.\n",
        "published": "2017",
        "authors": [
            "Wenbin Li",
            "Jeannette Bohg",
            "Mario Fritz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.03938v1",
        "title": "CARLA: An Open Urban Driving Simulator",
        "abstract": "  We introduce CARLA, an open-source simulator for autonomous driving research.\nCARLA has been developed from the ground up to support development, training,\nand validation of autonomous urban driving systems. In addition to open-source\ncode and protocols, CARLA provides open digital assets (urban layouts,\nbuildings, vehicles) that were created for this purpose and can be used freely.\nThe simulation platform supports flexible specification of sensor suites and\nenvironmental conditions. We use CARLA to study the performance of three\napproaches to autonomous driving: a classic modular pipeline, an end-to-end\nmodel trained via imitation learning, and an end-to-end model trained via\nreinforcement learning. The approaches are evaluated in controlled scenarios of\nincreasing difficulty, and their performance is examined via metrics provided\nby CARLA, illustrating the platform's utility for autonomous driving research.\nThe supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E\n",
        "published": "2017",
        "authors": [
            "Alexey Dosovitskiy",
            "German Ros",
            "Felipe Codevilla",
            "Antonio Lopez",
            "Vladlen Koltun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.06623v2",
        "title": "Driven to Distraction: Self-Supervised Distractor Learning for Robust\n  Monocular Visual Odometry in Urban Environments",
        "abstract": "  We present a self-supervised approach to ignoring \"distractors\" in camera\nimages for the purposes of robustly estimating vehicle motion in cluttered\nurban environments. We leverage offline multi-session mapping approaches to\nautomatically generate a per-pixel ephemerality mask and depth map for each\ninput image, which we use to train a deep convolutional network. At run-time we\nuse the predicted ephemerality and depth as an input to a monocular visual\nodometry (VO) pipeline, using either sparse features or dense photometric\nmatching. Our approach yields metric-scale VO using only a single camera and\ncan recover the correct egomotion even when 90% of the image is obscured by\ndynamic, independently moving objects. We evaluate our robust VO methods on\nmore than 400km of driving from the Oxford RobotCar Dataset and demonstrate\nreduced odometry drift and significantly improved egomotion estimation in the\npresence of large moving vehicles in urban traffic.\n",
        "published": "2017",
        "authors": [
            "Dan Barnes",
            "Will Maddern",
            "Geoffrey Pascoe",
            "Ingmar Posner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.00411v2",
        "title": "Dense 3D Object Reconstruction from a Single Depth View",
        "abstract": "  In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs\nthe complete 3D structure of a given object from a single arbitrary depth view\nusing generative adversarial networks. Unlike existing work which typically\nrequires multiple views of the same object or class labels to recover the full\n3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation\nof a depth view of the object as input, and is able to generate the complete 3D\noccupancy grid with a high resolution of 256^3 by recovering the\noccluded/missing regions. The key idea is to combine the generative\ncapabilities of autoencoders and the conditional Generative Adversarial\nNetworks (GAN) framework, to infer accurate and fine-grained 3D structures of\nobjects in high-dimensional voxel space. Extensive experiments on large\nsynthetic datasets and real-world Kinect datasets show that the proposed\n3D-RecGAN++ significantly outperforms the state of the art in single view 3D\nobject reconstruction, and is able to reconstruct unseen types of objects.\n",
        "published": "2018",
        "authors": [
            "Bo Yang",
            "Stefano Rosa",
            "Andrew Markham",
            "Niki Trigoni",
            "Hongkai Wen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.01557v1",
        "title": "One-Shot Imitation from Observing Humans via Domain-Adaptive\n  Meta-Learning",
        "abstract": "  Humans and animals are capable of learning a new behavior by observing others\nperform the skill just once. We consider the problem of allowing a robot to do\nthe same -- learning from a raw video pixels of a human, even when there is\nsubstantial domain shift in the perspective, environment, and embodiment\nbetween the robot and the observed human. Prior approaches to this problem have\nhand-specified how human and robot actions correspond and often relied on\nexplicit human pose detection systems. In this work, we present an approach for\none-shot learning from a video of a human by using human and robot\ndemonstration data from a variety of previous tasks to build up prior knowledge\nthrough meta-learning. Then, combining this prior knowledge and only a single\nvideo demonstration from a human, the robot can perform the task that the human\ndemonstrated. We show experiments on both a PR2 arm and a Sawyer arm,\ndemonstrating that after meta-learning, the robot can learn to place, push, and\npick-and-place new objects using just one video of a human performing the\nmanipulation.\n",
        "published": "2018",
        "authors": [
            "Tianhe Yu",
            "Chelsea Finn",
            "Annie Xie",
            "Sudeep Dasari",
            "Tianhao Zhang",
            "Pieter Abbeel",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.00653v1",
        "title": "Semi-parametric Topological Memory for Navigation",
        "abstract": "  We introduce a new memory architecture for navigation in previously unseen\nenvironments, inspired by landmark-based navigation in animals. The proposed\nsemi-parametric topological memory (SPTM) consists of a (non-parametric) graph\nwith nodes corresponding to locations in the environment and a (parametric)\ndeep network capable of retrieving nodes from the graph based on observations.\nThe graph stores no metric information, only connectivity of locations\ncorresponding to the nodes. We use SPTM as a planning module in a navigation\nsystem. Given only 5 minutes of footage of a previously unseen maze, an\nSPTM-based navigation agent can build a topological map of the environment and\nuse it to confidently navigate towards goals. The average success rate of the\nSPTM agent in goal-directed navigation across test environments is higher than\nthe best-performing baseline by a factor of three. A video of the agent is\navailable at https://youtu.be/vRF7f4lhswo\n",
        "published": "2018",
        "authors": [
            "Nikolay Savinov",
            "Alexey Dosovitskiy",
            "Vladlen Koltun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.00275v2",
        "title": "Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from\n  LiDAR and Monocular Camera",
        "abstract": "  Depth completion, the technique of estimating a dense depth image from sparse\ndepth measurements, has a variety of applications in robotics and autonomous\ndriving. However, depth completion faces 3 main challenges: the irregularly\nspaced pattern in the sparse depth input, the difficulty in handling multiple\nsensor modalities (when color images are available), as well as the lack of\ndense, pixel-level ground truth depth labels. In this work, we address all\nthese challenges. Specifically, we develop a deep regression model to learn a\ndirect mapping from sparse depth (and color images) to dense depth. We also\npropose a self-supervised training framework that requires only sequences of\ncolor and sparse depth images, without the need for dense depth labels. Our\nexperiments demonstrate that our network, when trained with semi-dense\nannotations, attains state-of-the- art accuracy and is the winning approach on\nthe KITTI depth completion benchmark at the time of submission. Furthermore,\nthe self-supervised framework outperforms a number of existing solutions\ntrained with semi- dense annotations.\n",
        "published": "2018",
        "authors": [
            "Fangchang Ma",
            "Guilherme Venturelli Cavalheiro",
            "Sertac Karaman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.00703v1",
        "title": "Introducing the Simulated Flying Shapes and Simulated Planar Manipulator\n  Datasets",
        "abstract": "  We release two artificial datasets, Simulated Flying Shapes and Simulated\nPlanar Manipulator that allow to test the learning ability of video processing\nsystems. In particular, the dataset is meant as a tool which allows to easily\nassess the sanity of deep neural network models that aim to encode, reconstruct\nor predict video frame sequences. The datasets each consist of 90000 videos.\nThe Simulated Flying Shapes dataset comprises scenes showing two objects of\nequal shape (rectangle, triangle and circle) and size in which one object\napproaches its counterpart. The Simulated Planar Manipulator shows a 3-DOF\nplanar manipulator that executes a pick-and-place task in which it has to place\na size-varying circle on a squared platform. Different from other widely used\ndatasets such as moving MNIST [1], [2], the two presented datasets involve\ngoal-oriented tasks (e.g. the manipulator grasping an object and placing it on\na platform), rather than showing random movements. This makes our datasets more\nsuitable for testing prediction capabilities and the learning of sophisticated\nmotions by a machine learning model. This technical document aims at providing\nan introduction into the usage of both datasets.\n",
        "published": "2018",
        "authors": [
            "Fabio Ferreira",
            "Jonas Rothfuss",
            "Eren Erdal Aksoy",
            "You Zhou",
            "Tamim Asfour"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.03480v2",
        "title": "Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video\n  Demonstration",
        "abstract": "  Our goal is to generate a policy to complete an unseen task given just a\nsingle video demonstration of the task in a given domain. We hypothesize that\nto successfully generalize to unseen complex tasks from a single video\ndemonstration, it is necessary to explicitly incorporate the compositional\nstructure of the tasks into the model. To this end, we propose Neural Task\nGraph (NTG) Networks, which use conjugate task graph as the intermediate\nrepresentation to modularize both the video demonstration and the derived\npolicy. We empirically show NTG achieves inter-task generalization on two\ncomplex tasks: Block Stacking in BulletPhysics and Object Collection in\nAI2-THOR. NTG improves data efficiency with visual input as well as achieve\nstrong generalization without the need for dense hierarchical supervision. We\nfurther show that similar performance trends hold when applied to real-world\ndata. We show that NTG can effectively predict task structure on the JIGSAWS\nsurgical dataset and generalize to unseen tasks.\n",
        "published": "2018",
        "authors": [
            "De-An Huang",
            "Suraj Nair",
            "Danfei Xu",
            "Yuke Zhu",
            "Animesh Garg",
            "Li Fei-Fei",
            "Silvio Savarese",
            "Juan Carlos Niebles"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.06757v1",
        "title": "On Evaluation of Embodied Navigation Agents",
        "abstract": "  Skillful mobile operation in three-dimensional environments is a primary\ntopic of study in Artificial Intelligence. The past two years have seen a surge\nof creative work on navigation. This creative output has produced a plethora of\nsometimes incompatible task definitions and evaluation protocols. To coordinate\nongoing and future research in this area, we have convened a working group to\nstudy empirical methodology in navigation research. The present document\nsummarizes the consensus recommendations of this working group. We discuss\ndifferent problem statements and the role of generalization, present evaluation\nmeasures, and provide standard scenarios that can be used for benchmarking.\n",
        "published": "2018",
        "authors": [
            "Peter Anderson",
            "Angel Chang",
            "Devendra Singh Chaplot",
            "Alexey Dosovitskiy",
            "Saurabh Gupta",
            "Vladlen Koltun",
            "Jana Kosecka",
            "Jitendra Malik",
            "Roozbeh Mottaghi",
            "Manolis Savva",
            "Amir R. Zamir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.07049v1",
        "title": "Robot Learning in Homes: Improving Generalization and Reducing Dataset\n  Bias",
        "abstract": "  Data-driven approaches to solving robotic tasks have gained a lot of traction\nin recent years. However, most existing policies are trained on large-scale\ndatasets collected in curated lab settings. If we aim to deploy these models in\nunstructured visual environments like people's homes, they will be unable to\ncope with the mismatch in data distribution. In such light, we present the\nfirst systematic effort in collecting a large dataset for robotic grasping in\nhomes. First, to scale and parallelize data collection, we built a low cost\nmobile manipulator assembled for under 3K USD. Second, data collected using low\ncost robots suffer from noisy labels due to imperfect execution and calibration\nerrors. To handle this, we develop a framework which factors out the noise as a\nlatent variable. Our model is trained on 28K grasps collected in several houses\nunder an array of different environmental conditions. We evaluate our models by\nphysically executing grasps on a collection of novel objects in multiple unseen\nhomes. The models trained with our home dataset showed a marked improvement of\n43.7% over a baseline model trained with data collected in lab. Our\narchitecture which explicitly models the latent noise in the dataset also\nperformed 10% better than one that did not factor out the noise. We hope this\neffort inspires the robotics community to look outside the lab and embrace\nlearning based approaches to handle inaccurate cheap robots.\n",
        "published": "2018",
        "authors": [
            "Abhinav Gupta",
            "Adithyavairavan Murali",
            "Dhiraj Gandhi",
            "Lerrel Pinto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.05105v2",
        "title": "Uncertainty-Aware Driver Trajectory Prediction at Urban Intersections",
        "abstract": "  Predicting the motion of a driver's vehicle is crucial for advanced driving\nsystems, enabling detection of potential risks towards shared control between\nthe driver and automation systems. In this paper, we propose a variational\nneural network approach that predicts future driver trajectory distributions\nfor the vehicle based on multiple sensors. Our predictor generates both a\nconditional variational distribution of future trajectories, as well as a\nconfidence estimate for different time horizons. Our approach allows us to\nhandle inherently uncertain situations, and reason about information gain from\neach input, as well as combine our model with additional predictors, creating a\nmixture of experts. We show how to augment the variational predictor with a\nphysics-based predictor, and based on their confidence estimations, improve\noverall system performance. The resulting combined model is aware of the\nuncertainty associated with its predictions, which can help the vehicle\nautonomy to make decisions with more confidence. The model is validated on\nreal-world urban driving data collected in multiple locations. This validation\ndemonstrates that our approach improves the prediction error of a physics-based\nmodel by 25% while successfully identifying the uncertain cases with 82%\naccuracy.\n",
        "published": "2019",
        "authors": [
            "Xin Huang",
            "Stephen McGill",
            "Brian C. Williams",
            "Luke Fletcher",
            "Guy Rosman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.10915v2",
        "title": "Benchmarking Classic and Learned Navigation in Complex 3D Environments",
        "abstract": "  Navigation research is attracting renewed interest with the advent of\nlearning-based methods. However, this new line of work is largely disconnected\nfrom well-established classic navigation approaches. In this paper, we take a\nstep towards coordinating these two directions of research. We set up classic\nand learning-based navigation systems in common simulated environments and\nthoroughly evaluate them in indoor spaces of varying complexity, with access to\ndifferent sensory modalities. Additionally, we measure human performance in the\nsame environments. We find that a classic pipeline, when properly tuned, can\nperform very well in complex cluttered environments. On the other hand, learned\nsystems can operate more robustly with a limited sensor suite. Overall, both\napproaches are still far from human-level performance.\n",
        "published": "2019",
        "authors": [
            "Dmytro Mishkin",
            "Alexey Dosovitskiy",
            "Vladlen Koltun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.10968v1",
        "title": "Bootstrapping Robotic Ecological Perception from a Limited Set of\n  Hypotheses Through Interactive Perception",
        "abstract": "  To solve its task, a robot needs to have the ability to interpret its\nperceptions. In vision, this interpretation is particularly difficult and\nrelies on the understanding of the structure of the scene, at least to the\nextent of its task and sensorimotor abilities. A robot with the ability to\nbuild and adapt this interpretation process according to its own tasks and\ncapabilities would push away the limits of what robots can achieve in a non\ncontrolled environment. A solution is to provide the robot with processes to\nbuild such representations that are not specific to an environment or a\nsituation. A lot of works focus on objects segmentation, recognition and\nmanipulation. Defining an object solely on the basis of its visual appearance\nis challenging given the wide range of possible objects and environments.\nTherefore, current works make simplifying assumptions about the structure of a\nscene. Such assumptions reduce the adaptivity of the object extraction process\nto the environments in which the assumption holds. To limit such assumptions,\nwe introduce an exploration method aimed at identifying moveable elements in a\nscene without considering the concept of object. By using the interactive\nperception framework, we aim at bootstrapping the acquisition process of a\nrepresentation of the environment with a minimum of context specific\nassumptions. The robotic system builds a perceptual map called relevance map\nwhich indicates the moveable parts of the current scene. A classifier is\ntrained online to predict the category of each region (moveable or\nnon-moveable). It is also used to select a region with which to interact, with\nthe goal of minimizing the uncertainty of the classification. A specific\nclassifier is introduced to fit these needs: the collaborative mixture models\nclassifier. The method is tested on a set of scenarios of increasing\ncomplexity, using both simulations and a PR2 robot.\n",
        "published": "2019",
        "authors": [
            "L\u00e9ni K. Le Goff",
            "Ghanim Mukhtar",
            "Alexandre Coninx",
            "St\u00e9phane Doncieux"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.05947v1",
        "title": "DIViS: Domain Invariant Visual Servoing for Collision-Free Goal Reaching",
        "abstract": "  Robots should understand both semantics and physics to be functional in the\nreal world. While robot platforms provide means for interacting with the\nphysical world they cannot autonomously acquire object-level semantics without\nneeding human. In this paper, we investigate how to minimize human effort and\nintervention to teach robots perform real world tasks that incorporate\nsemantics. We study this question in the context of visual servoing of mobile\nrobots and propose DIViS, a Domain Invariant policy learning approach for\ncollision free Visual Servoing. DIViS incorporates high level semantics from\npreviously collected static human-labeled datasets and learns collision free\nservoing entirely in simulation and without any real robot data. However, DIViS\ncan directly be deployed on a real robot and is capable of servoing to the\nuser-specified object categories while avoiding collisions in the real world.\nDIViS is not constrained to be queried by the final view of goal but rather is\nrobust to servo to image goals taken from initial robot view with high\nocclusions without this impairing its ability to maintain a collision free\npath. We show the generalization capability of DIViS on real mobile robots in\nmore than 90 real world test scenarios with various unseen object goals in\nunstructured environments. DIViS is compared to prior approaches via real world\nexperiments and rigorous tests in simulation. For supplementary videos, see:\n\\href{https://fsadeghi.github.io/DIViS}{https://fsadeghi.github.io/DIViS}\n",
        "published": "2019",
        "authors": [
            "Fereshteh Sadeghi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.10194v1",
        "title": "Learning to See the Wood for the Trees: Deep Laser Localization in Urban\n  and Natural Environments on a CPU",
        "abstract": "  Localization in challenging, natural environments such as forests or\nwoodlands is an important capability for many applications from guiding a robot\nnavigating along a forest trail to monitoring vegetation growth with handheld\nsensors. In this work we explore laser-based localization in both urban and\nnatural environments, which is suitable for online applications. We propose a\ndeep learning approach capable of learning meaningful descriptors directly from\n3D point clouds by comparing triplets (anchor, positive and negative examples).\nThe approach learns a feature space representation for a set of segmented point\nclouds that are matched between a current and previous observations. Our\nlearning method is tailored towards loop closure detection resulting in a small\nmodel which can be deployed using only a CPU. The proposed learning method\nwould allow the full pipeline to run on robots with limited computational\npayload such as drones, quadrupeds or UGVs.\n",
        "published": "2019",
        "authors": [
            "Georgi Tinchev",
            "Adrian Penate-Sanchez",
            "Maurice Fallon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.03997v2",
        "title": "Semantic Understanding of Foggy Scenes with Purely Synthetic Data",
        "abstract": "  This work addresses the problem of semantic scene understanding under foggy\nroad conditions. Although marked progress has been made in semantic scene\nunderstanding over the recent years, it is mainly concentrated on clear weather\noutdoor scenes. Extending semantic segmentation methods to adverse weather\nconditions like fog is crucially important for outdoor applications such as\nself-driving cars. In this paper, we propose a novel method, which uses purely\nsynthetic data to improve the performance on unseen real-world foggy scenes\ncaptured in the streets of Zurich and its surroundings. Our results highlight\nthe potential and power of photo-realistic synthetic images for training and\nespecially fine-tuning deep neural nets. Our contributions are threefold, 1) we\ncreated a purely synthetic, high-quality foggy dataset of 25,000 unique outdoor\nscenes, that we call Foggy Synscapes and plan to release publicly 2) we show\nthat with this data we outperform previous approaches on real-world foggy test\ndata 3) we show that a combination of our data and previously used data can\neven further improve the performance on real-world foggy data.\n",
        "published": "2019",
        "authors": [
            "Martin Hahner",
            "Dengxin Dai",
            "Christos Sakaridis",
            "Jan-Nico Zaech",
            "Luc Van Gool"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.06988v1",
        "title": "Autonomous Aerial Cinematography In Unstructured Environments With\n  Learned Artistic Decision-Making",
        "abstract": "  Aerial cinematography is revolutionizing industries that require live and\ndynamic camera viewpoints such as entertainment, sports, and security. However,\nsafely piloting a drone while filming a moving target in the presence of\nobstacles is immensely taxing, often requiring multiple expert human operators.\nHence, there is demand for an autonomous cinematographer that can reason about\nboth geometry and scene context in real-time. Existing approaches do not\naddress all aspects of this problem; they either require high-precision\nmotion-capture systems or GPS tags to localize targets, rely on prior maps of\nthe environment, plan for short time horizons, or only follow artistic\nguidelines specified before flight.\n  In this work, we address the problem in its entirety and propose a complete\nsystem for real-time aerial cinematography that for the first time combines:\n(1) vision-based target estimation; (2) 3D signed-distance mapping for\nocclusion estimation; (3) efficient trajectory optimization for long\ntime-horizon camera motion; and (4) learning-based artistic shot selection. We\nextensively evaluate our system both in simulation and in field experiments by\nfilming dynamic targets moving through unstructured environments. Our results\nindicate that our system can operate reliably in the real world without\nrestrictive assumptions. We also provide in-depth analysis and discussions for\neach module, with the hope that our design tradeoffs can generalize to other\nrelated applications. Videos of the complete system can be found at:\nhttps://youtu.be/ookhHnqmlaU.\n",
        "published": "2019",
        "authors": [
            "Rogerio Bonatti",
            "Wenshan Wang",
            "Cherie Ho",
            "Aayush Ahuja",
            "Mirko Gschwindt",
            "Efe Camci",
            "Erdal Kayacan",
            "Sanjiban Choudhury",
            "Sebastian Scherer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.14442v3",
        "title": "Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive\n  Navigation in Cluttered Environments",
        "abstract": "  We present Interactive Gibson Benchmark, the first comprehensive benchmark\nfor training and evaluating Interactive Navigation: robot navigation strategies\nwhere physical interaction with objects is allowed and even encouraged to\naccomplish a task. For example, the robot can move objects if needed in order\nto clear a path leading to the goal location. Our benchmark comprises two novel\nelements: 1) a new experimental setup, the Interactive Gibson Environment\n(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high\nfidelity physical dynamics of the robot and common objects found in these\nscenes; 2) a set of Interactive Navigation metrics which allows one to study\nthe interplay between navigation and physical interaction. We present and\nevaluate multiple learning-based baselines in Interactive Gibson, and provide\ninsights into regimes of navigation with different trade-offs between\nnavigation path efficiency and disturbance of surrounding objects. We make our\nbenchmark publicly\navailable(https://sites.google.com/view/interactivegibsonenv) and encourage\nresearchers from all disciplines in robotics (e.g. planning, learning, control)\nto propose, evaluate, and compare their Interactive Navigation solutions in\nInteractive Gibson.\n",
        "published": "2019",
        "authors": [
            "Fei Xia",
            "William B. Shen",
            "Chengshu Li",
            "Priya Kasimbeg",
            "Micael Tchapmi",
            "Alexander Toshev",
            "Li Fei-Fei",
            "Roberto Mart\u00edn-Mart\u00edn",
            "Silvio Savarese"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.02673v1",
        "title": "SHOP-VRB: A Visual Reasoning Benchmark for Object Perception",
        "abstract": "  In this paper we present an approach and a benchmark for visual reasoning in\nrobotics applications, in particular small object grasping and manipulation.\nThe approach and benchmark are focused on inferring object properties from\nvisual and text data. It concerns small household objects with their\nproperties, functionality, natural language descriptions as well as\nquestion-answer pairs for visual reasoning queries along with their\ncorresponding scene semantic representations. We also present a method for\ngenerating synthetic data which allows to extend the benchmark to other objects\nor scenes and propose an evaluation protocol that is more challenging than in\nthe existing datasets. We propose a reasoning system based on symbolic program\nexecution. A disentangled representation of the visual and textual inputs is\nobtained and used to execute symbolic programs that represent a 'reasoning\nprocess' of the algorithm. We perform a set of experiments on the proposed\nbenchmark and compare to results for the state of the art methods. These\nresults expose the shortcomings of the existing benchmarks that may lead to\nmisleading conclusions on the actual performance of the visual reasoning\nsystems.\n",
        "published": "2020",
        "authors": [
            "Michal Nazarczuk",
            "Krystian Mikolajczyk"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.04851v1",
        "title": "Spatial Priming for Detecting Human-Object Interactions",
        "abstract": "  The relative spatial layout of a human and an object is an important cue for\ndetermining how they interact. However, until now, spatial layout has been used\njust as side-information for detecting human-object interactions (HOIs). In\nthis paper, we present a method for exploiting this spatial layout information\nfor detecting HOIs in images. The proposed method consists of a layout module\nwhich primes a visual module to predict the type of interaction between a human\nand an object. The visual and layout modules share information through lateral\nconnections at several stages. The model uses predictions from the layout\nmodule as a prior to the visual module and the prediction from the visual\nmodule is given as the final output. It also incorporates semantic information\nabout the object using word2vec vectors. The proposed model reaches an mAP of\n24.79% for HICO-Det dataset which is about 2.8% absolute points higher than the\ncurrent state-of-the-art.\n",
        "published": "2020",
        "authors": [
            "Ankan Bansal",
            "Sai Saketh Rambhatla",
            "Abhinav Shrivastava",
            "Rama Chellappa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.04954v1",
        "title": "Learning to Visually Navigate in Photorealistic Environments Without any\n  Supervision",
        "abstract": "  Learning to navigate in a realistic setting where an agent must rely solely\non visual inputs is a challenging task, in part because the lack of position\ninformation makes it difficult to provide supervision during training. In this\npaper, we introduce a novel approach for learning to navigate from image inputs\nwithout external supervision or reward. Our approach consists of three stages:\nlearning a good representation of first-person views, then learning to explore\nusing memory, and finally learning to navigate by setting its own goals. The\nmodel is trained with intrinsic rewards only so that it can be applied to any\nenvironment with image observations. We show the benefits of our approach by\ntraining an agent to navigate challenging photo-realistic environments from the\nGibson dataset with RGB inputs only.\n",
        "published": "2020",
        "authors": [
            "Lina Mezghani",
            "Sainbayar Sukhbaatar",
            "Arthur Szlam",
            "Armand Joulin",
            "Piotr Bojanowski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.07950v2",
        "title": "Learning visual policies for building 3D shape categories",
        "abstract": "  Manipulation and assembly tasks require non-trivial planning of actions\ndepending on the environment and the final goal. Previous work in this domain\noften assembles particular instances of objects from known sets of primitives.\nIn contrast, we aim to handle varying sets of primitives and to construct\ndifferent objects of a shape category. Given a single object instance of a\ncategory, e.g. an arch, and a binary shape classifier, we learn a visual policy\nto assemble other instances of the same category. In particular, we propose a\ndisassembly procedure and learn a state policy that discovers new object\ninstances and their assembly plans in state space. We then render simulated\nstates in the observation space and learn a heatmap representation to predict\nalternative actions from a given input image. To validate our approach, we\nfirst demonstrate its efficiency for building object categories in state space.\nWe then show the success of our visual policies for building arches from\ndifferent primitives. Moreover, we demonstrate (i) the reactive ability of our\nmethod to re-assemble objects using additional primitives and (ii) the robust\nperformance of our policy for unseen primitives resembling building blocks used\nduring training. Our visual assembly policies are trained with no real images\nand reach up to 95% success rate when evaluated on a real robot.\n",
        "published": "2020",
        "authors": [
            "Alexander Pashevich",
            "Igor Kalevatykh",
            "Ivan Laptev",
            "Cordelia Schmid"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.08051v3",
        "title": "Approximate Inverse Reinforcement Learning from Vision-based Imitation\n  Learning",
        "abstract": "  In this work, we present a method for obtaining an implicit objective\nfunction for vision-based navigation. The proposed methodology relies on\nImitation Learning, Model Predictive Control (MPC), and an interpretation\ntechnique used in Deep Neural Networks. We use Imitation Learning as a means to\ndo Inverse Reinforcement Learning in order to create an approximate cost\nfunction generator for a visual navigation challenge. The resulting cost\nfunction, the costmap, is used in conjunction with MPC for real-time control\nand outperforms other state-of-the-art costmap generators in novel\nenvironments. The proposed process allows for simple training and robustness to\nout-of-sample data. We apply our method to the task of vision-based autonomous\ndriving in multiple real and simulated environments and show its\ngeneralizability.\n",
        "published": "2020",
        "authors": [
            "Keuntaek Lee",
            "Bogdan Vlahov",
            "Jason Gibson",
            "James M. Rehg",
            "Evangelos A. Theodorou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.09141v2",
        "title": "Spatial Action Maps for Mobile Manipulation",
        "abstract": "  Typical end-to-end formulations for learning robotic navigation involve\npredicting a small set of steering command actions (e.g., step forward, turn\nleft, turn right, etc.) from images of the current state (e.g., a bird's-eye\nview of a SLAM reconstruction). Instead, we show that it can be advantageous to\nlearn with dense action representations defined in the same domain as the\nstate. In this work, we present \"spatial action maps,\" in which the set of\npossible actions is represented by a pixel map (aligned with the input image of\nthe current state), where each pixel represents a local navigational endpoint\nat the corresponding scene location. Using ConvNets to infer spatial action\nmaps from state images, action predictions are thereby spatially anchored on\nlocal visual features in the scene, enabling significantly faster learning of\ncomplex behaviors for mobile manipulation tasks with reinforcement learning. In\nour experiments, we task a robot with pushing objects to a goal location, and\nfind that policies learned with spatial action maps achieve much better\nperformance than traditional alternatives.\n",
        "published": "2020",
        "authors": [
            "Jimmy Wu",
            "Xingyuan Sun",
            "Andy Zeng",
            "Shuran Song",
            "Johnny Lee",
            "Szymon Rusinkiewicz",
            "Thomas Funkhouser"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.04905v1",
        "title": "One-Shot Visual Imitation Learning via Meta-Learning",
        "abstract": "  In order for a robot to be a generalist that can perform a wide range of\njobs, it must be able to acquire a wide variety of skills quickly and\nefficiently in complex unstructured environments. High-capacity models such as\ndeep neural networks can enable a robot to represent complex skills, but\nlearning each skill from scratch then becomes infeasible. In this work, we\npresent a meta-imitation learning method that enables a robot to learn how to\nlearn more efficiently, allowing it to acquire new skills from just a single\ndemonstration. Unlike prior methods for one-shot imitation, our method can\nscale to raw pixel inputs and requires data from significantly fewer prior\ntasks for effective learning of new skills. Our experiments on both simulated\nand real robot platforms demonstrate the ability to learn new tasks,\nend-to-end, from a single visual demonstration.\n",
        "published": "2017",
        "authors": [
            "Chelsea Finn",
            "Tianhe Yu",
            "Tianhao Zhang",
            "Pieter Abbeel",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.07857v2",
        "title": "Using Simulation and Domain Adaptation to Improve Efficiency of Deep\n  Robotic Grasping",
        "abstract": "  Instrumenting and collecting annotated visual grasping datasets to train\nmodern machine learning algorithms can be extremely time-consuming and\nexpensive. An appealing alternative is to use off-the-shelf simulators to\nrender synthetic data for which ground-truth annotations are generated\nautomatically. Unfortunately, models trained purely on simulated data often\nfail to generalize to the real world. We study how randomized simulated\nenvironments and domain adaptation methods can be extended to train a grasping\nsystem to grasp novel objects from raw monocular RGB images. We extensively\nevaluate our approaches with a total of more than 25,000 physical test grasps,\nstudying a range of simulation conditions and domain adaptation methods,\nincluding a novel extension of pixel-level domain adaptation that we term the\nGraspGAN. We show that, by using synthetic data and domain adaptation, we are\nable to reduce the number of real-world samples needed to achieve a given level\nof performance by up to 50 times, using only randomly generated simulated\nobjects. We also show that by using only unlabeled real-world data and our\nGraspGAN methodology, we obtain real-world grasping performance without any\nreal-world labels that is similar to that achieved with 939,777 labeled\nreal-world samples.\n",
        "published": "2017",
        "authors": [
            "Konstantinos Bousmalis",
            "Alex Irpan",
            "Paul Wohlhart",
            "Yunfei Bai",
            "Matthew Kelcey",
            "Mrinal Kalakrishnan",
            "Laura Downs",
            "Julian Ibarz",
            "Peter Pastor",
            "Kurt Konolige",
            "Sergey Levine",
            "Vincent Vanhoucke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.09882v2",
        "title": "Are we done with object recognition? The iCub robot's perspective",
        "abstract": "  We report on an extensive study of the benefits and limitations of current\ndeep learning approaches to object recognition in robot vision scenarios,\nintroducing a novel dataset used for our investigation. To avoid the biases in\ncurrently available datasets, we consider a natural human-robot interaction\nsetting to design a data-acquisition protocol for visual object recognition on\nthe iCub humanoid robot. Analyzing the performance of off-the-shelf models\ntrained off-line on large-scale image retrieval datasets, we show the necessity\nfor knowledge transfer. We evaluate different ways in which this last step can\nbe done, and identify the major bottlenecks affecting robotic scenarios. By\nstudying both object categorization and identification problems, we highlight\nkey differences between object recognition in robotics applications and in\nimage retrieval tasks, for which the considered deep learning approaches have\nbeen originally designed. In a nutshell, our results confirm the remarkable\nimprovements yield by deep learning in this setting, while pointing to specific\nopen challenges that need be addressed for seamless deployment in robotics.\n",
        "published": "2017",
        "authors": [
            "Giulia Pasquale",
            "Carlo Ciliberto",
            "Francesca Odone",
            "Lorenzo Rosasco",
            "Lorenzo Natale"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.05553v1",
        "title": "MLR (Memory, Learning and Recognition): A General Cognitive Model --\n  applied to Intelligent Robots and Systems Control",
        "abstract": "  This paper introduces a new perspective of intelligent robots and systems\ncontrol. The presented and proposed cognitive model: Memory, Learning and\nRecognition (MLR), is an effort to bridge the gap between Robotics, AI,\nCognitive Science, and Neuroscience. The currently existing gap prevents us\nfrom integrating the current advancement and achievements of these four\nresearch fields which are actively trying to define intelligence in either\napplication-based way or in generic way. This cognitive model defines\nintelligence more specifically, parametrically and detailed. The proposed MLR\nmodel helps us create a general control model for robots and systems\nindependent of their application domains and platforms since it is mainly based\non the dataset provided for robots and systems controls. This paper is mainly\nproposing and introducing this concept and trying to prove this concept in a\nsmall scale, firstly through experimentation. The proposed concept is also\napplicable to other different platforms in real-time as well as in simulation.\n",
        "published": "2019",
        "authors": [
            "Aras R. Dargazany"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1604.01802v2",
        "title": "Learning to Track at 100 FPS with Deep Regression Networks",
        "abstract": "  Machine learning techniques are often used in computer vision due to their\nability to leverage large amounts of training data to improve performance.\nUnfortunately, most generic object trackers are still trained from scratch\nonline and do not benefit from the large number of videos that are readily\navailable for offline training. We propose a method for offline training of\nneural networks that can track novel objects at test-time at 100 fps. Our\ntracker is significantly faster than previous methods that use neural networks\nfor tracking, which are typically very slow to run and not practical for\nreal-time applications. Our tracker uses a simple feed-forward network with no\nonline training required. The tracker learns a generic relationship between\nobject motion and appearance and can be used to track novel objects that do not\nappear in the training set. We test our network on a standard tracking\nbenchmark to demonstrate our tracker's state-of-the-art performance. Further,\nour performance improves as we add more videos to our offline training set. To\nthe best of our knowledge, our tracker is the first neural-network tracker that\nlearns to track generic objects at 100 fps.\n",
        "published": "2016",
        "authors": [
            "David Held",
            "Sebastian Thrun",
            "Silvio Savarese"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1604.03692v2",
        "title": "Learning Social Affordance for Human-Robot Interaction",
        "abstract": "  In this paper, we present an approach for robot learning of social affordance\nfrom human activity videos. We consider the problem in the context of\nhuman-robot interaction: Our approach learns structural representations of\nhuman-human (and human-object-human) interactions, describing how body-parts of\neach agent move with respect to each other and what spatial relations they\nshould maintain to complete each sub-event (i.e., sub-goal). This enables the\nrobot to infer its own movement in reaction to the human body motion, allowing\nit to naturally replicate such interactions.\n  We introduce the representation of social affordance and propose a generative\nmodel for its weakly supervised learning from human demonstration videos. Our\napproach discovers critical steps (i.e., latent sub-events) in an interaction\nand the typical motion associated with them, learning what body-parts should be\ninvolved and how. The experimental results demonstrate that our Markov Chain\nMonte Carlo (MCMC) based learning algorithm automatically discovers\nsemantically meaningful interactive affordance from RGB-D videos, which allows\nus to generate appropriate full body motion for an agent.\n",
        "published": "2016",
        "authors": [
            "Tianmin Shu",
            "M. S. Ryoo",
            "Song-Chun Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.00716v1",
        "title": "InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes\n  Dataset",
        "abstract": "  Datasets have gained an enormous amount of popularity in the computer vision\ncommunity, from training and evaluation of Deep Learning-based methods to\nbenchmarking Simultaneous Localization and Mapping (SLAM). Without a doubt,\nsynthetic imagery bears a vast potential due to scalability in terms of amounts\nof data obtainable without tedious manual ground truth annotations or\nmeasurements. Here, we present a dataset with the aim of providing a higher\ndegree of photo-realism, larger scale, more variability as well as serving a\nwider range of purposes compared to existing datasets. Our dataset leverages\nthe availability of millions of professional interior designs and millions of\nproduction-level furniture and object assets -- all coming with fine geometric\ndetails and high-resolution texture. We render high-resolution and high\nframe-rate video sequences following realistic trajectories while supporting\nvarious camera types as well as providing inertial measurements. Together with\nthe release of the dataset, we will make executable program of our interactive\nsimulator software as well as our renderer available at\nhttps://interiornetdataset.github.io. To showcase the usability and uniqueness\nof our dataset, we show benchmarking results of both sparse and dense SLAM\nalgorithms.\n",
        "published": "2018",
        "authors": [
            "Wenbin Li",
            "Sajad Saeedi",
            "John McCormac",
            "Ronald Clark",
            "Dimos Tzoumanikas",
            "Qing Ye",
            "Yuzhong Huang",
            "Rui Tang",
            "Stefan Leutenegger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.08626v3",
        "title": "Domain Adaptation for Robot Predictive Maintenance Systems",
        "abstract": "  Industrial robots play an increasingly important role in a growing number of\nfields. For example, robotics is used to increase productivity while reducing\ncosts in various aspects of manufacturing. Since robots are often set up in\nproduction lines, the breakdown of a single robot has a negative impact on the\nentire process, in the worst case bringing the whole line to a halt until the\nissue is resolved, leading to substantial financial losses due to the\nunforeseen downtime. Therefore, predictive maintenance systems based on the\ninternal signals of robots have gained attention as an essential component of\nrobotics service offerings. The main shortcoming of existing predictive\nmaintenance algorithms is that the extracted features typically differ\nsignificantly from the learnt model when the operation of the robot changes,\nincurring false alarms. In order to mitigate this problem, predictive\nmaintenance algorithms require the model to be retrained with normal data of\nthe new operation. In this paper, we propose a novel solution based on transfer\nlearning to pass the knowledge of the trained model from one operation to\nanother in order to prevent the need for retraining and to eliminate such false\nalarms. The deployment of the proposed unsupervised transfer learning algorithm\non real-world datasets demonstrates that the algorithm can not only distinguish\nbetween operation and mechanical condition change, it further yields a sharper\ndeviation from the trained model in case of a mechanical condition change and\nthus detects mechanical issues with higher confidence.\n",
        "published": "2018",
        "authors": [
            "Arash Golibagh Mahyari",
            "Thomas Locker"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.00568v1",
        "title": "Visual Foresight: Model-Based Deep Reinforcement Learning for\n  Vision-Based Robotic Control",
        "abstract": "  Deep reinforcement learning (RL) algorithms can learn complex robotic skills\nfrom raw sensory inputs, but have yet to achieve the kind of broad\ngeneralization and applicability demonstrated by deep learning methods in\nsupervised domains. We present a deep RL method that is practical for\nreal-world robotics tasks, such as robotic manipulation, and generalizes\neffectively to never-before-seen tasks and objects. In these settings, ground\ntruth reward signals are typically unavailable, and we therefore propose a\nself-supervised model-based approach, where a predictive model learns to\ndirectly predict the future from raw sensory readings, such as camera images.\nAt test time, we explore three distinct goal specification methods: designated\npixels, where a user specifies desired object manipulation tasks by selecting\nparticular pixels in an image and corresponding goal positions, goal images,\nwhere the desired goal state is specified with an image, and image classifiers,\nwhich define spaces of goal states. Our deep predictive models are trained\nusing data collected autonomously and continuously by a robot interacting with\nhundreds of objects, without human supervision. We demonstrate that visual MPC\ncan generalize to never-before-seen objects---both rigid and deformable---and\nsolve a range of user-defined object manipulation tasks using the same model.\n",
        "published": "2018",
        "authors": [
            "Frederik Ebert",
            "Chelsea Finn",
            "Sudeep Dasari",
            "Annie Xie",
            "Alex Lee",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.00971v2",
        "title": "Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using\n  Meta-Learning",
        "abstract": "  Learning is an inherently continuous phenomenon. When humans learn a new task\nthere is no explicit distinction between training and inference. As we learn a\ntask, we keep learning about it while performing the task. What we learn and\nhow we learn it varies during different stages of learning. Learning how to\nlearn and adapt is a key property that enables us to generalize effortlessly to\nnew settings. This is in contrast with conventional settings in machine\nlearning where a trained model is frozen during inference. In this paper we\nstudy the problem of learning to learn at both training and test time in the\ncontext of visual navigation. A fundamental challenge in navigation is\ngeneralization to unseen scenes. In this paper we propose a self-adaptive\nvisual navigation method (SAVN) which learns to adapt to new environments\nwithout any explicit supervision. Our solution is a meta-reinforcement learning\napproach where an agent learns a self-supervised interaction loss that\nencourages effective navigation. Our experiments, performed in the AI2-THOR\nframework, show major improvements in both success rate and SPL for visual\nnavigation in novel scenes. Our code and data are available at:\nhttps://github.com/allenai/savn .\n",
        "published": "2018",
        "authors": [
            "Mitchell Wortsman",
            "Kiana Ehsani",
            "Mohammad Rastegari",
            "Ali Farhadi",
            "Roozbeh Mottaghi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.00445v1",
        "title": "A Behavioral Approach to Visual Navigation with Graph Localization\n  Networks",
        "abstract": "  Inspired by research in psychology, we introduce a behavioral approach for\nvisual navigation using topological maps. Our goal is to enable a robot to\nnavigate from one location to another, relying only on its visual input and the\ntopological map of the environment. We propose using graph neural networks for\nlocalizing the agent in the map, and decompose the action space into primitive\nbehaviors implemented as convolutional or recurrent neural networks. Using the\nGibson simulator, we verify that our approach outperforms relevant baselines\nand is able to navigate in both seen and unseen environments.\n",
        "published": "2019",
        "authors": [
            "Kevin Chen",
            "Juan Pablo de Vicente",
            "Gabriel Sepulveda",
            "Fei Xia",
            "Alvaro Soto",
            "Marynel Vazquez",
            "Silvio Savarese"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.01534v1",
        "title": "Selective Sensor Fusion for Neural Visual-Inertial Odometry",
        "abstract": "  Deep learning approaches for Visual-Inertial Odometry (VIO) have proven\nsuccessful, but they rarely focus on incorporating robust fusion strategies for\ndealing with imperfect input sensory data. We propose a novel end-to-end\nselective sensor fusion framework for monocular VIO, which fuses monocular\nimages and inertial measurements in order to estimate the trajectory whilst\nimproving robustness to real-life issues, such as missing and corrupted data or\nbad sensor synchronization. In particular, we propose two fusion modalities\nbased on different masking strategies: deterministic soft fusion and stochastic\nhard fusion, and we compare with previously proposed direct fusion baselines.\nDuring testing, the network is able to selectively process the features of the\navailable sensor modalities and produce a trajectory at scale. We present a\nthorough investigation on the performances on three public autonomous driving,\nMicro Aerial Vehicle (MAV) and hand-held VIO datasets. The results demonstrate\nthe effectiveness of the fusion strategies, which offer better performances\ncompared to direct fusion, particularly in presence of corrupted data. In\naddition, we study the interpretability of the fusion networks by visualising\nthe masking layers in different scenarios and with varying data corruption,\nrevealing interesting correlations between the fusion networks and imperfect\nsensory input data.\n",
        "published": "2019",
        "authors": [
            "Changhao Chen",
            "Stefano Rosa",
            "Yishu Miao",
            "Chris Xiaoxuan Lu",
            "Wei Wu",
            "Andrew Markham",
            "Niki Trigoni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.04128v1",
        "title": "Manipulation by Feel: Touch-Based Control with Deep Predictive Models",
        "abstract": "  Touch sensing is widely acknowledged to be important for dexterous robotic\nmanipulation, but exploiting tactile sensing for continuous, non-prehensile\nmanipulation is challenging. General purpose control techniques that are able\nto effectively leverage tactile sensing as well as accurate physics models of\ncontacts and forces remain largely elusive, and it is unclear how to even\nspecify a desired behavior in terms of tactile percepts. In this paper, we take\na step towards addressing these issues by combining high-resolution tactile\nsensing with data-driven modeling using deep neural network dynamics models. We\npropose deep tactile MPC, a framework for learning to perform tactile servoing\nfrom raw tactile sensor inputs, without manual supervision. We show that this\nmethod enables a robot equipped with a GelSight-style tactile sensor to\nmanipulate a ball, analog stick, and 20-sided die, learning from unsupervised\nautonomous interaction and then using the learned tactile predictive model to\nreposition each object to user-specified configurations, indicated by a goal\ntactile reading. Videos, visualizations and the code are available here:\nhttps://sites.google.com/view/deeptactilempc\n",
        "published": "2019",
        "authors": [
            "Stephen Tian",
            "Frederik Ebert",
            "Dinesh Jayaraman",
            "Mayur Mudigonda",
            "Chelsea Finn",
            "Roberto Calandra",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.11174v1",
        "title": "Improved Generalization of Heading Direction Estimation for Aerial\n  Filming Using Semi-supervised Regression",
        "abstract": "  In the task of Autonomous aerial filming of a moving actor (e.g. a person or\na vehicle), it is crucial to have a good heading direction estimation for the\nactor from the visual input. However, the models obtained in other similar\ntasks, such as pedestrian collision risk analysis and human-robot interaction,\nare very difficult to generalize to the aerial filming task, because of the\ndifference in data distributions. Towards improving generalization with less\namount of labeled data, this paper presents a semi-supervised algorithm for\nheading direction estimation problem. We utilize temporal continuity as the\nunsupervised signal to regularize the model and achieve better generalization\nability. This semi-supervised algorithm is applied to both training and testing\nphases, which increases the testing performance by a large margin. We show that\nby leveraging unlabeled sequences, the amount of labeled data required can be\nsignificantly reduced. We also discuss several important details on improving\nthe performance by balancing labeled and unlabeled loss, and making good\ncombinations. Experimental results show that our approach robustly outputs the\nheading direction for different types of actor. The aesthetic value of the\nvideo is also improved in the aerial filming task.\n",
        "published": "2019",
        "authors": [
            "Wenshan Wang",
            "Aayush Ahuja",
            "Yanfu Zhang",
            "Rogerio Bonatti",
            "Sebastian Scherer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.00177v2",
        "title": "Urban Driving with Conditional Imitation Learning",
        "abstract": "  Hand-crafting generalised decision-making rules for real-world urban\nautonomous driving is hard. Alternatively, learning behaviour from\neasy-to-collect human driving demonstrations is appealing. Prior work has\nstudied imitation learning (IL) for autonomous driving with a number of\nlimitations. Examples include only performing lane-following rather than\nfollowing a user-defined route, only using a single camera view or heavily\ncropped frames lacking state observability, only lateral (steering) control,\nbut not longitudinal (speed) control and a lack of interaction with traffic.\nImportantly, the majority of such systems have been primarily evaluated in\nsimulation - a simple domain, which lacks real-world complexities. Motivated by\nthese challenges, we focus on learning representations of semantics, geometry\nand motion with computer vision for IL from human driving demonstrations. As\nour main contribution, we present an end-to-end conditional imitation learning\napproach, combining both lateral and longitudinal control on a real vehicle for\nfollowing urban routes with simple traffic. We address inherent dataset bias by\ndata balancing, training our final policy on approximately 30 hours of\ndemonstrations gathered over six months. We evaluate our method on an\nautonomous vehicle by driving 35km of novel routes in European urban streets.\n",
        "published": "2019",
        "authors": [
            "Jeffrey Hawke",
            "Richard Shen",
            "Corina Gurau",
            "Siddharth Sharma",
            "Daniele Reda",
            "Nikolay Nikolov",
            "Przemyslaw Mazur",
            "Sean Micklethwaite",
            "Nicolas Griffiths",
            "Amar Shah",
            "Alex Kendall"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.06321v2",
        "title": "Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World\n  Performance?",
        "abstract": "  Does progress in simulation translate to progress on robots? If one method\noutperforms another in simulation, how likely is that trend to hold in reality\non a robot? We examine this question for embodied PointGoal navigation,\ndeveloping engineering tools and a research paradigm for evaluating a simulator\nby its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy),\na library for seamless execution of identical code on simulated agents and\nrobots, transferring simulation-trained agents to a LoCoBot platform with a\none-line code change. Second, we investigate the sim2real predictivity of\nHabitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create\na virtualized replica, and run parallel tests of 9 different models in reality\nand simulation. We present a new metric called Sim-vs-Real Correlation\nCoefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as\nused for the CVPR19 challenge is low (0.18 for the success metric), suggesting\nthat performance differences in this simulator-based challenge do not persist\nafter physical deployment. This gap is largely due to AI agents learning to\nexploit simulator imperfections, abusing collision dynamics to 'slide' along\nwalls, leading to shortcuts through otherwise non-navigable space. Naturally,\nsuch exploits do not work in the real world. Our experiments show that it is\npossible to tune simulation parameters to improve sim2real predictivity (e.g.\nimproving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that\nin-simulation comparisons will translate to deployed systems in reality.\n",
        "published": "2019",
        "authors": [
            "Abhishek Kadian",
            "Joanne Truong",
            "Aaron Gokaslan",
            "Alexander Clegg",
            "Erik Wijmans",
            "Stefan Lee",
            "Manolis Savva",
            "Sonia Chernova",
            "Dhruv Batra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.12294v1",
        "title": "Learning by Cheating",
        "abstract": "  Vision-based urban driving is hard. The autonomous system needs to learn to\nperceive the world and act in it. We show that this challenging learning\nproblem can be simplified by decomposing it into two stages. We first train an\nagent that has access to privileged information. This privileged agent cheats\nby observing the ground-truth layout of the environment and the positions of\nall traffic participants. In the second stage, the privileged agent acts as a\nteacher that trains a purely vision-based sensorimotor agent. The resulting\nsensorimotor agent does not have access to any privileged information and does\nnot cheat. This two-stage training procedure is counter-intuitive at first, but\nhas a number of important advantages that we analyze and empirically\ndemonstrate. We use the presented approach to train a vision-based autonomous\ndriving system that substantially outperforms the state of the art on the CARLA\nbenchmark and the recent NoCrash benchmark. Our approach achieves, for the\nfirst time, 100% success rate on all tasks in the original CARLA benchmark,\nsets a new record on the NoCrash benchmark, and reduces the frequency of\ninfractions by an order of magnitude compared to the prior state of the art.\nFor the video that summarizes this work, see https://youtu.be/u9ZCxxD-UUw\n",
        "published": "2019",
        "authors": [
            "Dian Chen",
            "Brady Zhou",
            "Vladlen Koltun",
            "Philipp Kr\u00e4henb\u00fchl"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.04835v2",
        "title": "Knowledge Representations in Technical Systems -- A Taxonomy",
        "abstract": "  The recent usage of technical systems in human-centric environments leads to\nthe question, how to teach technical systems, e.g., robots, to understand,\nlearn, and perform tasks desired by the human. Therefore, an accurate\nrepresentation of knowledge is essential for the system to work as expected.\nThis article mainly gives insight into different knowledge representation\ntechniques and their categorization into various problem domains in artificial\nintelligence. Additionally, applications of presented knowledge representations\nare introduced in everyday robotics tasks. By means of the provided taxonomy,\nthe search for a proper knowledge representation technique regarding a specific\nproblem should be facilitated.\n",
        "published": "2020",
        "authors": [
            "Kristina Scharei",
            "Florian Heidecker",
            "Maarten Bieshaar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.00504v1",
        "title": "MonoPair: Monocular 3D Object Detection Using Pairwise Spatial\n  Relationships",
        "abstract": "  Monocular 3D object detection is an essential component in autonomous driving\nwhile challenging to solve, especially for those occluded samples which are\nonly partially visible. Most detectors consider each 3D object as an\nindependent training target, inevitably resulting in a lack of useful\ninformation for occluded samples. To this end, we propose a novel method to\nimprove the monocular 3D object detection by considering the relationship of\npaired samples. This allows us to encode spatial constraints for\npartially-occluded objects from their adjacent neighbors. Specifically, the\nproposed detector computes uncertainty-aware predictions for object locations\nand 3D distances for the adjacent object pairs, which are subsequently jointly\noptimized by nonlinear least squares. Finally, the one-stage uncertainty-aware\nprediction structure and the post-optimization module are dedicatedly\nintegrated for ensuring the run-time efficiency. Experiments demonstrate that\nour method yields the best performance on KITTI 3D detection benchmark, by\noutperforming state-of-the-art competitors by wide margins, especially for the\nhard samples.\n",
        "published": "2020",
        "authors": [
            "Yongjian Chen",
            "Lei Tai",
            "Kai Sun",
            "Mingyang Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.06082v4",
        "title": "An Adversarial Objective for Scalable Exploration",
        "abstract": "  Model-based curiosity combines active learning approaches to optimal sampling\nwith the information gain based incentives for exploration presented in the\ncuriosity literature. Existing model-based curiosity methods look to\napproximate prediction uncertainty with approaches which struggle to scale to\nmany prediction-planning pipelines used in robotics tasks. We address these\nscalability issues with an adversarial curiosity method minimizing a score\ngiven by a discriminator network. This discriminator is optimized jointly with\na prediction model and enables our active learning approach to sample sequences\nof observations and actions which result in predictions considered the least\nrealistic by the discriminator. We demonstrate progressively increasing\nadvantages as compute is restricted of our adversarial curiosity approach over\nleading model-based exploration strategies in simulated environments. We\nfurther demonstrate the ability of our adversarial curiosity method to scale to\na robotic manipulation prediction-planning pipeline where we improve sample\nefficiency and prediction performance for a domain transfer problem.\n",
        "published": "2020",
        "authors": [
            "Bernadette Bucher",
            "Karl Schmeckpeper",
            "Nikolai Matni",
            "Kostas Daniilidis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.11973v1",
        "title": "GISNet: Graph-Based Information Sharing Network For Vehicle Trajectory\n  Prediction",
        "abstract": "  The trajectory prediction is a critical and challenging problem in the design\nof an autonomous driving system. Many AI-oriented companies, such as Google\nWaymo, Uber and DiDi, are investigating more accurate vehicle trajectory\nprediction algorithms. However, the prediction performance is governed by lots\nof entangled factors, such as the stochastic behaviors of surrounding vehicles,\nhistorical information of self-trajectory, and relative positions of neighbors,\netc. In this paper, we propose a novel graph-based information sharing network\n(GISNet) that allows the information sharing between the target vehicle and its\nsurrounding vehicles. Meanwhile, the model encodes the historical trajectory\ninformation of all the vehicles in the scene. Experiments are carried out on\nthe public NGSIM US-101 and I-80 Dataset and the prediction performance is\nmeasured by the Root Mean Square Error (RMSE). The quantitative and qualitative\nexperimental results show that our model significantly improves the trajectory\nprediction accuracy, by up to 50.00%, compared to existing models.\n",
        "published": "2020",
        "authors": [
            "Ziyi Zhao",
            "Haowen Fang",
            "Zhao Jin",
            "Qinru Qiu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.10034v2",
        "title": "Semantic Visual Navigation by Watching YouTube Videos",
        "abstract": "  Semantic cues and statistical regularities in real-world environment layouts\ncan improve efficiency for navigation in novel environments. This paper learns\nand leverages such semantic cues for navigating to objects of interest in novel\nenvironments, by simply watching YouTube videos. This is challenging because\nYouTube videos don't come with labels for actions or goals, and may not even\nshowcase optimal behavior. Our method tackles these challenges through the use\nof Q-learning on pseudo-labeled transition quadruples (image, action, next\nimage, reward). We show that such off-policy Q-learning from passive data is\nable to learn meaningful semantic cues for navigation. These cues, when used in\na hierarchical navigation policy, lead to improved efficiency at the ObjectGoal\ntask in visually realistic simulations. We observe a relative improvement of\n15-83% over end-to-end RL, behavior cloning, and classical methods, while using\nminimal direct interaction.\n",
        "published": "2020",
        "authors": [
            "Matthew Chang",
            "Arjun Gupta",
            "Saurabh Gupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.07792v2",
        "title": "ReLMoGen: Leveraging Motion Generation in Reinforcement Learning for\n  Mobile Manipulation",
        "abstract": "  Many Reinforcement Learning (RL) approaches use joint control signals\n(positions, velocities, torques) as action space for continuous control tasks.\nWe propose to lift the action space to a higher level in the form of subgoals\nfor a motion generator (a combination of motion planner and trajectory\nexecutor). We argue that, by lifting the action space and by leveraging\nsampling-based motion planners, we can efficiently use RL to solve complex,\nlong-horizon tasks that could not be solved with existing RL methods in the\noriginal action space. We propose ReLMoGen -- a framework that combines a\nlearned policy to predict subgoals and a motion generator to plan and execute\nthe motion needed to reach these subgoals. To validate our method, we apply\nReLMoGen to two types of tasks: 1) Interactive Navigation tasks, navigation\nproblems where interactions with the environment are required to reach the\ndestination, and 2) Mobile Manipulation tasks, manipulation tasks that require\nmoving the robot base. These problems are challenging because they are usually\nlong-horizon, hard to explore during training, and comprise alternating phases\nof navigation and interaction. Our method is benchmarked on a diverse set of\nseven robotics tasks in photo-realistic simulation environments. In all\nsettings, ReLMoGen outperforms state-of-the-art Reinforcement Learning and\nHierarchical Reinforcement Learning baselines. ReLMoGen also shows outstanding\ntransferability between different motion generators at test time, indicating a\ngreat potential to transfer to real robots.\n",
        "published": "2020",
        "authors": [
            "Fei Xia",
            "Chengshu Li",
            "Roberto Mart\u00edn-Mart\u00edn",
            "Or Litany",
            "Alexander Toshev",
            "Silvio Savarese"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.04689v1",
        "title": "LaND: Learning to Navigate from Disengagements",
        "abstract": "  Consistently testing autonomous mobile robots in real world scenarios is a\nnecessary aspect of developing autonomous navigation systems. Each time the\nhuman safety monitor disengages the robot's autonomy system due to the robot\nperforming an undesirable maneuver, the autonomy developers gain insight into\nhow to improve the autonomy system. However, we believe that these\ndisengagements not only show where the system fails, which is useful for\ntroubleshooting, but also provide a direct learning signal by which the robot\ncan learn to navigate. We present a reinforcement learning approach for\nlearning to navigate from disengagements, or LaND. LaND learns a neural network\nmodel that predicts which actions lead to disengagements given the current\nsensory observation, and then at test time plans and executes actions that\navoid disengagements. Our results demonstrate LaND can successfully learn to\nnavigate in diverse, real world sidewalk environments, outperforming both\nimitation learning and reinforcement learning approaches. Videos, code, and\nother material are available on our website\nhttps://sites.google.com/view/sidewalk-learning\n",
        "published": "2020",
        "authors": [
            "Gregory Kahn",
            "Pieter Abbeel",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.08776v1",
        "title": "The NVIDIA PilotNet Experiments",
        "abstract": "  Four years ago, an experimental system known as PilotNet became the first\nNVIDIA system to steer an autonomous car along a roadway. This system\nrepresents a departure from the classical approach for self-driving in which\nthe process is manually decomposed into a series of modules, each performing a\ndifferent task. In PilotNet, on the other hand, a single deep neural network\n(DNN) takes pixels as input and produces a desired vehicle trajectory as\noutput; there are no distinct internal modules connected by human-designed\ninterfaces. We believe that handcrafted interfaces ultimately limit performance\nby restricting information flow through the system and that a learned approach,\nin combination with other artificial intelligence systems that add redundancy,\nwill lead to better overall performing systems. We continue to conduct research\ntoward that goal.\n  This document describes the PilotNet lane-keeping effort, carried out over\nthe past five years by our NVIDIA PilotNet group in Holmdel, New Jersey. Here\nwe present a snapshot of system status in mid-2020 and highlight some of the\nwork done by the PilotNet group.\n",
        "published": "2020",
        "authors": [
            "Mariusz Bojarski",
            "Chenyi Chen",
            "Joyjit Daw",
            "Alperen De\u011firmenci",
            "Joya Deri",
            "Bernhard Firner",
            "Beat Flepp",
            "Sachin Gogri",
            "Jesse Hong",
            "Lawrence Jackel",
            "Zhenhua Jia",
            "BJ Lee",
            "Bo Liu",
            "Fei Liu",
            "Urs Muller",
            "Samuel Payne",
            "Nischal Kota Nagendra Prasad",
            "Artem Provodin",
            "John Roach",
            "Timur Rvachov",
            "Neha Tadimeti",
            "Jesper van Engelen",
            "Haiguang Wen",
            "Eric Yang",
            "Zongyi Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.09164v3",
        "title": "Evidential Sparsification of Multimodal Latent Spaces in Conditional\n  Variational Autoencoders",
        "abstract": "  Discrete latent spaces in variational autoencoders have been shown to\neffectively capture the data distribution for many real-world problems such as\nnatural language understanding, human intent prediction, and visual scene\nrepresentation. However, discrete latent spaces need to be sufficiently large\nto capture the complexities of real-world data, rendering downstream tasks\ncomputationally challenging. For instance, performing motion planning in a\nhigh-dimensional latent representation of the environment could be intractable.\nWe consider the problem of sparsifying the discrete latent space of a trained\nconditional variational autoencoder, while preserving its learned\nmultimodality. As a post hoc latent space reduction technique, we use\nevidential theory to identify the latent classes that receive direct evidence\nfrom a particular input condition and filter out those that do not. Experiments\non diverse tasks, such as image generation and human behavior prediction,\ndemonstrate the effectiveness of our proposed technique at reducing the\ndiscrete latent sample space size of a model while maintaining its learned\nmultimodality.\n",
        "published": "2020",
        "authors": [
            "Masha Itkina",
            "Boris Ivanovic",
            "Ransalu Senanayake",
            "Mykel J. Kochenderfer",
            "Marco Pavone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.09273v1",
        "title": "DeepReflecs: Deep Learning for Automotive Object Classification with\n  Radar Reflections",
        "abstract": "  This paper presents an novel object type classification method for automotive\napplications which uses deep learning with radar reflections. The method\nprovides object class information such as pedestrian, cyclist, car, or\nnon-obstacle. The method is both powerful and efficient, by using a\nlight-weight deep learning approach on reflection level radar data. It fills\nthe gap between low-performant methods of handcrafted features and\nhigh-performant methods with convolutional neural networks. The proposed\nnetwork exploits the specific characteristics of radar reflection data: It\nhandles unordered lists of arbitrary length as input and it combines both\nextraction of local and global features. In experiments with real data the\nproposed network outperforms existing methods of handcrafted or learned\nfeatures. An ablation study analyzes the impact of the proposed global context\nlayer.\n",
        "published": "2020",
        "authors": [
            "Michael Ulrich",
            "Claudius Gl\u00e4ser",
            "Fabian Timm"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.09662v3",
        "title": "Attention Augmented ConvLSTM for Environment Prediction",
        "abstract": "  Safe and proactive planning in robotic systems generally requires accurate\npredictions of the environment. Prior work on environment prediction applied\nvideo frame prediction techniques to bird's-eye view environment\nrepresentations, such as occupancy grids. ConvLSTM-based frameworks used\npreviously often result in significant blurring and vanishing of moving\nobjects, thus hindering their applicability for use in safety-critical\napplications. In this work, we propose two extensions to the ConvLSTM to\naddress these issues. We present the Temporal Attention Augmented ConvLSTM\n(TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks\nfor spatiotemporal occupancy prediction, and demonstrate improved performance\nover baseline architectures on the real-world KITTI and Waymo datasets.\n",
        "published": "2020",
        "authors": [
            "Bernard Lange",
            "Masha Itkina",
            "Mykel J. Kochenderfer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.10903v1",
        "title": "Visual Navigation in Real-World Indoor Environments Using End-to-End\n  Deep Reinforcement Learning",
        "abstract": "  Visual navigation is essential for many applications in robotics, from\nmanipulation, through mobile robotics to automated driving. Deep reinforcement\nlearning (DRL) provides an elegant map-free approach integrating image\nprocessing, localization, and planning in one module, which can be trained and\ntherefore optimized for a given environment. However, to date, DRL-based visual\nnavigation was validated exclusively in simulation, where the simulator\nprovides information that is not available in the real world, e.g., the robot's\nposition or image segmentation masks. This precludes the use of the learned\npolicy on a real robot. Therefore, we propose a novel approach that enables a\ndirect deployment of the trained policy on real robots. We have designed visual\nauxiliary tasks, a tailored reward scheme, and a new powerful simulator to\nfacilitate domain randomization. The policy is fine-tuned on images collected\nfrom real-world environments. We have evaluated the method on a mobile robot in\na real office environment. The training took ~30 hours on a single GPU. In 30\nnavigation experiments, the robot reached a 0.3-meter neighborhood of the goal\nin more than 86.7% of cases. This result makes the proposed method directly\napplicable to tasks like mobile manipulation.\n",
        "published": "2020",
        "authors": [
            "Jon\u00e1\u0161 Kulh\u00e1nek",
            "Erik Derner",
            "Robert Babu\u0161ka"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.13957v2",
        "title": "MELD: Meta-Reinforcement Learning from Images via Latent State Models",
        "abstract": "  Meta-reinforcement learning algorithms can enable autonomous agents, such as\nrobots, to quickly acquire new behaviors by leveraging prior experience in a\nset of related training tasks. However, the onerous data requirements of\nmeta-training compounded with the challenge of learning from sensory inputs\nsuch as images have made meta-RL challenging to apply to real robotic systems.\nLatent state models, which learn compact state representations from a sequence\nof observations, can accelerate representation learning from visual inputs. In\nthis paper, we leverage the perspective of meta-learning as task inference to\nshow that latent state models can \\emph{also} perform meta-learning given an\nappropriately defined observation space. Building on this insight, we develop\nmeta-RL with latent dynamics (MELD), an algorithm for meta-RL from images that\nperforms inference in a latent state model to quickly acquire new skills given\nobservations and rewards. MELD outperforms prior meta-RL methods on several\nsimulated image-based robotic control problems, and enables a real WidowX\nrobotic arm to insert an Ethernet cable into new locations given a sparse task\ncompletion signal after only $8$ hours of real world meta-training. To our\nknowledge, MELD is the first meta-RL algorithm trained in a real-world robotic\ncontrol setting from images.\n",
        "published": "2020",
        "authors": [
            "Tony Z. Zhao",
            "Anusha Nagabandi",
            "Kate Rakelly",
            "Chelsea Finn",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.14296v1",
        "title": "Fit to Measure: Reasoning about Sizes for Robust Object Recognition",
        "abstract": "  Service robots can help with many of our daily tasks, especially in those\ncases where it is inconvenient or unsafe for us to intervene: e.g., under\nextreme weather conditions or when social distance needs to be maintained.\nHowever, before we can successfully delegate complex tasks to robots, we need\nto enhance their ability to make sense of dynamic, real world environments. In\nthis context, the first prerequisite to improving the Visual Intelligence of a\nrobot is building robust and reliable object recognition systems. While object\nrecognition solutions are traditionally based on Machine Learning methods,\naugmenting them with knowledge based reasoners has been shown to improve their\nperformance. In particular, based on our prior work on identifying the\nepistemic requirements of Visual Intelligence, we hypothesise that knowledge of\nthe typical size of objects could significantly improve the accuracy of an\nobject recognition system. To verify this hypothesis, in this paper we present\nan approach to integrating knowledge about object sizes in a ML based\narchitecture. Our experiments in a real world robotic scenario show that this\ncombined approach ensures a significant performance increase over state of the\nart Machine Learning methods.\n",
        "published": "2020",
        "authors": [
            "Agnese Chiatti",
            "Enrico Motta",
            "Enrico Daga",
            "Gianluca Bardaro"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.15413v3",
        "title": "Measuring and Harnessing Transference in Multi-Task Learning",
        "abstract": "  Multi-task learning can leverage information learned by one task to benefit\nthe training of other tasks. Despite this capacity, naive formulations often\ndegrade performance and in particular, identifying the tasks that would benefit\nfrom co-training remains a challenging design question. In this paper, we\nanalyze the dynamics of information transfer, or transference, across tasks\nthroughout training. Specifically, we develop a similarity measure that can\nquantify transference among tasks and use this quantity to both better\nunderstand the optimization dynamics of multi-task learning as well as improve\noverall learning performance. In the latter case, we propose two methods to\nleverage our transference metric. The first operates at a macro-level by\nselecting which tasks should train together while the second functions at a\nmicro-level by determining how to combine task gradients at each training step.\nWe find these methods can lead to significant improvement over prior work on\nthree supervised multi-task learning benchmarks and one multi-task\nreinforcement learning paradigm.\n",
        "published": "2020",
        "authors": [
            "Christopher Fifty",
            "Ehsan Amid",
            "Zhe Zhao",
            "Tianhe Yu",
            "Rohan Anil",
            "Chelsea Finn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.16298v1",
        "title": "Learning Vision-based Reactive Policies for Obstacle Avoidance",
        "abstract": "  In this paper, we address the problem of vision-based obstacle avoidance for\nrobotic manipulators. This topic poses challenges for both perception and\nmotion generation. While most work in the field aims at improving one of those\naspects, we provide a unified framework for approaching this problem. The main\ngoal of this framework is to connect perception and motion by identifying the\nrelationship between the visual input and the corresponding motion\nrepresentation. To this end, we propose a method for learning reactive obstacle\navoidance policies. We evaluate our method on goal-reaching tasks for single\nand multiple obstacles scenarios. We show the ability of the proposed method to\nefficiently learn stable obstacle avoidance strategies at a high success rate,\nwhile maintaining closed-loop responsiveness required for critical applications\nlike human-robot interaction.\n",
        "published": "2020",
        "authors": [
            "Elie Aljalbout",
            "Ji Chen",
            "Konstantin Ritt",
            "Maximilian Ulmer",
            "Sami Haddadin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.00316v1",
        "title": "Energy-constrained Self-training for Unsupervised Domain Adaptation",
        "abstract": "  Unsupervised domain adaptation (UDA) aims to transfer the knowledge on a\nlabeled source domain distribution to perform well on an unlabeled target\ndomain. Recently, the deep self-training involves an iterative process of\npredicting on the target domain and then taking the confident predictions as\nhard pseudo-labels for retraining. However, the pseudo-labels are usually\nunreliable, and easily leading to deviated solutions with propagated errors. In\nthis paper, we resort to the energy-based model and constrain the training of\nthe unlabeled target sample with the energy function minimization objective. It\ncan be applied as a simple additional regularization. In this framework, it is\npossible to gain the benefits of the energy-based model, while retaining strong\ndiscriminative performance following a plug-and-play fashion. We deliver\nextensive experiments on the most popular and large scale UDA benchmarks of\nimage classification as well as semantic segmentation to demonstrate its\ngenerality and effectiveness.\n",
        "published": "2021",
        "authors": [
            "Xiaofeng Liu",
            "Bo Hu",
            "Xiongchang Liu",
            "Jun Lu",
            "Jane You",
            "Lingsheng Kong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.02722v1",
        "title": "The Distracting Control Suite -- A Challenging Benchmark for\n  Reinforcement Learning from Pixels",
        "abstract": "  Robots have to face challenging perceptual settings, including changes in\nviewpoint, lighting, and background. Current simulated reinforcement learning\n(RL) benchmarks such as DM Control provide visual input without such\ncomplexity, which limits the transfer of well-performing methods to the real\nworld. In this paper, we extend DM Control with three kinds of visual\ndistractions (variations in background, color, and camera pose) to produce a\nnew challenging benchmark for vision-based control, and we analyze state of the\nart RL algorithms in these settings. Our experiments show that current RL\nmethods for vision-based control perform poorly under distractions, and that\ntheir performance decreases with increasing distraction complexity, showing\nthat new methods are needed to cope with the visual complexities of the real\nworld. We also find that combinations of multiple distraction types are more\ndifficult than a mere combination of their individual effects.\n",
        "published": "2021",
        "authors": [
            "Austin Stone",
            "Oscar Ramirez",
            "Kurt Konolige",
            "Rico Jonschkowski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.04882v1",
        "title": "Asymmetric self-play for automatic goal discovery in robotic\n  manipulation",
        "abstract": "  We train a single, goal-conditioned policy that can solve many robotic\nmanipulation tasks, including tasks with previously unseen goals and objects.\nWe rely on asymmetric self-play for goal discovery, where two agents, Alice and\nBob, play a game. Alice is asked to propose challenging goals and Bob aims to\nsolve them. We show that this method can discover highly diverse and complex\ngoals without any human priors. Bob can be trained with only sparse rewards,\nbecause the interaction between Alice and Bob results in a natural curriculum\nand Bob can learn from Alice's trajectory when relabeled as a goal-conditioned\ndemonstration. Finally, our method scales, resulting in a single policy that\ncan generalize to many unseen tasks such as setting a table, stacking blocks,\nand solving simple puzzles. Videos of a learned policy is available at\nhttps://robotics-self-play.github.io.\n",
        "published": "2021",
        "authors": [
            "OpenAI OpenAI",
            "Matthias Plappert",
            "Raul Sampedro",
            "Tao Xu",
            "Ilge Akkaya",
            "Vineet Kosaraju",
            "Peter Welinder",
            "Ruben D'Sa",
            "Arthur Petron",
            "Henrique P. d. O. Pinto",
            "Alex Paino",
            "Hyeonwoo Noh",
            "Lilian Weng",
            "Qiming Yuan",
            "Casey Chu",
            "Wojciech Zaremba"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.05307v2",
        "title": "Explainability of deep vision-based autonomous driving systems: Review\n  and challenges",
        "abstract": "  This survey reviews explainability methods for vision-based self-driving\nsystems trained with behavior cloning. The concept of explainability has\nseveral facets and the need for explainability is strong in driving, a\nsafety-critical application. Gathering contributions from several research\nfields, namely computer vision, deep learning, autonomous driving, explainable\nAI (X-AI), this survey tackles several points. First, it discusses definitions,\ncontext, and motivation for gaining more interpretability and explainability\nfrom self-driving systems, as well as the challenges that are specific to this\napplication. Second, methods providing explanations to a black-box self-driving\nsystem in a post-hoc fashion are comprehensively organized and detailed. Third,\napproaches from the literature that aim at building more interpretable\nself-driving systems by design are presented and discussed in detail. Finally,\nremaining open-challenges and potential future research directions are\nidentified and examined.\n",
        "published": "2021",
        "authors": [
            "\u00c9loi Zablocki",
            "H\u00e9di Ben-Younes",
            "Patrick P\u00e9rez",
            "Matthieu Cord"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.06541v1",
        "title": "SceneGen: Learning to Generate Realistic Traffic Scenes",
        "abstract": "  We consider the problem of generating realistic traffic scenes automatically.\nExisting methods typically insert actors into the scene according to a set of\nhand-crafted heuristics and are limited in their ability to model the true\ncomplexity and diversity of real traffic scenes, thus inducing a content gap\nbetween synthesized traffic scenes versus real ones. As a result, existing\nsimulators lack the fidelity necessary to train and test self-driving vehicles.\nTo address this limitation, we present SceneGen, a neural autoregressive model\nof traffic scenes that eschews the need for rules and heuristics. In\nparticular, given the ego-vehicle state and a high definition map of\nsurrounding area, SceneGen inserts actors of various classes into the scene and\nsynthesizes their sizes, orientations, and velocities. We demonstrate on two\nlarge-scale datasets SceneGen's ability to faithfully model distributions of\nreal traffic scenes. Moreover, we show that SceneGen coupled with sensor\nsimulation can be used to train perception models that generalize to the real\nworld.\n",
        "published": "2021",
        "authors": [
            "Shuhan Tan",
            "Kelvin Wong",
            "Shenlong Wang",
            "Sivabalan Manivasagam",
            "Mengye Ren",
            "Raquel Urtasun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.06547v3",
        "title": "LookOut: Diverse Multi-Future Prediction and Planning for Self-Driving",
        "abstract": "  In this paper, we present LookOut, a novel autonomy system that perceives the\nenvironment, predicts a diverse set of futures of how the scene might unroll\nand estimates the trajectory of the SDV by optimizing a set of contingency\nplans over these future realizations. In particular, we learn a diverse joint\ndistribution over multi-agent future trajectories in a traffic scene that\ncovers a wide range of future modes with high sample efficiency while\nleveraging the expressive power of generative models. Unlike previous work in\ndiverse motion forecasting, our diversity objective explicitly rewards sampling\nfuture scenarios that require distinct reactions from the self-driving vehicle\nfor improved safety. Our contingency planner then finds comfortable and\nnon-conservative trajectories that ensure safe reactions to a wide range of\nfuture scenarios. Through extensive evaluations, we show that our model\ndemonstrates significantly more diverse and sample-efficient motion forecasting\nin a large-scale self-driving dataset as well as safer and less-conservative\nmotion plans in long-term closed-loop simulations when compared to current\nstate-of-the-art models.\n",
        "published": "2021",
        "authors": [
            "Alexander Cui",
            "Sergio Casas",
            "Abbas Sadat",
            "Renjie Liao",
            "Raquel Urtasun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.06549v4",
        "title": "AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles",
        "abstract": "  As self-driving systems become better, simulating scenarios where the\nautonomy stack may fail becomes more important. Traditionally, those scenarios\nare generated for a few scenes with respect to the planning module that takes\nground-truth actor states as input. This does not scale and cannot identify all\npossible autonomy failures, such as perception failures due to occlusion. In\nthis paper, we propose AdvSim, an adversarial framework to generate\nsafety-critical scenarios for any LiDAR-based autonomy system. Given an initial\ntraffic scenario, AdvSim modifies the actors' trajectories in a physically\nplausible manner and updates the LiDAR sensor data to match the perturbed\nworld. Importantly, by simulating directly from sensor data, we obtain\nadversarial scenarios that are safety-critical for the full autonomy stack. Our\nexperiments show that our approach is general and can identify thousands of\nsemantically meaningful safety-critical scenarios for a wide range of modern\nself-driving systems. Furthermore, we show that the robustness and safety of\nthese systems can be further improved by training them with scenarios generated\nby AdvSim.\n",
        "published": "2021",
        "authors": [
            "Jingkang Wang",
            "Ava Pun",
            "James Tu",
            "Sivabalan Manivasagam",
            "Abbas Sadat",
            "Sergio Casas",
            "Mengye Ren",
            "Raquel Urtasun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.06557v1",
        "title": "TrafficSim: Learning to Simulate Realistic Multi-Agent Behaviors",
        "abstract": "  Simulation has the potential to massively scale evaluation of self-driving\nsystems enabling rapid development as well as safe deployment. To close the gap\nbetween simulation and the real world, we need to simulate realistic\nmulti-agent behaviors. Existing simulation environments rely on heuristic-based\nmodels that directly encode traffic rules, which cannot capture irregular\nmaneuvers (e.g., nudging, U-turns) and complex interactions (e.g., yielding,\nmerging). In contrast, we leverage real-world data to learn directly from human\ndemonstration and thus capture a more diverse set of actor behaviors. To this\nend, we propose TrafficSim, a multi-agent behavior model for realistic traffic\nsimulation. In particular, we leverage an implicit latent variable model to\nparameterize a joint actor policy that generates socially-consistent plans for\nall actors in the scene jointly. To learn a robust policy amenable for long\nhorizon simulation, we unroll the policy in training and optimize through the\nfully differentiable simulation across time. Our learning objective\nincorporates both human demonstrations as well as common sense. We show\nTrafficSim generates significantly more realistic and diverse traffic scenarios\nas compared to a diverse set of baselines. Notably, we can exploit trajectories\ngenerated by TrafficSim as effective data augmentation for training better\nmotion planner.\n",
        "published": "2021",
        "authors": [
            "Simon Suo",
            "Sebastian Regalado",
            "Sergio Casas",
            "Raquel Urtasun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.06806v1",
        "title": "MP3: A Unified Model to Map, Perceive, Predict and Plan",
        "abstract": "  High-definition maps (HD maps) are a key component of most modern\nself-driving systems due to their valuable semantic and geometric information.\nUnfortunately, building HD maps has proven hard to scale due to their cost as\nwell as the requirements they impose in the localization system that has to\nwork everywhere with centimeter-level accuracy. Being able to drive without an\nHD map would be very beneficial to scale self-driving solutions as well as to\nincrease the failure tolerance of existing ones (e.g., if localization fails or\nthe map is not up-to-date). Towards this goal, we propose MP3, an end-to-end\napproach to mapless driving where the input is raw sensor data and a high-level\ncommand (e.g., turn left at the intersection). MP3 predicts intermediate\nrepresentations in the form of an online map and the current and future state\nof dynamic agents, and exploits them in a novel neural motion planner to make\ninterpretable decisions taking into account uncertainty. We show that our\napproach is significantly safer, more comfortable, and can follow commands\nbetter than the baselines in challenging long-term closed-loop simulations, as\nwell as when compared to an expert driver in a large-scale real-world dataset.\n",
        "published": "2021",
        "authors": [
            "Sergio Casas",
            "Abbas Sadat",
            "Raquel Urtasun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.06832v2",
        "title": "Deep Structured Reactive Planning",
        "abstract": "  An intelligent agent operating in the real-world must balance achieving its\ngoal with maintaining the safety and comfort of not only itself, but also other\nparticipants within the surrounding scene. This requires jointly reasoning\nabout the behavior of other actors while deciding its own actions as these two\nprocesses are inherently intertwined - a vehicle will yield to us if we decide\nto proceed first at the intersection but will proceed first if we decide to\nyield. However, this is not captured in most self-driving pipelines, where\nplanning follows prediction. In this paper we propose a novel data-driven,\nreactive planning objective which allows a self-driving vehicle to jointly\nreason about its own plans as well as how other actors will react to them. We\nformulate the problem as an energy-based deep structured model that is learned\nfrom observational data and encodes both the planning and prediction problems.\nThrough simulations based on both real-world driving and synthetically\ngenerated dense traffic, we demonstrate that our reactive model outperforms a\nnon-reactive variant in successfully completing highly complex maneuvers (lane\nmerges/turns in traffic) faster, without trading off collision rate.\n",
        "published": "2021",
        "authors": [
            "Jerry Liu",
            "Wenyuan Zeng",
            "Raquel Urtasun",
            "Ersin Yumer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.07907v1",
        "title": "IntentNet: Learning to Predict Intention from Raw Sensor Data",
        "abstract": "  In order to plan a safe maneuver, self-driving vehicles need to understand\nthe intent of other traffic participants. We define intent as a combination of\ndiscrete high-level behaviors as well as continuous trajectories describing\nfuture motion. In this paper, we develop a one-stage detector and forecaster\nthat exploits both 3D point clouds produced by a LiDAR sensor as well as\ndynamic maps of the environment. Our multi-task model achieves better accuracy\nthan the respective separate modules while saving computation, which is\ncritical to reducing reaction time in self-driving applications.\n",
        "published": "2021",
        "authors": [
            "Sergio Casas",
            "Wenjie Luo",
            "Raquel Urtasun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.10892v2",
        "title": "Online Body Schema Adaptation through Cost-Sensitive Active Learning",
        "abstract": "  Humanoid robots have complex bodies and kinematic chains with several\nDegrees-of-Freedom (DoF) which are difficult to model. Learning the parameters\nof a kinematic model can be achieved by observing the position of the robot\nlinks during prospective motions and minimising the prediction errors. This\nwork proposes a movement efficient approach for estimating online the\nbody-schema of a humanoid robot arm in the form of Denavit-Hartenberg (DH)\nparameters. A cost-sensitive active learning approach based on the A-Optimality\ncriterion is used to select optimal joint configurations. The chosen joint\nconfigurations simultaneously minimise the error in the estimation of the body\nschema and minimise the movement between samples. This reduces energy\nconsumption, along with mechanical fatigue and wear, while not compromising the\nlearning accuracy. The work was implemented in a simulation environment, using\nthe 7DoF arm of the iCub robot simulator. The hand pose is measured with a\nsingle camera via markers placed in the palm and back of the robot's hand. A\nnon-parametric occlusion model is proposed to avoid choosing joint\nconfigurations where the markers are not visible, thus preventing worthless\nattempts. The results show cost-sensitive active learning has similar accuracy\nto the standard active learning approach, while reducing in about half the\nexecuted movement.\n",
        "published": "2021",
        "authors": [
            "Gon\u00e7alo Cunha",
            "Pedro Vicente",
            "Alexandre Bernardino",
            "Ricardo Ribeiro",
            "Pl\u00ednio Moreno"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.11812v1",
        "title": "SwingBot: Learning Physical Features from In-hand Tactile Exploration\n  for Dynamic Swing-up Manipulation",
        "abstract": "  Several robot manipulation tasks are extremely sensitive to variations of the\nphysical properties of the manipulated objects. One such task is manipulating\nobjects by using gravity or arm accelerations, increasing the importance of\nmass, center of mass, and friction information. We present SwingBot, a robot\nthat is able to learn the physical features of a held object through tactile\nexploration. Two exploration actions (tilting and shaking) provide the tactile\ninformation used to create a physical feature embedding space. With this\nembedding, SwingBot is able to predict the swing angle achieved by a robot\nperforming dynamic swing-up manipulations on a previously unseen object. Using\nthese predictions, it is able to search for the optimal control parameters for\na desired swing-up angle. We show that with the learned physical features our\nend-to-end self-supervised learning pipeline is able to substantially improve\nthe accuracy of swinging up unseen objects. We also show that objects with\nsimilar dynamics are closer to each other on the embedding space and that the\nembedding can be disentangled into values of specific physical properties.\n",
        "published": "2021",
        "authors": [
            "Chen Wang",
            "Shaoxiong Wang",
            "Branden Romero",
            "Filipe Veiga",
            "Edward Adelson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.00373v4",
        "title": "Investigating the Impact of Multi-LiDAR Placement on Object Detection\n  for Autonomous Driving",
        "abstract": "  The past few years have witnessed an increasing interest in improving the\nperception performance of LiDARs on autonomous vehicles. While most of the\nexisting works focus on developing new deep learning algorithms or model\narchitectures, we study the problem from the physical design perspective, i.e.,\nhow different placements of multiple LiDARs influence the learning-based\nperception. To this end, we introduce an easy-to-compute information-theoretic\nsurrogate metric to quantitatively and fast evaluate LiDAR placement for 3D\ndetection of different types of objects. We also present a new data collection,\ndetection model training and evaluation framework in the realistic CARLA\nsimulator to evaluate disparate multi-LiDAR configurations. Using several\nprevalent placements inspired by the designs of self-driving companies, we show\nthe correlation between our surrogate metric and object detection performance\nof different representative algorithms on KITTI through extensive experiments,\nvalidating the effectiveness of our LiDAR placement evaluation approach. Our\nresults show that sensor placement is non-negligible in 3D point cloud-based\nobject detection, which will contribute up to 10% performance discrepancy in\nterms of average precision in challenging 3D object detection settings. We\nbelieve that this is one of the first studies to quantitatively investigate the\ninfluence of LiDAR placement on perception performance. The code is available\non https://github.com/HanjiangHu/Multi-LiDAR-Placement-for-3D-Detection.\n",
        "published": "2021",
        "authors": [
            "Hanjiang Hu",
            "Zuxin Liu",
            "Sharad Chitlangia",
            "Akhil Agnihotri",
            "Ding Zhao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.01060v2",
        "title": "Curious Representation Learning for Embodied Intelligence",
        "abstract": "  Self-supervised representation learning has achieved remarkable success in\nrecent years. By subverting the need for supervised labels, such approaches are\nable to utilize the numerous unlabeled images that exist on the Internet and in\nphotographic datasets. Yet to build truly intelligent agents, we must construct\nrepresentation learning algorithms that can learn not only from datasets but\nalso learn from environments. An agent in a natural environment will not\ntypically be fed curated data. Instead, it must explore its environment to\nacquire the data it will learn from. We propose a framework, curious\nrepresentation learning (CRL), which jointly learns a reinforcement learning\npolicy and a visual representation model. The policy is trained to maximize the\nerror of the representation learner, and in doing so is incentivized to explore\nits environment. At the same time, the learned representation becomes stronger\nand stronger as the policy feeds it ever harder data to learn from. Our learned\nrepresentations enable promising transfer to downstream navigation tasks,\nperforming better than or comparably to ImageNet pretraining without using any\nsupervision at all. In addition, despite being trained in simulation, our\nlearned representations can obtain interpretable results on real images. Code\nis available at https://yilundu.github.io/crl/.\n",
        "published": "2021",
        "authors": [
            "Yilun Du",
            "Chuang Gan",
            "Phillip Isola"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.09371v2",
        "title": "VOILA: Visual-Observation-Only Imitation Learning for Autonomous\n  Navigation",
        "abstract": "  While imitation learning for vision based autonomous mobile robot navigation\nhas recently received a great deal of attention in the research community,\nexisting approaches typically require state action demonstrations that were\ngathered using the deployment platform. However, what if one cannot easily\noutfit their platform to record these demonstration signals or worse yet the\ndemonstrator does not have access to the platform at all? Is imitation learning\nfor vision based autonomous navigation even possible in such scenarios? In this\nwork, we hypothesize that the answer is yes and that recent ideas from the\nImitation from Observation (IfO) literature can be brought to bear such that a\nrobot can learn to navigate using only ego centric video collected by a\ndemonstrator, even in the presence of viewpoint mismatch. To this end, we\nintroduce a new algorithm, Visual Observation only Imitation Learning for\nAutonomous navigation (VOILA), that can successfully learn navigation policies\nfrom a single video demonstration collected from a physically different agent.\nWe evaluate VOILA in the photorealistic AirSim simulator and show that VOILA\nnot only successfully imitates the expert, but that it also learns navigation\npolicies that can generalize to novel environments. Further, we demonstrate the\neffectiveness of VOILA in a real world setting by showing that it allows a\nwheeled Jackal robot to successfully imitate a human walking in an environment\nusing a video recorded using a mobile phone camera.\n",
        "published": "2021",
        "authors": [
            "Haresh Karnan",
            "Garrett Warnell",
            "Xuesu Xiao",
            "Peter Stone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.09903v2",
        "title": "Multi-Perspective Anomaly Detection",
        "abstract": "  Anomaly detection is a critical problem in the manufacturing industry. In\nmany applications, images of objects to be analyzed are captured from multiple\nperspectives which can be exploited to improve the robustness of anomaly\ndetection. In this work, we build upon the deep support vector data description\nalgorithm and address multi-perspective anomaly detection using three different\nfusion techniques, i.e., early fusion, late fusion, and late fusion with\nmultiple decoders. We employ different augmentation techniques with a denoising\nprocess to deal with scarce one-class data, which further improves the\nperformance (ROC AUC $= 80\\%$). Furthermore, we introduce the dices dataset,\nwhich consists of over 2000 grayscale images of falling dices from multiple\nperspectives, with 5\\% of the images containing rare anomalies (e.g., drill\nholes, sawing, or scratches). We evaluate our approach on the new dices dataset\nusing images from two different perspectives and also benchmark on the standard\nMNIST dataset. Extensive experiments demonstrate that our proposed\n{multi-perspective} approach exceeds the state-of-the-art {single-perspective\nanomaly detection on both the MNIST and dices datasets}. To the best of our\nknowledge, this is the first work that focuses on addressing multi-perspective\nanomaly detection in images by jointly using different perspectives together\nwith one single objective function for anomaly detection.\n",
        "published": "2021",
        "authors": [
            "Peter Jakob",
            "Manav Madan",
            "Tobias Schmid-Schirling",
            "Abhinav Valada"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.10014v1",
        "title": "Evaluating Robustness over High Level Driving Instruction for Autonomous\n  Driving",
        "abstract": "  In recent years, we have witnessed increasingly high performance in the field\nof autonomous end-to-end driving. In particular, more and more research is\nbeing done on driving in urban environments, where the car has to follow high\nlevel commands to navigate. However, few evaluations are made on the ability of\nthese agents to react in an unexpected situation. Specifically, no evaluations\nare conducted on the robustness of driving agents in the event of a bad\nhigh-level command. We propose here an evaluation method, namely a benchmark\nthat allows to assess the robustness of an agent, and to appreciate its\nunderstanding of the environment through its ability to keep a safe behavior,\nregardless of the instruction.\n",
        "published": "2021",
        "authors": [
            "Florence Carton",
            "David Filliat",
            "Jaonary Rabarisoa",
            "Quoc Cuong Pham"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.11283v2",
        "title": "Coarse-to-Fine for Sim-to-Real: Sub-Millimetre Precision Across Wide\n  Task Spaces",
        "abstract": "  In this paper, we study the problem of zero-shot sim-to-real when the task\nrequires both highly precise control with sub-millimetre error tolerance, and\nwide task space generalisation. Our framework involves a coarse-to-fine\ncontroller, where trajectories begin with classical motion planning using\nICP-based pose estimation, and transition to a learned end-to-end controller\nwhich maps images to actions and is trained in simulation with domain\nrandomisation. In this way, we achieve precise control whilst also generalising\nthe controller across wide task spaces, and keeping the robustness of\nvision-based, end-to-end control. Real-world experiments on a range of\ndifferent tasks show that, by exploiting the best of both worlds, our framework\nsignificantly outperforms purely motion planning methods, and purely\nlearning-based methods. Furthermore, we answer a range of questions on best\npractices for precise sim-to-real transfer, such as how different image sensor\nmodalities and image feature representations perform.\n",
        "published": "2021",
        "authors": [
            "Eugene Valassakis",
            "Norman Di Palo",
            "Edward Johns"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.12774v1",
        "title": "DSLR: Dynamic to Static LiDAR Scan Reconstruction Using Adversarially\n  Trained Autoencoder",
        "abstract": "  Accurate reconstruction of static environments from LiDAR scans of scenes\ncontaining dynamic objects, which we refer to as Dynamic to Static Translation\n(DST), is an important area of research in Autonomous Navigation. This problem\nhas been recently explored for visual SLAM, but to the best of our knowledge no\nwork has been attempted to address DST for LiDAR scans. The problem is of\ncritical importance due to wide-spread adoption of LiDAR in Autonomous\nVehicles. We show that state-of the art methods developed for the visual domain\nwhen adapted for LiDAR scans perform poorly.\n  We develop DSLR, a deep generative model which learns a mapping between\ndynamic scan to its static counterpart through an adversarially trained\nautoencoder. Our model yields the first solution for DST on LiDAR that\ngenerates static scans without using explicit segmentation labels. DSLR cannot\nalways be applied to real world data due to lack of paired dynamic-static\nscans. Using Unsupervised Domain Adaptation, we propose DSLR-UDA for transfer\nto real world data and experimentally show that this performs well in real\nworld settings. Additionally, if segmentation information is available, we\nextend DSLR to DSLR-Seg to further improve the reconstruction quality.\n  DSLR gives the state of the art performance on simulated and real-world\ndatasets and also shows at least 4x improvement. We show that DSLR, unlike the\nexisting baselines, is a practically viable model with its reconstruction\nquality within the tolerable limits for tasks pertaining to autonomous\nnavigation like SLAM in dynamic environments.\n",
        "published": "2021",
        "authors": [
            "Prashant Kumar",
            "Sabyasachi Sahoo",
            "Vanshil Shah",
            "Vineetha Kondameedi",
            "Abhinav Jain",
            "Akshaj Verma",
            "Chiranjib Bhattacharyya",
            "Vinay Viswanathan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.12923v1",
        "title": "Robust Navigation for Racing Drones based on Imitation Learning and\n  Modularization",
        "abstract": "  This paper presents a vision-based modularized drone racing navigation system\nthat uses a customized convolutional neural network (CNN) for the perception\nmodule to produce high-level navigation commands and then leverages a\nstate-of-the-art planner and controller to generate low-level control commands,\nthus exploiting the advantages of both data-based and model-based approaches.\nUnlike the state-of-the-art method which only takes the current camera image as\nthe CNN input, we further add the latest three drone states as part of the\ninputs. Our method outperforms the state-of-the-art method in various track\nlayouts and offers two switchable navigation behaviors with a single trained\nnetwork. The CNN-based perception module is trained to imitate an expert policy\nthat automatically generates ground truth navigation commands based on the\npre-computed global trajectories. Owing to the extensive randomization and our\nmodified dataset aggregation (DAgger) policy during data collection, our\nnavigation system, which is purely trained in simulation with synthetic\ntextures, successfully operates in environments with randomly-chosen\nphotorealistic textures without further fine-tuning.\n",
        "published": "2021",
        "authors": [
            "Tianqi Wang",
            "Dong Eui Chang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.14829v2",
        "title": "Q-attention: Enabling Efficient Learning for Vision-based Robotic\n  Manipulation",
        "abstract": "  Despite the success of reinforcement learning methods, they have yet to have\ntheir breakthrough moment when applied to a broad range of robotic manipulation\ntasks. This is partly due to the fact that reinforcement learning algorithms\nare notoriously difficult and time consuming to train, which is exacerbated\nwhen training from images rather than full-state inputs. As humans perform\nmanipulation tasks, our eyes closely monitor every step of the process with our\ngaze focusing sequentially on the objects being manipulated. With this in mind,\nwe present our Attention-driven Robotic Manipulation (ARM) algorithm, which is\na general manipulation algorithm that can be applied to a range of\nsparse-rewarded tasks, given only a small number of demonstrations. ARM splits\nthe complex task of manipulation into a 3 stage pipeline: (1) a Q-attention\nagent extracts relevant pixel locations from RGB and point cloud inputs, (2) a\nnext-best pose agent that accepts crops from the Q-attention agent and outputs\nposes, and (3) a control agent that takes the goal pose and outputs joint\nactions. We show that current learning algorithms fail on a range of RLBench\ntasks, whilst ARM is successful.\n",
        "published": "2021",
        "authors": [
            "Stephen James",
            "Andrew J. Davison"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.02308v1",
        "title": "A visual introduction to Gaussian Belief Propagation",
        "abstract": "  In this article, we present a visual introduction to Gaussian Belief\nPropagation (GBP), an approximate probabilistic inference algorithm that\noperates by passing messages between the nodes of arbitrarily structured factor\ngraphs. A special case of loopy belief propagation, GBP updates rely only on\nlocal information and will converge independently of the message schedule. Our\nkey argument is that, given recent trends in computing hardware, GBP has the\nright computational properties to act as a scalable distributed probabilistic\ninference framework for future machine learning systems.\n",
        "published": "2021",
        "authors": [
            "Joseph Ortiz",
            "Talfan Evans",
            "Andrew J. Davison"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.02792v1",
        "title": "Learned Visual Navigation for Under-Canopy Agricultural Robots",
        "abstract": "  We describe a system for visually guided autonomous navigation of\nunder-canopy farm robots. Low-cost under-canopy robots can drive between crop\nrows under the plant canopy and accomplish tasks that are infeasible for\nover-the-canopy drones or larger agricultural equipment. However, autonomously\nnavigating them under the canopy presents a number of challenges: unreliable\nGPS and LiDAR, high cost of sensing, challenging farm terrain, clutter due to\nleaves and weeds, and large variability in appearance over the season and\nacross crop types. We address these challenges by building a modular system\nthat leverages machine learning for robust and generalizable perception from\nmonocular RGB images from low-cost cameras, and model predictive control for\naccurate control in challenging terrain. Our system, CropFollow, is able to\nautonomously drive 485 meters per intervention on average, outperforming a\nstate-of-the-art LiDAR based system (286 meters per intervention) in extensive\nfield testing spanning over 25 km.\n",
        "published": "2021",
        "authors": [
            "Arun Narenthiran Sivakumar",
            "Sahil Modi",
            "Mateus Valverde Gasparino",
            "Che Ellis",
            "Andres Eduardo Baquero Velasquez",
            "Girish Chowdhary",
            "Saurabh Gupta"
        ]
    }
]