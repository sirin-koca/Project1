[
    {
        "id": "http://arxiv.org/abs/2107.02974v1",
        "title": "RAM-VO: Less is more in Visual Odometry",
        "abstract": "  Building vehicles capable of operating without human supervision requires the\ndetermination of the agent's pose. Visual Odometry (VO) algorithms estimate the\negomotion using only visual changes from the input images. The most recent VO\nmethods implement deep-learning techniques using convolutional neural networks\n(CNN) extensively, which add a substantial cost when dealing with\nhigh-resolution images. Furthermore, in VO tasks, more input data does not mean\na better prediction; on the contrary, the architecture may filter out useless\ninformation. Therefore, the implementation of computationally efficient and\nlightweight architectures is essential. In this work, we propose the RAM-VO, an\nextension of the Recurrent Attention Model (RAM) for visual odometry tasks.\nRAM-VO improves the visual and temporal representation of information and\nimplements the Proximal Policy Optimization (PPO) algorithm to learn robust\npolicies. The results indicate that RAM-VO can perform regressions with six\ndegrees of freedom from monocular input images using approximately 3 million\nparameters. In addition, experiments on the KITTI dataset demonstrate that\nRAM-VO achieves competitive results using only 5.7% of the available visual\ninformation.\n",
        "published": "2021",
        "authors": [
            "Iury Cleveston",
            "Esther L. Colombini"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.04000v1",
        "title": "Active Safety Envelopes using Light Curtains with Probabilistic\n  Guarantees",
        "abstract": "  To safely navigate unknown environments, robots must accurately perceive\ndynamic obstacles. Instead of directly measuring the scene depth with a LiDAR\nsensor, we explore the use of a much cheaper and higher resolution sensor:\nprogrammable light curtains. Light curtains are controllable depth sensors that\nsense only along a surface that a user selects. We use light curtains to\nestimate the safety envelope of a scene: a hypothetical surface that separates\nthe robot from all obstacles. We show that generating light curtains that sense\nrandom locations (from a particular distribution) can quickly discover the\nsafety envelope for scenes with unknown objects. Importantly, we produce\ntheoretical safety guarantees on the probability of detecting an obstacle using\nrandom curtains. We combine random curtains with a machine learning based model\nthat forecasts and tracks the motion of the safety envelope efficiently. Our\nmethod accurately estimates safety envelopes while providing probabilistic\nsafety guarantees that can be used to certify the efficacy of a robot\nperception system to detect and avoid dynamic obstacles. We evaluate our\napproach in a simulated urban driving environment and a real-world environment\nwith moving pedestrians using a light curtain device and show that we can\nestimate safety envelopes efficiently and effectively. Project website:\nhttps://siddancha.github.io/projects/active-safety-envelopes-with-guarantees\n",
        "published": "2021",
        "authors": [
            "Siddharth Ancha",
            "Gaurav Pathak",
            "Srinivasa G. Narasimhan",
            "David Held"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.04013v1",
        "title": "Multi-Modality Task Cascade for 3D Object Detection",
        "abstract": "  Point clouds and RGB images are naturally complementary modalities for 3D\nvisual understanding - the former provides sparse but accurate locations of\npoints on objects, while the latter contains dense color and texture\ninformation. Despite this potential for close sensor fusion, many methods train\ntwo models in isolation and use simple feature concatenation to represent 3D\nsensor data. This separated training scheme results in potentially sub-optimal\nperformance and prevents 3D tasks from being used to benefit 2D tasks that are\noften useful on their own. To provide a more integrated approach, we propose a\nnovel Multi-Modality Task Cascade network (MTC-RCNN) that leverages 3D box\nproposals to improve 2D segmentation predictions, which are then used to\nfurther refine the 3D boxes. We show that including a 2D network between two\nstages of 3D modules significantly improves both 2D and 3D task performance.\nMoreover, to prevent the 3D module from over-relying on the overfitted 2D\npredictions, we propose a dual-head 2D segmentation training and inference\nscheme, allowing the 2nd 3D module to learn to interpret imperfect 2D\nsegmentation predictions. Evaluating our model on the challenging SUN RGB-D\ndataset, we improve upon state-of-the-art results of both single modality and\nfusion networks by a large margin ($\\textbf{+3.8}$ mAP@0.5). Code will be\nreleased $\\href{https://github.com/Divadi/MTC_RCNN}{\\text{here.}}$\n",
        "published": "2021",
        "authors": [
            "Jinhyung Park",
            "Xinshuo Weng",
            "Yunze Man",
            "Kris Kitani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.04034v1",
        "title": "RMA: Rapid Motor Adaptation for Legged Robots",
        "abstract": "  Successful real-world deployment of legged robots would require them to adapt\nin real-time to unseen scenarios like changing terrains, changing payloads,\nwear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to\nsolve this problem of real-time online adaptation in quadruped robots. RMA\nconsists of two components: a base policy and an adaptation module. The\ncombination of these components enables the robot to adapt to novel situations\nin fractions of a second. RMA is trained completely in simulation without using\nany domain knowledge like reference trajectories or predefined foot trajectory\ngenerators and is deployed on the A1 robot without any fine-tuning. We train\nRMA on a varied terrain generator using bioenergetics-inspired rewards and\ndeploy it on a variety of difficult terrains including rocky, slippery,\ndeformable surfaces in environments with grass, long vegetation, concrete,\npebbles, stairs, sand, etc. RMA shows state-of-the-art performance across\ndiverse real-world as well as simulation experiments. Video results at\nhttps://ashish-kmr.github.io/rma-legged-robots/\n",
        "published": "2021",
        "authors": [
            "Ashish Kumar",
            "Zipeng Fu",
            "Deepak Pathak",
            "Jitendra Malik"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.04619v1",
        "title": "Diverse Video Generation using a Gaussian Process Trigger",
        "abstract": "  Generating future frames given a few context (or past) frames is a\nchallenging task. It requires modeling the temporal coherence of videos and\nmulti-modality in terms of diversity in the potential future states. Current\nvariational approaches for video generation tend to marginalize over\nmulti-modal future outcomes. Instead, we propose to explicitly model the\nmulti-modality in the future outcomes and leverage it to sample diverse\nfutures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to\nlearn priors on future states given the past and maintains a probability\ndistribution over possible futures given a particular sample. In addition, we\nleverage the changes in this distribution over time to control the sampling of\ndiverse future states by estimating the end of ongoing sequences. That is, we\nuse the variance of GP over the output function space to trigger a change in an\naction sequence. We achieve state-of-the-art results on diverse future frame\ngeneration in terms of reconstruction quality and diversity of the generated\nsequences.\n",
        "published": "2021",
        "authors": [
            "Gaurav Shrivastava",
            "Abhinav Shrivastava"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.08142v3",
        "title": "Autonomy 2.0: Why is self-driving always 5 years away?",
        "abstract": "  Despite the numerous successes of machine learning over the past decade\n(image recognition, decision-making, NLP, image synthesis), self-driving\ntechnology has not yet followed the same trend. In this paper, we study the\nhistory, composition, and development bottlenecks of the modern self-driving\nstack. We argue that the slow progress is caused by approaches that require too\nmuch hand-engineering, an over-reliance on road testing, and high fleet\ndeployment costs. We observe that the classical stack has several bottlenecks\nthat preclude the necessary scale needed to capture the long tail of rare\nevents. To resolve these problems, we outline the principles of Autonomy 2.0,\nan ML-first approach to self-driving, as a viable alternative to the currently\nadopted state-of-the-art. This approach is based on (i) a fully differentiable\nAV stack trainable from human demonstrations, (ii) closed-loop data-driven\nreactive simulation, and (iii) large-scale, low-cost data collections as\ncritical solutions towards scalability issues. We outline the general\narchitecture, survey promising works in this direction and propose key\nchallenges to be addressed by the community in the future.\n",
        "published": "2021",
        "authors": [
            "Ashesh Jain",
            "Luca Del Pero",
            "Hugo Grimmett",
            "Peter Ondruska"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.09046v1",
        "title": "Playful Interactions for Representation Learning",
        "abstract": "  One of the key challenges in visual imitation learning is collecting large\namounts of expert demonstrations for a given task. While methods for collecting\nhuman demonstrations are becoming easier with teleoperation methods and the use\nof low-cost assistive tools, we often still require 100-1000 demonstrations for\nevery task to learn a visual representation and policy. To address this, we\nturn to an alternate form of data that does not require task-specific\ndemonstrations -- play. Playing is a fundamental method children use to learn a\nset of skills and behaviors and visual representations in early learning.\nImportantly, play data is diverse, task-agnostic, and relatively cheap to\nobtain. In this work, we propose to use playful interactions in a\nself-supervised manner to learn visual representations for downstream tasks. We\ncollect 2 hours of playful data in 19 diverse environments and use\nself-predictive learning to extract visual representations. Given these\nrepresentations, we train policies using imitation learning for two downstream\ntasks: Pushing and Stacking. We demonstrate that our visual representations\ngeneralize better than standard behavior cloning and can achieve similar\nperformance with only half the number of required demonstrations. Our\nrepresentations, which are trained from scratch, compare favorably against\nImageNet pretrained representations. Finally, we provide an experimental\nanalysis on the effects of different pretraining modes on downstream task\nlearning.\n",
        "published": "2021",
        "authors": [
            "Sarah Young",
            "Jyothish Pari",
            "Pieter Abbeel",
            "Lerrel Pinto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.09170v1",
        "title": "DeepSocNav: Social Navigation by Imitating Human Behaviors",
        "abstract": "  Current datasets to train social behaviors are usually borrowed from\nsurveillance applications that capture visual data from a bird's-eye\nperspective. This leaves aside precious relationships and visual cues that\ncould be captured through a first-person view of a scene. In this work, we\npropose a strategy to exploit the power of current game engines, such as Unity,\nto transform pre-existing bird's-eye view datasets into a first-person view, in\nparticular, a depth view. Using this strategy, we are able to generate large\nvolumes of synthetic data that can be used to pre-train a social navigation\nmodel. To test our ideas, we present DeepSocNav, a deep learning based model\nthat takes advantage of the proposed approach to generate synthetic data.\nFurthermore, DeepSocNav includes a self-supervised strategy that is included as\nan auxiliary task. This consists of predicting the next depth frame that the\nagent will face. Our experiments show the benefits of the proposed model that\nis able to outperform relevant baselines in terms of social navigation scores.\n",
        "published": "2021",
        "authors": [
            "Juan Pablo de Vicente",
            "Alvaro Soto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.13389v1",
        "title": "SimROD: A Simple Adaptation Method for Robust Object Detection",
        "abstract": "  This paper presents a Simple and effective unsupervised adaptation method for\nRobust Object Detection (SimROD). To overcome the challenging issues of domain\nshift and pseudo-label noise, our method integrates a novel domain-centric\naugmentation method, a gradual self-labeling adaptation procedure, and a\nteacher-guided fine-tuning mechanism. Using our method, target domain samples\ncan be leveraged to adapt object detection models without changing the model\narchitecture or generating synthetic data. When applied to image corruptions\nand high-level cross-domain adaptation benchmarks, our method outperforms prior\nbaselines on multiple domain adaptation benchmarks. SimROD achieves new\nstate-of-the-art on standard real-to-synthetic and cross-camera setup\nbenchmarks. On the image corruption benchmark, models adapted with our method\nachieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6%\nAP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method\noutperformed the best baseline performance by up to 8% AP50 on Comic dataset\nand up to 4% on Watercolor dataset.\n",
        "published": "2021",
        "authors": [
            "Rindra Ramamonjison",
            "Amin Banitalebi-Dehkordi",
            "Xinyu Kang",
            "Xiaolong Bai",
            "Yong Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.14229v4",
        "title": "Physics-informed Guided Disentanglement in Generative Networks",
        "abstract": "  Image-to-image translation (i2i) networks suffer from entanglement effects in\npresence of physics-related phenomena in target domain (such as occlusions,\nfog, etc), lowering altogether the translation quality, controllability and\nvariability. In this paper, we propose a general framework to disentangle\nvisual traits in target images. Primarily, we build upon collection of simple\nphysics models, guiding the disentanglement with a physical model that renders\nsome of the target traits, and learning the remaining ones. Because physics\nallows explicit and interpretable outputs, our physical models (optimally\nregressed on target) allows generating unseen scenarios in a controllable\nmanner. Secondarily, we show the versatility of our framework to neural-guided\ndisentanglement where a generative network is used in place of a physical model\nin case the latter is not directly accessible. Altogether, we introduce three\nstrategies of disentanglement being guided from either a fully differentiable\nphysics model, a (partially) non-differentiable physics model, or a neural\nnetwork. The results show our disentanglement strategies dramatically increase\nperformances qualitatively and quantitatively in several challenging scenarios\nfor image translation.\n",
        "published": "2021",
        "authors": [
            "Fabio Pizzati",
            "Pietro Cerri",
            "Raoul de Charette"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.14483v5",
        "title": "ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale\n  Demonstrations",
        "abstract": "  Object manipulation from 3D visual inputs poses many challenges on building\ngeneralizable perception and policy models. However, 3D assets in existing\nbenchmarks mostly lack the diversity of 3D shapes that align with real-world\nintra-class complexity in topology and geometry. Here we propose SAPIEN\nManipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over\ndiverse objects in a full-physics simulator. 3D assets in ManiSkill include\nlarge intra-class topological and geometric variations. Tasks are carefully\nchosen to cover distinct types of manipulation challenges. Latest progress in\n3D vision also makes us believe that we should customize the benchmark so that\nthe challenge is inviting to researchers working on 3D deep learning. To this\nend, we simulate a moving panoramic camera that returns ego-centric point\nclouds or RGB-D images. In addition, we would like ManiSkill to serve a broad\nset of researchers interested in manipulation research. Besides supporting the\nlearning of policies from interactions, we also support\nlearning-from-demonstrations (LfD) methods, by providing a large number of\nhigh-quality demonstrations (~36,000 successful trajectories, ~1.5M point\ncloud/RGB-D frames in total). We provide baselines using 3D deep learning and\nLfD algorithms. All code of our benchmark (simulator, environment, SDK, and\nbaselines) is open-sourced, and a challenge facing interdisciplinary\nresearchers will be held based on the benchmark.\n",
        "published": "2021",
        "authors": [
            "Tongzhou Mu",
            "Zhan Ling",
            "Fanbo Xiang",
            "Derek Yang",
            "Xuanlin Li",
            "Stone Tao",
            "Zhiao Huang",
            "Zhiwei Jia",
            "Hao Su"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.03059v1",
        "title": "Evaluation of Runtime Monitoring for UAV Emergency Landing",
        "abstract": "  To certify UAV operations in populated areas, risk mitigation strategies --\nsuch as Emergency Landing (EL) -- must be in place to account for potential\nfailures. EL aims at reducing ground risk by finding safe landing areas using\non-board sensors. The first contribution of this paper is to present a new EL\napproach, in line with safety requirements introduced in recent research. In\nparticular, the proposed EL pipeline includes mechanisms to monitor learning\nbased components during execution. This way, another contribution is to study\nthe behavior of Machine Learning Runtime Monitoring (MLRM) approaches within\nthe context of a real-world critical system. A new evaluation methodology is\nintroduced, and applied to assess the practical safety benefits of three MLRM\nmechanisms. The proposed approach is compared to a default mitigation strategy\n(open a parachute when a failure is detected), and appears to be much safer.\n",
        "published": "2022",
        "authors": [
            "Joris Guerin",
            "Kevin Delmas",
            "J\u00e9r\u00e9mie Guiochet"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.03957v1",
        "title": "Bingham Policy Parameterization for 3D Rotations in Reinforcement\n  Learning",
        "abstract": "  We propose a new policy parameterization for representing 3D rotations during\nreinforcement learning. Today in the continuous control reinforcement learning\nliterature, many stochastic policy parameterizations are Gaussian. We argue\nthat universally applying a Gaussian policy parameterization is not always\ndesirable for all environments. One such case in particular where this is true\nare tasks that involve predicting a 3D rotation output, either in isolation, or\ncoupled with translation as part of a full 6D pose output. Our proposed Bingham\nPolicy Parameterization (BPP) models the Bingham distribution and allows for\nbetter rotation (quaternion) prediction over a Gaussian policy parameterization\nin a range of reinforcement learning tasks. We evaluate BPP on the rotation\nWahba problem task, as well as a set of vision-based next-best pose robot\nmanipulation tasks from RLBench. We hope that this paper encourages more\nresearch into developing other policy parameterization that are more suited for\nparticular environments, rather than always assuming Gaussian.\n",
        "published": "2022",
        "authors": [
            "Stephen James",
            "Pieter Abbeel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.05200v3",
        "title": "Visual Servoing for Pose Control of Soft Continuum Arm in a Structured\n  Environment",
        "abstract": "  For soft continuum arms, visual servoing is a popular control strategy that\nrelies on visual feedback to close the control loop. However, robust visual\nservoing is challenging as it requires reliable feature extraction from the\nimage, accurate control models and sensors to perceive the shape of the arm,\nboth of which can be hard to implement in a soft robot. This letter circumvents\nthese challenges by presenting a deep neural network-based method to perform\nsmooth and robust 3D positioning tasks on a soft arm by visual servoing using a\ncamera mounted at the distal end of the arm. A convolutional neural network is\ntrained to predict the actuations required to achieve the desired pose in a\nstructured environment. Integrated and modular approaches for estimating the\nactuations from the image are proposed and are experimentally compared. A\nproportional control law is implemented to reduce the error between the desired\nand current image as seen by the camera. The model together with the\nproportional feedback control makes the described approach robust to several\nvariations such as new targets, lighting, loads, and diminution of the soft\narm. Furthermore, the model lends itself to be transferred to a new environment\nwith minimal effort.\n",
        "published": "2022",
        "authors": [
            "Shivani Kamtikar",
            "Samhita Marri",
            "Benjamin Walt",
            "Naveen Kumar Uppalapati",
            "Girish Krishnan",
            "Girish Chowdhary"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.05832v2",
        "title": "SafePicking: Learning Safe Object Extraction via Object-Level Mapping",
        "abstract": "  Robots need object-level scene understanding to manipulate objects while\nreasoning about contact, support, and occlusion among objects. Given a pile of\nobjects, object recognition and reconstruction can identify the boundary of\nobject instances, giving important cues as to how the objects form and support\nthe pile. In this work, we present a system, SafePicking, that integrates\nobject-level mapping and learning-based motion planning to generate a motion\nthat safely extracts occluded target objects from a pile. Planning is done by\nlearning a deep Q-network that receives observations of predicted poses and a\ndepth-based heightmap to output a motion trajectory, trained to maximize a\nsafety metric reward. Our results show that the observation fusion of poses and\ndepth-sensing gives both better performance and robustness to the model. We\nevaluate our methods using the YCB objects in both simulation and the real\nworld, achieving safe object extraction from piles.\n",
        "published": "2022",
        "authors": [
            "Kentaro Wada",
            "Stephen James",
            "Andrew J. Davison"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.10448v2",
        "title": "Robotic Telekinesis: Learning a Robotic Hand Imitator by Watching Humans\n  on Youtube",
        "abstract": "  We build a system that enables any human to control a robot hand and arm,\nsimply by demonstrating motions with their own hand. The robot observes the\nhuman operator via a single RGB camera and imitates their actions in real-time.\nHuman hands and robot hands differ in shape, size, and joint structure, and\nperforming this translation from a single uncalibrated camera is a highly\nunderconstrained problem. Moreover, the retargeted trajectories must\neffectively execute tasks on a physical robot, which requires them to be\ntemporally smooth and free of self-collisions. Our key insight is that while\npaired human-robot correspondence data is expensive to collect, the internet\ncontains a massive corpus of rich and diverse human hand videos. We leverage\nthis data to train a system that understands human hands and retargets a human\nvideo stream into a robot hand-arm trajectory that is smooth, swift, safe, and\nsemantically similar to the guiding demonstration. We demonstrate that it\nenables previously untrained people to teleoperate a robot on various dexterous\nmanipulation tasks. Our low-cost, glove-free, marker-free remote teleoperation\nsystem makes robot teaching more accessible and we hope that it can aid robots\nin learning to act autonomously in the real world. Videos at\nhttps://robotic-telekinesis.github.io/\n",
        "published": "2022",
        "authors": [
            "Aravind Sivakumar",
            "Kenneth Shaw",
            "Deepak Pathak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.12555v2",
        "title": "6D Rotation Representation For Unconstrained Head Pose Estimation",
        "abstract": "  In this paper, we present a method for unconstrained end-to-end head pose\nestimation. We address the problem of ambiguous rotation labels by introducing\nthe rotation matrix formalism for our ground truth data and propose a\ncontinuous 6D rotation matrix representation for efficient and robust direct\nregression. This way, our method can learn the full rotation appearance which\nis contrary to previous approaches that restrict the pose prediction to a\nnarrow-angle for satisfactory results. In addition, we propose a geodesic\ndistance-based loss to penalize our network with respect to the SO(3) manifold\ngeometry. Experiments on the public AFLW2000 and BIWI datasets demonstrate that\nour proposed method significantly outperforms other state-of-the-art methods by\nup to 20\\%. We open-source our training and testing code along with our\npre-trained models: https://github.com/thohemp/6DRepNet.\n",
        "published": "2022",
        "authors": [
            "Thorsten Hempel",
            "Ahmed A. Abdelrahman",
            "Ayoub Al-Hamadi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.13427v1",
        "title": "Meta-path Analysis on Spatio-Temporal Graphs for Pedestrian Trajectory\n  Prediction",
        "abstract": "  Spatio-temporal graphs (ST-graphs) have been used to model time series tasks\nsuch as traffic forecasting, human motion modeling, and action recognition. The\nhigh-level structure and corresponding features from ST-graphs have led to\nimproved performance over traditional architectures. However, current methods\ntend to be limited by simple features, despite the rich information provided by\nthe full graph structure, which leads to inefficiencies and suboptimal\nperformance in downstream tasks. We propose the use of features derived from\nmeta-paths, walks across different types of edges, in ST-graphs to improve the\nperformance of Structural Recurrent Neural Network. In this paper, we present\nthe Meta-path Enhanced Structural Recurrent Neural Network (MESRNN), a generic\nframework that can be applied to any spatio-temporal task in a simple and\nscalable manner. We employ MESRNN for pedestrian trajectory prediction,\nutilizing these meta-path based features to capture the relationships between\nthe trajectories of pedestrians at different points in time and space. We\ncompare our MESRNN against state-of-the-art ST-graph methods on standard\ndatasets to show the performance boost provided by meta-path information. The\nproposed model consistently outperforms the baselines in trajectory prediction\nover long time horizons by over 32\\%, and produces more socially compliant\ntrajectories in dense crowds. For more information please refer to the project\nwebsite at https://sites.google.com/illinois.edu/mesrnn/home.\n",
        "published": "2022",
        "authors": [
            "Aamir Hasan",
            "Pranav Sriram",
            "Katherine Driggs-Campbell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1503.01820v1",
        "title": "Latent Hierarchical Model for Activity Recognition",
        "abstract": "  We present a novel hierarchical model for human activity recognition. In\ncontrast to approaches that successively recognize actions and activities, our\napproach jointly models actions and activities in a unified framework, and\ntheir labels are simultaneously predicted. The model is embedded with a latent\nlayer that is able to capture a richer class of contextual information in both\nstate-state and observation-state pairs. Although loops are present in the\nmodel, the model has an overall linear-chain structure, where the exact\ninference is tractable. Therefore, the model is very efficient in both\ninference and learning. The parameters of the graphical model are learned with\na Structured Support Vector Machine (Structured-SVM). A data-driven approach is\nused to initialize the latent variables; therefore, no manual labeling for the\nlatent states is required. The experimental results from using two benchmark\ndatasets show that our model outperforms the state-of-the-art approach, and our\nmodel is computationally more efficient.\n",
        "published": "2015",
        "authors": [
            "Ninghang Hu",
            "Gwenn Englebienne",
            "Zhongyu Lou",
            "Ben Kr\u00f6se"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.02378v3",
        "title": "SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks",
        "abstract": "  We introduce SE3-Nets, which are deep neural networks designed to model and\nlearn rigid body motion from raw point cloud data. Based only on sequences of\ndepth images along with action vectors and point wise data associations,\nSE3-Nets learn to segment effected object parts and predict their motion\nresulting from the applied force. Rather than learning point wise flow vectors,\nSE3-Nets predict SE3 transformations for different parts of the scene. Using\nsimulated depth data of a table top scene and a robot manipulator, we show that\nthe structure underlying SE3-Nets enables them to generate a far more\nconsistent prediction of object motion than traditional flow based networks.\nAdditional experiments with a depth camera observing a Baxter robot pushing\nobjects on a table show that SE3-Nets also work well on real data.\n",
        "published": "2016",
        "authors": [
            "Arunkumar Byravan",
            "Dieter Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1610.00696v2",
        "title": "Deep Visual Foresight for Planning Robot Motion",
        "abstract": "  A key challenge in scaling up robot learning to many skills and environments\nis removing the need for human supervision, so that robots can collect their\nown data and improve their own performance without being limited by the cost of\nrequesting human feedback. Model-based reinforcement learning holds the promise\nof enabling an agent to learn to predict the effects of its actions, which\ncould provide flexible predictive models for a wide range of tasks and\nenvironments, without detailed human supervision. We develop a method for\ncombining deep action-conditioned video prediction models with model-predictive\ncontrol that uses entirely unlabeled training data. Our approach does not\nrequire a calibrated camera, an instrumented training set-up, nor precise\nsensing and actuation. Our results show that our method enables a real robot to\nperform nonprehensile manipulation -- pushing objects -- and can handle novel\nobjects not seen during training.\n",
        "published": "2016",
        "authors": [
            "Chelsea Finn",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1610.01238v3",
        "title": "Find Your Own Way: Weakly-Supervised Segmentation of Path Proposals for\n  Urban Autonomy",
        "abstract": "  We present a weakly-supervised approach to segmenting proposed drivable paths\nin images with the goal of autonomous driving in complex urban environments.\nUsing recorded routes from a data collection vehicle, our proposed method\ngenerates vast quantities of labelled images containing proposed paths and\nobstacles without requiring manual annotation, which we then use to train a\ndeep semantic segmentation network. With the trained network we can segment\nproposed paths and obstacles at run-time using a vehicle equipped with only a\nmonocular camera without relying on explicit modelling of road or lane\nmarkings. We evaluate our method on the large-scale KITTI and Oxford RobotCar\ndatasets and demonstrate reliable path proposal and obstacle segmentation in a\nwide variety of environments under a range of lighting, weather and traffic\nconditions. We illustrate how the method can generalise to multiple path\nproposals at intersections and outline plans to incorporate the system into a\nframework for autonomous urban driving.\n",
        "published": "2016",
        "authors": [
            "Dan Barnes",
            "Will Maddern",
            "Ingmar Posner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1610.03677v2",
        "title": "Deep Fruit Detection in Orchards",
        "abstract": "  An accurate and reliable image based fruit detection system is critical for\nsupporting higher level agriculture tasks such as yield mapping and robotic\nharvesting. This paper presents the use of a state-of-the-art object detection\nframework, Faster R-CNN, in the context of fruit detection in orchards,\nincluding mangoes, almonds and apples. Ablation studies are presented to better\nunderstand the practical deployment of the detection network, including how\nmuch training data is required to capture variability in the dataset. Data\naugmentation techniques are shown to yield significant performance gains,\nresulting in a greater than two-fold reduction in the number of training images\nrequired. In contrast, transferring knowledge between orchards contributed to\nnegligible performance gain over initialising the Deep Convolutional Neural\nNetwork directly from ImageNet features. Finally, to operate over orchard data\ncontaining between 100-1000 fruit per image, a tiling approach is introduced\nfor the Faster R-CNN framework. The study has resulted in the best yet\ndetection performance for these orchards relative to previous works, with an\nF1-score of >0.9 achieved for apples and mangoes.\n",
        "published": "2016",
        "authors": [
            "Suchet Bargoti",
            "James Underwood"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1611.03673v3",
        "title": "Learning to Navigate in Complex Environments",
        "abstract": "  Learning to navigate in complex environments with dynamic elements is an\nimportant milestone in developing AI agents. In this work we formulate the\nnavigation question as a reinforcement learning problem and show that data\nefficiency and task performance can be dramatically improved by relying on\nadditional auxiliary tasks leveraging multimodal sensory inputs. In particular\nwe consider jointly learning the goal-driven reinforcement learning problem\nwith auxiliary depth prediction and loop closure classification tasks. This\napproach can learn to navigate from raw sensory input in complicated 3D mazes,\napproaching human-level performance even under conditions where the goal\nlocation changes frequently. We provide detailed analysis of the agent\nbehaviour, its ability to localise, and its network activity dynamics, showing\nthat the agent implicitly learns key navigation abilities.\n",
        "published": "2016",
        "authors": [
            "Piotr Mirowski",
            "Razvan Pascanu",
            "Fabio Viola",
            "Hubert Soyer",
            "Andrew J. Ballard",
            "Andrea Banino",
            "Misha Denil",
            "Ross Goroshin",
            "Laurent Sifre",
            "Koray Kavukcuoglu",
            "Dharshan Kumaran",
            "Raia Hadsell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1708.07303v4",
        "title": "Learning 6-DOF Grasping Interaction via Deep Geometry-aware 3D\n  Representations",
        "abstract": "  This paper focuses on the problem of learning 6-DOF grasping with a parallel\njaw gripper in simulation. We propose the notion of a geometry-aware\nrepresentation in grasping based on the assumption that knowledge of 3D\ngeometry is at the heart of interaction. Our key idea is constraining and\nregularizing grasping interaction learning through 3D geometry prediction.\nSpecifically, we formulate the learning of deep geometry-aware grasping model\nin two steps: First, we learn to build mental geometry-aware representation by\nreconstructing the scene (i.e., 3D occupancy grid) from RGBD input via\ngenerative 3D shape modeling. Second, we learn to predict grasping outcome with\nits internal geometry-aware representation. The learned outcome prediction\nmodel is used to sequentially propose grasping solutions via\nanalysis-by-synthesis optimization. Our contributions are fourfold: (1) To best\nof our knowledge, we are presenting for the first time a method to learn a\n6-DOF grasping net from RGBD input; (2) We build a grasping dataset from\ndemonstrations in virtual reality with rich sensory and interaction\nannotations. This dataset includes 101 everyday objects spread across 7\ncategories, additionally, we propose a data augmentation strategy for effective\nlearning; (3) We demonstrate that the learned geometry-aware representation\nleads to about 10 percent relative performance improvement over the baseline\nCNN on grasping objects from our dataset. (4) We further demonstrate that the\nmodel generalizes to novel viewpoints and object instances.\n",
        "published": "2017",
        "authors": [
            "Xinchen Yan",
            "Jasmine Hsu",
            "Mohi Khansari",
            "Yunfei Bai",
            "Arkanath Pathak",
            "Abhinav Gupta",
            "James Davidson",
            "Honglak Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1708.07969v1",
        "title": "3D Object Reconstruction from a Single Depth View with Adversarial\n  Learning",
        "abstract": "  In this paper, we propose a novel 3D-RecGAN approach, which reconstructs the\ncomplete 3D structure of a given object from a single arbitrary depth view\nusing generative adversarial networks. Unlike the existing work which typically\nrequires multiple views of the same object or class labels to recover the full\n3D geometry, the proposed 3D-RecGAN only takes the voxel grid representation of\na depth view of the object as input, and is able to generate the complete 3D\noccupancy grid by filling in the occluded/missing regions. The key idea is to\ncombine the generative capabilities of autoencoders and the conditional\nGenerative Adversarial Networks (GAN) framework, to infer accurate and\nfine-grained 3D structures of objects in high-dimensional voxel space.\nExtensive experiments on large synthetic datasets show that the proposed\n3D-RecGAN significantly outperforms the state of the art in single view 3D\nobject reconstruction, and is able to reconstruct unseen types of objects. Our\ncode and data are available at: https://github.com/Yang7879/3D-RecGAN.\n",
        "published": "2017",
        "authors": [
            "Bo Yang",
            "Hongkai Wen",
            "Sen Wang",
            "Ronald Clark",
            "Andrew Markham",
            "Niki Trigoni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.06422v2",
        "title": "Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from\n  Simulation",
        "abstract": "  Learning-based approaches to robotic manipulation are limited by the\nscalability of data collection and accessibility of labels. In this paper, we\npresent a multi-task domain adaptation framework for instance grasping in\ncluttered scenes by utilizing simulated robot experiments. Our neural network\ntakes monocular RGB images and the instance segmentation mask of a specified\ntarget object as inputs, and predicts the probability of successfully grasping\nthe specified object for each candidate motor command. The proposed transfer\nlearning framework trains a model for instance grasping in simulation and uses\na domain-adversarial loss to transfer the trained model to real robots using\nindiscriminate grasping data, which is available both in simulation and the\nreal world. We evaluate our model in real-world robot experiments, comparing it\nwith alternative model architectures as well as an indiscriminate grasping\nbaseline.\n",
        "published": "2017",
        "authors": [
            "Kuan Fang",
            "Yunfei Bai",
            "Stefan Hinterstoisser",
            "Silvio Savarese",
            "Mrinal Kalakrishnan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.01523v1",
        "title": "Stochastic Adversarial Video Prediction",
        "abstract": "  Being able to predict what may happen in the future requires an in-depth\nunderstanding of the physical and causal rules that govern the world. A model\nthat is able to do so has a number of appealing applications, from robotic\nplanning to representation learning. However, learning to predict raw future\nobservations, such as frames in a video, is exceedingly challenging -- the\nambiguous nature of the problem can cause a naively designed model to average\ntogether possible futures into a single, blurry prediction. Recently, this has\nbeen addressed by two distinct approaches: (a) latent variational variable\nmodels that explicitly model underlying stochasticity and (b)\nadversarially-trained models that aim to produce naturalistic images. However,\na standard latent variable model can struggle to produce realistic results, and\na standard adversarially-trained model underutilizes latent variables and fails\nto produce diverse predictions. We show that these distinct methods are in fact\ncomplementary. Combining the two produces predictions that look more realistic\nto human raters and better cover the range of possible futures. Our method\noutperforms prior and concurrent work in these aspects.\n",
        "published": "2018",
        "authors": [
            "Alex X. Lee",
            "Richard Zhang",
            "Frederik Ebert",
            "Pieter Abbeel",
            "Chelsea Finn",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.07193v2",
        "title": "The EuroCity Persons Dataset: A Novel Benchmark for Object Detection",
        "abstract": "  Big data has had a great share in the success of deep learning in computer\nvision. Recent works suggest that there is significant further potential to\nincrease object detection performance by utilizing even bigger datasets. In\nthis paper, we introduce the EuroCity Persons dataset, which provides a large\nnumber of highly diverse, accurate and detailed annotations of pedestrians,\ncyclists and other riders in urban traffic scenes. The images for this dataset\nwere collected on-board a moving vehicle in 31 cities of 12 European countries.\nWith over 238200 person instances manually labeled in over 47300 images,\nEuroCity Persons is nearly one order of magnitude larger than person datasets\nused previously for benchmarking. The dataset furthermore contains a large\nnumber of person orientation annotations (over 211200). We optimize four\nstate-of-the-art deep learning approaches (Faster R-CNN, R-FCN, SSD and YOLOv3)\nto serve as baselines for the new object detection benchmark. In experiments\nwith previous datasets we analyze the generalization capabilities of these\ndetectors when trained with the new dataset. We furthermore study the effect of\nthe training set size, the dataset diversity (day- vs. night-time, geographical\nregion), the dataset detail (i.e. availability of object orientation\ninformation) and the annotation quality on the detector performance. Finally,\nwe analyze error sources and discuss the road ahead.\n",
        "published": "2018",
        "authors": [
            "Markus Braun",
            "Sebastian Krebs",
            "Fabian Flohr",
            "Dariu M. Gavrila"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.00758v2",
        "title": "Robust Attentional Aggregation of Deep Feature Sets for Multi-view 3D\n  Reconstruction",
        "abstract": "  We study the problem of recovering an underlying 3D shape from a set of\nimages. Existing learning based approaches usually resort to recurrent neural\nnets, e.g., GRU, or intuitive pooling operations, e.g., max/mean poolings, to\nfuse multiple deep features encoded from input images. However, GRU based\napproaches are unable to consistently estimate 3D shapes given different\npermutations of the same set of input images as the recurrent unit is\npermutation variant. It is also unlikely to refine the 3D shape given more\nimages due to the long-term memory loss of GRU. Commonly used pooling\napproaches are limited to capturing partial information, e.g., max/mean values,\nignoring other valuable features. In this paper, we present a new feed-forward\nneural module, named AttSets, together with a dedicated training algorithm,\nnamed FASet, to attentively aggregate an arbitrarily sized deep feature set for\nmulti-view 3D reconstruction. The AttSets module is permutation invariant,\ncomputationally efficient and flexible to implement, while the FASet algorithm\nenables the AttSets based network to be remarkably robust and generalize to an\narbitrary number of input images. We thoroughly evaluate FASet and the\nproperties of AttSets on multiple large public datasets. Extensive experiments\nshow that AttSets together with FASet algorithm significantly outperforms\nexisting aggregation approaches.\n",
        "published": "2018",
        "authors": [
            "Bo Yang",
            "Sen Wang",
            "Andrew Markham",
            "Niki Trigoni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.00912v1",
        "title": "Visual Curiosity: Learning to Ask Questions to Learn Visual Recognition",
        "abstract": "  In an open-world setting, it is inevitable that an intelligent agent (e.g., a\nrobot) will encounter visual objects, attributes or relationships it does not\nrecognize. In this work, we develop an agent empowered with visual curiosity,\ni.e. the ability to ask questions to an Oracle (e.g., human) about the contents\nin images (e.g., What is the object on the left side of the red cube?) and\nbuild visual recognition model based on the answers received (e.g., Cylinder).\nIn order to do this, the agent must (1) understand what it recognizes and what\nit does not, (2) formulate a valid, unambiguous and informative language query\n(a question) to ask the Oracle, (3) derive the parameters of visual classifiers\nfrom the Oracle response and (4) leverage the updated visual classifiers to ask\nmore clarified questions. Specifically, we propose a novel framework and\nformulate the learning of visual curiosity as a reinforcement learning problem.\nIn this framework, all components of our agent, visual recognition module (to\nsee), question generation policy (to ask), answer digestion module (to\nunderstand) and graph memory module (to memorize), are learned entirely\nend-to-end to maximize the reward derived from the scene graph obtained by the\nagent as a consequence of the dialog with the Oracle. Importantly, the question\ngeneration policy is disentangled from the visual recognition system and\nspecifics of the environment. Consequently, we demonstrate a sort of double\ngeneralization. Our question generation policy generalizes to new environments\nand a new pair of eyes, i.e., new visual system. Trained on a synthetic\ndataset, our results show that our agent learns new visual concepts\nsignificantly faster than several heuristic baselines, even when tested on\nsynthetic environments with novel objects, as well as in a realistic\nenvironment.\n",
        "published": "2018",
        "authors": [
            "Jianwei Yang",
            "Jiasen Lu",
            "Stefan Lee",
            "Dhruv Batra",
            "Devi Parikh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.03237v1",
        "title": "Task-Embedded Control Networks for Few-Shot Imitation Learning",
        "abstract": "  Much like humans, robots should have the ability to leverage knowledge from\npreviously learned tasks in order to learn new tasks quickly in new and\nunfamiliar environments. Despite this, most robot learning approaches have\nfocused on learning a single task, from scratch, with a limited notion of\ngeneralisation, and no way of leveraging the knowledge to learn other tasks\nmore efficiently. One possible solution is meta-learning, but many of the\nrelated approaches are limited in their ability to scale to a large number of\ntasks and to learn further tasks without forgetting previously learned ones.\nWith this in mind, we introduce Task-Embedded Control Networks, which employ\nideas from metric learning in order to create a task embedding that can be used\nby a robot to learn new tasks from one or more demonstrations. In the area of\nvisually-guided manipulation, we present simulation results in which we surpass\nthe performance of a state-of-the-art method when using only visual information\nfrom each demonstration. Additionally, we demonstrate that our approach can\nalso be used in conjunction with domain randomisation to train our few-shot\nlearning ability in simulation and then deploy in the real world without any\nadditional training. Once deployed, the robot can learn new tasks from a single\nreal-world demonstration.\n",
        "published": "2018",
        "authors": [
            "Stephen James",
            "Michael Bloesch",
            "Andrew J. Davison"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.04871v1",
        "title": "A Data-Efficient Framework for Training and Sim-to-Real Transfer of\n  Navigation Policies",
        "abstract": "  Learning effective visuomotor policies for robots purely from data is\nchallenging, but also appealing since a learning-based system should not\nrequire manual tuning or calibration. In the case of a robot operating in a\nreal environment the training process can be costly, time-consuming, and even\ndangerous since failures are common at the start of training. For this reason,\nit is desirable to be able to leverage \\textit{simulation} and\n\\textit{off-policy} data to the extent possible to train the robot. In this\nwork, we introduce a robust framework that plans in simulation and transfers\nwell to the real environment. Our model incorporates a gradient-descent based\nplanning module, which, given the initial image and goal image, encodes the\nimages to a lower dimensional latent state and plans a trajectory to reach the\ngoal. The model, consisting of the encoder and planner modules, is trained\nthrough a meta-learning strategy in simulation first. We subsequently perform\nadversarial domain transfer on the encoder by using a bank of unlabelled but\nrandom images from the simulation and real environments to enable the encoder\nto map images from the real and simulated environments to a similarly\ndistributed latent representation. By fine tuning the entire model (encoder +\nplanner) with far fewer real world expert demonstrations, we show successful\nplanning performances in different navigation tasks.\n",
        "published": "2018",
        "authors": [
            "Homanga Bharadhwaj",
            "Zihan Wang",
            "Yoshua Bengio",
            "Liam Paull"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.05017v1",
        "title": "One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL",
        "abstract": "  Humans are experts at high-fidelity imitation -- closely mimicking a\ndemonstration, often in one attempt. Humans use this ability to quickly solve a\ntask instance, and to bootstrap learning of new tasks. Achieving these\nabilities in autonomous agents is an open problem. In this paper, we introduce\nan off-policy RL algorithm (MetaMimic) to narrow this gap. MetaMimic can learn\nboth (i) policies for high-fidelity one-shot imitation of diverse novel skills,\nand (ii) policies that enable the agent to solve tasks more efficiently than\nthe demonstrators. MetaMimic relies on the principle of storing all experiences\nin a memory and replaying these to learn massive deep neural network policies\nby off-policy RL. This paper introduces, to the best of our knowledge, the\nlargest existing neural networks for deep RL and shows that larger networks\nwith normalization are needed to achieve one-shot high-fidelity imitation on a\nchallenging manipulation task. The results also show that both types of policy\ncan be learned from vision, in spite of the task rewards being sparse, and\nwithout access to demonstrator actions.\n",
        "published": "2018",
        "authors": [
            "Tom Le Paine",
            "Sergio G\u00f3mez Colmenarejo",
            "Ziyu Wang",
            "Scott Reed",
            "Yusuf Aytar",
            "Tobias Pfaff",
            "Matt W. Hoffman",
            "Gabriel Barth-Maron",
            "Serkan Cabi",
            "David Budden",
            "Nando de Freitas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.02266v3",
        "title": "Continuous Direct Sparse Visual Odometry from RGB-D Images",
        "abstract": "  This paper reports on a novel formulation and evaluation of visual odometry\nfrom RGB-D images. Assuming a static scene, the developed theoretical framework\ngeneralizes the widely used direct energy formulation (photometric error\nminimization) technique for obtaining a rigid body transformation that aligns\ntwo overlapping RGB-D images to a continuous formulation. The continuity is\nachieved through functional treatment of the problem and representing the\nprocess models over RGB-D images in a reproducing kernel Hilbert space;\nconsequently, the registration is not limited to the specific image resolution\nand the framework is fully analytical with a closed-form derivation of the\ngradient. We solve the problem by maximizing the inner product between two\nfunctions defined over RGB-D images, while the continuous action of the rigid\nbody motion Lie group is captured through the integration of the flow in the\ncorresponding Lie algebra. Energy-based approaches have been extremely\nsuccessful and the developed framework in this paper shares many of their\ndesired properties such as the parallel structure on both CPUs and GPUs,\nsparsity, semi-dense tracking, avoiding explicit data association which is\ncomputationally expensive, and possible extensions to the simultaneous\nlocalization and mapping frameworks. The evaluations on experimental data and\ncomparison with the equivalent energy-based formulation of the problem confirm\nthe effectiveness of the proposed technique, especially, when the lack of\nstructure and texture in the environment is evident.\n",
        "published": "2019",
        "authors": [
            "Maani Ghaffari",
            "William Clark",
            "Anthony Bloch",
            "Ryan M. Eustice",
            "Jessy W. Grizzle"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.04281v1",
        "title": "3D Local Features for Direct Pairwise Registration",
        "abstract": "  We present a novel, data driven approach for solving the problem of\nregistration of two point cloud scans. Our approach is direct in the sense that\na single pair of corresponding local patches already provides the necessary\ntransformation cue for the global registration. To achieve that, we first endow\nthe state of the art PPF-FoldNet auto-encoder (AE) with a pose-variant sibling,\nwhere the discrepancy between the two leads to pose-specific descriptors. Based\nupon this, we introduce RelativeNet, a relative pose estimation network to\nassign correspondence-specific orientations to the keypoints, eliminating any\nlocal reference frame computations. Finally, we devise a simple yet effective\nhypothesize-and-verify algorithm to quickly use the predictions and align two\npoint sets. Our extensive quantitative and qualitative experiments suggests\nthat our approach outperforms the state of the art in challenging real datasets\nof pairwise registration and that augmenting the keypoints with local pose\ninformation leads to better generalization and a dramatic speed-up.\n",
        "published": "2019",
        "authors": [
            "Haowen Deng",
            "Tolga Birdal",
            "Slobodan Ilic"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.04404v1",
        "title": "Embodied Visual Recognition",
        "abstract": "  Passive visual systems typically fail to recognize objects in the amodal\nsetting where they are heavily occluded. In contrast, humans and other embodied\nagents have the ability to move in the environment, and actively control the\nviewing angle to better understand object shapes and semantics. In this work,\nwe introduce the task of Embodied Visual Recognition (EVR): An agent is\ninstantiated in a 3D environment close to an occluded target object, and is\nfree to move in the environment to perform object classification, amodal object\nlocalization, and amodal object segmentation. To address this, we develop a new\nmodel called Embodied Mask R-CNN, for agents to learn to move strategically to\nimprove their visual recognition abilities. We conduct experiments using the\nHouse3D environment. Experimental results show that: 1) agents with embodiment\n(movement) achieve better visual recognition performance than passive ones; 2)\nin order to improve visual recognition abilities, agents can learn strategical\nmoving paths that are different from shortest paths.\n",
        "published": "2019",
        "authors": [
            "Jianwei Yang",
            "Zhile Ren",
            "Mingze Xu",
            "Xinlei Chen",
            "David Crandall",
            "Devi Parikh",
            "Dhruv Batra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.08159v2",
        "title": "3D Object Recognition with Ensemble Learning --- A Study of Point\n  Cloud-Based Deep Learning Models",
        "abstract": "  In this study, we present an analysis of model-based ensemble learning for 3D\npoint-cloud object classification and detection. An ensemble of multiple model\ninstances is known to outperform a single model instance, but there is little\nstudy of the topic of ensemble learning for 3D point clouds. First, an ensemble\nof multiple model instances trained on the same part of the\n$\\textit{ModelNet40}$ dataset was tested for seven deep learning, point\ncloud-based classification algorithms: $\\textit{PointNet}$,\n$\\textit{PointNet++}$, $\\textit{SO-Net}$, $\\textit{KCNet}$,\n$\\textit{DeepSets}$, $\\textit{DGCNN}$, and $\\textit{PointCNN}$. Second, the\nensemble of different architectures was tested. Results of our experiments show\nthat the tested ensemble learning methods improve over state-of-the-art on the\n$\\textit{ModelNet40}$ dataset, from $92.65\\%$ to $93.64\\%$ for the ensemble of\nsingle architecture instances, $94.03\\%$ for two different architectures, and\n$94.15\\%$ for five different architectures. We show that the ensemble of two\nmodels with different architectures can be as effective as the ensemble of 10\nmodels with the same architecture. Third, a study on classic bagging i.e. with\ndifferent subsets used for training multiple model instances) was tested and\nsources of ensemble accuracy growth were investigated for best-performing\narchitecture, i.e. $\\textit{SO-Net}$. We also investigate the ensemble learning\nof $\\textit{Frustum PointNet}$ approach in the task of 3D object detection,\nincreasing the average precision of 3D box detection on the $\\textit{KITTI}$\ndataset from $63.1\\%$ to $66.5\\%$ using only three model instances. We measure\nthe inference time of all 3D classification architectures on a $\\textit{Nvidia\nJetson TX2}$, a common embedded computer for mobile robots, to allude to the\nuse of these models in real-life applications.\n",
        "published": "2019",
        "authors": [
            "Daniel Koguciuk",
            "\u0141ukasz Chechli\u0144ski",
            "Tarek El-Gaaly"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.09503v2",
        "title": "Model-free Deep Reinforcement Learning for Urban Autonomous Driving",
        "abstract": "  Urban autonomous driving decision making is challenging due to complex road\ngeometry and multi-agent interactions. Current decision making methods are\nmostly manually designing the driving policy, which might result in sub-optimal\nsolutions and is expensive to develop, generalize and maintain at scale. On the\nother hand, with reinforcement learning (RL), a policy can be learned and\nimproved automatically without any manual designs. However, current RL methods\ngenerally do not work well on complex urban scenarios. In this paper, we\npropose a framework to enable model-free deep reinforcement learning in\nchallenging urban autonomous driving scenarios. We design a specific input\nrepresentation and use visual encoding to capture the low-dimensional latent\nstates. Several state-of-the-art model-free deep RL algorithms are implemented\ninto our framework, with several tricks to improve their performance. We\nevaluate our method in a challenging roundabout task with dense surrounding\nvehicles in a high-definition driving simulator. The result shows that our\nmethod can solve the task well and is significantly better than the baseline.\n",
        "published": "2019",
        "authors": [
            "Jianyu Chen",
            "Bodi Yuan",
            "Masayoshi Tomizuka"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.02843v1",
        "title": "FANTrack: 3D Multi-Object Tracking with Feature Association Network",
        "abstract": "  We propose a data-driven approach to online multi-object tracking (MOT) that\nuses a convolutional neural network (CNN) for data association in a\ntracking-by-detection framework. The problem of multi-target tracking aims to\nassign noisy detections to a-priori unknown and time-varying number of tracked\nobjects across a sequence of frames. A majority of the existing solutions focus\non either tediously designing cost functions or formulating the task of data\nassociation as a complex optimization problem that can be solved effectively.\nInstead, we exploit the power of deep learning to formulate the data\nassociation problem as inference in a CNN. To this end, we propose to learn a\nsimilarity function that combines cues from both image and spatial features of\nobjects. Our solution learns to perform global assignments in 3D purely from\ndata, handles noisy detections and a varying number of targets, and is easy to\ntrain. We evaluate our approach on the challenging KITTI dataset and show\ncompetitive results. Our code is available at\nhttps://git.uwaterloo.ca/wise-lab/fantrack.\n",
        "published": "2019",
        "authors": [
            "Erkan Baser",
            "Venkateshwaran Balasubramanian",
            "Prarthana Bhattacharyya",
            "Krzysztof Czarnecki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.12612v2",
        "title": "Learning Navigation Subroutines from Egocentric Videos",
        "abstract": "  Planning at a higher level of abstraction instead of low level torques\nimproves the sample efficiency in reinforcement learning, and computational\nefficiency in classical planning. We propose a method to learn such\nhierarchical abstractions, or subroutines from egocentric video data of experts\nperforming tasks. We learn a self-supervised inverse model on small amounts of\nrandom interaction data to pseudo-label the expert egocentric videos with agent\nactions. Visuomotor subroutines are acquired from these pseudo-labeled videos\nby learning a latent intent-conditioned policy that predicts the inferred\npseudo-actions from the corresponding image observations. We demonstrate our\nproposed approach in context of navigation, and show that we can successfully\nlearn consistent and diverse visuomotor subroutines from passive egocentric\nvideos. We demonstrate the utility of our acquired visuomotor subroutines by\nusing them as is for exploration, and as sub-policies in a hierarchical RL\nframework for reaching point goals and semantic goals. We also demonstrate\nbehavior of our subroutines in the real world, by deploying them on a real\nrobotic platform. Project website:\nhttps://ashishkumar1993.github.io/subroutines/.\n",
        "published": "2019",
        "authors": [
            "Ashish Kumar",
            "Saurabh Gupta",
            "Jitendra Malik"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.12887v2",
        "title": "Does computer vision matter for action?",
        "abstract": "  Computer vision produces representations of scene content. Much computer\nvision research is predicated on the assumption that these intermediate\nrepresentations are useful for action. Recent work at the intersection of\nmachine learning and robotics calls this assumption into question by training\nsensorimotor systems directly for the task at hand, from pixels to actions,\nwith no explicit intermediate representations. Thus the central question of our\nwork: Does computer vision matter for action? We probe this question and its\noffshoots via immersive simulation, which allows us to conduct controlled\nreproducible experiments at scale. We instrument immersive three-dimensional\nenvironments to simulate challenges such as urban driving, off-road trail\ntraversal, and battle. Our main finding is that computer vision does matter.\nModels equipped with intermediate representations train faster, achieve higher\ntask performance, and generalize better to previously unseen environments. A\nvideo that summarizes the work and illustrates the results can be found at\nhttps://youtu.be/4MfWa2yZ0Jc\n",
        "published": "2019",
        "authors": [
            "Brady Zhou",
            "Philipp Kr\u00e4henb\u00fchl",
            "Vladlen Koltun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.00208v2",
        "title": "RGB and LiDAR fusion based 3D Semantic Segmentation for Autonomous\n  Driving",
        "abstract": "  LiDAR has become a standard sensor for autonomous driving applications as\nthey provide highly precise 3D point clouds. LiDAR is also robust for low-light\nscenarios at night-time or due to shadows where the performance of cameras is\ndegraded. LiDAR perception is gradually becoming mature for algorithms\nincluding object detection and SLAM. However, semantic segmentation algorithm\nremains to be relatively less explored. Motivated by the fact that semantic\nsegmentation is a mature algorithm on image data, we explore sensor fusion\nbased 3D segmentation. Our main contribution is to convert the RGB image to a\npolar-grid mapping representation used for LiDAR and design early and mid-level\nfusion architectures. Additionally, we design a hybrid fusion architecture that\ncombines both fusion algorithms. We evaluate our algorithm on KITTI dataset\nwhich provides segmentation annotation for cars, pedestrians and cyclists. We\nevaluate two state-of-the-art architectures namely SqueezeSeg and PointSeg and\nimprove the mIoU score by 10 % in both cases relative to the LiDAR only\nbaseline.\n",
        "published": "2019",
        "authors": [
            "Khaled El Madawy",
            "Hazem Rashed",
            "Ahmad El Sallab",
            "Omar Nasr",
            "Hanan Kamel",
            "Senthil Yogamani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.01140v2",
        "title": "Learning Object Bounding Boxes for 3D Instance Segmentation on Point\n  Clouds",
        "abstract": "  We propose a novel, conceptually simple and general framework for instance\nsegmentation on 3D point clouds. Our method, called 3D-BoNet, follows the\nsimple design philosophy of per-point multilayer perceptrons (MLPs). The\nframework directly regresses 3D bounding boxes for all instances in a point\ncloud, while simultaneously predicting a point-level mask for each instance. It\nconsists of a backbone network followed by two parallel network branches for 1)\nbounding box regression and 2) point mask prediction. 3D-BoNet is single-stage,\nanchor-free and end-to-end trainable. Moreover, it is remarkably\ncomputationally efficient as, unlike existing approaches, it does not require\nany post-processing steps such as non-maximum suppression, feature sampling,\nclustering or voting. Extensive experiments show that our approach surpasses\nexisting work on both ScanNet and S3DIS datasets while being approximately 10x\nmore computationally efficient. Comprehensive ablation studies demonstrate the\neffectiveness of our design.\n",
        "published": "2019",
        "authors": [
            "Bo Yang",
            "Jianan Wang",
            "Ronald Clark",
            "Qingyong Hu",
            "Sen Wang",
            "Andrew Markham",
            "Niki Trigoni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.03853v2",
        "title": "DensePhysNet: Learning Dense Physical Object Representations via\n  Multi-step Dynamic Interactions",
        "abstract": "  We study the problem of learning physical object representations for robot\nmanipulation. Understanding object physics is critical for successful object\nmanipulation, but also challenging because physical object properties can\nrarely be inferred from the object's static appearance. In this paper, we\npropose DensePhysNet, a system that actively executes a sequence of dynamic\ninteractions (e.g., sliding and colliding), and uses a deep predictive model\nover its visual observations to learn dense, pixel-wise representations that\nreflect the physical properties of observed objects. Our experiments in both\nsimulation and real settings demonstrate that the learned representations carry\nrich physical information, and can directly be used to decode physical object\nproperties such as friction and mass. The use of dense representation enables\nDensePhysNet to generalize well to novel scenes with more objects than in\ntraining. With knowledge of object physics, the learned representation also\nleads to more accurate and efficient manipulation in downstream tasks than the\nstate-of-the-art.\n",
        "published": "2019",
        "authors": [
            "Zhenjia Xu",
            "Jiajun Wu",
            "Andy Zeng",
            "Joshua B. Tenenbaum",
            "Shuran Song"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.08236v1",
        "title": "PyRobot: An Open-source Robotics Framework for Research and Benchmarking",
        "abstract": "  This paper introduces PyRobot, an open-source robotics framework for research\nand benchmarking. PyRobot is a light-weight, high-level interface on top of ROS\nthat provides a consistent set of hardware independent mid-level APIs to\ncontrol different robots. PyRobot abstracts away details about low-level\ncontrollers and inter-process communication, and allows non-robotics\nresearchers (ML, CV researchers) to focus on building high-level AI\napplications. PyRobot aims to provide a research ecosystem with convenient\naccess to robotics datasets, algorithm implementations and models that can be\nused to quickly create a state-of-the-art baseline. We believe PyRobot, when\npaired up with low-cost robot platforms such as LoCoBot, will reduce the entry\nbarrier into robotics, and democratize robotics. PyRobot is open-source, and\ncan be accessed via https://pyrobot.org.\n",
        "published": "2019",
        "authors": [
            "Adithyavairavan Murali",
            "Tao Chen",
            "Kalyan Vasudev Alwala",
            "Dhiraj Gandhi",
            "Lerrel Pinto",
            "Saurabh Gupta",
            "Abhinav Gupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.09295v2",
        "title": "Learning Your Way Without Map or Compass: Panoramic Target Driven Visual\n  Navigation",
        "abstract": "  We present a robot navigation system that uses an imitation learning\nframework to successfully navigate in complex environments. Our framework takes\na pre-built 3D scan of a real environment and trains an agent from\npre-generated expert trajectories to navigate to any position given a panoramic\nview of the goal and the current visual input without relying on map, compass,\nodometry, or relative position of the target at runtime. Our end-to-end trained\nagent uses RGB and depth (RGBD) information and can handle large environments\n(up to $1031m^2$) across multiple rooms (up to $40$) and generalizes to unseen\ntargets. We show that when compared to several baselines our method (1)\nrequires fewer training examples and less training time, (2) reaches the goal\nlocation with higher accuracy, and (3) produces better solutions with shorter\npaths for long-range navigation tasks.\n",
        "published": "2019",
        "authors": [
            "David Watkins-Valls",
            "Jingxi Xu",
            "Nicholas Waytowich",
            "Peter Allen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.10304v2",
        "title": "Where to Look Next: Unsupervised Active Visual Exploration on 360\u00b0\n  Input",
        "abstract": "  We address the problem of active visual exploration of large 360{\\deg}\ninputs. In our setting an active agent with a limited camera bandwidth explores\nits 360{\\deg} environment by changing its viewing direction at limited discrete\ntime steps. As such, it observes the world as a sequence of narrow\nfield-of-view 'glimpses', deciding for itself where to look next. Our proposed\nmethod exceeds previous works' performance by a significant margin without the\nneed for deep reinforcement learning or training separate networks as\nsidekicks. A key component of our system are the spatial memory maps that make\nthe system aware of the glimpses' orientations (locations in the 360{\\deg}\nimage). Further, we stress the advantages of retina-like glimpses when the\nagent's sensor bandwidth and time-steps are limited. Finally, we use our\ntrained model to do classification of the whole scene using only the\ninformation observed in the glimpses.\n",
        "published": "2019",
        "authors": [
            "Soroush Seifi",
            "Tinne Tuytelaars"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.10312v2",
        "title": "How to improve CNN-based 6-DoF camera pose estimation",
        "abstract": "  Convolutional neural networks (CNNs) and transfer learning have recently been\nused for 6 degrees of freedom (6-DoF) camera pose estimation. While they do not\nreach the same accuracy as visual SLAM-based approaches and are restricted to a\nspecific environment, they excel in robustness and can be applied even to a\nsingle image. In this paper, we study PoseNet [1] and investigate modifications\nbased on datasets' characteristics to improve the accuracy of the pose\nestimates. In particular, we emphasize the importance of field-of-view over\nimage resolution; we present a data augmentation scheme to reduce overfitting;\nwe study the effect of Long-Short-Term-Memory (LSTM) cells. Lastly, we combine\nthese modifications and improve PoseNet's performance for monocular CNN based\ncamera pose regression.\n",
        "published": "2019",
        "authors": [
            "Soroush Seifi",
            "Tinne Tuytelaars"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.11730v4",
        "title": "\"Good Robot!\": Efficient Reinforcement Learning for Multi-Step Visual\n  Tasks with Sim to Real Transfer",
        "abstract": "  Current Reinforcement Learning (RL) algorithms struggle with long-horizon\ntasks where time can be wasted exploring dead ends and task progress may be\neasily reversed. We develop the SPOT framework, which explores within action\nsafety zones, learns about unsafe regions without exploring them, and\nprioritizes experiences that reverse earlier progress to learn with remarkable\nefficiency.\n  The SPOT framework successfully completes simulated trials of a variety of\ntasks, improving a baseline trial success rate from 13% to 100% when stacking 4\ncubes, from 13% to 99% when creating rows of 4 cubes, and from 84% to 95% when\nclearing toys arranged in adversarial patterns. Efficiency with respect to\nactions per trial typically improves by 30% or more, while training takes just\n1-20k actions, depending on the task.\n  Furthermore, we demonstrate direct sim to real transfer. We are able to\ncreate real stacks in 100% of trials with 61% efficiency and real rows in 100%\nof trials with 59% efficiency by directly loading the simulation-trained model\non the real robot with no additional real-world fine-tuning. To our knowledge,\nthis is the first instance of reinforcement learning with successful sim to\nreal transfer applied to long term multi-step tasks such as block-stacking and\nrow-making with consideration of progress reversal. Code is available at\nhttps://github.com/jhu-lcsr/good_robot .\n",
        "published": "2019",
        "authors": [
            "Andrew Hundt",
            "Benjamin Killeen",
            "Nicholas Greene",
            "Hongtao Wu",
            "Heeyeon Kwon",
            "Chris Paxton",
            "Gregory D. Hager"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.12271v1",
        "title": "RLBench: The Robot Learning Benchmark & Learning Environment",
        "abstract": "  We present a challenging new benchmark and learning-environment for robot\nlearning: RLBench. The benchmark features 100 completely unique, hand-designed\ntasks ranging in difficulty, from simple target reaching and door opening, to\nlonger multi-stage tasks, such as opening an oven and placing a tray in it. We\nprovide an array of both proprioceptive observations and visual observations,\nwhich include rgb, depth, and segmentation masks from an over-the-shoulder\nstereo camera and an eye-in-hand monocular camera. Uniquely, each task comes\nwith an infinite supply of demos through the use of motion planners operating\non a series of waypoints given during task creation time; enabling an exciting\nflurry of demonstration-based learning. RLBench has been designed with\nscalability in mind; new tasks, along with their motion-planned demos, can be\neasily created and then verified by a series of tools, allowing users to submit\ntheir own tasks to the RLBench task repository. This large-scale benchmark aims\nto accelerate progress in a number of vision-guided manipulation research\nareas, including: reinforcement learning, imitation learning, multi-task\nlearning, geometric computer vision, and in particular, few-shot learning. With\nthe benchmark's breadth of tasks and demonstrations, we propose the first\nlarge-scale few-shot challenge in robotics. We hope that the scale and\ndiversity of RLBench offers unparalleled research opportunities in the robot\nlearning community and beyond.\n",
        "published": "2019",
        "authors": [
            "Stephen James",
            "Zicong Ma",
            "David Rovick Arrojo",
            "Andrew J. Davison"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.01103v1",
        "title": "Learning One-Shot Imitation from Humans without Humans",
        "abstract": "  Humans can naturally learn to execute a new task by seeing it performed by\nother individuals once, and then reproduce it in a variety of configurations.\nEndowing robots with this ability of imitating humans from third person is a\nvery immediate and natural way of teaching new tasks. Only recently, through\nmeta-learning, there have been successful attempts to one-shot imitation\nlearning from humans; however, these approaches require a lot of human\nresources to collect the data in the real world to train the robot. But is\nthere a way to remove the need for real world human demonstrations during\ntraining? We show that with Task-Embedded Control Networks, we can infer\ncontrol polices by embedding human demonstrations that can condition a control\npolicy and achieve one-shot imitation learning. Importantly, we do not use a\nreal human arm to supply demonstrations during training, but instead leverage\ndomain randomisation in an application that has not been seen before:\nsim-to-real transfer on humans. Upon evaluating our approach on pushing and\nplacing tasks in both simulation and in the real world, we show that in\ncomparison to a system that was trained on real-world data we are able to\nachieve similar results by utilising only simulation data.\n",
        "published": "2019",
        "authors": [
            "Alessandro Bonardi",
            "Stephen James",
            "Andrew J. Davison"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.07246v1",
        "title": "IKEA Furniture Assembly Environment for Long-Horizon Complex\n  Manipulation Tasks",
        "abstract": "  The IKEA Furniture Assembly Environment is one of the first benchmarks for\ntesting and accelerating the automation of complex manipulation tasks. The\nenvironment is designed to advance reinforcement learning from simple toy tasks\nto complex tasks requiring both long-term planning and sophisticated low-level\ncontrol. Our environment supports over 80 different furniture models, Sawyer\nand Baxter robot simulation, and domain randomization. The IKEA Furniture\nAssembly Environment is a testbed for methods aiming to solve complex\nmanipulation tasks. The environment is publicly available at\nhttps://clvrai.com/furniture\n",
        "published": "2019",
        "authors": [
            "Youngwoon Lee",
            "Edward S. Hu",
            "Zhengyu Yang",
            "Alex Yin",
            "Joseph J. Lim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.12256v2",
        "title": "Neural Topological SLAM for Visual Navigation",
        "abstract": "  This paper studies the problem of image-goal navigation which involves\nnavigating to the location indicated by a goal image in a novel previously\nunseen environment. To tackle this problem, we design topological\nrepresentations for space that effectively leverage semantics and afford\napproximate geometric reasoning. At the heart of our representations are nodes\nwith associated semantic features, that are interconnected using coarse\ngeometric information. We describe supervised learning-based algorithms that\ncan build, maintain and use such representations under noisy actuation.\nExperimental study in visually and physically realistic simulation suggests\nthat our method builds effective representations that capture structural\nregularities and efficiently solve long-horizon navigation problems. We observe\na relative improvement of more than 50% over existing methods that study this\ntask.\n",
        "published": "2020",
        "authors": [
            "Devendra Singh Chaplot",
            "Ruslan Salakhutdinov",
            "Abhinav Gupta",
            "Saurabh Gupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.09093v1",
        "title": "Holistic Grid Fusion Based Stop Line Estimation",
        "abstract": "  Intersection scenarios provide the most complex traffic situations in\nAutonomous Driving and Driving Assistance Systems. Knowing where to stop in\nadvance in an intersection is an essential parameter in controlling the\nlongitudinal velocity of the vehicle. Most of the existing methods in\nliterature solely use cameras to detect stop lines, which is typically not\nsufficient in terms of detection range. To address this issue, we propose a\nmethod that takes advantage of fused multi-sensory data including stereo camera\nand lidar as input and utilizes a carefully designed convolutional neural\nnetwork architecture to detect stop lines. Our experiments show that the\nproposed approach can improve detection range compared to camera data alone,\nworks under heavy occlusion without observing the ground markings explicitly,\nis able to predict stop lines for all lanes and allows detection at a distance\nup to 50 meters.\n",
        "published": "2020",
        "authors": [
            "Runsheng Xu",
            "Faezeh Tafazzoli",
            "Li Zhang",
            "Timo Rehfeld",
            "Gunther Krehl",
            "Arunava Seal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.11044v2",
        "title": "Unsupervised Feature Learning for Event Data: Direct vs Inverse Problem\n  Formulation",
        "abstract": "  Event-based cameras record an asynchronous stream of per-pixel brightness\nchanges. As such, they have numerous advantages over the standard frame-based\ncameras, including high temporal resolution, high dynamic range, and no motion\nblur. Due to the asynchronous nature, efficient learning of compact\nrepresentation for event data is challenging. While it remains not explored the\nextent to which the spatial and temporal event \"information\" is useful for\npattern recognition tasks. In this paper, we focus on single-layer\narchitectures. We analyze the performance of two general problem formulations:\nthe direct and the inverse, for unsupervised feature learning from local event\ndata (local volumes of events described in space-time). We identify and show\nthe main advantages of each approach. Theoretically, we analyze guarantees for\nan optimal solution, possibility for asynchronous, parallel parameter update,\nand the computational complexity. We present numerical experiments for object\nrecognition. We evaluate the solution under the direct and the inverse problem\nand give a comparison with the state-of-the-art methods. Our empirical results\nhighlight the advantages of both approaches for representation learning from\nevent data. We show improvements of up to 9 % in the recognition accuracy\ncompared to the state-of-the-art methods from the same class of methods.\n",
        "published": "2020",
        "authors": [
            "Dimche Kostadinov",
            "Davide Scaramuzza"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.01619v2",
        "title": "Relational Graph Learning on Visual and Kinematics Embeddings for\n  Accurate Gesture Recognition in Robotic Surgery",
        "abstract": "  Automatic surgical gesture recognition is fundamentally important to enable\nintelligent cognitive assistance in robotic surgery. With recent advancement in\nrobot-assisted minimally invasive surgery, rich information including surgical\nvideos and robotic kinematics can be recorded, which provide complementary\nknowledge for understanding surgical gestures. However, existing methods either\nsolely adopt uni-modal data or directly concatenate multi-modal\nrepresentations, which can not sufficiently exploit the informative\ncorrelations inherent in visual and kinematics data to boost gesture\nrecognition accuracies. In this regard, we propose a novel online approach of\nmulti-modal relational graph network (i.e., MRG-Net) to dynamically integrate\nvisual and kinematics information through interactive message propagation in\nthe latent feature space. In specific, we first extract embeddings from video\nand kinematics sequences with temporal convolutional networks and LSTM units.\nNext, we identify multi-relations in these multi-modal embeddings and leverage\nthem through a hierarchical relational graph learning module. The effectiveness\nof our method is demonstrated with state-of-the-art results on the public\nJIGSAWS dataset, outperforming current uni-modal and multi-modal methods on\nboth suturing and knot typing tasks. Furthermore, we validated our method on\nin-house visual-kinematics datasets collected with da Vinci Research Kit (dVRK)\nplatforms in two centers, with consistent promising performance achieved.\n",
        "published": "2020",
        "authors": [
            "Yonghao Long",
            "Jie Ying Wu",
            "Bo Lu",
            "Yueming Jin",
            "Mathias Unberath",
            "Yun-Hui Liu",
            "Pheng Ann Heng",
            "Qi Dou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.01975v1",
        "title": "Rearrangement: A Challenge for Embodied AI",
        "abstract": "  We describe a framework for research and evaluation in Embodied AI. Our\nproposal is based on a canonical task: Rearrangement. A standard task can focus\nthe development of new techniques and serve as a source of trained models that\ncan be transferred to other settings. In the rearrangement task, the goal is to\nbring a given physical environment into a specified state. The goal state can\nbe specified by object poses, by images, by a description in language, or by\nletting the agent experience the environment in the goal state. We characterize\nrearrangement scenarios along different axes and describe metrics for\nbenchmarking rearrangement performance. To facilitate research and exploration,\nwe present experimental testbeds of rearrangement scenarios in four different\nsimulation environments. We anticipate that other datasets will be released and\nnew simulation platforms will be built to support training of rearrangement\nagents and their deployment on physical systems.\n",
        "published": "2020",
        "authors": [
            "Dhruv Batra",
            "Angel X. Chang",
            "Sonia Chernova",
            "Andrew J. Davison",
            "Jia Deng",
            "Vladlen Koltun",
            "Sergey Levine",
            "Jitendra Malik",
            "Igor Mordatch",
            "Roozbeh Mottaghi",
            "Manolis Savva",
            "Hao Su"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.06464v1",
        "title": "3D-OES: Viewpoint-Invariant Object-Factorized Environment Simulators",
        "abstract": "  We propose an action-conditioned dynamics model that predicts scene changes\ncaused by object and agent interactions in a viewpoint-invariant 3D neural\nscene representation space, inferred from RGB-D videos. In this 3D feature\nspace, objects do not interfere with one another and their appearance persists\nover time and across viewpoints. This permits our model to predict future\nscenes long in the future by simply \"moving\" 3D object features based on\ncumulative object motion predictions. Object motion predictions are computed by\na graph neural network that operates over the object features extracted from\nthe 3D neural scene representation. Our model's simulations can be decoded by a\nneural renderer into2D image views from any desired viewpoint, which aids the\ninterpretability of our latent 3D simulation space. We show our model\ngeneralizes well its predictions across varying number and appearances of\ninteracting objects as well as across camera viewpoints, outperforming existing\n2D and 3D dynamics models. We further demonstrate sim-to-real transfer of the\nlearnt dynamics by applying our model trained solely in simulation to\nmodel-based control for pushing objects to desired locations under clutter on a\nreal robotic setup\n",
        "published": "2020",
        "authors": [
            "Hsiao-Yu Fish Tung",
            "Zhou Xian",
            "Mihir Prabhudesai",
            "Shamit Lal",
            "Katerina Fragkiadaki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.06498v1",
        "title": "Fit2Form: 3D Generative Model for Robot Gripper Form Design",
        "abstract": "  The 3D shape of a robot's end-effector plays a critical role in determining\nit's functionality and overall performance. Many industrial applications rely\non task-specific gripper designs to ensure the system's robustness and\naccuracy. However, the process of manual hardware design is both costly and\ntime-consuming, and the quality of the resulting design is dependent on the\nengineer's experience and domain expertise, which can easily be out-dated or\ninaccurate. The goal of this work is to use machine learning algorithms to\nautomate the design of task-specific gripper fingers. We propose Fit2Form, a 3D\ngenerative design framework that generates pairs of finger shapes to maximize\ndesign objectives (i.e., grasp success, stability, and robustness) for target\ngrasp objects. We model the design objectives by training a Fitness network to\npredict their values for pairs of gripper fingers and their corresponding grasp\nobjects. This Fitness network then provides supervision to a 3D Generative\nnetwork that produces a pair of 3D finger geometries for the target grasp\nobject. Our experiments demonstrate that the proposed 3D generative design\nframework generates parallel jaw gripper finger shapes that achieve more stable\nand robust grasps compared to other general-purpose and task-specific gripper\ndesign algorithms. Video can be found at https://youtu.be/utKHP3qb1bg.\n",
        "published": "2020",
        "authors": [
            "Huy Ha",
            "Shubham Agrawal",
            "Shuran Song"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.06507v2",
        "title": "Reinforcement Learning with Videos: Combining Offline Observations with\n  Interaction",
        "abstract": "  Reinforcement learning is a powerful framework for robots to acquire skills\nfrom experience, but often requires a substantial amount of online data\ncollection. As a result, it is difficult to collect sufficiently diverse\nexperiences that are needed for robots to generalize broadly. Videos of humans,\non the other hand, are a readily available source of broad and interesting\nexperiences. In this paper, we consider the question: can we perform\nreinforcement learning directly on experience collected by humans? This problem\nis particularly difficult, as such videos are not annotated with actions and\nexhibit substantial visual domain shift relative to the robot's embodiment. To\naddress these challenges, we propose a framework for reinforcement learning\nwith videos (RLV). RLV learns a policy and value function using experience\ncollected by humans in combination with data collected by robots. In our\nexperiments, we find that RLV is able to leverage such videos to learn\nchallenging vision-based skills with less than half as many samples as RL\nmethods that learn from scratch.\n",
        "published": "2020",
        "authors": [
            "Karl Schmeckpeper",
            "Oleh Rybkin",
            "Kostas Daniilidis",
            "Sergey Levine",
            "Chelsea Finn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.07613v1",
        "title": "BirdSLAM: Monocular Multibody SLAM in Bird's-Eye View",
        "abstract": "  In this paper, we present BirdSLAM, a novel simultaneous localization and\nmapping (SLAM) system for the challenging scenario of autonomous driving\nplatforms equipped with only a monocular camera. BirdSLAM tackles challenges\nfaced by other monocular SLAM systems (such as scale ambiguity in monocular\nreconstruction, dynamic object localization, and uncertainty in feature\nrepresentation) by using an orthographic (bird's-eye) view as the configuration\nspace in which localization and mapping are performed. By assuming only the\nheight of the ego-camera above the ground, BirdSLAM leverages single-view\nmetrology cues to accurately localize the ego-vehicle and all other traffic\nparticipants in bird's-eye view. We demonstrate that our system outperforms\nprior work that uses strictly greater information, and highlight the relevance\nof each design decision via an ablation analysis.\n",
        "published": "2020",
        "authors": [
            "Swapnil Daga",
            "Gokul B. Nair",
            "Anirudha Ramesh",
            "Rahul Sajnani",
            "Junaid Ahmed Ansari",
            "K. Madhava Krishna"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.07748v3",
        "title": "Fast Uncertainty Quantification for Deep Object Pose Estimation",
        "abstract": "  Deep learning-based object pose estimators are often unreliable and\noverconfident especially when the input image is outside the training domain,\nfor instance, with sim2real transfer. Efficient and robust uncertainty\nquantification (UQ) in pose estimators is critically needed in many robotic\ntasks. In this work, we propose a simple, efficient, and plug-and-play UQ\nmethod for 6-DoF object pose estimation. We ensemble 2-3 pre-trained models\nwith different neural network architectures and/or training data sources, and\ncompute their average pairwise disagreement against one another to obtain the\nuncertainty quantification. We propose four disagreement metrics, including a\nlearned metric, and show that the average distance (ADD) is the best\nlearning-free metric and it is only slightly worse than the learned metric,\nwhich requires labeled target data. Our method has several advantages compared\nto the prior art: 1) our method does not require any modification of the\ntraining process or the model inputs; and 2) it needs only one forward pass for\neach model. We evaluate the proposed UQ method on three tasks where our\nuncertainty quantification yields much stronger correlations with pose\nestimation errors than the baselines. Moreover, in a real robot grasping task,\nour method increases the grasping success rate from 35% to 90%.\n",
        "published": "2020",
        "authors": [
            "Guanya Shi",
            "Yifeng Zhu",
            "Jonathan Tremblay",
            "Stan Birchfield",
            "Fabio Ramos",
            "Animashree Anandkumar",
            "Yuke Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.08518v1",
        "title": "DeepSeqSLAM: A Trainable CNN+RNN for Joint Global Description and\n  Sequence-based Place Recognition",
        "abstract": "  Sequence-based place recognition methods for all-weather navigation are\nwell-known for producing state-of-the-art results under challenging day-night\nor summer-winter transitions. These systems, however, rely on complex\nhandcrafted heuristics for sequential matching - which are applied on top of a\npre-computed pairwise similarity matrix between reference and query image\nsequences of a single route - to further reduce false-positive rates compared\nto single-frame retrieval methods. As a result, performing multi-frame place\nrecognition can be extremely slow for deployment on autonomous vehicles or\nevaluation on large datasets, and fail when using relatively short parameter\nvalues such as a sequence length of 2 frames. In this paper, we propose\nDeepSeqSLAM: a trainable CNN+RNN architecture for jointly learning visual and\npositional representations from a single monocular image sequence of a route.\nWe demonstrate our approach on two large benchmark datasets, Nordland and\nOxford RobotCar - recorded over 728 km and 10 km routes, respectively, each\nduring 1 year with multiple seasons, weather, and lighting conditions. On\nNordland, we compare our method to two state-of-the-art sequence-based methods\nacross the entire route under summer-winter changes using a sequence length of\n2 and show that our approach can get over 72% AUC compared to 27% AUC for Delta\nDescriptors and 2% AUC for SeqSLAM; while drastically reducing the deployment\ntime from around 1 hour to 1 minute against both. The framework code and video\nare available at https://mchancan.github.io/deepseqslam\n",
        "published": "2020",
        "authors": [
            "Marvin Chanc\u00e1n",
            "Michael Milford"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.12149v2",
        "title": "SpinNet: Learning a General Surface Descriptor for 3D Point Cloud\n  Registration",
        "abstract": "  Extracting robust and general 3D local features is key to downstream tasks\nsuch as point cloud registration and reconstruction. Existing learning-based\nlocal descriptors are either sensitive to rotation transformations, or rely on\nclassical handcrafted features which are neither general nor representative. In\nthis paper, we introduce a new, yet conceptually simple, neural architecture,\ntermed SpinNet, to extract local features which are rotationally invariant\nwhilst sufficiently informative to enable accurate registration. A Spatial\nPoint Transformer is first introduced to map the input local surface into a\ncarefully designed cylindrical space, enabling end-to-end optimization with\nSO(2) equivariant representation. A Neural Feature Extractor which leverages\nthe powerful point-based and 3D cylindrical convolutional neural layers is then\nutilized to derive a compact and representative descriptor for matching.\nExtensive experiments on both indoor and outdoor datasets demonstrate that\nSpinNet outperforms existing state-of-the-art techniques by a large margin.\nMore critically, it has the best generalization ability across unseen scenarios\nwith different sensor modalities. The code is available at\nhttps://github.com/QingyongHu/SpinNet.\n",
        "published": "2020",
        "authors": [
            "Sheng Ao",
            "Qingyong Hu",
            "Bo Yang",
            "Andrew Markham",
            "Yulan Guo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.03040v2",
        "title": "Understanding Bird's-Eye View of Road Semantics using an Onboard Camera",
        "abstract": "  Autonomous navigation requires scene understanding of the action-space to\nmove or anticipate events. For planner agents moving on the ground plane, such\nas autonomous vehicles, this translates to scene understanding in the\nbird's-eye view (BEV). However, the onboard cameras of autonomous cars are\ncustomarily mounted horizontally for a better view of the surrounding. In this\nwork, we study scene understanding in the form of online estimation of semantic\nBEV maps using the video input from a single onboard camera. We study three key\naspects of this task, image-level understanding, BEV level understanding, and\nthe aggregation of temporal information. Based on these three pillars we\npropose a novel architecture that combines these three aspects. In our\nextensive experiments, we demonstrate that the considered aspects are\ncomplementary to each other for BEV understanding. Furthermore, the proposed\narchitecture significantly surpasses the current state-of-the-art. Code:\nhttps://github.com/ybarancan/BEV_feat_stitch.\n",
        "published": "2020",
        "authors": [
            "Yigit Baran Can",
            "Alexander Liniger",
            "Ozan Unal",
            "Danda Paudel",
            "Luc Van Gool"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.03806v1",
        "title": "Perspectives on Sim2Real Transfer for Robotics: A Summary of the R:SS\n  2020 Workshop",
        "abstract": "  This report presents the debates, posters, and discussions of the Sim2Real\nworkshop held in conjunction with the 2020 edition of the \"Robotics: Science\nand System\" conference. Twelve leaders of the field took competing debate\npositions on the definition, viability, and importance of transferring skills\nfrom simulation to the real world in the context of robotics problems. The\ndebaters also joined a large panel discussion, answering audience questions and\noutlining the future of Sim2Real in robotics. Furthermore, we invited extended\nabstracts to this workshop which are summarized in this report. Based on the\nworkshop, this report concludes with directions for practitioners exploiting\nthis technology and for researchers further exploring open problems in this\narea.\n",
        "published": "2020",
        "authors": [
            "Sebastian H\u00f6fer",
            "Kostas Bekris",
            "Ankur Handa",
            "Juan Camilo Gamboa",
            "Florian Golemo",
            "Melissa Mozifian",
            "Chris Atkeson",
            "Dieter Fox",
            "Ken Goldberg",
            "John Leonard",
            "C. Karen Liu",
            "Jan Peters",
            "Shuran Song",
            "Peter Welinder",
            "Martha White"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.03912v1",
        "title": "MultiON: Benchmarking Semantic Map Memory using Multi-Object Navigation",
        "abstract": "  Navigation tasks in photorealistic 3D environments are challenging because\nthey require perception and effective planning under partial observability.\nRecent work shows that map-like memory is useful for long-horizon navigation\ntasks. However, a focused investigation of the impact of maps on navigation\ntasks of varying complexity has not yet been performed. We propose the multiON\ntask, which requires navigation to an episode-specific sequence of objects in a\nrealistic environment. MultiON generalizes the ObjectGoal navigation task and\nexplicitly tests the ability of navigation agents to locate previously observed\ngoal objects. We perform a set of multiON experiments to examine how a variety\nof agent models perform across a spectrum of navigation task complexities. Our\nexperiments show that: i) navigation performance degrades dramatically with\nescalating task complexity; ii) a simple semantic map agent performs\nsurprisingly well relative to more complex neural image feature map agents; and\niii) even oracle map agents achieve relatively low performance, indicating the\npotential for future work in training embodied navigation agents using maps.\nVideo summary: https://youtu.be/yqTlHNIcgnY\n",
        "published": "2020",
        "authors": [
            "Saim Wani",
            "Shivansh Patel",
            "Unnat Jain",
            "Angel X. Chang",
            "Manolis Savva"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.04060v2",
        "title": "Semantic and Geometric Modeling with Neural Message Passing in 3D Scene\n  Graphs for Hierarchical Mechanical Search",
        "abstract": "  Searching for objects in indoor organized environments such as homes or\noffices is part of our everyday activities. When looking for a target object,\nwe jointly reason about the rooms and containers the object is likely to be in;\nthe same type of container will have a different probability of having the\ntarget depending on the room it is in. We also combine geometric and semantic\ninformation to infer what container is best to search, or what other objects\nare best to move, if the target object is hidden from view. We propose to use a\n3D scene graph representation to capture the hierarchical, semantic, and\ngeometric aspects of this problem. To exploit this representation in a search\nprocess, we introduce Hierarchical Mechanical Search (HMS), a method that\nguides an agent's actions towards finding a target object specified with a\nnatural language description. HMS is based on a novel neural network\narchitecture that uses neural message passing of vectors with visual,\ngeometric, and linguistic information to allow HMS to reason across layers of\nthe graph while combining semantic and geometric cues. HMS is evaluated on a\nnovel dataset of 500 3D scene graphs with dense placements of semantically\nrelated objects in storage locations, and is shown to be significantly better\nthan several baselines at finding objects and close to the oracle policy in\nterms of the median number of actions required. Additional qualitative results\ncan be found at https://ai.stanford.edu/mech-search/hms.\n",
        "published": "2020",
        "authors": [
            "Andrey Kurenkov",
            "Roberto Mart\u00edn-Mart\u00edn",
            "Jeff Ichnowski",
            "Ken Goldberg",
            "Silvio Savarese"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.04983v1",
        "title": "Driving Behavior Explanation with Multi-level Fusion",
        "abstract": "  In this era of active development of autonomous vehicles, it becomes crucial\nto provide driving systems with the capacity to explain their decisions. In\nthis work, we focus on generating high-level driving explanations as the\nvehicle drives. We present BEEF, for BEhavior Explanation with Fusion, a deep\narchitecture which explains the behavior of a trajectory prediction model.\nSupervised by annotations of human driving decisions justifications, BEEF\nlearns to fuse features from multiple levels. Leveraging recent advances in the\nmulti-modal fusion literature, BEEF is carefully designed to model the\ncorrelations between high-level decisions features and mid-level perceptual\nfeatures. The flexibility and efficiency of our approach are validated with\nextensive experiments on the HDD and BDD-X datasets.\n",
        "published": "2020",
        "authors": [
            "H\u00e9di Ben-Younes",
            "\u00c9loi Zablocki",
            "Patrick P\u00e9rez",
            "Matthieu Cord"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.06117v1",
        "title": "How to Train PointGoal Navigation Agents on a (Sample and Compute)\n  Budget",
        "abstract": "  PointGoal navigation has seen significant recent interest and progress,\nspurred on by the Habitat platform and associated challenge. In this paper, we\nstudy PointGoal navigation under both a sample budget (75 million frames) and a\ncompute budget (1 GPU for 1 day). We conduct an extensive set of experiments,\ncumulatively totaling over 50,000 GPU-hours, that let us identify and discuss a\nnumber of ostensibly minor but significant design choices -- the advantage\nestimation procedure (a key component in training), visual encoder\narchitecture, and a seemingly minor hyper-parameter change. Overall, these\ndesign choices to lead considerable and consistent improvements over the\nbaselines present in Savva et al. Under a sample budget, performance for RGB-D\nagents improves 8 SPL on Gibson (14% relative improvement) and 20 SPL on\nMatterport3D (38% relative improvement). Under a compute budget, performance\nfor RGB-D agents improves by 19 SPL on Gibson (32% relative improvement) and 35\nSPL on Matterport3D (220% relative improvement). We hope our findings and\nrecommendations will make serve to make the community's experiments more\nefficient.\n",
        "published": "2020",
        "authors": [
            "Erik Wijmans",
            "Irfan Essa",
            "Dhruv Batra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.09242v1",
        "title": "S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point\n  Clouds",
        "abstract": "  With the increasing reliance of self-driving and similar robotic systems on\nrobust 3D vision, the processing of LiDAR scans with deep convolutional neural\nnetworks has become a trend in academia and industry alike. Prior attempts on\nthe challenging Semantic Scene Completion task - which entails the inference of\ndense 3D structure and associated semantic labels from \"sparse\" representations\n- have been, to a degree, successful in small indoor scenes when provided with\ndense point clouds or dense depth maps often fused with semantic segmentation\nmaps from RGB images. However, the performance of these systems drop\ndrastically when applied to large outdoor scenes characterized by dynamic and\nexponentially sparser conditions. Likewise, processing of the entire sparse\nvolume becomes infeasible due to memory limitations and workarounds introduce\ncomputational inefficiency as practitioners are forced to divide the overall\nvolume into multiple equal segments and infer on each individually, rendering\nreal-time performance impossible. In this work, we formulate a method that\nsubsumes the sparsity of large-scale environments and present S3CNet, a sparse\nconvolution based neural network that predicts the semantically completed scene\nfrom a single, unified LiDAR point cloud. We show that our proposed method\noutperforms all counterparts on the 3D task, achieving state-of-the art results\non the SemanticKITTI benchmark. Furthermore, we propose a 2D variant of S3CNet\nwith a multi-view fusion strategy to complement our 3D network, providing\nrobustness to occlusions and extreme sparsity in distant regions. We conduct\nexperiments for the 2D semantic scene completion task and compare the results\nof our sparse 2D network against several leading LiDAR segmentation models\nadapted for bird's eye view segmentation on two open-source datasets.\n",
        "published": "2020",
        "authors": [
            "Ran Cheng",
            "Christopher Agia",
            "Yuan Ren",
            "Xinhai Li",
            "Liu Bingbing"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.11717v3",
        "title": "Social NCE: Contrastive Learning of Socially-aware Motion\n  Representations",
        "abstract": "  Learning socially-aware motion representations is at the core of recent\nadvances in multi-agent problems, such as human motion forecasting and robot\nnavigation in crowds. Despite promising progress, existing representations\nlearned with neural networks still struggle to generalize in closed-loop\npredictions (e.g., output colliding trajectories). This issue largely arises\nfrom the non-i.i.d. nature of sequential prediction in conjunction with\nill-distributed training data. Intuitively, if the training data only comes\nfrom human behaviors in safe spaces, i.e., from \"positive\" examples, it is\ndifficult for learning algorithms to capture the notion of \"negative\" examples\nlike collisions. In this work, we aim to address this issue by explicitly\nmodeling negative examples through self-supervision: (i) we introduce a social\ncontrastive loss that regularizes the extracted motion representation by\ndiscerning the ground-truth positive events from synthetic negative ones; (ii)\nwe construct informative negative samples based on our prior knowledge of rare\nbut dangerous circumstances. Our method substantially reduces the collision\nrates of recent trajectory forecasting, behavioral cloning and reinforcement\nlearning algorithms, outperforming state-of-the-art methods on several\nbenchmarks. Our code is available at https://github.com/vita-epfl/social-nce.\n",
        "published": "2020",
        "authors": [
            "Yuejiang Liu",
            "Qi Yan",
            "Alexandre Alahi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.14359v1",
        "title": "Commonsense Visual Sensemaking for Autonomous Driving: On Generalised\n  Neurosymbolic Online Abduction Integrating Vision and Semantics",
        "abstract": "  We demonstrate the need and potential of systematically integrated vision and\nsemantics solutions for visual sensemaking in the backdrop of autonomous\ndriving. A general neurosymbolic method for online visual sensemaking using\nanswer set programming (ASP) is systematically formalised and fully\nimplemented. The method integrates state of the art in visual computing, and is\ndeveloped as a modular framework that is generally usable within hybrid\narchitectures for realtime perception and control. We evaluate and demonstrate\nwith community established benchmarks KITTIMOD, MOT-2017, and MOT-2020. As\nuse-case, we focus on the significance of human-centred visual sensemaking --\ne.g., involving semantic representation and explainability, question-answering,\ncommonsense interpolation -- in safety-critical autonomous driving situations.\nThe developed neurosymbolic framework is domain-independent, with the case of\nautonomous driving designed to serve as an exemplar for online visual\nsensemaking in diverse cognitive interaction settings in the backdrop of select\nhuman-centred AI technology design considerations.\n  Keywords: Cognitive Vision, Deep Semantics, Declarative Spatial Reasoning,\nKnowledge Representation and Reasoning, Commonsense Reasoning, Visual\nAbduction, Answer Set Programming, Autonomous Driving, Human-Centred Computing\nand Design, Standardisation in Driving Technology, Spatial Cognition and AI.\n",
        "published": "2020",
        "authors": [
            "Jakob Suchan",
            "Mehul Bhatt",
            "Srikrishna Varadarajan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.15373v1",
        "title": "Model-Based Visual Planning with Self-Supervised Functional Distances",
        "abstract": "  A generalist robot must be able to complete a variety of tasks in its\nenvironment. One appealing way to specify each task is in terms of a goal\nobservation. However, learning goal-reaching policies with reinforcement\nlearning remains a challenging problem, particularly when hand-engineered\nreward functions are not available. Learned dynamics models are a promising\napproach for learning about the environment without rewards or task-directed\ndata, but planning to reach goals with such a model requires a notion of\nfunctional similarity between observations and goal states. We present a\nself-supervised method for model-based visual goal reaching, which uses both a\nvisual dynamics model as well as a dynamical distance function learned using\nmodel-free reinforcement learning. Our approach learns entirely using offline,\nunlabeled data, making it practical to scale to large and diverse datasets. In\nour experiments, we find that our method can successfully learn models that\nperform a variety of tasks at test-time, moving objects amid distractors with a\nsimulated robotic arm and even learning to open and close a drawer using a\nreal-world robot. In comparisons, we find that this approach substantially\noutperforms both model-free and model-based prior methods. Videos and\nvisualizations are available here: http://sites.google.com/berkeley.edu/mbold.\n",
        "published": "2020",
        "authors": [
            "Stephen Tian",
            "Suraj Nair",
            "Frederik Ebert",
            "Sudeep Dasari",
            "Benjamin Eysenbach",
            "Chelsea Finn",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.04353v5",
        "title": "Unlocking Pixels for Reinforcement Learning via Implicit Attention",
        "abstract": "  There has recently been significant interest in training reinforcement\nlearning (RL) agents in vision-based environments. This poses many challenges,\nsuch as high dimensionality and the potential for observational overfitting\nthrough spurious correlations. A promising approach to solve both of these\nproblems is an attention bottleneck, which provides a simple and effective\nframework for learning high performing policies, even in the presence of\ndistractions. However, due to poor scalability of attention architectures,\nthese methods cannot be applied beyond low resolution visual inputs, using\nlarge patches (thus small attention matrices). In this paper we make use of new\nefficient attention algorithms, recently shown to be highly effective for\nTransformers, and demonstrate that these techniques can be successfully adopted\nfor the RL setting. This allows our attention-based controllers to scale to\nlarger visual inputs, and facilitate the use of smaller patches, even\nindividual pixels, improving generalization. We show this on a range of tasks\nfrom the Distracting Control Suite to vision-based quadruped robots locomotion.\nWe provide rigorous theoretical analysis of the proposed algorithm.\n",
        "published": "2021",
        "authors": [
            "Krzysztof Marcin Choromanski",
            "Deepali Jain",
            "Wenhao Yu",
            "Xingyou Song",
            "Jack Parker-Holder",
            "Tingnan Zhang",
            "Valerii Likhosherstov",
            "Aldo Pacchiano",
            "Anirban Santara",
            "Yunhao Tang",
            "Jie Tan",
            "Adrian Weller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.04738v2",
        "title": "End-to-End Deep Learning of Lane Detection and Path Prediction for\n  Real-Time Autonomous Driving",
        "abstract": "  Inspired by the UNet architecture of semantic image segmentation, we propose\na lightweight UNet using depthwise separable convolutions (DSUNet) for\nend-to-end learning of lane detection and path prediction (PP) in autonomous\ndriving. We also design and integrate a PP algorithm with convolutional neural\nnetwork (CNN) to form a simulation model (CNN-PP) that can be used to assess\nCNN's performance qualitatively, quantitatively, and dynamically in a host\nagent car driving along with other agents all in a real-time autonomous manner.\nDSUNet is 5.16x lighter in model size and 1.61x faster in inference than UNet.\nDSUNet-PP outperforms UNet-PP in mean average errors of predicted curvature and\nlateral offset for path planning in dynamic simulation. DSUNet-PP outperforms a\nmodified UNet in lateral error, which is tested in a real car on real road.\nThese results show that DSUNet is efficient and effective for lane detection\nand path prediction in autonomous driving.\n",
        "published": "2021",
        "authors": [
            "Der-Hau Lee",
            "Jinn-Liang Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.04750v1",
        "title": "Where is my hand? Deep hand segmentation for visual self-recognition in\n  humanoid robots",
        "abstract": "  The ability to distinguish between the self and the background is of\nparamount importance for robotic tasks. The particular case of hands, as the\nend effectors of a robotic system that more often enter into contact with other\nelements of the environment, must be perceived and tracked with precision to\nexecute the intended tasks with dexterity and without colliding with obstacles.\nThey are fundamental for several applications, from Human-Robot Interaction\ntasks to object manipulation. Modern humanoid robots are characterized by high\nnumber of degrees of freedom which makes their forward kinematics models very\nsensitive to uncertainty. Thus, resorting to vision sensing can be the only\nsolution to endow these robots with a good perception of the self, being able\nto localize their body parts with precision. In this paper, we propose the use\nof a Convolution Neural Network (CNN) to segment the robot hand from an image\nin an egocentric view. It is known that CNNs require a huge amount of data to\nbe trained. To overcome the challenge of labeling real-world images, we propose\nthe use of simulated datasets exploiting domain randomization techniques. We\nfine-tuned the Mask-RCNN network for the specific task of segmenting the hand\nof the humanoid robot Vizzy. We focus our attention on developing a methodology\nthat requires low amounts of data to achieve reasonable performance while\ngiving detailed insight on how to properly generate variability in the training\ndataset. Moreover, we analyze the fine-tuning process within the complex model\nof Mask-RCNN, understanding which weights should be transferred to the new task\nof segmenting robot hands. Our final model was trained solely on synthetic\nimages and achieves an average IoU of 82% on synthetic validation data and\n56.3% on real test data. These results were achieved with only 1000 training\nimages and 3 hours of training time using a single GPU.\n",
        "published": "2021",
        "authors": [
            "Alexandre Almeida",
            "Pedro Vicente",
            "Alexandre Bernardino"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.08094v1",
        "title": "Composing Pick-and-Place Tasks By Grounding Language",
        "abstract": "  Controlling robots to perform tasks via natural language is one of the most\nchallenging topics in human-robot interaction. In this work, we present a robot\nsystem that follows unconstrained language instructions to pick and place\narbitrary objects and effectively resolves ambiguities through dialogues. Our\napproach infers objects and their relationships from input images and language\nexpressions and can place objects in accordance with the spatial relations\nexpressed by the user. Unlike previous approaches, we consider grounding not\nonly for the picking but also for the placement of everyday objects from\nlanguage. Specifically, by grounding objects and their spatial relations, we\nallow specification of complex placement instructions, e.g. \"place it behind\nthe middle red bowl\". Our results obtained using a real-world PR2 robot\ndemonstrate the effectiveness of our method in understanding pick-and-place\nlanguage instructions and sequentially composing them to solve tabletop\nmanipulation tasks. Videos are available at\nhttp://speechrobot.cs.uni-freiburg.de\n",
        "published": "2021",
        "authors": [
            "Oier Mees",
            "Wolfram Burgard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.08945v1",
        "title": "Weakly Supervised Learning of Rigid 3D Scene Flow",
        "abstract": "  We propose a data-driven scene flow estimation algorithm exploiting the\nobservation that many 3D scenes can be explained by a collection of agents\nmoving as rigid bodies. At the core of our method lies a deep architecture able\nto reason at the \\textbf{object-level} by considering 3D scene flow in\nconjunction with other 3D tasks. This object level abstraction, enables us to\nrelax the requirement for dense scene flow supervision with simpler binary\nbackground segmentation mask and ego-motion annotations. Our mild supervision\nrequirements make our method well suited for recently released massive data\ncollections for autonomous driving, which do not contain dense scene flow\nannotations. As output, our model provides low-level cues like pointwise flow\nand higher-level cues such as holistic scene understanding at the level of\nrigid objects. We further propose a test-time optimization refining the\npredicted rigid scene flow. We showcase the effectiveness and generalization\ncapacity of our method on four different autonomous driving datasets. We\nrelease our source code and pre-trained models under\n\\url{github.com/zgojcic/Rigid3DSceneFlow}.\n",
        "published": "2021",
        "authors": [
            "Zan Gojcic",
            "Or Litany",
            "Andreas Wieser",
            "Leonidas J. Guibas",
            "Tolga Birdal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.02074v1",
        "title": "Sequential Place Learning: Heuristic-Free High-Performance Long-Term\n  Place Recognition",
        "abstract": "  Sequential matching using hand-crafted heuristics has been standard practice\nin route-based place recognition for enhancing pairwise similarity results for\nnearly a decade. However, precision-recall performance of these algorithms\ndramatically degrades when searching on short temporal window (TW) lengths,\nwhile demanding high compute and storage costs on large robotic datasets for\nautonomous navigation research. Here, influenced by biological systems that\nrobustly navigate spacetime scales even without vision, we develop a joint\nvisual and positional representation learning technique, via a sequential\nprocess, and design a learning-based CNN+LSTM architecture, trainable via\nbackpropagation through time, for viewpoint- and appearance-invariant place\nrecognition. Our approach, Sequential Place Learning (SPL), is based on a CNN\nfunction that visually encodes an environment from a single traversal, thus\nreducing storage capacity, while an LSTM temporally fuses each visual embedding\nwith corresponding positional data -- obtained from any source of motion\nestimation -- for direct sequential inference. Contrary to classical two-stage\npipelines, e.g., match-then-temporally-filter, our network directly eliminates\nfalse-positive rates while jointly learning sequence matching from a single\nmonocular image sequence, even using short TWs. Hence, we demonstrate that our\nmodel outperforms 15 classical methods while setting new state-of-the-art\nperformance standards on 4 challenging benchmark datasets, where one of them\ncan be considered solved with recall rates of 100% at 100% precision, correctly\nmatching all places under extreme sunlight-darkness changes. In addition, we\nshow that SPL can be up to 70x faster to deploy than classical methods on a 729\nkm route comprising 35,768 consecutive frames. Extensive experiments\ndemonstrate the... Baseline code available at\nhttps://github.com/mchancan/deepseqslam\n",
        "published": "2021",
        "authors": [
            "Marvin Chanc\u00e1n",
            "Michael Milford"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.04174v3",
        "title": "Greedy Hierarchical Variational Autoencoders for Large-Scale Video\n  Prediction",
        "abstract": "  A video prediction model that generalizes to diverse scenes would enable\nintelligent agents such as robots to perform a variety of tasks via planning\nwith the model. However, while existing video prediction models have produced\npromising results on small datasets, they suffer from severe underfitting when\ntrained on large and diverse datasets. To address this underfitting challenge,\nwe first observe that the ability to train larger video prediction models is\noften bottlenecked by the memory constraints of GPUs or TPUs. In parallel, deep\nhierarchical latent variable models can produce higher quality predictions by\ncapturing the multi-level stochasticity of future observations, but end-to-end\noptimization of such models is notably difficult. Our key insight is that\ngreedy and modular optimization of hierarchical autoencoders can simultaneously\naddress both the memory constraints and the optimization challenges of\nlarge-scale video prediction. We introduce Greedy Hierarchical Variational\nAutoencoders (GHVAEs), a method that learns high-fidelity video predictions by\ngreedily training each level of a hierarchical autoencoder. In comparison to\nstate-of-the-art models, GHVAEs provide 17-55% gains in prediction performance\non four video datasets, a 35-40% higher success rate on real robot tasks, and\ncan improve performance monotonically by simply adding more modules.\n",
        "published": "2021",
        "authors": [
            "Bohan Wu",
            "Suraj Nair",
            "Roberto Martin-Martin",
            "Li Fei-Fei",
            "Chelsea Finn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.04351v1",
        "title": "Learning a State Representation and Navigation in Cluttered and Dynamic\n  Environments",
        "abstract": "  In this work, we present a learning-based pipeline to realise local\nnavigation with a quadrupedal robot in cluttered environments with static and\ndynamic obstacles. Given high-level navigation commands, the robot is able to\nsafely locomote to a target location based on frames from a depth camera\nwithout any explicit mapping of the environment. First, the sequence of images\nand the current trajectory of the camera are fused to form a model of the world\nusing state representation learning. The output of this lightweight module is\nthen directly fed into a target-reaching and obstacle-avoiding policy trained\nwith reinforcement learning. We show that decoupling the pipeline into these\ncomponents results in a sample efficient policy learning stage that can be\nfully trained in simulation in just a dozen minutes. The key part is the state\nrepresentation, which is trained to not only estimate the hidden state of the\nworld in an unsupervised fashion, but also helps bridging the reality gap,\nenabling successful sim-to-real transfer. In our experiments with the\nquadrupedal robot ANYmal in simulation and in reality, we show that our system\ncan handle noisy depth images, avoid dynamic obstacles unseen during training,\nand is endowed with local spatial awareness.\n",
        "published": "2021",
        "authors": [
            "David Hoeller",
            "Lorenz Wellhausen",
            "Farbod Farshidian",
            "Marco Hutter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.14025v1",
        "title": "The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion\n  Planning Benchmark for Physically Realistic Embodied AI",
        "abstract": "  We introduce a visually-guided and physics-driven task-and-motion planning\nbenchmark, which we call the ThreeDWorld Transport Challenge. In this\nchallenge, an embodied agent equipped with two 9-DOF articulated arms is\nspawned randomly in a simulated physical home environment. The agent is\nrequired to find a small set of objects scattered around the house, pick them\nup, and transport them to a desired final location. We also position containers\naround the house that can be used as tools to assist with transporting objects\nefficiently. To complete the task, an embodied agent must plan a sequence of\nactions to change the state of a large number of objects in the face of\nrealistic physical constraints. We build this benchmark challenge using the\nThreeDWorld simulation: a virtual 3D environment where all objects respond to\nphysics, and where can be controlled using fully physics-driven navigation and\ninteraction API. We evaluate several existing agents on this benchmark.\nExperimental results suggest that: 1) a pure RL model struggles on this\nchallenge; 2) hierarchical planning-based agents can transport some objects but\nstill far from solving this task. We anticipate that this benchmark will\nempower researchers to develop more intelligent physics-driven robots for the\nphysical world.\n",
        "published": "2021",
        "authors": [
            "Chuang Gan",
            "Siyuan Zhou",
            "Jeremy Schwartz",
            "Seth Alter",
            "Abhishek Bhandwaldar",
            "Dan Gutfreund",
            "Daniel L. K. Yamins",
            "James J DiCarlo",
            "Josh McDermott",
            "Antonio Torralba",
            "Joshua B. Tenenbaum"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.14231v1",
        "title": "Congestion-aware Multi-agent Trajectory Prediction for Collision\n  Avoidance",
        "abstract": "  Predicting agents' future trajectories plays a crucial role in modern AI\nsystems, yet it is challenging due to intricate interactions exhibited in\nmulti-agent systems, especially when it comes to collision avoidance. To\naddress this challenge, we propose to learn congestion patterns as contextual\ncues explicitly and devise a novel \"Sense--Learn--Reason--Predict\" framework by\nexploiting advantages of three different doctrines of thought, which yields the\nfollowing desirable benefits: (i) Representing congestion as contextual cues\nvia latent factors subsumes the concept of social force commonly used in\nphysics-based approaches and implicitly encodes the distance as a cost, similar\nto the way a planning-based method models the environment. (ii) By decomposing\nthe learning phases into two stages, a \"student\" can learn contextual cues from\na \"teacher\" while generating collision-free trajectories. To make the framework\ncomputationally tractable, we formulate it as an optimization problem and\nderive an upper bound by leveraging the variational parametrization. In\nexperiments, we demonstrate that the proposed model is able to generate\ncollision-free trajectory predictions in a synthetic dataset designed for\ncollision avoidance evaluation and remains competitive on the commonly used\nNGSIM US-101 highway dataset.\n",
        "published": "2021",
        "authors": [
            "Xu Xie",
            "Chi Zhang",
            "Yixin Zhu",
            "Ying Nian Wu",
            "Song-Chun Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.14283v1",
        "title": "OmniHang: Learning to Hang Arbitrary Objects using Contact Point\n  Correspondences and Neural Collision Estimation",
        "abstract": "  In this paper, we explore whether a robot can learn to hang arbitrary objects\nonto a diverse set of supporting items such as racks or hooks. Endowing robots\nwith such an ability has applications in many domains such as domestic\nservices, logistics, or manufacturing. Yet, it is a challenging manipulation\ntask due to the large diversity of geometry and topology of everyday objects.\nIn this paper, we propose a system that takes partial point clouds of an object\nand a supporting item as input and learns to decide where and how to hang the\nobject stably. Our system learns to estimate the contact point correspondences\nbetween the object and supporting item to get an estimated stable pose. We then\nrun a deep reinforcement learning algorithm to refine the predicted stable\npose. Then, the robot needs to find a collision-free path to move the object\nfrom its initial pose to stable hanging pose. To this end, we train a neural\nnetwork based collision estimator that takes as input partial point clouds of\nthe object and supporting item. We generate a new and challenging, large-scale,\nsynthetic dataset annotated with stable poses of objects hung on various\nsupporting items and their contact point correspondences. In this dataset, we\nshow that our system is able to achieve a 68.3% success rate of predicting\nstable object poses and has a 52.1% F1 score in terms of finding feasible\npaths. Supplemental material and videos are available on our project webpage.\n",
        "published": "2021",
        "authors": [
            "Yifan You",
            "Lin Shao",
            "Toki Migimatsu",
            "Jeannette Bohg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.16775v1",
        "title": "Attention, please! A survey of Neural Attention Models in Deep Learning",
        "abstract": "  In humans, Attention is a core property of all perceptual and cognitive\noperations. Given our limited ability to process competing sources, attention\nmechanisms select, modulate, and focus on the information most relevant to\nbehavior. For decades, concepts and functions of attention have been studied in\nphilosophy, psychology, neuroscience, and computing. For the last six years,\nthis property has been widely explored in deep neural networks. Currently, the\nstate-of-the-art in Deep Learning is represented by neural attention models in\nseveral application domains. This survey provides a comprehensive overview and\nanalysis of developments in neural attention models. We systematically reviewed\nhundreds of architectures in the area, identifying and discussing those in\nwhich attention has shown a significant impact. We also developed and made\npublic an automated methodology to facilitate the development of reviews in the\narea. By critically analyzing 650 works, we describe the primary uses of\nattention in convolutional, recurrent networks and generative models,\nidentifying common subgroups of uses and applications. Furthermore, we describe\nthe impact of attention in different application domains and their impact on\nneural networks' interpretability. Finally, we list possible trends and\nopportunities for further research, hoping that this review will provide a\nsuccinct overview of the main attentional models in the area and guide\nresearchers in developing future approaches that will drive further\nimprovements.\n",
        "published": "2021",
        "authors": [
            "Alana de Santana Correia",
            "Esther Luna Colombini"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.16817v1",
        "title": "Learning Generalizable Robotic Reward Functions from \"In-The-Wild\" Human\n  Videos",
        "abstract": "  We are motivated by the goal of generalist robots that can complete a wide\nrange of tasks across many environments. Critical to this is the robot's\nability to acquire some metric of task success or reward, which is necessary\nfor reinforcement learning, planning, or knowing when to ask for help. For a\ngeneral-purpose robot operating in the real world, this reward function must\nalso be able to generalize broadly across environments, tasks, and objects,\nwhile depending only on on-board sensor observations (e.g. RGB images). While\ndeep learning on large and diverse datasets has shown promise as a path towards\nsuch generalization in computer vision and natural language, collecting high\nquality datasets of robotic interaction at scale remains an open challenge. In\ncontrast, \"in-the-wild\" videos of humans (e.g. YouTube) contain an extensive\ncollection of people doing interesting tasks across a diverse range of\nsettings. In this work, we propose a simple approach, Domain-agnostic Video\nDiscriminator (DVD), that learns multitask reward functions by training a\ndiscriminator to classify whether two videos are performing the same task, and\ncan generalize by virtue of learning from a small amount of robot data with a\nbroad dataset of human videos. We find that by leveraging diverse human\ndatasets, this reward function (a) can generalize zero shot to unseen\nenvironments, (b) generalize zero shot to unseen tasks, and (c) can be combined\nwith visual model predictive control to solve robotic manipulation tasks on a\nreal WidowX200 robot in an unseen environment from a single human demo.\n",
        "published": "2021",
        "authors": [
            "Annie S. Chen",
            "Suraj Nair",
            "Chelsea Finn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.00442v2",
        "title": "Touch-based Curiosity for Sparse-Reward Tasks",
        "abstract": "  Robots in many real-world settings have access to force/torque sensors in\ntheir gripper and tactile sensing is often necessary in tasks that involve\ncontact-rich motion. In this work, we leverage surprise from mismatches in\ntouch feedback to guide exploration in hard sparse-reward reinforcement\nlearning tasks. Our approach, Touch-based Curiosity (ToC), learns what visible\nobjects interactions are supposed to \"feel\" like. We encourage exploration by\nrewarding interactions where the expectation and the experience don't match. In\nour proposed method, an initial task-independent exploration phase is followed\nby an on-task learning phase, in which the original interactions are relabeled\nwith on-task rewards. We test our approach on a range of touch-intensive robot\narm tasks (e.g. pushing objects, opening doors), which we also release as part\nof this work. Across multiple experiments in a simulated setting, we\ndemonstrate that our method is able to learn these difficult tasks through\nsparse reward and curiosity alone. We compare our cross-modal approach to\nsingle-modality (touch- or vision-only) approaches as well as other\ncuriosity-based methods and find that our method performs better and is more\nsample-efficient.\n",
        "published": "2021",
        "authors": [
            "Sai Rajeswar",
            "Cyril Ibrahim",
            "Nitin Surya",
            "Florian Golemo",
            "David Vazquez",
            "Aaron Courville",
            "Pedro O. Pinheiro"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.01328v2",
        "title": "Uncertainty for Identifying Open-Set Errors in Visual Object Detection",
        "abstract": "  Deployed into an open world, object detectors are prone to open-set errors,\nfalse positive detections of object classes not present in the training\ndataset. We propose GMM-Det, a real-time method for extracting epistemic\nuncertainty from object detectors to identify and reject open-set errors.\nGMM-Det trains the detector to produce a structured logit space that is\nmodelled with class-specific Gaussian Mixture Models. At test time, open-set\nerrors are identified by their low log-probability under all Gaussian Mixture\nModels. We test two common detector architectures, Faster R-CNN and RetinaNet,\nacross three varied datasets spanning robotics and computer vision. Our results\nshow that GMM-Det consistently outperforms existing uncertainty techniques for\nidentifying and rejecting open-set detections, especially at the low-error-rate\noperating point required for safety-critical applications. GMM-Det maintains\nobject detection performance, and introduces only minimal computational\noverhead. We also introduce a methodology for converting existing object\ndetection datasets into specific open-set datasets to evaluate open-set\nperformance in object detection.\n",
        "published": "2021",
        "authors": [
            "Dimity Miller",
            "Niko S\u00fcnderhauf",
            "Michael Milford",
            "Feras Dayoub"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.02646v1",
        "title": "gradSim: Differentiable simulation for system identification and\n  visuomotor control",
        "abstract": "  We consider the problem of estimating an object's physical properties such as\nmass, friction, and elasticity directly from video sequences. Such a system\nidentification problem is fundamentally ill-posed due to the loss of\ninformation during image formation. Current solutions require precise 3D labels\nwhich are labor-intensive to gather, and infeasible to create for many systems\nsuch as deformable solids or cloth. We present gradSim, a framework that\novercomes the dependence on 3D supervision by leveraging differentiable\nmultiphysics simulation and differentiable rendering to jointly model the\nevolution of scene dynamics and image formation. This novel combination enables\nbackpropagation from pixels in a video sequence through to the underlying\nphysical attributes that generated them. Moreover, our unified computation\ngraph -- spanning from the dynamics and through the rendering process --\nenables learning in challenging visuomotor control tasks, without relying on\nstate-based (3D) supervision, while obtaining performance competitive to or\nbetter than techniques that rely on precise 3D labels.\n",
        "published": "2021",
        "authors": [
            "Krishna Murthy Jatavallabhula",
            "Miles Macklin",
            "Florian Golemo",
            "Vikram Voleti",
            "Linda Petrini",
            "Martin Weiss",
            "Breandan Considine",
            "Jerome Parent-Levesque",
            "Kevin Xie",
            "Kenny Erleben",
            "Liam Paull",
            "Florian Shkurti",
            "Derek Nowrouzezahrai",
            "Sanja Fidler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.03956v1",
        "title": "Just Label What You Need: Fine-Grained Active Selection for Perception\n  and Prediction through Partially Labeled Scenes",
        "abstract": "  Self-driving vehicles must perceive and predict the future positions of\nnearby actors in order to avoid collisions and drive safely. A learned deep\nlearning module is often responsible for this task, requiring large-scale,\nhigh-quality training datasets. As data collection is often significantly\ncheaper than labeling in this domain, the decision of which subset of examples\nto label can have a profound impact on model performance. Active learning\ntechniques, which leverage the state of the current model to iteratively select\nexamples for labeling, offer a promising solution to this problem. However,\ndespite the appeal of this approach, there has been little scientific analysis\nof active learning approaches for the perception and prediction (P&P) problem.\nIn this work, we study active learning techniques for P&P and find that the\ntraditional active learning formulation is ill-suited for the P&P setting. We\nthus introduce generalizations that ensure that our approach is both cost-aware\nand allows for fine-grained selection of examples through partially labeled\nscenes. Our experiments on a real-world, large-scale self-driving dataset\nsuggest that fine-grained selection can improve the performance across\nperception, prediction, and downstream planning tasks.\n",
        "published": "2021",
        "authors": [
            "Sean Segal",
            "Nishanth Kumar",
            "Sergio Casas",
            "Wenyuan Zeng",
            "Mengye Ren",
            "Jingkang Wang",
            "Raquel Urtasun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.09224v1",
        "title": "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving",
        "abstract": "  How should representations from complementary sensors be integrated for\nautonomous driving? Geometry-based sensor fusion has shown great promise for\nperception tasks such as object detection and motion forecasting. However, for\nthe actual driving task, the global context of the 3D scene is key, e.g. a\nchange in traffic light state can affect the behavior of a vehicle\ngeometrically distant from that traffic light. Geometry alone may therefore be\ninsufficient for effectively fusing representations in end-to-end driving\nmodels. In this work, we demonstrate that imitation learning policies based on\nexisting sensor fusion methods under-perform in the presence of a high density\nof dynamic agents and complex scenarios, which require global contextual\nreasoning, such as handling traffic oncoming from multiple directions at\nuncontrolled intersections. Therefore, we propose TransFuser, a novel\nMulti-Modal Fusion Transformer, to integrate image and LiDAR representations\nusing attention. We experimentally validate the efficacy of our approach in\nurban settings involving complex scenarios using the CARLA urban driving\nsimulator. Our approach achieves state-of-the-art driving performance while\nreducing collisions by 76% compared to geometry-based fusion.\n",
        "published": "2021",
        "authors": [
            "Aditya Prakash",
            "Kashyap Chitta",
            "Andreas Geiger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.11165v1",
        "title": "Hierarchical growing grid networks for skeleton based action recognition",
        "abstract": "  In this paper, a novel cognitive architecture for action recognition is\ndeveloped by applying layers of growing grid neural networks.Using these layers\nmakes the system capable of automatically arranging its representational\nstructure. In addition to the expansion of the neural map during the growth\nphase, the system is provided with a prior knowledge of the input space, which\nincreases the processing speed of the learning phase. Apart from two layers of\ngrowing grid networks the architecture is composed of a preprocessing layer, an\nordered vector representation layer and a one-layer supervised neural network.\nThese layers are designed to solve the action recognition problem. The\nfirst-layer growing grid receives the input data of human actions and the\nneural map generates an action pattern vector representing each action sequence\nby connecting the elicited activation of the trained map. The pattern vectors\nare then sent to the ordered vector representation layer to build the\ntime-invariant input vectors of key activations for the second-layer growing\ngrid. The second-layer growing grid categorizes the input vectors to the\ncorresponding action clusters/sub-clusters and finally the one-layer supervised\nneural network labels the shaped clusters with action labels. Three experiments\nusing different datasets of actions show that the system is capable of learning\nto categorize the actions quickly and efficiently. The performance of the\ngrowing grid architecture is com-pared with the results from a system based on\nSelf-Organizing Maps, showing that the growing grid architecture performs\nsignificantly superior on the action recognition tasks.\n",
        "published": "2021",
        "authors": [
            "Zahra Gharaee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.11213v1",
        "title": "ManipulaTHOR: A Framework for Visual Object Manipulation",
        "abstract": "  The domain of Embodied AI has recently witnessed substantial progress,\nparticularly in navigating agents within their environments. These early\nsuccesses have laid the building blocks for the community to tackle tasks that\nrequire agents to actively interact with objects in their environment. Object\nmanipulation is an established research domain within the robotics community\nand poses several challenges including manipulator motion, grasping and\nlong-horizon planning, particularly when dealing with oft-overlooked practical\nsetups involving visually rich and complex scenes, manipulation using mobile\nagents (as opposed to tabletop manipulation), and generalization to unseen\nenvironments and objects. We propose a framework for object manipulation built\nupon the physics-enabled, visually rich AI2-THOR framework and present a new\nchallenge to the Embodied AI community known as ArmPointNav. This task extends\nthe popular point navigation task to object manipulation and offers new\nchallenges including 3D obstacle avoidance, manipulating objects in the\npresence of occlusion, and multi-object manipulation that necessitates long\nterm planning. Popular learning paradigms that are successful on PointNav\nchallenges show promise, but leave a large room for improvement.\n",
        "published": "2021",
        "authors": [
            "Kiana Ehsani",
            "Winson Han",
            "Alvaro Herrasti",
            "Eli VanderBilt",
            "Luca Weihs",
            "Eric Kolve",
            "Aniruddha Kembhavi",
            "Roozbeh Mottaghi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.02930v1",
        "title": "Spectral Temporal Graph Neural Network for Trajectory Prediction",
        "abstract": "  An effective understanding of the contextual environment and accurate motion\nforecasting of surrounding agents is crucial for the development of autonomous\nvehicles and social mobile robots. This task is challenging since the behavior\nof an autonomous agent is not only affected by its own intention, but also by\nthe static environment and surrounding dynamically interacting agents. Previous\nworks focused on utilizing the spatial and temporal information in time domain\nwhile not sufficiently taking advantage of the cues in frequency domain. To\nthis end, we propose a Spectral Temporal Graph Neural Network (SpecTGNN), which\ncan capture inter-agent correlations and temporal dependency simultaneously in\nfrequency domain in addition to time domain. SpecTGNN operates on both an agent\ngraph with dynamic state information and an environment graph with the features\nextracted from context images in two streams. The model integrates graph\nFourier transform, spectral graph convolution and temporal gated convolution to\nencode history information and forecast future trajectories. Moreover, we\nincorporate a multi-head spatio-temporal attention mechanism to mitigate the\neffect of error propagation in a long time horizon. We demonstrate the\nperformance of SpecTGNN on two public trajectory prediction benchmark datasets,\nwhich achieves state-of-the-art performance in terms of prediction accuracy.\n",
        "published": "2021",
        "authors": [
            "Defu Cao",
            "Jiachen Li",
            "Hengbo Ma",
            "Masayoshi Tomizuka"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.03911v3",
        "title": "XIRL: Cross-embodiment Inverse Reinforcement Learning",
        "abstract": "  We investigate the visual cross-embodiment imitation setting, in which agents\nlearn policies from videos of other agents (such as humans) demonstrating the\nsame task, but with stark differences in their embodiments -- shape, actions,\nend-effector dynamics, etc. In this work, we demonstrate that it is possible to\nautomatically discover and learn vision-based reward functions from\ncross-embodiment demonstration videos that are robust to these differences.\nSpecifically, we present a self-supervised method for Cross-embodiment Inverse\nReinforcement Learning (XIRL) that leverages temporal cycle-consistency\nconstraints to learn deep visual embeddings that capture task progression from\noffline videos of demonstrations across multiple expert agents, each performing\nthe same task differently due to embodiment differences. Prior to our work,\nproducing rewards from self-supervised embeddings typically required alignment\nwith a reference trajectory, which may be difficult to acquire under stark\nembodiment differences. We show empirically that if the embeddings are aware of\ntask progress, simply taking the negative distance between the current state\nand goal state in the learned embedding space is useful as a reward for\ntraining policies with reinforcement learning. We find our learned reward\nfunction not only works for embodiments seen during training, but also\ngeneralizes to entirely new embodiments. Additionally, when transferring\nreal-world human demonstrations to a simulated robot, we find that XIRL is more\nsample efficient than current best methods. Qualitative results, code, and\ndatasets are available at https://x-irl.github.io\n",
        "published": "2021",
        "authors": [
            "Kevin Zakka",
            "Andy Zeng",
            "Pete Florence",
            "Jonathan Tompson",
            "Jeannette Bohg",
            "Debidatta Dwibedi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.05966v1",
        "title": "Learning by Watching",
        "abstract": "  When in a new situation or geographical location, human drivers have an\nextraordinary ability to watch others and learn maneuvers that they themselves\nmay have never performed. In contrast, existing techniques for learning to\ndrive preclude such a possibility as they assume direct access to an\ninstrumented ego-vehicle with fully known observations and expert driver\nactions. However, such measurements cannot be directly accessed for the non-ego\nvehicles when learning by watching others. Therefore, in an application where\ndata is regarded as a highly valuable asset, current approaches completely\ndiscard the vast portion of the training data that can be potentially obtained\nthrough indirect observation of surrounding vehicles. Motivated by this key\ninsight, we propose the Learning by Watching (LbW) framework which enables\nlearning a driving policy without requiring full knowledge of neither the state\nnor expert actions. To increase its data, i.e., with new perspectives and\nmaneuvers, LbW makes use of the demonstrations of other vehicles in a given\nscene by (1) transforming the ego-vehicle's observations to their points of\nview, and (2) inferring their expert actions. Our LbW agent learns more robust\ndriving policies while enabling data-efficient learning, including quick\nadaptation of the policy to rare and novel scenarios. In particular, LbW drives\nrobustly even with a fraction of available driving data required by existing\nmethods, achieving an average success rate of 92% on the original CARLA\nbenchmark with only 30 minutes of total driving data and 82% with only 10\nminutes.\n",
        "published": "2021",
        "authors": [
            "Jimuyang Zhang",
            "Eshed Ohn-Bar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.09678v1",
        "title": "SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual\n  Policies",
        "abstract": "  Generalization has been a long-standing challenge for reinforcement learning\n(RL). Visual RL, in particular, can be easily distracted by irrelevant factors\nin high-dimensional observation space. In this work, we consider robust policy\nlearning which targets zero-shot generalization to unseen visual environments\nwith large distributional shift. We propose SECANT, a novel self-expert cloning\ntechnique that leverages image augmentation in two stages to decouple robust\nrepresentation learning from policy optimization. Specifically, an expert\npolicy is first trained by RL from scratch with weak augmentations. A student\nnetwork then learns to mimic the expert policy by supervised learning with\nstrong augmentations, making its representation more robust against visual\nvariations compared to the expert. Extensive experiments demonstrate that\nSECANT significantly advances the state of the art in zero-shot generalization\nacross 4 challenging domains. Our average reward improvements over prior SOTAs\nare: DeepMind Control (+26.5%), robotic manipulation (+337.8%), vision-based\nautonomous driving (+47.7%), and indoor object navigation (+15.8%). Code\nrelease and video are available at https://linxifan.github.io/secant-site/.\n",
        "published": "2021",
        "authors": [
            "Linxi Fan",
            "Guanzhi Wang",
            "De-An Huang",
            "Zhiding Yu",
            "Li Fei-Fei",
            "Yuke Zhu",
            "Anima Anandkumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.12534v2",
        "title": "Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic\n  Manipulation via Discretisation",
        "abstract": "  We present a coarse-to-fine discretisation method that enables the use of\ndiscrete reinforcement learning approaches in place of unstable and\ndata-inefficient actor-critic methods in continuous robotics domains. This\napproach builds on the recently released ARM algorithm, which replaces the\ncontinuous next-best pose agent with a discrete one, with coarse-to-fine\nQ-attention. Given a voxelised scene, coarse-to-fine Q-attention learns what\npart of the scene to 'zoom' into. When this 'zooming' behaviour is applied\niteratively, it results in a near-lossless discretisation of the translation\nspace, and allows the use of a discrete action, deep Q-learning method. We show\nthat our new coarse-to-fine algorithm achieves state-of-the-art performance on\nseveral difficult sparsely rewarded RLBench vision-based robotics tasks, and\ncan train real-world policies, tabula rasa, in a matter of minutes, with as\nlittle as 3 demonstrations.\n",
        "published": "2021",
        "authors": [
            "Stephen James",
            "Kentaro Wada",
            "Tristan Laidlow",
            "Andrew J. Davison"
        ]
    }
]