[
    {
        "id": "http://arxiv.org/abs/2306.16927v1",
        "title": "End-to-end Autonomous Driving: Challenges and Frontiers",
        "abstract": "  The autonomous driving community has witnessed a rapid growth in approaches\nthat embrace an end-to-end algorithm framework, utilizing raw sensor input to\ngenerate vehicle motion plans, instead of concentrating on individual tasks\nsuch as detection and motion prediction. End-to-end systems, in comparison to\nmodular pipelines, benefit from joint feature optimization for perception and\nplanning. This field has flourished due to the availability of large-scale\ndatasets, closed-loop evaluation, and the increasing need for autonomous\ndriving algorithms to perform effectively in challenging scenarios. In this\nsurvey, we provide a comprehensive analysis of more than 250 papers, covering\nthe motivation, roadmap, methodology, challenges, and future trends in\nend-to-end autonomous driving. We delve into several critical challenges,\nincluding multi-modality, interpretability, causal confusion, robustness, and\nworld models, amongst others. Additionally, we discuss current advancements in\nfoundation models and visual pre-training, as well as how to incorporate these\ntechniques within the end-to-end driving framework. To facilitate future\nresearch, we maintain an active repository that contains up-to-date links to\nrelevant literature and open-source projects at\nhttps://github.com/OpenDriveLab/End-to-end-Autonomous-Driving.\n",
        "published": "2023",
        "authors": [
            "Li Chen",
            "Penghao Wu",
            "Kashyap Chitta",
            "Bernhard Jaeger",
            "Andreas Geiger",
            "Hongyang Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.17536v1",
        "title": "DisPlacing Objects: Improving Dynamic Vehicle Detection via Visual Place\n  Recognition under Adverse Conditions",
        "abstract": "  Can knowing where you are assist in perceiving objects in your surroundings,\nespecially under adverse weather and lighting conditions? In this work we\ninvestigate whether a prior map can be leveraged to aid in the detection of\ndynamic objects in a scene without the need for a 3D map or pixel-level\nmap-query correspondences. We contribute an algorithm which refines an initial\nset of candidate object detections and produces a refined subset of highly\naccurate detections using a prior map. We begin by using visual place\nrecognition (VPR) to retrieve a reference map image for a given query image,\nthen use a binary classification neural network that compares the query and\nmapping image regions to validate the query detection. Once our classification\nnetwork is trained, on approximately 1000 query-map image pairs, it is able to\nimprove the performance of vehicle detection when combined with an existing\noff-the-shelf vehicle detector. We demonstrate our approach using standard\ndatasets across two cities (Oxford and Zurich) under different settings of\ntrain-test separation of map-query traverse pairs. We further emphasize the\nperformance gains of our approach against alternative design choices and show\nthat VPR suffices for the task, eliminating the need for precise ground truth\nlocalization.\n",
        "published": "2023",
        "authors": [
            "Stephen Hausler",
            "Sourav Garg",
            "Punarjay Chakravarty",
            "Shubham Shrivastava",
            "Ankit Vora",
            "Michael Milford"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.00741v1",
        "title": "UnLoc: A Universal Localization Method for Autonomous Vehicles using\n  LiDAR, Radar and/or Camera Input",
        "abstract": "  Localization is a fundamental task in robotics for autonomous navigation.\nExisting localization methods rely on a single input data modality or train\nseveral computational models to process different modalities. This leads to\nstringent computational requirements and sub-optimal results that fail to\ncapitalize on the complementary information in other data streams. This paper\nproposes UnLoc, a novel unified neural modeling approach for localization with\nmulti-sensor input in all weather conditions. Our multi-stream network can\nhandle LiDAR, Camera and RADAR inputs for localization on demand, i.e., it can\nwork with one or more input sensors, making it robust to sensor failure. UnLoc\nuses 3D sparse convolutions and cylindrical partitioning of the space to\nprocess LiDAR frames and implements ResNet blocks with a slot attention-based\nfeature filtering module for the Radar and image modalities. We introduce a\nunique learnable modality encoding scheme to distinguish between the input\nsensor data. Our method is extensively evaluated on Oxford Radar RobotCar,\nApolloSouthBay and Perth-WA datasets. The results ascertain the efficacy of our\ntechnique.\n",
        "published": "2023",
        "authors": [
            "Muhammad Ibrahim",
            "Naveed Akhtar",
            "Saeed Anwar",
            "Ajmal Mian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.04370v2",
        "title": "Recent Advancements in End-to-End Autonomous Driving using Deep\n  Learning: A Survey",
        "abstract": "  End-to-End driving is a promising paradigm as it circumvents the drawbacks\nassociated with modular systems, such as their overwhelming complexity and\npropensity for error propagation. Autonomous driving transcends conventional\ntraffic patterns by proactively recognizing critical events in advance,\nensuring passengers' safety and providing them with comfortable transportation,\nparticularly in highly stochastic and variable traffic settings. This paper\npresents a comprehensive review of the End-to-End autonomous driving stack. It\nprovides a taxonomy of automated driving tasks wherein neural networks have\nbeen employed in an End-to-End manner, encompassing the entire driving process\nfrom perception to control, while addressing key challenges encountered in\nreal-world applications. Recent developments in End-to-End autonomous driving\nare analyzed, and research is categorized based on underlying principles,\nmethodologies, and core functionality. These categories encompass sensorial\ninput, main and auxiliary output, learning approaches ranging from imitation to\nreinforcement learning, and model evaluation techniques. The survey\nincorporates a detailed discussion of the explainability and safety aspects.\nFurthermore, it assesses the state-of-the-art, identifies challenges, and\nexplores future possibilities. We maintained the latest advancements and their\ncorresponding open-source implementations at\nhttps://github.com/Pranav-chib/Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning.\n",
        "published": "2023",
        "authors": [
            "Pranav Singh Chib",
            "Pravendra Singh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.13850v1",
        "title": "MAEA: Multimodal Attribution for Embodied AI",
        "abstract": "  Understanding multimodal perception for embodied AI is an open question\nbecause such inputs may contain highly complementary as well as redundant\ninformation for the task. A relevant direction for multimodal policies is\nunderstanding the global trends of each modality at the fusion layer. To this\nend, we disentangle the attributions for visual, language, and previous action\ninputs across different policies trained on the ALFRED dataset. Attribution\nanalysis can be utilized to rank and group the failure scenarios, investigate\nmodeling and dataset biases, and critically analyze multimodal EAI policies for\nrobustness and user trust before deployment. We present MAEA, a framework to\ncompute global attributions per modality of any differentiable policy. In\naddition, we show how attributions enable lower-level behavior analysis in EAI\npolicies for language and visual attributions.\n",
        "published": "2023",
        "authors": [
            "Vidhi Jain",
            "Jayant Sravan Tamarapalli",
            "Sahiti Yerramilli",
            "Yonatan Bisk"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.00377v1",
        "title": "Shape Completion with Prediction of Uncertain Regions",
        "abstract": "  Shape completion, i.e., predicting the complete geometry of an object from a\npartial observation, is highly relevant for several downstream tasks, most\nnotably robotic manipulation. When basing planning or prediction of real grasps\non object shape reconstruction, an indication of severe geometric uncertainty\nis indispensable. In particular, there can be an irreducible uncertainty in\nextended regions about the presence of entire object parts when given ambiguous\nobject views. To treat this important case, we propose two novel methods for\npredicting such uncertain regions as straightforward extensions of any method\nfor predicting local spatial occupancy, one through postprocessing occupancy\nscores, the other through direct prediction of an uncertainty indicator. We\ncompare these methods together with two known approaches to probabilistic shape\ncompletion. Moreover, we generate a dataset, derived from ShapeNet, of\nrealistically rendered depth images of object views with ground-truth\nannotations for the uncertain regions. We train on this dataset and test each\nmethod in shape completion and prediction of uncertain regions for known and\nnovel object instances and on synthetic and real data. While direct uncertainty\nprediction is by far the most accurate in the segmentation of uncertain\nregions, both novel methods outperform the two baselines in shape completion\nand uncertain region prediction, and avoiding the predicted uncertain regions\nincreases the quality of grasps for all tested methods. Web:\nhttps://github.com/DLR-RM/shape-completion\n",
        "published": "2023",
        "authors": [
            "Matthias Humt",
            "Dominik Winkelbauer",
            "Ulrich Hillenbrand"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.01471v1",
        "title": "Implicit Occupancy Flow Fields for Perception and Prediction in\n  Self-Driving",
        "abstract": "  A self-driving vehicle (SDV) must be able to perceive its surroundings and\npredict the future behavior of other traffic participants. Existing works\neither perform object detection followed by trajectory forecasting of the\ndetected objects, or predict dense occupancy and flow grids for the whole\nscene. The former poses a safety concern as the number of detections needs to\nbe kept low for efficiency reasons, sacrificing object recall. The latter is\ncomputationally expensive due to the high-dimensionality of the output grid,\nand suffers from the limited receptive field inherent to fully convolutional\nnetworks. Furthermore, both approaches employ many computational resources\npredicting areas or objects that might never be queried by the motion planner.\nThis motivates our unified approach to perception and future prediction that\nimplicitly represents occupancy and flow over time with a single neural\nnetwork. Our method avoids unnecessary computation, as it can be directly\nqueried by the motion planner at continuous spatio-temporal locations.\nMoreover, we design an architecture that overcomes the limited receptive field\nof previous explicit occupancy prediction methods by adding an efficient yet\neffective global attention mechanism. Through extensive experiments in both\nurban and highway settings, we demonstrate that our implicit model outperforms\nthe current state-of-the-art. For more information, visit the project website:\nhttps://waabi.ai/research/implicito.\n",
        "published": "2023",
        "authors": [
            "Ben Agro",
            "Quinlan Sykora",
            "Sergio Casas",
            "Raquel Urtasun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.07439v2",
        "title": "Interaction-Aware Personalized Vehicle Trajectory Prediction Using\n  Temporal Graph Neural Networks",
        "abstract": "  Accurate prediction of vehicle trajectories is vital for advanced driver\nassistance systems and autonomous vehicles. Existing methods mainly rely on\ngeneric trajectory predictions derived from large datasets, overlooking the\npersonalized driving patterns of individual drivers. To address this gap, we\npropose an approach for interaction-aware personalized vehicle trajectory\nprediction that incorporates temporal graph neural networks. Our method\nutilizes Graph Convolution Networks (GCN) and Long Short-Term Memory (LSTM) to\nmodel the spatio-temporal interactions between target vehicles and their\nsurrounding traffic. To personalize the predictions, we establish a pipeline\nthat leverages transfer learning: the model is initially pre-trained on a\nlarge-scale trajectory dataset and then fine-tuned for each driver using their\nspecific driving data. We employ human-in-the-loop simulation to collect\npersonalized naturalistic driving trajectories and corresponding surrounding\nvehicle trajectories. Experimental results demonstrate the superior performance\nof our personalized GCN-LSTM model, particularly for longer prediction\nhorizons, compared to its generic counterpart. Moreover, the personalized model\noutperforms individual models created without pre-training, emphasizing the\nsignificance of pre-training on a large dataset to avoid overfitting. By\nincorporating personalization, our approach enhances trajectory prediction\naccuracy.\n",
        "published": "2023",
        "authors": [
            "Amr Abdelraouf",
            "Rohit Gupta",
            "Kyungtae Han"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.07748v1",
        "title": "Exploiting Sparsity in Automotive Radar Object Detection Networks",
        "abstract": "  Having precise perception of the environment is crucial for ensuring the\nsecure and reliable functioning of autonomous driving systems. Radar object\ndetection networks are one fundamental part of such systems. CNN-based object\ndetectors showed good performance in this context, but they require large\ncompute resources. This paper investigates sparse convolutional object\ndetection networks, which combine powerful grid-based detection with low\ncompute resources. We investigate radar specific challenges and propose sparse\nkernel point pillars (SKPP) and dual voxel point convolutions (DVPC) as\nremedies for the grid rendering and sparse backbone architectures. We evaluate\nour SKPP-DPVCN architecture on nuScenes, which outperforms the baseline by\n5.89% and the previous state of the art by 4.19% in Car AP4.0. Moreover,\nSKPP-DPVCN reduces the average scale error (ASE) by 21.41% over the baseline.\n",
        "published": "2023",
        "authors": [
            "Marius Lippke",
            "Maurice Quach",
            "Sascha Braun",
            "Daniel K\u00f6hler",
            "Michael Ulrich",
            "Bastian Bischoff",
            "Wei Yap Tan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.10135v1",
        "title": "A Review on Objective-Driven Artificial Intelligence",
        "abstract": "  While advancing rapidly, Artificial Intelligence still falls short of human\nintelligence in several key aspects due to inherent limitations in current AI\ntechnologies and our understanding of cognition. Humans have an innate ability\nto understand context, nuances, and subtle cues in communication, which allows\nus to comprehend jokes, sarcasm, and metaphors. Machines struggle to interpret\nsuch contextual information accurately. Humans possess a vast repository of\ncommon-sense knowledge that helps us make logical inferences and predictions\nabout the world. Machines lack this innate understanding and often struggle\nwith making sense of situations that humans find trivial. In this article, we\nreview the prospective Machine Intelligence candidates, a review from Prof.\nYann LeCun, and other work that can help close this gap between human and\nmachine intelligence. Specifically, we talk about what's lacking with the\ncurrent AI techniques such as supervised learning, reinforcement learning,\nself-supervised learning, etc. Then we show how Hierarchical planning-based\napproaches can help us close that gap and deep-dive into energy-based,\nlatent-variable methods and Joint embedding predictive architecture methods.\n",
        "published": "2023",
        "authors": [
            "Apoorv Singh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.16484v2",
        "title": "Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning",
        "abstract": "  Affordable 3D scanners often produce sparse and non-uniform point clouds that\nnegatively impact downstream applications in robotic systems. While existing\npoint cloud upsampling architectures have demonstrated promising results on\nstandard benchmarks, they tend to experience significant performance drops when\nthe test data have different distributions from the training data. To address\nthis issue, this paper proposes a test-time adaption approach to enhance model\ngenerality of point cloud upsampling. The proposed approach leverages\nmeta-learning to explicitly learn network parameters for test-time adaption.\nOur method does not require any prior information about the test data. During\nmeta-training, the model parameters are learned from a collection of\ninstance-level tasks, each of which consists of a sparse-dense pair of point\nclouds from the training data. During meta-testing, the trained model is\nfine-tuned with a few gradient updates to produce a unique set of network\nparameters for each test instance. The updated model is then used for the final\nprediction. Our framework is generic and can be applied in a plug-and-play\nmanner with existing backbone networks in point cloud upsampling. Extensive\nexperiments demonstrate that our approach improves the performance of\nstate-of-the-art models.\n",
        "published": "2023",
        "authors": [
            "Ahmed Hatem",
            "Yiming Qian",
            "Yang Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.16893v1",
        "title": "Language-Conditioned Path Planning",
        "abstract": "  Contact is at the core of robotic manipulation. At times, it is desired (e.g.\nmanipulation and grasping), and at times, it is harmful (e.g. when avoiding\nobstacles). However, traditional path planning algorithms focus solely on\ncollision-free paths, limiting their applicability in contact-rich tasks. To\naddress this limitation, we propose the domain of Language-Conditioned Path\nPlanning, where contact-awareness is incorporated into the path planning\nproblem. As a first step in this domain, we propose Language-Conditioned\nCollision Functions (LACO) a novel approach that learns a collision function\nusing only a single-view image, language prompt, and robot configuration. LACO\npredicts collisions between the robot and the environment, enabling flexible,\nconditional path planning without the need for manual object annotations, point\ncloud data, or ground-truth object meshes. In both simulation and the real\nworld, we demonstrate that LACO can facilitate complex, nuanced path plans that\nallow for interaction with objects that are safe to collide, rather than\nprohibiting any collision.\n",
        "published": "2023",
        "authors": [
            "Amber Xie",
            "Youngwoon Lee",
            "Pieter Abbeel",
            "Stephen James"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.00957v1",
        "title": "Visual-Kinematics Graph Learning for Procedure-agnostic Instrument Tip\n  Segmentation in Robotic Surgeries",
        "abstract": "  Accurate segmentation of surgical instrument tip is an important task for\nenabling downstream applications in robotic surgery, such as surgical skill\nassessment, tool-tissue interaction and deformation modeling, as well as\nsurgical autonomy. However, this task is very challenging due to the small\nsizes of surgical instrument tips, and significant variance of surgical scenes\nacross different procedures. Although much effort has been made on visual-based\nmethods, existing segmentation models still suffer from low robustness thus not\nusable in practice. Fortunately, kinematics data from the robotic system can\nprovide reliable prior for instrument location, which is consistent regardless\nof different surgery types. To make use of such multi-modal information, we\npropose a novel visual-kinematics graph learning framework to accurately\nsegment the instrument tip given various surgical procedures. Specifically, a\ngraph learning framework is proposed to encode relational features of\ninstrument parts from both image and kinematics. Next, a cross-modal\ncontrastive loss is designed to incorporate robust geometric prior from\nkinematics to image for tip segmentation. We have conducted experiments on a\nprivate paired visual-kinematics dataset including multiple procedures, i.e.,\nprostatectomy, total mesorectal excision, fundoplication and distal gastrectomy\non cadaver, and distal gastrectomy on porcine. The leave-one-procedure-out\ncross validation demonstrated that our proposed multi-modal segmentation method\nsignificantly outperformed current image-based state-of-the-art approaches,\nexceeding averagely 11.2% on Dice.\n",
        "published": "2023",
        "authors": [
            "Jiaqi Liu",
            "Yonghao Long",
            "Kai Chen",
            "Cheuk Hei Leung",
            "Zerui Wang",
            "Qi Dou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.03113v1",
        "title": "Detecting Manufacturing Defects in PCBs via Data-Centric Machine\n  Learning on Solder Paste Inspection Features",
        "abstract": "  Automated detection of defects in Printed Circuit Board (PCB) manufacturing\nusing Solder Paste Inspection (SPI) and Automated Optical Inspection (AOI)\nmachines can help improve operational efficiency and significantly reduce the\nneed for manual intervention. In this paper, using SPI-extracted features of 6\nmillion pins, we demonstrate a data-centric approach to train Machine Learning\n(ML) models to detect PCB defects at three stages of PCB manufacturing. The 6\nmillion PCB pins correspond to 2 million components that belong to 15,387 PCBs.\nUsing a base extreme gradient boosting (XGBoost) ML model, we iterate on the\ndata pre-processing step to improve detection performance. Combining pin-level\nSPI features using component and PCB IDs, we developed training instances also\nat the component and PCB level. This allows the ML model to capture any\ninter-pin, inter-component, or spatial effects that may not be apparent at the\npin level. Models are trained at the pin, component, and PCB levels, and the\ndetection results from the different models are combined to identify defective\ncomponents.\n",
        "published": "2023",
        "authors": [
            "Jubilee Prasad-Rao",
            "Roohollah Heidary",
            "Jesse Williams"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.05665v2",
        "title": "Robot Parkour Learning",
        "abstract": "  Parkour is a grand challenge for legged locomotion that requires robots to\novercome various obstacles rapidly in complex environments. Existing methods\ncan generate either diverse but blind locomotion skills or vision-based but\nspecialized skills by using reference animal data or complex rewards. However,\nautonomous parkour requires robots to learn generalizable skills that are both\nvision-based and diverse to perceive and react to various scenarios. In this\nwork, we propose a system for learning a single end-to-end vision-based parkour\npolicy of diverse parkour skills using a simple reward without any reference\nmotion data. We develop a reinforcement learning method inspired by direct\ncollocation to generate parkour skills, including climbing over high obstacles,\nleaping over large gaps, crawling beneath low barriers, squeezing through thin\nslits, and running. We distill these skills into a single vision-based parkour\npolicy and transfer it to a quadrupedal robot using its egocentric depth\ncamera. We demonstrate that our system can empower two different low-cost\nrobots to autonomously select and execute appropriate parkour skills to\ntraverse challenging real-world environments.\n",
        "published": "2023",
        "authors": [
            "Ziwen Zhuang",
            "Zipeng Fu",
            "Jianren Wang",
            "Christopher Atkeson",
            "Soeren Schwertfeger",
            "Chelsea Finn",
            "Hang Zhao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.06597v2",
        "title": "Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and\n  Reasoning",
        "abstract": "  The widespread adoption of commercial autonomous vehicles (AVs) and advanced\ndriver assistance systems (ADAS) may largely depend on their acceptance by\nsociety, for which their perceived trustworthiness and interpretability to\nriders are crucial. In general, this task is challenging because modern\nautonomous systems software relies heavily on black-box artificial intelligence\nmodels. Towards this goal, this paper introduces a novel dataset, Rank2Tell, a\nmulti-modal ego-centric dataset for Ranking the importance level and Telling\nthe reason for the importance. Using various close and open-ended visual\nquestion answering, the dataset provides dense annotations of various semantic,\nspatial, temporal, and relational attributes of various important objects in\ncomplex traffic scenarios. The dense annotations and unique attributes of the\ndataset make it a valuable resource for researchers working on visual scene\nunderstanding and related fields. Furthermore, we introduce a joint model for\njoint importance level ranking and natural language captions generation to\nbenchmark our dataset and demonstrate performance with quantitative\nevaluations.\n",
        "published": "2023",
        "authors": [
            "Enna Sachdeva",
            "Nakul Agarwal",
            "Suhas Chundi",
            "Sean Roelofs",
            "Jiachen Li",
            "Mykel Kochenderfer",
            "Chiho Choi",
            "Behzad Dariush"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.07808v1",
        "title": "What Matters to Enhance Traffic Rule Compliance of Imitation Learning\n  for Automated Driving",
        "abstract": "  More research attention has recently been given to end-to-end autonomous\ndriving technologies where the entire driving pipeline is replaced with a\nsingle neural network because of its simpler structure and faster inference\ntime. Despite this appealing approach largely reducing the components in\ndriving pipeline, its simplicity also leads to interpretability problems and\nsafety issues arXiv:2003.06404. The trained policy is not always compliant with\nthe traffic rules and it is also hard to discover the reason for the\nmisbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are\nalso critical to autonomous driving's security and feasibility to perceive the\nsurrounding environment under complex driving scenarios. In this paper, we\nproposed P-CSG, a novel penalty-based imitation learning approach with cross\nsemantics generation sensor fusion technologies to increase the overall\nperformance of End-to-End Autonomous Driving. We conducted an assessment of our\nmodel's performance using the Town 05 Long benchmark, achieving an impressive\ndriving score improvement of over 15%. Furthermore, we conducted robustness\nevaluations against adversarial attacks like FGSM and Dot attacks, revealing a\nsubstantial increase in robustness compared to baseline models.More detailed\ninformation, such as code-based resources, ablation studies and videos can be\nfound at https://hk-zh.github.io/p-csg-plus.\n",
        "published": "2023",
        "authors": [
            "Hongkuan Zhou",
            "Aifen Sui",
            "Wei Cao",
            "Letian Shi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.08086v1",
        "title": "Fast and Accurate Deep Loop Closing and Relocalization for Reliable\n  LiDAR SLAM",
        "abstract": "  Loop closing and relocalization are crucial techniques to establish reliable\nand robust long-term SLAM by addressing pose estimation drift and degeneration.\nThis article begins by formulating loop closing and relocalization within a\nunified framework. Then, we propose a novel multi-head network LCR-Net to\ntackle both tasks effectively. It exploits novel feature extraction and\npose-aware attention mechanism to precisely estimate similarities and 6-DoF\nposes between pairs of LiDAR scans. In the end, we integrate our LCR-Net into a\nSLAM system and achieve robust and accurate online LiDAR SLAM in outdoor\ndriving environments. We thoroughly evaluate our LCR-Net through three setups\nderived from loop closing and relocalization, including candidate retrieval,\nclosed-loop point cloud registration, and continuous relocalization using\nmultiple datasets. The results demonstrate that LCR-Net excels in all three\ntasks, surpassing the state-of-the-art methods and exhibiting a remarkable\ngeneralization ability. Notably, our LCR-Net outperforms baseline methods\nwithout using a time-consuming robust pose estimator, rendering it suitable for\nonline SLAM applications. To our best knowledge, the integration of LCR-Net\nyields the first LiDAR SLAM with the capability of deep loop closing and\nrelocalization. The implementation of our methods will be made open-source.\n",
        "published": "2023",
        "authors": [
            "Chenghao Shi",
            "Xieyuanli Chen",
            "Junhao Xiao",
            "Bin Dai",
            "Huimin Lu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.09737v3",
        "title": "RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud",
        "abstract": "  Mobile autonomy relies on the precise perception of dynamic environments.\nRobustly tracking moving objects in 3D world thus plays a pivotal role for\napplications like trajectory prediction, obstacle avoidance, and path planning.\nWhile most current methods utilize LiDARs or cameras for Multiple Object\nTracking (MOT), the capabilities of 4D imaging radars remain largely\nunexplored. Recognizing the challenges posed by radar noise and point sparsity\nin 4D radar data, we introduce RaTrack, an innovative solution tailored for\nradar-based tracking. Bypassing the typical reliance on specific object types\nand 3D bounding boxes, our method focuses on motion segmentation and\nclustering, enriched by a motion estimation module. Evaluated on the\nView-of-Delft dataset, RaTrack showcases superior tracking precision of moving\nobjects, largely surpassing the performance of the state of the art.\n",
        "published": "2023",
        "authors": [
            "Zhijun Pan",
            "Fangqiang Ding",
            "Hantao Zhong",
            "Chris Xiaoxuan Lu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.09844v1",
        "title": "CC-SGG: Corner Case Scenario Generation using Learned Scene Graphs",
        "abstract": "  Corner case scenarios are an essential tool for testing and validating the\nsafety of autonomous vehicles (AVs). As these scenarios are often\ninsufficiently present in naturalistic driving datasets, augmenting the data\nwith synthetic corner cases greatly enhances the safe operation of AVs in\nunique situations. However, the generation of synthetic, yet realistic, corner\ncases poses a significant challenge. In this work, we introduce a novel\napproach based on Heterogeneous Graph Neural Networks (HGNNs) to transform\nregular driving scenarios into corner cases. To achieve this, we first generate\nconcise representations of regular driving scenes as scene graphs, minimally\nmanipulating their structure and properties. Our model then learns to perturb\nthose graphs to generate corner cases using attention and triple embeddings.\nThe input and perturbed graphs are then imported back into the simulation to\ngenerate corner case scenarios. Our model successfully learned to produce\ncorner cases from input scene graphs, achieving 89.9% prediction accuracy on\nour testing dataset. We further validate the generated scenarios on baseline\nautonomous driving methods, demonstrating our model's ability to effectively\ncreate critical situations for the baselines.\n",
        "published": "2023",
        "authors": [
            "George Drayson",
            "Efimia Panagiotaki",
            "Daniel Omeiza",
            "Lars Kunze"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.09979v2",
        "title": "General In-Hand Object Rotation with Vision and Touch",
        "abstract": "  We introduce RotateIt, a system that enables fingertip-based object rotation\nalong multiple axes by leveraging multimodal sensory inputs. Our system is\ntrained in simulation, where it has access to ground-truth object shapes and\nphysical properties. Then we distill it to operate on realistic yet noisy\nsimulated visuotactile and proprioceptive sensory inputs. These multimodal\ninputs are fused via a visuotactile transformer, enabling online inference of\nobject shapes and physical properties during deployment. We show significant\nperformance improvements over prior methods and the importance of visual and\ntactile sensing.\n",
        "published": "2023",
        "authors": [
            "Haozhi Qi",
            "Brent Yi",
            "Sudharshan Suresh",
            "Mike Lambeta",
            "Yi Ma",
            "Roberto Calandra",
            "Jitendra Malik"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.10790v2",
        "title": "Guide Your Agent with Adaptive Multimodal Rewards",
        "abstract": "  Developing an agent capable of adapting to unseen environments remains a\ndifficult challenge in imitation learning. This work presents Adaptive\nReturn-conditioned Policy (ARP), an efficient framework designed to enhance the\nagent's generalization ability using natural language task descriptions and\npre-trained multimodal encoders. Our key idea is to calculate a similarity\nbetween visual observations and natural language instructions in the\npre-trained multimodal embedding space (such as CLIP) and use it as a reward\nsignal. We then train a return-conditioned policy using expert demonstrations\nlabeled with multimodal rewards. Because the multimodal rewards provide\nadaptive signals at each timestep, our ARP effectively mitigates the goal\nmisgeneralization. This results in superior generalization performances even\nwhen faced with unseen text instructions, compared to existing text-conditioned\npolicies. To improve the quality of rewards, we also introduce a fine-tuning\nmethod for pre-trained multimodal encoders, further enhancing the performance.\nVideo demonstrations and source code are available on the project website:\n\\url{https://sites.google.com/view/2023arp}.\n",
        "published": "2023",
        "authors": [
            "Changyeon Kim",
            "Younggyo Seo",
            "Hao Liu",
            "Lisa Lee",
            "Jinwoo Shin",
            "Honglak Lee",
            "Kimin Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.11648v1",
        "title": "Orbital AI-based Autonomous Refuelling Solution",
        "abstract": "  Cameras are rapidly becoming the choice for on-board sensors towards space\nrendezvous due to their small form factor and inexpensive power, mass, and\nvolume costs. When it comes to docking, however, they typically serve a\nsecondary role, whereas the main work is done by active sensors such as lidar.\nThis paper documents the development of a proposed AI-based (artificial\nintelligence) navigation algorithm intending to mature the use of on-board\nvisible wavelength cameras as a main sensor for docking and on-orbit servicing\n(OOS), reducing the dependency on lidar and greatly reducing costs.\nSpecifically, the use of AI enables the expansion of the relative navigation\nsolution towards multiple classes of scenarios, e.g., in terms of targets or\nillumination conditions, which would otherwise have to be crafted on a\ncase-by-case manner using classical image processing methods. Multiple\nconvolutional neural network (CNN) backbone architectures are benchmarked on\nsynthetically generated data of docking manoeuvres with the International Space\nStation (ISS), achieving position and attitude estimates close to 1%\nrange-normalised and 1 deg, respectively. The integration of the solution with\na physical prototype of the refuelling mechanism is validated in laboratory\nusing a robotic arm to simulate a berthing procedure.\n",
        "published": "2023",
        "authors": [
            "Duarte Rondao",
            "Lei He",
            "Nabil Aouf"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.12295v2",
        "title": "Learning to Drive Anywhere",
        "abstract": "  Human drivers can seamlessly adapt their driving decisions across\ngeographical locations with diverse conditions and rules of the road, e.g.,\nleft vs. right-hand traffic. In contrast, existing models for autonomous\ndriving have been thus far only deployed within restricted operational domains,\ni.e., without accounting for varying driving behaviors across locations or\nmodel scalability. In this work, we propose AnyD, a single geographically-aware\nconditional imitation learning (CIL) model that can efficiently learn from\nheterogeneous and globally distributed data with dynamic environmental,\ntraffic, and social characteristics. Our key insight is to introduce a\nhigh-capacity geo-location-based channel attention mechanism that effectively\nadapts to local nuances while also flexibly modeling similarities among regions\nin a data-driven manner. By optimizing a contrastive imitation objective, our\nproposed approach can efficiently scale across inherently imbalanced data\ndistributions and location-dependent events. We demonstrate the benefits of our\nAnyD agent across multiple datasets, cities, and scalable deployment paradigms,\ni.e., centralized, semi-supervised, and distributed agent training.\nSpecifically, AnyD outperforms CIL baselines by over 14% in open-loop\nevaluation and 30% in closed-loop testing on CARLA.\n",
        "published": "2023",
        "authors": [
            "Ruizhao Zhu",
            "Peng Huang",
            "Eshed Ohn-Bar",
            "Venkatesh Saligrama"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.12300v1",
        "title": "See to Touch: Learning Tactile Dexterity through Visual Incentives",
        "abstract": "  Equipping multi-fingered robots with tactile sensing is crucial for achieving\nthe precise, contact-rich, and dexterous manipulation that humans excel at.\nHowever, relying solely on tactile sensing fails to provide adequate cues for\nreasoning about objects' spatial configurations, limiting the ability to\ncorrect errors and adapt to changing situations. In this paper, we present\nTactile Adaptation from Visual Incentives (TAVI), a new framework that enhances\ntactile-based dexterity by optimizing dexterous policies using vision-based\nrewards. First, we use a contrastive-based objective to learn visual\nrepresentations. Next, we construct a reward function using these visual\nrepresentations through optimal-transport based matching on one human\ndemonstration. Finally, we use online reinforcement learning on our robot to\noptimize tactile-based policies that maximize the visual reward. On six\nchallenging tasks, such as peg pick-and-place, unstacking bowls, and flipping\nslender objects, TAVI achieves a success rate of 73% using our four-fingered\nAllegro robot hand. The increase in performance is 108% higher than policies\nusing tactile and vision-based rewards and 135% higher than policies without\ntactile observational input. Robot videos are best viewed on our project\nwebsite: https://see-to-touch.github.io/.\n",
        "published": "2023",
        "authors": [
            "Irmak Guzey",
            "Yinlong Dai",
            "Ben Evans",
            "Soumith Chintala",
            "Lerrel Pinto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.12312v2",
        "title": "ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals",
        "abstract": "  We present ForceSight, a system for text-guided mobile manipulation that\npredicts visual-force goals using a deep neural network. Given a single RGBD\nimage combined with a text prompt, ForceSight determines a target end-effector\npose in the camera frame (kinematic goal) and the associated forces (force\ngoal). Together, these two components form a visual-force goal. Prior work has\ndemonstrated that deep models outputting human-interpretable kinematic goals\ncan enable dexterous manipulation by real robots. Forces are critical to\nmanipulation, yet have typically been relegated to lower-level execution in\nthese systems. When deployed on a mobile manipulator equipped with an\neye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps,\ndrawer opening, and object handovers with an 81% success rate in unseen\nenvironments with object instances that differed significantly from the\ntraining data. In a separate experiment, relying exclusively on visual servoing\nand ignoring force goals dropped the success rate from 90% to 45%,\ndemonstrating that force goals can significantly enhance performance. The\nappendix, videos, code, and trained models are available at\nhttps://force-sight.github.io/.\n",
        "published": "2023",
        "authors": [
            "Jeremy A. Collins",
            "Cody Houff",
            "You Liang Tan",
            "Charles C. Kemp"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.13181v1",
        "title": "Diagnosing and exploiting the computational demands of videos games for\n  deep reinforcement learning",
        "abstract": "  Humans learn by interacting with their environments and perceiving the\noutcomes of their actions. A landmark in artificial intelligence has been the\ndevelopment of deep reinforcement learning (dRL) algorithms capable of doing\nthe same in video games, on par with or better than humans. However, it remains\nunclear whether the successes of dRL models reflect advances in visual\nrepresentation learning, the effectiveness of reinforcement learning algorithms\nat discovering better policies, or both. To address this question, we introduce\nthe Learning Challenge Diagnosticator (LCD), a tool that separately measures\nthe perceptual and reinforcement learning demands of a task. We use LCD to\ndiscover a novel taxonomy of challenges in the Procgen benchmark, and\ndemonstrate that these predictions are both highly reliable and can instruct\nalgorithmic development. More broadly, the LCD reveals multiple failure cases\nthat can occur when optimizing dRL algorithms over entire video game benchmarks\nlike Procgen, and provides a pathway towards more efficient progress.\n",
        "published": "2023",
        "authors": [
            "Lakshmi Narasimhan Govindarajan",
            "Rex G Liu",
            "Drew Linsley",
            "Alekh Karkada Ashok",
            "Max Reuter",
            "Michael J Frank",
            "Thomas Serre"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.14236v1",
        "title": "MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation",
        "abstract": "  Robotic systems that aspire to operate in uninstrumented real-world\nenvironments must perceive the world directly via onboard sensing. Vision-based\nlearning systems aim to eliminate the need for environment instrumentation by\nbuilding an implicit understanding of the world based on raw pixels, but\nnavigating the contact-rich high-dimensional search space from solely sparse\nvisual reward signals significantly exacerbates the challenge of exploration.\nThe applicability of such systems is thus typically restricted to simulated or\nheavily engineered environments since agent exploration in the real-world\nwithout the guidance of explicit state estimation and dense rewards can lead to\nunsafe behavior and safety faults that are catastrophic. In this study, we\nisolate the root causes behind these limitations to develop a system, called\nMoDem-V2, capable of learning contact-rich manipulation directly in the\nuninstrumented real world. Building on the latest algorithmic advancements in\nmodel-based reinforcement learning (MBRL), demo-bootstrapping, and effective\nexploration, MoDem-V2 can acquire contact-rich dexterous manipulation skills\ndirectly in the real world. We identify key ingredients for leveraging\ndemonstrations in model learning while respecting real-world safety\nconsiderations -- exploration centering, agency handover, and actor-critic\nensembles. We empirically demonstrate the contribution of these ingredients in\nfour complex visuo-motor manipulation problems in both simulation and the real\nworld. To the best of our knowledge, our work presents the first successful\nsystem for demonstration-augmented visual MBRL trained directly in the real\nworld. Visit https://sites.google.com/view/modem-v2 for videos and more\ndetails.\n",
        "published": "2023",
        "authors": [
            "Patrick Lancaster",
            "Nicklas Hansen",
            "Aravind Rajeswaran",
            "Vikash Kumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.15278v1",
        "title": "Out of Sight, Still in Mind: Reasoning and Planning about Unobserved\n  Objects with Video Tracking Enabled Memory Models",
        "abstract": "  Robots need to have a memory of previously observed, but currently occluded\nobjects to work reliably in realistic environments. We investigate the problem\nof encoding object-oriented memory into a multi-object manipulation reasoning\nand planning framework. We propose DOOM and LOOM, which leverage transformer\nrelational dynamics to encode the history of trajectories given partial-view\npoint clouds and an object discovery and tracking engine. Our approaches can\nperform multiple challenging tasks including reasoning with occluded objects,\nnovel objects appearance, and object reappearance. Throughout our extensive\nsimulation and real-world experiments, we find that our approaches perform well\nin terms of different numbers of objects and different numbers of distractor\nactions. Furthermore, we show our approaches outperform an implicit memory\nbaseline.\n",
        "published": "2023",
        "authors": [
            "Yixuan Huang",
            "Jialin Yuan",
            "Chanho Kim",
            "Pupul Pradhan",
            "Bryan Chen",
            "Li Fuxin",
            "Tucker Hermans"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.15302v2",
        "title": "STERLING: Self-Supervised Terrain Representation Learning from\n  Unconstrained Robot Experience",
        "abstract": "  Terrain awareness, i.e., the ability to identify and distinguish different\ntypes of terrain, is a critical ability that robots must have to succeed at\nautonomous off-road navigation. Current approaches that provide robots with\nthis awareness either rely on labeled data which is expensive to collect,\nengineered features and cost functions that may not generalize, or expert human\ndemonstrations which may not be available. Towards endowing robots with terrain\nawareness without these limitations, we introduce Self-supervised TErrain\nRepresentation LearnING (STERLING), a novel approach for learning terrain\nrepresentations that relies solely on easy-to-collect, unconstrained (e.g.,\nnon-expert), and unlabelled robot experience, with no additional constraints on\ndata collection. STERLING employs a novel multi-modal self-supervision\nobjective through non-contrastive representation learning to learn relevant\nterrain representations for terrain-aware navigation. Through physical robot\nexperiments in off-road environments, we evaluate STERLING features on the task\nof preference-aligned visual navigation and find that STERLING features perform\non par with fully supervised approaches and outperform other state-of-the-art\nmethods with respect to preference alignment. Additionally, we perform a\nlarge-scale experiment of autonomously hiking a 3-mile long trail which\nSTERLING completes successfully with only two manual interventions,\ndemonstrating its robustness to real-world off-road conditions.\n",
        "published": "2023",
        "authors": [
            "Haresh Karnan",
            "Elvin Yang",
            "Daniel Farkash",
            "Garrett Warnell",
            "Joydeep Biswas",
            "Peter Stone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.16534v1",
        "title": "MotionLM: Multi-Agent Motion Forecasting as Language Modeling",
        "abstract": "  Reliable forecasting of the future behavior of road agents is a critical\ncomponent to safe planning in autonomous vehicles. Here, we represent\ncontinuous trajectories as sequences of discrete motion tokens and cast\nmulti-agent motion prediction as a language modeling task over this domain. Our\nmodel, MotionLM, provides several advantages: First, it does not require\nanchors or explicit latent variable optimization to learn multimodal\ndistributions. Instead, we leverage a single standard language modeling\nobjective, maximizing the average log probability over sequence tokens. Second,\nour approach bypasses post-hoc interaction heuristics where individual agent\ntrajectory generation is conducted prior to interactive scoring. Instead,\nMotionLM produces joint distributions over interactive agent futures in a\nsingle autoregressive decoding process. In addition, the model's sequential\nfactorization enables temporally causal conditional rollouts. The proposed\napproach establishes new state-of-the-art performance for multi-agent motion\nprediction on the Waymo Open Motion Dataset, ranking 1st on the interactive\nchallenge leaderboard.\n",
        "published": "2023",
        "authors": [
            "Ari Seff",
            "Brian Cera",
            "Dian Chen",
            "Mason Ng",
            "Aurick Zhou",
            "Nigamaa Nayakanti",
            "Khaled S. Refaat",
            "Rami Al-Rfou",
            "Benjamin Sapp"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.17338v2",
        "title": "Improving Trajectory Prediction in Dynamic Multi-Agent Environment by\n  Dropping Waypoints",
        "abstract": "  The inherently diverse and uncertain nature of trajectories presents a\nformidable challenge in accurately modeling them. Motion prediction systems\nmust effectively learn spatial and temporal information from the past to\nforecast the future trajectories of the agent. Many existing methods learn\ntemporal motion via separate components within stacked models to capture\ntemporal features. Furthermore, prediction methods often operate under the\nassumption that observed trajectory waypoint sequences are complete,\ndisregarding scenarios where missing values may occur, which can influence\ntheir performance. Moreover, these models may be biased toward particular\nwaypoint sequences when making predictions. We propose a novel approach called\nTemporal Waypoint Dropping (TWD) that explicitly incorporates temporal\ndependencies during the training of a trajectory prediction model. By\nstochastically dropping waypoints from past observed trajectories, the model is\nforced to learn the underlying temporal representation from the remaining\nwaypoints, resulting in an improved model. Incorporating stochastic temporal\nwaypoint dropping into the model learning process significantly enhances its\nperformance in scenarios with missing values. Experimental results demonstrate\nour approach's substantial improvement in trajectory prediction capabilities.\nOur approach can complement existing trajectory prediction methods to improve\ntheir prediction accuracy. We evaluate our proposed approach on three datasets:\nNBA Sports VU, ETH-UCY, and TrajNet++.\n",
        "published": "2023",
        "authors": [
            "Pranav Singh Chib",
            "Pravendra Singh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.02219v1",
        "title": "What do we learn from a large-scale study of pre-trained visual\n  representations in sim and real environments?",
        "abstract": "  We present a large empirical investigation on the use of pre-trained visual\nrepresentations (PVRs) for training downstream policies that execute real-world\ntasks. Our study spans five different PVRs, two different policy-learning\nparadigms (imitation and reinforcement learning), and three different robots\nfor 5 distinct manipulation and indoor navigation tasks. From this effort, we\ncan arrive at three insights: 1) the performance trends of PVRs in the\nsimulation are generally indicative of their trends in the real world, 2) the\nuse of PVRs enables a first-of-its-kind result with indoor ImageNav (zero-shot\ntransfer to a held-out scene in the real world), and 3) the benefits from\nvariations in PVRs, primarily data-augmentation and fine-tuning, also transfer\nto the real-world performance. See project website for additional details and\nvisuals.\n",
        "published": "2023",
        "authors": [
            "Sneha Silwal",
            "Karmesh Yadav",
            "Tingfan Wu",
            "Jay Vakil",
            "Arjun Majumdar",
            "Sergio Arnaud",
            "Claire Chen",
            "Vincent-Pierre Berges",
            "Dhruv Batra",
            "Aravind Rajeswaran",
            "Mrinal Kalakrishnan",
            "Franziska Meier",
            "Oleksandr Maksymets"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.03023v1",
        "title": "Human-oriented Representation Learning for Robotic Manipulation",
        "abstract": "  Humans inherently possess generalizable visual representations that empower\nthem to efficiently explore and interact with the environments in manipulation\ntasks. We advocate that such a representation automatically arises from\nsimultaneously learning about multiple simple perceptual skills that are\ncritical for everyday scenarios (e.g., hand detection, state estimate, etc.)\nand is better suited for learning robot manipulation policies compared to\ncurrent state-of-the-art visual representations purely based on self-supervised\nobjectives. We formalize this idea through the lens of human-oriented\nmulti-task fine-tuning on top of pre-trained visual encoders, where each task\nis a perceptual skill tied to human-environment interactions. We introduce Task\nFusion Decoder as a plug-and-play embedding translator that utilizes the\nunderlying relationships among these perceptual skills to guide the\nrepresentation learning towards encoding meaningful structure for what's\nimportant for all perceptual skills, ultimately empowering learning of\ndownstream robotic manipulation tasks. Extensive experiments across a range of\nrobotic tasks and embodiments, in both simulations and real-world environments,\nshow that our Task Fusion Decoder consistently improves the representation of\nthree state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for\ndownstream manipulation policy-learning. Project page:\nhttps://sites.google.com/view/human-oriented-robot-learning\n",
        "published": "2023",
        "authors": [
            "Mingxiao Huo",
            "Mingyu Ding",
            "Chenfeng Xu",
            "Thomas Tian",
            "Xinghao Zhu",
            "Yao Mu",
            "Lingfeng Sun",
            "Masayoshi Tomizuka",
            "Wei Zhan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.03314v1",
        "title": "Enhanced Human-Robot Collaboration using Constrained Probabilistic\n  Human-Motion Prediction",
        "abstract": "  Human motion prediction is an essential step for efficient and safe\nhuman-robot collaboration. Current methods either purely rely on representing\nthe human joints in some form of neural network-based architecture or use\nregression models offline to fit hyper-parameters in the hope of capturing a\nmodel encompassing human motion. While these methods provide good initial\nresults, they are missing out on leveraging well-studied human body kinematic\nmodels as well as body and scene constraints which can help boost the efficacy\nof these prediction frameworks while also explicitly avoiding implausible human\njoint configurations. We propose a novel human motion prediction framework that\nincorporates human joint constraints and scene constraints in a Gaussian\nProcess Regression (GPR) model to predict human motion over a set time horizon.\nThis formulation is combined with an online context-aware constraints model to\nleverage task-dependent motions. It is tested on a human arm kinematic model\nand implemented on a human-robot collaborative setup with a UR5 robot arm to\ndemonstrate the real-time capability of our approach. Simulations were also\nperformed on datasets like HA4M and ANDY. The simulation and experimental\nresults demonstrate considerable improvements in a Gaussian Process framework\nwhen these constraints are explicitly considered.\n",
        "published": "2023",
        "authors": [
            "Aadi Kothari",
            "Tony Tohme",
            "Xiaotong Zhang",
            "Kamal Youcef-Toumi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.03739v1",
        "title": "Aligning Text-to-Image Diffusion Models with Reward Backpropagation",
        "abstract": "  Text-to-image diffusion models have recently emerged at the forefront of\nimage generation, powered by very large-scale unsupervised or weakly supervised\ntext-to-image training datasets. Due to their unsupervised training,\ncontrolling their behavior in downstream tasks, such as maximizing\nhuman-perceived image quality, image-text alignment, or ethical image\ngeneration, is difficult. Recent works finetune diffusion models to downstream\nreward functions using vanilla reinforcement learning, notorious for the high\nvariance of the gradient estimators. In this paper, we propose AlignProp, a\nmethod that aligns diffusion models to downstream reward functions using\nend-to-end backpropagation of the reward gradient through the denoising\nprocess. While naive implementation of such backpropagation would require\nprohibitive memory resources for storing the partial derivatives of modern\ntext-to-image models, AlignProp finetunes low-rank adapter weight modules and\nuses gradient checkpointing, to render its memory usage viable. We test\nAlignProp in finetuning diffusion models to various objectives, such as\nimage-text semantic alignment, aesthetics, compressibility and controllability\nof the number of objects present, as well as their combinations. We show\nAlignProp achieves higher rewards in fewer training steps than alternatives,\nwhile being conceptually simpler, making it a straightforward choice for\noptimizing diffusion models for differentiable reward functions of interest.\nCode and Visualization results are available at https://align-prop.github.io/.\n",
        "published": "2023",
        "authors": [
            "Mihir Prabhudesai",
            "Anirudh Goyal",
            "Deepak Pathak",
            "Katerina Fragkiadaki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.04566v1",
        "title": "Knolling bot: A Transformer-based Approach to Organizing a Messy Table",
        "abstract": "  In this study, we propose an approach to equip domestic robots with the\nability to perform simple household tidying tasks. We focus specifically on\n'knolling,' an activity related to organizing scattered items into neat and\nspace-efficient arrangements. Unlike the uniformity of industrial environments,\nhousehold settings present unique challenges due to their diverse array of\nitems and the subjectivity of tidiness. Here, we draw inspiration from natural\nlanguage processing (NLP) and utilize a transformer-based approach that\npredicts the next position of an item in a sequence of neatly positioned items.\nWe integrate the knolling model with a visual perception model and a physical\nrobot arm to demonstrate a machine that declutters and organizes a dozen\nfreeform items of various shapes and sizes.\n",
        "published": "2023",
        "authors": [
            "Yuhang Hu",
            "Zhizhuo Zhang",
            "Ruibo Liu",
            "Philippe Wyder",
            "Hod Lipson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.04617v1",
        "title": "SlotGNN: Unsupervised Discovery of Multi-Object Representations and\n  Visual Dynamics",
        "abstract": "  Learning multi-object dynamics from visual data using unsupervised techniques\nis challenging due to the need for robust, object representations that can be\nlearned through robot interactions. This paper presents a novel framework with\ntwo new architectures: SlotTransport for discovering object representations\nfrom RGB images and SlotGNN for predicting their collective dynamics from RGB\nimages and robot interactions. Our SlotTransport architecture is based on slot\nattention for unsupervised object discovery and uses a feature transport\nmechanism to maintain temporal alignment in object-centric representations.\nThis enables the discovery of slots that consistently reflect the composition\nof multi-object scenes. These slots robustly bind to distinct objects, even\nunder heavy occlusion or absence. Our SlotGNN, a novel unsupervised graph-based\ndynamics model, predicts the future state of multi-object scenes. SlotGNN\nlearns a graph representation of the scene using the discovered slots from\nSlotTransport and performs relational and spatial reasoning to predict the\nfuture appearance of each slot conditioned on robot actions. We demonstrate the\neffectiveness of SlotTransport in learning object-centric features that\naccurately encode both visual and positional information. Further, we highlight\nthe accuracy of SlotGNN in downstream robotic tasks, including challenging\nmulti-object rearrangement and long-horizon prediction. Finally, our\nunsupervised approach proves effective in the real world. With only minimal\nadditional data, our framework robustly predicts slots and their corresponding\ndynamics in real-world control tasks.\n",
        "published": "2023",
        "authors": [
            "Alireza Rezazadeh",
            "Athreyi Badithela",
            "Karthik Desingh",
            "Changhyun Choi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.08588v1",
        "title": "Octopus: Embodied Vision-Language Programmer from Environmental Feedback",
        "abstract": "  Large vision-language models (VLMs) have achieved substantial progress in\nmultimodal perception and reasoning. Furthermore, when seamlessly integrated\ninto an embodied agent, it signifies a crucial stride towards the creation of\nautonomous and context-aware systems capable of formulating plans and executing\ncommands with precision. In this paper, we introduce Octopus, a novel VLM\ndesigned to proficiently decipher an agent's vision and textual task objectives\nand to formulate intricate action sequences and generate executable code. Our\ndesign allows the agent to adeptly handle a wide spectrum of tasks, ranging\nfrom mundane daily chores in simulators to sophisticated interactions in\ncomplex video games. Octopus is trained by leveraging GPT-4 to control an\nexplorative agent to generate training data, i.e., action blueprints and the\ncorresponding executable code, within our experimental environment called\nOctoVerse. We also collect the feedback that allows the enhanced training\nscheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a\nseries of experiments, we illuminate Octopus's functionality and present\ncompelling results, and the proposed RLEF turns out to refine the agent's\ndecision-making. By open-sourcing our model architecture, simulator, and\ndataset, we aspire to ignite further innovation and foster collaborative\napplications within the broader embodied AI community.\n",
        "published": "2023",
        "authors": [
            "Jingkang Yang",
            "Yuhao Dong",
            "Shuai Liu",
            "Bo Li",
            "Ziyue Wang",
            "Chencheng Jiang",
            "Haoran Tan",
            "Jiamu Kang",
            "Yuanhan Zhang",
            "Kaiyang Zhou",
            "Ziwei Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.10625v1",
        "title": "Video Language Planning",
        "abstract": "  We are interested in enabling visual planning for complex long-horizon tasks\nin the space of generated videos and language, leveraging recent advances in\nlarge generative models pretrained on Internet-scale data. To this end, we\npresent video language planning (VLP), an algorithm that consists of a tree\nsearch procedure, where we train (i) vision-language models to serve as both\npolicies and value functions, and (ii) text-to-video models as dynamics models.\nVLP takes as input a long-horizon task instruction and current image\nobservation, and outputs a long video plan that provides detailed multimodal\n(video and language) specifications that describe how to complete the final\ntask. VLP scales with increasing computation budget where more computation time\nresults in improved video plans, and is able to synthesize long-horizon video\nplans across different robotics domains: from multi-object rearrangement, to\nmulti-camera bi-arm dexterous manipulation. Generated video plans can be\ntranslated into real robot actions via goal-conditioned policies, conditioned\non each intermediate frame of the generated video. Experiments show that VLP\nsubstantially improves long-horizon task success rates compared to prior\nmethods on both simulated and real robots (across 3 hardware platforms).\n",
        "published": "2023",
        "authors": [
            "Yilun Du",
            "Mengjiao Yang",
            "Pete Florence",
            "Fei Xia",
            "Ayzaan Wahid",
            "Brian Ichter",
            "Pierre Sermanet",
            "Tianhe Yu",
            "Pieter Abbeel",
            "Joshua B. Tenenbaum",
            "Leslie Kaelbling",
            "Andy Zeng",
            "Jonathan Tompson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.15020v2",
        "title": "Invariance is Key to Generalization: Examining the Role of\n  Representation in Sim-to-Real Transfer for Visual Navigation",
        "abstract": "  The data-driven approach to robot control has been gathering pace rapidly,\nyet generalization to unseen task domains remains a critical challenge. We\nargue that the key to generalization is representations that are (i) rich\nenough to capture all task-relevant information and (ii) invariant to\nsuperfluous variability between the training and the test domains. We\nexperimentally study such a representation -- containing both depth and\nsemantic information -- for visual navigation and show that it enables a\ncontrol policy trained entirely in simulated indoor scenes to generalize to\ndiverse real-world environments, both indoors and outdoors. Further, we show\nthat our representation reduces the A-distance between the training and test\ndomains, improving the generalization error bound as a result. Our proposed\napproach is scalable: the learned policy improves continuously, as the\nfoundation models that it exploits absorb more diverse data during\npre-training.\n",
        "published": "2023",
        "authors": [
            "Bo Ai",
            "Zhanxin Wu",
            "David Hsu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.15059v1",
        "title": "Robot Skill Generalization via Keypoint Integrated Soft Actor-Critic\n  Gaussian Mixture Models",
        "abstract": "  A long-standing challenge for a robotic manipulation system operating in\nreal-world scenarios is adapting and generalizing its acquired motor skills to\nunseen environments. We tackle this challenge employing hybrid skill models\nthat integrate imitation and reinforcement paradigms, to explore how the\nlearning and adaptation of a skill, along with its core grounding in the scene\nthrough a learned keypoint, can facilitate such generalization. To that end, we\ndevelop Keypoint Integrated Soft Actor-Critic Gaussian Mixture Models (KIS-GMM)\napproach that learns to predict the reference of a dynamical system within the\nscene as a 3D keypoint, leveraging visual observations obtained by the robot's\nphysical interactions during skill learning. Through conducting comprehensive\nevaluations in both simulated and real-world environments, we show that our\nmethod enables a robot to gain a significant zero-shot generalization to novel\nenvironments and to refine skills in the target environments faster than\nlearning from scratch. Importantly, this is achieved without the need for new\nground truth data. Moreover, our method effectively copes with scene\ndisplacements.\n",
        "published": "2023",
        "authors": [
            "Iman Nematollahi",
            "Kirill Yankov",
            "Wolfram Burgard",
            "Tim Welschehold"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.16014v1",
        "title": "Human-in-the-Loop Task and Motion Planning for Imitation Learning",
        "abstract": "  Imitation learning from human demonstrations can teach robots complex\nmanipulation skills, but is time-consuming and labor intensive. In contrast,\nTask and Motion Planning (TAMP) systems are automated and excel at solving\nlong-horizon tasks, but they are difficult to apply to contact-rich tasks. In\nthis paper, we present Human-in-the-Loop Task and Motion Planning (HITL-TAMP),\na novel system that leverages the benefits of both approaches. The system\nemploys a TAMP-gated control mechanism, which selectively gives and takes\ncontrol to and from a human teleoperator. This enables the human teleoperator\nto manage a fleet of robots, maximizing data collection efficiency. The\ncollected human data is then combined with an imitation learning framework to\ntrain a TAMP-gated policy, leading to superior performance compared to training\non full task demonstrations. We compared HITL-TAMP to a conventional\nteleoperation system -- users gathered more than 3x the number of demos given\nthe same time budget. Furthermore, proficient agents (75\\%+ success) could be\ntrained from just 10 minutes of non-expert teleoperation data. Finally, we\ncollected 2.1K demos with HITL-TAMP across 12 contact-rich, long-horizon tasks\nand show that the system often produces near-perfect agents. Videos and\nadditional results at https://hitltamp.github.io .\n",
        "published": "2023",
        "authors": [
            "Ajay Mandlekar",
            "Caelan Garrett",
            "Danfei Xu",
            "Dieter Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.16029v1",
        "title": "Finetuning Offline World Models in the Real World",
        "abstract": "  Reinforcement Learning (RL) is notoriously data-inefficient, which makes\ntraining on a real robot difficult. While model-based RL algorithms (world\nmodels) improve data-efficiency to some extent, they still require hours or\ndays of interaction to learn skills. Recently, offline RL has been proposed as\na framework for training RL policies on pre-existing datasets without any\nonline interaction. However, constraining an algorithm to a fixed dataset\ninduces a state-action distribution shift between training and inference, and\nlimits its applicability to new tasks. In this work, we seek to get the best of\nboth worlds: we consider the problem of pretraining a world model with offline\ndata collected on a real robot, and then finetuning the model on online data\ncollected by planning with the learned model. To mitigate extrapolation errors\nduring online interaction, we propose to regularize the planner at test-time by\nbalancing estimated returns and (epistemic) model uncertainty. We evaluate our\nmethod on a variety of visuo-motor control tasks in simulation and on a real\nrobot, and find that our method enables few-shot finetuning to seen and unseen\ntasks even when offline data is limited. Videos, code, and data are available\nat https://yunhaifeng.com/FOWM .\n",
        "published": "2023",
        "authors": [
            "Yunhai Feng",
            "Nicklas Hansen",
            "Ziyan Xiong",
            "Chandramouli Rajagopalan",
            "Xiaolong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.16828v1",
        "title": "TD-MPC2: Scalable, Robust World Models for Continuous Control",
        "abstract": "  TD-MPC is a model-based reinforcement learning (RL) algorithm that performs\nlocal trajectory optimization in the latent space of a learned implicit\n(decoder-free) world model. In this work, we present TD-MPC2: a series of\nimprovements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves\nsignificantly over baselines across 104 online RL tasks spanning 4 diverse task\ndomains, achieving consistently strong results with a single set of\nhyperparameters. We further show that agent capabilities increase with model\nand data size, and successfully train a single 317M parameter agent to perform\n80 tasks across multiple task domains, embodiments, and action spaces. We\nconclude with an account of lessons, opportunities, and risks associated with\nlarge TD-MPC2 agents. Explore videos, models, data, code, and more at\nhttps://nicklashansen.github.io/td-mpc2\n",
        "published": "2023",
        "authors": [
            "Nicklas Hansen",
            "Hao Su",
            "Xiaolong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.17596v1",
        "title": "MimicGen: A Data Generation System for Scalable Robot Learning using\n  Human Demonstrations",
        "abstract": "  Imitation learning from a large set of human demonstrations has proved to be\nan effective paradigm for building capable robot agents. However, the\ndemonstrations can be extremely costly and time-consuming to collect. We\nintroduce MimicGen, a system for automatically synthesizing large-scale, rich\ndatasets from only a small number of human demonstrations by adapting them to\nnew contexts. We use MimicGen to generate over 50K demonstrations across 18\ntasks with diverse scene configurations, object instances, and robot arms from\njust ~200 human demonstrations. We show that robot agents can be effectively\ntrained on this generated dataset by imitation learning to achieve strong\nperformance in long-horizon and high-precision tasks, such as multi-part\nassembly and coffee preparation, across broad initial state distributions. We\nfurther demonstrate that the effectiveness and utility of MimicGen data compare\nfavorably to collecting additional human demonstrations, making it a powerful\nand economical approach towards scaling up robot learning. Datasets, simulation\nenvironments, videos, and more at https://mimicgen.github.io .\n",
        "published": "2023",
        "authors": [
            "Ajay Mandlekar",
            "Soroush Nasiriany",
            "Bowen Wen",
            "Iretiayo Akinola",
            "Yashraj Narang",
            "Linxi Fan",
            "Yuke Zhu",
            "Dieter Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.19944v1",
        "title": "Conditional Unscented Autoencoders for Trajectory Prediction",
        "abstract": "  The \\ac{CVAE} is one of the most widely-used models in trajectory prediction\nfor \\ac{AD}. It captures the interplay between a driving context and its\nground-truth future into a probabilistic latent space and uses it to produce\npredictions. In this paper, we challenge key components of the CVAE. We\nleverage recent advances in the space of the VAE, the foundation of the CVAE,\nwhich show that a simple change in the sampling procedure can greatly benefit\nperformance. We find that unscented sampling, which draws samples from any\nlearned distribution in a deterministic manner, can naturally be better suited\nto trajectory prediction than potentially dangerous random sampling. We go\nfurther and offer additional improvements, including a more structured mixture\nlatent space, as well as a novel, potentially more expressive way to do\ninference with CVAEs. We show wide applicability of our models by evaluating\nthem on the INTERACTION prediction dataset, outperforming the state of the art,\nas well as at the task of image modeling on the CelebA dataset, outperforming\nthe baseline vanilla CVAE. Code is available at\nhttps://github.com/boschresearch/cuae-prediction.\n",
        "published": "2023",
        "authors": [
            "Faris Janjo\u0161",
            "Marcel Hallgarten",
            "Anthony Knittel",
            "Maxim Dolgov",
            "Andreas Zell",
            "J. Marius Z\u00f6llner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.20350v1",
        "title": "Combining Shape Completion and Grasp Prediction for Fast and Versatile\n  Grasping with a Multi-Fingered Hand",
        "abstract": "  Grasping objects with limited or no prior knowledge about them is a highly\nrelevant skill in assistive robotics. Still, in this general setting, it has\nremained an open problem, especially when it comes to only partial\nobservability and versatile grasping with multi-fingered hands. We present a\nnovel, fast, and high fidelity deep learning pipeline consisting of a shape\ncompletion module that is based on a single depth image, and followed by a\ngrasp predictor that is based on the predicted object shape. The shape\ncompletion network is based on VQDIF and predicts spatial occupancy values at\narbitrary query points. As grasp predictor, we use our two-stage architecture\nthat first generates hand poses using an autoregressive model and then\nregresses finger joint configurations per pose. Critical factors turn out to be\nsufficient data realism and augmentation, as well as special attention to\ndifficult cases during training. Experiments on a physical robot platform\ndemonstrate successful grasping of a wide range of household objects based on a\ndepth image from a single viewpoint. The whole pipeline is fast, taking only\nabout 1 s for completing the object's shape (0.7 s) and generating 1000 grasps\n(0.3 s).\n",
        "published": "2023",
        "authors": [
            "Matthias Humt",
            "Dominik Winkelbauer",
            "Ulrich Hillenbrand",
            "Berthold B\u00e4uml"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.01017v3",
        "title": "Learning Unsupervised World Models for Autonomous Driving via Discrete\n  Diffusion",
        "abstract": "  Learning world models can teach an agent how the world works in an\nunsupervised manner. Even though it can be viewed as a special case of sequence\nmodeling, progress for scaling world models on robotic applications such as\nautonomous driving has been somewhat less rapid than scaling language models\nwith Generative Pre-trained Transformers (GPT). We identify two reasons as\nmajor bottlenecks: dealing with complex and unstructured observation space, and\nhaving a scalable generative model. Consequently, we propose a novel world\nmodeling approach that first tokenizes sensor observations with VQVAE, then\npredicts the future via discrete diffusion. To efficiently decode and denoise\ntokens in parallel, we recast Masked Generative Image Transformer into the\ndiscrete diffusion framework with a few simple changes, resulting in notable\nimprovement. When applied to learning world models on point cloud observations,\nour model reduces prior SOTA Chamfer distance by more than 65% for 1s\nprediction, and more than 50% for 3s prediction, across NuScenes, KITTI\nOdometry, and Argoverse2 datasets. Our results demonstrate that discrete\ndiffusion on tokenized agent experience can unlock the power of GPT-like\nunsupervised learning for robotic agents.\n",
        "published": "2023",
        "authors": [
            "Lunjun Zhang",
            "Yuwen Xiong",
            "Ze Yang",
            "Sergio Casas",
            "Rui Hu",
            "Raquel Urtasun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.01455v2",
        "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning\n  via Generative Simulation",
        "abstract": "  We present RoboGen, a generative robotic agent that automatically learns\ndiverse robotic skills at scale via generative simulation. RoboGen leverages\nthe latest advancements in foundation and generative models. Instead of\ndirectly using or adapting these models to produce policies or low-level\nactions, we advocate for a generative scheme, which uses these models to\nautomatically generate diversified tasks, scenes, and training supervisions,\nthereby scaling up robotic skill learning with minimal human supervision. Our\napproach equips a robotic agent with a self-guided propose-generate-learn\ncycle: the agent first proposes interesting tasks and skills to develop, and\nthen generates corresponding simulation environments by populating pertinent\nobjects and assets with proper spatial configurations. Afterwards, the agent\ndecomposes the proposed high-level task into sub-tasks, selects the optimal\nlearning approach (reinforcement learning, motion planning, or trajectory\noptimization), generates required training supervision, and then learns\npolicies to acquire the proposed skill. Our work attempts to extract the\nextensive and versatile knowledge embedded in large-scale models and transfer\nthem to the field of robotics. Our fully generative pipeline can be queried\nrepeatedly, producing an endless stream of skill demonstrations associated with\ndiverse tasks and environments.\n",
        "published": "2023",
        "authors": [
            "Yufei Wang",
            "Zhou Xian",
            "Feng Chen",
            "Tsun-Hsuan Wang",
            "Yian Wang",
            "Zackory Erickson",
            "David Held",
            "Chuang Gan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.03357v1",
        "title": "Exploitation-Guided Exploration for Semantic Embodied Navigation",
        "abstract": "  In the recent progress in embodied navigation and sim-to-robot transfer,\nmodular policies have emerged as a de facto framework. However, there is more\nto compositionality beyond the decomposition of the learning load into modular\ncomponents. In this work, we investigate a principled way to syntactically\ncombine these components. Particularly, we propose Exploitation-Guided\nExploration (XGX) where separate modules for exploration and exploitation come\ntogether in a novel and intuitive manner. We configure the exploitation module\nto take over in the deterministic final steps of navigation i.e. when the goal\nbecomes visible. Crucially, an exploitation module teacher-forces the\nexploration module and continues driving an overridden policy optimization.\nXGX, with effective decomposition and novel guidance, improves the\nstate-of-the-art performance on the challenging object navigation task from 70%\nto 73%. Along with better accuracy, through targeted analysis, we show that XGX\nis also more efficient at goal-conditioned exploration. Finally, we show\nsim-to-real transfer to robot hardware and XGX performs over two-fold better\nthan the best baseline from simulation benchmarking. Project page:\nxgxvisnav.github.io\n",
        "published": "2023",
        "authors": [
            "Justin Wasserman",
            "Girish Chowdhary",
            "Abhinav Gupta",
            "Unnat Jain"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.03622v1",
        "title": "TWIST: Teacher-Student World Model Distillation for Efficient\n  Sim-to-Real Transfer",
        "abstract": "  Model-based RL is a promising approach for real-world robotics due to its\nimproved sample efficiency and generalization capabilities compared to\nmodel-free RL. However, effective model-based RL solutions for vision-based\nreal-world applications require bridging the sim-to-real gap for any world\nmodel learnt. Due to its significant computational cost, standard domain\nrandomisation does not provide an effective solution to this problem. This\npaper proposes TWIST (Teacher-Student World Model Distillation for Sim-to-Real\nTransfer) to achieve efficient sim-to-real transfer of vision-based model-based\nRL using distillation. Specifically, TWIST leverages state observations as\nreadily accessible, privileged information commonly garnered from a simulator\nto significantly accelerate sim-to-real transfer. Specifically, a teacher world\nmodel is trained efficiently on state information. At the same time, a matching\ndataset is collected of domain-randomised image observations. The teacher world\nmodel then supervises a student world model that takes the domain-randomised\nimage observations as input. By distilling the learned latent dynamics model\nfrom the teacher to the student model, TWIST achieves efficient and effective\nsim-to-real transfer for vision-based model-based RL tasks. Experiments in\nsimulated and real robotics tasks demonstrate that our approach outperforms\nnaive domain randomisation and model-free methods in terms of sample efficiency\nand task performance of sim-to-real transfer.\n",
        "published": "2023",
        "authors": [
            "Jun Yamada",
            "Marc Rigter",
            "Jack Collins",
            "Ingmar Posner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.08094v1",
        "title": "Act-VIT: A Representationally Robust Attention Architecture for Skeleton\n  Based Action Recognition Using Vision Transformer",
        "abstract": "  Skeleton-based action recognition receives the attention of many researchers\nas it is robust to viewpoint and illumination changes, and its processing is\nmuch more efficient than video frames. With the emergence of deep learning\nmodels, it has become very popular to represent the skeleton data in\npseudo-image form and apply Convolutional Neural Networks for action\nrecognition. Thereafter, studies concentrated on finding effective methods for\nforming pseudo-images. Recently, attention networks, more specifically\ntransformers have provided promising results in various vision problems. In\nthis study, the effectiveness of vision transformers for skeleton-based action\nrecognition is examined and its robustness on the pseudo-image representation\nscheme is investigated. To this end, a three-level architecture, Act-VIT is\nproposed, which forms a set of pseudo images apply a classifier on each of the\nrepresentation and combine their results to find the final action class. The\nclassifiers of Act-VIT are first realized by CNNs and then by VITs and their\nperformances are compared. Experimental studies reveal that the vision\ntransformer is less sensitive to the initial pseudo-image representation\ncompared to CNN. Nevertheless, even with the vision transformer, the\nrecognition performance can be further improved by consensus of classifiers.\n",
        "published": "2023",
        "authors": [
            "Ozge Oztimur Karadag"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.08393v2",
        "title": "MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable\n  Trajectory Generation",
        "abstract": "  The learn-from-observation (LfO) paradigm is a human-inspired mode for a\nrobot to learn to perform a task simply by watching it being performed. LfO can\nfacilitate robot integration on factory floors by minimizing disruption and\nreducing tedious programming. A key component of the LfO pipeline is a\ntransformation of the depth camera frames to the corresponding task state and\naction pairs, which are then relayed to learning techniques such as imitation\nor inverse reinforcement learning for understanding the task parameters. While\nseveral existing computer vision models analyze videos for activity\nrecognition, SA-Net specifically targets robotic LfO from RGB-D data. However,\nSA-Net and many other models analyze frame data captured from a single\nviewpoint. Their analysis is therefore highly sensitive to occlusions of the\nobserved task, which are frequent in deployments. An obvious way of reducing\nocclusions is to simultaneously observe the task from multiple viewpoints and\nsynchronously fuse the multiple streams in the model. Toward this, we present\nmulti-view SA-Net, which generalizes the SA-Net model to allow the perception\nof multiple viewpoints of the task activity, integrate them, and better\nrecognize the state and action in each frame. Performance evaluations on two\ndistinct domains establish that MVSA-Net recognizes the state-action pairs\nunder occlusion more accurately compared to single-view MVSA-Net and other\nbaselines. Our ablation studies further evaluate its performance under\ndifferent ambient conditions and establish the contribution of the architecture\ncomponents. As such, MVSA-Net offers a significantly more robust and deployable\nstate-action trajectory generation compared to previous methods.\n",
        "published": "2023",
        "authors": [
            "Ehsan Asali",
            "Prashant Doshi",
            "Jin Sun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.13750v2",
        "title": "Towards Transferable Multi-modal Perception Representation Learning for\n  Autonomy: NeRF-Supervised Masked AutoEncoder",
        "abstract": "  This work proposes a unified self-supervised pre-training framework for\ntransferable multi-modal perception representation learning via masked\nmulti-modal reconstruction in Neural Radiance Field (NeRF), namely\nNeRF-Supervised Masked AutoEncoder (NS-MAE). Specifically, conditioned on\ncertain view directions and locations, multi-modal embeddings extracted from\ncorrupted multi-modal input signals, i.e., Lidar point clouds and images, are\nrendered into projected multi-modal feature maps via neural rendering. Then,\noriginal multi-modal signals serve as reconstruction targets for the rendered\nmulti-modal feature maps to enable self-supervised representation learning.\nExtensive experiments show that the representation learned via NS-MAE shows\npromising transferability for diverse multi-modal and single-modal (camera-only\nand Lidar-only) perception models on diverse 3D perception downstream tasks (3D\nobject detection and BEV map segmentation) with diverse amounts of fine-tuning\nlabeled data. Moreover, we empirically find that NS-MAE enjoys the synergy of\nboth the mechanism of masked autoencoder and neural radiance field. We hope\nthis study can inspire exploration of more general multi-modal representation\nlearning for autonomous agents.\n",
        "published": "2023",
        "authors": [
            "Xiaohao Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.16098v1",
        "title": "On Bringing Robots Home",
        "abstract": "  Throughout history, we have successfully integrated various machines into our\nhomes. Dishwashers, laundry machines, stand mixers, and robot vacuums are a few\nrecent examples. However, these machines excel at performing only a single task\neffectively. The concept of a \"generalist machine\" in homes - a domestic\nassistant that can adapt and learn from our needs, all while remaining\ncost-effective - has long been a goal in robotics that has been steadily\npursued for decades. In this work, we initiate a large-scale effort towards\nthis goal by introducing Dobb-E, an affordable yet versatile general-purpose\nsystem for learning robotic manipulation within household settings. Dobb-E can\nlearn a new task with only five minutes of a user showing it how to do it,\nthanks to a demonstration collection tool (\"The Stick\") we built out of cheap\nparts and iPhones. We use the Stick to collect 13 hours of data in 22 homes of\nNew York City, and train Home Pretrained Representations (HPR). Then, in a\nnovel home environment, with five minutes of demonstrations and fifteen minutes\nof adapting the HPR model, we show that Dobb-E can reliably solve the task on\nthe Stretch, a mobile robot readily available on the market. Across roughly 30\ndays of experimentation in homes of New York City and surrounding areas, we\ntest our system in 10 homes, with a total of 109 tasks in different\nenvironments, and finally achieve a success rate of 81%. Beyond success\npercentages, our experiments reveal a plethora of unique challenges absent or\nignored in lab robotics. These range from effects of strong shadows, to\nvariable demonstration quality by non-expert users. With the hope of\naccelerating research on home robots, and eventually seeing robot butlers in\nevery home, we open-source Dobb-E software stack and models, our data, and our\nhardware designs at https://dobb-e.com\n",
        "published": "2023",
        "authors": [
            "Nur Muhammad Mahi Shafiullah",
            "Anant Rai",
            "Haritheja Etukuru",
            "Yiqian Liu",
            "Ishan Misra",
            "Soumith Chintala",
            "Lerrel Pinto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.16102v2",
        "title": "Diffusion-TTA: Test-time Adaptation of Discriminative Models via\n  Generative Feedback",
        "abstract": "  The advancements in generative modeling, particularly the advent of diffusion\nmodels, have sparked a fundamental question: how can these models be\neffectively used for discriminative tasks? In this work, we find that\ngenerative models can be great test-time adapters for discriminative models.\nOur method, Diffusion-TTA, adapts pre-trained discriminative models such as\nimage classifiers, segmenters and depth predictors, to each unlabelled example\nin the test set using generative feedback from a diffusion model. We achieve\nthis by modulating the conditioning of the diffusion model using the output of\nthe discriminative model. We then maximize the image likelihood objective by\nbackpropagating the gradients to discriminative model's parameters. We show\nDiffusion-TTA significantly enhances the accuracy of various large-scale\npre-trained discriminative models, such as, ImageNet classifiers, CLIP models,\nimage pixel labellers and image depth predictors. Diffusion-TTA outperforms\nexisting test-time adaptation methods, including TTT-MAE and TENT, and\nparticularly shines in online adaptation setups, where the discriminative model\nis continually adapted to each example in the test set. We provide access to\ncode, results, and visualizations on our website:\nhttps://diffusion-tta.github.io/.\n",
        "published": "2023",
        "authors": [
            "Mihir Prabhudesai",
            "Tsung-Wei Ke",
            "Alexander C. Li",
            "Deepak Pathak",
            "Katerina Fragkiadaki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.17053v1",
        "title": "DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative\n  Diffusion Models",
        "abstract": "  Nature evolves creatures with a high complexity of morphological and\nbehavioral intelligence, meanwhile computational methods lag in approaching\nthat diversity and efficacy. Co-optimization of artificial creatures'\nmorphology and control in silico shows promise for applications in physical\nsoft robotics and virtual character creation; such approaches, however, require\ndeveloping new learning algorithms that can reason about function atop pure\nstructure. In this paper, we present DiffuseBot, a physics-augmented diffusion\nmodel that generates soft robot morphologies capable of excelling in a wide\nspectrum of tasks. DiffuseBot bridges the gap between virtually generated\ncontent and physical utility by (i) augmenting the diffusion process with a\nphysical dynamical simulation which provides a certificate of performance, and\n(ii) introducing a co-design procedure that jointly optimizes physical design\nand control by leveraging information about physical sensitivities from\ndifferentiable simulation. We showcase a range of simulated and fabricated\nrobots along with their capabilities. Check our website at\nhttps://diffusebot.github.io/\n",
        "published": "2023",
        "authors": [
            "Tsun-Hsuan Wang",
            "Juntian Zheng",
            "Pingchuan Ma",
            "Yilun Du",
            "Byungchul Kim",
            "Andrew Spielberg",
            "Joshua Tenenbaum",
            "Chuang Gan",
            "Daniela Rus"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.03009v1",
        "title": "I-PHYRE: Interactive Physical Reasoning",
        "abstract": "  Current evaluation protocols predominantly assess physical reasoning in\nstationary scenes, creating a gap in evaluating agents' abilities to interact\nwith dynamic events. While contemporary methods allow agents to modify initial\nscene configurations and observe consequences, they lack the capability to\ninteract with events in real time. To address this, we introduce I-PHYRE, a\nframework that challenges agents to simultaneously exhibit intuitive physical\nreasoning, multi-step planning, and in-situ intervention. Here, intuitive\nphysical reasoning refers to a quick, approximate understanding of physics to\naddress complex problems; multi-step denotes the need for extensive sequence\nplanning in I-PHYRE, considering each intervention can significantly alter\nsubsequent choices; and in-situ implies the necessity for timely object\nmanipulation within a scene, where minor timing deviations can result in task\nfailure. We formulate four game splits to scrutinize agents' learning and\ngeneralization of essential principles of interactive physical reasoning,\nfostering learning through interaction with representative scenarios. Our\nexploration involves three planning strategies and examines several supervised\nand reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The\noutcomes highlight a notable gap between existing learning algorithms and human\nperformance, emphasizing the imperative for more research in enhancing agents\nwith interactive physical reasoning capabilities. The environment and baselines\nwill be made publicly available.\n",
        "published": "2023",
        "authors": [
            "Shiqian Li",
            "Kewen Wu",
            "Chi Zhang",
            "Yixin Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.04670v1",
        "title": "Rapid Motor Adaptation for Robotic Manipulator Arms",
        "abstract": "  Developing generalizable manipulation skills is a core challenge in embodied\nAI. This includes generalization across diverse task configurations,\nencompassing variations in object shape, density, friction coefficient, and\nexternal disturbances such as forces applied to the robot. Rapid Motor\nAdaptation (RMA) offers a promising solution to this challenge. It posits that\nessential hidden variables influencing an agent's task performance, such as\nobject mass and shape, can be effectively inferred from the agent's action and\nproprioceptive history. Drawing inspiration from RMA in locomotion and in-hand\nrotation, we use depth perception to develop agents tailored for rapid motor\nadaptation in a variety of manipulation tasks. We evaluated our agents on four\nchallenging tasks from the Maniskill2 benchmark, namely pick-and-place\noperations with hundreds of objects from the YCB and EGAD datasets, peg\ninsertion with precise position and orientation, and operating a variety of\nfaucets and handles, with customized environment variations. Empirical results\ndemonstrate that our agents surpass state-of-the-art methods like automatic\ndomain randomization and vision-based policies, obtaining better generalization\nperformance and sample efficiency.\n",
        "published": "2023",
        "authors": [
            "Yichao Liang",
            "Kevin Ellis",
            "Jo\u00e3o Henriques"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.04947v1",
        "title": "Benchmarking and Analysis of Unsupervised Object Segmentation from\n  Real-world Single Images",
        "abstract": "  In this paper, we study the problem of unsupervised object segmentation from\nsingle images. We do not introduce a new algorithm, but systematically\ninvestigate the effectiveness of existing unsupervised models on challenging\nreal-world images. We first introduce seven complexity factors to\nquantitatively measure the distributions of background and foreground object\nbiases in appearance and geometry for datasets with human annotations. With the\naid of these factors, we empirically find that, not surprisingly, existing\nunsupervised models fail to segment generic objects in real-world images,\nalthough they can easily achieve excellent performance on numerous simple\nsynthetic datasets, due to the vast gap in objectness biases between synthetic\nand real images. By conducting extensive experiments on multiple groups of\nablated real-world datasets, we ultimately find that the key factors underlying\nthe failure of existing unsupervised models on real-world images are the\nchallenging distributions of background and foreground object biases in\nappearance and geometry. Because of this, the inductive biases introduced in\nexisting unsupervised models can hardly capture the diverse object\ndistributions. Our research results suggest that future work should exploit\nmore explicit objectness biases in the network design.\n",
        "published": "2023",
        "authors": [
            "Yafei Yang",
            "Bo Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.05925v1",
        "title": "Language-Conditioned Semantic Search-Based Policy for Robotic\n  Manipulation Tasks",
        "abstract": "  Reinforcement learning and Imitation Learning approaches utilize policy\nlearning strategies that are difficult to generalize well with just a few\nexamples of a task. In this work, we propose a language-conditioned semantic\nsearch-based method to produce an online search-based policy from the available\ndemonstration dataset of state-action trajectories. Here we directly acquire\nactions from the most similar manipulation trajectories found in the dataset.\nOur approach surpasses the performance of the baselines on the CALVIN benchmark\nand exhibits strong zero-shot adaptation capabilities. This holds great\npotential for expanding the use of our online search-based policy approach to\ntasks typically addressed by Imitation Learning or Reinforcement Learning-based\npolicies.\n",
        "published": "2023",
        "authors": [
            "Jannik Sheikh",
            "Andrew Melnik",
            "Gora Chand Nandi",
            "Robert Haschke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.06583v1",
        "title": "3D Hand Pose Estimation in Egocentric Images in the Wild",
        "abstract": "  We present WildHands, a method for 3D hand pose estimation in egocentric\nimages in the wild. This is challenging due to (a) lack of 3D hand pose\nannotations for images in the wild, and (b) a form of perspective\ndistortion-induced shape ambiguity that arises in the analysis of crops around\nhands. For the former, we use auxiliary supervision on in-the-wild data in the\nform of segmentation masks & grasp labels in addition to 3D supervision\navailable in lab datasets. For the latter, we provide spatial cues about the\nlocation of the hand crop in the camera's field of view. Our approach achieves\nthe best 3D hand pose on the ARCTIC leaderboard and outperforms FrankMocap, a\npopular and robust approach for estimating hand pose in the wild, by 45.3% when\nevaluated on 2D hand pose on our EPIC-HandKps dataset.\n",
        "published": "2023",
        "authors": [
            "Aditya Prakash",
            "Ruisen Tu",
            "Matthew Chang",
            "Saurabh Gupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.06639v1",
        "title": "Harmonic Mobile Manipulation",
        "abstract": "  Recent advancements in robotics have enabled robots to navigate complex\nscenes or manipulate diverse objects independently. However, robots are still\nimpotent in many household tasks requiring coordinated behaviors such as\nopening doors. The factorization of navigation and manipulation, while\neffective for some tasks, fails in scenarios requiring coordinated actions. To\naddress this challenge, we introduce, HarmonicMM, an end-to-end learning method\nthat optimizes both navigation and manipulation, showing notable improvement\nover existing techniques in everyday tasks. This approach is validated in\nsimulated and real-world environments and adapts to novel unseen settings\nwithout additional tuning. Our contributions include a new benchmark for mobile\nmanipulation and the successful deployment in a real unseen apartment,\ndemonstrating the potential for practical indoor robot deployment in daily\nlife. More results are on our project site:\nhttps://rchalyang.github.io/HarmonicMM/\n",
        "published": "2023",
        "authors": [
            "Ruihan Yang",
            "Yejin Kim",
            "Aniruddha Kembhavi",
            "Xiaolong Wang",
            "Kiana Ehsani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.08782v2",
        "title": "Toward General-Purpose Robots via Foundation Models: A Survey and\n  Meta-Analysis",
        "abstract": "  Building general-purpose robots that can operate seamlessly, in any\nenvironment, with any object, and utilizing various skills to complete diverse\ntasks has been a long-standing goal in Artificial Intelligence. Unfortunately,\nhowever, most existing robotic systems have been constrained - having been\ndesigned for specific tasks, trained on specific datasets, and deployed within\nspecific environments. These systems usually require extensively-labeled data,\nrely on task-specific models, have numerous generalization issues when deployed\nin real-world scenarios, and struggle to remain robust to distribution shifts.\nMotivated by the impressive open-set performance and content generation\ncapabilities of web-scale, large-capacity pre-trained models (i.e., foundation\nmodels) in research fields such as Natural Language Processing (NLP) and\nComputer Vision (CV), we devote this survey to exploring (i) how these existing\nfoundation models from NLP and CV can be applied to the field of robotics, and\nalso exploring (ii) what a robotics-specific foundation model would look like.\nWe begin by providing an overview of what constitutes a conventional robotic\nsystem and the fundamental barriers to making it universally applicable. Next,\nwe establish a taxonomy to discuss current work exploring ways to leverage\nexisting foundation models for robotics and develop ones catered to robotics.\nFinally, we discuss key challenges and promising future directions in using\nfoundation models for enabling general-purpose robotic systems. We encourage\nreaders to view our living GitHub repository of resources, including papers\nreviewed in this survey as well as related projects and repositories for\ndeveloping foundation models for robotics.\n",
        "published": "2023",
        "authors": [
            "Yafei Hu",
            "Quanting Xie",
            "Vidhi Jain",
            "Jonathan Francis",
            "Jay Patrikar",
            "Nikhil Keetha",
            "Seungchan Kim",
            "Yaqi Xie",
            "Tianyi Zhang",
            "Shibo Zhao",
            "Yu Quan Chong",
            "Chen Wang",
            "Katia Sycara",
            "Matthew Johnson-Roberson",
            "Dhruv Batra",
            "Xiaolong Wang",
            "Sebastian Scherer",
            "Zsolt Kira",
            "Fei Xia",
            "Yonatan Bisk"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.10571v1",
        "title": "Multi-level Reasoning for Robotic Assembly: From Sequence Inference to\n  Contact Selection",
        "abstract": "  Automating the assembly of objects from their parts is a complex problem with\ninnumerable applications in manufacturing, maintenance, and recycling. Unlike\nexisting research, which is limited to target segmentation, pose regression, or\nusing fixed target blueprints, our work presents a holistic multi-level\nframework for part assembly planning consisting of part assembly sequence\ninference, part motion planning, and robot contact optimization. We present the\nPart Assembly Sequence Transformer (PAST) -- a sequence-to-sequence neural\nnetwork -- to infer assembly sequences recursively from a target blueprint. We\nthen use a motion planner and optimization to generate part movements and\ncontacts. To train PAST, we introduce D4PAS: a large-scale Dataset for Part\nAssembly Sequences (D4PAS) consisting of physically valid sequences for\nindustrial objects. Experimental results show that our approach generalizes\nbetter than prior methods while needing significantly less computational time\nfor inference.\n",
        "published": "2023",
        "authors": [
            "Xinghao Zhu",
            "Devesh K. Jha",
            "Diego Romeres",
            "Lingfeng Sun",
            "Masayoshi Tomizuka",
            "Anoop Cherian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.13906v1",
        "title": "EfficientPPS: Part-aware Panoptic Segmentation of Transparent Objects\n  for Robotic Manipulation",
        "abstract": "  The use of autonomous robots for assistance tasks in hospitals has the\npotential to free up qualified staff and im-prove patient care. However, the\nubiquity of deformable and transparent objects in hospital settings poses\nsignif-icant challenges to vision-based perception systems. We present\nEfficientPPS, a neural architecture for part-aware panoptic segmentation that\nprovides robots with semantically rich visual information for grasping and\nma-nipulation tasks. We also present an unsupervised data collection and\nlabelling method to reduce the need for human involvement in the training\nprocess. EfficientPPS is evaluated on a dataset containing real-world hospital\nobjects and demonstrated to be robust and efficient in grasping transparent\ntransfusion bags with a collaborative robot arm.\n",
        "published": "2023",
        "authors": [
            "Benjamin Alt",
            "Minh Dang Nguyen",
            "Andreas Hermann",
            "Darko Katic",
            "Rainer J\u00e4kel",
            "R\u00fcdiger Dillmann",
            "Eric Sax"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.15339v1",
        "title": "MaDi: Learning to Mask Distractions for Generalization in Visual Deep\n  Reinforcement Learning",
        "abstract": "  The visual world provides an abundance of information, but many input pixels\nreceived by agents often contain distracting stimuli. Autonomous agents need\nthe ability to distinguish useful information from task-irrelevant perceptions,\nenabling them to generalize to unseen environments with new distractions.\nExisting works approach this problem using data augmentation or large auxiliary\nnetworks with additional loss functions. We introduce MaDi, a novel algorithm\nthat learns to mask distractions by the reward signal only. In MaDi, the\nconventional actor-critic structure of deep reinforcement learning agents is\ncomplemented by a small third sibling, the Masker. This lightweight neural\nnetwork generates a mask to determine what the actor and critic will receive,\nsuch that they can focus on learning the task. The masks are created\ndynamically, depending on the current input. We run experiments on the DeepMind\nControl Generalization Benchmark, the Distracting Control Suite, and a real UR5\nRobotic Arm. Our algorithm improves the agent's focus with useful masks, while\nits efficient Masker network only adds 0.2% more parameters to the original\nstructure, in contrast to previous work. MaDi consistently achieves\ngeneralization results better than or competitive to state-of-the-art methods.\n",
        "published": "2023",
        "authors": [
            "Bram Grooten",
            "Tristan Tomilin",
            "Gautham Vasan",
            "Matthew E. Taylor",
            "A. Rupam Mahmood",
            "Meng Fang",
            "Mykola Pechenizkiy",
            "Decebal Constantin Mocanu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.00391v1",
        "title": "Controllable Safety-Critical Closed-loop Traffic Simulation via Guided\n  Diffusion",
        "abstract": "  Evaluating the performance of autonomous vehicle planning algorithms\nnecessitates simulating long-tail traffic scenarios. Traditional methods for\ngenerating safety-critical scenarios often fall short in realism and\ncontrollability. Furthermore, these techniques generally neglect the dynamics\nof agent interactions. To mitigate these limitations, we introduce a novel\nclosed-loop simulation framework rooted in guided diffusion models. Our\napproach yields two distinct advantages: 1) the generation of realistic\nlong-tail scenarios that closely emulate real-world conditions, and 2) enhanced\ncontrollability, enabling more comprehensive and interactive evaluations. We\nachieve this through novel guidance objectives that enhance road progress while\nlowering collision and off-road rates. We develop a novel approach to simulate\nsafety-critical scenarios through an adversarial term in the denoising process,\nwhich allows the adversarial agent to challenge a planner with plausible\nmaneuvers, while all agents in the scene exhibit reactive and realistic\nbehaviors. We validate our framework empirically using the NuScenes dataset,\ndemonstrating improvements in both realism and controllability. These findings\naffirm that guided diffusion models provide a robust and versatile foundation\nfor safety-critical, interactive traffic simulation, extending their utility\nacross the broader landscape of autonomous driving. For additional resources\nand demonstrations, visit our project page at https://safe-sim.github.io.\n",
        "published": "2023",
        "authors": [
            "Wei-Jer Chang",
            "Francesco Pittaluga",
            "Masayoshi Tomizuka",
            "Wei Zhan",
            "Manmohan Chandraker"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.01425v1",
        "title": "SwapTransformer: highway overtaking tactical planner model via imitation\n  learning on OSHA dataset",
        "abstract": "  This paper investigates the high-level decision-making problem in highway\nscenarios regarding lane changing and over-taking other slower vehicles. In\nparticular, this paper aims to improve the Travel Assist feature for automatic\novertaking and lane changes on highways. About 9 million samples including lane\nimages and other dynamic objects are collected in simulation. This data;\nOvertaking on Simulated HighwAys (OSHA) dataset is released to tackle this\nchallenge. To solve this problem, an architecture called SwapTransformer is\ndesigned and implemented as an imitation learning approach on the OSHA dataset.\nMoreover, auxiliary tasks such as future points and car distance network\npredictions are proposed to aid the model in better understanding the\nsurrounding environment. The performance of the proposed solution is compared\nwith a multi-layer perceptron (MLP) and multi-head self-attention networks as\nbaselines in a simulation environment. We also demonstrate the performance of\nthe model with and without auxiliary tasks. All models are evaluated based on\ndifferent metrics such as time to finish each lap, number of overtakes, and\nspeed difference with speed limit. The evaluation shows that the\nSwapTransformer model outperforms other models in different traffic densities\nin the inference phase.\n",
        "published": "2024",
        "authors": [
            "Alireza Shamsoshoara",
            "Safin B Salih",
            "Pedram Aghazadeh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.02416v1",
        "title": "ODIN: A Single Model for 2D and 3D Perception",
        "abstract": "  State-of-the-art models on contemporary 3D perception benchmarks like ScanNet\nconsume and label dataset-provided 3D point clouds, obtained through post\nprocessing of sensed multiview RGB-D images. They are typically trained\nin-domain, forego large-scale 2D pre-training and outperform alternatives that\nfeaturize the posed RGB-D multiview images instead. The gap in performance\nbetween methods that consume posed images versus post-processed 3D point clouds\nhas fueled the belief that 2D and 3D perception require distinct model\narchitectures. In this paper, we challenge this view and propose ODIN\n(Omni-Dimensional INstance segmentation), a model that can segment and label\nboth 2D RGB images and 3D point clouds, using a transformer architecture that\nalternates between 2D within-view and 3D cross-view information fusion. Our\nmodel differentiates 2D and 3D feature operations through the positional\nencodings of the tokens involved, which capture pixel coordinates for 2D patch\ntokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art\nperformance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation\nbenchmarks, and competitive performance on ScanNet, S3DIS and COCO. It\noutperforms all previous works by a wide margin when the sensed 3D point cloud\nis used in place of the point cloud sampled from 3D mesh. When used as the 3D\nperception engine in an instructable embodied agent architecture, it sets a new\nstate-of-the-art on the TEACh action-from-dialogue benchmark. Our code and\ncheckpoints can be found at the project website: https://odin-seg.github.io.\n",
        "published": "2024",
        "authors": [
            "Ayush Jain",
            "Pushkal Katara",
            "Nikolaos Gkanatsios",
            "Adam W. Harley",
            "Gabriel Sarch",
            "Kriti Aggarwal",
            "Vishrav Chaudhary",
            "Katerina Fragkiadaki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.08525v1",
        "title": "GATS: Gather-Attend-Scatter",
        "abstract": "  As the AI community increasingly adopts large-scale models, it is crucial to\ndevelop general and flexible tools to integrate them. We introduce\nGather-Attend-Scatter (GATS), a novel module that enables seamless combination\nof pretrained foundation models, both trainable and frozen, into larger\nmultimodal networks. GATS empowers AI systems to process and generate\ninformation across multiple modalities at different rates. In contrast to\ntraditional fine-tuning, GATS allows for the original component models to\nremain frozen, avoiding the risk of them losing important knowledge acquired\nduring the pretraining phase. We demonstrate the utility and versatility of\nGATS with a few experiments across games, robotics, and multimodal input-output\nsystems.\n",
        "published": "2024",
        "authors": [
            "Konrad Zolna",
            "Serkan Cabi",
            "Yutian Chen",
            "Eric Lau",
            "Claudio Fantacci",
            "Jurgis Pasukonis",
            "Jost Tobias Springenberg",
            "Sergio Gomez Colmenarejo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.03412v3",
        "title": "Convergence rates for pretraining and dropout: Guiding learning\n  parameters using network structure",
        "abstract": "  Unsupervised pretraining and dropout have been well studied, especially with\nrespect to regularization and output consistency. However, our understanding\nabout the explicit convergence rates of the parameter estimates, and their\ndependence on the learning (like denoising and dropout rate) and structural\n(like depth and layer lengths) aspects of the network is less mature. An\ninteresting question in this context is to ask if the network structure could\n\"guide\" the choices of such learning parameters. In this work, we explore these\ngaps between network structure, the learning mechanisms and their interaction\nwith parameter convergence rates. We present a way to address these issues\nbased on the backpropagation convergence rates for general nonconvex objectives\nusing first-order information. We then incorporate two learning mechanisms into\nthis general framework -- denoising autoencoder and dropout, and subsequently\nderive the convergence rates of deep networks. Building upon these bounds, we\nprovide insights into the choices of learning parameters and network sizes that\nachieve certain levels of convergence accuracy. The results derived here\nsupport existing empirical observations, and we also conduct a set of\nexperiments to evaluate them.\n",
        "published": "2015",
        "authors": [
            "Vamsi K. Ithapu",
            "Sathya Ravi",
            "Vikas Singh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1602.01168v2",
        "title": "Learning Discriminative Features via Label Consistent Neural Network",
        "abstract": "  Deep Convolutional Neural Networks (CNN) enforces supervised information only\nat the output layer, and hidden layers are trained by back propagating the\nprediction error from the output layer without explicit supervision. We propose\na supervised feature learning approach, Label Consistent Neural Network, which\nenforces direct supervision in late hidden layers. We associate each neuron in\na hidden layer with a particular class label and encourage it to be activated\nfor input signals from the same class. More specifically, we introduce a label\nconsistency regularization called \"discriminative representation error\" loss\nfor late hidden layers and combine it with classification error loss to build\nour overall objective function. This label consistency constraint alleviates\nthe common problem of gradient vanishing and tends to faster convergence; it\nalso makes the features derived from late hidden layers discriminative enough\nfor classification even using a simple $k$-NN classifier, since input signals\nfrom the same class will have very similar representations. Experimental\nresults demonstrate that our approach achieves state-of-the-art performances on\nseveral public benchmarks for action and object category recognition.\n",
        "published": "2016",
        "authors": [
            "Zhuolin Jiang",
            "Yaming Wang",
            "Larry Davis",
            "Walt Andrews",
            "Viktor Rozgic"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.01101v1",
        "title": "Adversarial Examples for Semantic Image Segmentation",
        "abstract": "  Machine learning methods in general and Deep Neural Networks in particular\nhave shown to be vulnerable to adversarial perturbations. So far this\nphenomenon has mainly been studied in the context of whole-image\nclassification. In this contribution, we analyse how adversarial perturbations\ncan affect the task of semantic segmentation. We show how existing adversarial\nattackers can be transferred to this task and that it is possible to create\nimperceptible adversarial perturbations that lead a deep network to misclassify\nalmost all pixels of a chosen class while leaving network prediction nearly\nunchanged outside this class.\n",
        "published": "2017",
        "authors": [
            "Volker Fischer",
            "Mummadi Chaithanya Kumar",
            "Jan Hendrik Metzen",
            "Thomas Brox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.00318v2",
        "title": "Towards Building an Intelligent Anti-Malware System: A Deep Learning\n  Approach using Support Vector Machine (SVM) for Malware Classification",
        "abstract": "  Effective and efficient mitigation of malware is a long-time endeavor in the\ninformation security community. The development of an anti-malware system that\ncan counteract an unknown malware is a prolific activity that may benefit\nseveral sectors. We envision an intelligent anti-malware system that utilizes\nthe power of deep learning (DL) models. Using such models would enable the\ndetection of newly-released malware through mathematical generalization. That\nis, finding the relationship between a given malware $x$ and its corresponding\nmalware family $y$, $f: x \\mapsto y$. To accomplish this feat, we used the\nMalimg dataset (Nataraj et al., 2011) which consists of malware images that\nwere processed from malware binaries, and then we trained the following DL\nmodels 1 to classify each malware family: CNN-SVM (Tang, 2013), GRU-SVM\n(Agarap, 2017), and MLP-SVM. Empirical evidence has shown that the GRU-SVM\nstands out among the DL models with a predictive accuracy of ~84.92%. This\nstands to reason for the mentioned model had the relatively most sophisticated\narchitecture design among the presented models. The exploration of an even more\noptimal DL-SVM model is the next stage towards the engineering of an\nintelligent anti-malware system.\n",
        "published": "2017",
        "authors": [
            "Abien Fred Agarap"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.10181v2",
        "title": "Unsupervised Learning by Competing Hidden Units",
        "abstract": "  It is widely believed that the backpropagation algorithm is essential for\nlearning good feature detectors in early layers of artificial neural networks,\nso that these detectors are useful for the task performed by the higher layers\nof that neural network. At the same time, the traditional form of\nbackpropagation is biologically implausible. In the present paper we propose an\nunusual learning rule, which has a degree of biological plausibility, and which\nis motivated by Hebb's idea that change of the synapse strength should be local\n- i.e. should depend only on the activities of the pre and post synaptic\nneurons. We design a learning algorithm that utilizes global inhibition in the\nhidden layer, and is capable of learning early feature detectors in a\ncompletely unsupervised way. These learned lower layer feature detectors can be\nused to train higher layer weights in a usual supervised way so that the\nperformance of the full network is comparable to the performance of standard\nfeedforward networks trained end-to-end with a backpropagation algorithm.\n",
        "published": "2018",
        "authors": [
            "Dmitry Krotov",
            "John Hopfield"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.07211v2",
        "title": "Classifiers Based on Deep Sparse Coding Architectures are Robust to Deep\n  Learning Transferable Examples",
        "abstract": "  Although deep learning has shown great success in recent years, researchers\nhave discovered a critical flaw where small, imperceptible changes in the input\nto the system can drastically change the output classification. These attacks\nare exploitable in nearly all of the existing deep learning classification\nframeworks. However, the susceptibility of deep sparse coding models to\nadversarial examples has not been examined. Here, we show that classifiers\nbased on a deep sparse coding model whose classification accuracy is\ncompetitive with a variety of deep neural network models are robust to\nadversarial examples that effectively fool those same deep learning models. We\ndemonstrate both quantitatively and qualitatively that the robustness of deep\nsparse coding models to adversarial examples arises from two key properties.\nFirst, because deep sparse coding models learn general features corresponding\nto generators of the dataset as a whole, rather than highly discriminative\nfeatures for distinguishing specific classes, the resulting classifiers are\nless dependent on idiosyncratic features that might be more easily exploited.\nSecond, because deep sparse coding models utilize fixed point attractor\ndynamics with top-down feedback, it is more difficult to find small changes to\nthe input that drive the resulting representations out of the correct attractor\nbasin.\n",
        "published": "2018",
        "authors": [
            "Jacob M. Springer",
            "Charles S. Strauss",
            "Austin M. Thresher",
            "Edward Kim",
            "Garrett T. Kenyon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.02947v1",
        "title": "Understanding the One-Pixel Attack: Propagation Maps and Locality\n  Analysis",
        "abstract": "  Deep neural networks were shown to be vulnerable to single pixel\nmodifications. However, the reason behind such phenomena has never been\nelucidated. Here, we propose Propagation Maps which show the influence of the\nperturbation in each layer of the network. Propagation Maps reveal that even in\nextremely deep networks such as Resnet, modification in one pixel easily\npropagates until the last layer. In fact, this initial local perturbation is\nalso shown to spread becoming a global one and reaching absolute difference\nvalues that are close to the maximum value of the original feature maps in a\ngiven layer. Moreover, we do a locality analysis in which we demonstrate that\nnearby pixels of the perturbed one in the one-pixel attack tend to share the\nsame vulnerability, revealing that the main vulnerability lies in neither\nneurons nor pixels but receptive fields. Hopefully, the analysis conducted in\nthis work together with a new technique called propagation maps shall shed\nlight into the inner workings of other adversarial samples and be the basis of\nnew defense systems to come.\n",
        "published": "2019",
        "authors": [
            "Danilo Vasconcellos Vargas",
            "Jiawei Su"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.07802v5",
        "title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search",
        "abstract": "  Recent state-of-the-art methods for neural architecture search (NAS) exploit\ngradient-based optimization by relaxing the problem into continuous\noptimization over architectures and shared-weights, a noisy process that\nremains poorly understood. We argue for the study of single-level empirical\nrisk minimization to understand NAS with weight-sharing, reducing the design of\nNAS methods to devising optimizers and regularizers that can quickly obtain\nhigh-quality solutions to this problem. Invoking the theory of mirror descent,\nwe present a geometry-aware framework that exploits the underlying structure of\nthis optimization to return sparse architectural parameters, leading to simple\nyet novel algorithms that enjoy fast convergence guarantees and achieve\nstate-of-the-art accuracy on the latest NAS benchmarks in computer vision.\nNotably, we exceed the best published results for both CIFAR and ImageNet on\nboth the DARTS search space and NAS-Bench201; on the latter we achieve\nnear-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory\nand experiments demonstrate a principled way to co-design optimizers and\ncontinuous relaxations of discrete NAS search spaces.\n",
        "published": "2020",
        "authors": [
            "Liam Li",
            "Mikhail Khodak",
            "Maria-Florina Balcan",
            "Ameet Talwalkar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.01003v2",
        "title": "Accurate, reliable and fast robustness evaluation",
        "abstract": "  Throughout the past five years, the susceptibility of neural networks to\nminimal adversarial perturbations has moved from a peculiar phenomenon to a\ncore issue in Deep Learning. Despite much attention, however, progress towards\nmore robust models is significantly impaired by the difficulty of evaluating\nthe robustness of neural network models. Today's methods are either fast but\nbrittle (gradient-based attacks), or they are fairly reliable but slow (score-\nand decision-based attacks). We here develop a new set of gradient-based\nadversarial attacks which (a) are more reliable in the face of gradient-masking\nthan other gradient-based attacks, (b) perform better and are more query\nefficient than current state-of-the-art gradient-based attacks, (c) can be\nflexibly adapted to a wide range of adversarial criteria and (d) require\nvirtually no hyperparameter tuning. These findings are carefully validated\nacross a diverse set of six different models and hold for L0, L1, L2 and Linf\nin both targeted as well as untargeted scenarios. Implementations will soon be\navailable in all major toolboxes (Foolbox, CleverHans and ART). We hope that\nthis class of attacks will make robustness evaluations easier and more\nreliable, thus contributing to more signal in the search for more robust\nmachine learning models.\n",
        "published": "2019",
        "authors": [
            "Wieland Brendel",
            "Jonas Rauber",
            "Matthias K\u00fcmmerer",
            "Ivan Ustyuzhaninov",
            "Matthias Bethge"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.04355v4",
        "title": "Adversarial Neural Pruning with Latent Vulnerability Suppression",
        "abstract": "  Despite the remarkable performance of deep neural networks on various\ncomputer vision tasks, they are known to be susceptible to adversarial\nperturbations, which makes it challenging to deploy them in real-world\nsafety-critical applications. In this paper, we conjecture that the leading\ncause of adversarial vulnerability is the distortion in the latent feature\nspace, and provide methods to suppress them effectively. Explicitly, we define\n\\emph{vulnerability} for each latent feature and then propose a new loss for\nadversarial learning, \\emph{Vulnerability Suppression (VS)} loss, that aims to\nminimize the feature-level vulnerability during training. We further propose a\nBayesian framework to prune features with high vulnerability to reduce both\nvulnerability and loss on adversarial samples. We validate our\n\\emph{Adversarial Neural Pruning with Vulnerability Suppression (ANP-VS)}\nmethod on multiple benchmark datasets, on which it not only obtains\nstate-of-the-art adversarial robustness but also improves the performance on\nclean examples, using only a fraction of the parameters used by the full\nnetwork. Further qualitative analysis suggests that the improvements come from\nthe suppression of feature-level vulnerability.\n",
        "published": "2019",
        "authors": [
            "Divyam Madaan",
            "Jinwoo Shin",
            "Sung Ju Hwang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.08993v1",
        "title": "Local Unsupervised Learning for Image Analysis",
        "abstract": "  Local Hebbian learning is believed to be inferior in performance to\nend-to-end training using a backpropagation algorithm. We question this popular\nbelief by designing a local algorithm that can learn convolutional filters at\nscale on large image datasets. These filters combined with patch normalization\nand very steep non-linearities result in a good classification accuracy for\nshallow networks trained locally, as opposed to end-to-end. The filters learned\nby our algorithm contain both orientation selective units and unoriented color\nunits, resembling the responses of pyramidal neurons located in the cytochrome\noxidase 'interblob' and 'blob' regions in the primary visual cortex of\nprimates. It is shown that convolutional networks with patch normalization\nsignificantly outperform standard convolutional networks on the task of\nrecovering the original classes when shadows are superimposed on top of\nstandard CIFAR-10 images. Patch normalization approximates the retinal\nadaptation to the mean light intensity, important for human vision. We also\ndemonstrate a successful transfer of learned representations between CIFAR-10\nand ImageNet 32x32 datasets. All these results taken together hint at the\npossibility that local unsupervised training might be a powerful tool for\nlearning general representations (without specifying the task) directly from\nunlabeled data.\n",
        "published": "2019",
        "authors": [
            "Leopold Grinberg",
            "John Hopfield",
            "Dmitry Krotov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1712.02328v3",
        "title": "Generative Adversarial Perturbations",
        "abstract": "  In this paper, we propose novel generative models for creating adversarial\nexamples, slightly perturbed images resembling natural images but maliciously\ncrafted to fool pre-trained models. We present trainable deep neural networks\nfor transforming images to adversarial perturbations. Our proposed models can\nproduce image-agnostic and image-dependent perturbations for both targeted and\nnon-targeted attacks. We also demonstrate that similar architectures can\nachieve impressive results in fooling classification and semantic segmentation\nmodels, obviating the need for hand-crafting attack methods for each task.\nUsing extensive experiments on challenging high-resolution datasets such as\nImageNet and Cityscapes, we show that our perturbations achieve high fooling\nrates with small perturbation norms. Moreover, our attacks are considerably\nfaster than current iterative methods at inference time.\n",
        "published": "2017",
        "authors": [
            "Omid Poursaeed",
            "Isay Katsman",
            "Bicheng Gao",
            "Serge Belongie"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1712.04248v2",
        "title": "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box\n  Machine Learning Models",
        "abstract": "  Many machine learning algorithms are vulnerable to almost imperceptible\nperturbations of their inputs. So far it was unclear how much risk adversarial\nperturbations carry for the safety of real-world machine learning applications\nbecause most methods used to generate such perturbations rely either on\ndetailed model information (gradient-based attacks) or on confidence scores\nsuch as class probabilities (score-based attacks), neither of which are\navailable in most real-world scenarios. In many such cases one currently needs\nto retreat to transfer-based attacks which rely on cumbersome substitute\nmodels, need access to the training data and can be defended against. Here we\nemphasise the importance of attacks which solely rely on the final model\ndecision. Such decision-based attacks are (1) applicable to real-world\nblack-box models such as autonomous cars, (2) need less knowledge and are\neasier to apply than transfer-based attacks and (3) are more robust to simple\ndefences than gradient- or score-based attacks. Previous attacks in this\ncategory were limited to simple models or simple datasets. Here we introduce\nthe Boundary Attack, a decision-based attack that starts from a large\nadversarial perturbation and then seeks to reduce the perturbation while\nstaying adversarial. The attack is conceptually simple, requires close to no\nhyperparameter tuning, does not rely on substitute models and is competitive\nwith the best gradient-based attacks in standard computer vision tasks like\nImageNet. We apply the attack on two black-box algorithms from Clarifai.com.\nThe Boundary Attack in particular and the class of decision-based attacks in\ngeneral open new avenues to study the robustness of machine learning models and\nraise new questions regarding the safety of deployed machine learning systems.\nAn implementation of the attack is available as part of Foolbox at\nhttps://github.com/bethgelab/foolbox .\n",
        "published": "2017",
        "authors": [
            "Wieland Brendel",
            "Jonas Rauber",
            "Matthias Bethge"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.03721v2",
        "title": "Deep Asymmetric Networks with a Set of Node-wise Variant Activation\n  Functions",
        "abstract": "  This work presents deep asymmetric networks with a set of node-wise variant\nactivation functions. The nodes' sensitivities are affected by activation\nfunction selections such that the nodes with smaller indices become\nincreasingly more sensitive. As a result, features learned by the nodes are\nsorted by the node indices in the order of their importance. Asymmetric\nnetworks not only learn input features but also the importance of those\nfeatures. Nodes of lesser importance in asymmetric networks can be pruned to\nreduce the complexity of the networks, and the pruned networks can be retrained\nwithout incurring performance losses. We validate the feature-sorting property\nusing both shallow and deep asymmetric networks as well as deep asymmetric\nnetworks transferred from famous networks.\n",
        "published": "2018",
        "authors": [
            "Jinhyeok Jang",
            "Hyunjoong Cho",
            "Jaehong Kim",
            "Jaeyeon Lee",
            "Seungjoon Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.04430v3",
        "title": "Deep learning to achieve clinically applicable segmentation of head and\n  neck anatomy for radiotherapy",
        "abstract": "  Over half a million individuals are diagnosed with head and neck cancer each\nyear worldwide. Radiotherapy is an important curative treatment for this\ndisease, but it requires manual time consuming delineation of radio-sensitive\norgans at risk (OARs). This planning process can delay treatment, while also\nintroducing inter-operator variability with resulting downstream radiation dose\ndifferences. While auto-segmentation algorithms offer a potentially time-saving\nsolution, the challenges in defining, quantifying and achieving expert\nperformance remain. Adopting a deep learning approach, we demonstrate a 3D\nU-Net architecture that achieves expert-level performance in delineating 21\ndistinct head and neck OARs commonly segmented in clinical practice. The model\nwas trained on a dataset of 663 deidentified computed tomography (CT) scans\nacquired in routine clinical practice and with both segmentations taken from\nclinical practice and segmentations created by experienced radiographers as\npart of this research, all in accordance with consensus OAR definitions. We\ndemonstrate the model's clinical applicability by assessing its performance on\na test set of 21 CT scans from clinical practice, each with the 21 OARs\nsegmented by two independent experts. We also introduce surface Dice similarity\ncoefficient (surface DSC), a new metric for the comparison of organ\ndelineation, to quantify deviation between OAR surface contours rather than\nvolumes, better reflecting the clinical task of correcting errors in the\nautomated organ segmentations. The model's generalisability is then\ndemonstrated on two distinct open source datasets, reflecting different centres\nand countries to model training. With appropriate validation studies and\nregulatory approvals, this system could improve the efficiency, consistency,\nand safety of radiotherapy pathways.\n",
        "published": "2018",
        "authors": [
            "Stanislav Nikolov",
            "Sam Blackwell",
            "Alexei Zverovitch",
            "Ruheena Mendes",
            "Michelle Livne",
            "Jeffrey De Fauw",
            "Yojan Patel",
            "Clemens Meyer",
            "Harry Askham",
            "Bernardino Romera-Paredes",
            "Christopher Kelly",
            "Alan Karthikesalingam",
            "Carlton Chu",
            "Dawn Carnell",
            "Cheng Boon",
            "Derek D'Souza",
            "Syed Ali Moinuddin",
            "Bethany Garie",
            "Yasmin McQuinlan",
            "Sarah Ireland",
            "Kiarna Hampton",
            "Krystle Fuller",
            "Hugh Montgomery",
            "Geraint Rees",
            "Mustafa Suleyman",
            "Trevor Back",
            "C\u00edan Hughes",
            "Joseph R. Ledsam",
            "Olaf Ronneberger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.11359v2",
        "title": "Scaling up the randomized gradient-free adversarial attack reveals\n  overestimation of robustness using established attacks",
        "abstract": "  Modern neural networks are highly non-robust against adversarial\nmanipulation. A significant amount of work has been invested in techniques to\ncompute lower bounds on robustness through formal guarantees and to build\nprovably robust models. However, it is still difficult to get guarantees for\nlarger networks or robustness against larger perturbations. Thus attack\nstrategies are needed to provide tight upper bounds on the actual robustness.\nWe significantly improve the randomized gradient-free attack for ReLU networks\n[9], in particular by scaling it up to large networks. We show that our attack\nachieves similar or significantly smaller robust accuracy than state-of-the-art\nattacks like PGD or the one of Carlini and Wagner, thus revealing an\noverestimation of the robustness by these state-of-the-art methods. Our attack\nis not based on a gradient descent scheme and in this sense gradient-free,\nwhich makes it less sensitive to the choice of hyperparameters as no careful\nselection of the stepsize is required.\n",
        "published": "2019",
        "authors": [
            "Francesco Croce",
            "Jonas Rauber",
            "Matthias Hein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.01494v1",
        "title": "Supervised and Unsupervised End-to-End Deep Learning for Gene Ontology\n  Classification of Neural In Situ Hybridization Images",
        "abstract": "  In recent years, large datasets of high-resolution mammalian neural images\nhave become available, which has prompted active research on the analysis of\ngene expression data. Traditional image processing methods are typically\napplied for learning functional representations of genes, based on their\nexpressions in these brain images. In this paper, we describe a novel\nend-to-end deep learning-based method for generating compact representations of\nin situ hybridization (ISH) images, which are invariant-to-translation. In\ncontrast to traditional image processing methods, our method relies, instead,\non deep convolutional denoising autoencoders (CDAE) for processing raw pixel\ninputs, and generating the desired compact image representations. We provide an\nin-depth description of our deep learning-based approach, and present extensive\nexperimental results, demonstrating that representations extracted by CDAE can\nhelp learn features of functional gene ontology categories for their\nclassification in a highly accurate manner. Our methods improve the previous\nstate-of-the-art classification rate (Liscovitch, et al.) from an average AUC\nof 0.92 to 0.997, i.e., it achieves 96% reduction in error rate. Furthermore,\nthe representation vectors generated due to our method are more compact in\ncomparison to previous state-of-the-art methods, allowing for a more efficient\nhigh-level representation of images. These results are obtained with\nsignificantly downsampled images in comparison to the original high-resolution\nones, further underscoring the robustness of our proposed method.\n",
        "published": "2019",
        "authors": [
            "Ido Cohen",
            "Eli David",
            "Nathan S. Netanyahu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.03310v1",
        "title": "Geometric Capsule Autoencoders for 3D Point Clouds",
        "abstract": "  We propose a method to learn object representations from 3D point clouds\nusing bundles of geometrically interpretable hidden units, which we call\ngeometric capsules. Each geometric capsule represents a visual entity, such as\nan object or a part, and consists of two components: a pose and a feature. The\npose encodes where the entity is, while the feature encodes what it is. We use\nthese capsules to construct a Geometric Capsule Autoencoder that learns to\ngroup 3D points into parts (small local surfaces), and these parts into the\nwhole object, in an unsupervised manner. Our novel Multi-View Agreement voting\nmechanism is used to discover an object's canonical pose and its pose-invariant\nfeature vector. Using the ShapeNet and ModelNet40 datasets, we analyze the\nproperties of the learned representations and show the benefits of having\nmultiple votes agree. We perform alignment and retrieval of arbitrarily rotated\nobjects -- tasks that evaluate our model's object identification and canonical\npose recovery capabilities -- and obtained insightful results.\n",
        "published": "2019",
        "authors": [
            "Nitish Srivastava",
            "Hanlin Goh",
            "Ruslan Salakhutdinov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.04958v2",
        "title": "Analyzing and Improving the Image Quality of StyleGAN",
        "abstract": "  The style-based GAN architecture (StyleGAN) yields state-of-the-art results\nin data-driven unconditional generative image modeling. We expose and analyze\nseveral of its characteristic artifacts, and propose changes in both model\narchitecture and training methods to address them. In particular, we redesign\nthe generator normalization, revisit progressive growing, and regularize the\ngenerator to encourage good conditioning in the mapping from latent codes to\nimages. In addition to improving image quality, this path length regularizer\nyields the additional benefit that the generator becomes significantly easier\nto invert. This makes it possible to reliably attribute a generated image to a\nparticular network. We furthermore visualize how well the generator utilizes\nits output resolution, and identify a capacity problem, motivating us to train\nlarger models for additional quality improvements. Overall, our improved model\nredefines the state of the art in unconditional image modeling, both in terms\nof existing distribution quality metrics as well as perceived image quality.\n",
        "published": "2019",
        "authors": [
            "Tero Karras",
            "Samuli Laine",
            "Miika Aittala",
            "Janne Hellsten",
            "Jaakko Lehtinen",
            "Timo Aila"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.05844v1",
        "title": "Adversarial Example Generation using Evolutionary Multi-objective\n  Optimization",
        "abstract": "  This paper proposes Evolutionary Multi-objective Optimization (EMO)-based\nAdversarial Example (AE) design method that performs under black-box setting.\nPrevious gradient-based methods produce AEs by changing all pixels of a target\nimage, while previous EC-based method changes small number of pixels to produce\nAEs. Thanks to EMO's property of population based-search, the proposed method\nproduces various types of AEs involving ones locating between AEs generated by\nthe previous two approaches, which helps to know the characteristics of a\ntarget model or to know unknown attack patterns. Experimental results showed\nthe potential of the proposed method, e.g., it can generate robust AEs and,\nwith the aid of DCT-based perturbation pattern generation, AEs for high\nresolution images.\n",
        "published": "2019",
        "authors": [
            "Takahiro Suzuki",
            "Shingo Takeshita",
            "Satoshi Ono"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.09363v2",
        "title": "Building One-Shot Semi-supervised (BOSS) Learning up to Fully Supervised\n  Performance",
        "abstract": "  Reaching the performance of fully supervised learning with unlabeled data and\nonly labeling one sample per class might be ideal for deep learning\napplications. We demonstrate for the first time the potential for building\none-shot semi-supervised (BOSS) learning on Cifar-10 and SVHN up to attain test\naccuracies that are comparable to fully supervised learning. Our method\ncombines class prototype refining, class balancing, and self-training. A good\nprototype choice is essential and we propose a technique for obtaining iconic\nexamples. In addition, we demonstrate that class balancing methods\nsubstantially improve accuracy results in semi-supervised learning to levels\nthat allow self-training to reach the level of fully supervised learning\nperformance. Rigorous empirical evaluations provide evidence that labeling\nlarge datasets is not necessary for training deep neural networks. We made our\ncode available at https://github.com/lnsmith54/BOSS to facilitate replication\nand for use with future real-world applications.\n",
        "published": "2020",
        "authors": [
            "Leslie N. Smith",
            "Adam Conovaloff"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.12379v1",
        "title": "Deep Learning feature selection to unhide demographic recommender\n  systems factors",
        "abstract": "  Extracting demographic features from hidden factors is an innovative concept\nthat provides multiple and relevant applications. The matrix factorization\nmodel generates factors which do not incorporate semantic knowledge. This paper\nprovides a deep learning-based method: DeepUnHide, able to extract demographic\ninformation from the users and items factors in collaborative filtering\nrecommender systems. The core of the proposed method is the gradient-based\nlocalization used in the image processing literature to highlight the\nrepresentative areas of each classification class. Validation experiments make\nuse of two public datasets and current baselines. Results show the superiority\nof DeepUnHide to make feature selection and demographic classification,\ncompared to the state of art of feature selection methods. Relevant and direct\napplications include recommendations explanation, fairness in collaborative\nfiltering and recommendation to groups of users.\n",
        "published": "2020",
        "authors": [
            "Jes\u00fas Bobadilla",
            "\u00c1ngel Gonz\u00e1lez-Prieto",
            "Fernando Ortega",
            "Ra\u00fal Lara-Cabrera"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.08282v3",
        "title": "Out-of-Sample Extension for Dimensionality Reduction of Noisy Time\n  Series",
        "abstract": "  This paper proposes an out-of-sample extension framework for a global\nmanifold learning algorithm (Isomap) that uses temporal information in\nout-of-sample points in order to make the embedding more robust to noise and\nartifacts. Given a set of noise-free training data and its embedding, the\nproposed framework extends the embedding for a noisy time series. This is\nachieved by adding a spatio-temporal compactness term to the optimization\nobjective of the embedding. To the best of our knowledge, this is the first\nmethod for out-of-sample extension of manifold embeddings that leverages timing\ninformation available for the extension set. Experimental results demonstrate\nthat our out-of-sample extension algorithm renders a more robust and accurate\nembedding of sequentially ordered image data in the presence of various noise\nand artifacts when compared to other timing-aware embeddings. Additionally, we\nshow that an out-of-sample extension framework based on the proposed algorithm\noutperforms the state of the art in eye-gaze estimation.\n",
        "published": "2016",
        "authors": [
            "Hamid Dadkhahi",
            "Marco F. Duarte",
            "Benjamin Marlin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.11797v2",
        "title": "Grow and Prune Compact, Fast, and Accurate LSTMs",
        "abstract": "  Long short-term memory (LSTM) has been widely used for sequential data\nmodeling. Researchers have increased LSTM depth by stacking LSTM cells to\nimprove performance. This incurs model redundancy, increases run-time delay,\nand makes the LSTMs more prone to overfitting. To address these problems, we\npropose a hidden-layer LSTM (H-LSTM) that adds hidden layers to LSTM's original\none level non-linear control gates. H-LSTM increases accuracy while employing\nfewer external stacked layers, thus reducing the number of parameters and\nrun-time latency significantly. We employ grow-and-prune (GP) training to\niteratively adjust the hidden layers through gradient-based growth and\nmagnitude-based pruning of connections. This learns both the weights and the\ncompact architecture of H-LSTM control gates. We have GP-trained H-LSTMs for\nimage captioning and speech recognition applications. For the NeuralTalk\narchitecture on the MSCOCO dataset, our three models reduce the number of\nparameters by 38.7x [floating-point operations (FLOPs) by 45.5x], run-time\nlatency by 4.5x, and improve the CIDEr score by 2.6. For the DeepSpeech2\narchitecture on the AN4 dataset, our two models reduce the number of parameters\nby 19.4x (FLOPs by 23.5x), run-time latency by 15.7%, and the word error rate\nfrom 12.9% to 8.7%. Thus, GP-trained H-LSTMs can be seen to be compact, fast,\nand accurate.\n",
        "published": "2018",
        "authors": [
            "Xiaoliang Dai",
            "Hongxu Yin",
            "Niraj K. Jha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.08121v1",
        "title": "An Improvement of Data Classification Using Random Multimodel Deep\n  Learning (RMDL)",
        "abstract": "  The exponential growth in the number of complex datasets every year requires\nmore enhancement in machine learning methods to provide robust and accurate\ndata classification. Lately, deep learning approaches have achieved surpassing\nresults in comparison to previous machine learning algorithms. However, finding\nthe suitable structure for these models has been a challenge for researchers.\nThis paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble,\ndeep learning approach for classification. RMDL solves the problem of finding\nthe best deep learning structure and architecture while simultaneously\nimproving robustness and accuracy through ensembles of deep learning\narchitectures. In short, RMDL trains multiple randomly generated models of Deep\nNeural Network (DNN), Convolutional Neural Network (CNN) and Recurrent Neural\nNetwork (RNN) in parallel and combines their results to produce better result\nof any of those models individually. In this paper, we describe RMDL model and\ncompare the results for image and text classification as well as face\nrecognition. We used MNIST and CIFAR-10 datasets as ground truth datasets for\nimage classification and WOS, Reuters, IMDB, and 20newsgroup datasets for text\nclassification. Lastly, we used ORL dataset to compare the model performance on\nface recognition task.\n",
        "published": "2018",
        "authors": [
            "Mojtaba Heidarysafa",
            "Kamran Kowsari",
            "Donald E. Brown",
            "Kiana Jafari Meimandi",
            "Laura E. Barnes"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.01406v1",
        "title": "Super-Resolution via Conditional Implicit Maximum Likelihood Estimation",
        "abstract": "  Single-image super-resolution (SISR) is a canonical problem with diverse\napplications. Leading methods like SRGAN produce images that contain various\nartifacts, such as high-frequency noise, hallucinated colours and shape\ndistortions, which adversely affect the realism of the result. In this paper,\nwe propose an alternative approach based on an extension of the method of\nImplicit Maximum Likelihood Estimation (IMLE). We demonstrate greater\neffectiveness at noise reduction and preservation of the original colours and\nshapes, yielding more realistic super-resolved images.\n",
        "published": "2018",
        "authors": [
            "Ke Li",
            "Shichong Peng",
            "Jitendra Malik"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.09619v2",
        "title": "Sparse DNNs with Improved Adversarial Robustness",
        "abstract": "  Deep neural networks (DNNs) are computationally/memory-intensive and\nvulnerable to adversarial attacks, making them prohibitive in some real-world\napplications. By converting dense models into sparse ones, pruning appears to\nbe a promising solution to reducing the computation/memory cost. This paper\nstudies classification models, especially DNN-based ones, to demonstrate that\nthere exists intrinsic relationships between their sparsity and adversarial\nrobustness. Our analyses reveal, both theoretically and empirically, that\nnonlinear DNN-based classifiers behave differently under $l_2$ attacks from\nsome linear ones. We further demonstrate that an appropriately higher model\nsparsity implies better robustness of nonlinear DNNs, whereas over-sparsified\nmodels can be more difficult to resist adversarial examples.\n",
        "published": "2018",
        "authors": [
            "Yiwen Guo",
            "Chao Zhang",
            "Changshui Zhang",
            "Yurong Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.09945v2",
        "title": "Analyzing Neuroimaging Data Through Recurrent Deep Learning Models",
        "abstract": "  The application of deep learning (DL) models to neuroimaging data poses\nseveral challenges, due to the high dimensionality, low sample size and complex\ntemporo-spatial dependency structure of these datasets. Even further, DL models\nact as as black-box models, impeding insight into the association of cognitive\nstate and brain activity. To approach these challenges, we introduce the\nDeepLight framework, which utilizes long short-term memory (LSTM) based DL\nmodels to analyze whole-brain functional Magnetic Resonance Imaging (fMRI)\ndata. To decode a cognitive state (e.g., seeing the image of a house),\nDeepLight separates the fMRI volume into a sequence of axial brain slices,\nwhich is then sequentially processed by an LSTM. To maintain interpretability,\nDeepLight adapts the layer-wise relevance propagation (LRP) technique. Thereby,\ndecomposing its decoding decision into the contributions of the single input\nvoxels to this decision. Importantly, the decomposition is performed on the\nlevel of single fMRI volumes, enabling DeepLight to study the associations\nbetween cognitive state and brain activity on several levels of data\ngranularity, from the level of the group down to the level of single time\npoints. To demonstrate the versatility of DeepLight, we apply it to a large\nfMRI dataset of the Human Connectome Project. We show that DeepLight\noutperforms conventional approaches of uni- and multivariate fMRI analysis in\ndecoding the cognitive states and in identifying the physiologically\nappropriate brain regions associated with these states. We further demonstrate\nDeepLight's ability to study the fine-grained temporo-spatial variability of\nbrain activity over sequences of single fMRI samples.\n",
        "published": "2018",
        "authors": [
            "Armin W. Thomas",
            "Hauke R. Heekeren",
            "Klaus-Robert M\u00fcller",
            "Wojciech Samek"
        ]
    }
]