[
    {
        "id": "http://arxiv.org/abs/2208.14937v1",
        "title": "Explainable Artificial Intelligence Applications in Cyber Security:\n  State-of-the-Art in Research",
        "abstract": "  This survey presents a comprehensive review of current literature on\nExplainable Artificial Intelligence (XAI) methods for cyber security\napplications. Due to the rapid development of Internet-connected systems and\nArtificial Intelligence in recent years, Artificial Intelligence including\nMachine Learning (ML) and Deep Learning (DL) has been widely utilized in the\nfields of cyber security including intrusion detection, malware detection, and\nspam filtering. However, although Artificial Intelligence-based approaches for\nthe detection and defense of cyber attacks and threats are more advanced and\nefficient compared to the conventional signature-based and rule-based cyber\nsecurity strategies, most ML-based techniques and DL-based techniques are\ndeployed in the black-box manner, meaning that security experts and customers\nare unable to explain how such procedures reach particular conclusions. The\ndeficiencies of transparency and interpretability of existing Artificial\nIntelligence techniques would decrease human users' confidence in the models\nutilized for the defense against cyber attacks, especially in current\nsituations where cyber attacks become increasingly diverse and complicated.\nTherefore, it is essential to apply XAI in the establishment of cyber security\nmodels to create more explainable models while maintaining high accuracy and\nallowing human users to comprehend, trust, and manage the next generation of\ncyber defense mechanisms. Although there are papers reviewing Artificial\nIntelligence applications in cyber security areas and the vast literature on\napplying XAI in many fields including healthcare, financial services, and\ncriminal justice, the surprising fact is that there are currently no survey\nresearch articles that concentrate on XAI applications in cyber security.\n",
        "published": "2022",
        "authors": [
            "Zhibo Zhang",
            "Hussam Al Hamadi",
            "Ernesto Damiani",
            "Chan Yeob Yeun",
            "Fatma Taher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.09756v1",
        "title": "PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python",
        "abstract": "  Machine learning is a general-purpose technology holding promises for many\ninterdisciplinary research problems. However, significant barriers exist in\ncrossing disciplinary boundaries when most machine learning tools are developed\nin different areas separately. We present Pykale - a Python library for\nknowledge-aware machine learning on graphs, images, texts, and videos to enable\nand accelerate interdisciplinary research. We formulate new green machine\nlearning guidelines based on standard software engineering practices and\npropose a novel pipeline-based application programming interface (API). PyKale\nfocuses on leveraging knowledge from multiple sources for accurate and\ninterpretable prediction, thus supporting multimodal learning and transfer\nlearning (particularly domain adaptation) with latest deep learning and\ndimensionality reduction models. We build PyKale on PyTorch and leverage the\nrich PyTorch ecosystem. Our pipeline-based API design enforces standardization\nand minimalism, embracing green machine learning concepts via reducing\nrepetitions and redundancy, reusing existing resources, and recycling learning\nmodels across areas. We demonstrate its interdisciplinary nature via examples\nin bioinformatics, knowledge graph, image/video recognition, and medical\nimaging.\n",
        "published": "2021",
        "authors": [
            "Haiping Lu",
            "Xianyuan Liu",
            "Robert Turner",
            "Peizhen Bai",
            "Raivo E Koot",
            "Shuo Zhou",
            "Mustafa Chasmai",
            "Lawrence Schobs"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.07994v1",
        "title": "Binary Black-box Evasion Attacks Against Deep Learning-based Static\n  Malware Detectors with Adversarial Byte-Level Language Model",
        "abstract": "  Anti-malware engines are the first line of defense against malicious\nsoftware. While widely used, feature engineering-based anti-malware engines are\nvulnerable to unseen (zero-day) attacks. Recently, deep learning-based static\nanti-malware detectors have achieved success in identifying unseen attacks\nwithout requiring feature engineering and dynamic analysis. However, these\ndetectors are susceptible to malware variants with slight perturbations, known\nas adversarial examples. Generating effective adversarial examples is useful to\nreveal the vulnerabilities of such systems. Current methods for launching such\nattacks require accessing either the specifications of the targeted\nanti-malware model, the confidence score of the anti-malware response, or\ndynamic malware analysis, which are either unrealistic or expensive. We propose\nMalRNN, a novel deep learning-based approach to automatically generate evasive\nmalware variants without any of these restrictions. Our approach features an\nadversarial example generation process, which learns a language model via a\ngenerative sequence-to-sequence recurrent neural network to augment malware\nbinaries. MalRNN effectively evades three recent deep learning-based malware\ndetectors and outperforms current benchmark methods. Findings from applying our\nMalRNN on a real dataset with eight malware categories are discussed.\n",
        "published": "2020",
        "authors": [
            "Mohammadreza Ebrahimi",
            "Ning Zhang",
            "James Hu",
            "Muhammad Taqi Raza",
            "Hsinchun Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.07253v1",
        "title": "Quantifying Uncertainties in Natural Language Processing Tasks",
        "abstract": "  Reliable uncertainty quantification is a first step towards building\nexplainable, transparent, and accountable artificial intelligent systems.\nRecent progress in Bayesian deep learning has made such quantification\nrealizable. In this paper, we propose novel methods to study the benefits of\ncharacterizing model and data uncertainties for natural language processing\n(NLP) tasks. With empirical experiments on sentiment analysis, named entity\nrecognition, and language modeling using convolutional and recurrent neural\nnetwork models, we show that explicitly modeling uncertainties is not only\nnecessary to measure output confidence levels, but also useful at enhancing\nmodel performances in various NLP tasks.\n",
        "published": "2018",
        "authors": [
            "Yijun Xiao",
            "William Yang Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.03775v2",
        "title": "Creative AI Through Evolutionary Computation",
        "abstract": "  The main power of artificial intelligence is not in modeling what we already\nknow, but in creating solutions that are new. Such solutions exist in extremely\nlarge, high-dimensional, and complex search spaces. Population-based search\ntechniques, i.e. variants of evolutionary computation, are well suited to\nfinding them. These techniques are also well positioned to take advantage of\nlarge-scale parallel computing resources, making creative AI through\nevolutionary computation the likely \"next deep learning\".\n",
        "published": "2019",
        "authors": [
            "Risto Miikkulainen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.04212v3",
        "title": "Creative AI Through Evolutionary Computation: Principles and Examples",
        "abstract": "  The main power of artificial intelligence is not in modeling what we already\nknow, but in creating solutions that are new. Such solutions exist in extremely\nlarge, high-dimensional, and complex search spaces. Population-based search\ntechniques, i.e. variants of evolutionary computation, are well suited to\nfinding them. These techniques make it possible to find creative solutions to\npractical problems in the real world, making creative AI through evolutionary\ncomputation the likely \"next deep learning.\"\n",
        "published": "2020",
        "authors": [
            "Risto Miikkulainen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.02831v1",
        "title": "A Tale of Fairness Revisited: Beyond Adversarial Learning for Deep\n  Neural Network Fairness",
        "abstract": "  Motivated by the need for fair algorithmic decision making in the age of\nautomation and artificially-intelligent technology, this technical report\nprovides a theoretical insight into adversarial training for fairness in deep\nlearning. We build upon previous work in adversarial fairness, show the\npersistent tradeoff between fair predictions and model performance, and explore\nfurther mechanisms that help in offsetting this tradeoff.\n",
        "published": "2021",
        "authors": [
            "Becky Mashaido",
            "Winston Moh Tangongho"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.06118v1",
        "title": "DeepCreativity: Measuring Creativity with Deep Learning Techniques",
        "abstract": "  Measuring machine creativity is one of the most fascinating challenges in\nArtificial Intelligence. This paper explores the possibility of using\ngenerative learning techniques for automatic assessment of creativity. The\nproposed solution does not involve human judgement, it is modular and of\ngeneral applicability. We introduce a new measure, namely DeepCreativity, based\non Margaret Boden's definition of creativity as composed by value, novelty and\nsurprise. We evaluate our methodology (and related measure) considering a case\nstudy, i.e., the generation of 19th century American poetry, showing its\neffectiveness and expressiveness.\n",
        "published": "2022",
        "authors": [
            "Giorgio Franceschelli",
            "Mirco Musolesi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.13471v1",
        "title": "Characterising Research Areas in the field of AI",
        "abstract": "  Interest in Artificial Intelligence (AI) continues to grow rapidly, hence it\nis crucial to support researchers and organisations in understanding where AI\nresearch is heading. In this study, we conducted a bibliometric analysis on\n257K articles in AI, retrieved from OpenAlex. We identified the main conceptual\nthemes by performing clustering analysis on the co-occurrence network of\ntopics. Finally, we observed how such themes evolved over time. The results\nhighlight the growing academic interest in research themes like deep learning,\nmachine learning, and internet of things.\n",
        "published": "2022",
        "authors": [
            "Alessandra Belfiore",
            "Angelo Salatino",
            "Francesco Osborne"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.15092v1",
        "title": "Can We Rely on AI?",
        "abstract": "  Over the last decade, adversarial attack algorithms have revealed\ninstabilities in deep learning tools. These algorithms raise issues regarding\nsafety, reliability and interpretability in artificial intelligence; especially\nin high risk settings. From a practical perspective, there has been a war of\nescalation between those developing attack and defence strategies. At a more\ntheoretical level, researchers have also studied bigger picture questions\nconcerning the existence and computability of attacks. Here we give a brief\noverview of the topic, focusing on aspects that are likely to be of interest to\nresearchers in applied and computational mathematics.\n",
        "published": "2023",
        "authors": [
            "Desmond J. Higham"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.13691v1",
        "title": "Neural-Base Music Generation for Intelligence Duplication",
        "abstract": "  There are two aspects of machine learning and artificial intelligence: (1)\ninterpreting information, and (2) inventing new useful information. Much\nadvance has been made for (1) with a focus on pattern recognition techniques\n(e.g., interpreting visual data). This paper focuses on (2) with intelligent\nduplication (ID) for invention. We explore the possibility of learning a\nspecific individual's creative reasoning in order to leverage the learned\nexpertise and talent to invent new information. More specifically, we employ a\ndeep learning system to learn from the great composer Beethoven and capture his\ncomposition ability in a hash-based knowledge base. This new form of knowledge\nbase provides a reasoning facility to drive the music composition through a\nnovel music generation method.\n",
        "published": "2023",
        "authors": [
            "Jacob Galajda",
            "Kien Hua"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.02532v3",
        "title": "End-To-End Bias Mitigation: Removing Gender Bias in Deep Learning",
        "abstract": "  Machine Learning models have been deployed across many different aspects of\nsociety, often in situations that affect social welfare. Although these models\noffer streamlined solutions to large problems, they may contain biases and\ntreat groups or individuals unfairly based on protected attributes such as\ngender. In this paper, we introduce several examples of machine learning gender\nbias in practice followed by formalizations of fairness. We provide a survey of\nfairness research by detailing influential pre-processing, in-processing, and\npost-processing bias mitigation algorithms. We then propose an end-to-end bias\nmitigation framework, which employs a fusion of pre-, in-, and post-processing\nmethods to leverage the strengths of each individual technique. We test this\nmethod, along with the standard techniques we review, on a deep neural network\nto analyze bias mitigation in a deep learning setting. We find that our\nend-to-end bias mitigation framework outperforms the baselines with respect to\nseveral fairness metrics, suggesting its promise as a method for improving\nfairness. As society increasingly relies on artificial intelligence to help in\ndecision-making, addressing gender biases present in deep learning models is\nimperative. To provide readers with the tools to assess the fairness of machine\nlearning models and mitigate the biases present in them, we discuss multiple\nopen source packages for fairness in AI.\n",
        "published": "2021",
        "authors": [
            "Tal Feldman",
            "Ashley Peake"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.04548v1",
        "title": "Deep Learning for Procedural Content Generation",
        "abstract": "  Procedural content generation in video games has a long history. Existing\nprocedural content generation methods, such as search-based, solver-based,\nrule-based and grammar-based methods have been applied to various content types\nsuch as levels, maps, character models, and textures. A research field centered\non content generation in games has existed for more than a decade. More\nrecently, deep learning has powered a remarkable range of inventions in content\nproduction, which are applicable to games. While some cutting-edge deep\nlearning methods are applied on their own, others are applied in combination\nwith more traditional methods, or in an interactive setting. This article\nsurveys the various deep learning methods that have been applied to generate\ngame content directly or indirectly, discusses deep learning methods that could\nbe used for content generation purposes but are rarely used today, and\nenvisages some limitations and potential future directions of deep learning for\nprocedural content generation.\n",
        "published": "2020",
        "authors": [
            "Jialin Liu",
            "Sam Snodgrass",
            "Ahmed Khalifa",
            "Sebastian Risi",
            "Georgios N. Yannakakis",
            "Julian Togelius"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.08387v6",
        "title": "A Survey on Ensemble Learning under the Era of Deep Learning",
        "abstract": "  Due to the dominant position of deep learning (mostly deep neural networks)\nin various artificial intelligence applications, recently, ensemble learning\nbased on deep neural networks (ensemble deep learning) has shown significant\nperformances in improving the generalization of learning system. However, since\nmodern deep neural networks usually have millions to billions of parameters,\nthe time and space overheads for training multiple base deep learners and\ntesting with the ensemble deep learner are far greater than that of traditional\nensemble learning. Though several algorithms of fast ensemble deep learning\nhave been proposed to promote the deployment of ensemble deep learning in some\napplications, further advances still need to be made for many applications in\nspecific fields, where the developing time and computing resources are usually\nrestricted or the data to be processed is of large dimensionality. An urgent\nproblem needs to be solved is how to take the significant advantages of\nensemble deep learning while reduce the required expenses so that many more\napplications in specific fields can benefit from it. For the alleviation of\nthis problem, it is essential to know about how ensemble learning has developed\nunder the era of deep learning. Thus, in this article, we present fundamental\ndiscussions focusing on data analyses of published works, methodologies, recent\nadvances and unattainability of traditional ensemble learning and ensemble deep\nlearning. We hope this article will be helpful to realize the intrinsic\nproblems and technical challenges faced by future developments of ensemble\nlearning under the era of deep learning.\n",
        "published": "2021",
        "authors": [
            "Yongquan Yang",
            "Haijun Lv",
            "Ning Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.08546v1",
        "title": "A Generative Model based Adversarial Security of Deep Learning and\n  Linear Classifier Models",
        "abstract": "  In recent years, machine learning algorithms have been applied widely in\nvarious fields such as health, transportation, and the autonomous car. With the\nrapid developments of deep learning techniques, it is critical to take the\nsecurity concern into account for the application of the algorithms. While\nmachine learning offers significant advantages in terms of the application of\nalgorithms, the issue of security is ignored. Since it has many applications in\nthe real world, security is a vital part of the algorithms. In this paper, we\nhave proposed a mitigation method for adversarial attacks against machine\nlearning models with an autoencoder model that is one of the generative ones.\nThe main idea behind adversarial attacks against machine learning models is to\nproduce erroneous results by manipulating trained models. We have also\npresented the performance of autoencoder models to various attack methods from\ndeep neural networks to traditional algorithms by using different methods such\nas non-targeted and targeted attacks to multi-class logistic regression, a fast\ngradient sign method, a targeted fast gradient sign method and a basic\niterative method attack to neural networks for the MNIST dataset.\n",
        "published": "2020",
        "authors": [
            "erhat Ozgur Catak",
            "Samed Sivaslioglu",
            "Kevser Sahinbas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.01024v1",
        "title": "Hybrid Quantum Neural Network in High-dimensional Data Classification",
        "abstract": "  The research explores the potential of quantum deep learning models to\naddress challenging machine learning problems that classical deep learning\nmodels find difficult to tackle. We introduce a novel model architecture that\ncombines classical convolutional layers with a quantum neural network, aiming\nto surpass state-of-the-art accuracy while maintaining a compact model size.\nThe experiment is to classify high-dimensional audio data from the Bird-CLEF\n2021 dataset. Our evaluation focuses on key metrics, including training\nduration, model accuracy, and total model size. This research demonstrates the\npromising potential of quantum machine learning in enhancing machine learning\ntasks and solving practical machine learning challenges available today.\n",
        "published": "2023",
        "authors": [
            "Hao-Yuan Chen",
            "Yen-Jui Chang",
            "Shih-Wei Liao",
            "Ching-Ray Chang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1701.04503v1",
        "title": "Deep Learning for Computational Chemistry",
        "abstract": "  The rise and fall of artificial neural networks is well documented in the\nscientific literature of both computer science and computational chemistry. Yet\nalmost two decades later, we are now seeing a resurgence of interest in deep\nlearning, a machine learning algorithm based on multilayer neural networks.\nWithin the last few years, we have seen the transformative impact of deep\nlearning in many domains, particularly in speech recognition and computer\nvision, to the extent that the majority of expert practitioners in those field\nare now regularly eschewing prior established models in favor of deep learning\nmodels. In this review, we provide an introductory overview into the theory of\ndeep neural networks and their unique properties that distinguish them from\ntraditional machine learning algorithms used in cheminformatics. By providing\nan overview of the variety of emerging applications of deep neural networks, we\nhighlight its ubiquity and broad applicability to a wide range of challenges in\nthe field, including QSAR, virtual screening, protein structure prediction,\nquantum chemistry, materials design and property prediction. In reviewing the\nperformance of deep neural networks, we observed a consistent outperformance\nagainst non-neural networks state-of-the-art models across disparate research\ntopics, and deep neural network based models often exceeded the \"glass ceiling\"\nexpectations of their respective tasks. Coupled with the maturity of\nGPU-accelerated computing for training deep neural networks and the exponential\ngrowth of chemical data on which to train these networks on, we anticipate that\ndeep learning algorithms will be a valuable tool for computational chemistry.\n",
        "published": "2017",
        "authors": [
            "Garrett B. Goh",
            "Nathan O. Hodas",
            "Abhinav Vishnu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.16154v1",
        "title": "Breaking the Curse of Dimensionality in Deep Neural Networks by Learning\n  Invariant Representations",
        "abstract": "  Artificial intelligence, particularly the subfield of machine learning, has\nseen a paradigm shift towards data-driven models that learn from and adapt to\ndata. This has resulted in unprecedented advancements in various domains such\nas natural language processing and computer vision, largely attributed to deep\nlearning, a special class of machine learning models. Deep learning arguably\nsurpasses traditional approaches by learning the relevant features from raw\ndata through a series of computational layers.\n  This thesis explores the theoretical foundations of deep learning by studying\nthe relationship between the architecture of these models and the inherent\nstructures found within the data they process. In particular, we ask What\ndrives the efficacy of deep learning algorithms and allows them to beat the\nso-called curse of dimensionality-i.e. the difficulty of generally learning\nfunctions in high dimensions due to the exponentially increasing need for data\npoints with increased dimensionality? Is it their ability to learn relevant\nrepresentations of the data by exploiting their structure? How do different\narchitectures exploit different data structures? In order to address these\nquestions, we push forward the idea that the structure of the data can be\neffectively characterized by its invariances-i.e. aspects that are irrelevant\nfor the task at hand.\n  Our methodology takes an empirical approach to deep learning, combining\nexperimental studies with physics-inspired toy models. These simplified models\nallow us to investigate and interpret the complex behaviors we observe in deep\nlearning systems, offering insights into their inner workings, with the\nfar-reaching goal of bridging the gap between theory and practice.\n",
        "published": "2023",
        "authors": [
            "Leonardo Petrini"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.13393v1",
        "title": "Meaning Versus Information, Prediction Versus Memory, and Question\n  Versus Answer",
        "abstract": "  Brain science and artificial intelligence have made great progress toward the\nunderstanding and engineering of the human mind. The progress has accelerated\nsignificantly since the turn of the century thanks to new methods for probing\nthe brain (both structure and function), and rapid development in deep learning\nresearch. However, despite these new developments, there are still many open\nquestions, such as how to understand the brain at the system level, and various\nrobustness issues and limitations of deep learning. In this informal essay, I\nwill talk about some of the concepts that are central to brain science and\nartificial intelligence, such as information and memory, and discuss how a\ndifferent view on these concepts can help us move forward, beyond current\nlimits of our understanding in these fields.\n",
        "published": "2021",
        "authors": [
            "Yoonsuck Choe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.11107v1",
        "title": "Very simple statistical evidence that AlphaGo has exceeded human limits\n  in playing GO game",
        "abstract": "  Deep learning technology is making great progress in solving the challenging\nproblems of artificial intelligence, hence machine learning based on artificial\nneural networks is in the spotlight again. In some areas, artificial\nintelligence based on deep learning is beyond human capabilities. It seemed\nextremely difficult for a machine to beat a human in a Go game, but AlphaGo has\nshown to beat a professional player in the game. By looking at the statistical\ndistribution of the distance in which the Go stones are laid in succession, we\nfind a clear trace that Alphago has surpassed human abilities. The AlphaGo than\nprofessional players and professional players than ordinary players shows the\nlaying of stones in the distance becomes more frequent. In addition, AlphaGo\nshows a much more pronounced difference than that of ordinary players and\nprofessional players.\n",
        "published": "2020",
        "authors": [
            "Okyu Kwon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.14684v2",
        "title": "Robust Ultra-wideband Range Error Mitigation with Deep Learning at the\n  Edge",
        "abstract": "  Ultra-wideband (UWB) is the state-of-the-art and most popular technology for\nwireless localization. Nevertheless, precise ranging and localization in\nnon-line-of-sight (NLoS) conditions is still an open research topic. Indeed,\nmultipath effects, reflections, refractions, and complexity of the indoor radio\nenvironment can easily introduce a positive bias in the ranging measurement,\nresulting in highly inaccurate and unsatisfactory position estimation. This\narticle proposes an efficient representation learning methodology that exploits\nthe latest advancement in deep learning and graph optimization techniques to\nachieve effective ranging error mitigation at the edge. Channel Impulse\nResponse (CIR) signals are directly exploited to extract high semantic features\nto estimate corrections in either NLoS or LoS conditions. Extensive\nexperimentation with different settings and configurations has proved the\neffectiveness of our methodology and demonstrated the feasibility of a robust\nand low computational power UWB range error mitigation.\n",
        "published": "2020",
        "authors": [
            "Simone Angarano",
            "Vittorio Mazzia",
            "Francesco Salvetti",
            "Giovanni Fantin",
            "Marcello Chiaberge"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.14837v1",
        "title": "Recent Trends in Artificial Intelligence-inspired Electronic Thermal\n  Management",
        "abstract": "  The rise of computation-based methods in thermal management has gained\nimmense attention in recent years due to the ability of deep learning to solve\ncomplex 'physics' problems, which are otherwise difficult to be approached\nusing conventional techniques. Thermal management is required in electronic\nsystems to keep them from overheating and burning, enhancing their efficiency\nand lifespan. For a long time, numerical techniques have been employed to aid\nin the thermal management of electronics. However, they come with some\nlimitations. To increase the effectiveness of traditional numerical approaches\nand address the drawbacks faced in conventional approaches, researchers have\nlooked at using artificial intelligence at various stages of the thermal\nmanagement process. The present study discusses in detail, the current uses of\ndeep learning in the domain of 'electronic' thermal management.\n",
        "published": "2021",
        "authors": [
            "Aviral Chharia",
            "Nishi Mehta",
            "Shivam Gupta",
            "Shivam Prajapati"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.00582v1",
        "title": "Understanding the Energy Consumption of HPC Scale Artificial\n  Intelligence",
        "abstract": "  This paper contributes towards better understanding the energy consumption\ntrade-offs of HPC scale Artificial Intelligence (AI), and more specifically\nDeep Learning (DL) algorithms. For this task we developed benchmark-tracker, a\nbenchmark tool to evaluate the speed and energy consumption of DL algorithms in\nHPC environments. We exploited hardware counters and Python libraries to\ncollect energy information through software, which enabled us to instrument a\nknown AI benchmark tool, and to evaluate the energy consumption of numerous DL\nalgorithms and models. Through an experimental campaign, we show a case example\nof the potential of benchmark-tracker to measure the computing speed and the\nenergy consumption for training and inference DL algorithms, and also the\npotential of Benchmark-Tracker to help better understanding the energy behavior\nof DL algorithms in HPC platforms. This work is a step forward to better\nunderstand the energy consumption of Deep Learning in HPC, and it also\ncontributes with a new tool to help HPC DL developers to better balance the HPC\ninfrastructure in terms of speed and energy consumption.\n",
        "published": "2022",
        "authors": [
            "Danilo Carastan dos Santos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.01916v1",
        "title": "More Than Privacy: Applying Differential Privacy in Key Areas of\n  Artificial Intelligence",
        "abstract": "  Artificial Intelligence (AI) has attracted a great deal of attention in\nrecent years. However, alongside all its advancements, problems have also\nemerged, such as privacy violations, security issues and model fairness.\nDifferential privacy, as a promising mathematical model, has several attractive\nproperties that can help solve these problems, making it quite a valuable tool.\nFor this reason, differential privacy has been broadly applied in AI but to\ndate, no study has documented which differential privacy mechanisms can or have\nbeen leveraged to overcome its issues or the properties that make this\npossible. In this paper, we show that differential privacy can do more than\njust privacy preservation. It can also be used to improve security, stabilize\nlearning, build fair models, and impose composition in selected areas of AI.\nWith a focus on regular machine learning, distributed machine learning, deep\nlearning, and multi-agent systems, the purpose of this article is to deliver a\nnew view on many possibilities for improving AI performance with differential\nprivacy techniques.\n",
        "published": "2020",
        "authors": [
            "Tianqing Zhu",
            "Dayong Ye",
            "Wei Wang",
            "Wanlei Zhou",
            "Philip S. Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.01761v1",
        "title": "Distinguishing Human Generated Text From ChatGPT Generated Text Using\n  Machine Learning",
        "abstract": "  ChatGPT is a conversational artificial intelligence that is a member of the\ngenerative pre-trained transformer of the large language model family. This\ntext generative model was fine-tuned by both supervised learning and\nreinforcement learning so that it can produce text documents that seem to be\nwritten by natural intelligence. Although there are numerous advantages of this\ngenerative model, it comes with some reasonable concerns as well. This paper\npresents a machine learning-based solution that can identify the ChatGPT\ndelivered text from the human written text along with the comparative analysis\nof a total of 11 machine learning and deep learning algorithms in the\nclassification process. We have tested the proposed model on a Kaggle dataset\nconsisting of 10,000 texts out of which 5,204 texts were written by humans and\ncollected from news and social media. On the corpus generated by GPT-3.5, the\nproposed algorithm presents an accuracy of 77%.\n",
        "published": "2023",
        "authors": [
            "Niful Islam",
            "Debopom Sutradhar",
            "Humaira Noor",
            "Jarin Tasnim Raya",
            "Monowara Tabassum Maisha",
            "Dewan Md Farid"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.06339v1",
        "title": "Deep Reinforcement Learning",
        "abstract": "  We discuss deep reinforcement learning in an overview style. We draw a big\npicture, filled with details. We discuss six core elements, six important\nmechanisms, and twelve applications, focusing on contemporary work, and in\nhistorical contexts. We start with background of artificial intelligence,\nmachine learning, deep learning, and reinforcement learning (RL), with\nresources. Next we discuss RL core elements, including value function, policy,\nreward, model, exploration vs. exploitation, and representation. Then we\ndiscuss important mechanisms for RL, including attention and memory,\nunsupervised learning, hierarchical RL, multi-agent RL, relational RL, and\nlearning to learn. After that, we discuss RL applications, including games,\nrobotics, natural language processing (NLP), computer vision, finance, business\nmanagement, healthcare, education, energy, transportation, computer systems,\nand, science, engineering, and art. Finally we summarize briefly, discuss\nchallenges and opportunities, and close with an epilogue.\n",
        "published": "2018",
        "authors": [
            "Yuxi Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.10693v1",
        "title": "An appointment with Reproducing Kernel Hilbert Space generated by\n  Generalized Gaussian RBF as $L^2-$measure",
        "abstract": "  Gaussian Radial Basis Function (RBF) Kernels are the most-often-employed\nkernels in artificial intelligence and machine learning routines for providing\noptimally-best results in contrast to their respective counter-parts. However,\na little is known about the application of the Generalized Gaussian Radial\nBasis Function on various machine learning algorithms namely, kernel\nregression, support vector machine (SVM) and pattern-recognition via neural\nnetworks. The results that are yielded by Generalized Gaussian RBF in the\nkernel sense outperforms in stark contrast to Gaussian RBF Kernel, Sigmoid\nFunction and ReLU Function. This manuscript demonstrates the application of the\nGeneralized Gaussian RBF in the kernel sense on the aforementioned machine\nlearning routines along with the comparisons against the aforementioned\nfunctions as well.\n",
        "published": "2023",
        "authors": [
            "Himanshu Singh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.06545v3",
        "title": "Machine learning and AI-based approaches for bioactive ligand discovery\n  and GPCR-ligand recognition",
        "abstract": "  In the last decade, machine learning and artificial intelligence applications\nhave received a significant boost in performance and attention in both academic\nresearch and industry. The success behind most of the recent state-of-the-art\nmethods can be attributed to the latest developments in deep learning. When\napplied to various scientific domains that are concerned with the processing of\nnon-tabular data, for example, image or text, deep learning has been shown to\noutperform not only conventional machine learning but also highly specialized\ntools developed by domain experts. This review aims to summarize AI-based\nresearch for GPCR bioactive ligand discovery with a particular focus on the\nmost recent achievements and research trends. To make this article accessible\nto a broad audience of computational scientists, we provide instructive\nexplanations of the underlying methodology, including overviews of the most\ncommonly used deep learning architectures and feature representations of\nmolecular data. We highlight the latest AI-based research that has led to the\nsuccessful discovery of GPCR bioactive ligands. However, an equal focus of this\nreview is on the discussion of machine learning-based technology that has been\napplied to ligand discovery in general and has the potential to pave the way\nfor successful GPCR bioactive ligand discovery in the future. This review\nconcludes with a brief outlook highlighting the recent research trends in deep\nlearning, such as active learning and semi-supervised learning, which have\ngreat potential for advancing bioactive ligand discovery.\n",
        "published": "2020",
        "authors": [
            "Sebastian Raschka",
            "Benjamin Kaufman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.14037v1",
        "title": "Machine Learning and Deep Learning Methods for Building Intelligent\n  Systems in Medicine and Drug Discovery: A Comprehensive Survey",
        "abstract": "  With the advancements in computer technology, there is a rapid development of\nintelligent systems to understand the complex relationships in data to make\npredictions and classifications. Artificail Intelligence based framework is\nrapidly revolutionizing the healthcare industry. These intelligent systems are\nbuilt with machine learning and deep learning based robust models for early\ndiagnosis of diseases and demonstrates a promising supplementary diagnostic\nmethod for frontline clinical doctors and surgeons. Machine Learning and Deep\nLearning based systems can streamline and simplify the steps involved in\ndiagnosis of diseases from clinical and image-based data, thus providing\nsignificant clinician support and workflow optimization. They mimic human\ncognition and are even capable of diagnosing diseases that cannot be diagnosed\nwith human intelligence. This paper focuses on the survey of machine learning\nand deep learning applications in across 16 medical specialties, namely Dental\nmedicine, Haematology, Surgery, Cardiology, Pulmonology, Orthopedics,\nRadiology, Oncology, General medicine, Psychiatry, Endocrinology, Neurology,\nDermatology, Hepatology, Nephrology, Ophthalmology, and Drug discovery. In this\npaper along with the survey, we discuss the advancements of medical practices\nwith these systems and also the impact of these systems on medical\nprofessionals.\n",
        "published": "2021",
        "authors": [
            "G Jignesh Chowdary",
            "Suganya G",
            "Premalatha M",
            "Asnath Victy Phamila Y",
            "Karunamurthy K"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.01431v1",
        "title": "The Case for Meta-Cognitive Machine Learning: On Model Entropy and\n  Concept Formation in Deep Learning",
        "abstract": "  Machine learning is usually defined in behaviourist terms, where external\nvalidation is the primary mechanism of learning. In this paper, I argue for a\nmore holistic interpretation in which finding more probable, efficient and\nabstract representations is as central to learning as performance. In other\nwords, machine learning should be extended with strategies to reason over its\nown learning process, leading to so-called meta-cognitive machine learning. As\nsuch, the de facto definition of machine learning should be reformulated in\nthese intrinsically multi-objective terms, taking into account not only the\ntask performance but also internal learning objectives. To this end, we suggest\na \"model entropy function\" to be defined that quantifies the efficiency of the\ninternal learning processes. It is conjured that the minimization of this model\nentropy leads to concept formation. Besides philosophical aspects, some initial\nillustrations are included to support the claims.\n",
        "published": "2017",
        "authors": [
            "Johan Loeckx"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.14942v1",
        "title": "Memristors -- from In-memory computing, Deep Learning Acceleration,\n  Spiking Neural Networks, to the Future of Neuromorphic and Bio-inspired\n  Computing",
        "abstract": "  Machine learning, particularly in the form of deep learning, has driven most\nof the recent fundamental developments in artificial intelligence. Deep\nlearning is based on computational models that are, to a certain extent,\nbio-inspired, as they rely on networks of connected simple computing units\noperating in parallel. Deep learning has been successfully applied in areas\nsuch as object/pattern recognition, speech and natural language processing,\nself-driving vehicles, intelligent self-diagnostics tools, autonomous robots,\nknowledgeable personal assistants, and monitoring. These successes have been\nmostly supported by three factors: availability of vast amounts of data,\ncontinuous growth in computing power, and algorithmic innovations. The\napproaching demise of Moore's law, and the consequent expected modest\nimprovements in computing power that can be achieved by scaling, raise the\nquestion of whether the described progress will be slowed or halted due to\nhardware limitations. This paper reviews the case for a novel beyond CMOS\nhardware technology, memristors, as a potential solution for the implementation\nof power-efficient in-memory computing, deep learning accelerators, and spiking\nneural networks. Central themes are the reliance on non-von-Neumann computing\narchitectures and the need for developing tailored learning and inference\nalgorithms. To argue that lessons from biology can be useful in providing\ndirections for further progress in artificial intelligence, we briefly discuss\nan example based reservoir computing. We conclude the review by speculating on\nthe big picture view of future neuromorphic and brain-inspired computing\nsystems.\n",
        "published": "2020",
        "authors": [
            "Adnan Mehonic",
            "Abu Sebastian",
            "Bipin Rajendran",
            "Osvaldo Simeone",
            "Eleni Vasilaki",
            "Anthony J. Kenyon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1409.3358v1",
        "title": "Building Program Vector Representations for Deep Learning",
        "abstract": "  Deep learning has made significant breakthroughs in various fields of\nartificial intelligence. Advantages of deep learning include the ability to\ncapture highly complicated features, weak involvement of human engineering,\netc. However, it is still virtually impossible to use deep learning to analyze\nprograms since deep architectures cannot be trained effectively with pure back\npropagation. In this pioneering paper, we propose the \"coding criterion\" to\nbuild program vector representations, which are the premise of deep learning\nfor program analysis. Our representation learning approach directly makes deep\nlearning a reality in this new field. We evaluate the learned vector\nrepresentations both qualitatively and quantitatively. We conclude, based on\nthe experiments, the coding criterion is successful in building program\nrepresentations. To evaluate whether deep learning is beneficial for program\nanalysis, we feed the representations to deep neural networks, and achieve\nhigher accuracy in the program classification task than \"shallow\" methods, such\nas logistic regression and the support vector machine. This result confirms the\nfeasibility of deep learning to analyze programs. It also gives primary\nevidence of its success in this new field. We believe deep learning will become\nan outstanding technique for program analysis in the near future.\n",
        "published": "2014",
        "authors": [
            "Lili Mou",
            "Ge Li",
            "Yuxuan Liu",
            "Hao Peng",
            "Zhi Jin",
            "Yan Xu",
            "Lu Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.13754v1",
        "title": "Cross-Layer Optimization for Fault-Tolerant Deep Learning",
        "abstract": "  Fault-tolerant deep learning accelerator is the basis for highly reliable\ndeep learning processing and critical to deploy deep learning in\nsafety-critical applications such as avionics and robotics. Since deep learning\nis known to be computing- and memory-intensive, traditional fault-tolerant\napproaches based on redundant computing will incur substantial overhead\nincluding power consumption and chip area. To this end, we propose to\ncharacterize deep learning vulnerability difference across both neurons and\nbits of each neuron, and leverage the vulnerability difference to enable\nselective protection of the deep learning processing components from the\nperspective of architecture layer and circuit layer respectively. At the same\ntime, we observe the correlation between model quantization and bit protection\noverhead of the underlying processing elements of deep learning accelerators,\nand propose to reduce the bit protection overhead by adding additional\nquantization constrain without compromising the model accuracy. Finally, we\nemploy Bayesian optimization strategy to co-optimize the correlated cross-layer\ndesign parameters at algorithm layer, architecture layer, and circuit layer to\nminimize the hardware resource consumption while fulfilling multiple user\nconstraints including reliability, accuracy, and performance of the deep\nlearning processing at the same time.\n",
        "published": "2023",
        "authors": [
            "Qing Zhang",
            "Cheng Liu",
            "Bo Liu",
            "Haitong Huang",
            "Ying Wang",
            "Huawei Li",
            "Xiaowei Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.10010v1",
        "title": "Vision Paper: Causal Inference for Interpretable and Robust Machine\n  Learning in Mobility Analysis",
        "abstract": "  Artificial intelligence (AI) is revolutionizing many areas of our lives,\nleading a new era of technological advancement. Particularly, the\ntransportation sector would benefit from the progress in AI and advance the\ndevelopment of intelligent transportation systems. Building intelligent\ntransportation systems requires an intricate combination of artificial\nintelligence and mobility analysis. The past few years have seen rapid\ndevelopment in transportation applications using advanced deep neural networks.\nHowever, such deep neural networks are difficult to interpret and lack\nrobustness, which slows the deployment of these AI-powered algorithms in\npractice. To improve their usability, increasing research efforts have been\ndevoted to developing interpretable and robust machine learning methods, among\nwhich the causal inference approach recently gained traction as it provides\ninterpretable and actionable information. Moreover, most of these methods are\ndeveloped for image or sequential data which do not satisfy specific\nrequirements of mobility data analysis. This vision paper emphasizes research\nchallenges in deep learning-based mobility analysis that require\ninterpretability and robustness, summarizes recent developments in using causal\ninference for improving the interpretability and robustness of machine learning\nmethods, and highlights opportunities in developing causally-enabled machine\nlearning models tailored for mobility analysis. This research direction will\nmake AI in the transportation sector more interpretable and reliable, thus\ncontributing to safer, more efficient, and more sustainable future\ntransportation systems.\n",
        "published": "2022",
        "authors": [
            "Yanan Xin",
            "Natasa Tagasovska",
            "Fernando Perez-Cruz",
            "Martin Raubal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.06889v3",
        "title": "Visual Analytics in Deep Learning: An Interrogative Survey for the Next\n  Frontiers",
        "abstract": "  Deep learning has recently seen rapid development and received significant\nattention due to its state-of-the-art performance on previously-thought hard\nproblems. However, because of the internal complexity and nonlinear structure\nof deep neural networks, the underlying decision making processes for why these\nmodels are achieving such performance are challenging and sometimes mystifying\nto interpret. As deep learning spreads across domains, it is of paramount\nimportance that we equip users of deep learning with tools for understanding\nwhen a model works correctly, when it fails, and ultimately how to improve its\nperformance. Standardized toolkits for building neural networks have helped\ndemocratize deep learning; visual analytics systems have now been developed to\nsupport model explanation, interpretation, debugging, and improvement. We\npresent a survey of the role of visual analytics in deep learning research,\nwhich highlights its short yet impactful history and thoroughly summarizes the\nstate-of-the-art using a human-centered interrogative framework, focusing on\nthe Five W's and How (Why, Who, What, How, When, and Where). We conclude by\nhighlighting research directions and open research problems. This survey helps\nresearchers and practitioners in both visual analytics and deep learning to\nquickly learn key aspects of this young and rapidly growing body of research,\nwhose impact spans a diverse range of domains.\n",
        "published": "2018",
        "authors": [
            "Fred Hohman",
            "Minsuk Kahng",
            "Robert Pienta",
            "Duen Horng Chau"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.12385v2",
        "title": "Deep Learning in Single-Cell Analysis",
        "abstract": "  Single-cell technologies are revolutionizing the entire field of biology. The\nlarge volumes of data generated by single-cell technologies are\nhigh-dimensional, sparse, heterogeneous, and have complicated dependency\nstructures, making analyses using conventional machine learning approaches\nchallenging and impractical. In tackling these challenges, deep learning often\ndemonstrates superior performance compared to traditional machine learning\nmethods. In this work, we give a comprehensive survey on deep learning in\nsingle-cell analysis. We first introduce background on single-cell technologies\nand their development, as well as fundamental concepts of deep learning\nincluding the most popular deep architectures. We present an overview of the\nsingle-cell analytic pipeline pursued in research applications while noting\ndivergences due to data sources or specific applications. We then review seven\npopular tasks spanning through different stages of the single-cell analysis\npipeline, including multimodal integration, imputation, clustering, spatial\ndomain identification, cell-type deconvolution, cell segmentation, and\ncell-type annotation. Under each task, we describe the most recent developments\nin classical and deep learning methods and discuss their advantages and\ndisadvantages. Deep learning tools and benchmark datasets are also summarized\nfor each task. Finally, we discuss the future directions and the most recent\nchallenges. This survey will serve as a reference for biologists and computer\nscientists, encouraging collaborations.\n",
        "published": "2022",
        "authors": [
            "Dylan Molho",
            "Jiayuan Ding",
            "Zhaoheng Li",
            "Hongzhi Wen",
            "Wenzhuo Tang",
            "Yixin Wang",
            "Julian Venegas",
            "Wei Jin",
            "Renming Liu",
            "Runze Su",
            "Patrick Danaher",
            "Robert Yang",
            "Yu Leo Lei",
            "Yuying Xie",
            "Jiliang Tang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.04811v2",
        "title": "Deep learning models for price forecasting of financial time series: A\n  review of recent advancements: 2020-2022",
        "abstract": "  Accurately predicting the prices of financial time series is essential and\nchallenging for the financial sector. Owing to recent advancements in deep\nlearning techniques, deep learning models are gradually replacing traditional\nstatistical and machine learning models as the first choice for price\nforecasting tasks. This shift in model selection has led to a notable rise in\nresearch related to applying deep learning models to price forecasting,\nresulting in a rapid accumulation of new knowledge. Therefore, we conducted a\nliterature review of relevant studies over the past three years with a view to\naiding researchers and practitioners in the field. This review delves deeply\ninto deep learning-based forecasting models, presenting information on model\narchitectures, practical applications, and their respective advantages and\ndisadvantages. In particular, detailed information is provided on advanced\nmodels for price forecasting, such as Transformers, generative adversarial\nnetworks (GANs), graph neural networks (GNNs), and deep quantum neural networks\n(DQNNs). The present contribution also includes potential directions for future\nresearch, such as examining the effectiveness of deep learning models with\ncomplex structures for price forecasting, extending from point prediction to\ninterval prediction using deep learning models, scrutinising the reliability\nand validity of decomposition ensembles, and exploring the influence of data\nvolume on model performance.\n",
        "published": "2023",
        "authors": [
            "Cheng Zhang",
            "Nilam Nur Amir Sjarif",
            "Roslina Ibrahim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.01910v2",
        "title": "Probabilistic Deep Learning using Random Sum-Product Networks",
        "abstract": "  The need for consistent treatment of uncertainty has recently triggered\nincreased interest in probabilistic deep learning methods. However, most\ncurrent approaches have severe limitations when it comes to inference, since\nmany of these models do not even permit to evaluate exact data likelihoods.\nSum-product networks (SPNs), on the other hand, are an excellent architecture\nin that regard, as they allow to efficiently evaluate likelihoods, as well as\narbitrary marginalization and conditioning tasks. Nevertheless, SPNs have not\nbeen fully explored as serious deep learning models, likely due to their\nspecial structural requirements, which complicate learning. In this paper, we\nmake a drastic simplification and use random SPN structures which are trained\nin a \"classical deep learning manner\", i.e. employing automatic\ndifferentiation, SGD, and GPU support. The resulting models, called RAT-SPNs,\nyield prediction results comparable to deep neural networks, while still being\ninterpretable as generative model and maintaining well-calibrated\nuncertainties. This property makes them highly robust under missing input\nfeatures and enables them to naturally detect outliers and peculiar samples.\n",
        "published": "2018",
        "authors": [
            "Robert Peharz",
            "Antonio Vergari",
            "Karl Stelzner",
            "Alejandro Molina",
            "Martin Trapp",
            "Kristian Kersting",
            "Zoubin Ghahramani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.05870v1",
        "title": "ZhuSuan: A Library for Bayesian Deep Learning",
        "abstract": "  In this paper we introduce ZhuSuan, a python probabilistic programming\nlibrary for Bayesian deep learning, which conjoins the complimentary advantages\nof Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike\nexisting deep learning libraries, which are mainly designed for deterministic\nneural networks and supervised tasks, ZhuSuan is featured for its deep root\ninto Bayesian inference, thus supporting various kinds of probabilistic models,\nincluding both the traditional hierarchical Bayesian models and recent deep\ngenerative models. We use running examples to illustrate the probabilistic\nprogramming on ZhuSuan, including Bayesian logistic regression, variational\nauto-encoders, deep sigmoid belief networks and Bayesian recurrent neural\nnetworks.\n",
        "published": "2017",
        "authors": [
            "Jiaxin Shi",
            "Jianfei Chen",
            "Jun Zhu",
            "Shengyang Sun",
            "Yucen Luo",
            "Yihong Gu",
            "Yuhao Zhou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.10769v2",
        "title": "Estimating Uncertainty and Interpretability in Deep Learning for\n  Coronavirus (COVID-19) Detection",
        "abstract": "  Deep Learning has achieved state of the art performance in medical imaging.\nHowever, these methods for disease detection focus exclusively on improving the\naccuracy of classification or predictions without quantifying uncertainty in a\ndecision. Knowing how much confidence there is in a computer-based medical\ndiagnosis is essential for gaining clinicians trust in the technology and\ntherefore improve treatment. Today, the 2019 Coronavirus (SARS-CoV-2)\ninfections are a major healthcare challenge around the world. Detecting\nCOVID-19 in X-ray images is crucial for diagnosis, assessment and treatment.\nHowever, diagnostic uncertainty in the report is a challenging and yet\ninevitable task for radiologist. In this paper, we investigate how drop-weights\nbased Bayesian Convolutional Neural Networks (BCNN) can estimate uncertainty in\nDeep Learning solution to improve the diagnostic performance of the\nhuman-machine team using publicly available COVID-19 chest X-ray dataset and\nshow that the uncertainty in prediction is highly correlates with accuracy of\nprediction. We believe that the availability of uncertainty-aware deep learning\nsolution will enable a wider adoption of Artificial Intelligence (AI) in a\nclinical setting.\n",
        "published": "2020",
        "authors": [
            "Biraja Ghoshal",
            "Allan Tucker"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.13351v1",
        "title": "A Simple and Interpretable Predictive Model for Healthcare",
        "abstract": "  Deep Learning based models are currently dominating most state-of-the-art\nsolutions for disease prediction. Existing works employ RNNs along with\nmultiple levels of attention mechanisms to provide interpretability. These deep\nlearning models, with trainable parameters running into millions, require huge\namounts of compute and data to train and deploy. These requirements are\nsometimes so huge that they render usage of such models as unfeasible. We\naddress these challenges by developing a simpler yet interpretable non-deep\nlearning based model for application to EHR data. We model and showcase our\nwork's results on the task of predicting first occurrence of a diagnosis, often\noverlooked in existing works. We push the capabilities of a tree based model\nand come up with a strong baseline for more sophisticated models. Its\nperformance shows an improvement over deep learning based solutions (both, with\nand without the first-occurrence constraint) all the while maintaining\ninterpretability.\n",
        "published": "2020",
        "authors": [
            "Subhadip Maji",
            "Raghav Bali",
            "Sree Harsha Ankem",
            "Kishore V Ayyadevara"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.05468v9",
        "title": "Generalization in Deep Learning",
        "abstract": "  This paper provides theoretical insights into why and how deep learning can\ngeneralize well, despite its large capacity, complexity, possible algorithmic\ninstability, nonrobustness, and sharp minima, responding to an open question in\nthe literature. We also discuss approaches to provide non-vacuous\ngeneralization guarantees for deep learning. Based on theoretical observations,\nwe propose new open problems and discuss the limitations of our results.\n",
        "published": "2017",
        "authors": [
            "Kenji Kawaguchi",
            "Leslie Pack Kaelbling",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.02686v2",
        "title": "Holarchic Structures for Decentralized Deep Learning - A Performance\n  Analysis",
        "abstract": "  Structure plays a key role in learning performance. In centralized\ncomputational systems, hyperparameter optimization and regularization\ntechniques such as dropout are computational means to enhance learning\nperformance by adjusting the deep hierarchical structure. However, in\ndecentralized deep learning by the Internet of Things, the structure is an\nactual network of autonomous interconnected devices such as smart phones that\ninteract via complex network protocols. Self-adaptation of the learning\nstructure is a challenge. Uncertainties such as network latency, node and link\nfailures or even bottlenecks by limited processing capacity and energy\navailability can signif- icantly downgrade learning performance. Network\nself-organization and self-management is complex, while it requires additional\ncomputational and network resources that hinder the feasibility of\ndecentralized deep learning. In contrast, this paper introduces a self-adaptive\nlearning approach based on holarchic learning structures for exploring,\nmitigating and boosting learning performance in distributed environments with\nuncertainties. A large-scale performance analysis with 864000 experiments fed\nwith synthetic and real-world data from smart grid and smart city pilot\nprojects confirm the cost-effectiveness of holarchic structures for\ndecentralized deep learning.\n",
        "published": "2018",
        "authors": [
            "Evangelos Pournaras",
            "Srivatsan Yadhunathan",
            "Ada Diaconescu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.01811v2",
        "title": "McTorch, a manifold optimization library for deep learning",
        "abstract": "  In this paper, we introduce McTorch, a manifold optimization library for deep\nlearning that extends PyTorch. It aims to lower the barrier for users wishing\nto use manifold constraints in deep learning applications, i.e., when the\nparameters are constrained to lie on a manifold. Such constraints include the\npopular orthogonality and rank constraints, and have been recently used in a\nnumber of applications in deep learning. McTorch follows PyTorch's architecture\nand decouples manifold definitions and optimizers, i.e., once a new manifold is\nadded it can be used with any existing optimizer and vice-versa. McTorch is\navailable at https://github.com/mctorch .\n",
        "published": "2018",
        "authors": [
            "Mayank Meghwanshi",
            "Pratik Jawanpuria",
            "Anoop Kunchukuttan",
            "Hiroyuki Kasai",
            "Bamdev Mishra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.11815v2",
        "title": "Uncertainty Quantification for Sparse Deep Learning",
        "abstract": "  Deep learning methods continue to have a decided impact on machine learning,\nboth in theory and in practice. Statistical theoretical developments have been\nmostly concerned with approximability or rates of estimation when recovering\ninfinite dimensional objects (curves or densities). Despite the impressive\narray of available theoretical results, the literature has been largely silent\nabout uncertainty quantification for deep learning. This paper takes a step\nforward in this important direction by taking a Bayesian point of view. We\nstudy Gaussian approximability of certain aspects of posterior distributions of\nsparse deep ReLU architectures in non-parametric regression. Building on tools\nfrom Bayesian non-parametrics, we provide semi-parametric Bernstein-von Mises\ntheorems for linear and quadratic functionals, which guarantee that implied\nBayesian credible regions have valid frequentist coverage. Our results provide\nnew theoretical justifications for (Bayesian) deep learning with ReLU\nactivation functions, highlighting their inferential potential.\n",
        "published": "2020",
        "authors": [
            "Yuexi Wang",
            "Veronika Ro\u010dkov\u00e1"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.11840v2",
        "title": "Benchmarking Inference Performance of Deep Learning Models on Analog\n  Devices",
        "abstract": "  Analog hardware implemented deep learning models are promising for\ncomputation and energy constrained systems such as edge computing devices.\nHowever, the analog nature of the device and the associated many noise sources\nwill cause changes to the value of the weights in the trained deep learning\nmodels deployed on such devices. In this study, systematic evaluation of the\ninference performance of trained popular deep learning models for image\nclassification deployed on analog devices has been carried out, where additive\nwhite Gaussian noise has been added to the weights of the trained models during\ninference. It is observed that deeper models and models with more redundancy in\ndesign such as VGG are more robust to the noise in general. However, the\nperformance is also affected by the design philosophy of the model, the\ndetailed structure of the model, the exact machine learning task, as well as\nthe datasets.\n",
        "published": "2020",
        "authors": [
            "Omobayode Fagbohungbe",
            "Lijun Qian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.03103v1",
        "title": "Interpretable Artificial Intelligence through the Lens of Feature\n  Interaction",
        "abstract": "  Interpretation of deep learning models is a very challenging problem because\nof their large number of parameters, complex connections between nodes, and\nunintelligible feature representations. Despite this, many view\ninterpretability as a key solution to trustworthiness, fairness, and safety,\nespecially as deep learning is applied to more critical decision tasks like\ncredit approval, job screening, and recidivism prediction. There is an\nabundance of good research providing interpretability to deep learning models;\nhowever, many of the commonly used methods do not consider a phenomenon called\n\"feature interaction.\" This work first explains the historical and modern\nimportance of feature interactions and then surveys the modern interpretability\nmethods which do explicitly consider feature interactions. This survey aims to\nbring to light the importance of feature interactions in the larger context of\nmachine learning interpretability, especially in a modern context where deep\nlearning models heavily rely on feature interactions.\n",
        "published": "2021",
        "authors": [
            "Michael Tsang",
            "James Enouen",
            "Yan Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.03579v3",
        "title": "Expressive Power and Loss Surfaces of Deep Learning Models",
        "abstract": "  The goals of this paper are two-fold. The first goal is to serve as an\nexpository tutorial on the working of deep learning models which emphasizes\ngeometrical intuition about the reasons for success of deep learning. The\nsecond goal is to complement the current results on the expressive power of\ndeep learning models and their loss surfaces with novel insights and results.\nIn particular, we describe how deep neural networks carve out manifolds\nespecially when the multiplication neurons are introduced. Multiplication is\nused in dot products and the attention mechanism and it is employed in capsule\nnetworks and self-attention based transformers. We also describe how random\npolynomial, random matrix, spin glass and computational complexity perspectives\non the loss surfaces are interconnected.\n",
        "published": "2021",
        "authors": [
            "Simant Dube"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.14911v1",
        "title": "A Survey of Deep Learning Techniques for Dynamic Branch Prediction",
        "abstract": "  Branch prediction is an architectural feature that speeds up the execution of\nbranch instruction on pipeline processors and reduces the cost of branching.\nRecent advancements of Deep Learning (DL) in the post Moore's Law era is\naccelerating areas of automated chip design, low-power computer architectures,\nand much more. Traditional computer architecture design and algorithms could\nbenefit from dynamic predictors based on deep learning algorithms which learns\nfrom experience by optimizing its parameters on large number of data. In this\nsurvey paper, we focus on traditional branch prediction algorithms, analyzes\nits limitations, and presents a literature survey of how deep learning\ntechniques can be applied to create dynamic branch predictors capable of\npredicting conditional branch instructions. Prior surveys in this field focus\non dynamic branch prediction techniques based on neural network perceptrons. We\nplan to improve the survey based on latest research in DL and advanced Machine\nLearning (ML) based branch predictors.\n",
        "published": "2021",
        "authors": [
            "Rinu Joseph"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.01164v1",
        "title": "Interpretable Low-Resource Legal Decision Making",
        "abstract": "  Over the past several years, legal applications of deep learning have been on\nthe rise. However, as with other high-stakes decision making areas, the\nrequirement for interpretability is of crucial importance. Current models\nutilized by legal practitioners are more of the conventional machine learning\ntype, wherein they are inherently interpretable, yet unable to harness the\nperformance capabilities of data-driven deep learning models. In this work, we\nutilize deep learning models in the area of trademark law to shed light on the\nissue of likelihood of confusion between trademarks. Specifically, we introduce\na model-agnostic interpretable intermediate layer, a technique which proves to\nbe effective for legal documents. Furthermore, we utilize weakly supervised\nlearning by means of a curriculum learning strategy, effectively demonstrating\nthe improved performance of a deep learning model. This is in contrast to the\nconventional models which are only able to utilize the limited number of\nexpensive manually-annotated samples by legal experts. Although the methods\npresented in this work tackles the task of risk of confusion for trademarks, it\nis straightforward to extend them to other fields of law, or more generally, to\nother similar high-stakes application scenarios.\n",
        "published": "2022",
        "authors": [
            "Rohan Bhambhoria",
            "Hui Liu",
            "Samuel Dahan",
            "Xiaodan Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.10978v1",
        "title": "Machine Learning for Food Review and Recommendation",
        "abstract": "  Food reviews and recommendations have always been important for online food\nservice websites. However, reviewing and recommending food is not simple as it\nis likely to be overwhelmed by disparate contexts and meanings. In this paper,\nwe use different deep learning approaches to address the problems of sentiment\nanalysis, automatic review tag generation, and retrieval of food reviews. We\npropose to develop a web-based food review system at Nanyang Technological\nUniversity (NTU) named NTU Food Hunter, which incorporates different deep\nlearning approaches that help users with food selection. First, we implement\nthe BERT and LSTM deep learning models into the system for sentiment analysis\nof food reviews. Then, we develop a Part-of-Speech (POS) algorithm to\nautomatically identify and extract adjective-noun pairs from the review content\nfor review tag generation based on POS tagging and dependency parsing. Finally,\nwe also train a RankNet model for the re-ranking of the retrieval results to\nimprove the accuracy in our Solr-based food reviews search system. The\nexperimental results show that our proposed deep learning approaches are\npromising for the applications of real-world problems.\n",
        "published": "2022",
        "authors": [
            "Tan Khang Le",
            "Siu Cheung Hui"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.06415v1",
        "title": "The Free Energy Principle for Perception and Action: A Deep Learning\n  Perspective",
        "abstract": "  The free energy principle, and its corollary active inference, constitute a\nbio-inspired theory that assumes biological agents act to remain in a\nrestricted set of preferred states of the world, i.e., they minimize their free\nenergy. Under this principle, biological agents learn a generative model of the\nworld and plan actions in the future that will maintain the agent in an\nhomeostatic state that satisfies its preferences. This framework lends itself\nto being realized in silico, as it comprehends important aspects that make it\ncomputationally affordable, such as variational inference and amortized\nplanning. In this work, we investigate the tool of deep learning to design and\nrealize artificial agents based on active inference, presenting a deep-learning\noriented presentation of the free energy principle, surveying works that are\nrelevant in both machine learning and active inference areas, and discussing\nthe design choices that are involved in the implementation process. This\nmanuscript probes newer perspectives for the active inference framework,\ngrounding its theoretical aspects into more pragmatic affairs, offering a\npractical guide to active inference newcomers and a starting point for deep\nlearning practitioners that would like to investigate implementations of the\nfree energy principle.\n",
        "published": "2022",
        "authors": [
            "Pietro Mazzaglia",
            "Tim Verbelen",
            "Ozan \u00c7atal",
            "Bart Dhoedt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.02515v2",
        "title": "Deep Learning for Time Series Classification and Extrinsic Regression: A\n  Current Survey",
        "abstract": "  Time Series Classification and Extrinsic Regression are important and\nchallenging machine learning tasks. Deep learning has revolutionized natural\nlanguage processing and computer vision and holds great promise in other fields\nsuch as time series analysis where the relevant features must often be\nabstracted from the raw data but are not known a priori. This paper surveys the\ncurrent state of the art in the fast-moving field of deep learning for time\nseries classification and extrinsic regression. We review different network\narchitectures and training methods used for these tasks and discuss the\nchallenges and opportunities when applying deep learning to time series data.\nWe also summarize two critical applications of time series classification and\nextrinsic regression, human activity recognition and satellite earth\nobservation.\n",
        "published": "2023",
        "authors": [
            "Navid Mohammadi Foumani",
            "Lynn Miller",
            "Chang Wei Tan",
            "Geoffrey I. Webb",
            "Germain Forestier",
            "Mahsa Salehi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.06154v3",
        "title": "HypLL: The Hyperbolic Learning Library",
        "abstract": "  Deep learning in hyperbolic space is quickly gaining traction in the fields\nof machine learning, multimedia, and computer vision. Deep networks commonly\noperate in Euclidean space, implicitly assuming that data lies on regular\ngrids. Recent advances have shown that hyperbolic geometry provides a viable\nalternative foundation for deep learning, especially when data is hierarchical\nin nature and when working with few embedding dimensions. Currently however, no\naccessible open-source library exists to build hyperbolic network modules akin\nto well-known deep learning libraries. We present HypLL, the Hyperbolic\nLearning Library to bring the progress on hyperbolic deep learning together.\nHypLL is built on top of PyTorch, with an emphasis in its design for\nease-of-use, in order to attract a broad audience towards this new and\nopen-ended research direction. The code is available at:\nhttps://github.com/maxvanspengler/hyperbolic_learning_library.\n",
        "published": "2023",
        "authors": [
            "Max van Spengler",
            "Philipp Wirth",
            "Pascal Mettes"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.03243v1",
        "title": "Sparse Deep Learning for Time Series Data: Theory and Applications",
        "abstract": "  Sparse deep learning has become a popular technique for improving the\nperformance of deep neural networks in areas such as uncertainty\nquantification, variable selection, and large-scale network compression.\nHowever, most existing research has focused on problems where the observations\nare independent and identically distributed (i.i.d.), and there has been little\nwork on the problems where the observations are dependent, such as time series\ndata and sequential data in natural language processing. This paper aims to\naddress this gap by studying the theory for sparse deep learning with dependent\ndata. We show that sparse recurrent neural networks (RNNs) can be consistently\nestimated, and their predictions are asymptotically normally distributed under\nappropriate assumptions, enabling the prediction uncertainty to be correctly\nquantified. Our numerical results show that sparse deep learning outperforms\nstate-of-the-art methods, such as conformal predictions, in prediction\nuncertainty quantification for time series data. Furthermore, our results\nindicate that the proposed method can consistently identify the autoregressive\norder for time series data and outperform existing methods in large-scale model\ncompression. Our proposed method has important practical implications in fields\nsuch as finance, healthcare, and energy, where both accurate point estimates\nand prediction uncertainty quantification are of concern.\n",
        "published": "2023",
        "authors": [
            "Mingxuan Zhang",
            "Yan Sun",
            "Faming Liang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.05726v1",
        "title": "Neural Network Methods for Radiation Detectors and Imaging",
        "abstract": "  Recent advances in image data processing through machine learning and\nespecially deep neural networks (DNNs) allow for new optimization and\nperformance-enhancement schemes for radiation detectors and imaging hardware\nthrough data-endowed artificial intelligence. We give an overview of data\ngeneration at photon sources, deep learning-based methods for image processing\ntasks, and hardware solutions for deep learning acceleration. Most existing\ndeep learning approaches are trained offline, typically using large amounts of\ncomputational resources. However, once trained, DNNs can achieve fast inference\nspeeds and can be deployed to edge devices. A new trend is edge computing with\nless energy consumption (hundreds of watts or less) and real-time analysis\npotential. While popularly used for edge computing, electronic-based hardware\naccelerators ranging from general purpose processors such as central processing\nunits (CPUs) to application-specific integrated circuits (ASICs) are constantly\nreaching performance limits in latency, energy consumption, and other physical\nconstraints. These limits give rise to next-generation analog neuromorhpic\nhardware platforms, such as optical neural networks (ONNs), for high parallel,\nlow latency, and low energy computing to boost deep learning acceleration.\n",
        "published": "2023",
        "authors": [
            "S. Lin",
            "S. Ning",
            "H. Zhu",
            "T. Zhou",
            "C. L. Morris",
            "S. Clayton",
            "M. Cherukara",
            "R. T. Chen",
            "Z. Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.07970v2",
        "title": "Deep Learning for Physical Processes: Incorporating Prior Scientific\n  Knowledge",
        "abstract": "  We consider the use of Deep Learning methods for modeling complex phenomena\nlike those occurring in natural physical processes. With the large amount of\ndata gathered on these phenomena the data intensive paradigm could begin to\nchallenge more traditional approaches elaborated over the years in fields like\nmaths or physics. However, despite considerable successes in a variety of\napplication domains, the machine learning field is not yet ready to handle the\nlevel of complexity required by such problems. Using an example application,\nnamely Sea Surface Temperature Prediction, we show how general background\nknowledge gained from physics could be used as a guideline for designing\nefficient Deep Learning models. In order to motivate the approach and to assess\nits generality we demonstrate a formal link between the solution of a class of\ndifferential equations underlying a large family of physical phenomena and the\nproposed model. Experiments and comparison with series of baselines including a\nstate of the art numerical approach is then provided.\n",
        "published": "2017",
        "authors": [
            "Emmanuel de Bezenac",
            "Arthur Pajot",
            "Patrick Gallinari"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.11157v2",
        "title": "A Semantic Loss Function for Deep Learning with Symbolic Knowledge",
        "abstract": "  This paper develops a novel methodology for using symbolic knowledge in deep\nlearning. From first principles, we derive a semantic loss function that\nbridges between neural output vectors and logical constraints. This loss\nfunction captures how close the neural network is to satisfying the constraints\non its output. An experimental evaluation shows that it effectively guides the\nlearner to achieve (near-)state-of-the-art results on semi-supervised\nmulti-class classification. Moreover, it significantly increases the ability of\nthe neural network to predict structured objects, such as rankings and paths.\nThese discrete concepts are tremendously difficult to learn, and benefit from a\ntight integration of deep learning and symbolic reasoning methods.\n",
        "published": "2017",
        "authors": [
            "Jingyi Xu",
            "Zilu Zhang",
            "Tal Friedman",
            "Yitao Liang",
            "Guy Van den Broeck"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.08153v1",
        "title": "Learning the Localization Function: Machine Learning Approach to\n  Fingerprinting Localization",
        "abstract": "  Considered as a data-driven approach, Fingerprinting Localization Solutions\n(FPSs) enjoy huge popularity due to their good performance and minimal\nenvironment information requirement. This papers addresses applications of\nartificial intelligence to solve two problems in Received Signal Strength\nIndicator (RSSI) based FPS, first the cumbersome training database construction\nand second the extrapolation of fingerprinting algorithm for similar buildings\nwith slight environmental changes. After a concise overview of deep learning\ndesign techniques, two main techniques widely used in deep learning are\nexploited for the above mentioned issues namely data augmentation and transfer\nlearning. We train a multi-layer neural network that learns the mapping from\nthe observations to the locations. A data augmentation method is proposed to\nincrease the training database size based on the structure of RSSI measurements\nand hence reducing effectively the amount of training data. Then it is shown\nexperimentally how a model trained for a particular building can be transferred\nto a similar one by fine tuning with significantly smaller training numbers.\nThe paper implicitly discusses the new guidelines to consider about deep\nlearning designs when they are employed in a new application context.\n",
        "published": "2018",
        "authors": [
            "Linchen Xiao",
            "Arash Behboodi",
            "Rudolf Mathar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.08133v1",
        "title": "What is not where: the challenge of integrating spatial representations\n  into deep learning architectures",
        "abstract": "  This paper examines to what degree current deep learning architectures for\nimage caption generation capture spatial language. On the basis of the\nevaluation of examples of generated captions from the literature we argue that\nsystems capture what objects are in the image data but not where these objects\nare located: the captions generated by these systems are the output of a\nlanguage model conditioned on the output of an object detector that cannot\ncapture fine-grained location information. Although language models provide\nuseful knowledge for image captions, we argue that deep learning image\ncaptioning architectures should also model geometric relations between objects.\n",
        "published": "2018",
        "authors": [
            "John D. Kelleher",
            "Simon Dobnik"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.02761v1",
        "title": "Neural Algorithmic Reasoning",
        "abstract": "  Algorithms have been fundamental to recent global technological advances and,\nin particular, they have been the cornerstone of technical advances in one\nfield rapidly being applied to another. We argue that algorithms possess\nfundamentally different qualities to deep learning methods, and this strongly\nsuggests that, were deep learning methods better able to mimic algorithms,\ngeneralisation of the sort seen with algorithms would become possible with deep\nlearning -- something far out of the reach of current machine learning methods.\nFurthermore, by representing elements in a continuous space of learnt\nalgorithms, neural networks are able to adapt known algorithms more closely to\nreal-world problems, potentially finding more efficient and pragmatic solutions\nthan those proposed by human computer scientists.\n  Here we present neural algorithmic reasoning -- the art of building neural\nnetworks that are able to execute algorithmic computation -- and provide our\nopinion on its transformative potential for running classical algorithms on\ninputs previously considered inaccessible to them.\n",
        "published": "2021",
        "authors": [
            "Petar Veli\u010dkovi\u0107",
            "Charles Blundell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.05924v2",
        "title": "Compute Trends Across Three Eras of Machine Learning",
        "abstract": "  Compute, data, and algorithmic advances are the three fundamental factors\nthat guide the progress of modern Machine Learning (ML). In this paper we study\ntrends in the most readily quantified factor - compute. We show that before\n2010 training compute grew in line with Moore's law, doubling roughly every 20\nmonths. Since the advent of Deep Learning in the early 2010s, the scaling of\ntraining compute has accelerated, doubling approximately every 6 months. In\nlate 2015, a new trend emerged as firms developed large-scale ML models with 10\nto 100-fold larger requirements in training compute. Based on these\nobservations we split the history of compute in ML into three eras: the Pre\nDeep Learning Era, the Deep Learning Era and the Large-Scale Era. Overall, our\nwork highlights the fast-growing compute requirements for training advanced ML\nsystems.\n",
        "published": "2022",
        "authors": [
            "Jaime Sevilla",
            "Lennart Heim",
            "Anson Ho",
            "Tamay Besiroglu",
            "Marius Hobbhahn",
            "Pablo Villalobos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.03263v1",
        "title": "Function space analysis of deep learning representation layers",
        "abstract": "  In this paper we propose a function space approach to Representation Learning\nand the analysis of the representation layers in deep learning architectures.\nWe show how to compute a weak-type Besov smoothness index that quantifies the\ngeometry of the clustering in the feature space. This approach was already\napplied successfully to improve the performance of machine learning algorithms\nsuch as the Random Forest and tree-based Gradient Boosting. Our experiments\ndemonstrate that in well-known and well-performing trained networks, the Besov\nsmoothness of the training set, measured in the corresponding hidden layer\nfeature map representation, increases from layer to layer. We also contribute\nto the understanding of generalization by showing how the Besov smoothness of\nthe representations, decreases as we add more mis-labeling to the training\ndata. We hope this approach will contribute to the de-mystification of some\naspects of deep learning.\n",
        "published": "2017",
        "authors": [
            "Oren Elisha",
            "Shai Dekel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.01940v2",
        "title": "Can we trust deep learning models diagnosis? The impact of domain shift\n  in chest radiograph classification",
        "abstract": "  While deep learning models become more widespread, their ability to handle\nunseen data and generalize for any scenario is yet to be challenged. In medical\nimaging, there is a high heterogeneity of distributions among images based on\nthe equipment that generates them and their parametrization. This heterogeneity\ntriggers a common issue in machine learning called domain shift, which\nrepresents the difference between the training data distribution and the\ndistribution of where a model is employed. A high domain shift tends to\nimplicate in a poor generalization performance from the models. In this work,\nwe evaluate the extent of domain shift on four of the largest datasets of chest\nradiographs. We show how training and testing with different datasets (e.g.,\ntraining in ChestX-ray14 and testing in CheXpert) drastically affects model\nperformance, posing a big question over the reliability of deep learning models\ntrained on public datasets. We also show that models trained on CheXpert and\nMIMIC-CXR generalize better to other datasets.\n",
        "published": "2019",
        "authors": [
            "Eduardo H. P. Pooch",
            "Pedro L. Ballester",
            "Rodrigo C. Barros"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.00372v1",
        "title": "Interpretability of Blackbox Machine Learning Models through Dataview\n  Extraction and Shadow Model creation",
        "abstract": "  Deep learning models trained using massive amounts of data tend to capture\none view of the data and its associated mapping. Different deep learning models\nbuilt on the same training data may capture different views of the data based\non the underlying techniques used. For explaining the decisions arrived by\nblackbox deep learning models, we argue that it is essential to reproduce that\nmodel's view of the training data faithfully. This faithful reproduction can\nthen be used for explanation generation. We investigate two methods for data\nview extraction: hill-climbing approach and a GAN-driven approach. We then use\nthis synthesized data for creating shadow models for explanation generation:\nDecision-Tree model and Formal Concept Analysis based model. We evaluate these\napproaches on a Blackbox model trained on public datasets and show its\nusefulness in explanation generation.\n",
        "published": "2020",
        "authors": [
            "Rupam Patir",
            "Shubham Singhal",
            "C. Anantaram",
            "Vikram Goyal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.11867v4",
        "title": "Bridging the Gap between Spatial and Spectral Domains: A Survey on Graph\n  Neural Networks",
        "abstract": "  Deep learning's success has been widely recognized in a variety of machine\nlearning tasks, including image classification, audio recognition, and natural\nlanguage processing. As an extension of deep learning beyond these domains,\ngraph neural networks (GNNs) are designed to handle the non-Euclidean\ngraph-structure which is intractable to previous deep learning techniques.\nExisting GNNs are presented using various techniques, making direct comparison\nand cross-reference more complex. Although existing studies categorize GNNs\ninto spatial-based and spectral-based techniques, there hasn't been a thorough\nexamination of their relationship. To close this gap, this study presents a\nsingle framework that systematically incorporates most GNNs. We organize\nexisting GNNs into spatial and spectral domains, as well as expose the\nconnections within each domain. A review of spectral graph theory and\napproximation theory builds a strong relationship across the spatial and\nspectral domains in further investigation.\n",
        "published": "2020",
        "authors": [
            "Zhiqian Chen",
            "Fanglan Chen",
            "Lei Zhang",
            "Taoran Ji",
            "Kaiqun Fu",
            "Liang Zhao",
            "Feng Chen",
            "Lingfei Wu",
            "Charu Aggarwal",
            "Chang-Tien Lu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.01342v1",
        "title": "SAINT: Improved Neural Networks for Tabular Data via Row Attention and\n  Contrastive Pre-Training",
        "abstract": "  Tabular data underpins numerous high-impact applications of machine learning\nfrom fraud detection to genomics and healthcare. Classical approaches to\nsolving tabular problems, such as gradient boosting and random forests, are\nwidely used by practitioners. However, recent deep learning methods have\nachieved a degree of performance competitive with popular techniques. We devise\na hybrid deep learning approach to solving tabular data problems. Our method,\nSAINT, performs attention over both rows and columns, and it includes an\nenhanced embedding method. We also study a new contrastive self-supervised\npre-training method for use when labels are scarce. SAINT consistently improves\nperformance over previous deep learning methods, and it even outperforms\ngradient boosting methods, including XGBoost, CatBoost, and LightGBM, on\naverage over a variety of benchmark tasks.\n",
        "published": "2021",
        "authors": [
            "Gowthami Somepalli",
            "Micah Goldblum",
            "Avi Schwarzschild",
            "C. Bayan Bruss",
            "Tom Goldstein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.03532v1",
        "title": "Efficient Attack Detection in IoT Devices using Feature Engineering-Less\n  Machine Learning",
        "abstract": "  Through the generalization of deep learning, the research community has\naddressed critical challenges in the network security domain, like malware\nidentification and anomaly detection. However, they have yet to discuss\ndeploying them on Internet of Things (IoT) devices for day-to-day operations.\nIoT devices are often limited in memory and processing power, rendering the\ncompute-intensive deep learning environment unusable. This research proposes a\nway to overcome this barrier by bypassing feature engineering in the deep\nlearning pipeline and using raw packet data as input. We introduce a feature\nengineering-less machine learning (ML) process to perform malware detection on\nIoT devices. Our proposed model, \"Feature engineering-less-ML (FEL-ML),\" is a\nlighter-weight detection algorithm that expends no extra computations on\n\"engineered\" features. It effectively accelerates the low-powered IoT edge. It\nis trained on unprocessed byte-streams of packets. Aside from providing better\nresults, it is quicker than traditional feature-based methods. FEL-ML\nfacilitates resource-sensitive network traffic security with the added benefit\nof eliminating the significant investment by subject matter experts in feature\nengineering.\n",
        "published": "2023",
        "authors": [
            "Arshiya Khan",
            "Chase Cotton"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.03613v1",
        "title": "Explainable Artificial Intelligence (XAI): An Engineering Perspective",
        "abstract": "  The remarkable advancements in Deep Learning (DL) algorithms have fueled\nenthusiasm for using Artificial Intelligence (AI) technologies in almost every\ndomain; however, the opaqueness of these algorithms put a question mark on\ntheir applications in safety-critical systems. In this regard, the\n`explainability' dimension is not only essential to both explain the inner\nworkings of black-box algorithms, but it also adds accountability and\ntransparency dimensions that are of prime importance for regulators, consumers,\nand service providers. eXplainable Artificial Intelligence (XAI) is the set of\ntechniques and methods to convert the so-called black-box AI algorithms to\nwhite-box algorithms, where the results achieved by these algorithms and the\nvariables, parameters, and steps taken by the algorithm to reach the obtained\nresults, are transparent and explainable. To complement the existing literature\non XAI, in this paper, we take an `engineering' approach to illustrate the\nconcepts of XAI. We discuss the stakeholders in XAI and describe the\nmathematical contours of XAI from engineering perspective. Then we take the\nautonomous car as a use-case and discuss the applications of XAI for its\ndifferent components such as object detection, perception, control, action\ndecision, and so on. This work is an exploratory study to identify new avenues\nof research in the field of XAI.\n",
        "published": "2021",
        "authors": [
            "F. Hussain",
            "R. Hussain",
            "E. Hossain"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.02295v1",
        "title": "A Review of Explainable Artificial Intelligence in Manufacturing",
        "abstract": "  The implementation of Artificial Intelligence (AI) systems in the\nmanufacturing domain enables higher production efficiency, outstanding\nperformance, and safer operations, leveraging powerful tools such as deep\nlearning and reinforcement learning techniques. Despite the high accuracy of\nthese models, they are mostly considered black boxes: they are unintelligible\nto the human. Opaqueness affects trust in the system, a factor that is critical\nin the context of decision-making. We present an overview of Explainable\nArtificial Intelligence (XAI) techniques as a means of boosting the\ntransparency of models. We analyze different metrics to evaluate these\ntechniques and describe several application scenarios in the manufacturing\ndomain.\n",
        "published": "2021",
        "authors": [
            "Georgios Sofianidis",
            "Jo\u017ee M. Ro\u017eanec",
            "Dunja Mladeni\u0107",
            "Dimosthenis Kyriazis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.08612v1",
        "title": "Empowering Things with Intelligence: A Survey of the Progress,\n  Challenges, and Opportunities in Artificial Intelligence of Things",
        "abstract": "  In the Internet of Things (IoT) era, billions of sensors and devices collect\nand process data from the environment, transmit them to cloud centers, and\nreceive feedback via the internet for connectivity and perception. However,\ntransmitting massive amounts of heterogeneous data, perceiving complex\nenvironments from these data, and then making smart decisions in a timely\nmanner are difficult. Artificial intelligence (AI), especially deep learning,\nis now a proven success in various areas including computer vision, speech\nrecognition, and natural language processing. AI introduced into the IoT\nheralds the era of artificial intelligence of things (AIoT). This paper\npresents a comprehensive survey on AIoT to show how AI can empower the IoT to\nmake it faster, smarter, greener, and safer. Specifically, we briefly present\nthe AIoT architecture in the context of cloud computing, fog computing, and\nedge computing. Then, we present progress in AI research for IoT from four\nperspectives: perceiving, learning, reasoning, and behaving. Next, we summarize\nsome promising applications of AIoT that are likely to profoundly reshape our\nworld. Finally, we highlight the challenges facing AIoT and some potential\nresearch opportunities.\n",
        "published": "2020",
        "authors": [
            "Jing Zhang",
            "Dacheng Tao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.11261v1",
        "title": "How an Electrical Engineer Became an Artificial Intelligence Researcher,\n  a Multiphase Active Contours Analysis",
        "abstract": "  This essay examines how what is considered to be artificial intelligence (AI)\nhas changed over time and come to intersect with the expertise of the author.\nInitially, AI developed on a separate trajectory, both topically and\ninstitutionally, from pattern recognition, neural information processing,\ndecision and control systems, and allied topics by focusing on symbolic systems\nwithin computer science departments rather than on continuous systems in\nelectrical engineering departments. The separate evolutions continued\nthroughout the author's lifetime, with some crossover in reinforcement learning\nand graphical models, but were shocked into converging by the virality of deep\nlearning, thus making an electrical engineer into an AI researcher. Now that\nthis convergence has happened, opportunity exists to pursue an agenda that\ncombines learning and reasoning bridged by interpretable machine learning\nmodels.\n",
        "published": "2018",
        "authors": [
            "Kush R. Varshney"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.00093v4",
        "title": "Explainable Artificial Intelligence: a Systematic Review",
        "abstract": "  Explainable Artificial Intelligence (XAI) has experienced a significant\ngrowth over the last few years. This is due to the widespread application of\nmachine learning, particularly deep learning, that has led to the development\nof highly accurate models but lack explainability and interpretability. A\nplethora of methods to tackle this problem have been proposed, developed and\ntested. This systematic review contributes to the body of knowledge by\nclustering these methods with a hierarchical classification system with four\nmain clusters: review articles, theories and notions, methods and their\nevaluation. It also summarises the state-of-the-art in XAI and recommends\nfuture research directions.\n",
        "published": "2020",
        "authors": [
            "Giulia Vilone",
            "Luca Longo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.12063v1",
        "title": "Artificial Intelligence BlockCloud (AIBC) Technical Whitepaper",
        "abstract": "  The AIBC is an Artificial Intelligence and blockchain technology based\nlarge-scale decentralized ecosystem that allows system-wide low-cost sharing of\ncomputing and storage resources. The AIBC consists of four layers: a\nfundamental layer, a resource layer, an application layer, and an ecosystem\nlayer. The AIBC implements a two-consensus scheme to enforce upper-layer\neconomic policies and achieve fundamental layer performance and robustness: the\nDPoEV incentive consensus on the application and resource layers, and the DABFT\ndistributed consensus on the fundamental layer. The DABFT uses deep learning\ntechniques to predict and select the most suitable BFT algorithm in order to\nachieve the best balance of performance, robustness, and security. The DPoEV\nuses the knowledge map algorithm to accurately assess the economic value of\ndigital assets.\n",
        "published": "2019",
        "authors": [
            "Qi Deng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.07125v1",
        "title": "Opportunities for artificial intelligence in advancing precision\n  medicine",
        "abstract": "  Machine learning (ML), deep learning (DL), and artificial intelligence (AI)\nare of increasing importance in biomedicine. The goal of this work is to show\nprogress in ML in digital health, to exemplify future needs and trends, and to\nidentify any essential prerequisites of AI and ML for precision health.\nHigh-throughput technologies are delivering growing volumes of biomedical data,\nsuch as large-scale genome-wide sequencing assays, libraries of medical images,\nor drug perturbation screens of healthy, developing, and diseased tissue.\nMulti-omics data in biomedicine is deep and complex, offering an opportunity\nfor data-driven insights and automated disease classification. Learning from\nthese data will open our understanding and definition of healthy baselines and\ndisease signatures. State-of-the-art applications of deep neural networks\ninclude digital image recognition, single cell clustering, and virtual drug\nscreens, demonstrating breadths and power of ML in biomedicine. Significantly,\nAI and systems biology have embraced big data challenges and may enable novel\nbiotechnology-derived therapies to facilitate the implementation of precision\nmedicine approaches.\n",
        "published": "2019",
        "authors": [
            "Fabian V. Filipp"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.04599v1",
        "title": "A review of artificial intelligence methods combined with Raman\n  spectroscopy to identify the composition of substances",
        "abstract": "  In general, most of the substances in nature exist in mixtures, and the\nnoninvasive identification of mixture composition with high speed and accuracy\nremains a difficult task. However, the development of Raman spectroscopy,\nmachine learning, and deep learning techniques have paved the way for achieving\nefficient analytical tools capable of identifying mixture components, making an\napparent breakthrough in the identification of mixtures beyond the traditional\nchemical analysis methods. This article summarizes the work of Raman\nspectroscopy in identifying the composition of substances as well as provides\ndetailed reviews on the preprocessing process of Raman spectroscopy, the\nanalysis methods and applications of artificial intelligence. This review\nsummarizes the work of Raman spectroscopy in identifying the composition of\nsubstances and reviews the preprocessing process of Raman spectroscopy, the\nanalysis methods and applications of artificial intelligence. Finally, the\nadvantages and disadvantages and development prospects of Raman spectroscopy\nare discussed in detail.\n",
        "published": "2021",
        "authors": [
            "Liangrui Pan",
            "Peng Zhang",
            "Chalongrat Daengngam",
            "Mitchai Chongcheawchamnan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.10696v2",
        "title": "CC-Cert: A Probabilistic Approach to Certify General Robustness of\n  Neural Networks",
        "abstract": "  In safety-critical machine learning applications, it is crucial to defend\nmodels against adversarial attacks -- small modifications of the input that\nchange the predictions. Besides rigorously studied $\\ell_p$-bounded additive\nperturbations, recently proposed semantic perturbations (e.g. rotation,\ntranslation) raise a serious concern on deploying ML systems in real-world.\nTherefore, it is important to provide provable guarantees for deep learning\nmodels against semantically meaningful input transformations. In this paper, we\npropose a new universal probabilistic certification approach based on\nChernoff-Cramer bounds that can be used in general attack settings. We estimate\nthe probability of a model to fail if the attack is sampled from a certain\ndistribution. Our theoretical findings are supported by experimental results on\ndifferent datasets.\n",
        "published": "2021",
        "authors": [
            "Mikhail Pautov",
            "Nurislam Tursynbek",
            "Marina Munkhoeva",
            "Nikita Muravev",
            "Aleksandr Petiushko",
            "Ivan Oseledets"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.09130v2",
        "title": "Artificial Intelligence for Suicide Assessment using Audiovisual Cues: A\n  Review",
        "abstract": "  Death by suicide is the seventh leading death cause worldwide. The recent\nadvancement in Artificial Intelligence (AI), specifically AI applications in\nimage and voice processing, has created a promising opportunity to\nrevolutionize suicide risk assessment. Subsequently, we have witnessed\nfast-growing literature of research that applies AI to extract audiovisual\nnon-verbal cues for mental illness assessment. However, the majority of the\nrecent works focus on depression, despite the evident difference between\ndepression symptoms and suicidal behavior and non-verbal cues. This paper\nreviews recent works that study suicide ideation and suicide behavior detection\nthrough audiovisual feature analysis, mainly suicidal voice/speech acoustic\nfeatures analysis and suicidal visual cues. Automatic suicide assessment is a\npromising research direction that is still in the early stages. Accordingly,\nthere is a lack of large datasets that can be used to train machine learning\nand deep learning models proven to be effective in other, similar tasks.\n",
        "published": "2022",
        "authors": [
            "Sahraoui Dhelim",
            "Liming Chen",
            "Huansheng Ning",
            "Chris Nugent"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.02959v1",
        "title": "A Perspective on Robotic Telepresence and Teleoperation using Cognition:\n  Are we there yet?",
        "abstract": "  Telepresence and teleoperation robotics have attracted a great amount of\nattention in the last 10 years. With the Artificial Intelligence (AI)\nrevolution already being started, we can see a wide range of robotic\napplications being realized. Intelligent robotic systems are being deployed\nboth in industrial and domestic environments. Telepresence is the idea of being\npresent in a remote location virtually or via robotic avatars. Similarly, the\nidea of operating a robot from a remote location for various tasks is called\nteleoperation. These technologies find significant application in health care,\neducation, surveillance, disaster recovery, and corporate/government sectors.\nBut question still remains about their maturity, security and safety levels. We\nalso need to think about enhancing the user experience and trust in such\ntechnologies going into the next generation of computing.\n",
        "published": "2022",
        "authors": [
            "Hrishav Bakul Barua",
            "Ashis Sau",
            "Ruddra dev Roychoudhury"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.14919v1",
        "title": "A Deep Learning Approach for Automatic Detection of Qualitative Features\n  of Lecturing",
        "abstract": "  Artificial Intelligence in higher education opens new possibilities for\nimproving the lecturing process, such as enriching didactic materials, helping\nin assessing students' works or even providing directions to the teachers on\nhow to enhance the lectures. We follow this research path, and in this work, we\nexplore how an academic lecture can be assessed automatically by quantitative\nfeatures. First, we prepare a set of qualitative features based on teaching\npractices and then annotate the dataset of academic lecture videos collected\nfor this purpose. We then show how these features could be detected\nautomatically using machine learning and computer vision techniques. Our\nresults show the potential usefulness of our work.\n",
        "published": "2022",
        "authors": [
            "Anna Wroblewska",
            "Jozef Jasek",
            "Bogdan Jastrzebski",
            "Stanislaw Pawlak",
            "Anna Grzywacz",
            "Cheong Siew Ann",
            "Tan Seng Chee",
            "Tomasz Trzcinski",
            "Janusz Holyst"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.08430v1",
        "title": "Power-law Scaling to Assist with Key Challenges in Artificial\n  Intelligence",
        "abstract": "  Power-law scaling, a central concept in critical phenomena, is found to be\nuseful in deep learning, where optimized test errors on handwritten digit\nexamples converge as a power-law to zero with database size. For rapid decision\nmaking with one training epoch, each example is presented only once to the\ntrained network, the power-law exponent increased with the number of hidden\nlayers. For the largest dataset, the obtained test error was estimated to be in\nthe proximity of state-of-the-art algorithms for large epoch numbers. Power-law\nscaling assists with key challenges found in current artificial intelligence\napplications and facilitates an a priori dataset size estimation to achieve a\ndesired test accuracy. It establishes a benchmark for measuring training\ncomplexity and a quantitative hierarchy of machine learning tasks and\nalgorithms.\n",
        "published": "2022",
        "authors": [
            "Yuval Meir",
            "Shira Sardi",
            "Shiri Hodassman",
            "Karin Kisos",
            "Itamar Ben-Noam",
            "Amir Goldental",
            "Ido Kanter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.16346v1",
        "title": "Artificial Intelligence-Based Methods for Precision Medicine: Diabetes\n  Risk Prediction",
        "abstract": "  The rising prevalence of type 2 diabetes mellitus (T2DM) necessitates the\ndevelopment of predictive models for T2DM risk assessment. Artificial\nintelligence (AI) models are being extensively used for this purpose, but a\ncomprehensive review of their advancements and challenges is lacking. This\nscoping review analyzes existing literature on AI-based models for T2DM risk\nprediction. Forty studies were included, mainly published in the past four\nyears. Traditional machine learning models were more prevalent than deep\nlearning models. Electronic health records were the most commonly used data\nsource. Unimodal AI models relying on EHR data were prominent, while only a few\nutilized multimodal models. Both unimodal and multimodal models showed\npromising performance, with the latter outperforming the former. Internal\nvalidation was common, while external validation was limited. Interpretability\nmethods were reported in half of the studies. Few studies reported novel\nbiomarkers, and open-source code availability was limited. This review provides\ninsights into the current state and limitations of AI-based T2DM risk\nprediction models and highlights challenges for their development and clinical\nimplementation.\n",
        "published": "2023",
        "authors": [
            "Farida Mohsen",
            "Hamada R. H. Al-Absi",
            "Noha A. Yousri",
            "Nady El Hajj",
            "Zubair Shah"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.05787v1",
        "title": "Adaptive User-centered Neuro-symbolic Learning for Multimodal\n  Interaction with Autonomous Systems",
        "abstract": "  Recent advances in machine learning, particularly deep learning, have enabled\nautonomous systems to perceive and comprehend objects and their environments in\na perceptual subsymbolic manner. These systems can now perform object\ndetection, sensor data fusion, and language understanding tasks. However, there\nis a growing need to enhance these systems to understand objects and their\nenvironments more conceptually and symbolically. It is essential to consider\nboth the explicit teaching provided by humans (e.g., describing a situation or\nexplaining how to act) and the implicit teaching obtained by observing human\nbehavior (e.g., through the system's sensors) to achieve this level of powerful\nartificial intelligence. Thus, the system must be designed with multimodal\ninput and output capabilities to support implicit and explicit interaction\nmodels. In this position paper, we argue for considering both types of inputs,\nas well as human-in-the-loop and incremental learning techniques, for advancing\nthe field of artificial intelligence and enabling autonomous systems to learn\nlike humans. We propose several hypotheses and design guidelines and highlight\na use case from related work to achieve this goal.\n",
        "published": "2023",
        "authors": [
            "Amr Gomaa",
            "Michael Feld"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.05665v1",
        "title": "Explainable artificial intelligence for Healthcare applications using\n  Random Forest Classifier with LIME and SHAP",
        "abstract": "  With the advances in computationally efficient artificial Intelligence (AI)\ntechniques and their numerous applications in our everyday life, there is a\npressing need to understand the computational details hidden in black box AI\ntechniques such as most popular machine learning and deep learning techniques;\nthrough more detailed explanations. The origin of explainable AI (xAI) is\ncoined from these challenges and recently gained more attention by the\nresearchers by adding explainability comprehensively in traditional AI systems.\nThis leads to develop an appropriate framework for successful applications of\nxAI in real life scenarios with respect to innovations, risk mitigation,\nethical issues and logical values to the users. In this book chapter, an\nin-depth analysis of several xAI frameworks and methods including LIME (Local\nInterpretable Model-agnostic Explanations) and SHAP (SHapley Additive\nexPlanations) are provided. Random Forest Classifier as black box AI is used on\na publicly available Diabetes symptoms dataset with LIME and SHAP for better\ninterpretations. The results obtained are interesting in terms of transparency,\nvalid and trustworthiness in diabetes disease prediction.\n",
        "published": "2023",
        "authors": [
            "Mrutyunjaya Panda",
            "Soumya Ranjan Mahanta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.05556v2",
        "title": "Informing Artificial Intelligence Generative Techniques using Cognitive\n  Theories of Human Creativity",
        "abstract": "  The common view that our creativity is what makes us uniquely human suggests\nthat incorporating research on human creativity into generative deep learning\ntechniques might be a fruitful avenue for making their outputs more compelling\nand human-like. Using an original synthesis of Deep Dream-based convolutional\nneural networks and cognitive based computational art rendering systems, we\nshow how honing theory, intrinsic motivation, and the notion of a 'seed\nincident' can be implemented computationally, and demonstrate their impact on\nthe resulting generative art. Conversely, we discuss how explorations in deep\nlearn-ing convolutional neural net generative systems can inform our\nunderstanding of human creativity. We conclude with ideas for further\ncross-fertilization between AI based computational creativity and psychology of\ncreativity.\n",
        "published": "2018",
        "authors": [
            "Steve DiPaola",
            "Liane Gabora",
            "Graeme McCaig"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.11787v1",
        "title": "A Hitchhiker's Guide On Distributed Training of Deep Neural Networks",
        "abstract": "  Deep learning has led to tremendous advancements in the field of Artificial\nIntelligence. One caveat however is the substantial amount of compute needed to\ntrain these deep learning models. Training a benchmark dataset like ImageNet on\na single machine with a modern GPU can take upto a week, distributing training\non multiple machines has been observed to drastically bring this time down.\nRecent work has brought down ImageNet training time to a time as low as 4\nminutes by using a cluster of 2048 GPUs. This paper surveys the various\nalgorithms and techniques used to distribute training and presents the current\nstate of the art for a modern distributed training framework. More\nspecifically, we explore the synchronous and asynchronous variants of\ndistributed Stochastic Gradient Descent, various All Reduce gradient\naggregation strategies and best practices for obtaining higher throughout and\nlower latency over a cluster such as mixed precision training, large batch\ntraining and gradient compression.\n",
        "published": "2018",
        "authors": [
            "Karanbir Chahal",
            "Manraj Singh Grover",
            "Kuntal Dey"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.17020v2",
        "title": "A Law of Data Separation in Deep Learning",
        "abstract": "  While deep learning has enabled significant advances in many areas of\nscience, its black-box nature hinders architecture design for future artificial\nintelligence applications and interpretation for high-stakes decision makings.\nWe addressed this issue by studying the fundamental question of how deep neural\nnetworks process data in the intermediate layers. Our finding is a simple and\nquantitative law that governs how deep neural networks separate data according\nto class membership throughout all layers for classification. This law shows\nthat each layer improves data separation at a constant geometric rate, and its\nemergence is observed in a collection of network architectures and datasets\nduring training. This law offers practical guidelines for designing\narchitectures, improving model robustness and out-of-sample performance, as\nwell as interpreting the predictions.\n",
        "published": "2022",
        "authors": [
            "Hangfeng He",
            "Weijie J. Su"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.06455v2",
        "title": "Graph Neural Network contextual embedding for Deep Learning on Tabular\n  Data",
        "abstract": "  All industries are trying to leverage Artificial Intelligence (AI) based on\ntheir existing big data which is available in so called tabular form, where\neach record is composed of a number of heterogeneous continuous and categorical\ncolumns also known as features. Deep Learning (DL) has constituted a major\nbreakthrough for AI in fields related to human skills like natural language\nprocessing, but its applicability to tabular data has been more challenging.\nMore classical Machine Learning (ML) models like tree-based ensemble ones\nusually perform better. This paper presents a novel DL model using Graph Neural\nNetwork (GNN) more specifically Interaction Network (IN), for contextual\nembedding and modelling interactions among tabular features. Its results\noutperform those of a recently published survey with DL benchmark based on five\npublic datasets, also achieving competitive results when compared to\nboosted-tree solutions.\n",
        "published": "2023",
        "authors": [
            "Mario Villaiz\u00e1n-Vallelado",
            "Matteo Salvatori",
            "Bel\u00e9n Carro Martinez",
            "Antonio Javier Sanchez Esguevillas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.15464v1",
        "title": "Mathematical Challenges in Deep Learning",
        "abstract": "  Deep models are dominating the artificial intelligence (AI) industry since\nthe ImageNet challenge in 2012. The size of deep models is increasing ever\nsince, which brings new challenges to this field with applications in cell\nphones, personal computers, autonomous cars, and wireless base stations. Here\nwe list a set of problems, ranging from training, inference, generalization\nbound, and optimization with some formalism to communicate these challenges\nwith mathematicians, statisticians, and theoretical computer scientists. This\nis a subjective view of the research questions in deep learning that benefits\nthe tech industry in long run.\n",
        "published": "2023",
        "authors": [
            "Vahid Partovi Nia",
            "Guojun Zhang",
            "Ivan Kobyzev",
            "Michael R. Metel",
            "Xinlin Li",
            "Ke Sun",
            "Sobhan Hemati",
            "Masoud Asgharian",
            "Linglong Kong",
            "Wulong Liu",
            "Boxing Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.16733v1",
        "title": "Resilience of Deep Learning applications: a systematic survey of\n  analysis and hardening techniques",
        "abstract": "  Machine Learning (ML) is currently being exploited in numerous applications\nbeing one of the most effective Artificial Intelligence (AI) technologies, used\nin diverse fields, such as vision, autonomous systems, and alike. The trend\nmotivated a significant amount of contributions to the analysis and design of\nML applications against faults affecting the underlying hardware. The authors\ninvestigate the existing body of knowledge on Deep Learning (among ML\ntechniques) resilience against hardware faults systematically through a\nthoughtful review in which the strengths and weaknesses of this literature\nstream are presented clearly and then future avenues of research are set out.\nThe review is based on 163 scientific articles published between January 2019\nand March 2023. The authors adopt a classifying framework to interpret and\nhighlight research similarities and peculiarities, based on several parameters,\nstarting from the main scope of the work, the adopted fault and error models,\nto their reproducibility. This framework allows for a comparison of the\ndifferent solutions and the identification of possible synergies. Furthermore,\nsuggestions concerning the future direction of research are proposed in the\nform of open challenges to be addressed.\n",
        "published": "2023",
        "authors": [
            "Cristiana Bolchini",
            "Luca Cassano",
            "Antonio Miele"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.05640v1",
        "title": "Keyword spotting -- Detecting commands in speech using deep learning",
        "abstract": "  Speech recognition has become an important task in the development of machine\nlearning and artificial intelligence. In this study, we explore the important\ntask of keyword spotting using speech recognition machine learning and deep\nlearning techniques. We implement feature engineering by converting raw\nwaveforms to Mel Frequency Cepstral Coefficients (MFCCs), which we use as\ninputs to our models. We experiment with several different algorithms such as\nHidden Markov Model with Gaussian Mixture, Convolutional Neural Networks and\nvariants of Recurrent Neural Networks including Long Short-Term Memory and the\nAttention mechanism. In our experiments, RNN with BiLSTM and Attention achieves\nthe best performance with an accuracy of 93.9 %\n",
        "published": "2023",
        "authors": [
            "Sumedha Rai",
            "Tong Li",
            "Bella Lyu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.00340v1",
        "title": "Towards Adversarial Training with Moderate Performance Improvement for\n  Neural Network Classification",
        "abstract": "  It has been demonstrated that deep neural networks are prone to noisy\nexamples particular adversarial samples during inference process. The gap\nbetween robust deep learning systems in real world applications and vulnerable\nneural networks is still large. Current adversarial training strategies improve\nthe robustness against adversarial samples. However, these methods lead to\naccuracy reduction when the input examples are clean thus hinders the\npracticability. In this paper, we investigate an approach that protects the\nneural network classification from the adversarial samples and improves its\naccuracy when the input examples are clean. We demonstrate the versatility and\neffectiveness of our proposed approach on a variety of different networks and\ndatasets.\n",
        "published": "2018",
        "authors": [
            "Xinhan Di",
            "Pengqian Yu",
            "Meng Tian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.02919v1",
        "title": "Uniform State Abstraction For Reinforcement Learning",
        "abstract": "  Potential Based Reward Shaping combined with a potential function based on\nappropriately defined abstract knowledge has been shown to significantly\nimprove learning speed in Reinforcement Learning. MultiGrid Reinforcement\nLearning (MRL) has further shown that such abstract knowledge in the form of a\npotential function can be learned almost solely from agent interaction with the\nenvironment. However, we show that MRL faces the problem of not extending well\nto work with Deep Learning. In this paper we extend and improve MRL to take\nadvantage of modern Deep Learning algorithms such as Deep Q-Networks (DQN). We\nshow that DQN augmented with our approach perform significantly better on\ncontinuous control tasks than its Vanilla counterpart and DQN augmented with\nMRL.\n",
        "published": "2020",
        "authors": [
            "John Burden",
            "Daniel Kudenko"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1604.08880v1",
        "title": "Deep, Convolutional, and Recurrent Models for Human Activity Recognition\n  using Wearables",
        "abstract": "  Human activity recognition (HAR) in ubiquitous computing is beginning to\nadopt deep learning to substitute for well-established analysis techniques that\nrely on hand-crafted feature extraction and classification techniques. From\nthese isolated applications of custom deep architectures it is, however,\ndifficult to gain an overview of their suitability for problems ranging from\nthe recognition of manipulative gestures to the segmentation and identification\nof physical activities like running or ascending stairs. In this paper we\nrigorously explore deep, convolutional, and recurrent approaches across three\nrepresentative datasets that contain movement data captured with wearable\nsensors. We describe how to train recurrent approaches in this setting,\nintroduce a novel regularisation approach, and illustrate how they outperform\nthe state-of-the-art on a large benchmark dataset. Across thousands of\nrecognition experiments with randomly sampled model configurations we\ninvestigate the suitability of each model for different tasks in HAR, explore\nthe impact of hyperparameters using the fANOVA framework, and provide\nguidelines for the practitioner who wants to apply deep learning in their\nproblem setting.\n",
        "published": "2016",
        "authors": [
            "Nils Y. Hammerla",
            "Shane Halloran",
            "Thomas Ploetz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.12072v1",
        "title": "Towards Explainable Artificial Intelligence",
        "abstract": "  In recent years, machine learning (ML) has become a key enabling technology\nfor the sciences and industry. Especially through improvements in methodology,\nthe availability of large databases and increased computational power, today's\nML algorithms are able to achieve excellent performance (at times even\nexceeding the human level) on an increasing number of complex tasks. Deep\nlearning models are at the forefront of this development. However, due to their\nnested non-linear structure, these powerful models have been generally\nconsidered \"black boxes\", not providing any information about what exactly\nmakes them arrive at their predictions. Since in many applications, e.g., in\nthe medical domain, such lack of transparency may be not acceptable, the\ndevelopment of methods for visualizing, explaining and interpreting deep\nlearning models has recently attracted increasing attention. This introductory\npaper presents recent developments and applications in this field and makes a\nplea for a wider use of explainable learning algorithms in practice.\n",
        "published": "2019",
        "authors": [
            "Wojciech Samek",
            "Klaus-Robert M\u00fcller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.12848v1",
        "title": "Deep Non-Crossing Quantiles through the Partial Derivative",
        "abstract": "  Quantile Regression (QR) provides a way to approximate a single conditional\nquantile. To have a more informative description of the conditional\ndistribution, QR can be merged with deep learning techniques to simultaneously\nestimate multiple quantiles. However, the minimisation of the QR-loss function\ndoes not guarantee non-crossing quantiles, which affects the validity of such\npredictions and introduces a critical issue in certain scenarios. In this\narticle, we propose a generic deep learning algorithm for predicting an\narbitrary number of quantiles that ensures the quantile monotonicity constraint\nup to the machine precision and maintains its modelling performance with\nrespect to alternative models. The presented method is evaluated over several\nreal-world datasets obtaining state-of-the-art results as well as showing that\nit scales to large-size data sets.\n",
        "published": "2022",
        "authors": [
            "Axel Brando",
            "Joan Gimeno",
            "Jose A. Rodr\u00edguez-Serrano",
            "Jordi Vitri\u00e0"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1510.07526v3",
        "title": "Empirical Study on Deep Learning Models for Question Answering",
        "abstract": "  In this paper we explore deep learning models with memory component or\nattention mechanism for question answering task. We combine and compare three\nmodels, Neural Machine Translation, Neural Turing Machine, and Memory Networks\nfor a simulated QA data set. This paper is the first one that uses Neural\nMachine Translation and Neural Turing Machines for solving QA tasks. Our\nresults suggest that the combination of attention and memory have potential to\nsolve certain QA problem.\n",
        "published": "2015",
        "authors": [
            "Yang Yu",
            "Wei Zhang",
            "Chung-Wei Hang",
            "Bing Xiang",
            "Bowen Zhou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1702.08039v2",
        "title": "Criticality & Deep Learning I: Generally Weighted Nets",
        "abstract": "  Motivated by the idea that criticality and universality of phase transitions\nmight play a crucial role in achieving and sustaining learning and intelligent\nbehaviour in biological and artificial networks, we analyse a theoretical and a\npragmatic experimental set up for critical phenomena in deep learning. On the\ntheoretical side, we use results from statistical physics to carry out critical\npoint calculations in feed-forward/fully connected networks, while on the\nexperimental side we set out to find traces of criticality in deep neural\nnetworks. This is our first step in a series of upcoming investigations to map\nout the relationship between criticality and learning in deep networks.\n",
        "published": "2017",
        "authors": [
            "Dan Oprisa",
            "Peter Toth"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.02210v1",
        "title": "SLDR-DL: A Framework for SLD-Resolution with Deep Learning",
        "abstract": "  This paper introduces an SLD-resolution technique based on deep learning.\nThis technique enables neural networks to learn from old and successful\nresolution processes and to use learnt experiences to guide new resolution\nprocesses. An implementation of this technique is named SLDR-DL. It includes a\nProlog library of deep feedforward neural networks and some essential functions\nof resolution. In the SLDR-DL framework, users can define logical rules in the\nform of definite clauses and teach neural networks to use the rules in\nreasoning processes.\n",
        "published": "2017",
        "authors": [
            "Cheng-Hao Cai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.10342v1",
        "title": "Deep Learning for Ontology Reasoning",
        "abstract": "  In this work, we present a novel approach to ontology reasoning that is based\non deep learning rather than logic-based formal reasoning. To this end, we\nintroduce a new model for statistical relational learning that is built upon\ndeep recursive neural networks, and give experimental evidence that it can\neasily compete with, or even outperform, existing logic-based reasoners on the\ntask of ontology reasoning. More precisely, we compared our implemented system\nwith one of the best logic-based ontology reasoners at present, RDFox, on a\nnumber of large standard benchmark datasets, and found that our system attained\nhigh reasoning quality, while being up to two orders of magnitude faster.\n",
        "published": "2017",
        "authors": [
            "Patrick Hohenecker",
            "Thomas Lukasiewicz"
        ]
    }
]