[
    {
        "id": "http://arxiv.org/abs/1811.08225v1",
        "title": "Self Organizing Classifiers: First Steps in Structured Evolutionary\n  Machine Learning",
        "abstract": "  Learning classifier systems (LCSs) are evolutionary machine learning\nalgorithms, flexible enough to be applied to reinforcement, supervised and\nunsupervised learning problems with good performance. Recently, self organizing\nclassifiers were proposed which are similar to LCSs but have the advantage that\nin its structured population no balance between niching and fitness pressure is\nnecessary. However, more tests and analysis are required to verify its\nbenefits. Here, a variation of the first algorithm is proposed which uses a\nparameterless self organizing map (SOM). This algorithm is applied in\nchallenging problems such as big, noisy as well as dynamically changing\ncontinuous input-action mazes (growing and compressing mazes are included) with\ngood performance. Moreover, a genetic operator is proposed which utilizes the\ntopological information of the SOM's population structure, improving the\nresults. Thus, the first steps in structured evolutionary machine learning are\nshown, nonetheless, the problems faced are more difficult than the state-of-art\ncontinuous input-action multi-step ones.\n",
        "published": "2018",
        "authors": [
            "Danilo Vasconcellos Vargas",
            "Hirotaka Takano",
            "Junichi Murata"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.06212v1",
        "title": "Accelerating and Improving AlphaZero Using Population Based Training",
        "abstract": "  AlphaZero has been very successful in many games. Unfortunately, it still\nconsumes a huge amount of computing resources, the majority of which is spent\nin self-play. Hyperparameter tuning exacerbates the training cost since each\nhyperparameter configuration requires its own time to train one run, during\nwhich it will generate its own self-play records. As a result, multiple runs\nare usually needed for different hyperparameter configurations. This paper\nproposes using population based training (PBT) to help tune hyperparameters\ndynamically and improve strength during training time. Another significant\nadvantage is that this method requires a single run only, while incurring a\nsmall additional time cost, since the time for generating self-play records\nremains unchanged though the time for optimization is increased following the\nAlphaZero training algorithm. In our experiments for 9x9 Go, the PBT method is\nable to achieve a higher win rate for 9x9 Go than the baselines, each with its\nown hyperparameter configuration and trained individually. For 19x19 Go, with\nPBT, we are able to obtain improvements in playing strength. Specifically, the\nPBT agent can obtain up to 74% win rate against ELF OpenGo, an open-source\nstate-of-the-art AlphaZero program using a neural network of a comparable\ncapacity. This is compared to a saturated non-PBT agent, which achieves a win\nrate of 47% against ELF OpenGo under the same circumstances.\n",
        "published": "2020",
        "authors": [
            "Ti-Rong Wu",
            "Ting-Han Wei",
            "I-Chen Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.08858v1",
        "title": "L\u00e9vy walks derived from a Bayesian decision-making model in\n  non-stationary environments",
        "abstract": "  L\\'evy walks are found in the migratory behaviour patterns of various\norganisms, and the reason for this phenomenon has been much discussed. We use\nsimulations to demonstrate that learning causes the changes in confidence level\nduring decision-making in non-stationary environments, and results in\nL\\'evy-walk-like patterns. One inference algorithm involving confidence is\nBayesian inference. We propose an algorithm that introduces the effects of\nlearning and forgetting into Bayesian inference, and simulate an imitation game\nin which two decision-making agents incorporating the algorithm estimate each\nother's internal models from their opponent's observational data. For\nforgetting without learning, agent confidence levels remained low due to a lack\nof information on the counterpart and Brownian walks occurred for a wide range\nof forgetting rates. Conversely, when learning was introduced, high confidence\nlevels occasionally occurred even at high forgetting rates, and Brownian walks\nuniversally became L\\'evy walks through a mixture of high- and low-confidence\nstates.\n",
        "published": "2020",
        "authors": [
            "Shuji Shinohara",
            "Nobuhito Manome",
            "Yoshihiro Nakajima",
            "Yukio Pegio Gunji",
            "Toru Moriyama",
            "Hiroshi Okamoto",
            "Shunji Mitsuyoshi",
            "Ung-il Chung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.01636v1",
        "title": "Sparse Training Theory for Scalable and Efficient Agents",
        "abstract": "  A fundamental task for artificial intelligence is learning. Deep Neural\nNetworks have proven to cope perfectly with all learning paradigms, i.e.\nsupervised, unsupervised, and reinforcement learning. Nevertheless, traditional\ndeep learning approaches make use of cloud computing facilities and do not\nscale well to autonomous agents with low computational resources. Even in the\ncloud, they suffer from computational and memory limitations, and they cannot\nbe used to model adequately large physical worlds for agents which assume\nnetworks with billions of neurons. These issues are addressed in the last few\nyears by the emerging topic of sparse training, which trains sparse networks\nfrom scratch. This paper discusses sparse training state-of-the-art, its\nchallenges and limitations while introducing a couple of new theoretical\nresearch directions which has the potential of alleviating sparse training\nlimitations to push deep learning scalability well beyond its current\nboundaries. Nevertheless, the theoretical advancements impact in complex\nmulti-agents settings is discussed from a real-world perspective, using the\nsmart grid case study.\n",
        "published": "2021",
        "authors": [
            "Decebal Constantin Mocanu",
            "Elena Mocanu",
            "Tiago Pinto",
            "Selima Curci",
            "Phuong H. Nguyen",
            "Madeleine Gibescu",
            "Damien Ernst",
            "Zita A. Vale"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.03359v1",
        "title": "Cognitive Homeostatic Agents",
        "abstract": "  Human brain has been used as an inspiration for building autonomous agents,\nbut it is not obvious what level of computational description of the brain one\nshould use. This has led to overly opinionated symbolic approaches and overly\nunstructured connectionist approaches. We propose that using homeostasis as the\ncomputational description provides a good compromise. Similar to how\nphysiological homeostasis is the regulation of certain homeostatic variables,\ncognition can be interpreted as the regulation of certain 'cognitive\nhomeostatic variables'. We present an outline of a Cognitive Homeostatic Agent,\nbuilt as a hierarchy of physiological and cognitive homeostatic subsystems and\ndescribe structures and processes to guide future exploration. We expect this\nto be a fruitful line of investigation towards building sophisticated\nartificial agents that can act flexibly in complex environments, and produce\nbehaviors indicating planning, thinking and feelings.\n",
        "published": "2021",
        "authors": [
            "Amol Kelkar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/cs/0105025v1",
        "title": "Market-Based Reinforcement Learning in Partially Observable Worlds",
        "abstract": "  Unlike traditional reinforcement learning (RL), market-based RL is in\nprinciple applicable to worlds described by partially observable Markov\nDecision Processes (POMDPs), where an agent needs to learn short-term memories\nof relevant previous events in order to execute optimal actions. Most previous\nwork, however, has focused on reactive settings (MDPs) instead of POMDPs. Here\nwe reimplement a recent approach to market-based RL and for the first time\nevaluate it in a toy POMDP setting.\n",
        "published": "2001",
        "authors": [
            "Ivo Kwee",
            "Marcus Hutter",
            "Juergen Schmidhuber"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.08226v1",
        "title": "Self Organizing Classifiers and Niched Fitness",
        "abstract": "  Learning classifier systems are adaptive learning systems which have been\nwidely applied in a multitude of application domains. However, there are still\nsome generalization problems unsolved. The hurdle is that fitness and niching\npressures are difficult to balance. Here, a new algorithm called Self\nOrganizing Classifiers is proposed which faces this problem from a different\nperspective. Instead of balancing the pressures, both pressures are separated\nand no balance is necessary. In fact, the proposed algorithm possesses a\ndynamical population structure that self-organizes itself to better project the\ninput space into a map. The niched fitness concept is defined along with its\ndynamical population structure, both are indispensable for the understanding of\nthe proposed method. Promising results are shown on two continuous multi-step\nproblems. One of which is yet more challenging than previous problems of this\nclass in the literature.\n",
        "published": "2018",
        "authors": [
            "Danilo Vasconcellos Vargas",
            "Hirotaka Takano",
            "Junichi Murata"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.04143v4",
        "title": "Deep Reinforcement Learning in Parameterized Action Space",
        "abstract": "  Recent work has shown that deep neural networks are capable of approximating\nboth value functions and policies in reinforcement learning domains featuring\ncontinuous state and action spaces. However, to the best of our knowledge no\nprevious work has succeeded at using deep neural networks in structured\n(parameterized) continuous action spaces. To fill this gap, this paper focuses\non learning within the domain of simulated RoboCup soccer, which features a\nsmall set of discrete action types, each of which is parameterized with\ncontinuous variables. The best learned agent can score goals more reliably than\nthe 2012 RoboCup champion agent. As such, this paper represents a successful\nextension of deep reinforcement learning to the class of parameterized action\nspace MDPs.\n",
        "published": "2015",
        "authors": [
            "Matthew Hausknecht",
            "Peter Stone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.10545v5",
        "title": "Winning Isn't Everything: Enhancing Game Development with Intelligent\n  Agents",
        "abstract": "  Recently, there have been several high-profile achievements of agents\nlearning to play games against humans and beat them. In this paper, we study\nthe problem of training intelligent agents in service of game development.\nUnlike the agents built to \"beat the game\", our agents aim to produce\nhuman-like behavior to help with game evaluation and balancing. We discuss two\nfundamental metrics based on which we measure the human-likeness of agents,\nnamely skill and style, which are multi-faceted concepts with practical\nimplications outlined in this paper. We report four case studies in which the\nstyle and skill requirements inform the choice of algorithms and metrics used\nto train agents; ranging from A* search to state-of-the-art deep reinforcement\nlearning. We, further, show that the learning potential of state-of-the-art\ndeep RL models does not seamlessly transfer from the benchmark environments to\ntarget ones without heavily tuning their hyperparameters, leading to linear\nscaling of the engineering efforts and computational cost with the number of\ntarget domains.\n",
        "published": "2019",
        "authors": [
            "Yunqi Zhao",
            "Igor Borovikov",
            "Fernando de Mesentier Silva",
            "Ahmad Beirami",
            "Jason Rupert",
            "Caedmon Somers",
            "Jesse Harder",
            "John Kolen",
            "Jervis Pinto",
            "Reza Pourabolghasem",
            "James Pestrak",
            "Harold Chaput",
            "Mohsen Sardari",
            "Long Lin",
            "Sundeep Narravula",
            "Navid Aghdaie",
            "Kazi Zaman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.14616v1",
        "title": "Lineage Evolution Reinforcement Learning",
        "abstract": "  We propose a general agent population learning system, and on this basis, we\npropose lineage evolution reinforcement learning algorithm. Lineage evolution\nreinforcement learning is a kind of derivative algorithm which accords with the\ngeneral agent population learning system. We take the agents in DQN and its\nrelated variants as the basic agents in the population, and add the selection,\nmutation and crossover modules in the genetic algorithm to the reinforcement\nlearning algorithm. In the process of agent evolution, we refer to the\ncharacteristics of natural genetic behavior, add lineage factor to ensure the\nretention of potential performance of agent, and comprehensively consider the\ncurrent performance and lineage value when evaluating the performance of agent.\nWithout changing the parameters of the original reinforcement learning\nalgorithm, lineage evolution reinforcement learning can optimize different\nreinforcement learning algorithms. Our experiments show that the idea of\nevolution with lineage improves the performance of original reinforcement\nlearning algorithm in some games in Atari 2600.\n",
        "published": "2020",
        "authors": [
            "Zeyu Zhang",
            "Guisheng Yin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.02010v1",
        "title": "A Hybrid Algorithm for Metaheuristic Optimization",
        "abstract": "  We propose a novel, flexible algorithm for combining together\nmetaheuristicoptimizers for non-convex optimization problems. Our approach\ntreatsthe constituent optimizers as a team of complex agents that\ncommunicateinformation amongst each other at various intervals during the\nsimulationprocess. The information produced by each individual agent can be\ncombinedin various ways via higher-level operators. In our experiments on\nkeybenchmark functions, we investigate how the performance of our\nalgorithmvaries with respect to several of its key modifiable properties.\nFinally,we apply our proposed algorithm to classification problems involving\ntheoptimization of support-vector machine classifiers.\n",
        "published": "2019",
        "authors": [
            "Sujit Pramod Khanna",
            "Alexander Ororbia II"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.15691v2",
        "title": "Deep Multiagent Reinforcement Learning: Challenges and Directions",
        "abstract": "  This paper surveys the field of deep multiagent reinforcement learning. The\ncombination of deep neural networks with reinforcement learning has gained\nincreased traction in recent years and is slowly shifting the focus from\nsingle-agent to multiagent environments. Dealing with multiple agents is\ninherently more complex as (a) the future rewards depend on multiple players'\njoint actions and (b) the computational complexity increases. We present the\nmost common multiagent problem representations and their main challenges, and\nidentify five research areas that address one or more of these challenges:\ncentralised training and decentralised execution, opponent modelling,\ncommunication, efficient coordination, and reward shaping. We find that many\ncomputational studies rely on unrealistic assumptions or are not generalisable\nto other settings; they struggle to overcome the curse of dimensionality or\nnonstationarity. Approaches from psychology and sociology capture promising\nrelevant behaviours, such as communication and coordination, to help agents\nachieve better performance in multiagent settings. We suggest that, for\nmultiagent reinforcement learning to be successful, future research should\naddress these challenges with an interdisciplinary approach to open up new\npossibilities in multiagent reinforcement learning.\n",
        "published": "2021",
        "authors": [
            "Annie Wong",
            "Thomas B\u00e4ck",
            "Anna V. Kononova",
            "Aske Plaat"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.06855v2",
        "title": "DIAS: A Domain-Independent Alife-Based Problem-Solving System",
        "abstract": "  A domain-independent problem-solving system based on principles of Artificial\nLife is introduced. In this system, DIAS, the input and output dimensions of\nthe domain are laid out in a spatial medium. A population of actors, each\nseeing only part of this medium, solves problems collectively in it. The\nprocess is independent of the domain and can be implemented through different\nkinds of actors. Through a set of experiments on various problem domains, DIAS\nis shown able to solve problems with different dimensionality and complexity,\nto require no hyperparameter tuning for new problems, and to exhibit lifelong\nlearning, i.e. adapt rapidly to run-time changes in the problem domain, and do\nit better than a standard non-collective approach. DIAS therefore demonstrates\na role for Alife in building scalable, general, and adaptive problem-solving\nsystems.\n",
        "published": "2022",
        "authors": [
            "Babak Hodjat",
            "Hormoz Shahrzad",
            "Risto Miikkulainen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.04957v1",
        "title": "Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution",
        "abstract": "  Generating agents that can achieve Zero-Shot Coordination (ZSC) with unseen\npartners is a new challenge in cooperative Multi-Agent Reinforcement Learning\n(MARL). Recently, some studies have made progress in ZSC by exposing the agents\nto diverse partners during the training process. They usually involve self-play\nwhen training the partners, implicitly assuming that the tasks are homogeneous.\nHowever, many real-world tasks are heterogeneous, and hence previous methods\nmay fail. In this paper, we study the heterogeneous ZSC problem for the first\ntime and propose a general method based on coevolution, which coevolves two\npopulations of agents and partners through three sub-processes: pairing,\nupdating and selection. Experimental results on a collaborative cooking task\nshow the necessity of considering the heterogeneous setting and illustrate that\nour proposed method is a promising solution for heterogeneous cooperative MARL.\n",
        "published": "2022",
        "authors": [
            "Ke Xue",
            "Yutong Wang",
            "Lei Yuan",
            "Cong Guan",
            "Chao Qian",
            "Yang Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.06835v1",
        "title": "Multi-agent Dynamic Algorithm Configuration",
        "abstract": "  Automated algorithm configuration relieves users from tedious,\ntrial-and-error tuning tasks. A popular algorithm configuration tuning paradigm\nis dynamic algorithm configuration (DAC), in which an agent learns dynamic\nconfiguration policies across instances by reinforcement learning (RL).\nHowever, in many complex algorithms, there may exist different types of\nconfiguration hyperparameters, and such heterogeneity may bring difficulties\nfor classic DAC which uses a single-agent RL policy. In this paper, we aim to\naddress this issue and propose multi-agent DAC (MA-DAC), with one agent working\nfor one type of configuration hyperparameter. MA-DAC formulates the dynamic\nconfiguration of a complex algorithm with multiple types of hyperparameters as\na contextual multi-agent Markov decision process and solves it by a cooperative\nmulti-agent RL (MARL) algorithm. To instantiate, we apply MA-DAC to a\nwell-known optimization algorithm for multi-objective optimization problems.\nExperimental results show the effectiveness of MA-DAC in not only achieving\nsuperior performance compared with other configuration tuning approaches based\non heuristic rules, multi-armed bandits, and single-agent RL, but also being\ncapable of generalizing to different problem classes. Furthermore, we release\nthe environments in this paper as a benchmark for testing MARL algorithms, with\nthe hope of facilitating the application of MARL.\n",
        "published": "2022",
        "authors": [
            "Ke Xue",
            "Jiacheng Xu",
            "Lei Yuan",
            "Miqing Li",
            "Chao Qian",
            "Zongzhang Zhang",
            "Yang Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.10640v2",
        "title": "Evolving Strategies for Competitive Multi-Agent Search",
        "abstract": "  While evolutionary computation is well suited for automatic discovery in\nengineering, it can also be used to gain insight into how humans and\norganizations could perform more effectively. Using a real-world problem of\ninnovation search in organizations as the motivating example, this article\nfirst formalizes human creative problem solving as competitive multi-agent\nsearch (CMAS). CMAS is different from existing single-agent and team search\nproblems in that the agents interact through knowledge of other agents'\nsearches and through the dynamic changes in the search landscape that result\nfrom these searches. The main hypothesis is that evolutionary computation can\nbe used to discover effective strategies for CMAS; this hypothesis is verified\nin a series of experiments on the NK model, i.e.\\ partially correlated and\ntunably rugged fitness landscapes. Different specialized strategies are evolved\nfor each different competitive environment, and also general strategies that\nperform well across environments. These strategies are more effective and more\ncomplex than hand-designed strategies and a strategy based on traditional tree\nsearch. Using a novel spherical visualization of such landscapes, insight is\ngained about how successful strategies work, e.g.\\ by tracking positive changes\nin the landscape. The article thus provides a possible framework for studying\nvarious human creative activities as competitive multi-agent search in the\nfuture.\n",
        "published": "2023",
        "authors": [
            "Erkin Bahceci",
            "Riitta Katila",
            "Risto Miikkulainen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.01144v1",
        "title": "Self-Organization and Artificial Life: A Review",
        "abstract": "  Self-organization has been an important concept within a number of\ndisciplines, which Artificial Life (ALife) also has heavily utilized since its\ninception. The term and its implications, however, are often confusing or\nmisinterpreted. In this work, we provide a mini-review of self-organization and\nits relationship with ALife, aiming at initiating discussions on this important\ntopic with the interested audience. We first articulate some fundamental\naspects of self-organization, outline its usage, and review its applications to\nALife within its soft, hard, and wet domains. We also provide perspectives for\nfurther research.\n",
        "published": "2018",
        "authors": [
            "Carlos Gershenson",
            "Vito Trianni",
            "Justin Werfel",
            "Hiroki Sayama"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.01385v1",
        "title": "Embodied Multimodal Multitask Learning",
        "abstract": "  Recent efforts on training visual navigation agents conditioned on language\nusing deep reinforcement learning have been successful in learning policies for\ndifferent multimodal tasks, such as semantic goal navigation and embodied\nquestion answering. In this paper, we propose a multitask model capable of\njointly learning these multimodal tasks, and transferring knowledge of words\nand their grounding in visual objects across the tasks. The proposed model uses\na novel Dual-Attention unit to disentangle the knowledge of words in the\ntextual representations and visual concepts in the visual representations, and\nalign them with each other. This disentangled task-invariant alignment of\nrepresentations facilitates grounding and knowledge transfer across both tasks.\nWe show that the proposed model outperforms a range of baselines on both tasks\nin simulated 3D environments. We also show that this disentanglement of\nrepresentations makes our model modular, interpretable, and allows for transfer\nto instructions containing new words by leveraging object detectors.\n",
        "published": "2019",
        "authors": [
            "Devendra Singh Chaplot",
            "Lisa Lee",
            "Ruslan Salakhutdinov",
            "Devi Parikh",
            "Dhruv Batra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/cs/0502086v1",
        "title": "The Self-Organization of Speech Sounds",
        "abstract": "  The speech code is a vehicle of language: it defines a set of forms used by a\ncommunity to carry information. Such a code is necessary to support the\nlinguistic interactions that allow humans to communicate. How then may a speech\ncode be formed prior to the existence of linguistic interactions? Moreover, the\nhuman speech code is discrete and compositional, shared by all the individuals\nof a community but different across communities, and phoneme inventories are\ncharacterized by statistical regularities. How can a speech code with these\nproperties form? We try to approach these questions in the paper, using the\n\"methodology of the artificial\". We build a society of artificial agents, and\ndetail a mechanism that shows the formation of a discrete speech code without\npre-supposing the existence of linguistic capacities or of coordinated\ninteractions. The mechanism is based on a low-level model of sensory-motor\ninteractions. We show that the integration of certain very simple and non\nlanguage-specific neural devices leads to the formation of a speech code that\nhas properties similar to the human speech code. This result relies on the\nself-organizing properties of a generic coupling between perception and\nproduction within agents, and on the interactions between agents. The\nartificial system helps us to develop better intuitions on how speech might\nhave appeared, by showing how self-organization might have helped natural\nselection to find speech.\n",
        "published": "2005",
        "authors": [
            "Pierre-Yves Oudeyer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.01215v4",
        "title": "ES-MAML: Simple Hessian-Free Meta Learning",
        "abstract": "  We introduce ES-MAML, a new framework for solving the model agnostic meta\nlearning (MAML) problem based on Evolution Strategies (ES). Existing algorithms\nfor MAML are based on policy gradients, and incur significant difficulties when\nattempting to estimate second derivatives using backpropagation on stochastic\npolicies. We show how ES can be applied to MAML to obtain an algorithm which\navoids the problem of estimating second derivatives, and is also conceptually\nsimple and easy to implement. Moreover, ES-MAML can handle new types of\nnonsmooth adaptation operators, and other techniques for improving performance\nand estimation of ES methods become applicable. We show empirically that\nES-MAML is competitive with existing methods and often yields better adaptation\nwith fewer queries.\n",
        "published": "2019",
        "authors": [
            "Xingyou Song",
            "Wenbo Gao",
            "Yuxiang Yang",
            "Krzysztof Choromanski",
            "Aldo Pacchiano",
            "Yunhao Tang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.04155v4",
        "title": "Vision-based Navigation with Language-based Assistance via Imitation\n  Learning with Indirect Intervention",
        "abstract": "  We present Vision-based Navigation with Language-based Assistance (VNLA), a\ngrounded vision-language task where an agent with visual perception is guided\nvia language to find objects in photorealistic indoor environments. The task\nemulates a real-world scenario in that (a) the requester may not know how to\nnavigate to the target objects and thus makes requests by only specifying\nhigh-level end-goals, and (b) the agent is capable of sensing when it is lost\nand querying an advisor, who is more qualified at the task, to obtain language\nsubgoals to make progress. To model language-based assistance, we develop a\ngeneral framework termed Imitation Learning with Indirect Intervention (I3L),\nand propose a solution that is effective on the VNLA task. Empirical results\nshow that this approach significantly improves the success rate of the learning\nagent over other baselines in both seen and unseen environments. Our code and\ndata are publicly available at https://github.com/debadeepta/vnla .\n",
        "published": "2018",
        "authors": [
            "Khanh Nguyen",
            "Debadeepta Dey",
            "Chris Brockett",
            "Bill Dolan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.01828v3",
        "title": "Emergent Discrete Communication in Semantic Spaces",
        "abstract": "  Neural agents trained in reinforcement learning settings can learn to\ncommunicate among themselves via discrete tokens, accomplishing as a team what\nagents would be unable to do alone. However, the current standard of using\none-hot vectors as discrete communication tokens prevents agents from acquiring\nmore desirable aspects of communication such as zero-shot understanding.\nInspired by word embedding techniques from natural language processing, we\npropose neural agent architectures that enables them to communicate via\ndiscrete tokens derived from a learned, continuous space. We show in a decision\ntheoretic framework that our technique optimizes communication over a wide\nrange of scenarios, whereas one-hot tokens are only optimal under restrictive\nassumptions. In self-play experiments, we validate that our trained agents\nlearn to cluster tokens in semantically-meaningful ways, allowing them\ncommunicate in noisy environments where other techniques fail. Lastly, we\ndemonstrate both that agents using our method can effectively respond to novel\nhuman communication and that humans can understand unlabeled emergent agent\ncommunication, outperforming the use of one-hot communication.\n",
        "published": "2021",
        "authors": [
            "Mycal Tucker",
            "Huao Li",
            "Siddharth Agrawal",
            "Dana Hughes",
            "Katia Sycara",
            "Michael Lewis",
            "Julie Shah"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.10110v1",
        "title": "Towards Distraction-Robust Active Visual Tracking",
        "abstract": "  In active visual tracking, it is notoriously difficult when distracting\nobjects appear, as distractors often mislead the tracker by occluding the\ntarget or bringing a confusing appearance. To address this issue, we propose a\nmixed cooperative-competitive multi-agent game, where a target and multiple\ndistractors form a collaborative team to play against a tracker and make it\nfail to follow. Through learning in our game, diverse distracting behaviors of\nthe distractors naturally emerge, thereby exposing the tracker's weakness,\nwhich helps enhance the distraction-robustness of the tracker. For effective\nlearning, we then present a bunch of practical methods, including a reward\nfunction for distractors, a cross-modal teacher-student learning strategy, and\na recurrent attention mechanism for the tracker. The experimental results show\nthat our tracker performs desired distraction-robust active visual tracking and\ncan be well generalized to unseen environments. We also show that the\nmulti-agent game can be used to adversarially test the robustness of trackers.\n",
        "published": "2021",
        "authors": [
            "Fangwei Zhong",
            "Peng Sun",
            "Wenhan Luo",
            "Tingyun Yan",
            "Yizhou Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.09495v3",
        "title": "Among Us: Adversarially Robust Collaborative Perception by Consensus",
        "abstract": "  Multiple robots could perceive a scene (e.g., detect objects) collaboratively\nbetter than individuals, although easily suffer from adversarial attacks when\nusing deep learning. This could be addressed by the adversarial defense, but\nits training requires the often-unknown attacking mechanism. Differently, we\npropose ROBOSAC, a novel sampling-based defense strategy generalizable to\nunseen attackers. Our key idea is that collaborative perception should lead to\nconsensus rather than dissensus in results compared to individual perception.\nThis leads to our hypothesize-and-verify framework: perception results with and\nwithout collaboration from a random subset of teammates are compared until\nreaching a consensus. In such a framework, more teammates in the sampled subset\noften entail better perception performance but require longer sampling time to\nreject potential attackers. Thus, we derive how many sampling trials are needed\nto ensure the desired size of an attacker-free subset, or equivalently, the\nmaximum size of such a subset that we can successfully sample within a given\nnumber of trials. We validate our method on the task of collaborative 3D object\ndetection in autonomous driving scenarios.\n",
        "published": "2023",
        "authors": [
            "Yiming Li",
            "Qi Fang",
            "Jiamu Bai",
            "Siheng Chen",
            "Felix Juefei-Xu",
            "Chen Feng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.15824v1",
        "title": "CLAS: Coordinating Multi-Robot Manipulation with Central Latent Action\n  Spaces",
        "abstract": "  Multi-robot manipulation tasks involve various control entities that can be\nseparated into dynamically independent parts. A typical example of such\nreal-world tasks is dual-arm manipulation. Learning to naively solve such tasks\nwith reinforcement learning is often unfeasible due to the sample complexity\nand exploration requirements growing with the dimensionality of the action and\nstate spaces. Instead, we would like to handle such environments as multi-agent\nsystems and have several agents control parts of the whole. However,\ndecentralizing the generation of actions requires coordination across agents\nthrough a channel limited to information central to the task. This paper\nproposes an approach to coordinating multi-robot manipulation through learned\nlatent action spaces that are shared across different agents. We validate our\nmethod in simulated multi-robot manipulation tasks and demonstrate improvement\nover previous baselines in terms of sample efficiency and learning performance.\n",
        "published": "2022",
        "authors": [
            "Elie Aljalbout",
            "Maximilian Karl",
            "Patrick van der Smagt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.09055v1",
        "title": "Interactive Robot Learning of Gestures, Language and Affordances",
        "abstract": "  A growing field in robotics and Artificial Intelligence (AI) research is\nhuman-robot collaboration, whose target is to enable effective teamwork between\nhumans and robots. However, in many situations human teams are still superior\nto human-robot teams, primarily because human teams can easily agree on a\ncommon goal with language, and the individual members observe each other\neffectively, leveraging their shared motor repertoire and sensorimotor\nresources. This paper shows that for cognitive robots it is possible, and\nindeed fruitful, to combine knowledge acquired from interacting with elements\nof the environment (affordance exploration) with the probabilistic observation\nof another agent's actions.\n  We propose a model that unites (i) learning robot affordances and word\ndescriptions with (ii) statistical recognition of human gestures with vision\nsensors. We discuss theoretical motivations, possible implementations, and we\nshow initial results which highlight that, after having acquired knowledge of\nits surrounding environment, a humanoid robot can generalize this knowledge to\nthe case when it observes another agent (human partner) performing the same\nmotor actions previously executed during training.\n",
        "published": "2017",
        "authors": [
            "Giovanni Saponaro",
            "Lorenzo Jamone",
            "Alexandre Bernardino",
            "Giampiero Salvi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.00047v1",
        "title": "Following High-level Navigation Instructions on a Simulated Quadcopter\n  with Imitation Learning",
        "abstract": "  We introduce a method for following high-level navigation instructions by\nmapping directly from images, instructions and pose estimates to continuous\nlow-level velocity commands for real-time control. The Grounded Semantic\nMapping Network (GSMN) is a fully-differentiable neural network architecture\nthat builds an explicit semantic map in the world reference frame by\nincorporating a pinhole camera projection model within the network. The\ninformation stored in the map is learned from experience, while the\nlocal-to-world transformation is computed explicitly. We train the model using\nDAggerFM, a modified variant of DAgger that trades tabular convergence\nguarantees for improved training speed and memory use. We test GSMN in virtual\nenvironments on a realistic quadcopter simulator and show that incorporating an\nexplicit mapping and grounding modules allows GSMN to outperform strong neural\nbaselines and almost reach an expert policy performance. Finally, we analyze\nthe learned map representations and show that using an explicit map leads to an\ninterpretable instruction-following model.\n",
        "published": "2018",
        "authors": [
            "Valts Blukis",
            "Nataly Brukhim",
            "Andrew Bennett",
            "Ross A. Knepper",
            "Yoav Artzi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.04179v2",
        "title": "Mapping Navigation Instructions to Continuous Control Actions with\n  Position-Visitation Prediction",
        "abstract": "  We propose an approach for mapping natural language instructions and raw\nobservations to continuous control of a quadcopter drone. Our model predicts\ninterpretable position-visitation distributions indicating where the agent\nshould go during execution and where it should stop, and uses the predicted\ndistributions to select the actions to execute. This two-step model\ndecomposition allows for simple and efficient training using a combination of\nsupervised learning and imitation learning. We evaluate our approach with a\nrealistic drone simulator, and demonstrate absolute task-completion accuracy\nimprovements of 16.85% over two state-of-the-art instruction-following methods.\n",
        "published": "2018",
        "authors": [
            "Valts Blukis",
            "Dipendra Misra",
            "Ross A. Knepper",
            "Yoav Artzi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.09705v1",
        "title": "Beyond the Self: Using Grounded Affordances to Interpret and Describe\n  Others' Actions",
        "abstract": "  We propose a developmental approach that allows a robot to interpret and\ndescribe the actions of human agents by reusing previous experience. The robot\nfirst learns the association between words and object affordances by\nmanipulating the objects in its environment. It then uses this information to\nlearn a mapping between its own actions and those performed by a human in a\nshared environment. It finally fuses the information from these two models to\ninterpret and describe human actions in light of its own experience. In our\nexperiments, we show that the model can be used flexibly to do inference on\ndifferent aspects of the scene. We can predict the effects of an action on the\nbasis of object properties. We can revise the belief that a certain action\noccurred, given the observed effects of the human action. In an early action\nrecognition fashion, we can anticipate the effects when the action has only\nbeen partially observed. By estimating the probability of words given the\nevidence and feeding them into a pre-defined grammar, we can generate relevant\ndescriptions of the scene. We believe that this is a step towards providing\nrobots with the fundamental skills to engage in social collaboration with\nhumans.\n",
        "published": "2019",
        "authors": [
            "Giovanni Saponaro",
            "Lorenzo Jamone",
            "Alexandre Bernardino",
            "Giampiero Salvi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.09664v1",
        "title": "Learning to Map Natural Language Instructions to Physical Quadcopter\n  Control using Simulated Flight",
        "abstract": "  We propose a joint simulation and real-world learning framework for mapping\nnavigation instructions and raw first-person observations to continuous\ncontrol. Our model estimates the need for environment exploration, predicts the\nlikelihood of visiting environment positions during execution, and controls the\nagent to both explore and visit high-likelihood positions. We introduce\nSupervised Reinforcement Asynchronous Learning (SuReAL). Learning uses both\nsimulation and real environments without requiring autonomous flight in the\nphysical environment during training, and combines supervised learning for\npredicting positions to visit and reinforcement learning for continuous\ncontrol. We evaluate our approach on a natural language instruction-following\ntask with a physical quadcopter, and demonstrate effective execution and\nexploration behavior.\n",
        "published": "2019",
        "authors": [
            "Valts Blukis",
            "Yannick Terme",
            "Eyvind Niklasson",
            "Ross A. Knepper",
            "Yoav Artzi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.02172v1",
        "title": "AAAI-2019 Workshop on Games and Simulations for Artificial Intelligence",
        "abstract": "  This volume represents the accepted submissions from the AAAI-2019 Workshop\non Games and Simulations for Artificial Intelligence held on January 29, 2019\nin Honolulu, Hawaii, USA. https://www.gamesim.ai\n",
        "published": "2019",
        "authors": [
            "Marwan Mattar",
            "Roozbeh Mottaghi",
            "Julian Togelius",
            "Danny Lange"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.01734v2",
        "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday\n  Tasks",
        "abstract": "  We present ALFRED (Action Learning From Realistic Environments and\nDirectives), a benchmark for learning a mapping from natural language\ninstructions and egocentric vision to sequences of actions for household tasks.\nALFRED includes long, compositional tasks with non-reversible state changes to\nshrink the gap between research benchmarks and real-world applications. ALFRED\nconsists of expert demonstrations in interactive visual environments for 25k\nnatural language directives. These directives contain both high-level goals\nlike \"Rinse off a mug and place it in the coffee maker.\" and low-level language\ninstructions like \"Walk to the coffee maker on the right.\" ALFRED tasks are\nmore complex in terms of sequence length, action space, and language than\nexisting vision-and-language task datasets. We show that a baseline model based\non recent embodied vision-and-language tasks performs poorly on ALFRED,\nsuggesting that there is significant room for developing innovative grounded\nvisual language understanding models with this benchmark.\n",
        "published": "2019",
        "authors": [
            "Mohit Shridhar",
            "Jesse Thomason",
            "Daniel Gordon",
            "Yonatan Bisk",
            "Winson Han",
            "Roozbeh Mottaghi",
            "Luke Zettlemoyer",
            "Dieter Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.03768v2",
        "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive\n  Learning",
        "abstract": "  Given a simple request like Put a washed apple in the kitchen fridge, humans\ncan reason in purely abstract terms by imagining action sequences and scoring\ntheir likelihood of success, prototypicality, and efficiency, all without\nmoving a muscle. Once we see the kitchen in question, we can update our\nabstract plans to fit the scene. Embodied agents require the same abilities,\nbut existing work does not yet provide the infrastructure necessary for both\nreasoning abstractly and executing concretely. We address this limitation by\nintroducing ALFWorld, a simulator that enables agents to learn abstract, text\nbased policies in TextWorld (C\\^ot\\'e et al., 2018) and then execute goals from\nthe ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment.\nALFWorld enables the creation of a new BUTLER agent whose abstract knowledge,\nlearned in TextWorld, corresponds directly to concrete, visually grounded\nactions. In turn, as we demonstrate empirically, this fosters better agent\ngeneralization than training only in the visually grounded environment.\nBUTLER's simple, modular design factors the problem to allow researchers to\nfocus on models for improving every piece of the pipeline (language\nunderstanding, planning, navigation, and visual scene understanding).\n",
        "published": "2020",
        "authors": [
            "Mohit Shridhar",
            "Xingdi Yuan",
            "Marc-Alexandre C\u00f4t\u00e9",
            "Yonatan Bisk",
            "Adam Trischler",
            "Matthew Hausknecht"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.11589v2",
        "title": "VISITRON: Visual Semantics-Aligned Interactively Trained\n  Object-Navigator",
        "abstract": "  Interactive robots navigating photo-realistic environments need to be trained\nto effectively leverage and handle the dynamic nature of dialogue in addition\nto the challenges underlying vision-and-language navigation (VLN). In this\npaper, we present VISITRON, a multi-modal Transformer-based navigator better\nsuited to the interactive regime inherent to Cooperative Vision-and-Dialog\nNavigation (CVDN). VISITRON is trained to: i) identify and associate\nobject-level concepts and semantics between the environment and dialogue\nhistory, ii) identify when to interact vs. navigate via imitation learning of a\nbinary classification head. We perform extensive pre-training and fine-tuning\nablations with VISITRON to gain empirical insights and improve performance on\nCVDN. VISITRON's ability to identify when to interact leads to a natural\ngeneralization of the game-play mode introduced by Roman et al.\n(arXiv:2005.00728) for enabling the use of such models in different\nenvironments. VISITRON is competitive with models on the static CVDN\nleaderboard and attains state-of-the-art performance on the Success weighted by\nPath Length (SPL) metric.\n",
        "published": "2021",
        "authors": [
            "Ayush Shrivastava",
            "Karthik Gopalakrishnan",
            "Yang Liu",
            "Robinson Piramuthu",
            "Gokhan T\u00fcr",
            "Devi Parikh",
            "Dilek Hakkani-T\u00fcr"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.05612v3",
        "title": "A Persistent Spatial Semantic Representation for High-level Natural\n  Language Instruction Execution",
        "abstract": "  Natural language provides an accessible and expressive interface to specify\nlong-term tasks for robotic agents. However, non-experts are likely to specify\nsuch tasks with high-level instructions, which abstract over specific robot\nactions through several layers of abstraction. We propose that key to bridging\nthis gap between language and robot actions over long execution horizons are\npersistent representations. We propose a persistent spatial semantic\nrepresentation method, and show how it enables building an agent that performs\nhierarchical reasoning to effectively execute long-term tasks. We evaluate our\napproach on the ALFRED benchmark and achieve state-of-the-art results, despite\ncompletely avoiding the commonly used step-by-step instructions.\n",
        "published": "2021",
        "authors": [
            "Valts Blukis",
            "Chris Paxton",
            "Dieter Fox",
            "Animesh Garg",
            "Yoav Artzi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.12514v2",
        "title": "Language Grounding with 3D Objects",
        "abstract": "  Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" Flat images\nof candidate mice may not provide the discriminative information needed for\n\"wireless.\" The world, and objects in it, are not flat images but complex 3D\nshapes. If a human requests an object based on any of its basic properties,\nsuch as color, shape, or texture, robots should perform the necessary\nexploration to accomplish the task. In particular, while substantial effort and\nprogress has been made on understanding explicitly visual attributes like color\nand category, comparatively little progress has been made on understanding\nlanguage about shapes and contours. In this work, we introduce a novel\nreasoning task that targets both visual and non-visual language about 3D\nobjects. Our new benchmark, ShapeNet Annotated with Referring Expressions\n(SNARE) requires a model to choose which of two objects is being referenced by\na natural language description. We introduce several CLIP-based models for\ndistinguishing objects and demonstrate that while recent advances in jointly\nmodeling vision and language are useful for robotic language understanding, it\nis still the case that these image-based models are weaker at understanding the\n3D nature of objects -- properties which play a key role in manipulation. We\nfind that adding view estimation to language grounding models improves accuracy\non both SNARE and when identifying objects referred to in language on a robot\nplatform, but note that a large gap remains between these models and human\nperformance.\n",
        "published": "2021",
        "authors": [
            "Jesse Thomason",
            "Mohit Shridhar",
            "Yonatan Bisk",
            "Chris Paxton",
            "Luke Zettlemoyer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.07028v3",
        "title": "One Step at a Time: Long-Horizon Vision-and-Language Navigation with\n  Milestones",
        "abstract": "  We study the problem of developing autonomous agents that can follow human\ninstructions to infer and perform a sequence of actions to complete the\nunderlying task. Significant progress has been made in recent years, especially\nfor tasks with short horizons. However, when it comes to long-horizon tasks\nwith extended sequences of actions, an agent can easily ignore some\ninstructions or get stuck in the middle of the long instructions and eventually\nfail the task. To address this challenge, we propose a model-agnostic\nmilestone-based task tracker (M-TRACK) to guide the agent and monitor its\nprogress. Specifically, we propose a milestone builder that tags the\ninstructions with navigation and interaction milestones which the agent needs\nto complete step by step, and a milestone checker that systemically checks the\nagent's progress in its current milestone and determines when to proceed to the\nnext. On the challenging ALFRED dataset, our M-TRACK leads to a notable 33% and\n52% relative improvement in unseen success rate over two competitive base\nmodels.\n",
        "published": "2022",
        "authors": [
            "Chan Hee Song",
            "Jihyung Kil",
            "Tai-Yu Pan",
            "Brian M. Sadler",
            "Wei-Lun Chao",
            "Yu Su"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.01201v2",
        "title": "Habitat: A Platform for Embodied AI Research",
        "abstract": "  We present Habitat, a platform for research in embodied artificial\nintelligence (AI). Habitat enables training embodied agents (virtual robots) in\nhighly efficient photorealistic 3D simulation. Specifically, Habitat consists\nof: (i) Habitat-Sim: a flexible, high-performance 3D simulator with\nconfigurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is\nfast -- when rendering a scene from Matterport3D, it achieves several thousand\nframes per second (fps) running single-threaded, and can reach over 10,000 fps\nmulti-process on a single GPU. (ii) Habitat-API: a modular high-level library\nfor end-to-end development of embodied AI algorithms -- defining tasks (e.g.,\nnavigation, instruction following, question answering), configuring, training,\nand benchmarking embodied agents.\n  These large-scale engineering contributions enable us to answer scientific\nquestions requiring experiments that were till now impracticable or 'merely'\nimpractical. Specifically, in the context of point-goal navigation: (1) we\nrevisit the comparison between learning and SLAM approaches from two recent\nworks and find evidence for the opposite conclusion -- that learning\noutperforms SLAM if scaled to an order of magnitude more experience than\nprevious investigations, and (2) we conduct the first cross-dataset\ngeneralization experiments {train, test} x {Matterport3D, Gibson} for multiple\nsensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors\ngeneralize across datasets. We hope that our open-source platform and these\nfindings will advance research in embodied AI.\n",
        "published": "2019",
        "authors": [
            "Manolis Savva",
            "Abhishek Kadian",
            "Oleksandr Maksymets",
            "Yili Zhao",
            "Erik Wijmans",
            "Bhavana Jain",
            "Julian Straub",
            "Jia Liu",
            "Vladlen Koltun",
            "Jitendra Malik",
            "Devi Parikh",
            "Dhruv Batra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.00728v2",
        "title": "RMM: A Recursive Mental Model for Dialog Navigation",
        "abstract": "  Language-guided robots must be able to both ask humans questions and\nunderstand answers. Much existing work focuses only on the latter. In this\npaper, we go beyond instruction following and introduce a two-agent task where\none agent navigates and asks questions that a second, guiding agent answers.\nInspired by theory of mind, we propose the Recursive Mental Model (RMM). The\nnavigating agent models the guiding agent to simulate answers given candidate\ngenerated questions. The guiding agent in turn models the navigating agent to\nsimulate navigation steps it would take to generate answers. We use the\nprogress agents make towards the goal as a reinforcement learning reward signal\nto directly inform not only navigation actions, but also both question and\nanswer generation. We demonstrate that RMM enables better generalization to\nnovel environments. Interlocutor modelling may be a way forward for human-agent\ndialogue where robots need to both ask and answer questions.\n",
        "published": "2020",
        "authors": [
            "Homero Roman Roman",
            "Yonatan Bisk",
            "Jesse Thomason",
            "Asli Celikyilmaz",
            "Jianfeng Gao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.07384v1",
        "title": "Few-shot Object Grounding and Mapping for Natural Language Robot\n  Instruction Following",
        "abstract": "  We study the problem of learning a robot policy to follow natural language\ninstructions that can be easily extended to reason about new objects. We\nintroduce a few-shot language-conditioned object grounding method trained from\naugmented reality data that uses exemplars to identify objects and align them\nto their mentions in instructions. We present a learned map representation that\nencodes object locations and their instructed use, and construct it from our\nfew-shot grounding output. We integrate this mapping approach into an\ninstruction-following policy, thereby allowing it to reason about previously\nunseen objects at test-time by simply adding exemplars. We evaluate on the task\nof learning to map raw observations and instructions to continuous control of a\nphysical quadcopter. Our approach significantly outperforms the prior state of\nthe art in the presence of new objects, even when the prior approach observes\nall objects during training.\n",
        "published": "2020",
        "authors": [
            "Valts Blukis",
            "Ross A. Knepper",
            "Yoav Artzi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.13948v4",
        "title": "Core Challenges in Embodied Vision-Language Planning",
        "abstract": "  Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.\n",
        "published": "2021",
        "authors": [
            "Jonathan Francis",
            "Nariaki Kitamura",
            "Felix Labelle",
            "Xiaopeng Lu",
            "Ingrid Navarro",
            "Jean Oh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.01517v2",
        "title": "Skill Induction and Planning with Latent Language",
        "abstract": "  We present a framework for learning hierarchical policies from\ndemonstrations, using sparse natural language annotations to guide the\ndiscovery of reusable skills for autonomous decision-making. We formulate a\ngenerative model of action sequences in which goals generate sequences of\nhigh-level subtask descriptions, and these descriptions generate sequences of\nlow-level actions. We describe how to train this model using primarily\nunannotated demonstrations by parsing demonstrations into sequences of named\nhigh-level subtasks, using only a small number of seed annotations to ground\nlanguage in action. In trained models, natural language commands index a\ncombinatorial library of skills; agents can use these skills to plan by\ngenerating high-level instruction sequences tailored to novel goals. We\nevaluate this approach in the ALFRED household simulation environment,\nproviding natural language annotations for only 10% of demonstrations. It\nachieves task completion rates comparable to state-of-the-art models\n(outperforming several recent methods with access to ground-truth plans during\ntraining and evaluation) while providing structured and human-readable\nhigh-level plans.\n",
        "published": "2021",
        "authors": [
            "Pratyusha Sharma",
            "Antonio Torralba",
            "Jacob Andreas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.10189v1",
        "title": "StructFormer: Learning Spatial Structure for Language-Guided Semantic\n  Rearrangement of Novel Objects",
        "abstract": "  Geometric organization of objects into semantically meaningful arrangements\npervades the built world. As such, assistive robots operating in warehouses,\noffices, and homes would greatly benefit from the ability to recognize and\nrearrange objects into these semantically meaningful structures. To be useful,\nthese robots must contend with previously unseen objects and receive\ninstructions without significant programming. While previous works have\nexamined recognizing pairwise semantic relations and sequential manipulation to\nchange these simple relations none have shown the ability to arrange objects\ninto complex structures such as circles or table settings. To address this\nproblem we propose a novel transformer-based neural network, StructFormer,\nwhich takes as input a partial-view point cloud of the current object\narrangement and a structured language command encoding the desired object\nconfiguration. We show through rigorous experiments that StructFormer enables a\nphysical robot to rearrange novel objects into semantically meaningful\nstructures with multi-object relational constraints inferred from the language\ncommand.\n",
        "published": "2021",
        "authors": [
            "Weiyu Liu",
            "Chris Paxton",
            "Tucker Hermans",
            "Dieter Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.03227v4",
        "title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for\n  Long-Horizon Robot Manipulation Tasks",
        "abstract": "  General-purpose robots coexisting with humans in their environment must learn\nto relate human language to their perceptions and actions to be useful in a\nrange of daily tasks. Moreover, they need to acquire a diverse repertoire of\ngeneral-purpose skills that allow composing long-horizon tasks by following\nunconstrained language instructions. In this paper, we present CALVIN\n(Composing Actions from Language and Vision), an open-source simulated\nbenchmark to learn long-horizon language-conditioned tasks. Our aim is to make\nit possible to develop agents that can solve many robotic manipulation tasks\nover a long horizon, from onboard sensors, and specified only via human\nlanguage. CALVIN tasks are more complex in terms of sequence length, action\nspace, and language than existing vision-and-language task datasets and\nsupports flexible specification of sensor suites. We evaluate the agents in\nzero-shot to novel language instructions and to novel environments and objects.\nWe show that a baseline model based on multi-context imitation learning\nperforms poorly on CALVIN, suggesting that there is significant room for\ndeveloping innovative agents that learn to relate human language to their world\nmodels with this benchmark.\n",
        "published": "2021",
        "authors": [
            "Oier Mees",
            "Lukas Hermann",
            "Erick Rosete-Beas",
            "Wolfram Burgard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.07207v2",
        "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge\n  for Embodied Agents",
        "abstract": "  Can world knowledge learned by large language models (LLMs) be used to act in\ninteractive environments? In this paper, we investigate the possibility of\ngrounding high-level tasks, expressed in natural language (e.g. \"make\nbreakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While\nprior work focused on learning from explicit step-by-step examples of how to\nact, we surprisingly find that if pre-trained LMs are large enough and prompted\nappropriately, they can effectively decompose high-level tasks into mid-level\nplans without any further training. However, the plans produced naively by LLMs\noften cannot map precisely to admissible actions. We propose a procedure that\nconditions on existing demonstrations and semantically translates the plans to\nadmissible actions. Our evaluation in the recent VirtualHome environment shows\nthat the resulting method substantially improves executability over the LLM\nbaseline. The conducted human evaluation reveals a trade-off between\nexecutability and correctness but shows a promising sign towards extracting\nactionable knowledge from language models. Website at\nhttps://huangwl18.github.io/language-planner\n",
        "published": "2022",
        "authors": [
            "Wenlong Huang",
            "Pieter Abbeel",
            "Deepak Pathak",
            "Igor Mordatch"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.05186v1",
        "title": "Correcting Robot Plans with Natural Language Feedback",
        "abstract": "  When humans design cost or goal specifications for robots, they often produce\nspecifications that are ambiguous, underspecified, or beyond planners' ability\nto solve. In these cases, corrections provide a valuable tool for\nhuman-in-the-loop robot control. Corrections might take the form of new goal\nspecifications, new constraints (e.g. to avoid specific objects), or hints for\nplanning algorithms (e.g. to visit specific waypoints). Existing correction\nmethods (e.g. using a joystick or direct manipulation of an end effector)\nrequire full teleoperation or real-time interaction. In this paper, we explore\nnatural language as an expressive and flexible tool for robot correction. We\ndescribe how to map from natural language sentences to transformations of cost\nfunctions. We show that these transformations enable users to correct goals,\nupdate robot motions to accommodate additional user preferences, and recover\nfrom planning errors. These corrections can be leveraged to get 81% and 93%\nsuccess rates on tasks where the original planner failed, with either one or\ntwo language corrections. Our method makes it possible to compose multiple\nconstraints and generalizes to unseen scenes, objects, and sentences in\nsimulated environments and real-world environments.\n",
        "published": "2022",
        "authors": [
            "Pratyusha Sharma",
            "Balakumar Sundaralingam",
            "Valts Blukis",
            "Chris Paxton",
            "Tucker Hermans",
            "Antonio Torralba",
            "Jacob Andreas",
            "Dieter Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.05608v1",
        "title": "Inner Monologue: Embodied Reasoning through Planning with Language\n  Models",
        "abstract": "  Recent works have shown how the reasoning capabilities of Large Language\nModels (LLMs) can be applied to domains beyond natural language processing,\nsuch as planning and interaction for robots. These embodied problems require an\nagent to understand many semantic aspects of the world: the repertoire of\nskills available, how these skills influence the world, and how changes to the\nworld map back to the language. LLMs planning in embodied environments need to\nconsider not just what skills to do, but also how and when to do them - answers\nthat change over time in response to the agent's own choices. In this work, we\ninvestigate to what extent LLMs used in such embodied contexts can reason over\nsources of feedback provided through natural language, without any additional\ntraining. We propose that by leveraging environment feedback, LLMs are able to\nform an inner monologue that allows them to more richly process and plan in\nrobotic control scenarios. We investigate a variety of sources of feedback,\nsuch as success detection, scene description, and human interaction. We find\nthat closed-loop language feedback significantly improves high-level\ninstruction completion on three domains, including simulated and real table top\nrearrangement tasks and long-horizon mobile manipulation tasks in a kitchen\nenvironment in the real world.\n",
        "published": "2022",
        "authors": [
            "Wenlong Huang",
            "Fei Xia",
            "Ted Xiao",
            "Harris Chan",
            "Jacky Liang",
            "Pete Florence",
            "Andy Zeng",
            "Jonathan Tompson",
            "Igor Mordatch",
            "Yevgen Chebotar",
            "Pierre Sermanet",
            "Noah Brown",
            "Tomas Jackson",
            "Linda Luu",
            "Sergey Levine",
            "Karol Hausman",
            "Brian Ichter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.02918v3",
        "title": "LATTE: LAnguage Trajectory TransformEr",
        "abstract": "  Natural language is one of the most intuitive ways to express human intent.\nHowever, translating instructions and commands towards robotic motion\ngeneration and deployment in the real world is far from being an easy task. The\nchallenge of combining a robot's inherent low-level geometric and kinodynamic\nconstraints with a human's high-level semantic instructions traditionally is\nsolved using task-specific solutions with little generalizability between\nhardware platforms, often with the use of static sets of target actions and\ncommands. This work instead proposes a flexible language-based framework that\nallows a user to modify generic robotic trajectories. Our method leverages\npre-trained language models (BERT and CLIP) to encode the user's intent and\ntarget objects directly from a free-form text input and scene images, fuses\ngeometrical features generated by a transformer encoder network, and finally\noutputs trajectories using a transformer decoder, without the need of priors\nrelated to the task or robot information. We significantly extend our own\nprevious work presented in Bucker et al. by expanding the trajectory\nparametrization space to 3D and velocity as opposed to just XY movements. In\naddition, we now train the model to use actual images of the objects in the\nscene for context (as opposed to textual descriptions), and we evaluate the\nsystem in a diverse set of scenarios beyond manipulation, such as aerial and\nlegged robots. Our simulated and real-life experiments demonstrate that our\ntransformer model can successfully follow human intent, modifying the shape and\nspeed of trajectories within multiple environments. Codebase available at:\nhttps://github.com/arthurfenderbucker/LaTTe-Language-Trajectory-TransformEr.git\n",
        "published": "2022",
        "authors": [
            "Arthur Bucker",
            "Luis Figueredo",
            "Sami Haddadin",
            "Ashish Kapoor",
            "Shuang Ma",
            "Sai Vemprala",
            "Rogerio Bonatti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.04899v3",
        "title": "Instruction-driven history-aware policies for robotic manipulations",
        "abstract": "  In human environments, robots are expected to accomplish a variety of\nmanipulation tasks given simple natural language instructions. Yet, robotic\nmanipulation is extremely challenging as it requires fine-grained motor\ncontrol, long-term memory as well as generalization to previously unseen tasks\nand environments. To address these challenges, we propose a unified\ntransformer-based approach that takes into account multiple inputs. In\nparticular, our transformer architecture integrates (i) natural language\ninstructions and (ii) multi-view scene observations while (iii) keeping track\nof the full history of observations and actions. Such an approach enables\nlearning dependencies between history and instructions and improves\nmanipulation precision using multiple views. We evaluate our method on the\nchallenging RLBench benchmark and on a real-world robot. Notably, our approach\nscales to 74 diverse RLBench tasks and outperforms the state of the art. We\nalso address instruction-conditioned tasks and demonstrate excellent\ngeneralization to previously unseen variations.\n",
        "published": "2022",
        "authors": [
            "Pierre-Louis Guhur",
            "Shizhe Chen",
            "Ricardo Garcia",
            "Makarand Tapaswi",
            "Ivan Laptev",
            "Cordelia Schmid"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.05451v2",
        "title": "Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation",
        "abstract": "  Transformers have revolutionized vision and natural language processing with\ntheir ability to scale with large datasets. But in robotic manipulation, data\nis both limited and expensive. Can manipulation still benefit from Transformers\nwith the right problem formulation? We investigate this question with PerAct, a\nlanguage-conditioned behavior-cloning agent for multi-task 6-DoF manipulation.\nPerAct encodes language goals and RGB-D voxel observations with a Perceiver\nTransformer, and outputs discretized actions by ``detecting the next best voxel\naction''. Unlike frameworks that operate on 2D images, the voxelized 3D\nobservation and action space provides a strong structural prior for efficiently\nlearning 6-DoF actions. With this formulation, we train a single multi-task\nTransformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks\n(with 18 variations) from just a few demonstrations per task. Our results show\nthat PerAct significantly outperforms unstructured image-to-action agents and\n3D ConvNet baselines for a wide range of tabletop tasks.\n",
        "published": "2022",
        "authors": [
            "Mohit Shridhar",
            "Lucas Manuelli",
            "Dieter Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.01911v3",
        "title": "Grounding Language with Visual Affordances over Unstructured Data",
        "abstract": "  Recent works have shown that Large Language Models (LLMs) can be applied to\nground natural language to a wide variety of robot skills. However, in\npractice, learning multi-task, language-conditioned robotic skills typically\nrequires large-scale data collection and frequent human intervention to reset\nthe environment or help correcting the current policies. In this work, we\npropose a novel approach to efficiently learn general-purpose\nlanguage-conditioned robot skills from unstructured, offline and reset-free\ndata in the real world by exploiting a self-supervised visuo-lingual affordance\nmodel, which requires annotating as little as 1% of the total data with\nlanguage. We evaluate our method in extensive experiments both in simulated and\nreal-world robotic tasks, achieving state-of-the-art performance on the\nchallenging CALVIN benchmark and learning over 25 distinct visuomotor\nmanipulation tasks with a single policy in the real world. We find that when\npaired with LLMs to break down abstract natural language instructions into\nsubgoals via few-shot prompting, our method is capable of completing\nlong-horizon, multi-tier tasks in the real world, while requiring an order of\nmagnitude less data than previous approaches. Code and videos are available at\nhttp://hulc2.cs.uni-freiburg.de\n",
        "published": "2022",
        "authors": [
            "Oier Mees",
            "Jessica Borja-Diaz",
            "Wolfram Burgard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.05714v4",
        "title": "Visual Language Maps for Robot Navigation",
        "abstract": "  Grounding language to the visual observations of a navigating agent can be\nperformed using off-the-shelf visual-language models pretrained on\nInternet-scale data (e.g., image captions). While this is useful for matching\nimages to natural language descriptions of object goals, it remains disjoint\nfrom the process of mapping the environment, so that it lacks the spatial\nprecision of classic geometric maps. To address this problem, we propose\nVLMaps, a spatial map representation that directly fuses pretrained\nvisual-language features with a 3D reconstruction of the physical world. VLMaps\ncan be autonomously built from video feed on robots using standard exploration\napproaches and enables natural language indexing of the map without additional\nlabeled data. Specifically, when combined with large language models (LLMs),\nVLMaps can be used to (i) translate natural language commands into a sequence\nof open-vocabulary navigation goals (which, beyond prior work, can be spatial\nby construction, e.g., \"in between the sofa and TV\" or \"three meters to the\nright of the chair\") directly localized in the map, and (ii) can be shared\namong multiple robots with different embodiments to generate new obstacle maps\non-the-fly (by using a list of obstacle categories). Extensive experiments\ncarried out in simulated and real world environments show that VLMaps enable\nnavigation according to more complex language instructions than existing\nmethods. Videos are available at https://vlmaps.github.io.\n",
        "published": "2022",
        "authors": [
            "Chenguang Huang",
            "Oier Mees",
            "Andy Zeng",
            "Wolfram Burgard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.04604v2",
        "title": "StructDiffusion: Language-Guided Creation of Physically-Valid Structures\n  using Unseen Objects",
        "abstract": "  Robots operating in human environments must be able to rearrange objects into\nsemantically-meaningful configurations, even if these objects are previously\nunseen. In this work, we focus on the problem of building physically-valid\nstructures without step-by-step instructions. We propose StructDiffusion, which\ncombines a diffusion model and an object-centric transformer to construct\nstructures given partial-view point clouds and high-level language goals, such\nas \"set the table\". Our method can perform multiple challenging\nlanguage-conditioned multi-step 3D planning tasks using one model.\nStructDiffusion even improves the success rate of assembling physically-valid\nstructures out of unseen objects by on average 16% over an existing multi-modal\ntransformer model trained on specific structures. We show experiments on\nheld-out objects in both simulation and on real-world rearrangement tasks.\nImportantly, we show how integrating both a diffusion model and a\ncollision-discriminator model allows for improved generalization over other\nmethods when rearranging previously-unseen objects. For videos and additional\nresults, see our website: https://structdiffusion.github.io/.\n",
        "published": "2022",
        "authors": [
            "Weiyu Liu",
            "Yilun Du",
            "Tucker Hermans",
            "Sonia Chernova",
            "Chris Paxton"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.04088v3",
        "title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large\n  Language Models",
        "abstract": "  This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner\n",
        "published": "2022",
        "authors": [
            "Chan Hee Song",
            "Jiaman Wu",
            "Clayton Washington",
            "Brian M. Sadler",
            "Wei-Lun Chao",
            "Yu Su"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.06817v2",
        "title": "RT-1: Robotics Transformer for Real-World Control at Scale",
        "abstract": "  By transferring knowledge from large, diverse, task-agnostic datasets, modern\nmachine learning models can solve specific downstream tasks either zero-shot or\nwith small task-specific datasets to a high level of performance. While this\ncapability has been demonstrated in other fields such as computer vision,\nnatural language processing or speech recognition, it remains to be shown in\nrobotics, where the generalization capabilities of the models are particularly\ncritical due to the difficulty of collecting real-world robotic data. We argue\nthat one of the keys to the success of such general robotic models lies with\nopen-ended task-agnostic training, combined with high-capacity architectures\nthat can absorb all of the diverse, robotic data. In this paper, we present a\nmodel class, dubbed Robotics Transformer, that exhibits promising scalable\nmodel properties. We verify our conclusions in a study of different model\nclasses and their ability to generalize as a function of the data size, model\nsize, and data diversity based on a large-scale data collection on real robots\nperforming real-world tasks. The project's website and videos can be found at\nrobotics-transformer1.github.io\n",
        "published": "2022",
        "authors": [
            "Anthony Brohan",
            "Noah Brown",
            "Justice Carbajal",
            "Yevgen Chebotar",
            "Joseph Dabis",
            "Chelsea Finn",
            "Keerthana Gopalakrishnan",
            "Karol Hausman",
            "Alex Herzog",
            "Jasmine Hsu",
            "Julian Ibarz",
            "Brian Ichter",
            "Alex Irpan",
            "Tomas Jackson",
            "Sally Jesmonth",
            "Nikhil J Joshi",
            "Ryan Julian",
            "Dmitry Kalashnikov",
            "Yuheng Kuang",
            "Isabel Leal",
            "Kuang-Huei Lee",
            "Sergey Levine",
            "Yao Lu",
            "Utsav Malla",
            "Deeksha Manjunath",
            "Igor Mordatch",
            "Ofir Nachum",
            "Carolina Parada",
            "Jodilyn Peralta",
            "Emily Perez",
            "Karl Pertsch",
            "Jornell Quiambao",
            "Kanishka Rao",
            "Michael Ryoo",
            "Grecia Salazar",
            "Pannag Sanketi",
            "Kevin Sayed",
            "Jaspiar Singh",
            "Sumedh Sontakke",
            "Austin Stone",
            "Clayton Tan",
            "Huong Tran",
            "Vincent Vanhoucke",
            "Steve Vega",
            "Quan Vuong",
            "Fei Xia",
            "Ted Xiao",
            "Peng Xu",
            "Sichun Xu",
            "Tianhe Yu",
            "Brianna Zitkovich"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.11550v1",
        "title": "Scaling Robot Learning with Semantically Imagined Experience",
        "abstract": "  Recent advances in robot learning have shown promise in enabling robots to\nperform a variety of manipulation tasks and generalize to novel scenarios. One\nof the key contributing factors to this progress is the scale of robot data\nused to train the models. To obtain large-scale datasets, prior approaches have\nrelied on either demonstrations requiring high human involvement or\nengineering-heavy autonomous data collection schemes, both of which are\nchallenging to scale. To mitigate this issue, we propose an alternative route\nand leverage text-to-image foundation models widely used in computer vision and\nnatural language processing to obtain meaningful data for robot learning\nwithout requiring additional robot data. We term our method Robot Learning with\nSemantically Imagened Experience (ROSIE). Specifically, we make use of the\nstate of the art text-to-image diffusion models and perform aggressive data\naugmentation on top of our existing robotic manipulation datasets via\ninpainting various unseen objects for manipulation, backgrounds, and\ndistractors with text guidance. Through extensive real-world experiments, we\nshow that manipulation policies trained on data augmented this way are able to\nsolve completely unseen tasks with new objects and can behave more robustly\nw.r.t. novel distractors. In addition, we find that we can improve the\nrobustness and generalization of high-level robot learning tasks such as\nsuccess detection through training with the diffusion-based data augmentation.\nThe project's website and videos can be found at diffusion-rosie.github.io\n",
        "published": "2023",
        "authors": [
            "Tianhe Yu",
            "Ted Xiao",
            "Austin Stone",
            "Jonathan Tompson",
            "Anthony Brohan",
            "Su Wang",
            "Jaspiar Singh",
            "Clayton Tan",
            "Dee M",
            "Jodilyn Peralta",
            "Brian Ichter",
            "Karol Hausman",
            "Fei Xia"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.12766v1",
        "title": "Language-Driven Representation Learning for Robotics",
        "abstract": "  Recent work in visual representation learning for robotics demonstrates the\nviability of learning from large video datasets of humans performing everyday\ntasks. Leveraging methods such as masked autoencoding and contrastive learning,\nthese representations exhibit strong transfer to policy learning for visuomotor\ncontrol. But, robot learning encompasses a diverse set of problems beyond\ncontrol including grasp affordance prediction, language-conditioned imitation\nlearning, and intent scoring for human-robot collaboration, amongst others.\nFirst, we demonstrate that existing representations yield inconsistent results\nacross these tasks: masked autoencoding approaches pick up on low-level spatial\nfeatures at the cost of high-level semantics, while contrastive learning\napproaches capture the opposite. We then introduce Voltron, a framework for\nlanguage-driven representation learning from human videos and associated\ncaptions. Voltron trades off language-conditioned visual reconstruction to\nlearn low-level visual patterns, and visually-grounded language generation to\nencode high-level semantics. We also construct a new evaluation suite spanning\nfive distinct robot learning problems $\\unicode{x2013}$ a unified platform for\nholistically evaluating visual representations for robotics. Through\ncomprehensive, controlled experiments across all five problems, we find that\nVoltron's language-driven representations outperform the prior\nstate-of-the-art, especially on targeted problems requiring higher-level\nfeatures.\n",
        "published": "2023",
        "authors": [
            "Siddharth Karamcheti",
            "Suraj Nair",
            "Annie S. Chen",
            "Thomas Kollar",
            "Chelsea Finn",
            "Dorsa Sadigh",
            "Percy Liang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.00855v2",
        "title": "Grounded Decoding: Guiding Text Generation with Grounded Models for\n  Embodied Agents",
        "abstract": "  Recent progress in large language models (LLMs) has demonstrated the ability\nto learn and leverage Internet-scale knowledge through pre-training with\nautoregressive models. Unfortunately, applying such models to settings with\nembodied agents, such as robots, is challenging due to their lack of experience\nwith the physical world, inability to parse non-language observations, and\nignorance of rewards or safety constraints that robots may require. On the\nother hand, language-conditioned robotic policies that learn from interaction\ndata can provide the necessary grounding that allows the agent to be correctly\nsituated in the real world, but such policies are limited by the lack of\nhigh-level semantic understanding due to the limited breadth of the interaction\ndata available for training them. Thus, if we want to make use of the semantic\nknowledge in a language model while still situating it in an embodied setting,\nwe must construct an action sequence that is both likely according to the\nlanguage model and also realizable according to grounded models of the\nenvironment. We frame this as a problem similar to probabilistic filtering:\ndecode a sequence that both has high probability under the language model and\nhigh probability under a set of grounded model objectives. We demonstrate how\nsuch grounded models can be obtained across three simulation and real-world\ndomains, and that the proposed decoding strategy is able to solve complex,\nlong-horizon embodiment tasks in a robotic setting by leveraging the knowledge\nof both models. The project's website can be found at\ngrounded-decoding.github.io.\n",
        "published": "2023",
        "authors": [
            "Wenlong Huang",
            "Fei Xia",
            "Dhruv Shah",
            "Danny Driess",
            "Andy Zeng",
            "Yao Lu",
            "Pete Florence",
            "Igor Mordatch",
            "Sergey Levine",
            "Karol Hausman",
            "Brian Ichter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.07522v2",
        "title": "Audio Visual Language Maps for Robot Navigation",
        "abstract": "  While interacting in the world is a multi-sensory experience, many robots\ncontinue to predominantly rely on visual perception to map and navigate in\ntheir environments. In this work, we propose Audio-Visual-Language Maps\n(AVLMaps), a unified 3D spatial map representation for storing cross-modal\ninformation from audio, visual, and language cues. AVLMaps integrate the\nopen-vocabulary capabilities of multimodal foundation models pre-trained on\nInternet-scale data by fusing their features into a centralized 3D voxel grid.\nIn the context of navigation, we show that AVLMaps enable robot systems to\nindex goals in the map based on multimodal queries, e.g., textual descriptions,\nimages, or audio snippets of landmarks. In particular, the addition of audio\ninformation enables robots to more reliably disambiguate goal locations.\nExtensive experiments in simulation show that AVLMaps enable zero-shot\nmultimodal goal navigation from multimodal prompts and provide 50% better\nrecall in ambiguous scenarios. These capabilities extend to mobile robots in\nthe real world - navigating to landmarks referring to visual, audio, and\nspatial concepts. Videos and code are available at: https://avlmaps.github.io.\n",
        "published": "2023",
        "authors": [
            "Chenguang Huang",
            "Oier Mees",
            "Andy Zeng",
            "Wolfram Burgard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.14391v3",
        "title": "Energy-based Models are Zero-Shot Planners for Compositional Scene\n  Rearrangement",
        "abstract": "  Language is compositional; an instruction can express multiple relation\nconstraints to hold among objects in a scene that a robot is tasked to\nrearrange. Our focus in this work is an instructable scene-rearranging\nframework that generalizes to longer instructions and to spatial concept\ncompositions never seen at training time. We propose to represent\nlanguage-instructed spatial concepts with energy functions over relative object\narrangements. A language parser maps instructions to corresponding energy\nfunctions and an open-vocabulary visual-language model grounds their arguments\nto relevant objects in the scene. We generate goal scene configurations by\ngradient descent on the sum of energy functions, one per language predicate in\nthe instruction. Local vision-based policies then re-locate objects to the\ninferred goal locations. We test our model on established instruction-guided\nmanipulation benchmarks, as well as benchmarks of compositional instructions we\nintroduce. We show our model can execute highly compositional instructions\nzero-shot in simulation and in the real world. It outperforms\nlanguage-to-action reactive policies and Large Language Model planners by a\nlarge margin, especially for long instructions that involve compositions of\nmultiple spatial concepts. Simulation and real-world robot execution videos, as\nwell as our code and datasets are publicly available on our website:\nhttps://ebmplanner.github.io.\n",
        "published": "2023",
        "authors": [
            "Nikolaos Gkanatsios",
            "Ayush Jain",
            "Zhou Xian",
            "Yunchu Zhang",
            "Christopher Atkeson",
            "Katerina Fragkiadaki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.05658v2",
        "title": "TidyBot: Personalized Robot Assistance with Large Language Models",
        "abstract": "  For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people's preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.\n",
        "published": "2023",
        "authors": [
            "Jimmy Wu",
            "Rika Antonova",
            "Adam Kan",
            "Marion Lepert",
            "Andy Zeng",
            "Shuran Song",
            "Jeannette Bohg",
            "Szymon Rusinkiewicz",
            "Thomas Funkhouser"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.11692v1",
        "title": "Surgical-VQLA: Transformer with Gated Vision-Language Embedding for\n  Visual Question Localized-Answering in Robotic Surgery",
        "abstract": "  Despite the availability of computer-aided simulators and recorded videos of\nsurgical procedures, junior residents still heavily rely on experts to answer\ntheir queries. However, expert surgeons are often overloaded with clinical and\nacademic workloads and limit their time in answering. For this purpose, we\ndevelop a surgical question-answering system to facilitate robot-assisted\nsurgical scene and activity understanding from recorded videos. Most of the\nexisting VQA methods require an object detector and regions based feature\nextractor to extract visual features and fuse them with the embedded text of\nthe question for answer generation. However, (1) surgical object detection\nmodel is scarce due to smaller datasets and lack of bounding box annotation;\n(2) current fusion strategy of heterogeneous modalities like text and image is\nnaive; (3) the localized answering is missing, which is crucial in complex\nsurgical scenarios. In this paper, we propose Visual Question\nLocalized-Answering in Robotic Surgery (Surgical-VQLA) to localize the specific\nsurgical area during the answer prediction. To deal with the fusion of the\nheterogeneous modalities, we design gated vision-language embedding (GVLE) to\nbuild input patches for the Language Vision Transformer (LViT) to predict the\nanswer. To get localization, we add the detection head in parallel with the\nprediction head of the LViT. We also integrate GIoU loss to boost localization\nperformance by preserving the accuracy of the question-answering model. We\nannotate two datasets of VQLA by utilizing publicly available surgical videos\nfrom MICCAI challenges EndoVis-17 and 18. Our validation results suggest that\nSurgical-VQLA can better understand the surgical scene and localize the\nspecific area related to the question-answering. GVLE presents an efficient\nlanguage-vision embedding technique by showing superior performance over the\nexisting benchmarks.\n",
        "published": "2023",
        "authors": [
            "Long Bai",
            "Mobarakol Islam",
            "Lalithkumar Seenivasan",
            "Hongliang Ren"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.05973v2",
        "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with\n  Language Models",
        "abstract": "  Large language models (LLMs) are shown to possess a wealth of actionable\nknowledge that can be extracted for robot manipulation in the form of reasoning\nand planning. Despite the progress, most still rely on pre-defined motion\nprimitives to carry out the physical interactions with the environment, which\nremains a major bottleneck. In this work, we aim to synthesize robot\ntrajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a\nlarge variety of manipulation tasks given an open-set of instructions and an\nopen-set of objects. We achieve this by first observing that LLMs excel at\ninferring affordances and constraints given a free-form language instruction.\nMore importantly, by leveraging their code-writing capabilities, they can\ninteract with a vision-language model (VLM) to compose 3D value maps to ground\nthe knowledge into the observation space of the agent. The composed value maps\nare then used in a model-based planning framework to zero-shot synthesize\nclosed-loop robot trajectories with robustness to dynamic perturbations. We\nfurther demonstrate how the proposed framework can benefit from online\nexperiences by efficiently learning a dynamics model for scenes that involve\ncontact-rich interactions. We present a large-scale study of the proposed\nmethod in both simulated and real-robot environments, showcasing the ability to\nperform a large variety of everyday manipulation tasks specified in free-form\nnatural language. Videos and code at https://voxposer.github.io\n",
        "published": "2023",
        "authors": [
            "Wenlong Huang",
            "Chen Wang",
            "Ruohan Zhang",
            "Yunzhu Li",
            "Jiajun Wu",
            "Li Fei-Fei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.12981v1",
        "title": "3D-LLM: Injecting the 3D World into Large Language Models",
        "abstract": "  Large language models (LLMs) and Vision-Language Models (VLMs) have been\nproven to excel at multiple tasks, such as commonsense reasoning. Powerful as\nthese models can be, they are not grounded in the 3D physical world, which\ninvolves richer concepts such as spatial relationships, affordances, physics,\nlayout, and so on. In this work, we propose to inject the 3D world into large\nlanguage models and introduce a whole new family of 3D-LLMs. Specifically,\n3D-LLMs can take 3D point clouds and their features as input and perform a\ndiverse set of 3D-related tasks, including captioning, dense captioning, 3D\nquestion answering, task decomposition, 3D grounding, 3D-assisted dialog,\nnavigation, and so on. Using three types of prompting mechanisms that we\ndesign, we are able to collect over 300k 3D-language data covering these tasks.\nTo efficiently train 3D-LLMs, we first utilize a 3D feature extractor that\nobtains 3D features from rendered multi- view images. Then, we use 2D VLMs as\nour backbones to train our 3D-LLMs. By introducing a 3D localization mechanism,\n3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show\nthat our model outperforms state-of-the-art baselines by a large margin (e.g.,\nthe BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore,\nexperiments on our held-in datasets for 3D captioning, task composition, and\n3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative\nexamples also show that our model could perform more tasks beyond the scope of\nexisting LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.\n",
        "published": "2023",
        "authors": [
            "Yining Hong",
            "Haoyu Zhen",
            "Peihao Chen",
            "Shuhong Zheng",
            "Yilun Du",
            "Zhenfang Chen",
            "Chuang Gan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.07931v2",
        "title": "Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation",
        "abstract": "  Self-supervised and language-supervised image models contain rich knowledge\nof the world that is important for generalization. Many robotic tasks, however,\nrequire a detailed understanding of 3D geometry, which is often lacking in 2D\nimage features. This work bridges this 2D-to-3D gap for robotic manipulation by\nleveraging distilled feature fields to combine accurate 3D geometry with rich\nsemantics from 2D foundation models. We present a few-shot learning method for\n6-DOF grasping and placing that harnesses these strong spatial and semantic\npriors to achieve in-the-wild generalization to unseen objects. Using features\ndistilled from a vision-language model, CLIP, we present a way to designate\nnovel objects for manipulation via free-text natural language, and demonstrate\nits ability to generalize to unseen expressions and novel categories of\nobjects.\n",
        "published": "2023",
        "authors": [
            "William Shen",
            "Ge Yang",
            "Alan Yu",
            "Jansen Wong",
            "Leslie Pack Kaelbling",
            "Phillip Isola"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.12311v1",
        "title": "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language\n  Model as an Agent",
        "abstract": "  3D visual grounding is a critical skill for household robots, enabling them\nto navigate, manipulate objects, and answer questions based on their\nenvironment. While existing approaches often rely on extensive labeled data or\nexhibit limitations in handling complex language queries, we propose\nLLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model\n(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to\ndecompose complex natural language queries into semantic constituents and\nemploys a visual grounding tool, such as OpenScene or LERF, to identify objects\nin a 3D scene. The LLM then evaluates the spatial and commonsense relations\namong the proposed objects to make a final grounding decision. Our method does\nnot require any labeled training data and can generalize to novel 3D scenes and\narbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and\ndemonstrate state-of-the-art zero-shot grounding accuracy. Our findings\nindicate that LLMs significantly improve the grounding capability, especially\nfor complex language queries, making LLM-Grounder an effective approach for 3D\nvision-language tasks in robotics. Videos and interactive demos can be found on\nthe project website https://chat-with-nerf.github.io/ .\n",
        "published": "2023",
        "authors": [
            "Jianing Yang",
            "Xuweiyi Chen",
            "Shengyi Qian",
            "Nikhil Madaan",
            "Madhavan Iyengar",
            "David F. Fouhey",
            "Joyce Chai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.03026v2",
        "title": "LanguageMPC: Large Language Models as Decision Makers for Autonomous\n  Driving",
        "abstract": "  Existing learning-based autonomous driving (AD) systems face challenges in\ncomprehending high-level information, generalizing to rare events, and\nproviding interpretability. To address these problems, this work employs Large\nLanguage Models (LLMs) as a decision-making component for complex AD scenarios\nthat require human commonsense understanding. We devise cognitive pathways to\nenable comprehensive reasoning with LLMs, and develop algorithms for\ntranslating LLM decisions into actionable driving commands. Through this\napproach, LLM decisions are seamlessly integrated with low-level controllers by\nguided parameter matrix adaptation. Extensive experiments demonstrate that our\nproposed method not only consistently surpasses baseline approaches in\nsingle-vehicle tasks, but also helps handle complex driving behaviors even\nmulti-vehicle coordination, thanks to the commonsense reasoning capabilities of\nLLMs. This paper presents an initial step toward leveraging LLMs as effective\ndecision-makers for intricate AD scenarios in terms of safety, efficiency,\ngeneralizability, and interoperability. We aspire for it to serve as\ninspiration for future research in this field. Project page:\nhttps://sites.google.com/view/llm-mpc\n",
        "published": "2023",
        "authors": [
            "Hao Sha",
            "Yao Mu",
            "Yuxuan Jiang",
            "Li Chen",
            "Chenfeng Xu",
            "Ping Luo",
            "Shengbo Eben Li",
            "Masayoshi Tomizuka",
            "Wei Zhan",
            "Mingyu Ding"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.15648v1",
        "title": "Reinforcement Learning from Diffusion Feedback: Q* for Image Search",
        "abstract": "  Large vision-language models are steadily gaining personalization\ncapabilities at the cost of fine-tuning or data augmentation. We present two\nmodels for image generation using model-agnostic learning that align semantic\npriors with generative capabilities. RLDF, or Reinforcement Learning from\nDiffusion Feedback, is a singular approach for visual imitation through\nprior-preserving reward function guidance. This employs Q-learning (with\nstandard Q*) for generation and follows a semantic-rewarded trajectory for\nimage search through finite encoding-tailored actions. The second proposed\nmethod, noisy diffusion gradient, is optimization driven. At the root of both\nmethods is a special CFG encoding that we propose for continual semantic\nguidance. Using only a single input image and no text input, RLDF generates\nhigh-quality images over varied domains including retail, sports and\nagriculture showcasing class-consistency and strong visual diversity. Project\nwebsite is available at https://infernolia.github.io/RLDF.\n",
        "published": "2023",
        "authors": [
            "Aboli Marathe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.17593v1",
        "title": "LanGWM: Language Grounded World Model",
        "abstract": "  Recent advances in deep reinforcement learning have showcased its potential\nin tackling complex tasks. However, experiments on visual control tasks have\nrevealed that state-of-the-art reinforcement learning models struggle with\nout-of-distribution generalization. Conversely, expressing higher-level\nconcepts and global contexts is relatively easy using language.\n  Building upon recent success of the large language models, our main objective\nis to improve the state abstraction technique in reinforcement learning by\nleveraging language for robust action selection. Specifically, we focus on\nlearning language-grounded visual features to enhance the world model learning,\na model-based reinforcement learning technique.\n  To enforce our hypothesis explicitly, we mask out the bounding boxes of a few\nobjects in the image observation and provide the text prompt as descriptions\nfor these masked objects. Subsequently, we predict the masked objects along\nwith the surrounding regions as pixel reconstruction, similar to the\ntransformer-based masked autoencoder approach.\n  Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art\nperformance in out-of-distribution test at the 100K interaction steps\nbenchmarks of iGibson point navigation tasks. Furthermore, our proposed\ntechnique of explicit language-grounded visual representation learning has the\npotential to improve models for human-robot interaction because our extracted\nvisual features are language grounded.\n",
        "published": "2023",
        "authors": [
            "Rudra P. K. Poudel",
            "Harit Pandya",
            "Chao Zhang",
            "Roberto Cipolla"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.17842v2",
        "title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic\n  Vision-Language Planning",
        "abstract": "  In this study, we are interested in imbuing robots with the capability of\nphysically-grounded task planning. Recent advancements have shown that large\nlanguage models (LLMs) possess extensive knowledge useful in robotic tasks,\nespecially in reasoning and planning. However, LLMs are constrained by their\nlack of world grounding and dependence on external affordance models to\nperceive environmental information, which cannot jointly reason with LLMs. We\nargue that a task planner should be an inherently grounded, unified multimodal\nsystem. To this end, we introduce Robotic Vision-Language Planning (ViLa), a\nnovel approach for long-horizon robotic planning that leverages vision-language\nmodels (VLMs) to generate a sequence of actionable steps. ViLa directly\nintegrates perceptual data into its reasoning and planning process, enabling a\nprofound understanding of commonsense knowledge in the visual world, including\nspatial layouts and object attributes. It also supports flexible multimodal\ngoal specification and naturally incorporates visual feedback. Our extensive\nevaluation, conducted in both real-robot and simulated environments,\ndemonstrates ViLa's superiority over existing LLM-based planners, highlighting\nits effectiveness in a wide array of open-world manipulation tasks.\n",
        "published": "2023",
        "authors": [
            "Yingdong Hu",
            "Fanqi Lin",
            "Tong Zhang",
            "Li Yi",
            "Yang Gao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.05230v1",
        "title": "Language Models, Agent Models, and World Models: The LAW for Machine\n  Reasoning and Planning",
        "abstract": "  Despite their tremendous success in many applications, large language models\noften fall short of consistent reasoning and planning in various (language,\nembodied, and social) scenarios, due to inherent limitations in their\ninference, learning, and modeling capabilities. In this position paper, we\npresent a new perspective of machine reasoning, LAW, that connects the concepts\nof Language models, Agent models, and World models, for more robust and\nversatile reasoning capabilities. In particular, we propose that world and\nagent models are a better abstraction of reasoning, that introduces the crucial\nelements of deliberate human-like reasoning, including beliefs about the world\nand other agents, anticipation of consequences, goals/rewards, and strategic\nplanning. Crucially, language models in LAW serve as a backend to implement the\nsystem or its elements and hence provide the computational power and\nadaptability. We review the recent studies that have made relevant progress and\ndiscuss future research directions towards operationalizing the LAW framework.\n",
        "published": "2023",
        "authors": [
            "Zhiting Hu",
            "Tianmin Shu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.08577v1",
        "title": "MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in\n  3D World",
        "abstract": "  Human beings possess the capability to multiply a melange of multisensory\ncues while actively exploring and interacting with the 3D world. Current\nmulti-modal large language models, however, passively absorb sensory data as\ninputs, lacking the capacity to actively interact with the objects in the 3D\nenvironment and dynamically collect their multisensory information. To usher in\nthe study of this area, we propose MultiPLY, a multisensory embodied large\nlanguage model that could incorporate multisensory interactive data, including\nvisual, audio, tactile, and thermal information into large language models,\nthereby establishing the correlation among words, actions, and percepts. To\nthis end, we first collect Multisensory Universe, a large-scale multisensory\ninteraction dataset comprising 500k data by deploying an LLM-powered embodied\nagent to engage with the 3D environment. To perform instruction tuning with\npre-trained LLM on such generated data, we first encode the 3D scene as\nabstracted object-centric representations and then introduce action tokens\ndenoting that the embodied agent takes certain actions within the environment,\nas well as state tokens that represent the multisensory state observations of\nthe agent at each time step. In the inference time, MultiPLY could generate\naction tokens, instructing the agent to take the action in the environment and\nobtain the next multisensory state observation. The observation is then\nappended back to the LLM via state tokens to generate subsequent text or action\ntokens. We demonstrate that MultiPLY outperforms baselines by a large margin\nthrough a diverse set of embodied tasks involving object retrieval, tool use,\nmultisensory captioning, and task decomposition.\n",
        "published": "2024",
        "authors": [
            "Yining Hong",
            "Zishuo Zheng",
            "Peihao Chen",
            "Yian Wang",
            "Junyan Li",
            "Chuang Gan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.09340v1",
        "title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene\n  Understanding",
        "abstract": "  3D vision-language grounding, which focuses on aligning language with the 3D\nphysical environment, stands as a cornerstone in the development of embodied\nagents. In comparison to recent advancements in the 2D domain, grounding\nlanguage in 3D scenes faces several significant challenges: (i) the inherent\ncomplexity of 3D scenes due to the diverse object configurations, their rich\nattributes, and intricate relationships; (ii) the scarcity of paired 3D\nvision-language data to support grounded learning; and (iii) the absence of a\nunified learning framework to distill knowledge from grounded 3D data. In this\nwork, we aim to address these three major challenges in 3D vision-language by\nexamining the potential of systematically upscaling 3D vision-language learning\nin indoor environments. We introduce the first million-scale 3D vision-language\ndataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising\n2.5M vision-language pairs derived from both human annotations and our scalable\nscene-graph-based generation approach. We demonstrate that this scaling allows\nfor a unified pre-training framework, Grounded Pre-training for Scenes (GPS),\nfor 3D vision-language learning. Through extensive experiments, we showcase the\neffectiveness of GPS by achieving state-of-the-art performance on all existing\n3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is\nunveiled through zero-shot transfer experiments in the challenging 3D\nvision-language tasks. Project website: https://scene-verse.github.io .\n",
        "published": "2024",
        "authors": [
            "Baoxiong Jia",
            "Yixin Chen",
            "Huangyue Yu",
            "Yan Wang",
            "Xuesong Niu",
            "Tengyu Liu",
            "Qing Li",
            "Siyuan Huang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.06064v1",
        "title": "Gaussian Process Decentralized Data Fusion Meets Transfer Learning in\n  Large-Scale Distributed Cooperative Perception",
        "abstract": "  This paper presents novel Gaussian process decentralized data fusion\nalgorithms exploiting the notion of agent-centric support sets for distributed\ncooperative perception of large-scale environmental phenomena. To overcome the\nlimitations of scale in existing works, our proposed algorithms allow every\nmobile sensing agent to choose a different support set and dynamically switch\nto another during execution for encapsulating its own data into a local summary\nthat, perhaps surprisingly, can still be assimilated with the other agents'\nlocal summaries (i.e., based on their current choices of support sets) into a\nglobally consistent summary to be used for predicting the phenomenon. To\nachieve this, we propose a novel transfer learning mechanism for a team of\nagents capable of sharing and transferring information encapsulated in a\nsummary based on a support set to that utilizing a different support set with\nsome loss that can be theoretically bounded and analyzed. To alleviate the\nissue of information loss accumulating over multiple instances of transfer\nlearning, we propose a new information sharing mechanism to be incorporated\ninto our algorithms in order to achieve memory-efficient lazy transfer\nlearning. Empirical evaluation on real-world datasets show that our algorithms\noutperform the state-of-the-art methods.\n",
        "published": "2017",
        "authors": [
            "Ruofei Ouyang",
            "Kian Hsiang Low"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.04537v1",
        "title": "Defensive Escort Teams via Multi-Agent Deep Reinforcement Learning",
        "abstract": "  Coordinated defensive escorts can aid a navigating payload by positioning\nthemselves in order to maintain the safety of the payload from obstacles. In\nthis paper, we present a novel, end-to-end solution for coordinating an escort\nteam for protecting high-value payloads. Our solution employs deep\nreinforcement learning (RL) in order to train a team of escorts to maintain\npayload safety while navigating alongside the payload. This is done in a\ndistributed fashion, relying only on limited range positional information of\nother escorts, the payload, and the obstacles. When compared to a state-of-art\nalgorithm for obstacle avoidance, our solution with a single escort increases\nnavigation success up to 31%. Additionally, escort teams increase success rate\nby up to 75% percent over escorts in static formations. We also show that this\nlearned solution is general to several adaptations in the scenario including: a\nchanging number of escorts in the team, changing obstacle density, and changes\nin payload conformation. Video: https://youtu.be/SoYesKti4VA.\n",
        "published": "2019",
        "authors": [
            "Arpit Garg",
            "Yazied A. Hasan",
            "Adam Ya\u00f1ez",
            "Lydia Tapia"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.02945v1",
        "title": "A pedestrian path-planning model in accordance with obstacle's danger\n  with reinforcement learning",
        "abstract": "  Most microscopic pedestrian navigation models use the concept of \"forces\"\napplied to the pedestrian agents to replicate the navigation environment. While\nthe approach could provide believable results in regular situations, it does\nnot always resemble natural pedestrian navigation behaviour in many typical\nsettings. In our research, we proposed a novel approach using reinforcement\nlearning for simulation of pedestrian agent path planning and collision\navoidance problem. The primary focus of this approach is using human perception\nof the environment and danger awareness of interferences. The implementation of\nour model has shown that the path planned by the agent shares many similarities\nwith a human pedestrian in several aspects such as following common walking\nconventions and human behaviours.\n",
        "published": "2019",
        "authors": [
            "Thanh-Trung Trinh",
            "Dinh-Minh Vu",
            "Masaomi Kimura"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.09001v2",
        "title": "MagNet: Discovering Multi-agent Interaction Dynamics using Neural\n  Network",
        "abstract": "  We present the MagNet, a neural network-based multi-agent interaction model\nto discover the governing dynamics and predict evolution of a complex\nmulti-agent system from observations. We formulate a multi-agent system as a\ncoupled non-linear network with a generic ordinary differential equation (ODE)\nbased state evolution, and develop a neural network-based realization of its\ntime-discretized model. MagNet is trained to discover the core dynamics of a\nmulti-agent system from observations, and tuned on-line to learn agent-specific\nparameters of the dynamics to ensure accurate prediction even when physical or\nrelational attributes of agents, or number of agents change. We evaluate MagNet\non a point-mass system in two-dimensional space, Kuramoto phase synchronization\ndynamics and predator-swarm interaction dynamics demonstrating orders of\nmagnitude improvement in prediction accuracy over traditional deep learning\nmodels.\n",
        "published": "2020",
        "authors": [
            "Priyabrata Saha",
            "Arslan Ali",
            "Burhan A. Mudassar",
            "Yun Long",
            "Saibal Mukhopadhyay"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.03900v2",
        "title": "FormulaZero: Distributionally Robust Online Adaptation via Offline\n  Population Synthesis",
        "abstract": "  Balancing performance and safety is crucial to deploying autonomous vehicles\nin multi-agent environments. In particular, autonomous racing is a domain that\npenalizes safe but conservative policies, highlighting the need for robust,\nadaptive strategies. Current approaches either make simplifying assumptions\nabout other agents or lack robust mechanisms for online adaptation. This work\nmakes algorithmic contributions to both challenges. First, to generate a\nrealistic, diverse set of opponents, we develop a novel method for self-play\nbased on replica-exchange Markov chain Monte Carlo. Second, we propose a\ndistributionally robust bandit optimization procedure that adaptively adjusts\nrisk aversion relative to uncertainty in beliefs about opponents' behaviors. We\nrigorously quantify the tradeoffs in performance and robustness when\napproximating these computations in real-time motion-planning, and we\ndemonstrate our methods experimentally on autonomous vehicles that achieve\nscaled speeds comparable to Formula One racecars.\n",
        "published": "2020",
        "authors": [
            "Aman Sinha",
            "Matthew O'Kelly",
            "Hongrui Zheng",
            "Rahul Mangharam",
            "John Duchi",
            "Russ Tedrake"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.01825v2",
        "title": "Robust Reinforcement Learning using Adversarial Populations",
        "abstract": "  Reinforcement Learning (RL) is an effective tool for controller design but\ncan struggle with issues of robustness, failing catastrophically when the\nunderlying system dynamics are perturbed. The Robust RL formulation tackles\nthis by adding worst-case adversarial noise to the dynamics and constructing\nthe noise distribution as the solution to a zero-sum minimax game. However,\nexisting work on learning solutions to the Robust RL formulation has primarily\nfocused on training a single RL agent against a single adversary. In this work,\nwe demonstrate that using a single adversary does not consistently yield\nrobustness to dynamics variations under standard parametrizations of the\nadversary; the resulting policy is highly exploitable by new adversaries. We\npropose a population-based augmentation to the Robust RL formulation in which\nwe randomly initialize a population of adversaries and sample from the\npopulation uniformly during training. We empirically validate across robotics\nbenchmarks that the use of an adversarial population results in a more robust\npolicy that also improves out-of-distribution generalization. Finally, we\ndemonstrate that this approach provides comparable robustness and\ngeneralization as domain randomization on these benchmarks while avoiding a\nubiquitous domain randomization failure mode.\n",
        "published": "2020",
        "authors": [
            "Eugene Vinitsky",
            "Yuqing Du",
            "Kanaad Parvate",
            "Kathy Jang",
            "Pieter Abbeel",
            "Alexandre Bayen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.00497v2",
        "title": "Accelerating Cooperative Planning for Automated Vehicles with Learned\n  Heuristics and Monte Carlo Tree Search",
        "abstract": "  Efficient driving in urban traffic scenarios requires foresight. The\nobservation of other traffic participants and the inference of their possible\nnext actions depending on the own action is considered cooperative prediction\nand planning. Humans are well equipped with the capability to predict the\nactions of multiple interacting traffic participants and plan accordingly,\nwithout the need to directly communicate with others. Prior work has shown that\nit is possible to achieve effective cooperative planning without the need for\nexplicit communication. However, the search space for cooperative plans is so\nlarge that most of the computational budget is spent on exploring the search\nspace in unpromising regions that are far away from the solution. To accelerate\nthe planning process, we combined learned heuristics with a cooperative\nplanning method to guide the search towards regions with promising actions,\nyielding better solutions at lower computational costs.\n",
        "published": "2020",
        "authors": [
            "Karl Kurzer",
            "Marcus Fechner",
            "J. Marius Z\u00f6llner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.08937v4",
        "title": "Institutional Grammar 2.0 Codebook",
        "abstract": "  The Grammar of Institutions, or Institutional Grammar, is an established\napproach to encode policy information in terms of institutional statements\nbased on a set of pre-defined syntactic components. This codebook provides\ncoding guidelines for a revised version of the Institutional Grammar, the\nInstitutional Grammar 2.0 (IG 2.0). IG 2.0 is a specification that aims at\nfacilitating the encoding of policy to meet varying analytical objectives. To\nthis end, it revises the grammar with respect to comprehensiveness,\nflexibility, and specificity by offering multiple levels of expressiveness (IG\nCore, IG Extended, IG Logico). In addition to the encoding of regulative\nstatements, it further introduces the encoding of constitutive institutional\nstatements, as well as statements that exhibit both constitutive and regulative\ncharacteristics. Introducing those aspects, the codebook initially covers\nfundamental concepts of IG 2.0, before providing an overview of pre-coding\nsteps relevant for document preparation. Detailed coding guidelines are\nprovided for both regulative and constitutive statements across all levels of\nexpressiveness, along with the encoding guidelines for statements of mixed form\n-- hybrid and polymorphic institutional statements. The document further\nprovides an overview of taxonomies used in the encoding process and referred to\nthroughout the codebook. The codebook concludes with a summary and discussion\nof relevant considerations to facilitate the coding process. An initial\nReader's Guide helps the reader tailor the content to her interest.\n  Note that this codebook specifically focuses on operational aspects of IG 2.0\nin the context of policy coding. Links to additional resources such as the\nunderlying scientific literature (that offers a comprehensive treatment of the\nunderlying theoretical concepts) are referred to in the DOI and the concluding\nsection of the codebook.\n",
        "published": "2020",
        "authors": [
            "Christopher K. Frantz",
            "Saba N. Siddiki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1106.4509v1",
        "title": "Machine Learning Markets",
        "abstract": "  Prediction markets show considerable promise for developing flexible\nmechanisms for machine learning. Here, machine learning markets for\nmultivariate systems are defined, and a utility-based framework is established\nfor their analysis. This differs from the usual approach of defining static\nbetting functions. It is shown that such markets can implement model\ncombination methods used in machine learning, such as product of expert and\nmixture of expert approaches as equilibrium pricing models, by varying agent\nutility functions. They can also implement models composed of local potentials,\nand message passing methods. Prediction markets also allow for more flexible\ncombinations, by combining multiple different utility functions. Conversely,\nthe market mechanisms implement inference in the relevant probabilistic models.\nThis means that market mechanism can be utilized for implementing parallelized\nmodel building and inference for probabilistic modelling.\n",
        "published": "2011",
        "authors": [
            "Amos Storkey"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.03539v2",
        "title": "Uncovering the Social Interaction in Swarm Intelligence with Network\n  Science",
        "abstract": "  Swarm intelligence is the collective behavior emerging in systems with\nlocally interacting components. Because of their self-organization\ncapabilities, swarm-based systems show essential properties for handling\nreal-world problems such as robustness, scalability, and flexibility. Yet, we\ndo not know why swarm-based algorithms work well and neither we can compare the\ndifferent approaches in the literature. The lack of a common framework capable\nof characterizing these several swarm-based algorithms, transcending their\nparticularities, has led to a stream of publications inspired by different\naspects of nature without a systematic comparison over existing approaches.\nHere, we address this gap by introducing a network-based framework---the\ninteraction network---to examine computational swarm-based systems via the\noptics of the social dynamics of such interaction network; a clear example of\nnetwork science being applied to bring further clarity to a complicated field\nwithin artificial intelligence. We discuss the social interactions of four\nwell-known swarm-based algorithms and provide an in-depth case study of the\nParticle Swarm Optimization. The interaction network enables researchers to\nstudy swarm algorithms as systems, removing the algorithm particularities from\nthe analyses while focusing on the structure of the social interactions.\n",
        "published": "2018",
        "authors": [
            "Marcos Oliveira",
            "Diego Pinheiro",
            "Mariana Macedo",
            "Carmelo Bastos-Filho",
            "Ronaldo Menezes"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.08354v1",
        "title": "Learning Instance Segmentation by Interaction",
        "abstract": "  We present an approach for building an active agent that learns to segment\nits visual observations into individual objects by interacting with its\nenvironment in a completely self-supervised manner. The agent uses its current\nsegmentation model to infer pixels that constitute objects and refines the\nsegmentation model by interacting with these pixels. The model learned from\nover 50K interactions generalizes to novel objects and backgrounds. To deal\nwith noisy training signal for segmenting objects obtained by self-supervised\ninteractions, we propose robust set loss. A dataset of robot's interactions\nalong-with a few human labeled examples is provided as a benchmark for future\nresearch. We test the utility of the learned segmentation model by providing\nresults on a downstream vision-based control task of rearranging multiple\nobjects into target configurations from visual inputs alone. Videos, code, and\nrobotic interaction dataset are available at\nhttps://pathak22.github.io/seg-by-interaction/\n",
        "published": "2018",
        "authors": [
            "Deepak Pathak",
            "Yide Shentu",
            "Dian Chen",
            "Pulkit Agrawal",
            "Trevor Darrell",
            "Sergey Levine",
            "Jitendra Malik"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.09453v1",
        "title": "Context-Aware Pedestrian Motion Prediction In Urban Intersections",
        "abstract": "  This paper presents a novel context-based approach for pedestrian motion\nprediction in crowded, urban intersections, with the additional flexibility of\nprediction in similar, but new, environments. Previously, Chen et. al. combined\nMarkovian-based and clustering-based approaches to learn motion primitives in a\ngrid-based world and subsequently predict pedestrian trajectories by modeling\nthe transition between learned primitives as a Gaussian Process (GP). This work\nextends that prior approach by incorporating semantic features from the\nenvironment (relative distance to curbside and status of pedestrian traffic\nlights) in the GP formulation for more accurate predictions of pedestrian\ntrajectories over the same timescale. We evaluate the new approach on\nreal-world data collected using one of the vehicles in the MIT Mobility On\nDemand fleet. The results show 12.5% improvement in prediction accuracy and a\n2.65 times reduction in Area Under the Curve (AUC), which is used as a metric\nto quantify the span of predicted set of trajectories, such that a lower AUC\ncorresponds to a higher level of confidence in the future direction of\npedestrian motion.\n",
        "published": "2018",
        "authors": [
            "Golnaz Habibi",
            "Nikita Jaipuria",
            "Jonathan P. How"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.10293v3",
        "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic\n  Manipulation",
        "abstract": "  In this paper, we study the problem of learning vision-based dynamic\nmanipulation skills using a scalable reinforcement learning approach. We study\nthis problem in the context of grasping, a longstanding challenge in robotic\nmanipulation. In contrast to static learning behaviors that choose a grasp\npoint and then execute the desired grasp, our method enables closed-loop\nvision-based control, whereby the robot continuously updates its grasp strategy\nbased on the most recent observations to optimize long-horizon grasp success.\nTo that end, we introduce QT-Opt, a scalable self-supervised vision-based\nreinforcement learning framework that can leverage over 580k real-world grasp\nattempts to train a deep neural network Q-function with over 1.2M parameters to\nperform closed-loop, real-world grasping that generalizes to 96% grasp success\non unseen objects. Aside from attaining a very high success rate, our method\nexhibits behaviors that are quite distinct from more standard grasping systems:\nusing only RGB vision-based perception from an over-the-shoulder camera, our\nmethod automatically learns regrasping strategies, probes objects to find the\nmost effective grasps, learns to reposition objects and perform other\nnon-prehensile pre-grasp manipulations, and responds dynamically to\ndisturbances and perturbations.\n",
        "published": "2018",
        "authors": [
            "Dmitry Kalashnikov",
            "Alex Irpan",
            "Peter Pastor",
            "Julian Ibarz",
            "Alexander Herzog",
            "Eric Jang",
            "Deirdre Quillen",
            "Ethan Holly",
            "Mrinal Kalakrishnan",
            "Vincent Vanhoucke",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.03777v1",
        "title": "Semantic Label Reduction Techniques for Autonomous Driving",
        "abstract": "  Semantic segmentation maps can be used as input to models for maneuvering the\ncontrols of a car. However, not all labels may be necessary for making the\ncontrol decision. One would expect that certain labels such as road lanes or\nsidewalks would be more critical in comparison with labels for vegetation or\nbuildings which may not have a direct influence on the car's driving decision.\nIn this appendix, we evaluate and quantify how sensitive and important the\ndifferent semantic labels are for controlling the car. Labels that do not\ninfluence the driving decision are remapped to other classes, thereby\nsimplifying the task by reducing to only labels critical for driving of the\nvehicle.\n",
        "published": "2019",
        "authors": [
            "Qadeer Khan",
            "Torsten Sch\u00f6n",
            "Patrick Wenzel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.04272v1",
        "title": "Towards Self-Supervised High Level Sensor Fusion",
        "abstract": "  In this paper, we present a framework to control a self-driving car by fusing\nraw information from RGB images and depth maps. A deep neural network\narchitecture is used for mapping the vision and depth information,\nrespectively, to steering commands. This fusion of information from two sensor\nsources allows to provide redundancy and fault tolerance in the presence of\nsensor failures. Even if one of the input sensors fails to produce the correct\noutput, the other functioning sensor would still be able to maneuver the car.\nSuch redundancy is crucial in the critical application of self-driving cars.\nThe experimental results have showed that our method is capable of learning to\nuse the relevant sensor information even when one of the sensors fail without\nany explicit signal.\n",
        "published": "2019",
        "authors": [
            "Qadeer Khan",
            "Torsten Sch\u00f6n",
            "Patrick Wenzel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.07113v1",
        "title": "Solving Rubik's Cube with a Robot Hand",
        "abstract": "  We demonstrate that models trained only in simulation can be used to solve a\nmanipulation problem of unprecedented complexity on a real robot. This is made\npossible by two key components: a novel algorithm, which we call automatic\ndomain randomization (ADR) and a robot platform built for machine learning. ADR\nautomatically generates a distribution over randomized environments of\never-increasing difficulty. Control policies and vision state estimators\ntrained with ADR exhibit vastly improved sim2real transfer. For control\npolicies, memory-augmented models trained on an ADR-generated distribution of\nenvironments show clear signs of emergent meta-learning at test time. The\ncombination of ADR with our custom robot platform allows us to solve a Rubik's\ncube with a humanoid robot hand, which involves both control and state\nestimation problems. Videos summarizing our results are available:\nhttps://openai.com/blog/solving-rubiks-cube/\n",
        "published": "2019",
        "authors": [
            " OpenAI",
            "Ilge Akkaya",
            "Marcin Andrychowicz",
            "Maciek Chociej",
            "Mateusz Litwin",
            "Bob McGrew",
            "Arthur Petron",
            "Alex Paino",
            "Matthias Plappert",
            "Glenn Powell",
            "Raphael Ribas",
            "Jonas Schneider",
            "Nikolas Tezak",
            "Jerry Tworek",
            "Peter Welinder",
            "Lilian Weng",
            "Qiming Yuan",
            "Wojciech Zaremba",
            "Lei Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.08225v4",
        "title": "Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill\n  Discovery",
        "abstract": "  Reinforcement learning requires manual specification of a reward function to\nlearn a task. While in principle this reward function only needs to specify the\ntask goal, in practice reinforcement learning can be very time-consuming or\neven infeasible unless the reward function is shaped so as to provide a smooth\ngradient towards a successful outcome. This shaping is difficult to specify by\nhand, particularly when the task is learned from raw observations, such as\nimages. In this paper, we study how we can automatically learn dynamical\ndistances: a measure of the expected number of time steps to reach a given goal\nstate from any other state. These dynamical distances can be used to provide\nwell-shaped reward functions for reaching new goals, making it possible to\nlearn complex tasks efficiently. We show that dynamical distances can be used\nin a semi-supervised regime, where unsupervised interaction with the\nenvironment is used to learn the dynamical distances, while a small amount of\npreference supervision is used to determine the task goal, without any manually\nengineered reward function or goal examples. We evaluate our method both on a\nreal-world robot and in simulation. We show that our method can learn to turn a\nvalve with a real-world 9-DoF hand, using raw image observations and just ten\npreference labels, without any other supervision. Videos of the learned skills\ncan be found on the project website:\nhttps://sites.google.com/view/dynamical-distance-learning.\n",
        "published": "2019",
        "authors": [
            "Kristian Hartikainen",
            "Xinyang Geng",
            "Tuomas Haarnoja",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.11239v3",
        "title": "TossingBot: Learning to Throw Arbitrary Objects with Residual Physics",
        "abstract": "  We investigate whether a robot arm can learn to pick and throw arbitrary\nobjects into selected boxes quickly and accurately. Throwing has the potential\nto increase the physical reachability and picking speed of a robot arm.\nHowever, precisely throwing arbitrary objects in unstructured settings presents\nmany challenges: from acquiring reliable pre-throw conditions (e.g. initial\npose of object in manipulator) to handling varying object-centric properties\n(e.g. mass distribution, friction, shape) and dynamics (e.g. aerodynamics). In\nthis work, we propose an end-to-end formulation that jointly learns to infer\ncontrol parameters for grasping and throwing motion primitives from visual\nobservations (images of arbitrary objects in a bin) through trial and error.\nWithin this formulation, we investigate the synergies between grasping and\nthrowing (i.e., learning grasps that enable more accurate throws) and between\nsimulation and deep learning (i.e., using deep networks to predict residuals on\ntop of control parameters predicted by a physics simulator). The resulting\nsystem, TossingBot, is able to grasp and throw arbitrary objects into boxes\nlocated outside its maximum reach range at 500+ mean picks per hour (600+\ngrasps per hour with 85% throwing accuracy); and generalizes to new objects and\ntarget locations. Videos are available at https://tossingbot.cs.princeton.edu\n",
        "published": "2019",
        "authors": [
            "Andy Zeng",
            "Shuran Song",
            "Johnny Lee",
            "Alberto Rodriguez",
            "Thomas Funkhouser"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.08444v1",
        "title": "Relational Mimic for Visual Adversarial Imitation Learning",
        "abstract": "  In this work, we introduce a new method for imitation learning from video\ndemonstrations. Our method, Relational Mimic (RM), improves on previous visual\nimitation learning methods by combining generative adversarial networks and\nrelational learning. RM is flexible and can be used in conjunction with other\nrecent advances in generative adversarial imitation learning to better address\nthe need for more robust and sample-efficient approaches. In addition, we\nintroduce a new neural network architecture that improves upon the previous\nstate-of-the-art in reinforcement learning and illustrate how increasing the\nrelational reasoning capabilities of the agent enables the latter to achieve\nincreasingly higher performance in a challenging locomotion task with pixel\ninputs. Finally, we study the effects and contributions of relational learning\nin policy evaluation, policy improvement and reward learning through ablation\nstudies.\n",
        "published": "2019",
        "authors": [
            "Lionel Blond\u00e9",
            "Yichuan Charlie Tang",
            "Jian Zhang",
            "Russ Webb"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.06417v3",
        "title": "Sparse Graphical Memory for Robust Planning",
        "abstract": "  To operate effectively in the real world, agents should be able to act from\nhigh-dimensional raw sensory input such as images and achieve diverse goals\nacross long time-horizons. Current deep reinforcement and imitation learning\nmethods can learn directly from high-dimensional inputs but do not scale well\nto long-horizon tasks. In contrast, classical graphical methods like A* search\nare able to solve long-horizon tasks, but assume that the state space is\nabstracted away from raw sensory input. Recent works have attempted to combine\nthe strengths of deep learning and classical planning; however, dominant\nmethods in this domain are still quite brittle and scale poorly with the size\nof the environment. We introduce Sparse Graphical Memory (SGM), a new data\nstructure that stores states and feasible transitions in a sparse memory. SGM\naggregates states according to a novel two-way consistency objective, adapting\nclassic state aggregation criteria to goal-conditioned RL: two states are\nredundant when they are interchangeable both as goals and as starting states.\nTheoretically, we prove that merging nodes according to two-way consistency\nleads to an increase in shortest path lengths that scales only linearly with\nthe merging threshold. Experimentally, we show that SGM significantly\noutperforms current state of the art methods on long horizon, sparse-reward\nvisual navigation tasks. Project video and code are available at\nhttps://mishalaskin.github.io/sgm/\n",
        "published": "2020",
        "authors": [
            "Scott Emmons",
            "Ajay Jain",
            "Michael Laskin",
            "Thanard Kurutach",
            "Pieter Abbeel",
            "Deepak Pathak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.02636v1",
        "title": "The Importance of Prior Knowledge in Precise Multimodal Prediction",
        "abstract": "  Roads have well defined geometries, topologies, and traffic rules. While this\nhas been widely exploited in motion planning methods to produce maneuvers that\nobey the law, little work has been devoted to utilize these priors in\nperception and motion forecasting methods. In this paper we propose to\nincorporate these structured priors as a loss function. In contrast to imposing\nhard constraints, this approach allows the model to handle non-compliant\nmaneuvers when those happen in the real world. Safe motion planning is the end\ngoal, and thus a probabilistic characterization of the possible future\ndevelopments of the scene is key to choose the plan with the lowest expected\ncost. Towards this goal, we design a framework that leverages REINFORCE to\nincorporate non-differentiable priors over sample trajectories from a\nprobabilistic model, thus optimizing the whole distribution. We demonstrate the\neffectiveness of our approach on real-world self-driving datasets containing\ncomplex road topologies and multi-agent interactions. Our motion forecasts not\nonly exhibit better precision and map understanding, but most importantly\nresult in safer motion plans taken by our self-driving vehicle. We emphasize\nthat despite the importance of this evaluation, it has been often overlooked by\nprevious perception and motion forecasting works.\n",
        "published": "2020",
        "authors": [
            "Sergio Casas",
            "Cole Gulino",
            "Simon Suo",
            "Raquel Urtasun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.13205v2",
        "title": "Long-Horizon Visual Planning with Goal-Conditioned Hierarchical\n  Predictors",
        "abstract": "  The ability to predict and plan into the future is fundamental for agents\nacting in the world. To reach a faraway goal, we predict trajectories at\nmultiple timescales, first devising a coarse plan towards the goal and then\ngradually filling in details. In contrast, current learning approaches for\nvisual prediction and planning fail on long-horizon tasks as they generate\npredictions (1) without considering goal information, and (2) at the finest\ntemporal resolution, one step at a time. In this work we propose a framework\nfor visual prediction and planning that is able to overcome both of these\nlimitations. First, we formulate the problem of predicting towards a goal and\npropose the corresponding class of latent space goal-conditioned predictors\n(GCPs). GCPs significantly improve planning efficiency by constraining the\nsearch space to only those trajectories that reach the goal. Further, we show\nhow GCPs can be naturally formulated as hierarchical models that, given two\nobservations, predict an observation between them, and by recursively\nsubdividing each part of the trajectory generate complete sequences. This\ndivide-and-conquer strategy is effective at long-term prediction, and enables\nus to design an effective hierarchical planning algorithm that optimizes\ntrajectories in a coarse-to-fine manner. We show that by using both\ngoal-conditioning and hierarchical prediction, GCPs enable us to solve visual\nplanning tasks with much longer horizon than previously possible.\n",
        "published": "2020",
        "authors": [
            "Karl Pertsch",
            "Oleh Rybkin",
            "Frederik Ebert",
            "Chelsea Finn",
            "Dinesh Jayaraman",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.05930v1",
        "title": "Perceive, Predict, and Plan: Safe Motion Planning Through Interpretable\n  Semantic Representations",
        "abstract": "  In this paper we propose a novel end-to-end learnable network that performs\njoint perception, prediction and motion planning for self-driving vehicles and\nproduces interpretable intermediate representations. Unlike existing neural\nmotion planners, our motion planning costs are consistent with our perception\nand prediction estimates. This is achieved by a novel differentiable semantic\noccupancy representation that is explicitly used as cost by the motion planning\nprocess. Our network is learned end-to-end from human demonstrations. The\nexperiments in a large-scale manual-driving dataset and closed-loop simulation\nshow that the proposed model significantly outperforms state-of-the-art\nplanners in imitating the human behaviors while producing much safer\ntrajectories.\n",
        "published": "2020",
        "authors": [
            "Abbas Sadat",
            "Sergio Casas",
            "Mengye Ren",
            "Xinyu Wu",
            "Pranaab Dhawan",
            "Raquel Urtasun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.11174v4",
        "title": "Learning Obstacle Representations for Neural Motion Planning",
        "abstract": "  Motion planning and obstacle avoidance is a key challenge in robotics\napplications. While previous work succeeds to provide excellent solutions for\nknown environments, sensor-based motion planning in new and dynamic\nenvironments remains difficult. In this work we address sensor-based motion\nplanning from a learning perspective. Motivated by recent advances in visual\nrecognition, we argue the importance of learning appropriate representations\nfor motion planning. We propose a new obstacle representation based on the\nPointNet architecture and train it jointly with policies for obstacle\navoidance. We experimentally evaluate our approach for rigid body motion\nplanning in challenging environments and demonstrate significant improvements\nof the state of the art in terms of accuracy and efficiency.\n",
        "published": "2020",
        "authors": [
            "Robin Strudel",
            "Ricardo Garcia",
            "Justin Carpentier",
            "Jean-Paul Laumond",
            "Ivan Laptev",
            "Cordelia Schmid"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.06742v1",
        "title": "Deep Parametric Continuous Convolutional Neural Networks",
        "abstract": "  Standard convolutional neural networks assume a grid structured input is\navailable and exploit discrete convolutions as their fundamental building\nblocks. This limits their applicability to many real-world applications. In\nthis paper we propose Parametric Continuous Convolution, a new learnable\noperator that operates over non-grid structured data. The key idea is to\nexploit parameterized kernel functions that span the full continuous vector\nspace. This generalization allows us to learn over arbitrary data structures as\nlong as their support relationship is computable. Our experiments show\nsignificant improvement over the state-of-the-art in point cloud segmentation\nof indoor and outdoor scenes, and lidar motion estimation of driving scenes.\n",
        "published": "2021",
        "authors": [
            "Shenlong Wang",
            "Simon Suo",
            "Wei-Chiu Ma",
            "Andrei Pokrovsky",
            "Raquel Urtasun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.07593v2",
        "title": "Differentiable SLAM-net: Learning Particle SLAM for Visual Navigation",
        "abstract": "  Simultaneous localization and mapping (SLAM) remains challenging for a number\nof downstream applications, such as visual robot navigation, because of rapid\nturns, featureless walls, and poor camera quality. We introduce the\nDifferentiable SLAM Network (SLAM-net) along with a navigation architecture to\nenable planar robot navigation in previously unseen indoor environments.\nSLAM-net encodes a particle filter based SLAM algorithm in a differentiable\ncomputation graph, and learns task-oriented neural network components by\nbackpropagating through the SLAM algorithm. Because it can optimize all model\ncomponents jointly for the end-objective, SLAM-net learns to be robust in\nchallenging conditions. We run experiments in the Habitat platform with\ndifferent real-world RGB and RGB-D datasets. SLAM-net significantly outperforms\nthe widely adapted ORB-SLAM in noisy conditions. Our navigation architecture\nwith SLAM-net improves the state-of-the-art for the Habitat Challenge 2020\nPointNav task by a large margin (37% to 64% success). Project website:\nhttp://sites.google.com/view/slamnet\n",
        "published": "2021",
        "authors": [
            "Peter Karkus",
            "Shaojun Cai",
            "David Hsu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.00645v2",
        "title": "Universal Planning Networks",
        "abstract": "  A key challenge in complex visuomotor control is learning abstract\nrepresentations that are effective for specifying goals, planning, and\ngeneralization. To this end, we introduce universal planning networks (UPN).\nUPNs embed differentiable planning within a goal-directed policy. This planning\ncomputation unrolls a forward model in a latent space and infers an optimal\naction plan through gradient descent trajectory optimization. The\nplan-by-gradient-descent process and its underlying representations are learned\nend-to-end to directly optimize a supervised imitation learning objective. We\nfind that the representations learned are not only effective for goal-directed\nvisual imitation via gradient-based trajectory optimization, but can also\nprovide a metric for specifying goals using images. The learned representations\ncan be leveraged to specify distance-based rewards to reach new target states\nfor model-free reinforcement learning, resulting in substantially more\neffective learning when solving new tasks described via image-based goals. We\nwere able to achieve successful transfer of visuomotor planning strategies\nacross robots with significantly different morphologies and actuation\ncapabilities.\n",
        "published": "2018",
        "authors": [
            "Aravind Srinivas",
            "Allan Jabri",
            "Pieter Abbeel",
            "Sergey Levine",
            "Chelsea Finn"
        ]
    }
]