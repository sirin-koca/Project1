[
    {
        "id": "http://arxiv.org/abs/1905.12260v1",
        "title": "Learning Multilingual Word Embeddings Using Image-Text Data",
        "abstract": "  There has been significant interest recently in learning multilingual word\nembeddings -- in which semantically similar words across languages have similar\nembeddings. State-of-the-art approaches have relied on expensive labeled data,\nwhich is unavailable for low-resource languages, or have involved post-hoc\nunification of monolingual embeddings. In the present paper, we investigate the\nefficacy of multilingual embeddings learned from weakly-supervised image-text\ndata. In particular, we propose methods for learning multilingual embeddings\nusing image-text data, by enforcing similarity between the representations of\nthe image and that of the text. Our experiments reveal that even without using\nany expensive labeled data, a bag-of-words-based embedding model trained on\nimage-text data achieves performance comparable to the state-of-the-art on\ncrosslingual semantic similarity tasks.\n",
        "published": "2019",
        "authors": [
            "Karan Singhal",
            "Karthik Raman",
            "Balder ten Cate"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.07583v2",
        "title": "Inverse Visual Question Answering with Multi-Level Attentions",
        "abstract": "  In this paper, we propose a novel deep multi-level attention model to address\ninverse visual question answering. The proposed model generates regional visual\nand semantic features at the object level and then enhances them with the\nanswer cue by using attention mechanisms. Two levels of multiple attentions are\nemployed in the model, including the dual attention at the partial question\nencoding step and the dynamic attention at the next question word generation\nstep. We evaluate the proposed model on the VQA V1 dataset. It demonstrates\nstate-of-the-art performance in terms of multiple commonly used metrics.\n",
        "published": "2019",
        "authors": [
            "Yaser Alwattar",
            "Yuhong Guo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.09018v1",
        "title": "Corporate IT-support Help-Desk Process Hybrid-Automation Solution with\n  Machine Learning Approach",
        "abstract": "  Comprehensive IT support teams in large scale organizations require more man\npower for handling engagement and requests of employees from different channels\non a 24*7 basis. Automated email technical queries help desk is proposed to\nhave instant real-time quick solutions and email categorisation. Email topic\nmodelling with various machine learning, deep-learning approaches are compared\nwith different features for a scalable, generalised solution along with\nsure-shot static rules. Email's title, body, attachment, OCR text, and some\nfeature engineered custom features are given as input elements. XGBoost\ncascaded hierarchical models, Bi-LSTM model with word embeddings perform well\nshowing 77.3 overall accuracy For the real world corporate email data set. By\nintroducing the thresholding techniques, the overall automation system\narchitecture provides 85.6 percentage of accuracy for real world corporate\nemails. Combination of quick fixes, static rules, ML categorization as a low\ncost inference solution reduces 81 percentage of the human effort in the\nprocess of automation and real time implementation.\n",
        "published": "2019",
        "authors": [
            "Kuruparan Shanmugalingam",
            "Nisal Chandrasekara",
            "Calvin Hindle",
            "Gihan Fernando",
            "Chanaka Gunawardhana"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.03977v3",
        "title": "Multimodal Intelligence: Representation Learning, Information Fusion,\n  and Applications",
        "abstract": "  Deep learning methods have revolutionized speech recognition, image\nrecognition, and natural language processing since 2010. Each of these tasks\ninvolves a single modality in their input signals. However, many applications\nin the artificial intelligence field involve multiple modalities. Therefore, it\nis of broad interest to study the more difficult and complex problem of\nmodeling and learning across multiple modalities. In this paper, we provide a\ntechnical review of available models and learning methods for multimodal\nintelligence. The main focus of this review is the combination of vision and\nnatural language modalities, which has become an important topic in both the\ncomputer vision and natural language processing research communities. This\nreview provides a comprehensive analysis of recent works on multimodal deep\nlearning from three perspectives: learning multimodal representations, fusing\nmultimodal signals at various levels, and multimodal applications. Regarding\nmultimodal representation learning, we review the key concepts of embedding,\nwhich unify multimodal signals into a single vector space and thereby enable\ncross-modality signal processing. We also review the properties of many types\nof embeddings that are constructed and learned for general downstream tasks.\nRegarding multimodal fusion, this review focuses on special architectures for\nthe integration of representations of unimodal signals for a particular task.\nRegarding applications, selected areas of a broad interest in the current\nliterature are covered, including image-to-text caption generation,\ntext-to-image generation, and visual question answering. We believe that this\nreview will facilitate future studies in the emerging field of multimodal\nintelligence for related communities.\n",
        "published": "2019",
        "authors": [
            "Chao Zhang",
            "Zichao Yang",
            "Xiaodong He",
            "Li Deng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.00724v2",
        "title": "Obtaining Faithful Interpretations from Compositional Neural Networks",
        "abstract": "  Neural module networks (NMNs) are a popular approach for modeling\ncompositionality: they achieve high accuracy when applied to problems in\nlanguage and vision, while reflecting the compositional structure of the\nproblem in the network architecture. However, prior work implicitly assumed\nthat the structure of the network modules, describing the abstract reasoning\nprocess, provides a faithful explanation of the model's reasoning; that is,\nthat all modules perform their intended behaviour. In this work, we propose and\nconduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2\nand DROP, two datasets which require composing multiple reasoning steps. We\nfind that the intermediate outputs differ from the expected output,\nillustrating that the network structure does not provide a faithful explanation\nof model behaviour. To remedy that, we train the model with auxiliary\nsupervision and propose particular choices for module architecture that yield\nmuch better faithfulness, at a minimal cost to accuracy.\n",
        "published": "2020",
        "authors": [
            "Sanjay Subramanian",
            "Ben Bogin",
            "Nitish Gupta",
            "Tomer Wolfson",
            "Sameer Singh",
            "Jonathan Berant",
            "Matt Gardner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.07493v1",
        "title": "History for Visual Dialog: Do we really need it?",
        "abstract": "  Visual Dialog involves \"understanding\" the dialog history (what has been\ndiscussed previously) and the current question (what is asked), in addition to\ngrounding information in the image, to generate the correct response. In this\npaper, we show that co-attention models which explicitly encode dialog history\noutperform models that don't, achieving state-of-the-art performance (72 % NDCG\non val set). However, we also expose shortcomings of the crowd-sourcing dataset\ncollection procedure by showing that history is indeed only required for a\nsmall amount of the data and that the current evaluation metric encourages\ngeneric replies. To that end, we propose a challenging subset (VisDialConv) of\nthe VisDial val set and provide a benchmark of 63% NDCG.\n",
        "published": "2020",
        "authors": [
            "Shubham Agarwal",
            "Trung Bui",
            "Joon-Young Lee",
            "Ioannis Konstas",
            "Verena Rieser"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.02406v2",
        "title": "Video Moment Retrieval via Natural Language Queries",
        "abstract": "  In this paper, we propose a novel method for video moment retrieval (VMR)\nthat achieves state of the arts (SOTA) performance on R@1 metrics and\nsurpassing the SOTA on the high IoU metric (R@1, IoU=0.7).\n  First, we propose to use a multi-head self-attention mechanism, and further a\ncross-attention scheme to capture video/query interaction and long-range query\ndependencies from video context. The attention-based methods can develop\nframe-to-query interaction and query-to-frame interaction at arbitrary\npositions and the multi-head setting ensures the sufficient understanding of\ncomplicated dependencies. Our model has a simple architecture, which enables\nfaster training and inference while maintaining .\n  Second, We also propose to use multiple task training objective consists of\nmoment segmentation task, start/end distribution prediction and start/end\nlocation regression task. We have verified that start/end prediction are noisy\ndue to annotator disagreement and joint training with moment segmentation task\ncan provide richer information since frames inside the target clip are also\nutilized as positive training examples.\n  Third, we propose to use an early fusion approach, which achieves better\nperformance at the cost of inference time. However, the inference time will not\nbe a problem for our model since our model has a simple architecture which\nenables efficient training and inference.\n",
        "published": "2020",
        "authors": [
            "Xinli Yu",
            "Mohsen Malmir",
            "Cynthia He",
            "Yue Liu",
            "Rex Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.04965v3",
        "title": "Visual Relationship Detection with Visual-Linguistic Knowledge from\n  Multimodal Representations",
        "abstract": "  Visual relationship detection aims to reason over relationships among salient\nobjects in images, which has drawn increasing attention over the past few\nyears. Inspired by human reasoning mechanisms, it is believed that external\nvisual commonsense knowledge is beneficial for reasoning visual relationships\nof objects in images, which is however rarely considered in existing methods.\nIn this paper, we propose a novel approach named Relational Visual-Linguistic\nBidirectional Encoder Representations from Transformers (RVL-BERT), which\nperforms relational reasoning with both visual and language commonsense\nknowledge learned via self-supervised pre-training with multimodal\nrepresentations. RVL-BERT also uses an effective spatial module and a novel\nmask attention module to explicitly capture spatial information among the\nobjects. Moreover, our model decouples object detection from visual\nrelationship recognition by taking in object names directly, enabling it to be\nused on top of any object detection system. We show through quantitative and\nqualitative experiments that, with the transferred knowledge and novel modules,\nRVL-BERT achieves competitive results on two challenging visual relationship\ndetection datasets. The source code is available at\nhttps://github.com/coldmanck/RVL-BERT.\n",
        "published": "2020",
        "authors": [
            "Meng-Jiun Chiou",
            "Roger Zimmermann",
            "Jiashi Feng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.11278v1",
        "title": "X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal\n  Transformers",
        "abstract": "  Mirroring the success of masked language models, vision-and-language\ncounterparts like ViLBERT, LXMERT and UNITER have achieved state of the art\nperformance on a variety of multimodal discriminative tasks like visual\nquestion answering and visual grounding. Recent work has also successfully\nadapted such models towards the generative task of image captioning. This begs\nthe question: Can these models go the other way and generate images from pieces\nof text? Our analysis of a popular representative from this model family -\nLXMERT - finds that it is unable to generate rich and semantically meaningful\nimagery with its current training setup. We introduce X-LXMERT, an extension to\nLXMERT with training refinements including: discretizing visual\nrepresentations, using uniform masking with a large range of masking ratios and\naligning the right pre-training datasets to the right objectives which enables\nit to paint. X-LXMERT's image generation capabilities rival state of the art\ngenerative models while its question answering and captioning abilities remains\ncomparable to LXMERT. Finally, we demonstrate the generality of these training\nrefinements by adding image generation capabilities into UNITER to produce\nX-UNITER.\n",
        "published": "2020",
        "authors": [
            "Jaemin Cho",
            "Jiasen Lu",
            "Dustin Schwenk",
            "Hannaneh Hajishirzi",
            "Aniruddha Kembhavi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.00597v1",
        "title": "COOT: Cooperative Hierarchical Transformer for Video-Text Representation\n  Learning",
        "abstract": "  Many real-world video-text tasks involve different levels of granularity,\nsuch as frames and words, clip and sentences or videos and paragraphs, each\nwith distinct semantics. In this paper, we propose a Cooperative hierarchical\nTransformer (COOT) to leverage this hierarchy information and model the\ninteractions between different levels of granularity and different modalities.\nThe method consists of three major components: an attention-aware feature\naggregation layer, which leverages the local temporal context (intra-level,\ne.g., within a clip), a contextual transformer to learn the interactions\nbetween low-level and high-level semantics (inter-level, e.g. clip-video,\nsentence-paragraph), and a cross-modal cycle-consistency loss to connect video\nand text. The resulting method compares favorably to the state of the art on\nseveral benchmarks while having few parameters. All code is available\nopen-source at https://github.com/gingsi/coot-videotext\n",
        "published": "2020",
        "authors": [
            "Simon Ging",
            "Mohammadreza Zolfaghari",
            "Hamed Pirsiavash",
            "Thomas Brox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.09257v3",
        "title": "Inspecting state of the art performance and NLP metrics in image-based\n  medical report generation",
        "abstract": "  Several deep learning architectures have been proposed over the last years to\ndeal with the problem of generating a written report given an imaging exam as\ninput. Most works evaluate the generated reports using standard Natural\nLanguage Processing (NLP) metrics (e.g. BLEU, ROUGE), reporting significant\nprogress. In this article, we contrast this progress by comparing state of the\nart (SOTA) models against weak baselines. We show that simple and even naive\napproaches yield near SOTA performance on most traditional NLP metrics. We\nconclude that evaluation methods in this task should be further studied towards\ncorrectly measuring clinical accuracy, ideally involving physicians to\ncontribute to this end.\n",
        "published": "2020",
        "authors": [
            "Pablo Pino",
            "Denis Parra",
            "Pablo Messina",
            "Cecilia Besa",
            "Sergio Uribe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.10704v2",
        "title": "Neural Group Testing to Accelerate Deep Learning",
        "abstract": "  Recent advances in deep learning have made the use of large, deep neural\nnetworks with tens of millions of parameters. The sheer size of these networks\nimposes a challenging computational burden during inference. Existing work\nfocuses primarily on accelerating each forward pass of a neural network.\nInspired by the group testing strategy for efficient disease testing, we\npropose neural group testing, which accelerates by testing a group of samples\nin one forward pass. Groups of samples that test negative are ruled out. If a\ngroup tests positive, samples in that group are then retested adaptively. A key\nchallenge of neural group testing is to modify a deep neural network so that it\ncould test multiple samples in one forward pass. We propose three designs to\nachieve this without introducing any new parameters and evaluate their\nperformances. We applied neural group testing in an image moderation task to\ndetect rare but inappropriate images. We found that neural group testing can\ngroup up to 16 images in one forward pass and reduce the overall computation\ncost by over 73% while improving detection performance.\n",
        "published": "2020",
        "authors": [
            "Weixin Liang",
            "James Zou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.10731v1",
        "title": "LRTA: A Transparent Neural-Symbolic Reasoning Framework with Modular\n  Supervision for Visual Question Answering",
        "abstract": "  The predominant approach to visual question answering (VQA) relies on\nencoding the image and question with a \"black-box\" neural encoder and decoding\na single token as the answer like \"yes\" or \"no\". Despite this approach's strong\nquantitative results, it struggles to come up with intuitive, human-readable\nforms of justification for the prediction process. To address this\ninsufficiency, we reformulate VQA as a full answer generation task, which\nrequires the model to justify its predictions in natural language. We propose\nLRTA [Look, Read, Think, Answer], a transparent neural-symbolic reasoning\nframework for visual question answering that solves the problem step-by-step\nlike humans and provides human-readable form of justification at each step.\nSpecifically, LRTA learns to first convert an image into a scene graph and\nparse a question into multiple reasoning instructions. It then executes the\nreasoning instructions one at a time by traversing the scene graph using a\nrecurrent neural-symbolic execution module. Finally, it generates a full answer\nto the given question with natural language justifications. Our experiments on\nGQA dataset show that LRTA outperforms the state-of-the-art model by a large\nmargin (43.1% v.s. 28.0%) on the full answer generation task. We also create a\nperturbed GQA test set by removing linguistic cues (attributes and relations)\nin the questions for analyzing whether a model is having a smart guess with\nsuperficial data correlations. We show that LRTA makes a step towards truly\nunderstanding the question while the state-of-the-art model tends to learn\nsuperficial correlations from the training data.\n",
        "published": "2020",
        "authors": [
            "Weixin Liang",
            "Feiyang Niu",
            "Aishwarya Reganti",
            "Govind Thattai",
            "Gokhan Tur"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.11603v2",
        "title": "Interpretable Visual Reasoning via Induced Symbolic Space",
        "abstract": "  We study the problem of concept induction in visual reasoning, i.e.,\nidentifying concepts and their hierarchical relationships from question-answer\npairs associated with images; and achieve an interpretable model via working on\nthe induced symbolic concept space. To this end, we first design a new\nframework named object-centric compositional attention model (OCCAM) to perform\nthe visual reasoning task with object-level visual features. Then, we come up\nwith a method to induce concepts of objects and relations using clues from the\nattention patterns between objects' visual features and question words.\nFinally, we achieve a higher level of interpretability by imposing OCCAM on the\nobjects represented in the induced symbolic concept space. Our model design\nmakes this an easy adaption via first predicting the concepts of objects and\nrelations and then projecting the predicted concepts back to the visual feature\nspace so the compositional reasoning module can process normally. Experiments\non the CLEVR and GQA datasets demonstrate: 1) our OCCAM achieves a new state of\nthe art without human-annotated functional programs; 2) our induced concepts\nare both accurate and sufficient as OCCAM achieves an on-par performance on\nobjects represented either in visual features or in the induced symbolic\nconcept space.\n",
        "published": "2020",
        "authors": [
            "Zhonghao Wang",
            "Kai Wang",
            "Mo Yu",
            "Jinjun Xiong",
            "Wen-mei Hwu",
            "Mark Hasegawa-Johnson",
            "Humphrey Shi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.02813v1",
        "title": "Cross-Modal Generalization: Learning in Low Resource Modalities via\n  Meta-Alignment",
        "abstract": "  The natural world is abundant with concepts expressed via visual, acoustic,\ntactile, and linguistic modalities. Much of the existing progress in multimodal\nlearning, however, focuses primarily on problems where the same set of\nmodalities are present at train and test time, which makes learning in\nlow-resource modalities particularly difficult. In this work, we propose\nalgorithms for cross-modal generalization: a learning paradigm to train a model\nthat can (1) quickly perform new tasks in a target modality (i.e.\nmeta-learning) and (2) doing so while being trained on a different source\nmodality. We study a key research question: how can we ensure generalization\nacross modalities despite using separate encoders for different source and\ntarget modalities? Our solution is based on meta-alignment, a novel method to\nalign representation spaces using strongly and weakly paired cross-modal data\nwhile ensuring quick generalization to new tasks across different modalities.\nWe study this problem on 3 classification tasks: text to image, image to audio,\nand text to speech. Our results demonstrate strong performance even when the\nnew target modality has only a few (1-10) labeled samples and in the presence\nof noisy labels, a scenario particularly prevalent in low-resource modalities.\n",
        "published": "2020",
        "authors": [
            "Paul Pu Liang",
            "Peter Wu",
            "Liu Ziyin",
            "Louis-Philippe Morency",
            "Ruslan Salakhutdinov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.08508v3",
        "title": "Attention over learned object embeddings enables complex visual\n  reasoning",
        "abstract": "  Neural networks have achieved success in a wide array of perceptual tasks but\noften fail at tasks involving both perception and higher-level reasoning. On\nthese more challenging tasks, bespoke approaches (such as modular symbolic\ncomponents, independent dynamics models or semantic parsers) targeted towards\nthat specific type of task have typically performed better. The downside to\nthese targeted approaches, however, is that they can be more brittle than\ngeneral-purpose neural networks, requiring significant modification or even\nredesign according to the particular task at hand. Here, we propose a more\ngeneral neural-network-based approach to dynamic visual reasoning problems that\nobtains state-of-the-art performance on three different domains, in each case\noutperforming bespoke modular approaches tailored specifically to the task. Our\nmethod relies on learned object-centric representations, self-attention and\nself-supervised dynamics learning, and all three elements together are required\nfor strong performance to emerge. The success of this combination suggests that\nthere may be no need to trade off flexibility for performance on problems\ninvolving spatio-temporal or causal-style reasoning. With the right soft biases\nand learning objectives in a neural network we may be able to attain the best\nof both worlds.\n",
        "published": "2020",
        "authors": [
            "David Ding",
            "Felix Hill",
            "Adam Santoro",
            "Malcolm Reynolds",
            "Matt Botvinick"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.11587v1",
        "title": "Object-Centric Diagnosis of Visual Reasoning",
        "abstract": "  When answering questions about an image, it not only needs knowing what --\nunderstanding the fine-grained contents (e.g., objects, relationships) in the\nimage, but also telling why -- reasoning over grounding visual cues to derive\nthe answer for a question. Over the last few years, we have seen significant\nprogress on visual question answering. Though impressive as the accuracy grows,\nit still lags behind to get knowing whether these models are undertaking\ngrounding visual reasoning or just leveraging spurious correlations in the\ntraining data. Recently, a number of works have attempted to answer this\nquestion from perspectives such as grounding and robustness. However, most of\nthem are either focusing on the language side or coarsely studying the\npixel-level attention maps. In this paper, by leveraging the step-wise object\ngrounding annotations provided in the GQA dataset, we first present a\nsystematical object-centric diagnosis of visual reasoning on grounding and\nrobustness, particularly on the vision side. According to the extensive\ncomparisons across different models, we find that even models with high\naccuracy are not good at grounding objects precisely, nor robust to visual\ncontent perturbations. In contrast, symbolic and modular models have a\nrelatively better grounding and robustness, though at the cost of accuracy. To\nreconcile these different aspects, we further develop a diagnostic model,\nnamely Graph Reasoning Machine. Our model replaces purely symbolic visual\nrepresentation with probabilistic scene graph and then applies teacher-forcing\ntraining for the visual reasoning module. The designed model improves the\nperformance on all three metrics over the vanilla neural-symbolic model while\ninheriting the transparency. Further ablation studies suggest that this\nimprovement is mainly due to more accurate image understanding and proper\nintermediate reasoning supervisions.\n",
        "published": "2020",
        "authors": [
            "Jianwei Yang",
            "Jiayuan Mao",
            "Jiajun Wu",
            "Devi Parikh",
            "David D. Cox",
            "Joshua B. Tenenbaum",
            "Chuang Gan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.02779v2",
        "title": "Unifying Vision-and-Language Tasks via Text Generation",
        "abstract": "  Existing methods for vision-and-language learning typically require designing\ntask-specific architectures and objectives for each task. For example, a\nmulti-label answer classifier for visual question answering, a region scorer\nfor referring expression comprehension, and a language decoder for image\ncaptioning, etc. To alleviate these hassles, in this work, we propose a unified\nframework that learns different tasks in a single architecture with the same\nlanguage modeling objective, i.e., multimodal conditional text generation,\nwhere our models learn to generate labels in text based on the visual and\ntextual inputs. On 7 popular vision-and-language benchmarks, including visual\nquestion answering, referring expression comprehension, visual commonsense\nreasoning, most of which have been previously modeled as discriminative tasks,\nour generative approach (with a single unified architecture) reaches comparable\nperformance to recent task-specific state-of-the-art vision-and-language\nmodels. Moreover, our generative approach shows better generalization ability\non questions that have rare answers. Also, we show that our framework allows\nmulti-task learning in a single architecture with a single set of parameters,\nachieving similar performance to separately optimized single-task models. Our\ncode is publicly available at: https://github.com/j-min/VL-T5\n",
        "published": "2021",
        "authors": [
            "Jaemin Cho",
            "Jie Lei",
            "Hao Tan",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.08597v1",
        "title": "Beyond Fully-Connected Layers with Quaternions: Parameterization of\n  Hypercomplex Multiplications with $1/n$ Parameters",
        "abstract": "  Recent works have demonstrated reasonable success of representation learning\nin hypercomplex space. Specifically, \"fully-connected layers with Quaternions\"\n(4D hypercomplex numbers), which replace real-valued matrix multiplications in\nfully-connected layers with Hamilton products of Quaternions, both enjoy\nparameter savings with only 1/4 learnable parameters and achieve comparable\nperformance in various applications. However, one key caveat is that\nhypercomplex space only exists at very few predefined dimensions (4D, 8D, and\n16D). This restricts the flexibility of models that leverage hypercomplex\nmultiplications. To this end, we propose parameterizing hypercomplex\nmultiplications, allowing models to learn multiplication rules from data\nregardless of whether such rules are predefined. As a result, our method not\nonly subsumes the Hamilton product, but also learns to operate on any arbitrary\nnD hypercomplex space, providing more architectural flexibility using\narbitrarily $1/n$ learnable parameters compared with the fully-connected layer\ncounterpart. Experiments of applications to the LSTM and Transformer models on\nnatural language inference, machine translation, text style transfer, and\nsubject verb agreement demonstrate architectural flexibility and effectiveness\nof the proposed approach.\n",
        "published": "2021",
        "authors": [
            "Aston Zhang",
            "Yi Tay",
            "Shuai Zhang",
            "Alvin Chan",
            "Anh Tuan Luu",
            "Siu Cheung Hui",
            "Jie Fu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.08747v2",
        "title": "Learning Visual Models using a Knowledge Graph as a Trainer",
        "abstract": "  Traditional computer vision approaches, based on neural networks (NN), are\ntypically trained on a large amount of image data. By minimizing the\ncross-entropy loss between a prediction and a given class label, the NN and its\nvisual embedding space are learned to fulfill a given task. However, due to the\nsole dependence on the image data distribution of the training domain, these\nmodels tend to fail when applied to a target domain that differs from their\nsource domain. To learn a more robust NN to domain shifts, we propose the\nknowledge graph neural network (KG-NN), a neuro-symbolic approach that\nsupervises the training using image-data-invariant auxiliary knowledge. The\nauxiliary knowledge is first encoded in a knowledge graph with respective\nconcepts and their relationships, which is then transformed into a dense vector\nrepresentation via an embedding method. Using a contrastive loss function,\nKG-NN learns to adapt its visual embedding space and thus its weights according\nto the image-data invariant knowledge graph embedding space. We evaluate KG-NN\non visual transfer learning tasks for classification using the mini-ImageNet\ndataset and its derivatives, as well as road sign recognition datasets from\nGermany and China. The results show that a visual model trained with a\nknowledge graph as a trainer outperforms a model trained with cross-entropy in\nall experiments, in particular when the domain gap increases. Besides better\nperformance and stronger robustness to domain shifts, these KG-NN adapts to\nmultiple datasets and classes without suffering heavily from catastrophic\nforgetting.\n",
        "published": "2021",
        "authors": [
            "Sebastian Monka",
            "Lavdim Halilaj",
            "Stefan Schmid",
            "Achim Rettinger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.11318v1",
        "title": "Lie-Sensor: A Live Emotion Verifier or a Licensor for Chat Applications\n  using Emotional Intelligence",
        "abstract": "  Veracity is an essential key in research and development of innovative\nproducts. Live Emotion analysis and verification nullify deceit made to\ncomplainers on live chat, corroborate messages of both ends in messaging apps\nand promote an honest conversation between users. The main concept behind this\nemotion artificial intelligent verifier is to license or decline message\naccountability by comparing variegated emotions of chat app users recognized\nthrough facial expressions and text prediction. In this paper, a proposed\nemotion intelligent live detector acts as an honest arbiter who distributes\nfacial emotions into labels namely, Happiness, Sadness, Surprise, and Hate.\nFurther, it separately predicts a label of messages through text\nclassification. Finally, it compares both labels and declares the message as a\nfraud or a bonafide. For emotion detection, we deployed Convolutional Neural\nNetwork (CNN) using a miniXception model and for text prediction, we selected\nSupport Vector Machine (SVM) natural language processing probability classifier\ndue to receiving the best accuracy on training dataset after applying Support\nVector Machine (SVM), Random Forest Classifier, Naive Bayes Classifier, and\nLogistic regression.\n",
        "published": "2021",
        "authors": [
            "Falguni Patel",
            "NirmalKumar Patel",
            "Santosh Kumar Bharti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.00820v2",
        "title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded\n  Dialogues",
        "abstract": "  Compared to traditional visual question answering, video-grounded dialogues\nrequire additional reasoning over dialogue context to answer questions in a\nmulti-turn setting. Previous approaches to video-grounded dialogues mostly use\ndialogue context as a simple text input without modelling the inherent\ninformation flows at the turn level. In this paper, we propose a novel\nframework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers\ninformation flows among dialogue turns through a semantic graph constructed\nbased on lexical components in each question and answer. PDC model then learns\nto predict reasoning paths over this semantic graph. Our path prediction model\npredicts a path from the current turn through past dialogue turns that contain\nadditional visual cues to answer the current question. Our reasoning model\nsequentially processes both visual and textual information through this\nreasoning path and the propagated features are used to generate the answer. Our\nexperimental results demonstrate the effectiveness of our method and provide\nadditional insights on how models use semantic dependencies in a dialogue\ncontext to retrieve visual cues.\n",
        "published": "2021",
        "authors": [
            "Hung Le",
            "Nancy F. Chen",
            "Steven C. H. Hoi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.01075v1",
        "title": "OmniNet: Omnidirectional Representations from Transformers",
        "abstract": "  This paper proposes Omnidirectional Representations from Transformers\n(OmniNet). In OmniNet, instead of maintaining a strictly horizontal receptive\nfield, each token is allowed to attend to all tokens in the entire network.\nThis process can also be interpreted as a form of extreme or intensive\nattention mechanism that has the receptive field of the entire width and depth\nof the network. To this end, the omnidirectional attention is learned via a\nmeta-learner, which is essentially another self-attention based model. In order\nto mitigate the computationally expensive costs of full receptive field\nattention, we leverage efficient self-attention models such as kernel-based\n(Choromanski et al.), low-rank attention (Wang et al.) and/or Big Bird (Zaheer\net al.) as the meta-learner. Extensive experiments are conducted on\nautoregressive language modeling (LM1B, C4), Machine Translation, Long Range\nArena (LRA), and Image Recognition. The experiments show that OmniNet achieves\nconsiderable improvements across these tasks, including achieving\nstate-of-the-art performance on LM1B, WMT'14 En-De/En-Fr, and Long Range Arena.\nMoreover, using omnidirectional representation in Vision Transformers leads to\nsignificant improvements on image recognition tasks on both few-shot learning\nand fine-tuning setups.\n",
        "published": "2021",
        "authors": [
            "Yi Tay",
            "Mostafa Dehghani",
            "Vamsi Aribandi",
            "Jai Gupta",
            "Philip Pham",
            "Zhen Qin",
            "Dara Bahri",
            "Da-Cheng Juan",
            "Donald Metzler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.01209v4",
        "title": "Generative Adversarial Transformers",
        "abstract": "  We introduce the GANformer, a novel and efficient type of transformer, and\nexplore it for the task of visual generative modeling. The network employs a\nbipartite structure that enables long-range interactions across the image,\nwhile maintaining computation of linear efficiency, that can readily scale to\nhigh-resolution synthesis. It iteratively propagates information from a set of\nlatent variables to the evolving visual features and vice versa, to support the\nrefinement of each in light of the other and encourage the emergence of\ncompositional representations of objects and scenes. In contrast to the classic\ntransformer architecture, it utilizes multiplicative integration that allows\nflexible region-based modulation, and can thus be seen as a generalization of\nthe successful StyleGAN network. We demonstrate the model's strength and\nrobustness through a careful evaluation over a range of datasets, from\nsimulated multi-object environments to rich real-world indoor and outdoor\nscenes, showing it achieves state-of-the-art results in terms of image quality\nand diversity, while enjoying fast learning and better data-efficiency. Further\nqualitative and quantitative experiments offer us an insight into the model's\ninner workings, revealing improved interpretability and stronger\ndisentanglement, and illustrating the benefits and efficacy of our approach. An\nimplementation of the model is available at\nhttps://github.com/dorarad/gansformer.\n",
        "published": "2021",
        "authors": [
            "Drew A. Hudson",
            "C. Lawrence Zitnick"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.06450v3",
        "title": "Full Page Handwriting Recognition via Image to Sequence Extraction",
        "abstract": "  We present a Neural Network based Handwritten Text Recognition (HTR) model\narchitecture that can be trained to recognize full pages of handwritten or\nprinted text without image segmentation. Being based on Image to Sequence\narchitecture, it can extract text present in an image and then sequence it\ncorrectly without imposing any constraints regarding orientation, layout and\nsize of text and non-text. Further, it can also be trained to generate\nauxiliary markup related to formatting, layout and content. We use character\nlevel vocabulary, thereby enabling language and terminology of any subject. The\nmodel achieves a new state-of-art in paragraph level recognition on the IAM\ndataset. When evaluated on scans of real world handwritten free form test\nanswers - beset with curved and slanted lines, drawings, tables, math,\nchemistry and other symbols - it performs better than all commercially\navailable HTR cloud APIs. It is deployed in production as part of a commercial\nweb application.\n",
        "published": "2021",
        "authors": [
            "Sumeet S. Singh",
            "Sergey Karayev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.12975v1",
        "title": "VLGrammar: Grounded Grammar Induction of Vision and Language",
        "abstract": "  Cognitive grammar suggests that the acquisition of language grammar is\ngrounded within visual structures. While grammar is an essential representation\nof natural language, it also exists ubiquitously in vision to represent the\nhierarchical part-whole structure. In this work, we study grounded grammar\ninduction of vision and language in a joint learning framework. Specifically,\nwe present VLGrammar, a method that uses compound probabilistic context-free\ngrammars (compound PCFGs) to induce the language grammar and the image grammar\nsimultaneously. We propose a novel contrastive learning framework to guide the\njoint learning of both modules. To provide a benchmark for the grounded grammar\ninduction task, we collect a large-scale dataset, \\textsc{PartIt}, which\ncontains human-written sentences that describe part-level semantics for 3D\nobjects. Experiments on the \\textsc{PartIt} dataset show that VLGrammar\noutperforms all baselines in image grammar induction and language grammar\ninduction. The learned VLGrammar naturally benefits related downstream tasks.\nSpecifically, it improves the image unsupervised clustering accuracy by 30\\%,\nand performs well in image retrieval and text retrieval. Notably, the induced\ngrammar shows superior generalizability by easily generalizing to unseen\ncategories.\n",
        "published": "2021",
        "authors": [
            "Yining Hong",
            "Qing Li",
            "Song-Chun Zhu",
            "Siyuan Huang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.00676v1",
        "title": "Is Label Smoothing Truly Incompatible with Knowledge Distillation: An\n  Empirical Study",
        "abstract": "  This work aims to empirically clarify a recently discovered perspective that\nlabel smoothing is incompatible with knowledge distillation. We begin by\nintroducing the motivation behind on how this incompatibility is raised, i.e.,\nlabel smoothing erases relative information between teacher logits. We provide\na novel connection on how label smoothing affects distributions of semantically\nsimilar and dissimilar classes. Then we propose a metric to quantitatively\nmeasure the degree of erased information in sample's representation. After\nthat, we study its one-sidedness and imperfection of the incompatibility view\nthrough massive analyses, visualizations and comprehensive experiments on Image\nClassification, Binary Networks, and Neural Machine Translation. Finally, we\nbroadly discuss several circumstances wherein label smoothing will indeed lose\nits effectiveness. Project page:\nhttp://zhiqiangshen.com/projects/LS_and_KD/index.html.\n",
        "published": "2021",
        "authors": [
            "Zhiqiang Shen",
            "Zechun Liu",
            "Dejia Xu",
            "Zitian Chen",
            "Kwang-Ting Cheng",
            "Marios Savvides"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.00743v2",
        "title": "Towards General Purpose Vision Systems",
        "abstract": "  Computer vision systems today are primarily N-purpose systems, designed and\ntrained for a predefined set of tasks. Adapting such systems to new tasks is\nchallenging and often requires non-trivial modifications to the network\narchitecture (e.g. adding new output heads) or training process (e.g. adding\nnew losses). To reduce the time and expertise required to develop new\napplications, we would like to create general purpose vision systems that can\nlearn and perform a range of tasks without any modification to the architecture\nor learning process.\n  In this paper, we propose GPV-1, a task-agnostic vision-language architecture\nthat can learn and perform tasks that involve receiving an image and producing\ntext and/or bounding boxes, including classification, localization, visual\nquestion answering, captioning, and more. We also propose evaluations of\ngenerality of architecture, skill-concept transfer, and learning efficiency\nthat may inform future work on general purpose vision. Our experiments indicate\nGPV-1 is effective at multiple tasks, reuses some concept knowledge across\ntasks, can perform the Referring Expressions task zero-shot, and further\nimproves upon the zero-shot performance using a few training samples.\n",
        "published": "2021",
        "authors": [
            "Tanmay Gupta",
            "Amita Kamath",
            "Aniruddha Kembhavi",
            "Derek Hoiem"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.08773v4",
        "title": "Cross-Task Generalization via Natural Language Crowdsourcing\n  Instructions",
        "abstract": "  Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. Despite the success of the conventional supervised learning on\nindividual datasets, such models often struggle with generalization across\ntasks (e.g., a question-answering system cannot solve classification tasks). A\nlong-standing challenge in AI is to build a model that learns a new task by\nunderstanding the human-readable instructions that define it. To study this, we\nintroduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their\nhuman-authored instructions, and 193k task instances (input-output pairs). The\ninstructions are obtained from crowdsourcing instructions used to create\nexisting NLP datasets and mapped to a unified schema. Using this meta-dataset,\nwe measure cross-task generalization by training models on seen tasks and\nmeasuring generalization to the remaining unseen ones. We adopt generative\npre-trained language models to encode task-specific instructions along with\ninput and generate task output. Our results indicate that models benefit from\ninstructions when evaluated in terms of generalization to unseen tasks (19%\nbetter for models utilizing instructions). These models, however, are far\nbehind an estimated performance upperbound indicating significant room for more\nprogress in this direction.\n",
        "published": "2021",
        "authors": [
            "Swaroop Mishra",
            "Daniel Khashabi",
            "Chitta Baral",
            "Hannaneh Hajishirzi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.04570v3",
        "title": "BERT Learns to Teach: Knowledge Distillation with Meta Learning",
        "abstract": "  We present Knowledge Distillation with Meta Learning (MetaDistil), a simple\nyet effective alternative to traditional knowledge distillation (KD) methods\nwhere the teacher model is fixed during training. We show the teacher network\ncan learn to better transfer knowledge to the student network (i.e., learning\nto teach) with the feedback from the performance of the distilled student\nnetwork in a meta learning framework. Moreover, we introduce a pilot update\nmechanism to improve the alignment between the inner-learner and meta-learner\nin meta learning algorithms that focus on an improved inner-learner.\nExperiments on various benchmarks show that MetaDistil can yield significant\nimprovements compared with traditional KD algorithms and is less sensitive to\nthe choice of different student capacity and hyperparameters, facilitating the\nuse of KD on different tasks and models.\n",
        "published": "2021",
        "authors": [
            "Wangchunshu Zhou",
            "Canwen Xu",
            "Julian McAuley"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.00316v1",
        "title": "Chest ImaGenome Dataset for Clinical Reasoning",
        "abstract": "  Despite the progress in automatic detection of radiologic findings from chest\nX-ray (CXR) images in recent years, a quantitative evaluation of the\nexplainability of these models is hampered by the lack of locally labeled\ndatasets for different findings. With the exception of a few expert-labeled\nsmall-scale datasets for specific findings, such as pneumonia and pneumothorax,\nmost of the CXR deep learning models to date are trained on global \"weak\"\nlabels extracted from text reports, or trained via a joint image and\nunstructured text learning strategy. Inspired by the Visual Genome effort in\nthe computer vision community, we constructed the first Chest ImaGenome dataset\nwith a scene graph data structure to describe $242,072$ images. Local\nannotations are automatically produced using a joint rule-based natural\nlanguage processing (NLP) and atlas-based bounding box detection pipeline.\nThrough a radiologist constructed CXR ontology, the annotations for each CXR\nare connected as an anatomy-centered scene graph, useful for image-level\nreasoning and multimodal fusion applications. Overall, we provide: i) $1,256$\ncombinations of relation annotations between $29$ CXR anatomical locations\n(objects with bounding box coordinates) and their attributes, structured as a\nscene graph per image, ii) over $670,000$ localized comparison relations (for\nimproved, worsened, or no change) between the anatomical locations across\nsequential exams, as well as ii) a manually annotated gold standard scene graph\ndataset from $500$ unique patients.\n",
        "published": "2021",
        "authors": [
            "Joy T. Wu",
            "Nkechinyere N. Agu",
            "Ismini Lourentzou",
            "Arjun Sharma",
            "Joseph A. Paguio",
            "Jasper S. Yao",
            "Edward C. Dee",
            "William Mitchell",
            "Satyananda Kashyap",
            "Andrea Giovannini",
            "Leo A. Celi",
            "Mehdi Moradi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.04927v2",
        "title": "Embodied BERT: A Transformer Model for Embodied, Language-guided Visual\n  Task Completion",
        "abstract": "  Language-guided robots performing home and office tasks must navigate in and\ninteract with the world. Grounding language instructions against visual\nobservations and actions to take in an environment is an open challenge. We\npresent Embodied BERT (EmBERT), a transformer-based model which can attend to\nhigh-dimensional, multi-modal inputs across long temporal horizons for\nlanguage-conditioned task completion. Additionally, we bridge the gap between\nsuccessful object-centric navigation models used for non-interactive agents and\nthe language-guided visual task completion benchmark, ALFRED, by introducing\nobject navigation targets for EmBERT training. We achieve competitive\nperformance on the ALFRED benchmark, and EmBERT marks the first\ntransformer-based model to successfully handle the long-horizon, dense,\nmulti-modal histories of ALFRED, and the first ALFRED model to utilize\nobject-centric navigation targets.\n",
        "published": "2021",
        "authors": [
            "Alessandro Suglia",
            "Qiaozi Gao",
            "Jesse Thomason",
            "Govind Thattai",
            "Gaurav Sukhatme"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.00590v4",
        "title": "WebQA: Multihop and Multimodal QA",
        "abstract": "  Scaling Visual Question Answering (VQA) to the open-domain and multi-hop\nnature of web searches, requires fundamental advances in visual representation\nlearning, knowledge aggregation, and language generation. In this work, we\nintroduce WebQA, a challenging new benchmark that proves difficult for\nlarge-scale state-of-the-art models which lack language groundable visual\nrepresentations for novel objects and the ability to reason, yet trivial for\nhumans. WebQA mirrors the way humans use the web: 1) Ask a question, 2) Choose\nsources to aggregate, and 3) Produce a fluent language response. This is the\nbehavior we should be expecting from IoT devices and digital assistants.\nExisting work prefers to assume that a model can either reason about knowledge\nin images or in text. WebQA includes a secondary text-only QA task to ensure\nimproved visual performance does not come at the cost of language\nunderstanding. Our challenge for the community is to create unified multimodal\nreasoning models that answer questions regardless of the source modality,\nmoving us closer to digital assistants that not only query language knowledge,\nbut also the richer visual online world.\n",
        "published": "2021",
        "authors": [
            "Yingshan Chang",
            "Mridu Narang",
            "Hisami Suzuki",
            "Guihong Cao",
            "Jianfeng Gao",
            "Yonatan Bisk"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.05281v1",
        "title": "COSMic: A Coherence-Aware Generation Metric for Image Descriptions",
        "abstract": "  Developers of text generation models rely on automated evaluation metrics as\na stand-in for slow and expensive manual evaluations. However, image captioning\nmetrics have struggled to give accurate learned estimates of the semantic and\npragmatic success of output text. We address this weakness by introducing the\nfirst discourse-aware learned generation metric for evaluating image\ndescriptions. Our approach is inspired by computational theories of discourse\nfor capturing information goals using coherence. We present a dataset of\nimage$\\unicode{x2013}$description pairs annotated with coherence relations. We\nthen train a coherence-aware metric on a subset of the Conceptual Captions\ndataset and measure its effectiveness$\\unicode{x2014}$its ability to predict\nhuman ratings of output captions$\\unicode{x2014}$on a test set composed of\nout-of-domain images. We demonstrate a higher Kendall Correlation Coefficient\nfor our proposed metric with the human judgments for the results of a number of\nstate-of-the-art coherence-aware caption generation models when compared to\nseveral other metrics including recently proposed learned metrics such as\nBLEURT and BERTScore.\n",
        "published": "2021",
        "authors": [
            "Mert \u0130nan",
            "Piyush Sharma",
            "Baber Khalid",
            "Radu Soricut",
            "Matthew Stone",
            "Malihe Alikhani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.07867v1",
        "title": "Humanly Certifying Superhuman Classifiers",
        "abstract": "  Estimating the performance of a machine learning system is a longstanding\nchallenge in artificial intelligence research. Today, this challenge is\nespecially relevant given the emergence of systems which appear to increasingly\noutperform human beings. In some cases, this \"superhuman\" performance is\nreadily demonstrated; for example by defeating legendary human players in\ntraditional two player games. On the other hand, it can be challenging to\nevaluate classification models that potentially surpass human performance.\nIndeed, human annotations are often treated as a ground truth, which implicitly\nassumes the superiority of the human over any models trained on human\nannotations. In reality, human annotators can make mistakes and be subjective.\nEvaluating the performance with respect to a genuine oracle may be more\nobjective and reliable, even when querying the oracle is expensive or\nimpossible. In this paper, we first raise the challenge of evaluating the\nperformance of both humans and models with respect to an oracle which is\nunobserved. We develop a theory for estimating the accuracy compared to the\noracle, using only imperfect human annotations for reference. Our analysis\nprovides a simple recipe for detecting and certifying superhuman performance in\nthis setting, which we believe will assist in understanding the stage of\ncurrent research on classification. We validate the convergence of the bounds\nand the assumptions of our theory on carefully designed toy experiments with\nknown oracles. Moreover, we demonstrate the utility of our theory by\nmeta-analyzing large-scale natural language processing tasks, for which an\noracle does not exist, and show that under our assumptions a number of models\nfrom recent years are with high probability superhuman.\n",
        "published": "2021",
        "authors": [
            "Qiongkai Xu",
            "Christian Walder",
            "Chenchen Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.10686v2",
        "title": "Scale Efficiently: Insights from Pre-training and Fine-tuning\n  Transformers",
        "abstract": "  There remain many open questions pertaining to the scaling behaviour of\nTransformer architectures. These scaling decisions and findings can be\ncritical, as training runs often come with an associated computational cost\nwhich have both financial and/or environmental impact. The goal of this paper\nis to present scaling insights from pretraining and finetuning Transformers.\nWhile Kaplan et al. presents a comprehensive study of the scaling behaviour of\nTransformer language models, the scope is only on the upstream (pretraining)\nloss. Therefore, it is still unclear if these set of findings transfer to\ndownstream task within the context of the pretrain-finetune paradigm. The key\nfindings of this paper are as follows: (1) we show that aside from only the\nmodel size, model shape matters for downstream fine-tuning, (2) scaling\nprotocols operate differently at different compute regions, (3) widely adopted\nT5-base and T5-large sizes are Pareto-inefficient. To this end, we present\nimproved scaling protocols whereby our redesigned models achieve similar\ndownstream fine-tuning quality while having 50\\% fewer parameters and training\n40\\% faster compared to the widely adopted T5-base model. We publicly release\nover 100 pretrained checkpoints of different T5 configurations to facilitate\nfuture research and analysis.\n",
        "published": "2021",
        "authors": [
            "Yi Tay",
            "Mostafa Dehghani",
            "Jinfeng Rao",
            "William Fedus",
            "Samira Abnar",
            "Hyung Won Chung",
            "Sharan Narang",
            "Dani Yogatama",
            "Ashish Vaswani",
            "Donald Metzler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.13916v5",
        "title": "Unsolved Problems in ML Safety",
        "abstract": "  Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), reducing inherent model hazards\n(\"Alignment\"), and reducing systemic hazards (\"Systemic Safety\"). Throughout,\nwe clarify each problem's motivation and provide concrete research directions.\n",
        "published": "2021",
        "authors": [
            "Dan Hendrycks",
            "Nicholas Carlini",
            "John Schulman",
            "Jacob Steinhardt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.05877v1",
        "title": "OpenHands: Making Sign Language Recognition Accessible with Pose-based\n  Pretrained Models across Languages",
        "abstract": "  AI technologies for Natural Languages have made tremendous progress recently.\nHowever, commensurate progress has not been made on Sign Languages, in\nparticular, in recognizing signs as individual words or as complete sentences.\nWe introduce OpenHands, a library where we take four key ideas from the NLP\ncommunity for low-resource languages and apply them to sign languages for\nword-level recognition. First, we propose using pose extracted through\npretrained models as the standard modality of data to reduce training time and\nenable efficient inference, and we release standardized pose datasets for 6\ndifferent sign languages - American, Argentinian, Chinese, Greek, Indian, and\nTurkish. Second, we train and release checkpoints of 4 pose-based isolated sign\nlanguage recognition models across all 6 languages, providing baselines and\nready checkpoints for deployment. Third, to address the lack of labelled data,\nwe propose self-supervised pretraining on unlabelled data. We curate and\nrelease the largest pose-based pretraining dataset on Indian Sign Language\n(Indian-SL). Fourth, we compare different pretraining strategies and for the\nfirst time establish that pretraining is effective for sign language\nrecognition by demonstrating (a) improved fine-tuning performance especially in\nlow-resource settings, and (b) high crosslingual transfer from Indian-SL to few\nother sign languages. We open-source all models and datasets in OpenHands with\na hope that it makes research in sign languages more accessible, available here\nat https://github.com/AI4Bharat/OpenHands .\n",
        "published": "2021",
        "authors": [
            "Prem Selvaraj",
            "Gokul NC",
            "Pratyush Kumar",
            "Mitesh Khapra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.06537v6",
        "title": "Well-classified Examples are Underestimated in Classification with Deep\n  Neural Networks",
        "abstract": "  The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and margin growth. To counteract this deficiency, we propose to\nreward well-classified examples with additive bonuses to revive their\ncontribution to the learning process. This counterexample theoretically\naddresses these three issues. We empirically support this claim by directly\nverifying the theoretical results or significant performance improvement with\nour counterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that we\ncan deal with complex scenarios, such as imbalanced classification, OOD\ndetection, and applications under adversarial attacks because our idea can\nsolve these three issues. Code is available at:\nhttps://github.com/lancopku/well-classified-examples-are-underestimated.\n",
        "published": "2021",
        "authors": [
            "Guangxiang Zhao",
            "Wenkai Yang",
            "Xuancheng Ren",
            "Lei Li",
            "Yunfang Wu",
            "Xu Sun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.10834v1",
        "title": "Integrating Visuospatial, Linguistic and Commonsense Structure into\n  Story Visualization",
        "abstract": "  While much research has been done in text-to-image synthesis, little work has\nbeen done to explore the usage of linguistic structure of the input text. Such\ninformation is even more important for story visualization since its inputs\nhave an explicit narrative structure that needs to be translated into an image\nsequence (or visual story). Prior work in this domain has shown that there is\nample room for improvement in the generated image sequence in terms of visual\nquality, consistency and relevance. In this paper, we first explore the use of\nconstituency parse trees using a Transformer-based recurrent architecture for\nencoding structured input. Second, we augment the structured input with\ncommonsense information and study the impact of this external knowledge on the\ngeneration of visual story. Third, we also incorporate visual structure via\nbounding boxes and dense captioning to provide feedback about the\ncharacters/objects in generated images within a dual learning setup. We show\nthat off-the-shelf dense-captioning models trained on Visual Genome can improve\nthe spatial structure of images from a different target domain without needing\nfine-tuning. We train the model end-to-end using intra-story contrastive loss\n(between words and image sub-regions) and show significant improvements in\nseveral metrics (and human evaluation) for multiple datasets. Finally, we\nprovide an analysis of the linguistic and visuo-spatial information. Code and\ndata: https://github.com/adymaharana/VLCStoryGan.\n",
        "published": "2021",
        "authors": [
            "Adyasha Maharana",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.11205v3",
        "title": "Robustness through Data Augmentation Loss Consistency",
        "abstract": "  While deep learning through empirical risk minimization (ERM) has succeeded\nat achieving human-level performance at a variety of complex tasks, ERM is not\nrobust to distribution shifts or adversarial attacks. Synthetic data\naugmentation followed by empirical risk minimization (DA-ERM) is a simple and\nwidely used solution to improve robustness in ERM. In addition, consistency\nregularization can be applied to further improve the robustness of the model by\nforcing the representation of the original sample and the augmented one to be\nsimilar. However, existing consistency regularization methods are not\napplicable to covariant data augmentation, where the label in the augmented\nsample is dependent on the augmentation function. For example, dialog state\ncovaries with named entity when we augment data with a new named entity. In\nthis paper, we propose data augmented loss invariant regularization (DAIR), a\nsimple form of consistency regularization that is applied directly at the loss\nlevel rather than intermediate features, making it widely applicable to both\ninvariant and covariant data augmentation regardless of network architecture,\nproblem setup, and task. We apply DAIR to real-world learning problems\ninvolving covariant data augmentation: robust neural task-oriented dialog state\ntracking and robust visual question answering. We also apply DAIR to tasks\ninvolving invariant data augmentation: robust regression, robust classification\nagainst adversarial attacks, and robust ImageNet classification under\ndistribution shift. Our experiments show that DAIR consistently outperforms ERM\nand DA-ERM with little marginal computational cost and sets new\nstate-of-the-art results in several benchmarks involving covariant data\naugmentation. Our code of all experiments is available at:\nhttps://github.com/optimization-for-data-driven-science/DAIR.git\n",
        "published": "2021",
        "authors": [
            "Tianjian Huang",
            "Shaunak Halbe",
            "Chinnadhurai Sankar",
            "Pooyan Amini",
            "Satwik Kottur",
            "Alborz Geramifard",
            "Meisam Razaviyayn",
            "Ahmad Beirami"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.11403v1",
        "title": "SCENIC: A JAX Library for Computer Vision Research and Beyond",
        "abstract": "  Scenic is an open-source JAX library with a focus on Transformer-based models\nfor computer vision research and beyond. The goal of this toolkit is to\nfacilitate rapid experimentation, prototyping, and research of new vision\narchitectures and models. Scenic supports a diverse range of vision tasks\n(e.g., classification, segmentation, detection)and facilitates working on\nmulti-modal problems, along with GPU/TPU support for multi-host, multi-device\nlarge-scale training. Scenic also offers optimized implementations of\nstate-of-the-art research models spanning a wide range of modalities. Scenic\nhas been successfully used for numerous projects and published papers and\ncontinues serving as the library of choice for quick prototyping and\npublication of new research ideas.\n",
        "published": "2021",
        "authors": [
            "Mostafa Dehghani",
            "Alexey Gritsenko",
            "Anurag Arnab",
            "Matthias Minderer",
            "Yi Tay"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.13214v4",
        "title": "IconQA: A New Benchmark for Abstract Diagram Understanding and Visual\n  Language Reasoning",
        "abstract": "  Current visual question answering (VQA) tasks mainly consider answering\nhuman-annotated questions for natural images. However, aside from natural\nimages, abstract diagrams with semantic richness are still understudied in\nvisual understanding and reasoning research. In this work, we introduce a new\nchallenge of Icon Question Answering (IconQA) with the goal of answering a\nquestion in an icon image context. We release IconQA, a large-scale dataset\nthat consists of 107,439 questions and three sub-tasks: multi-image-choice,\nmulti-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by\nreal-world diagram word problems that highlight the importance of abstract\ndiagram understanding and comprehensive cognitive reasoning. Thus, IconQA\nrequires not only perception skills like object recognition and text\nunderstanding, but also diverse cognitive reasoning skills, such as geometric\nreasoning, commonsense reasoning, and arithmetic reasoning. To facilitate\npotential IconQA models to learn semantic representations for icon images, we\nfurther release an icon dataset Icon645 which contains 645,687 colored icons on\n377 classes. We conduct extensive user studies and blind experiments and\nreproduce a wide range of advanced VQA methods to benchmark the IconQA task.\nAlso, we develop a strong IconQA baseline Patch-TRM that applies a pyramid\ncross-modal Transformer with input diagram embeddings pre-trained on the icon\ndataset. IconQA and Icon645 are available at https://iconqa.github.io.\n",
        "published": "2021",
        "authors": [
            "Pan Lu",
            "Liang Qiu",
            "Jiaqi Chen",
            "Tony Xia",
            "Yizhou Zhao",
            "Wei Zhang",
            "Zhou Yu",
            "Xiaodan Liang",
            "Song-Chun Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.04138v1",
        "title": "Look at the Variance! Efficient Black-box Explanations with Sobol-based\n  Sensitivity Analysis",
        "abstract": "  We describe a novel attribution method which is grounded in Sensitivity\nAnalysis and uses Sobol indices. Beyond modeling the individual contributions\nof image regions, Sobol indices provide an efficient way to capture\nhigher-order interactions between image regions and their contributions to a\nneural network's prediction through the lens of variance. We describe an\napproach that makes the computation of these indices efficient for\nhigh-dimensional problems by using perturbation masks coupled with efficient\nestimators to handle the high dimensionality of images. Importantly, we show\nthat the proposed method leads to favorable scores on standard benchmarks for\nvision (and language models) while drastically reducing the computing time\ncompared to other black-box methods -- even surpassing the accuracy of\nstate-of-the-art white-box methods which require access to internal\nrepresentations. Our code is freely available:\nhttps://github.com/fel-thomas/Sobol-Attribution-Method\n",
        "published": "2021",
        "authors": [
            "Thomas Fel",
            "Remi Cadene",
            "Mathieu Chalvidal",
            "Matthieu Cord",
            "David Vigouroux",
            "Thomas Serre"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.04318v2",
        "title": "Auto-Encoding Knowledge Graph for Unsupervised Medical Report Generation",
        "abstract": "  Medical report generation, which aims to automatically generate a long and\ncoherent report of a given medical image, has been receiving growing research\ninterests. Existing approaches mainly adopt a supervised manner and heavily\nrely on coupled image-report pairs. However, in the medical domain, building a\nlarge-scale image-report paired dataset is both time-consuming and expensive.\nTo relax the dependency on paired data, we propose an unsupervised model\nKnowledge Graph Auto-Encoder (KGAE) which accepts independent sets of images\nand reports in training. KGAE consists of a pre-constructed knowledge graph, a\nknowledge-driven encoder and a knowledge-driven decoder. The knowledge graph\nworks as the shared latent space to bridge the visual and textual domains; The\nknowledge-driven encoder projects medical images and reports to the\ncorresponding coordinates in this latent space and the knowledge-driven decoder\ngenerates a medical report given a coordinate in this space. Since the\nknowledge-driven encoder and decoder can be trained with independent sets of\nimages and reports, KGAE is unsupervised. The experiments show that the\nunsupervised KGAE generates desirable medical reports without using any\nimage-report training pairs. Moreover, KGAE can also work in both\nsemi-supervised and supervised settings, and accept paired images and reports\nin training. By further fine-tuning with image-report pairs, KGAE consistently\noutperforms the current state-of-the-art models on two datasets.\n",
        "published": "2021",
        "authors": [
            "Fenglin Liu",
            "Chenyu You",
            "Xian Wu",
            "Shen Ge",
            "Sheng Wang",
            "Xu Sun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.06812v5",
        "title": "Sci-Net: Scale Invariant Model for Buildings Segmentation from Aerial\n  Imagery",
        "abstract": "  Buildings' segmentation is a fundamental task in the field of earth\nobservation and aerial imagery analysis. Most existing deep learning-based\nmethods in the literature can be applied to a fixed or narrow-range spatial\nresolution imagery. In practical scenarios, users deal with a broad spectrum of\nimage resolutions. Thus, a given aerial image often needs to be re-sampled to\nmatch the spatial resolution of the dataset used to train the deep learning\nmodel, which results in a degradation in segmentation performance. To overcome\nthis challenge, we propose, in this manuscript, Scale-invariant Neural Network\n(Sci-Net) architecture that segments buildings from wide-range spatial\nresolution aerial images. Specifically, our approach leverages UNet\nhierarchical representation and Dense Atrous Spatial Pyramid Pooling to extract\nfine-grained multi-scale representations. Sci-Net significantly outperforms\nstate of the art models on the Open Cities AI and the Multi-Scale Building\ndatasets with a steady improvement margin across different spatial resolutions.\n",
        "published": "2021",
        "authors": [
            "Hasan Nasrallah",
            "Mustafa Shukor",
            "Ali J. Ghandour"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.07074v3",
        "title": "Memotion Analysis through the Lens of Joint Embedding",
        "abstract": "  Joint embedding (JE) is a way to encode multi-modal data into a vector space\nwhere text remains as the grounding key and other modalities like image are to\nbe anchored with such keys. Meme is typically an image with embedded text onto\nit. Although, memes are commonly used for fun, they could also be used to\nspread hate and fake information. That along with its growing ubiquity over\nseveral social platforms has caused automatic analysis of memes to become a\nwidespread topic of research. In this paper, we report our initial experiments\non Memotion Analysis problem through joint embeddings. Results are marginally\nyielding SOTA.\n",
        "published": "2021",
        "authors": [
            "Nethra Gunti",
            "Sathyanarayanan Ramamoorthy",
            "Parth Patwa",
            "Amitava Das"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.07228v1",
        "title": "Curriculum Learning for Vision-and-Language Navigation",
        "abstract": "  Vision-and-Language Navigation (VLN) is a task where an agent navigates in an\nembodied indoor environment under human instructions. Previous works ignore the\ndistribution of sample difficulty and we argue that this potentially degrade\ntheir agent performance. To tackle this issue, we propose a novel\ncurriculum-based training paradigm for VLN tasks that can balance human prior\nknowledge and agent learning progress about training samples. We develop the\nprinciple of curriculum design and re-arrange the benchmark Room-to-Room (R2R)\ndataset to make it suitable for curriculum training. Experiments show that our\nmethod is model-agnostic and can significantly improve the performance, the\ngeneralizability, and the training efficiency of current state-of-the-art\nnavigation agents without increasing model complexity.\n",
        "published": "2021",
        "authors": [
            "Jiwen Zhang",
            "Zhongyu Wei",
            "Jianqing Fan",
            "Jiajie Peng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.14309v1",
        "title": "A General Framework for Defending Against Backdoor Attacks via Influence\n  Graph",
        "abstract": "  In this work, we propose a new and general framework to defend against\nbackdoor attacks, inspired by the fact that attack triggers usually follow a\n\\textsc{specific} type of attacking pattern, and therefore, poisoned training\nexamples have greater impacts on each other during training. We introduce the\nnotion of the {\\it influence graph}, which consists of nodes and edges\nrespectively representative of individual training points and associated\npair-wise influences. The influence between a pair of training points\nrepresents the impact of removing one training point on the prediction of\nanother, approximated by the influence function \\citep{koh2017understanding}.\nMalicious training points are extracted by finding the maximum average\nsub-graph subject to a particular size. Extensive experiments on computer\nvision and natural language processing tasks demonstrate the effectiveness and\ngenerality of the proposed framework.\n",
        "published": "2021",
        "authors": [
            "Xiaofei Sun",
            "Jiwei Li",
            "Xiaoya Li",
            "Ziyao Wang",
            "Tianwei Zhang",
            "Han Qiu",
            "Fei Wu",
            "Chun Fan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.05136v1",
        "title": "PTR: A Benchmark for Part-based Conceptual, Relational, and Physical\n  Reasoning",
        "abstract": "  A critical aspect of human visual perception is the ability to parse visual\nscenes into individual objects and further into object parts, forming\npart-whole hierarchies. Such composite structures could induce a rich set of\nsemantic concepts and relations, thus playing an important role in the\ninterpretation and organization of visual signals as well as for the\ngeneralization of visual perception and reasoning. However, existing visual\nreasoning benchmarks mostly focus on objects rather than parts. Visual\nreasoning based on the full part-whole hierarchy is much more challenging than\nobject-centric reasoning due to finer-grained concepts, richer geometry\nrelations, and more complex physics. Therefore, to better serve for part-based\nconceptual, relational and physical reasoning, we introduce a new large-scale\ndiagnostic visual reasoning dataset named PTR. PTR contains around 70k RGBD\nsynthetic images with ground truth object and part level annotations regarding\nsemantic instance segmentation, color attributes, spatial and geometric\nrelationships, and certain physical properties such as stability. These images\nare paired with 700k machine-generated questions covering various types of\nreasoning types, making them a good testbed for visual reasoning models. We\nexamine several state-of-the-art visual reasoning models on this dataset and\nobserve that they still make many surprising mistakes in situations where\nhumans can easily infer the correct answer. We believe this dataset will open\nup new opportunities for part-based reasoning.\n",
        "published": "2021",
        "authors": [
            "Yining Hong",
            "Li Yi",
            "Joshua B. Tenenbaum",
            "Antonio Torralba",
            "Chuang Gan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.06825v2",
        "title": "VL-Adapter: Parameter-Efficient Transfer Learning for\n  Vision-and-Language Tasks",
        "abstract": "  Recently, fine-tuning language models pre-trained on large text corpora have\nprovided huge improvements on vision-and-language (V&L) tasks as well as on\npure language tasks. However, fine-tuning the entire parameter set of\npre-trained models becomes impractical since the model size is growing rapidly.\nHence, in this paper, we introduce adapter-based parameter-efficient transfer\nlearning techniques to V&L models such as VL-BART and VLT5. We evaluate our\nmethods in a unified multi-task setup on both image-text and video-text\nbenchmarks. For the image-text tasks, we use four diverse V&L datasets: VQAv2,\nGQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,\nHow2QA, TVC, and YC2C. With careful training and thorough experiments, we\nbenchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)\nagainst the standard full fine-tuning and the recently proposed prompt-tuning\napproach. We also enhance the efficiency and performance of adapters by sharing\ntheir weights to attain knowledge across tasks. Our results demonstrate that\ntraining the adapter with the weight-sharing technique (4.18% of total\nparameters for image-text tasks and 3.39% for video-text tasks) can match the\nperformance of fine-tuning the entire model. Lastly, we present a comprehensive\nanalysis including the combination of adapter and task-specific prompts and the\nimpact of V&L pre-training on adapters. Our code is available at:\nhttps://github.com/ylsung/VL_adapter.\n",
        "published": "2021",
        "authors": [
            "Yi-Lin Sung",
            "Jaemin Cho",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.13906v1",
        "title": "Does CLIP Benefit Visual Question Answering in the Medical Domain as\n  Much as it Does in the General Domain?",
        "abstract": "  Contrastive Language--Image Pre-training (CLIP) has shown remarkable success\nin learning with cross-modal supervision from extensive amounts of image--text\npairs collected online. Thus far, the effectiveness of CLIP has been\ninvestigated primarily in general-domain multimodal problems. This work\nevaluates the effectiveness of CLIP for the task of Medical Visual Question\nAnswering (MedVQA). To this end, we present PubMedCLIP, a fine-tuned version of\nCLIP for the medical domain based on PubMed articles. Our experiments are\nconducted on two MedVQA benchmark datasets and investigate two MedVQA methods,\nMEVF (Mixture of Enhanced Visual Features) and QCR (Question answering via\nConditional Reasoning). For each of these, we assess the merits of visual\nrepresentation learning using PubMedCLIP, the original CLIP, and\nstate-of-the-art MAML (Model-Agnostic Meta-Learning) networks pre-trained only\non visual data. We open source the code for our MedVQA pipeline and\npre-training PubMedCLIP. CLIP and PubMedCLIP achieve improvements in comparison\nto MAML's visual encoder. PubMedCLIP achieves the best results with gains in\nthe overall accuracy of up to 3%. Individual examples illustrate the strengths\nof PubMedCLIP in comparison to the previously widely used MAML networks. Visual\nrepresentation learning with language supervision in PubMedCLIP leads to\nnoticeable improvements for MedVQA. Our experiments reveal distributional\ndifferences in the two MedVQA benchmark datasets that have not been imparted in\nprevious work and cause different back-end visual encoders in PubMedCLIP to\nexhibit different behavior on these datasets. Moreover, we witness fundamental\nperformance differences of VQA in general versus medical domains.\n",
        "published": "2021",
        "authors": [
            "Sedigheh Eslami",
            "Gerard de Melo",
            "Christoph Meinel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.10890v4",
        "title": "One Student Knows All Experts Know: From Sparse to Dense",
        "abstract": "  Human education system trains one student by multiple experts.\nMixture-of-experts (MoE) is a powerful sparse architecture including multiple\nexperts. However, sparse MoE model is easy to overfit, hard to deploy, and not\nhardware-friendly for practitioners. In this work, inspired by the human\neducation model, we propose a novel task, knowledge integration, to obtain a\ndense student model (OneS) as knowledgeable as one sparse MoE. We investigate\nthis task by proposing a general training framework including knowledge\ngathering and knowledge distillation. Specifically, to gather key knowledge\nfrom different pre-trained experts, we first investigate four different\npossible knowledge gathering methods, \\ie summation, averaging, Top-K Knowledge\nGathering (Top-KG), and Singular Value Decomposition Knowledge Gathering\n(SVD-KG) proposed in this paper. We then refine the dense student model by\nknowledge distillation to offset the noise from gathering. On ImageNet, our\nOneS preserves $61.7\\%$ benefits from MoE and achieves $78.4\\%$ top-1 accuracy\nImageNet with only $15$M parameters. On four natural language processing\ndatasets, OneS obtains $88.2\\%$ MoE benefits and outperforms the best baseline\nby $51.7\\%$ using the same architecture and training data. In addition,\ncompared with the MoE counterpart, OneS can achieve $3.7 \\times$ inference\nspeedup due to less computation and hardware-friendly architecture.\n",
        "published": "2022",
        "authors": [
            "Fuzhao Xue",
            "Xiaoxin He",
            "Xiaozhe Ren",
            "Yuxuan Lou",
            "Yang You"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.11114v2",
        "title": "Natural Language Descriptions of Deep Visual Features",
        "abstract": "  Some neurons in deep networks specialize in recognizing highly specific\nperceptual, structural, or semantic features of inputs. In computer vision,\ntechniques exist for identifying neurons that respond to individual concept\ncategories like colors, textures, and object classes. But these techniques are\nlimited in scope, labeling only a small subset of neurons and behaviors in any\nnetwork. Is a richer characterization of neuron-level computation possible? We\nintroduce a procedure (called MILAN, for mutual-information-guided linguistic\nannotation of neurons) that automatically labels neurons with open-ended,\ncompositional, natural language descriptions. Given a neuron, MILAN generates a\ndescription by searching for a natural language string that maximizes pointwise\nmutual information with the image regions in which the neuron is active. MILAN\nproduces fine-grained descriptions that capture categorical, relational, and\nlogical structure in learned features. These descriptions obtain high agreement\nwith human-generated feature descriptions across a diverse set of model\narchitectures and tasks, and can aid in understanding and controlling learned\nmodels. We highlight three applications of natural language neuron\ndescriptions. First, we use MILAN for analysis, characterizing the distribution\nand importance of neurons selective for attribute, category, and relational\ninformation in vision models. Second, we use MILAN for auditing, surfacing\nneurons sensitive to human faces in datasets designed to obscure them. Finally,\nwe use MILAN for editing, improving robustness in an image classifier by\ndeleting neurons sensitive to text features spuriously correlated with class\nlabels.\n",
        "published": "2022",
        "authors": [
            "Evan Hernandez",
            "Sarah Schwettmann",
            "David Bau",
            "Teona Bagashvili",
            "Antonio Torralba",
            "Jacob Andreas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.11794v1",
        "title": "A Survey on Visual Transfer Learning using Knowledge Graphs",
        "abstract": "  Recent approaches of computer vision utilize deep learning methods as they\nperform quite well if training and testing domains follow the same underlying\ndata distribution. However, it has been shown that minor variations in the\nimages that occur when using these methods in the real world can lead to\nunpredictable errors. Transfer learning is the area of machine learning that\ntries to prevent these errors. Especially, approaches that augment image data\nusing auxiliary knowledge encoded in language embeddings or knowledge graphs\n(KGs) have achieved promising results in recent years. This survey focuses on\nvisual transfer learning approaches using KGs. KGs can represent auxiliary\nknowledge either in an underlying graph-structured schema or in a vector-based\nknowledge graph embedding. Intending to enable the reader to solve visual\ntransfer learning problems with the help of specific KG-DL configurations we\nstart with a description of relevant modeling structures of a KG of various\nexpressions, such as directed labeled graphs, hypergraphs, and hyper-relational\ngraphs. We explain the notion of feature extractor, while specifically\nreferring to visual and semantic features. We provide a broad overview of\nknowledge graph embedding methods and describe several joint training\nobjectives suitable to combine them with high dimensional visual embeddings.\nThe main section introduces four different categories on how a KG can be\ncombined with a DL pipeline: 1) Knowledge Graph as a Reviewer; 2) Knowledge\nGraph as a Trainee; 3) Knowledge Graph as a Trainer; and 4) Knowledge Graph as\na Peer. To help researchers find evaluation benchmarks, we provide an overview\nof generic KGs and a set of image processing datasets and benchmarks including\nvarious types of auxiliary knowledge. Last, we summarize related surveys and\ngive an outlook about challenges and open issues for future research.\n",
        "published": "2022",
        "authors": [
            "Sebastian Monka",
            "Lavdim Halilaj",
            "Achim Rettinger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.02013v1",
        "title": "DIME: Fine-grained Interpretations of Multimodal Models via Disentangled\n  Local Explanations",
        "abstract": "  The ability for a human to understand an Artificial Intelligence (AI) model's\ndecision-making process is critical in enabling stakeholders to visualize model\nbehavior, perform model debugging, promote trust in AI models, and assist in\ncollaborative human-AI decision-making. As a result, the research fields of\ninterpretable and explainable AI have gained traction within AI communities as\nwell as interdisciplinary scientists seeking to apply AI in their subject\nareas. In this paper, we focus on advancing the state-of-the-art in\ninterpreting multimodal models - a class of machine learning methods that\ntackle core challenges in representing and capturing interactions between\nheterogeneous data sources such as images, text, audio, and time-series data.\nMultimodal models have proliferated numerous real-world applications across\nhealthcare, robotics, multimedia, affective computing, and human-computer\ninteraction. By performing model disentanglement into unimodal contributions\n(UC) and multimodal interactions (MI), our proposed approach, DIME, enables\naccurate and fine-grained analysis of multimodal models while maintaining\ngenerality across arbitrary modalities, model architectures, and tasks. Through\na comprehensive suite of experiments on both synthetic and real-world\nmultimodal tasks, we show that DIME generates accurate disentangled\nexplanations, helps users of multimodal models gain a deeper understanding of\nmodel behavior, and presents a step towards debugging and improving these\nmodels for real-world deployment. Code for our experiments can be found at\nhttps://github.com/lvyiwei1/DIME.\n",
        "published": "2022",
        "authors": [
            "Yiwei Lyu",
            "Paul Pu Liang",
            "Zihao Deng",
            "Ruslan Salakhutdinov",
            "Louis-Philippe Morency"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.05465v1",
        "title": "LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text\n  Retrieval",
        "abstract": "  Dual encoders and cross encoders have been widely used for image-text\nretrieval. Between the two, the dual encoder encodes the image and text\nindependently followed by a dot product, while the cross encoder jointly feeds\nimage and text as the input and performs dense multi-modal fusion. These two\narchitectures are typically modeled separately without interaction. In this\nwork, we propose LoopITR, which combines them in the same network for joint\nlearning. Specifically, we let the dual encoder provide hard negatives to the\ncross encoder, and use the more discriminative cross encoder to distill its\npredictions back to the dual encoder. Both steps are efficiently performed\ntogether in the same model. Our work centers on empirical analyses of this\ncombined architecture, putting the main focus on the design of the distillation\nobjective. Our experimental results highlight the benefits of training the two\nencoders in the same network, and demonstrate that distillation can be quite\neffective with just a few hard negative examples. Experiments on two standard\ndatasets (Flickr30K and COCO) show our approach achieves state-of-the-art dual\nencoder performance when compared with approaches using a similar amount of\ndata.\n",
        "published": "2022",
        "authors": [
            "Jie Lei",
            "Xinlei Chen",
            "Ning Zhang",
            "Mengjiao Wang",
            "Mohit Bansal",
            "Tamara L. Berg",
            "Licheng Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.05557v2",
        "title": "Conditional Prompt Learning for Vision-Language Models",
        "abstract": "  With the rise of powerful pre-trained vision-language models like CLIP, it\nbecomes essential to investigate ways to adapt these models to downstream\ndatasets. A recently proposed method named Context Optimization (CoOp)\nintroduces the concept of prompt learning -- a recent trend in NLP -- to the\nvision domain for adapting pre-trained vision-language models. Specifically,\nCoOp turns context words in a prompt into a set of learnable vectors and, with\nonly a few labeled images for learning, can achieve huge improvements over\nintensively-tuned manual prompts. In our study we identify a critical problem\nof CoOp: the learned context is not generalizable to wider unseen classes\nwithin the same dataset, suggesting that CoOp overfits base classes observed\nduring training. To address the problem, we propose Conditional Context\nOptimization (CoCoOp), which extends CoOp by further learning a lightweight\nneural network to generate for each image an input-conditional token (vector).\nCompared to CoOp's static prompts, our dynamic prompts adapt to each instance\nand are thus less sensitive to class shift. Extensive experiments show that\nCoCoOp generalizes much better than CoOp to unseen classes, even showing\npromising transferability beyond a single dataset; and yields stronger domain\ngeneralization performance as well. Code is available at\nhttps://github.com/KaiyangZhou/CoOp.\n",
        "published": "2022",
        "authors": [
            "Kaiyang Zhou",
            "Jingkang Yang",
            "Chen Change Loy",
            "Ziwei Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.09161v3",
        "title": "How Many Data Samples is an Additional Instruction Worth?",
        "abstract": "  Recently introduced instruction-paradigm empowers non-expert users to\nleverage NLP resources by defining a new task in natural language.\nInstruction-tuned models have significantly outperformed multitask learning\nmodels (without instruction); however they are far from state-of-the-art\ntask-specific models. Conventional approaches to improve model performance via\ncreating datasets with large number of task instances or architectural changes\nin the model may not be feasible for non-expert users. However, they can write\nalternate instructions to represent an instruction task. Is\nInstruction-augmentation helpful? We augment a subset of tasks in the expanded\nversion of NATURAL INSTRUCTIONS with additional instructions and find that it\nsignificantly improves model performance (up to 35%), especially in the\nlow-data regime. Our results indicate that an additional instruction can be\nequivalent to ~200 data samples on average across tasks.\n",
        "published": "2022",
        "authors": [
            "Ravsehaj Singh Puri",
            "Swaroop Mishra",
            "Mihir Parmar",
            "Chitta Baral"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.12667v3",
        "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future\n  Directions",
        "abstract": "  A long-term goal of AI research is to build intelligent agents that can\ncommunicate with humans in natural language, perceive the environment, and\nperform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental\nand interdisciplinary research topic towards this goal, and receives increasing\nattention from natural language processing, computer vision, robotics, and\nmachine learning communities. In this paper, we review contemporary studies in\nthe emerging field of VLN, covering tasks, evaluation metrics, methods, etc.\nThrough structured analysis of current progress and challenges, we highlight\nthe limitations of current VLN and opportunities for future work. This paper\nserves as a thorough reference for the VLN research community.\n",
        "published": "2022",
        "authors": [
            "Jing Gu",
            "Eliana Stefani",
            "Qi Wu",
            "Jesse Thomason",
            "Xin Eric Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.14936v3",
        "title": "FedVLN: Privacy-preserving Federated Vision-and-Language Navigation",
        "abstract": "  Data privacy is a central problem for embodied agents that can perceive the\nenvironment, communicate with humans, and act in the real world. While helping\nhumans complete tasks, the agent may observe and process sensitive information\nof users, such as house environments, human activities, etc. In this work, we\nintroduce privacy-preserving embodied agent learning for the task of\nVision-and-Language Navigation (VLN), where an embodied agent navigates house\nenvironments by following natural language instructions. We view each house\nenvironment as a local client, which shares nothing other than local updates\nwith the cloud server and other clients, and propose a novel federated\nvision-and-language navigation (FedVLN) framework to protect data privacy\nduring both training and pre-exploration. Particularly, we propose a\ndecentralized training strategy to limit the data of each client to its local\nmodel training and a federated pre-exploration method to do partial model\naggregation to improve model generalizability to unseen environments. Extensive\nresults on R2R and RxR datasets show that under our FedVLN framework,\ndecentralized VLN models achieve comparable results with centralized training\nwhile protecting seen environment privacy, and federated pre-exploration\nsignificantly outperforms centralized pre-exploration while preserving unseen\nenvironment privacy.\n",
        "published": "2022",
        "authors": [
            "Kaiwen Zhou",
            "Xin Eric Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.16639v1",
        "title": "FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic\n  descriptions, and Conceptual Relations",
        "abstract": "  We present a meta-learning framework for learning new visual concepts\nquickly, from just one or a few examples, guided by multiple naturally\noccurring data streams: simultaneously looking at images, reading sentences\nthat describe the objects in the scene, and interpreting supplemental sentences\nthat relate the novel concept with other concepts. The learned concepts support\ndownstream applications, such as answering questions by reasoning about unseen\nimages. Our model, namely FALCON, represents individual visual concepts, such\nas colors and shapes, as axis-aligned boxes in a high-dimensional space (the\n\"box embedding space\"). Given an input image and its paired sentence, our model\nfirst resolves the referential expression in the sentence and associates the\nnovel concept with particular objects in the scene. Next, our model interprets\nsupplemental sentences to relate the novel concept with other known concepts,\nsuch as \"X has property Y\" or \"X is a kind of Y\". Finally, it infers an optimal\nbox embedding for the novel concept that jointly 1) maximizes the likelihood of\nthe observed instances in the image, and 2) satisfies the relationships between\nthe novel concepts and the known ones. We demonstrate the effectiveness of our\nmodel on both synthetic and real-world datasets.\n",
        "published": "2022",
        "authors": [
            "Lingjie Mei",
            "Jiayuan Mao",
            "Ziqi Wang",
            "Chuang Gan",
            "Joshua B. Tenenbaum"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.17247v3",
        "title": "VL-InterpreT: An Interactive Visualization Tool for Interpreting\n  Vision-Language Transformers",
        "abstract": "  Breakthroughs in transformer-based models have revolutionized not only the\nNLP field, but also vision and multimodal systems. However, although\nvisualization and interpretability tools have become available for NLP models,\ninternal mechanisms of vision and multimodal transformers remain largely\nopaque. With the success of these transformers, it is increasingly critical to\nunderstand their inner workings, as unraveling these black-boxes will lead to\nmore capable and trustworthy models. To contribute to this quest, we propose\nVL-InterpreT, which provides novel interactive visualizations for interpreting\nthe attentions and hidden representations in multimodal transformers.\nVL-InterpreT is a task agnostic and integrated tool that (1) tracks a variety\nof statistics in attention heads throughout all layers for both vision and\nlanguage components, (2) visualizes cross-modal and intra-modal attentions\nthrough easily readable heatmaps, and (3) plots the hidden representations of\nvision and language tokens as they pass through the transformer layers. In this\npaper, we demonstrate the functionalities of VL-InterpreT through the analysis\nof KD-VLP, an end-to-end pretraining vision-language multimodal\ntransformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and\nWebQA, two visual question answering benchmarks. Furthermore, we also present a\nfew interesting findings about multimodal transformer behaviors that were\nlearned through our tool.\n",
        "published": "2022",
        "authors": [
            "Estelle Aflalo",
            "Meng Du",
            "Shao-Yen Tseng",
            "Yongfei Liu",
            "Chenfei Wu",
            "Nan Duan",
            "Vasudev Lal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.00598v2",
        "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
        "abstract": "  Large pretrained (e.g., \"foundation\") models exhibit distinct capabilities\ndepending on the domain of data they are trained on. While these domains are\ngeneric, they may only barely overlap. For example, visual-language models\n(VLMs) are trained on Internet-scale image captions, but large language models\n(LMs) are further trained on Internet-scale text with no images (e.g.,\nspreadsheets, SAT questions, code). As a result, these models store different\nforms of commonsense knowledge across different domains. In this work, we show\nthat this diversity is symbiotic, and can be leveraged through Socratic Models\n(SMs): a modular framework in which multiple pretrained models may be composed\nzero-shot i.e., via multimodal-informed prompting, to exchange information with\neach other and capture new multimodal capabilities, without requiring\nfinetuning. With minimal engineering, SMs are not only competitive with\nstate-of-the-art zero-shot image captioning and video-to-text retrieval, but\nalso enable new applications such as (i) answering free-form questions about\negocentric video, (ii) engaging in multimodal assistive dialogue with people\n(e.g., for cooking recipes) by interfacing with external APIs and databases\n(e.g., web search), and (iii) robot perception and planning.\n",
        "published": "2022",
        "authors": [
            "Andy Zeng",
            "Maria Attarian",
            "Brian Ichter",
            "Krzysztof Choromanski",
            "Adrian Wong",
            "Stefan Welker",
            "Federico Tombari",
            "Aveek Purohit",
            "Michael Ryoo",
            "Vikas Sindhwani",
            "Johnny Lee",
            "Vincent Vanhoucke",
            "Pete Florence"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.02803v1",
        "title": "A Transformer-Based Contrastive Learning Approach for Few-Shot Sign\n  Language Recognition",
        "abstract": "  Sign language recognition from sequences of monocular images or 2D poses is a\nchallenging field, not only due to the difficulty to infer 3D information from\n2D data, but also due to the temporal relationship between the sequences of\ninformation. Additionally, the wide variety of signs and the constant need to\nadd new ones on production environments makes it infeasible to use traditional\nclassification techniques. We propose a novel Contrastive Transformer-based\nmodel, which demonstrate to learn rich representations from body key points\nsequences, allowing better comparison between vector embedding. This allows us\nto apply these techniques to perform one-shot or few-shot tasks, such as\nclassification and translation. The experiments showed that the model could\ngeneralize well and achieved competitive results for sign classes never seen in\nthe training process.\n",
        "published": "2022",
        "authors": [
            "Silvan Ferreira",
            "Esdras Costa",
            "M\u00e1rcio Dahia",
            "Jampierre Rocha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.07660v1",
        "title": "It is Okay to Not Be Okay: Overcoming Emotional Bias in Affective Image\n  Captioning by Contrastive Data Collection",
        "abstract": "  Datasets that capture the connection between vision, language, and affection\nare limited, causing a lack of understanding of the emotional aspect of human\nintelligence. As a step in this direction, the ArtEmis dataset was recently\nintroduced as a large-scale dataset of emotional reactions to images along with\nlanguage explanations of these chosen emotions. We observed a significant\nemotional bias towards instance-rich emotions, making trained neural speakers\nless accurate in describing under-represented emotions. We show that collecting\nnew data, in the same way, is not effective in mitigating this emotional bias.\nTo remedy this problem, we propose a contrastive data collection approach to\nbalance ArtEmis with a new complementary dataset such that a pair of similar\nimages have contrasting emotions (one positive and one negative). We collected\n260,533 instances using the proposed method, we combine them with ArtEmis,\ncreating a second iteration of the dataset. The new combined dataset, dubbed\nArtEmis v2.0, has a balanced distribution of emotions with explanations\nrevealing more fine details in the associated painting. Our experiments show\nthat neural speakers trained on the new dataset improve CIDEr and METEOR\nevaluation metrics by 20% and 7%, respectively, compared to the biased dataset.\nFinally, we also show that the performance per emotion of neural speakers is\nimproved across all the emotion categories, significantly on under-represented\nemotions. The collected dataset and code are available at\nhttps://artemisdataset-v2.org.\n",
        "published": "2022",
        "authors": [
            "Youssef Mohamed",
            "Faizan Farooq Khan",
            "Kilichbek Haydarov",
            "Mohamed Elhoseiny"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.13749v2",
        "title": "Learning to Split for Automatic Bias Detection",
        "abstract": "  Classifiers are biased when trained on biased datasets. As a remedy, we\npropose Learning to Split (ls), an algorithm for automatic bias detection.\nGiven a dataset with input-label pairs, ls learns to split this dataset so that\npredictors trained on the training split cannot generalize to the testing\nsplit. This performance gap suggests that the testing split is\nunder-represented in the dataset, which is a signal of potential bias.\nIdentifying non-generalizable splits is challenging since we have no\nannotations about the bias. In this work, we show that the prediction\ncorrectness of each example in the testing split can be used as a source of\nweak supervision: generalization performance will drop if we move examples that\nare predicted correctly away from the testing split, leaving only those that\nare mis-predicted. ls is task-agnostic and can be applied to any supervised\nlearning problem, ranging from natural language understanding and image\nclassification to molecular property prediction. Empirical results show that ls\nis able to generate astonishingly challenging splits that correlate with\nhuman-identified biases. Moreover, we demonstrate that combining robust\nlearning algorithms (such as group DRO) with splits identified by ls enables\nautomatic de-biasing. Compared to previous state-of-the-art, we substantially\nimprove the worst-group performance (23.4% on average) when the source of\nbiases is unknown during training and validation.\n",
        "published": "2022",
        "authors": [
            "Yujia Bao",
            "Regina Barzilay"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.02937v1",
        "title": "Detection of Propaganda Techniques in Visuo-Lingual Metaphor in Memes",
        "abstract": "  The exponential rise of social media networks has allowed the production,\ndistribution, and consumption of data at a phenomenal rate. Moreover, the\nsocial media revolution has brought a unique phenomenon to social media\nplatforms called Internet memes. Internet memes are one of the most popular\ncontents used on social media, and they can be in the form of images with a\nwitty, catchy, or satirical text description. In this paper, we are dealing\nwith propaganda that is often seen in Internet memes in recent times.\nPropaganda is communication, which frequently includes psychological and\nrhetorical techniques to manipulate or influence an audience to act or respond\nas the propagandist wants. To detect propaganda in Internet memes, we propose a\nmultimodal deep learning fusion system that fuses the text and image feature\nrepresentations and outperforms individual models based solely on either text\nor image modalities.\n",
        "published": "2022",
        "authors": [
            "Sunil Gundapu",
            "Radhika Mamidi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.08383v1",
        "title": "Bias and Fairness on Multimodal Emotion Detection Algorithms",
        "abstract": "  Numerous studies have shown that machine learning algorithms can latch onto\nprotected attributes such as race and gender and generate predictions that\nsystematically discriminate against one or more groups. To date the majority of\nbias and fairness research has been on unimodal models. In this work, we\nexplore the biases that exist in emotion recognition systems in relationship to\nthe modalities utilized, and study how multimodal approaches affect system bias\nand fairness. We consider audio, text, and video modalities, as well as all\npossible multimodal combinations of those, and find that text alone has the\nleast bias, and accounts for the majority of the models' performances, raising\ndoubts about the worthiness of multimodal emotion recognition systems when bias\nand fairness are desired alongside model performance.\n",
        "published": "2022",
        "authors": [
            "Matheus Schmitz",
            "Rehan Ahmed",
            "Jimi Cao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.10505v3",
        "title": "A Study on Transformer Configuration and Training Objective",
        "abstract": "  Transformer-based models have delivered impressive results on many tasks,\nparticularly vision and language tasks. In many model training situations,\nconventional configurations are typically adopted. For example, we often set\nthe base model with hidden dimensions (i.e. model width) to be 768 and the\nnumber of transformer layers (i.e. model depth) to be 12. In this paper, we\nrevisit these conventional configurations. Through theoretical analysis and\nexperimental evaluation, we show that the masked autoencoder is effective in\nalleviating the over-smoothing issue in deep transformer training. Based on\nthis finding, we propose Bamboo, an idea of using deeper and narrower\ntransformer configurations, for masked autoencoder training. On ImageNet, with\nsuch a simple change in configuration, re-designed model achieves 87.1% top-1\naccuracy and outperforms SoTA models like MAE and BEiT. On language tasks,\nre-designed model outperforms BERT with default setting by 1.1 points on\naverage, on GLUE datasets.\n",
        "published": "2022",
        "authors": [
            "Fuzhao Xue",
            "Jianghai Chen",
            "Aixin Sun",
            "Xiaozhe Ren",
            "Zangwei Zheng",
            "Xiaoxin He",
            "Yongming Chen",
            "Xin Jiang",
            "Yang You"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.12191v2",
        "title": "Reassessing Evaluation Practices in Visual Question Answering: A Case\n  Study on Out-of-Distribution Generalization",
        "abstract": "  Vision-and-language (V&L) models pretrained on large-scale multimodal data\nhave demonstrated strong performance on various tasks such as image captioning\nand visual question answering (VQA). The quality of such models is commonly\nassessed by measuring their performance on unseen data that typically comes\nfrom the same distribution as the training data. However, when evaluated under\nout-of-distribution (out-of-dataset) settings for VQA, we observe that these\nmodels exhibit poor generalization. We comprehensively evaluate two pretrained\nV&L models under different settings (i.e. classification and open-ended text\ngeneration) by conducting cross-dataset evaluations. We find that these models\ntend to learn to solve the benchmark, rather than learning the high-level\nskills required by the VQA task. We also find that in most cases generative\nmodels are less susceptible to shifts in data distribution compared to\ndiscriminative ones, and that multimodal pretraining is generally helpful for\nOOD generalization. Finally, we revisit assumptions underlying the use of\nautomatic VQA evaluation metrics, and empirically show that their stringent\nnature repeatedly penalizes models for correct responses.\n",
        "published": "2022",
        "authors": [
            "Aishwarya Agrawal",
            "Ivana Kaji\u0107",
            "Emanuele Bugliarello",
            "Elnaz Davoodi",
            "Anita Gergely",
            "Phil Blunsom",
            "Aida Nematzadeh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.00621v2",
        "title": "Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal\n  Pre-training",
        "abstract": "  In this paper, we introduce Cross-View Language Modeling, a simple and\neffective pre-training framework that unifies cross-lingual and cross-modal\npre-training with shared architectures and objectives. Our approach is\nmotivated by a key observation that cross-lingual and cross-modal pre-training\nshare the same goal of aligning two different views of the same object into a\ncommon semantic space. To this end, the cross-view language modeling framework\nconsiders both multi-modal data (i.e., image-caption pairs) and multi-lingual\ndata (i.e., parallel sentence pairs) as two different views of the same object,\nand trains the model to align the two views by maximizing the mutual\ninformation between them with conditional masked language modeling and\ncontrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal Language\nModel, with the cross-view language modeling framework. Empirical results on\nIGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual image-text\nretrieval datasets show that while conceptually simpler, CCLM significantly\noutperforms the prior state-of-the-art with an average absolute improvement of\nover 10%. Moreover, CCLM is the first multi-lingual multi-modal pre-trained\nmodel that surpasses the translate-test performance of representative English\nvision-language models by zero-shot cross-lingual transfer.\n",
        "published": "2022",
        "authors": [
            "Yan Zeng",
            "Wangchunshu Zhou",
            "Ao Luo",
            "Ziming Cheng",
            "Xinsong Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.01720v1",
        "title": "Revisiting the \"Video\" in Video-Language Understanding",
        "abstract": "  What makes a video task uniquely suited for videos, beyond what can be\nunderstood from a single image? Building on recent progress in self-supervised\nimage-language models, we revisit this question in the context of video and\nlanguage tasks. We propose the atemporal probe (ATP), a new model for\nvideo-language analysis which provides a stronger bound on the baseline\naccuracy of multimodal models constrained by image-level understanding. By\napplying this model to standard discriminative video and language tasks, such\nas video question answering and text-to-video retrieval, we characterize the\nlimitations and potential of current video-language benchmarks. We find that\nunderstanding of event temporality is often not necessary to achieve strong or\nstate-of-the-art performance, even compared with recent large-scale\nvideo-language models and in contexts intended to benchmark deeper video-level\nunderstanding. We also demonstrate how ATP can improve both video-language\ndataset and model design. We describe a technique for leveraging ATP to better\ndisentangle dataset subsets with a higher concentration of temporally\nchallenging data, improving benchmarking efficacy for causal and temporal\nunderstanding. Further, we show that effectively integrating ATP into full\nvideo-level temporal models can improve efficiency and state-of-the-art\naccuracy.\n",
        "published": "2022",
        "authors": [
            "Shyamal Buch",
            "Crist\u00f3bal Eyzaguirre",
            "Adrien Gaidon",
            "Jiajun Wu",
            "Li Fei-Fei",
            "Juan Carlos Niebles"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.07137v3",
        "title": "Prioritized Training on Points that are Learnable, Worth Learning, and\n  Not Yet Learnt",
        "abstract": "  Training on web-scale data can take months. But most computation and time is\nwasted on redundant and noisy points that are already learnt or not learnable.\nTo accelerate training, we introduce Reducible Holdout Loss Selection\n(RHO-LOSS), a simple but principled technique which selects approximately those\npoints for training that most reduce the model's generalization loss. As a\nresult, RHO-LOSS mitigates the weaknesses of existing data selection methods:\ntechniques from the optimization literature typically select 'hard' (e.g. high\nloss) points, but such points are often noisy (not learnable) or less\ntask-relevant. Conversely, curriculum learning prioritizes 'easy' points, but\nsuch points need not be trained on once learned. In contrast, RHO-LOSS selects\npoints that are learnable, worth learning, and not yet learnt. RHO-LOSS trains\nin far fewer steps than prior art, improves accuracy, and speeds up training on\na wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and\nBERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in\n18x fewer steps and reaches 2% higher final accuracy than uniform data\nshuffling.\n",
        "published": "2022",
        "authors": [
            "S\u00f6ren Mindermann",
            "Jan Brauner",
            "Muhammed Razzak",
            "Mrinank Sharma",
            "Andreas Kirsch",
            "Winnie Xu",
            "Benedikt H\u00f6ltgen",
            "Aidan N. Gomez",
            "Adrien Morisot",
            "Sebastian Farquhar",
            "Yarin Gal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.08853v2",
        "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale\n  Knowledge",
        "abstract": "  Autonomous agents have made great strides in specialist domains like Atari\ngames and Go. However, they typically learn tabula rasa in isolated\nenvironments with limited and manually conceived objectives, thus failing to\ngeneralize across a wide spectrum of tasks and capabilities. Inspired by how\nhumans continually learn and adapt in the open world, we advocate a trinity of\ningredients for building generalist agents: 1) an environment that supports a\nmultitude of tasks and goals, 2) a large-scale database of multimodal\nknowledge, and 3) a flexible and scalable agent architecture. We introduce\nMineDojo, a new framework built on the popular Minecraft game that features a\nsimulation suite with thousands of diverse open-ended tasks and an\ninternet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and\nforum discussions. Using MineDojo's data, we propose a novel agent learning\nalgorithm that leverages large pre-trained video-language models as a learned\nreward function. Our agent is able to solve a variety of open-ended tasks\nspecified in free-form language without any manually designed dense shaping\nreward. We open-source the simulation suite, knowledge bases, algorithm\nimplementation, and pretrained models (https://minedojo.org) to promote\nresearch towards the goal of generally capable embodied agents.\n",
        "published": "2022",
        "authors": [
            "Linxi Fan",
            "Guanzhi Wang",
            "Yunfan Jiang",
            "Ajay Mandlekar",
            "Yuncong Yang",
            "Haoyi Zhu",
            "Andrew Tang",
            "De-An Huang",
            "Yuke Zhu",
            "Anima Anandkumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.09059v2",
        "title": "CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks",
        "abstract": "  Current state-of-the-art vision-and-language models are evaluated on tasks\neither individually or in a multi-task setting, overlooking the challenges of\ncontinually learning (CL) tasks as they arrive. Existing CL benchmarks have\nfacilitated research on task adaptation and mitigating \"catastrophic\nforgetting\", but are limited to vision-only and language-only tasks. We present\nCLiMB, a benchmark to study the challenge of learning multimodal tasks in a CL\nsetting, and to systematically evaluate how upstream continual learning can\nrapidly generalize to new multimodal and unimodal tasks. CLiMB includes\nimplementations of several CL algorithms and a modified Vision-Language\nTransformer (ViLT) model that can be deployed on both multimodal and unimodal\ntasks. We find that common CL methods can help mitigate forgetting during\nmultimodal task learning, but do not enable cross-task knowledge transfer. We\nenvision that CLiMB will facilitate research on a new class of CL algorithms\nfor this challenging multimodal setting.\n",
        "published": "2022",
        "authors": [
            "Tejas Srinivasan",
            "Ting-Yun Chang",
            "Leticia Leonor Pinto Alva",
            "Georgios Chochlakis",
            "Mohammad Rostami",
            "Jesse Thomason"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.02185v1",
        "title": "CLEAR: Improving Vision-Language Navigation with Cross-Lingual,\n  Environment-Agnostic Representations",
        "abstract": "  Vision-and-Language Navigation (VLN) tasks require an agent to navigate\nthrough the environment based on language instructions. In this paper, we aim\nto solve two key challenges in this task: utilizing multilingual instructions\nfor improved instruction-path grounding and navigating through new environments\nthat are unseen during training. To address these challenges, we propose CLEAR:\nCross-Lingual and Environment-Agnostic Representations. First, our agent learns\na shared and visually-aligned cross-lingual language representation for the\nthree languages (English, Hindi and Telugu) in the Room-Across-Room dataset.\nOur language representation learning is guided by text pairs that are aligned\nby visual information. Second, our agent learns an environment-agnostic visual\nrepresentation by maximizing the similarity between semantically-aligned image\npairs (with constraints on object-matching) from different environments. Our\nenvironment agnostic visual representation can mitigate the environment bias\ninduced by low-level visual information. Empirically, on the Room-Across-Room\ndataset, we show that our multilingual agent gets large improvements in all\nmetrics over the strong baseline model when generalizing to unseen environments\nwith the cross-lingual language representation and the environment-agnostic\nvisual representation. Furthermore, we show that our learned language and\nvisual representations can be successfully transferred to the Room-to-Room and\nCooperative Vision-and-Dialogue Navigation task, and present detailed\nqualitative and quantitative generalization and grounding analysis. Our code is\navailable at https://github.com/jialuli-luka/CLEAR\n",
        "published": "2022",
        "authors": [
            "Jialu Li",
            "Hao Tan",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.06991v2",
        "title": "Language Modelling with Pixels",
        "abstract": "  Language models are defined over a finite set of inputs, which creates a\nvocabulary bottleneck when we attempt to scale the number of supported\nlanguages. Tackling this bottleneck results in a trade-off between what can be\nrepresented in the embedding matrix and computational issues in the output\nlayer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which\nsuffers from neither of these issues. PIXEL is a pretrained language model that\nrenders text as images, making it possible to transfer representations across\nlanguages based on orthographic similarity or the co-activation of pixels.\nPIXEL is trained to reconstruct the pixels of masked patches instead of\npredicting a distribution over tokens. We pretrain the 86M parameter PIXEL\nmodel on the same English data as BERT and evaluate on syntactic and semantic\ntasks in typologically diverse languages, including various non-Latin scripts.\nWe find that PIXEL substantially outperforms BERT on syntactic and semantic\nprocessing tasks on scripts that are not found in the pretraining data, but\nPIXEL is slightly weaker than BERT when working with Latin scripts.\nFurthermore, we find that PIXEL is more robust than BERT to orthographic\nattacks and linguistic code-switching, further confirming the benefits of\nmodelling language with pixels.\n",
        "published": "2022",
        "authors": [
            "Phillip Rust",
            "Jonas F. Lotz",
            "Emanuele Bugliarello",
            "Elizabeth Salesky",
            "Miryam de Lhoneux",
            "Desmond Elliott"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.13242v1",
        "title": "Uncertainty-based Visual Question Answering: Estimating Semantic\n  Inconsistency between Image and Knowledge Base",
        "abstract": "  Knowledge-based visual question answering (KVQA) task aims to answer\nquestions that require additional external knowledge as well as an\nunderstanding of images and questions. Recent studies on KVQA inject an\nexternal knowledge in a multi-modal form, and as more knowledge is used,\nirrelevant information may be added and can confuse the question answering. In\norder to properly use the knowledge, this study proposes the following: 1) we\nintroduce a novel semantic inconsistency measure computed from caption\nuncertainty and semantic similarity; 2) we suggest a new external knowledge\nassimilation method based on the semantic inconsistency measure and apply it to\nintegrate explicit knowledge and implicit knowledge for KVQA; 3) the proposed\nmethod is evaluated with the OK-VQA dataset and achieves the state-of-the-art\nperformance.\n",
        "published": "2022",
        "authors": [
            "Jinyeong Chae",
            "Jihie Kim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.13243v6",
        "title": "Toward Transparent AI: A Survey on Interpreting the Inner Structures of\n  Deep Neural Networks",
        "abstract": "  The last decade of machine learning has seen drastic increases in scale and\ncapabilities. Deep neural networks (DNNs) are increasingly being deployed in\nthe real world. However, they are difficult to analyze, raising concerns about\nusing them without a rigorous understanding of how they function. Effective\ntools for interpreting them will be important for building more trustworthy AI\nby helping to identify problems, fix bugs, and improve basic understanding. In\nparticular, \"inner\" interpretability techniques, which focus on explaining the\ninternal components of DNNs, are well-suited for developing a mechanistic\nunderstanding, guiding manual modifications, and reverse engineering solutions.\n  Much recent work has focused on DNN interpretability, and rapid progress has\nthus far made a thorough systematization of methods difficult. In this survey,\nwe review over 300 works with a focus on inner interpretability tools. We\nintroduce a taxonomy that classifies methods by what part of the network they\nhelp to explain (weights, neurons, subnetworks, or latent representations) and\nwhether they are implemented during (intrinsic) or after (post hoc) training.\nTo our knowledge, we are also the first to survey a number of connections\nbetween interpretability research and work in adversarial robustness, continual\nlearning, modularity, network compression, and studying the human visual\nsystem. We discuss key challenges and argue that the status quo in\ninterpretability research is largely unproductive. Finally, we highlight the\nimportance of future work that emphasizes diagnostics, debugging, adversaries,\nand benchmarking in order to make interpretability tools more useful to\nengineers in practical applications.\n",
        "published": "2022",
        "authors": [
            "Tilman R\u00e4uker",
            "Anson Ho",
            "Stephen Casper",
            "Dylan Hadfield-Menell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.14525v1",
        "title": "Curriculum Learning for Data-Efficient Vision-Language Alignment",
        "abstract": "  Aligning image and text encoders from scratch using contrastive learning\nrequires large amounts of paired image-text data. We alleviate this need by\naligning individually pre-trained language and vision representation models\nusing a much smaller amount of paired data, augmented with a curriculum\nlearning algorithm to learn fine-grained vision-language alignments. TOnICS\n(Training with Ontology-Informed Contrastive Sampling) initially samples\nminibatches whose image-text pairs contain a wide variety of objects to learn\nobject-level alignment, and progressively samples minibatches where all\nimage-text pairs contain the same object to learn finer-grained contextual\nalignment. Aligning pre-trained BERT and VinVL models to each other using\nTOnICS outperforms CLIP on downstream zero-shot image retrieval while using\nless than 1% as much training data.\n",
        "published": "2022",
        "authors": [
            "Tejas Srinivasan",
            "Xiang Ren",
            "Jesse Thomason"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.00690v3",
        "title": "Generative Bias for Robust Visual Question Answering",
        "abstract": "  The task of Visual Question Answering (VQA) is known to be plagued by the\nissue of VQA models exploiting biases within the dataset to make its final\nprediction. Various previous ensemble based debiasing methods have been\nproposed where an additional model is purposefully trained to be biased in\norder to train a robust target model. However, these methods compute the bias\nfor a model simply from the label statistics of the training data or from\nsingle modal branches. In this work, in order to better learn the bias a target\nVQA model suffers from, we propose a generative method to train the bias model\ndirectly from the target model, called GenB. In particular, GenB employs a\ngenerative network to learn the bias in the target model through a combination\nof the adversarial objective and knowledge distillation. We then debias our\ntarget model with GenB as a bias model, and show through extensive experiments\nthe effects of our method on various VQA bias datasets including VQA-CP2,\nVQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT\narchitecture on VQA-CP2.\n",
        "published": "2022",
        "authors": [
            "Jae Won Cho",
            "Dong-jin Kim",
            "Hyeonggon Ryu",
            "In So Kweon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.04202v2",
        "title": "Analog Bits: Generating Discrete Data using Diffusion Models with\n  Self-Conditioning",
        "abstract": "  We present Bit Diffusion: a simple and generic approach for generating\ndiscrete data with continuous state and continuous time diffusion models. The\nmain idea behind our approach is to first represent the discrete data as binary\nbits, and then train a continuous diffusion model to model these bits as real\nnumbers which we call analog bits. To generate samples, the model first\ngenerates the analog bits, which are then thresholded to obtain the bits that\nrepresent the discrete variables. We further propose two simple techniques,\nnamely Self-Conditioning and Asymmetric Time Intervals, which lead to a\nsignificant improvement in sample quality. Despite its simplicity, the proposed\napproach can achieve strong performance in both discrete image generation and\nimage captioning tasks. For discrete image generation, we significantly improve\nprevious state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens)\nand ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the\nbest autoregressive model in both sample quality (measured by FID) and\nefficiency. For image captioning on MS-COCO dataset, our approach achieves\ncompetitive results compared to autoregressive models.\n",
        "published": "2022",
        "authors": [
            "Ting Chen",
            "Ruixiang Zhang",
            "Geoffrey Hinton"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.11253v1",
        "title": "FashionVQA: A Domain-Specific Visual Question Answering System",
        "abstract": "  Humans apprehend the world through various sensory modalities, yet language\nis their predominant communication channel. Machine learning systems need to\ndraw on the same multimodal richness to have informed discourses with humans in\nnatural language; this is particularly true for systems specialized in\nvisually-dense information, such as dialogue, recommendation, and search\nengines for clothing. To this end, we train a visual question answering (VQA)\nsystem to answer complex natural language questions about apparel in fashion\nphotoshoot images. The key to the successful training of our VQA model is the\nautomatic creation of a visual question-answering dataset with 168 million\nsamples from item attributes of 207 thousand images using diverse templates.\nThe sample generation employs a strategy that considers the difficulty of the\nquestion-answer pairs to emphasize challenging concepts. Contrary to the recent\ntrends in using several datasets for pretraining the visual question answering\nmodels, we focused on keeping the dataset fixed while training various models\nfrom scratch to isolate the improvements from model architecture changes. We\nsee that using the same transformer for encoding the question and decoding the\nanswer, as in language models, achieves maximum accuracy, showing that visual\nlanguage models (VLMs) make the best visual question answering systems for our\ndataset. The accuracy of the best model surpasses the human expert level, even\nwhen answering human-generated questions that are not confined to the template\nformats. Our approach for generating a large-scale multimodal domain-specific\ndataset provides a path for training specialized models capable of\ncommunicating in natural language. The training of such domain-expert models,\ne.g., our fashion VLM model, cannot rely solely on the large-scale\ngeneral-purpose datasets collected from the web.\n",
        "published": "2022",
        "authors": [
            "Min Wang",
            "Ata Mahjoubfar",
            "Anupama Joshi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.14610v3",
        "title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured\n  Mathematical Reasoning",
        "abstract": "  Mathematical reasoning, a core ability of human intelligence, presents unique\nchallenges for machines in abstract thinking and logical reasoning. Recent\nlarge pre-trained language models such as GPT-3 have achieved remarkable\nprogress on mathematical reasoning tasks written in text form, such as math\nword problems (MWP). However, it is unknown if the models can handle more\ncomplex problems that involve math reasoning over heterogeneous information,\nsuch as tabular data. To fill the gap, we present Tabular Math Word Problems\n(TabMWP), a new dataset containing 38,431 open-domain grade-level problems that\nrequire mathematical reasoning on both textual and tabular data. Each question\nin TabMWP is aligned with a tabular context, which is presented as an image,\nsemi-structured text, and a structured table. There are two types of questions:\nfree-text and multi-choice, and each problem is annotated with gold solutions\nto reveal the multi-step reasoning process. We evaluate different pre-trained\nmodels on TabMWP, including the GPT-3 model in a few-shot setting. As earlier\nstudies suggest, since few-shot GPT-3 relies on the selection of in-context\nexamples, its performance is unstable and can degrade to near chance. The\nunstable issue is more severe when handling complex problems like TabMWP. To\nmitigate this, we further propose a novel approach, PromptPG, which utilizes\npolicy gradient to learn to select in-context examples from a small amount of\ntraining data and then constructs the corresponding prompt for the test\nexample. Experimental results show that our method outperforms the best\nbaseline by 5.31% on the accuracy metric and reduces the prediction variance\nsignificantly compared to random selection, which verifies its effectiveness in\nselecting in-context examples.\n",
        "published": "2022",
        "authors": [
            "Pan Lu",
            "Liang Qiu",
            "Kai-Wei Chang",
            "Ying Nian Wu",
            "Song-Chun Zhu",
            "Tanmay Rajpurohit",
            "Peter Clark",
            "Ashwin Kalyan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.01191v1",
        "title": "Extending Compositional Attention Networks for Social Reasoning in\n  Videos",
        "abstract": "  We propose a novel deep architecture for the task of reasoning about social\ninteractions in videos. We leverage the multi-step reasoning capabilities of\nCompositional Attention Networks (MAC), and propose a multimodal extension\n(MAC-X). MAC-X is based on a recurrent cell that performs iterative mid-level\nfusion of input modalities (visual, auditory, text) over multiple reasoning\nsteps, by use of a temporal attention mechanism. We then combine MAC-X with\nLSTMs for temporal input processing in an end-to-end architecture. Our ablation\nstudies show that the proposed MAC-X architecture can effectively leverage\nmultimodal input cues using mid-level fusion mechanisms. We apply MAC-X to the\ntask of Social Video Question Answering in the Social IQ dataset and obtain a\n2.5% absolute improvement in terms of binary accuracy over the current\nstate-of-the-art.\n",
        "published": "2022",
        "authors": [
            "Christina Sartzetaki",
            "Georgios Paraskevopoulos",
            "Alexandros Potamianos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.01936v3",
        "title": "When and why vision-language models behave like bags-of-words, and what\n  to do about it?",
        "abstract": "  Despite the success of large vision and language models (VLMs) in many\ndownstream applications, it is unclear how well they encode compositional\ninformation. Here, we create the Attribution, Relation, and Order (ARO)\nbenchmark to systematically evaluate the ability of VLMs to understand\ndifferent types of relationships, attributes, and order. ARO consists of Visual\nGenome Attribution, to test the understanding of objects' properties; Visual\nGenome Relation, to test for relational understanding; and COCO &\nFlickr30k-Order, to test for order sensitivity. ARO is orders of magnitude\nlarger than previous benchmarks of compositionality, with more than 50,000 test\ncases. We show where state-of-the-art VLMs have poor relational understanding,\ncan blunder when linking objects to their attributes, and demonstrate a severe\nlack of order sensitivity. VLMs are predominantly trained and evaluated on\nlarge datasets with rich compositional structure in the images and captions.\nYet, training on these datasets has not been enough to address the lack of\ncompositional understanding, and evaluating on these datasets has failed to\nsurface this deficiency. To understand why these limitations emerge and are not\nrepresented in the standard tests, we zoom into the evaluation and training\nprocedures. We demonstrate that it is possible to perform well on retrieval\nover existing datasets without using the composition and order information.\nGiven that contrastive pretraining optimizes for retrieval on datasets with\nsimilar shortcuts, we hypothesize that this can explain why the models do not\nneed to learn to represent compositional information. This finding suggests a\nnatural solution: composition-aware hard negative mining. We show that a\nsimple-to-implement modification of contrastive learning significantly improves\nthe performance on tasks requiring understanding of order and compositionality.\n",
        "published": "2022",
        "authors": [
            "Mert Yuksekgonul",
            "Federico Bianchi",
            "Pratyusha Kalluri",
            "Dan Jurafsky",
            "James Zou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.03929v1",
        "title": "EgoTaskQA: Understanding Human Tasks in Egocentric Videos",
        "abstract": "  Understanding human tasks through video observations is an essential\ncapability of intelligent agents. The challenges of such capability lie in the\ndifficulty of generating a detailed understanding of situated actions, their\neffects on object states (i.e., state changes), and their causal dependencies.\nThese challenges are further aggravated by the natural parallelism from\nmulti-tasking and partial observations in multi-agent collaboration. Most prior\nworks leverage action localization or future prediction as an indirect metric\nfor evaluating such task understanding from videos. To make a direct\nevaluation, we introduce the EgoTaskQA benchmark that provides a single home\nfor the crucial dimensions of task understanding through question-answering on\nreal-world egocentric videos. We meticulously design questions that target the\nunderstanding of (1) action dependencies and effects, (2) intents and goals,\nand (3) agents' beliefs about others. These questions are divided into four\ntypes, including descriptive (what status?), predictive (what will?),\nexplanatory (what caused?), and counterfactual (what if?) to provide diagnostic\nanalyses on spatial, temporal, and causal understandings of goal-oriented\ntasks. We evaluate state-of-the-art video reasoning models on our benchmark and\nshow their significant gaps between humans in understanding complex\ngoal-oriented egocentric videos. We hope this effort will drive the vision\ncommunity to move onward with goal-oriented video understanding and reasoning.\n",
        "published": "2022",
        "authors": [
            "Baoxiong Jia",
            "Ting Lei",
            "Song-Chun Zhu",
            "Siyuan Huang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.04133v1",
        "title": "Adapting Pretrained Vision-Language Foundational Models to Medical\n  Imaging Domains",
        "abstract": "  Multi-modal foundation models are typically trained on millions of pairs of\nnatural images and text captions, frequently obtained through web-crawling\napproaches. Although such models depict excellent generative capabilities, they\ndo not typically generalize well to specific domains such as medical images\nthat have fundamentally shifted distributions compared to natural images.\nBuilding generative models for medical images that faithfully depict clinical\ncontext may help alleviate the paucity of healthcare datasets. Thus, in this\nstudy, we seek to research and expand the representational capabilities of\nlarge pretrained foundation models to medical concepts, specifically for\nleveraging the Stable Diffusion model to generate domain specific images found\nin medical imaging. We explore the sub-components of the Stable Diffusion\npipeline (the variational autoencoder, the U-Net and the text-encoder) to\nfine-tune the model to generate medical images. We benchmark the efficacy of\nthese efforts using quantitative image quality metrics and qualitative\nradiologist-driven evaluations that accurately represent the clinical content\nof conditional text prompts. Our best-performing model improves upon the stable\ndiffusion baseline and can be conditioned to insert a realistic-looking\nabnormality on a synthetic radiology image, while maintaining a 95% accuracy on\na classifier trained to detect the abnormality.\n",
        "published": "2022",
        "authors": [
            "Pierre Chambon",
            "Christian Bluethgen",
            "Curtis P. Langlotz",
            "Akshay Chaudhari"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.06791v1",
        "title": "SDW-ASL: A Dynamic System to Generate Large Scale Dataset for Continuous\n  American Sign Language",
        "abstract": "  Despite tremendous progress in natural language processing using deep\nlearning techniques in recent years, sign language production and comprehension\nhas advanced very little. One critical barrier is the lack of largescale\ndatasets available to the public due to the unbearable cost of labeled data\ngeneration. Efforts to provide public data for American Sign Language (ASL)\ncomprehension have yielded two datasets, comprising more than thousand video\nclips. These datasets are large enough to enable a meaningful start to deep\nlearning research on sign languages but are far too small to lead to any\nsolution that can be practically deployed. So far, there is still no suitable\ndataset for ASL production. We proposed a system that can generate large scale\nASL datasets for continuous ASL. It is suitable for general ASL processing and\nis particularly useful for ASL production. The continuous ASL dataset contains\nEnglish labeled human articulations in condensed body pose data formats. To\nbetter serve the research community, we are releasing the first version of our\nASL dataset, which contains 30k sentences, 416k words, a vocabulary of 18k\nwords, in a total of 104 hours. This is the largest continuous sign language\ndataset published to date in terms of video duration. We also describe a system\nthat can evolve and expand the dataset to incorporate better data processing\ntechniques and more contents when available. It is our hope that the release of\nthis ASL dataset and the sustainable dataset generation system to the public\nwill propel better deep-learning research in ASL natural language processing.\n",
        "published": "2022",
        "authors": [
            "Yehong Jiang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.07474v5",
        "title": "SQA3D: Situated Question Answering in 3D Scenes",
        "abstract": "  We propose a new task to benchmark scene understanding of embodied agents:\nSituated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g.,\n3D scan), SQA3D requires the tested agent to first understand its situation\n(position, orientation, etc.) in the 3D scene as described by text, then reason\nabout its surrounding environment and answer a question under that situation.\nBased upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k\nunique situations, along with 20.4k descriptions and 33.4k diverse reasoning\nquestions for these situations. These questions examine a wide spectrum of\nreasoning capabilities for an intelligent agent, ranging from spatial relation\ncomprehension to commonsense understanding, navigation, and multi-hop\nreasoning. SQA3D imposes a significant challenge to current multi-modal\nespecially 3D reasoning models. We evaluate various state-of-the-art approaches\nand find that the best one only achieves an overall score of 47.20%, while\namateur human participants can reach 90.06%. We believe SQA3D could facilitate\nfuture embodied AI research with stronger situation understanding and reasoning\ncapability.\n",
        "published": "2022",
        "authors": [
            "Xiaojian Ma",
            "Silong Yong",
            "Zilong Zheng",
            "Qing Li",
            "Yitao Liang",
            "Song-Chun Zhu",
            "Siyuan Huang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.07940v1",
        "title": "AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments",
        "abstract": "  Recent years have seen embodied visual navigation advance in two distinct\ndirections: (i) in equipping the AI agent to follow natural language\ninstructions, and (ii) in making the navigable world multimodal, e.g.,\naudio-visual navigation. However, the real world is not only multimodal, but\nalso often complex, and thus in spite of these advances, agents still need to\nunderstand the uncertainty in their actions and seek instructions to navigate.\nTo this end, we present AVLEN~ -- an interactive agent for\nAudio-Visual-Language Embodied Navigation. Similar to audio-visual navigation\ntasks, the goal of our embodied agent is to localize an audio event via\nnavigating the 3D visual world; however, the agent may also seek help from a\nhuman (oracle), where the assistance is provided in free-form natural language.\nTo realize these abilities, AVLEN uses a multimodal hierarchical reinforcement\nlearning backbone that learns: (a) high-level policies to choose either\naudio-cues for navigation or to query the oracle, and (b) lower-level policies\nto select navigation actions based on its audio-visual and language inputs. The\npolicies are trained via rewarding for the success on the navigation task while\nminimizing the number of queries to the oracle. To empirically evaluate AVLEN,\nwe present experiments on the SoundSpaces framework for semantic audio-visual\nnavigation tasks. Our results show that equipping the agent to ask for help\nleads to a clear improvement in performance, especially in challenging cases,\ne.g., when the sound is unheard during training or in the presence of\ndistractor sounds.\n",
        "published": "2022",
        "authors": [
            "Sudipta Paul",
            "Amit K. Roy-Chowdhury",
            "Anoop Cherian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.10209v2",
        "title": "Exclusive Supermask Subnetwork Training for Continual Learning",
        "abstract": "  Continual Learning (CL) methods focus on accumulating knowledge over time\nwhile avoiding catastrophic forgetting. Recently, Wortsman et al. (2020)\nproposed a CL method, SupSup, which uses a randomly initialized, fixed base\nnetwork (model) and finds a supermask for each new task that selectively keeps\nor removes each weight to produce a subnetwork. They prevent forgetting as the\nnetwork weights are not being updated. Although there is no forgetting, the\nperformance of SupSup is sub-optimal because fixed weights restrict its\nrepresentational power. Furthermore, there is no accumulation or transfer of\nknowledge inside the model when new tasks are learned. Hence, we propose\nExSSNeT (Exclusive Supermask SubNEtwork Training), that performs exclusive and\nnon-overlapping subnetwork weight training. This avoids conflicting updates to\nthe shared weights by subsequent tasks to improve performance while still\npreventing forgetting. Furthermore, we propose a novel KNN-based Knowledge\nTransfer (KKT) module that utilizes previously acquired knowledge to learn new\ntasks better and faster. We demonstrate that ExSSNeT outperforms strong\nprevious methods on both NLP and Vision domains while preventing forgetting.\nMoreover, ExSSNeT is particularly advantageous for sparse masks that activate\n2-10% of the model parameters, resulting in an average improvement of 8.3% over\nSupSup. Furthermore, ExSSNeT scales to a large number of tasks (100). Our code\nis available at https://github.com/prateeky2806/exessnet.\n",
        "published": "2022",
        "authors": [
            "Prateek Yadav",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.12737v1",
        "title": "RoentGen: Vision-Language Foundation Model for Chest X-ray Generation",
        "abstract": "  Multimodal models trained on large natural image-text pair datasets have\nexhibited astounding abilities in generating high-quality images. Medical\nimaging data is fundamentally different to natural images, and the language\nused to succinctly capture relevant details in medical data uses a different,\nnarrow but semantically rich, domain-specific vocabulary. Not surprisingly,\nmulti-modal models trained on natural image-text pairs do not tend to\ngeneralize well to the medical domain. Developing generative imaging models\nfaithfully representing medical concepts while providing compositional\ndiversity could mitigate the existing paucity of high-quality, annotated\nmedical imaging datasets. In this work, we develop a strategy to overcome the\nlarge natural-medical distributional shift by adapting a pre-trained latent\ndiffusion model on a corpus of publicly available chest x-rays (CXR) and their\ncorresponding radiology (text) reports. We investigate the model's ability to\ngenerate high-fidelity, diverse synthetic CXR conditioned on text prompts. We\nassess the model outputs quantitatively using image quality metrics, and\nevaluate image quality and text-image alignment by human domain experts. We\npresent evidence that the resulting model (RoentGen) is able to create visually\nconvincing, diverse synthetic CXR images, and that the output can be controlled\nto a new extent by using free-form text prompts including radiology-specific\nlanguage. Fine-tuning this model on a fixed training set and using it as a data\naugmentation method, we measure a 5% improvement of a classifier trained\njointly on synthetic and real images, and a 3% improvement when trained on a\nlarger but purely synthetic training set. Finally, we observe that this\nfine-tuning distills in-domain knowledge in the text-encoder and can improve\nits representation capabilities of certain diseases like pneumothorax by 25%.\n",
        "published": "2022",
        "authors": [
            "Pierre Chambon",
            "Christian Bluethgen",
            "Jean-Benoit Delbrouck",
            "Rogier Van der Sluijs",
            "Ma\u0142gorzata Po\u0142acin",
            "Juan Manuel Zambrano Chaves",
            "Tanishq Mathew Abraham",
            "Shivanshu Purohit",
            "Curtis P. Langlotz",
            "Akshay Chaudhari"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.04408v1",
        "title": "OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist\n  Models",
        "abstract": "  Generalist models, which are capable of performing diverse multi-modal tasks\nin a task-agnostic way within a single model, have been explored recently.\nBeing, hopefully, an alternative to approaching general-purpose AI, existing\ngeneralist models are still at an early stage, where modality and task coverage\nis limited. To empower multi-modal task-scaling and speed up this line of\nresearch, we release a generalist model learning system, OFASys, built on top\nof a declarative task interface named multi-modal instruction. At the core of\nOFASys is the idea of decoupling multi-modal task representations from the\nunderlying model implementations. In OFASys, a task involving multiple\nmodalities can be defined declaratively even with just a single line of code.\nThe system automatically generates task plans from such instructions for\ntraining and inference. It also facilitates multi-task training for diverse\nmulti-modal workloads. As a starting point, we provide presets of 7 different\nmodalities and 23 highly-diverse example tasks in OFASys, with which we also\ndevelop a first-in-kind, single model, OFA+, that can handle text, image,\nspeech, video, and motion data. The single OFA+ model achieves 95% performance\nin average with only 16% parameters of 15 task-finetuned models, showcasing the\nperformance reliability of multi-modal task-scaling provided by OFASys.\nAvailable at https://github.com/OFA-Sys/OFASys\n",
        "published": "2022",
        "authors": [
            "Jinze Bai",
            "Rui Men",
            "Hao Yang",
            "Xuancheng Ren",
            "Kai Dang",
            "Yichang Zhang",
            "Xiaohuan Zhou",
            "Peng Wang",
            "Sinan Tan",
            "An Yang",
            "Zeyu Cui",
            "Yu Han",
            "Shuai Bai",
            "Wenbin Ge",
            "Jianxin Ma",
            "Junyang Lin",
            "Jingren Zhou",
            "Chang Zhou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.13392v1",
        "title": "DeepCuts: Single-Shot Interpretability based Pruning for BERT",
        "abstract": "  As language models have grown in parameters and layers, it has become much\nharder to train and infer with them on single GPUs. This is severely\nrestricting the availability of large language models such as GPT-3,\nBERT-Large, and many others. A common technique to solve this problem is\npruning the network architecture by removing transformer heads, fully-connected\nweights, and other modules. The main challenge is to discern the important\nparameters from the less important ones. Our goal is to find strong metrics for\nidentifying such parameters. We thus propose two strategies: Cam-Cut based on\nthe GradCAM interpretations, and Smooth-Cut based on the SmoothGrad, for\ncalculating the importance scores. Through this work, we show that our scoring\nfunctions are able to assign more relevant task-based scores to the network\nparameters, and thus both our pruning approaches significantly outperform the\nstandard weight and gradient-based strategies, especially at higher compression\nratios in BERT-based models. We also analyze our pruning masks and find them to\nbe significantly different from the ones obtained using standard metrics.\n",
        "published": "2022",
        "authors": [
            "Jasdeep Singh Grover",
            "Bhavesh Gawri",
            "Ruskin Raj Manku"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.05226v1",
        "title": "See, Think, Confirm: Interactive Prompting Between Vision and Language\n  Models for Knowledge-based Visual Reasoning",
        "abstract": "  Large pre-trained vision and language models have demonstrated remarkable\ncapacities for various tasks. However, solving the knowledge-based visual\nreasoning tasks remains challenging, which requires a model to comprehensively\nunderstand image content, connect the external world knowledge, and perform\nstep-by-step reasoning to answer the questions correctly. To this end, we\npropose a novel framework named Interactive Prompting Visual Reasoner (IPVR)\nfor few-shot knowledge-based visual reasoning. IPVR contains three stages, see,\nthink and confirm. The see stage scans the image and grounds the visual concept\ncandidates with a visual perception model. The think stage adopts a pre-trained\nlarge language model (LLM) to attend to the key concepts from candidates\nadaptively. It then transforms them into text context for prompting with a\nvisual captioning model and adopts the LLM to generate the answer. The confirm\nstage further uses the LLM to generate the supporting rationale to the answer,\nverify the generated rationale with a cross-modality classifier and ensure that\nthe rationale can infer the predicted output consistently. We conduct\nexperiments on a range of knowledge-based visual reasoning datasets. We found\nour IPVR enjoys several benefits, 1). it achieves better performance than the\nprevious few-shot learning baselines; 2). it enjoys the total transparency and\ntrustworthiness of the whole reasoning process by providing rationales for each\nreasoning step; 3). it is computation-efficient compared with other fine-tuning\nbaselines.\n",
        "published": "2023",
        "authors": [
            "Zhenfang Chen",
            "Qinhong Zhou",
            "Yikang Shen",
            "Yining Hong",
            "Hao Zhang",
            "Chuang Gan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.07094v1",
        "title": "Learning Customized Visual Models with Retrieval-Augmented Knowledge",
        "abstract": "  Image-text contrastive learning models such as CLIP have demonstrated strong\ntask transfer ability. The high generality and usability of these visual models\nis achieved via a web-scale data collection process to ensure broad concept\ncoverage, followed by expensive pre-training to feed all the knowledge into\nmodel weights. Alternatively, we propose REACT, REtrieval-Augmented\nCusTomization, a framework to acquire the relevant web knowledge to build\ncustomized visual models for target domains. We retrieve the most relevant\nimage-text pairs (~3% of CLIP pre-training data) from the web-scale database as\nexternal knowledge, and propose to customize the model by only training new\nmodualized blocks while freezing all the original weights. The effectiveness of\nREACT is demonstrated via extensive experiments on classification, retrieval,\ndetection and segmentation tasks, including zero, few, and full-shot settings.\nParticularly, on the zero-shot classification task, compared with CLIP, it\nachieves up to 5.4% improvement on ImageNet and 3.7% on the ELEVATER benchmark\n(20 datasets).\n",
        "published": "2023",
        "authors": [
            "Haotian Liu",
            "Kilho Son",
            "Jianwei Yang",
            "Ce Liu",
            "Jianfeng Gao",
            "Yong Jae Lee",
            "Chunyuan Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.13823v4",
        "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs",
        "abstract": "  We propose an efficient method to ground pretrained text-only language models\nto the visual domain, enabling them to process arbitrarily interleaved\nimage-and-text data, and generate text interleaved with retrieved images. Our\nmethod leverages the abilities of language models learnt from large scale\ntext-only pretraining, such as in-context learning and free-form text\ngeneration. We keep the language model frozen, and finetune input and output\nlinear layers to enable cross-modality interactions. This allows our model to\nprocess arbitrarily interleaved image-and-text inputs, and generate free-form\ntext interleaved with retrieved images. We achieve strong zero-shot performance\non grounded tasks such as contextual image retrieval and multimodal dialogue,\nand showcase compelling interactive abilities. Our approach works with any\noff-the-shelf language model and paves the way towards an effective, general\nsolution for leveraging pretrained language models in visually grounded\nsettings.\n",
        "published": "2023",
        "authors": [
            "Jing Yu Koh",
            "Ruslan Salakhutdinov",
            "Daniel Fried"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.01328v3",
        "title": "IC3: Image Captioning by Committee Consensus",
        "abstract": "  If you ask a human to describe an image, they might do so in a thousand\ndifferent ways. Traditionally, image captioning models are trained to generate\na single \"best\" (most like a reference) image caption. Unfortunately, doing so\nencourages captions that are \"informationally impoverished,\" and focus on only\na subset of the possible details, while ignoring other potentially useful\ninformation in the scene. In this work, we introduce a simple, yet novel,\nmethod: \"Image Captioning by Committee Consensus\" (IC3), designed to generate a\nsingle caption that captures high-level details from several annotator\nviewpoints. Humans rate captions produced by IC3 at least as helpful as\nbaseline SOTA models more than two thirds of the time, and IC3 can improve the\nperformance of SOTA automated recall systems by up to 84%, outperforming single\nhuman-generated reference captions, and indicating significant improvements\nover SOTA approaches for visual description. Code is available at\nhttps://davidmchan.github.io/caption-by-committee/\n",
        "published": "2023",
        "authors": [
            "David M. Chan",
            "Austin Myers",
            "Sudheendra Vijayanarasimhan",
            "David A. Ross",
            "John Canny"
        ]
    }
]