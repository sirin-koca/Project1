[
    {
        "id": "http://arxiv.org/abs/2302.04269v1",
        "title": "Diagnosing and Rectifying Vision Models using Language",
        "abstract": "  Recent multi-modal contrastive learning models have demonstrated the ability\nto learn an embedding space suitable for building strong vision classifiers, by\nleveraging the rich information in large-scale image-caption datasets. Our work\nhighlights a distinct advantage of this multi-modal embedding space: the\nability to diagnose vision classifiers through natural language. The\ntraditional process of diagnosing model behaviors in deployment settings\ninvolves labor-intensive data acquisition and annotation. Our proposed method\ncan discover high-error data slices, identify influential attributes and\nfurther rectify undesirable model behaviors, without requiring any visual data.\nThrough a combination of theoretical explanation and empirical verification, we\npresent conditions under which classifiers trained on embeddings from one\nmodality can be equivalently applied to embeddings from another modality. On a\nrange of image datasets with known error slices, we demonstrate that our method\ncan effectively identify the error slices and influential attributes, and can\nfurther use language to rectify failure modes of the classifier.\n",
        "published": "2023",
        "authors": [
            "Yuhui Zhang",
            "Jeff Z. HaoChen",
            "Shih-Cheng Huang",
            "Kuan-Chieh Wang",
            "James Zou",
            "Serena Yeung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.04865v1",
        "title": "Learning by Asking for Embodied Visual Navigation and Task Completion",
        "abstract": "  The research community has shown increasing interest in designing intelligent\nembodied agents that can assist humans in accomplishing tasks. Despite recent\nprogress on related vision-language benchmarks, most prior work has focused on\nbuilding agents that follow instructions rather than endowing agents the\nability to ask questions to actively resolve ambiguities arising naturally in\nembodied environments. To empower embodied agents with the ability to interact\nwith humans, in this work, we propose an Embodied Learning-By-Asking (ELBA)\nmodel that learns when and what questions to ask to dynamically acquire\nadditional information for completing the task. We evaluate our model on the\nTEACH vision-dialog navigation and task completion dataset. Experimental\nresults show that ELBA achieves improved task performance compared to baseline\nmodels without question-answering capabilities.\n",
        "published": "2023",
        "authors": [
            "Ying Shen",
            "Ismini Lourentzou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.05608v1",
        "title": "Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis",
        "abstract": "  Often, deep network models are purely inductive during training and while\nperforming inference on unseen data. Thus, when such models are used for\npredictions, it is well known that they often fail to capture the semantic\ninformation and implicit dependencies that exist among objects (or concepts) on\na population level. Moreover, it is still unclear how domain or prior modal\nknowledge can be specified in a backpropagation friendly manner, especially in\nlarge-scale and noisy settings. In this work, we propose an end-to-end vision\nand language model incorporating explicit knowledge graphs. We also introduce\nan interactive out-of-distribution (OOD) layer using implicit network operator.\nThe layer is used to filter noise that is brought by external knowledge base.\nIn practice, we apply our model on several vision and language downstream tasks\nincluding visual question answering, visual reasoning, and image-text retrieval\non different datasets. Our experiments show that it is possible to design\nmodels that perform similarly to state-of-art results but with significantly\nfewer samples and training time.\n",
        "published": "2023",
        "authors": [
            "Zhu Wang",
            "Sourav Medya",
            "Sathya N. Ravi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.07740v1",
        "title": "Team Triple-Check at Factify 2: Parameter-Efficient Large Foundation\n  Models with Feature Representations for Multi-Modal Fact Verification",
        "abstract": "  Multi-modal fact verification has become an important but challenging issue\non social media due to the mismatch between the text and images in the\nmisinformation of news content, which has been addressed by considering\ncross-modalities to identify the veracity of the news in recent years. In this\npaper, we propose the Pre-CoFactv2 framework with new parameter-efficient\nfoundation models for modeling fine-grained text and input embeddings with\nlightening parameters, multi-modal multi-type fusion for not only capturing\nrelations for the same and different modalities but also for different types\n(i.e., claim and document), and feature representations for explicitly\nproviding metadata for each sample. In addition, we introduce a unified\nensemble method to boost model performance by adjusting the importance of each\ntrained model with not only the weights but also the powers. Extensive\nexperiments show that Pre-CoFactv2 outperforms Pre-CoFact by a large margin and\nachieved new state-of-the-art results at the Factify challenge at AAAI 2023. We\nfurther illustrate model variations to verify the relative contributions of\ndifferent components. Our team won the first prize (F1-score: 81.82%) and we\nmade our code publicly available at\nhttps://github.com/wwweiwei/Pre-CoFactv2-AAAI-2023.\n",
        "published": "2023",
        "authors": [
            "Wei-Wei Du",
            "Hong-Wei Wu",
            "Wei-Yao Wang",
            "Wen-Chih Peng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.07994v1",
        "title": "\u00c0-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable\n  Prompting",
        "abstract": "  We introduce \\`A-la-carte Prompt Tuning (APT), a transformer-based scheme to\ntune prompts on distinct data so that they can be arbitrarily composed at\ninference time. The individual prompts can be trained in isolation, possibly on\ndifferent devices, at different times, and on different distributions or\ndomains. Furthermore each prompt only contains information about the subset of\ndata it was exposed to during training. During inference, models can be\nassembled based on arbitrary selections of data sources, which we call\n\"\\`a-la-carte learning\". \\`A-la-carte learning enables constructing bespoke\nmodels specific to each user's individual access rights and preferences. We can\nadd or remove information from the model by simply adding or removing the\ncorresponding prompts without retraining from scratch. We demonstrate that\n\\`a-la-carte built models achieve accuracy within $5\\%$ of models trained on\nthe union of the respective sources, with comparable cost in terms of training\nand inference time. For the continual learning benchmarks Split CIFAR-100 and\nCORe50, we achieve state-of-the-art performance.\n",
        "published": "2023",
        "authors": [
            "Benjamin Bowman",
            "Alessandro Achille",
            "Luca Zancato",
            "Matthew Trager",
            "Pramuditha Perera",
            "Giovanni Paolini",
            "Stefano Soatto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.08672v1",
        "title": "Multimodal Subtask Graph Generation from Instructional Videos",
        "abstract": "  Real-world tasks consist of multiple inter-dependent subtasks (e.g., a dirty\npan needs to be washed before it can be used for cooking). In this work, we aim\nto model the causal dependencies between such subtasks from instructional\nvideos describing the task. This is a challenging problem since complete\ninformation about the world is often inaccessible from videos, which demands\nrobust learning mechanisms to understand the causal structure of events. We\npresent Multimodal Subtask Graph Generation (MSG2), an approach that constructs\na Subtask Graph defining the dependency between a task's subtasks relevant to a\ntask from noisy web videos. Graphs generated by our multimodal approach are\ncloser to human-annotated graphs compared to prior approaches. MSG2 further\nperforms the downstream task of next subtask prediction 85% and 30% more\naccurately than recent video transformer models in the ProceL and CrossTask\ndatasets, respectively.\n",
        "published": "2023",
        "authors": [
            "Yunseok Jang",
            "Sungryull Sohn",
            "Lajanugen Logeswaran",
            "Tiange Luo",
            "Moontae Lee",
            "Honglak Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.14115v2",
        "title": "Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense\n  Video Captioning",
        "abstract": "  In this work, we introduce Vid2Seq, a multi-modal single-stage dense event\ncaptioning model pretrained on narrated videos which are readily-available at\nscale. The Vid2Seq architecture augments a language model with special time\ntokens, allowing it to seamlessly predict event boundaries and textual\ndescriptions in the same output sequence. Such a unified model requires\nlarge-scale training data, which is not available in current annotated\ndatasets. We show that it is possible to leverage unlabeled narrated videos for\ndense video captioning, by reformulating sentence boundaries of transcribed\nspeech as pseudo event boundaries, and using the transcribed speech sentences\nas pseudo event captions. The resulting Vid2Seq model pretrained on the\nYT-Temporal-1B dataset improves the state of the art on a variety of dense\nvideo captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.\nVid2Seq also generalizes well to the tasks of video paragraph captioning and\nvideo clip captioning, and to few-shot settings. Our code is publicly available\nat https://antoyang.github.io/vid2seq.html.\n",
        "published": "2023",
        "authors": [
            "Antoine Yang",
            "Arsha Nagrani",
            "Paul Hongsuck Seo",
            "Antoine Miech",
            "Jordi Pont-Tuset",
            "Ivan Laptev",
            "Josef Sivic",
            "Cordelia Schmid"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.14383v3",
        "title": "Linear Spaces of Meanings: Compositional Structures in Vision-Language\n  Models",
        "abstract": "  We investigate compositional structures in data embeddings from pre-trained\nvision-language models (VLMs). Traditionally, compositionality has been\nassociated with algebraic operations on embeddings of words from a pre-existing\nvocabulary. In contrast, we seek to approximate representations from an encoder\nas combinations of a smaller set of vectors in the embedding space. These\nvectors can be seen as \"ideal words\" for generating concepts directly within\nthe embedding space of the model. We first present a framework for\nunderstanding compositional structures from a geometric perspective. We then\nexplain what these compositional structures entail probabilistically in the\ncase of VLM embeddings, providing intuitions for why they arise in practice.\nFinally, we empirically explore these structures in CLIP's embeddings and we\nevaluate their usefulness for solving different vision-language tasks such as\nclassification, debiasing, and retrieval. Our results show that simple linear\nalgebraic operations on embedding vectors can be used as compositional and\ninterpretable methods for regulating the behavior of VLMs.\n",
        "published": "2023",
        "authors": [
            "Matthew Trager",
            "Pramuditha Perera",
            "Luca Zancato",
            "Alessandro Achille",
            "Parminder Bhatia",
            "Stefano Soatto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.02472v2",
        "title": "ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration\n  Measure",
        "abstract": "  Studies have shown that modern neural networks tend to be poorly calibrated\ndue to over-confident predictions. Traditionally, post-processing methods have\nbeen used to calibrate the model after training. In recent years, various\ntrainable calibration measures have been proposed to incorporate them directly\ninto the training process. However, these methods all incorporate internal\nhyperparameters, and the performance of these calibration objectives relies on\ntuning these hyperparameters, incurring more computational costs as the size of\nneural networks and datasets become larger. As such, we present Expected\nSquared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable\ncalibration objective loss, where we view the calibration error from the\nperspective of the squared difference between the two expectations. With\nextensive experiments on several architectures (CNNs, Transformers) and\ndatasets, we demonstrate that (1) incorporating ESD into the training improves\nmodel calibration in various batch size settings without the need for internal\nhyperparameter tuning, (2) ESD yields the best-calibrated results compared with\nprevious approaches, and (3) ESD drastically improves the computational costs\nrequired for calibration during training due to the absence of internal\nhyperparameter. The code is publicly accessible at\nhttps://github.com/hee-suk-yoon/ESD.\n",
        "published": "2023",
        "authors": [
            "Hee Suk Yoon",
            "Joshua Tian Jin Tee",
            "Eunseop Yoon",
            "Sunjae Yoon",
            "Gwangsu Kim",
            "Yingzhen Li",
            "Chang D. Yoo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.10093v2",
        "title": "Investigating the Role of Attribute Context in Vision-Language Models\n  for Object Recognition and Detection",
        "abstract": "  Vision-language alignment learned from image-caption pairs has been shown to\nbenefit tasks like object recognition and detection. Methods are mostly\nevaluated in terms of how well object class names are learned, but captions\nalso contain rich attribute context that should be considered when learning\nobject alignment. It is unclear how methods use this context in learning, as\nwell as whether models succeed when tasks require attribute and object\nunderstanding. To address this gap, we conduct extensive analysis of the role\nof attributes in vision-language models. We specifically measure model\nsensitivity to the presence and meaning of attribute context, gauging influence\non object embeddings through unsupervised phrase grounding and classification\nvia description methods. We further evaluate the utility of attribute context\nin training for open-vocabulary object detection, fine-grained text-region\nretrieval, and attribution tasks. Our results show that attribute context can\nbe wasted when learning alignment for detection, attribute meaning is not\nadequately considered in embeddings, and describing classes by only their\nattributes is ineffective. A viable strategy that we find to increase benefits\nfrom attributes is contrastive training with adjective-based negative captions.\n",
        "published": "2023",
        "authors": [
            "Kyle Buettner",
            "Adriana Kovashka"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.13519v1",
        "title": "Learning and Verification of Task Structure in Instructional Videos",
        "abstract": "  Given the enormous number of instructional videos available online, learning\na diverse array of multi-step task models from videos is an appealing goal. We\nintroduce a new pre-trained video model, VideoTaskformer, focused on\nrepresenting the semantics and structure of instructional videos. We pre-train\nVideoTaskformer using a simple and effective objective: predicting weakly\nsupervised textual labels for steps that are randomly masked out from an\ninstructional video (masked step modeling). Compared to prior work which learns\nstep representations locally, our approach involves learning them globally,\nleveraging video of the entire surrounding task as context. From these learned\nrepresentations, we can verify if an unseen video correctly executes a given\ntask, as well as forecast which steps are likely to be taken after a given\nstep. We introduce two new benchmarks for detecting mistakes in instructional\nvideos, to verify if there is an anomalous step and if steps are executed in\nthe right order. We also introduce a long-term forecasting benchmark, where the\ngoal is to predict long-range future steps from a given step. Our method\noutperforms previous baselines on these tasks, and we believe the tasks will be\na valuable way for the community to measure the quality of step\nrepresentations. Additionally, we evaluate VideoTaskformer on 3 existing\nbenchmarks -- procedural activity recognition, step classification, and step\nforecasting -- and demonstrate on each that our method outperforms existing\nbaselines and achieves new state-of-the-art performance.\n",
        "published": "2023",
        "authors": [
            "Medhini Narasimhan",
            "Licheng Yu",
            "Sean Bell",
            "Ning Zhang",
            "Trevor Darrell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.15441v1",
        "title": "Zero-shot Model Diagnosis",
        "abstract": "  When it comes to deploying deep vision models, the behavior of these systems\nmust be explicable to ensure confidence in their reliability and fairness. A\ncommon approach to evaluate deep learning models is to build a labeled test set\nwith attributes of interest and assess how well it performs. However, creating\na balanced test set (i.e., one that is uniformly sampled over all the important\ntraits) is often time-consuming, expensive, and prone to mistakes. The question\nwe try to address is: can we evaluate the sensitivity of deep learning models\nto arbitrary visual attributes without an annotated test set? This paper argues\nthe case that Zero-shot Model Diagnosis (ZOOM) is possible without the need for\na test set nor labeling. To avoid the need for test sets, our system relies on\na generative model and CLIP. The key idea is enabling the user to select a set\nof prompts (relevant to the problem) and our system will automatically search\nfor semantic counterfactual images (i.e., synthesized images that flip the\nprediction in the case of a binary classifier) using the generative model. We\nevaluate several visual tasks (classification, key-point detection, and\nsegmentation) in multiple visual domains to demonstrate the viability of our\nmethodology. Extensive experiments demonstrate that our method is capable of\nproducing counterfactual images and offering sensitivity analysis for model\ndiagnosis without the need for a test set.\n",
        "published": "2023",
        "authors": [
            "Jinqi Luo",
            "Zhaoning Wang",
            "Chen Henry Wu",
            "Dong Huang",
            "Fernando De la Torre"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.16133v1",
        "title": "Exposing and Addressing Cross-Task Inconsistency in Unified\n  Vision-Language Models",
        "abstract": "  As general purpose vision models get increasingly effective at a wide set of\ntasks, it is imperative that they be consistent across the tasks they support.\nInconsistent AI models are considered brittle and untrustworthy by human users\nand are more challenging to incorporate into larger systems that take\ndependencies on their outputs. Measuring consistency between very heterogeneous\ntasks that might include outputs in different modalities is challenging since\nit is difficult to determine if the predictions are consistent with one\nanother. As a solution, we introduce a benchmark dataset, COCOCON, where we use\ncontrast sets created by modifying test instances for multiple tasks in small\nbut semantically meaningful ways to change the gold label, and outline metrics\nfor measuring if a model is consistent by ranking the original and perturbed\ninstances across tasks. We find that state-of-the-art systems suffer from a\nsurprisingly high degree of inconsistent behavior across tasks, especially for\nmore heterogeneous tasks. Finally, we propose using a rank correlation-based\nauxiliary objective computed over large automatically created cross-task\ncontrast sets to improve the multi-task consistency of large unified models,\nwhile retaining their original accuracy on downstream tasks. Project website\navailable at https://adymaharana.github.io/cococon/\n",
        "published": "2023",
        "authors": [
            "Adyasha Maharana",
            "Amita Kamath",
            "Christopher Clark",
            "Mohit Bansal",
            "Aniruddha Kembhavi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.16406v1",
        "title": "Hierarchical Video-Moment Retrieval and Step-Captioning",
        "abstract": "  There is growing interest in searching for information from large video\ncorpora. Prior works have studied relevant tasks, such as text-based video\nretrieval, moment retrieval, video summarization, and video captioning in\nisolation, without an end-to-end setup that can jointly search from video\ncorpora and generate summaries. Such an end-to-end setup would allow for many\ninteresting applications, e.g., a text-based search that finds a relevant video\nfrom a video corpus, extracts the most relevant moment from that video, and\nsegments the moment into important steps with captions. To address this, we\npresent the HiREST (HIerarchical REtrieval and STep-captioning) dataset and\npropose a new benchmark that covers hierarchical information retrieval and\nvisual/textual stepwise summarization from an instructional video corpus.\nHiREST consists of 3.4K text-video pairs from an instructional video dataset,\nwhere 1.1K videos have annotations of moment spans relevant to text query and\nbreakdown of each moment into key instruction steps with caption and timestamps\n(totaling 8.6K step captions). Our hierarchical benchmark consists of video\nretrieval, moment retrieval, and two novel moment segmentation and step\ncaptioning tasks. In moment segmentation, models break down a video moment into\ninstruction steps and identify start-end boundaries. In step captioning, models\ngenerate a textual summary for each step. We also present starting point\ntask-specific and end-to-end joint baseline models for our new benchmark. While\nthe baseline models show some promising results, there still exists large room\nfor future improvement by the community. Project website:\nhttps://hirest-cvpr2023.github.io\n",
        "published": "2023",
        "authors": [
            "Abhay Zala",
            "Jaemin Cho",
            "Satwik Kottur",
            "Xilun Chen",
            "Barlas O\u011fuz",
            "Yasher Mehdad",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging\n  Face",
        "abstract": "  Solving complicated AI tasks with different domains and modalities is a key\nstep toward artificial general intelligence. While there are numerous AI models\navailable for various domains and modalities, they cannot handle complicated AI\ntasks autonomously. Considering large language models (LLMs) have exhibited\nexceptional abilities in language understanding, generation, interaction, and\nreasoning, we advocate that LLMs could act as a controller to manage existing\nAI models to solve complicated AI tasks, with language serving as a generic\ninterface to empower this. Based on this philosophy, we present HuggingGPT, an\nLLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI\nmodels in machine learning communities (e.g., Hugging Face) to solve AI tasks.\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\nrequest, select models according to their function descriptions available in\nHugging Face, execute each subtask with the selected AI model, and summarize\nthe response according to the execution results. By leveraging the strong\nlanguage capability of ChatGPT and abundant AI models in Hugging Face,\nHuggingGPT can tackle a wide range of sophisticated AI tasks spanning different\nmodalities and domains and achieve impressive results in language, vision,\nspeech, and other challenging tasks, which paves a new way towards the\nrealization of artificial general intelligence.\n",
        "published": "2023",
        "authors": [
            "Yongliang Shen",
            "Kaitao Song",
            "Xu Tan",
            "Dongsheng Li",
            "Weiming Lu",
            "Yueting Zhuang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.00020v2",
        "title": "SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes\n  Analysis",
        "abstract": "  The prevalence of memes on social media has created the need to sentiment\nanalyze their underlying meanings for censoring harmful content. Meme censoring\nsystems by machine learning raise the need for a semi-supervised learning\nsolution to take advantage of the large number of unlabeled memes available on\nthe internet and make the annotation process less challenging. Moreover, the\napproach needs to utilize multimodal data as memes' meanings usually come from\nboth images and texts. This research proposes a multimodal semi-supervised\nlearning approach that outperforms other multimodal semi-supervised learning\nand supervised learning state-of-the-art models on two datasets, the Multimedia\nAutomatic Misogyny Identification and Hateful Memes dataset. Building on the\ninsights gained from Contrastive Language-Image Pre-training, which is an\neffective multimodal learning technique, this research introduces SemiMemes, a\nnovel training method that combines auto-encoder and classification task to\nmake use of the resourceful unlabeled data.\n",
        "published": "2023",
        "authors": [
            "Pham Thai Hoang Tung",
            "Nguyen Tan Viet",
            "Ngo Tien Anh",
            "Phan Duy Hung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.01083v1",
        "title": "Can the Inference Logic of Large Language Models be Disentangled into\n  Symbolic Concepts?",
        "abstract": "  In this paper, we explain the inference logic of large language models (LLMs)\nas a set of symbolic concepts. Many recent studies have discovered that\ntraditional DNNs usually encode sparse symbolic concepts. However, because an\nLLM has much more parameters than traditional DNNs, whether the LLM also\nencodes sparse symbolic concepts is still an open problem. Therefore, in this\npaper, we propose to disentangle the inference score of LLMs for dialogue tasks\ninto a small number of symbolic concepts. We verify that we can use those\nsparse concepts to well estimate all inference scores of the LLM on all\narbitrarily masking states of the input sentence. We also evaluate the\ntransferability of concepts encoded by an LLM and verify that symbolic concepts\nusually exhibit high transferability across similar input sentences. More\ncrucially, those symbolic concepts can be used to explain the exact reasons\naccountable for the LLM's prediction errors.\n",
        "published": "2023",
        "authors": [
            "Wen Shen",
            "Lei Cheng",
            "Yuxiao Yang",
            "Mingjie Li",
            "Quanshi Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.02168v2",
        "title": "I2I: Initializing Adapters with Improvised Knowledge",
        "abstract": "  Adapters present a promising solution to the catastrophic forgetting problem\nin continual learning. However, training independent Adapter modules for every\nnew task misses an opportunity for cross-task knowledge transfer. We propose\nImprovise to Initialize (I2I), a continual learning algorithm that initializes\nAdapters for incoming tasks by distilling knowledge from previously-learned\ntasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning\nbenchmark, by conducting experiments on sequences of visual question answering\ntasks. Adapters trained with I2I consistently achieve better task accuracy than\nindependently-trained Adapters, demonstrating that our algorithm facilitates\nknowledge transfer between task Adapters. I2I also results in better cross-task\nknowledge transfer than the state-of-the-art AdapterFusion without incurring\nthe associated parametric cost.\n",
        "published": "2023",
        "authors": [
            "Tejas Srinivasan",
            "Furong Jia",
            "Mohammad Rostami",
            "Jesse Thomason"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.04907v1",
        "title": "Improving Vision-and-Language Navigation by Generating Future-View Image\n  Semantics",
        "abstract": "  Vision-and-Language Navigation (VLN) is the task that requires an agent to\nnavigate through the environment based on natural language instructions. At\neach step, the agent takes the next action by selecting from a set of navigable\nlocations. In this paper, we aim to take one step further and explore whether\nthe agent can benefit from generating the potential future view during\nnavigation. Intuitively, humans will have an expectation of how the future\nenvironment will look like, based on the natural language instructions and\nsurrounding views, which will aid correct navigation. Hence, to equip the agent\nwith this ability to generate the semantics of future navigation views, we\nfirst propose three proxy tasks during the agent's in-domain pre-training:\nMasked Panorama Modeling (MPM), Masked Trajectory Modeling (MTM), and Action\nPrediction with Image Generation (APIG). These three objectives teach the model\nto predict missing views in a panorama (MPM), predict missing steps in the full\ntrajectory (MTM), and generate the next view based on the full instruction and\nnavigation history (APIG), respectively. We then fine-tune the agent on the VLN\ntask with an auxiliary loss that minimizes the difference between the view\nsemantics generated by the agent and the ground truth view semantics of the\nnext step. Empirically, our VLN-SIG achieves the new state-of-the-art on both\nthe Room-to-Room dataset and the CVDN dataset. We further show that our agent\nlearns to fill in missing patches in future views qualitatively, which brings\nmore interpretability over agents' predicted actions. Lastly, we demonstrate\nthat learning to predict future view semantics also enables the agent to have\nbetter performance on longer paths.\n",
        "published": "2023",
        "authors": [
            "Jialu Li",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.05265v3",
        "title": "Controllable Textual Inversion for Personalized Text-to-Image Generation",
        "abstract": "  The recent large-scale generative modeling has attained unprecedented\nperformance especially in producing high-fidelity images driven by text\nprompts. Text inversion (TI), alongside the text-to-image model backbones, is\nproposed as an effective technique in personalizing the generation when the\nprompts contain user-defined, unseen or long-tail concept tokens. Despite that,\nwe find and show that the deployment of TI remains full of \"dark-magics\" -- to\nname a few, the harsh requirement of additional datasets, arduous human efforts\nin the loop and lack of robustness. In this work, we propose a much-enhanced\nversion of TI, dubbed Controllable Textual Inversion (COTI), in resolving all\nthe aforementioned problems and in turn delivering a robust, data-efficient and\neasy-to-use framework. The core to COTI is a theoretically-guided loss\nobjective instantiated with a comprehensive and novel weighted scoring\nmechanism, encapsulated by an active-learning paradigm. The extensive results\nshow that COTI significantly outperforms the prior TI-related approaches with a\n26.05 decrease in the FID score and a 23.00% boost in the R-precision.\n",
        "published": "2023",
        "authors": [
            "Jianan Yang",
            "Haobo Wang",
            "Yanming Zhang",
            "Ruixuan Xiao",
            "Sai Wu",
            "Gang Chen",
            "Junbo Zhao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.08612v3",
        "title": "Bridging Discrete and Backpropagation: Straight-Through and Beyond",
        "abstract": "  Backpropagation, the cornerstone of deep learning, is limited to computing\ngradients for continuous variables. This limitation poses challenges for\nproblems involving discrete latent variables. To address this issue, we propose\na novel approach to approximate the gradient of parameters involved in\ngenerating discrete latent variables. First, we examine the widely used\nStraight-Through (ST) heuristic and demonstrate that it works as a first-order\napproximation of the gradient. Guided by our findings, we propose ReinMax,\nwhich achieves second-order accuracy by integrating Heun's method, a\nsecond-order numerical method for solving ODEs. ReinMax does not require\nHessian or other second-order derivatives, thus having negligible computation\noverheads. Extensive experimental results on various tasks demonstrate the\nsuperiority of ReinMax over the state of the art. Implementations are released\nat https://github.com/microsoft/ReinMax.\n",
        "published": "2023",
        "authors": [
            "Liyuan Liu",
            "Chengyu Dong",
            "Xiaodong Liu",
            "Bin Yu",
            "Jianfeng Gao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.09842v3",
        "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language\n  Models",
        "abstract": "  Large language models (LLMs) have achieved remarkable progress in solving\nvarious natural language processing tasks due to emergent reasoning abilities.\nHowever, LLMs have inherent limitations as they are incapable of accessing\nup-to-date information (stored on the Web or in task-specific knowledge bases),\nusing external tools, and performing precise mathematical and logical\nreasoning. In this paper, we present Chameleon, an AI system that mitigates\nthese limitations by augmenting LLMs with plug-and-play modules for\ncompositional reasoning. Chameleon synthesizes programs by composing various\ntools (e.g., LLMs, off-the-shelf vision models, web search engines, Python\nfunctions, and heuristic-based modules) for accomplishing complex reasoning\ntasks. At the heart of Chameleon is an LLM-based planner that assembles a\nsequence of tools to execute to generate the final response. We showcase the\neffectiveness of Chameleon on two multi-modal knowledge-intensive reasoning\ntasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%\noverall accuracy on ScienceQA, improving the best published few-shot result by\n11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,\nlifting the state of the art to 98.78%. Our analysis also shows that the\nGPT-4-powered planner exhibits more consistent and rational tool selection via\ninferring potential constraints from instructions, compared to a\nChatGPT-powered planner. The project is available at\nhttps://chameleon-llm.github.io.\n",
        "published": "2023",
        "authors": [
            "Pan Lu",
            "Baolin Peng",
            "Hao Cheng",
            "Michel Galley",
            "Kai-Wei Chang",
            "Ying Nian Wu",
            "Song-Chun Zhu",
            "Jianfeng Gao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.11093v1",
        "title": "Hi Sheldon! Creating Deep Personalized Characters from TV Shows",
        "abstract": "  Imagine an interesting multimodal interactive scenario that you can see,\nhear, and chat with an AI-generated digital character, who is capable of\nbehaving like Sheldon from The Big Bang Theory, as a DEEP copy from appearance\nto personality. Towards this fantastic multimodal chatting scenario, we propose\na novel task, named Deep Personalized Character Creation (DPCC): creating\nmultimodal chat personalized characters from multimodal data such as TV shows.\nSpecifically, given a single- or multi-modality input (text, audio, video), the\ngoal of DPCC is to generate a multi-modality (text, audio, video) response,\nwhich should be well-matched the personality of a specific character such as\nSheldon, and of high quality as well. To support this novel task, we further\ncollect a character centric multimodal dialogue dataset, named Deep\nPersonalized Character Dataset (DPCD), from TV shows. DPCD contains\ncharacter-specific multimodal dialogue data of ~10k utterances and ~6 hours of\naudio/video per character, which is around 10 times larger compared to existing\nrelated datasets.On DPCD, we present a baseline method for the DPCC task and\ncreate 5 Deep personalized digital Characters (DeepCharacters) from Big Bang TV\nShows. We conduct both subjective and objective experiments to evaluate the\nmultimodal response from DeepCharacters in terms of characterization and\nquality. The results demonstrates that, on our collected DPCD dataset, the\nproposed baseline can create personalized digital characters for generating\nmultimodal response.Our collected DPCD dataset, the code of data collection and\nour baseline will be published soon.\n",
        "published": "2023",
        "authors": [
            "Meidai Xuanyuan",
            "Yuwang Wang",
            "Honglei Guo",
            "Xiao Ma",
            "Yuchen Guo",
            "Tao Yu",
            "Qionghai Dai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.14933v2",
        "title": "An Empirical Study of Multimodal Model Merging",
        "abstract": "  Model merging (e.g., via interpolation or task arithmetic) fuses multiple\nmodels trained on different tasks to generate a multi-task solution. The\ntechnique has been proven successful in previous studies, where the models are\ntrained on similar tasks and with the same initialization. In this paper, we\nexpand on this concept to a multimodal setup by merging transformers trained on\ndifferent modalities. Furthermore, we conduct our study for a novel goal where\nwe can merge vision, language, and cross-modal transformers of a\nmodality-specific architecture to create a parameter-efficient\nmodality-agnostic architecture. Through comprehensive experiments, we\nsystematically investigate the key factors impacting model performance after\nmerging, including initialization, merging mechanisms, and model architectures.\nWe also propose two metrics that assess the distance between weights to be\nmerged and can serve as an indicator of the merging outcomes. Our analysis\nleads to an effective training recipe for matching the performance of the\nmodality-agnostic baseline (i.e., pre-trained from scratch) via model merging.\nOur method also outperforms naive merging significantly on various tasks, with\nimprovements of 3% on VQA, 7% on COCO retrieval, 25% on NLVR2, 14% on Flickr30k\nand 3% on ADE20k. Our code is available at https://github.com/ylsung/vl-merging\n",
        "published": "2023",
        "authors": [
            "Yi-Lin Sung",
            "Linjie Li",
            "Kevin Lin",
            "Zhe Gan",
            "Mohit Bansal",
            "Lijuan Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.02995v2",
        "title": "Accuracy on the Curve: On the Nonlinear Correlation of ML Performance\n  Between Data Subpopulations",
        "abstract": "  Understanding the performance of machine learning (ML) models across diverse\ndata distributions is critically important for reliable applications. Despite\nrecent empirical studies positing a near-perfect linear correlation between\nin-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically\ndemonstrate that this correlation is more nuanced under subpopulation shifts.\nThrough rigorous experimentation and analysis across a variety of datasets,\nmodels, and training epochs, we demonstrate that OOD performance often has a\nnonlinear correlation with ID performance in subpopulation shifts. Our\nfindings, which contrast previous studies that have posited a linear\ncorrelation in model performance during distribution shifts, reveal a \"moon\nshape\" correlation (parabolic uptrend curve) between the test performance on\nthe majority subpopulation and the minority subpopulation. This non-trivial\nnonlinear correlation holds across model architectures, hyperparameters,\ntraining durations, and the imbalance between subpopulations. Furthermore, we\nfound that the nonlinearity of this \"moon shape\" is causally influenced by the\ndegree of spurious correlations in the training data. Our controlled\nexperiments show that stronger spurious correlation in the training data\ncreates more nonlinear performance correlation. We provide complementary\nexperimental and theoretical analyses for this phenomenon, and discuss its\nimplications for ML reliability and fairness. Our work highlights the\nimportance of understanding the nonlinear effects of model improvement on\nperformance in different subpopulations, and has the potential to inform the\ndevelopment of more equitable and responsible machine learning models.\n",
        "published": "2023",
        "authors": [
            "Weixin Liang",
            "Yining Mao",
            "Yongchan Kwon",
            "Xinyu Yang",
            "James Zou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.06988v2",
        "title": "Self-Chained Image-Language Model for Video Localization and Question\n  Answering",
        "abstract": "  Recent studies have shown promising results on utilizing large pre-trained\nimage-language models for video question answering. While these image-language\nmodels can efficiently bootstrap the representation learning of video-language\nmodels, they typically concatenate uniformly sampled video frames as visual\ninputs without explicit language-aware, temporal modeling. When only a portion\nof a video input is relevant to the language query, such uniform frame sampling\ncan often lead to missing important visual cues. Although humans often find a\nvideo moment to focus on and rewind the moment to answer questions, training a\nquery-aware video moment localizer often requires expensive annotations and\nhigh computational costs. To address this issue, we propose Self-Chained Video\nLocalization-Answering (SeViLA), a novel framework that leverages a single\nimage-language model (BLIP-2) to tackle both temporal keyframe localization and\nQA on videos. SeViLA framework consists of two modules: Localizer and Answerer,\nwhere both are parameter-efficiently fine-tuned from BLIP-2. We propose two\nways of chaining these modules for cascaded inference and self-refinement.\nFirst, in the forward chain, the Localizer finds multiple language-aware\nkeyframes in a video, which the Answerer uses to predict the answer. Second, in\nthe reverse chain, the Answerer generates keyframe pseudo-labels to refine the\nLocalizer, alleviating the need for expensive video moment localization\nannotations. Our SeViLA framework outperforms several strong baselines on 5\nchallenging video QA and event prediction benchmarks, and achieves the\nstate-of-the-art in both fine-tuning (NExT-QA, STAR) and zero-shot (NExT-QA,\nSTAR, How2QA, VLEP) settings. We also analyze the impact of Localizer,\ncomparisons of Localizer with other temporal localization models,\npre-training/self-refinement of Localizer, and varying the number of keyframes.\n",
        "published": "2023",
        "authors": [
            "Shoubin Yu",
            "Jaemin Cho",
            "Prateek Yadav",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.09782v1",
        "title": "Analysis of Visual Question Answering Algorithms with attention model",
        "abstract": "  Visual question answering (VQA) usesimage processing algorithms to process\nthe image and natural language processing methods to understand and answer the\nquestion. VQA is helpful to a visually impaired person, can be used for the\nsecurity surveillance system and online chatbots that learn from the web. It\nuses NLP methods to learn the semantic of the question and to derive the\ntextual features. Computer vision techniques are used for generating image\nrepresentation in such a way that they can identify the objects about which\nquestion is asked. The Attention model tries to mimic the human behavior of\ngiving attention to a different region of an image according to our\nunderstanding of its context. This paper critically examines and reviews\nmethods of VQA algorithm such as generation of semantics of text,\nidentification of objects and answer classification techniques that use the\nco-attention approach.\n",
        "published": "2023",
        "authors": [
            "Param Ahir",
            "Hiteishi M. Diwanji"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.12689v2",
        "title": "FIT: Far-reaching Interleaved Transformers",
        "abstract": "  We present FIT: a transformer-based architecture with efficient\nself-attention and adaptive computation. Unlike original transformers, which\noperate on a single sequence of data tokens, we divide the data tokens into\ngroups, with each group being a shorter sequence of tokens. We employ two types\nof transformer layers: local layers operate on data tokens within each group,\nwhile global layers operate on a smaller set of introduced latent tokens. These\nlayers, comprising the same set of self-attention and feed-forward layers as\nstandard transformers, are interleaved, and cross-attention is used to\nfacilitate information exchange between data and latent tokens within the same\ngroup. The attention complexity is $O(n^2)$ locally within each group of size\n$n$, but can reach $O(L^{{4}/{3}})$ globally for sequence length of $L$. The\nefficiency can be further enhanced by relying more on global layers that\nperform adaptive computation using a smaller set of latent tokens. FIT is a\nversatile architecture and can function as an encoder, diffusion decoder, or\nautoregressive decoder. We provide initial evidence demonstrating its\neffectiveness in high-resolution image understanding and generation tasks.\nNotably, FIT exhibits potential in performing end-to-end training on\ngigabit-scale data, such as 6400$\\times$6400 images, or 160K tokens (after\npatch tokenization), within a memory capacity of 16GB, without requiring\nspecific optimizations or model parallelism.\n",
        "published": "2023",
        "authors": [
            "Ting Chen",
            "Lala Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.14325v1",
        "title": "Improving Factuality and Reasoning in Language Models through Multiagent\n  Debate",
        "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities in\nlanguage generation, understanding, and few-shot learning in recent years. An\nextensive body of work has explored how their performance may be further\nimproved through the tools of prompting, ranging from verification,\nself-consistency, or intermediate scratchpads. In this paper, we present a\ncomplementary approach to improve language responses where multiple language\nmodel instances propose and debate their individual responses and reasoning\nprocesses over multiple rounds to arrive at a common final answer. Our findings\nindicate that this approach significantly enhances mathematical and strategic\nreasoning across a number of tasks. We also demonstrate that our approach\nimproves the factual validity of generated content, reducing fallacious answers\nand hallucinations that contemporary models are prone to. Our approach may be\ndirectly applied to existing black-box models and uses identical procedure and\nprompts for all tasks we investigate. Overall, our findings suggest that such\n\"society of minds\" approach has the potential to significantly advance the\ncapabilities of LLMs and pave the way for further breakthroughs in language\ngeneration and understanding.\n",
        "published": "2023",
        "authors": [
            "Yilun Du",
            "Shuang Li",
            "Antonio Torralba",
            "Joshua B. Tenenbaum",
            "Igor Mordatch"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.17144v2",
        "title": "Ghost in the Minecraft: Generally Capable Agents for Open-World\n  Environments via Large Language Models with Text-based Knowledge and Memory",
        "abstract": "  The captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular \"ObtainDiamond\" task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https://github.com/OpenGVLab/GITM.\n",
        "published": "2023",
        "authors": [
            "Xizhou Zhu",
            "Yuntao Chen",
            "Hao Tian",
            "Chenxin Tao",
            "Weijie Su",
            "Chenyu Yang",
            "Gao Huang",
            "Bin Li",
            "Lewei Lu",
            "Xiaogang Wang",
            "Yu Qiao",
            "Zhaoxiang Zhang",
            "Jifeng Dai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.19195v1",
        "title": "PanoGen: Text-Conditioned Panoramic Environment Generation for\n  Vision-and-Language Navigation",
        "abstract": "  Vision-and-Language Navigation (VLN) requires the agent to follow language\ninstructions to navigate through 3D environments. One main challenge in VLN is\nthe limited availability of photorealistic training environments, which makes\nit hard to generalize to new and unseen environments. To address this problem,\nwe propose PanoGen, a generation method that can potentially create an infinite\nnumber of diverse panoramic environments conditioned on text. Specifically, we\ncollect room descriptions by captioning the room images in existing\nMatterport3D environments, and leverage a state-of-the-art text-to-image\ndiffusion model to generate the new panoramic environments. We use recursive\noutpainting over the generated images to create consistent 360-degree panorama\nviews. Our new panoramic environments share similar semantic information with\nthe original environments by conditioning on text descriptions, which ensures\nthe co-occurrence of objects in the panorama follows human intuition, and\ncreates enough diversity in room appearance and layout with image outpainting.\nLastly, we explore two ways of utilizing PanoGen in VLN pre-training and\nfine-tuning. We generate instructions for paths in our PanoGen environments\nwith a speaker built on a pre-trained vision-and-language model for VLN\npre-training, and augment the visual observation with our panoramic\nenvironments during agents' fine-tuning to avoid overfitting to seen\nenvironments. Empirically, learning with our PanoGen environments achieves the\nnew state-of-the-art on the Room-to-Room, Room-for-Room, and CVDN datasets.\nPre-training with our PanoGen speaker data is especially effective for CVDN,\nwhich has under-specified instructions and needs commonsense knowledge. Lastly,\nwe show that the agent can benefit from training with more generated panoramic\nenvironments, suggesting promising results for scaling up the PanoGen\nenvironments.\n",
        "published": "2023",
        "authors": [
            "Jialu Li",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.19216v1",
        "title": "Translation-Enhanced Multilingual Text-to-Image Generation",
        "abstract": "  Research on text-to-image generation (TTI) still predominantly focuses on the\nEnglish language due to the lack of annotated image-caption data in other\nlanguages; in the long run, this might widen inequitable access to TTI\ntechnology. In this work, we thus investigate multilingual TTI (termed mTTI)\nand the current potential of neural machine translation (NMT) to bootstrap mTTI\nsystems. We provide two key contributions. 1) Relying on a multilingual\nmulti-modal encoder, we provide a systematic empirical study of standard\nmethods used in cross-lingual NLP when applied to mTTI: Translate Train,\nTranslate Test, and Zero-Shot Transfer. 2) We propose Ensemble Adapter (EnsAd),\na novel parameter-efficient approach that learns to weigh and consolidate the\nmultilingual text knowledge within the mTTI framework, mitigating the language\ngap and thus improving mTTI performance. Our evaluations on standard mTTI\ndatasets COCO-CN, Multi30K Task2, and LAION-5B demonstrate the potential of\ntranslation-enhanced mTTI systems and also validate the benefits of the\nproposed EnsAd which derives consistent gains across all datasets. Further\ninvestigations on model variants, ablation studies, and qualitative analyses\nprovide additional insights on the inner workings of the proposed mTTI\napproaches.\n",
        "published": "2023",
        "authors": [
            "Yaoyiran Li",
            "Ching-Yun Chang",
            "Stephen Rawls",
            "Ivan Vuli\u0107",
            "Anna Korhonen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.19280v1",
        "title": "Large language models improve Alzheimer's disease diagnosis using\n  multi-modality data",
        "abstract": "  In diagnosing challenging conditions such as Alzheimer's disease (AD),\nimaging is an important reference. Non-imaging patient data such as patient\ninformation, genetic data, medication information, cognitive and memory tests\nalso play a very important role in diagnosis. Effect. However, limited by the\nability of artificial intelligence models to mine such information, most of the\nexisting models only use multi-modal image data, and cannot make full use of\nnon-image data. We use a currently very popular pre-trained large language\nmodel (LLM) to enhance the model's ability to utilize non-image data, and\nachieved SOTA results on the ADNI dataset.\n",
        "published": "2023",
        "authors": [
            "Yingjie Feng",
            "Jun Wang",
            "Xianfeng Gu",
            "Xiaoyin Xu",
            "Min Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.00219v2",
        "title": "Diffusion Brush: A Latent Diffusion Model-based Editing Tool for\n  AI-generated Images",
        "abstract": "  Text-to-image generative models have made remarkable advancements in\ngenerating high-quality images. However, generated images often contain\nundesirable artifacts or other errors due to model limitations. Existing\ntechniques to fine-tune generated images are time-consuming (manual editing),\nproduce poorly-integrated results (inpainting), or result in unexpected changes\nacross the entire image (variation selection and prompt fine-tuning). In this\nwork, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to\nefficiently fine-tune desired regions within an AI-synthesized image. Our\nmethod introduces new random noise patterns at targeted regions during the\nreverse diffusion process, enabling the model to efficiently make changes to\nthe specified regions while preserving the original context for the rest of the\nimage. We evaluate our method's usability and effectiveness through a user\nstudy with artists, comparing our technique against other state-of-the-art\nimage inpainting techniques and editing software for fine-tuning AI-generated\nimagery.\n",
        "published": "2023",
        "authors": [
            "Peyman Gholami",
            "Robert Xiao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.00503v1",
        "title": "MEWL: Few-shot multimodal word learning with referential uncertainty",
        "abstract": "  Without explicit feedback, humans can rapidly learn the meaning of words.\nChildren can acquire a new word after just a few passive exposures, a process\nknown as fast mapping. This word learning capability is believed to be the most\nfundamental building block of multimodal understanding and reasoning. Despite\nrecent advancements in multimodal learning, a systematic and rigorous\nevaluation is still missing for human-like word learning in machines. To fill\nin this gap, we introduce the MachinE Word Learning (MEWL) benchmark to assess\nhow machines learn word meaning in grounded visual scenes. MEWL covers human's\ncore cognitive toolkits in word learning: cross-situational reasoning,\nbootstrapping, and pragmatic learning. Specifically, MEWL is a few-shot\nbenchmark suite consisting of nine tasks for probing various word learning\ncapabilities. These tasks are carefully designed to be aligned with the\nchildren's core abilities in word learning and echo the theories in the\ndevelopmental literature. By evaluating multimodal and unimodal agents'\nperformance with a comparative analysis of human performance, we notice a sharp\ndivergence in human and machine word learning. We further discuss these\ndifferences between humans and machines and call for human-like few-shot word\nlearning in machines.\n",
        "published": "2023",
        "authors": [
            "Guangyuan Jiang",
            "Manjie Xu",
            "Shiji Xin",
            "Wei Liang",
            "Yujia Peng",
            "Chi Zhang",
            "Yixin Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.01708v2",
        "title": "TIES-Merging: Resolving Interference When Merging Models",
        "abstract": "  Transfer learning - i.e., further fine-tuning a pre-trained model on a\ndownstream task - can confer significant advantages, including improved\ndownstream performance, faster convergence, and better sample efficiency. These\nadvantages have led to a proliferation of task-specific fine-tuned models,\nwhich typically can only perform a single task and do not benefit from one\nanother. Recently, model merging techniques have emerged as a solution to\ncombine multiple task-specific models into a single multitask model without\nperforming additional training. However, existing merging methods often ignore\nthe interference between parameters of different models, resulting in large\nperformance drops when merging multiple models. In this paper, we demonstrate\nthat prior merging techniques inadvertently lose valuable information due to\ntwo major sources of interference: (a) interference due to redundant parameter\nvalues and (b) disagreement on the sign of a given parameter's values across\nmodels. To address this, we propose our method, TRIM, ELECT SIGN & MERGE\n(TIES-Merging), which introduces three novel steps when merging models: (1)\nresetting parameters that only changed a small amount during fine-tuning, (2)\nresolving sign conflicts, and (3) merging only the parameters that are in\nalignment with the final agreed-upon sign. We find that TIES-Merging\noutperforms several existing methods in diverse settings covering a range of\nmodalities, domains, number of tasks, model sizes, architectures, and\nfine-tuning settings. We further analyze the impact of different types of\ninterference on model parameters, and highlight the importance of resolving\nsign interference. Our code is available at\nhttps://github.com/prateeky2806/ties-merging\n",
        "published": "2023",
        "authors": [
            "Prateek Yadav",
            "Derek Tam",
            "Leshem Choshen",
            "Colin Raffel",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.03954v1",
        "title": "Recognition of Handwritten Japanese Characters Using Ensemble of\n  Convolutional Neural Networks",
        "abstract": "  The Japanese writing system is complex, with three character types of\nHiragana, Katakana, and Kanji. Kanji consists of thousands of unique\ncharacters, further adding to the complexity of character identification and\nliterature understanding. Being able to translate handwritten Japanese\ncharacters into digital text is useful for data analysis, translation, learning\nand cultural preservation. In this study, a machine learning approach to\nanalyzing and recognizing handwritten Japanese characters (Kanji) is proposed.\nThe study used an ensemble of three convolutional neural networks (CNNs) for\nrecognizing handwritten Kanji characters and utilized four datasets of MNIST,\nK-MNIST, Kuzushiji-49 (K49) and the top 150 represented classes in the\nKuzushiji-Kanji (K-Kanji) dataset for its performance evaluation. The results\nindicate feasibility of using proposed CNN-ensemble architecture for\nrecognizing handwritten characters, achieving 99.4%, 96.4%, 95.0% and 96.4%\nclassification accuracy on MNIST, K-MNIS, K49, and K-Kanji datasets\nrespectively.\n",
        "published": "2023",
        "authors": [
            "Angel I. Solis",
            "Justin Zarkovacki",
            "John Ly",
            "Adham Atyabi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.05500v1",
        "title": "Word-Level Explanations for Analyzing Bias in Text-to-Image Models",
        "abstract": "  Text-to-image models take a sentence (i.e., prompt) and generate images\nassociated with this input prompt. These models have created award wining-art,\nvideos, and even synthetic datasets. However, text-to-image (T2I) models can\ngenerate images that underrepresent minorities based on race and sex. This\npaper investigates which word in the input prompt is responsible for bias in\ngenerated images. We introduce a method for computing scores for each word in\nthe prompt; these scores represent its influence on biases in the model's\noutput. Our method follows the principle of \\emph{explaining by removing},\nleveraging masked language models to calculate the influence scores. We perform\nexperiments on Stable Diffusion to demonstrate that our method identifies the\nreplication of societal stereotypes in generated images.\n",
        "published": "2023",
        "authors": [
            "Alexander Lin",
            "Lucas Monteiro Paes",
            "Sree Harsha Tanneru",
            "Suraj Srinivas",
            "Himabindu Lakkaraju"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.06094v1",
        "title": "Leveraging Large Language Models for Scalable Vector Graphics-Driven\n  Image Understanding",
        "abstract": "  Recently, large language models (LLMs) have made significant advancements in\nnatural language understanding and generation. However, their potential in\ncomputer vision remains largely unexplored. In this paper, we introduce a new,\nexploratory approach that enables LLMs to process images using the Scalable\nVector Graphics (SVG) format. By leveraging the XML-based textual descriptions\nof SVG representations instead of raster images, we aim to bridge the gap\nbetween the visual and textual modalities, allowing LLMs to directly understand\nand manipulate images without the need for parameterized visual components. Our\nmethod facilitates simple image classification, generation, and in-context\nlearning using only LLM capabilities. We demonstrate the promise of our\napproach across discriminative and generative tasks, highlighting its (i)\nrobustness against distribution shift, (ii) substantial improvements achieved\nby tapping into the in-context learning abilities of LLMs, and (iii) image\nunderstanding and generation capabilities with human guidance. Our code, data,\nand models can be found here https://github.com/mu-cai/svg-llm.\n",
        "published": "2023",
        "authors": [
            "Mu Cai",
            "Zeyi Huang",
            "Yuheng Li",
            "Haohan Wang",
            "Yong Jae Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.09869v3",
        "title": "Energy-Based Cross Attention for Bayesian Context Update in\n  Text-to-Image Diffusion Models",
        "abstract": "  Despite the remarkable performance of text-to-image diffusion models in image\ngeneration tasks, recent studies have raised the issue that generated images\nsometimes cannot capture the intended semantic contents of the text prompts,\nwhich phenomenon is often called semantic misalignment. To address this, here\nwe present a novel energy-based model (EBM) framework for adaptive context\ncontrol by modeling the posterior of context vectors. Specifically, we first\nformulate EBMs of latent image representations and text embeddings in each\ncross-attention layer of the denoising autoencoder. Then, we obtain the\ngradient of the log posterior of context vectors, which can be updated and\ntransferred to the subsequent cross-attention layer, thereby implicitly\nminimizing a nested hierarchy of energy functions. Our latent EBMs further\nallow zero-shot compositional generation as a linear combination of\ncross-attention outputs from different contexts. Using extensive experiments,\nwe demonstrate that the proposed method is highly effective in handling various\nimage generation tasks, including multi-concept generation, text-guided image\ninpainting, and real and synthetic image editing. Code:\nhttps://github.com/EnergyAttention/Energy-Based-CrossAttention.\n",
        "published": "2023",
        "authors": [
            "Geon Yeong Park",
            "Jeongsol Kim",
            "Beomsu Kim",
            "Sang Wan Lee",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.12929v2",
        "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads\n  Do Nothing",
        "abstract": "  Transformer models have been widely adopted in various domains over the last\nyears, and especially large language models have advanced the field of AI\nsignificantly. Due to their size, the capability of these networks has\nincreased tremendously, but this has come at the cost of a significant increase\nin necessary compute. Quantization is one of the most effective ways to reduce\nthe computational time and memory consumption of neural networks. Many studies\nhave shown, however, that modern transformer models tend to learn strong\noutliers in their activations, making them difficult to quantize. To retain\nacceptable performance, the existence of these outliers requires activations to\nbe in higher bitwidth or the use of different numeric formats, extra\nfine-tuning, or other workarounds. We show that strong outliers are related to\nvery specific behavior of attention heads that try to learn a \"no-op\" or just a\npartial update of the residual. To achieve the exact zeros needed in the\nattention matrix for a no-update, the input to the softmax is pushed to be\nlarger and larger during training, causing outliers in other parts of the\nnetwork. Based on these observations, we propose two simple (independent)\nmodifications to the attention mechanism - clipped softmax and gated attention.\nWe empirically show that models pre-trained using our methods learn\nsignificantly smaller outliers while maintaining and sometimes even improving\nthe floating-point task performance. This enables us to quantize transformers\nto full INT8 quantization of the activations without any additional effort. We\ndemonstrate the effectiveness of our methods on both language models (BERT,\nOPT) and vision transformers.\n",
        "published": "2023",
        "authors": [
            "Yelysei Bondarenko",
            "Markus Nagel",
            "Tijmen Blankevoort"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.13549v1",
        "title": "A Survey on Multimodal Large Language Models",
        "abstract": "  Multimodal Large Language Model (MLLM) recently has been a new rising\nresearch hotspot, which uses powerful Large Language Models (LLMs) as a brain\nto perform multimodal tasks. The surprising emergent capabilities of MLLM, such\nas writing stories based on images and OCR-free math reasoning, are rare in\ntraditional methods, suggesting a potential path to artificial general\nintelligence. In this paper, we aim to trace and summarize the recent progress\nof MLLM. First of all, we present the formulation of MLLM and delineate its\nrelated concepts. Then, we discuss the key techniques and applications,\nincluding Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning\n(M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning\n(LAVR). Finally, we discuss existing challenges and point out promising\nresearch directions. In light of the fact that the era of MLLM has only just\nbegun, we will keep updating this survey and hope it can inspire more research.\nAn associated GitHub link collecting the latest papers is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\n",
        "published": "2023",
        "authors": [
            "Shukang Yin",
            "Chaoyou Fu",
            "Sirui Zhao",
            "Ke Li",
            "Xing Sun",
            "Tong Xu",
            "Enhong Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.13841v1",
        "title": "Is Pre-training Truly Better Than Meta-Learning?",
        "abstract": "  In the context of few-shot learning, it is currently believed that a fixed\npre-trained (PT) model, along with fine-tuning the final layer during\nevaluation, outperforms standard meta-learning algorithms. We re-evaluate these\nclaims under an in-depth empirical examination of an extensive set of formally\ndiverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlike\nprevious work, we emphasize a fair comparison by using: the same architecture,\nthe same optimizer, and all models trained to convergence. Crucially, we use a\nmore rigorous statistical tool -- the effect size (Cohen's d) -- to determine\nthe practical significance of the difference between a model trained with PT\nvs. a MAML. We then use a previously proposed metric -- the diversity\ncoefficient -- to compute the average formal diversity of a dataset. Using this\nanalysis, we demonstrate the following: 1. when the formal diversity of a data\nset is low, PT beats MAML on average and 2. when the formal diversity is high,\nMAML beats PT on average. The caveat is that the magnitude of the average\ndifference between a PT vs. MAML using the effect size is low (according to\nclassical statistical thresholds) -- less than 0.2. Nevertheless, this\nobservation is contrary to the currently held belief that a pre-trained model\nis always better than a meta-learning model. Our extensive experiments consider\n21 few-shot learning benchmarks, including the large-scale few-shot learning\ndataset Meta-Data set. We also show no significant difference between a MAML\nmodel vs. a PT model with GPT-2 on Openwebtext. We, therefore, conclude that a\npre-trained model does not always beat a meta-learned model and that the formal\ndiversity of a dataset is a driving factor.\n",
        "published": "2023",
        "authors": [
            "Brando Miranda",
            "Patrick Yu",
            "Saumya Goyal",
            "Yu-Xiong Wang",
            "Sanmi Koyejo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.00028v1",
        "title": "Seeing in Words: Learning to Classify through Language Bottlenecks",
        "abstract": "  Neural networks for computer vision extract uninterpretable features despite\nachieving high accuracy on benchmarks. In contrast, humans can explain their\npredictions using succinct and intuitive descriptions. To incorporate\nexplainability into neural networks, we train a vision model whose feature\nrepresentations are text. We show that such a model can effectively classify\nImageNet images, and we discuss the challenges we encountered when training it.\n",
        "published": "2023",
        "authors": [
            "Khalid Saifullah",
            "Yuxin Wen",
            "Jonas Geiping",
            "Micah Goldblum",
            "Tom Goldstein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.01139v1",
        "title": "SCITUNE: Aligning Large Language Models with Scientific Multimodal\n  Instructions",
        "abstract": "  Instruction finetuning is a popular paradigm to align large language models\n(LLM) with human intent. Despite its popularity, this idea is less explored in\nimproving the LLMs to align existing foundation models with scientific\ndisciplines, concepts and goals. In this work, we present SciTune as a tuning\nframework to improve the ability of LLMs to follow scientific multimodal\ninstructions. To test our methodology, we use a human-generated scientific\ninstruction tuning dataset and train a large multimodal model LLaMA-SciTune\nthat connects a vision encoder and LLM for science-focused visual and language\nunderstanding. In comparison to the models that are finetuned with machine\ngenerated data only, LLaMA-SciTune surpasses human performance on average and\nin many sub-categories on the ScienceQA benchmark.\n",
        "published": "2023",
        "authors": [
            "Sameera Horawalavithana",
            "Sai Munikoti",
            "Ian Stewart",
            "Henry Kvinge"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.03135v3",
        "title": "Distilling Large Vision-Language Model with Out-of-Distribution\n  Generalizability",
        "abstract": "  Large vision-language models have achieved outstanding performance, but their\nsize and computational requirements make their deployment on\nresource-constrained devices and time-sensitive tasks impractical. Model\ndistillation, the process of creating smaller, faster models that maintain the\nperformance of larger models, is a promising direction towards the solution.\nThis paper investigates the distillation of visual representations in large\nteacher vision-language models into lightweight student models using a small-\nor mid-scale dataset. Notably, this study focuses on open-vocabulary\nout-of-distribution (OOD) generalization, a challenging problem that has been\noverlooked in previous model distillation literature. We propose two principles\nfrom vision and language modality perspectives to enhance student's OOD\ngeneralization: (1) by better imitating teacher's visual representation space,\nand carefully promoting better coherence in vision-language alignment with the\nteacher; (2) by enriching the teacher's language representations with\ninformative and finegrained semantic attributes to effectively distinguish\nbetween different labels. We propose several metrics and conduct extensive\nexperiments to investigate their techniques. The results demonstrate\nsignificant improvements in zero-shot and few-shot student performance on\nopen-vocabulary out-of-distribution classification, highlighting the\neffectiveness of our proposed approaches. Poster:\nhttps://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf\nCode: https://github.com/xuanlinli17/large_vlm_distillation_ood\n",
        "published": "2023",
        "authors": [
            "Xuanlin Li",
            "Yunhao Fang",
            "Minghua Liu",
            "Zhan Ling",
            "Zhuowen Tu",
            "Hao Su"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.03254v1",
        "title": "Vision Language Transformers: A Survey",
        "abstract": "  Vision language tasks, such as answering questions about or generating\ncaptions that describe an image, are difficult tasks for computers to perform.\nA relatively recent body of research has adapted the pretrained transformer\narchitecture introduced in \\citet{vaswani2017attention} to vision language\nmodeling. Transformer models have greatly improved performance and versatility\nover previous vision language models. They do so by pretraining models on a\nlarge generic datasets and transferring their learning to new tasks with minor\nchanges in architecture and parameter values. This type of transfer learning\nhas become the standard modeling practice in both natural language processing\nand computer vision. Vision language transformers offer the promise of\nproducing similar advancements in tasks which require both vision and language.\nIn this paper, we provide a broad synthesis of the currently available research\non vision language transformer models and offer some analysis of their\nstrengths, limitations and some open questions that remain.\n",
        "published": "2023",
        "authors": [
            "Clayton Fields",
            "Casey Kennington"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.05134v2",
        "title": "TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation",
        "abstract": "  The progress in the generation of synthetic images has made it crucial to\nassess their quality. While several metrics have been proposed to assess the\nrendering of images, it is crucial for Text-to-Image (T2I) models, which\ngenerate images based on a prompt, to consider additional aspects such as to\nwhich extent the generated image matches the important content of the prompt.\nMoreover, although the generated images usually result from a random starting\npoint, the influence of this one is generally not considered. In this article,\nwe propose a new metric based on prompt templates to study the alignment\nbetween the content specified in the prompt and the corresponding generated\nimages. It allows us to better characterize the alignment in terms of the type\nof the specified objects, their number, and their color. We conducted a study\non several recent T2I models about various aspects. An additional interesting\nresult we obtained with our approach is that image quality can vary drastically\ndepending on the noise used as a seed for the images. We also quantify the\ninfluence of the number of concepts in the prompt, their order as well as their\n(color) attributes. Finally, our method allows us to identify some seeds that\nproduce better images than others, opening novel directions of research on this\nunderstudied topic.\n",
        "published": "2023",
        "authors": [
            "Paul Grimal",
            "Herv\u00e9 Le Borgne",
            "Olivier Ferret",
            "Julien Tourille"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.11661v2",
        "title": "Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts",
        "abstract": "  Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have\nrevolutionized visual representation learning by providing good performance on\ndownstream datasets. VLMs are 0-shot adapted to a downstream dataset by\ndesigning prompts that are relevant to the dataset. Such prompt engineering\nmakes use of domain expertise and a validation dataset. Meanwhile, recent\ndevelopments in generative pretrained models like GPT-4 mean they can be used\nas advanced internet search tools. They can also be manipulated to provide\nvisual information in any structure. In this work, we show that GPT-4 can be\nused to generate text that is visually descriptive and how this can be used to\nadapt CLIP to downstream tasks. We show considerable improvements in 0-shot\ntransfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD\n(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt.\nWe also design a simple few-shot adapter that learns to choose the best\npossible sentences to construct generalizable classifiers that outperform the\nrecently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized\nfine-grained datasets. The code, prompts, and auxiliary text dataset is\navailable at https://github.com/mayug/VDT-Adapter.\n",
        "published": "2023",
        "authors": [
            "Mayug Maniparambil",
            "Chris Vorster",
            "Derek Molloy",
            "Noel Murphy",
            "Kevin McGuinness",
            "Noel E. O'Connor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.15199v2",
        "title": "PromptStyler: Prompt-driven Style Generation for Source-free Domain\n  Generalization",
        "abstract": "  In a joint vision-language space, a text feature (e.g., from \"a photo of a\ndog\") could effectively represent its relevant image features (e.g., from dog\nphotos). Also, a recent study has demonstrated the cross-modal transferability\nphenomenon of this joint space. From these observations, we propose\nPromptStyler which simulates various distribution shifts in the joint space by\nsynthesizing diverse styles via prompts without using any images to deal with\nsource-free domain generalization. The proposed method learns to generate a\nvariety of style features (from \"a S* style of a\") via learnable style word\nvectors for pseudo-words S*. To ensure that learned styles do not distort\ncontent information, we force style-content features (from \"a S* style of a\n[class]\") to be located nearby their corresponding content features (from\n\"[class]\") in the joint vision-language space. After learning style word\nvectors, we train a linear classifier using synthesized style-content features.\nPromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and\nDomainNet, even though it does not require any images for training.\n",
        "published": "2023",
        "authors": [
            "Junhyeong Cho",
            "Gilhyun Nam",
            "Sungyeon Kim",
            "Hunmin Yang",
            "Suha Kwak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.00675v1",
        "title": "Tool Documentation Enables Zero-Shot Tool-Usage with Large Language\n  Models",
        "abstract": "  Today, large language models (LLMs) are taught to use new tools by providing\na few demonstrations of the tool's usage. Unfortunately, demonstrations are\nhard to acquire, and can result in undesirable biased usage if the wrong\ndemonstration is chosen. Even in the rare scenario that demonstrations are\nreadily available, there is no principled selection protocol to determine how\nmany and which ones to provide. As tasks grow more complex, the selection\nsearch grows combinatorially and invariably becomes intractable. Our work\nprovides an alternative to demonstrations: tool documentation. We advocate the\nuse of tool documentation, descriptions for the individual tool usage, over\ndemonstrations. We substantiate our claim through three main empirical findings\non 6 tasks across both vision and language modalities. First, on existing\nbenchmarks, zero-shot prompts with only tool documentation are sufficient for\neliciting proper tool usage, achieving performance on par with few-shot\nprompts. Second, on a newly collected realistic tool-use dataset with hundreds\nof available tool APIs, we show that tool documentation is significantly more\nvaluable than demonstrations, with zero-shot documentation significantly\noutperforming few-shot without documentation. Third, we highlight the benefits\nof tool documentations by tackling image generation and video tracking using\njust-released unseen state-of-the-art models as tools. Finally, we highlight\nthe possibility of using tool documentation to automatically enable new\napplications: by using nothing more than the documentation of GroundingDino,\nStable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the\njust-released Grounded-SAM and Track Anything models.\n",
        "published": "2023",
        "authors": [
            "Cheng-Yu Hsieh",
            "Si-An Chen",
            "Chun-Liang Li",
            "Yasuhisa Fujii",
            "Alexander Ratner",
            "Chen-Yu Lee",
            "Ranjay Krishna",
            "Tomas Pfister"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.01313v2",
        "title": "More Context, Less Distraction: Zero-shot Visual Classification by\n  Inferring and Conditioning on Contextual Attributes",
        "abstract": "  Vision-language models like CLIP are widely used in zero-shot image\nclassification due to their ability to understand various visual concepts and\nnatural language descriptions. However, how to fully leverage CLIP's\nunprecedented human-like understanding capabilities to achieve better\nperformance is still an open question. This paper draws inspiration from the\nhuman visual perception process: when classifying an object, humans first infer\ncontextual attributes (e.g., background and orientation) which help separate\nthe foreground object from the background, and then classify the object based\non this information. Inspired by it, we observe that providing CLIP with\ncontextual attributes improves zero-shot image classification and mitigates\nreliance on spurious features. We also observe that CLIP itself can reasonably\ninfer the attributes from an image. With these observations, we propose a\ntraining-free, two-step zero-shot classification method PerceptionCLIP. Given\nan image, it first infers contextual attributes (e.g., background) and then\nperforms object classification conditioning on them. Our experiments show that\nPerceptionCLIP achieves better generalization, group robustness, and\ninterpretability. For example, PerceptionCLIP with ViT-L/14 improves the worst\ngroup accuracy by 16.5% on the Waterbirds dataset and by 3.5% on CelebA.\n",
        "published": "2023",
        "authors": [
            "Bang An",
            "Sicheng Zhu",
            "Michael-Andrei Panaitescu-Liess",
            "Chaithanya Kumar Mummadi",
            "Furong Huang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.02570v1",
        "title": "Learning Implicit Entity-object Relations by Bidirectional Generative\n  Alignment for Multimodal NER",
        "abstract": "  The challenge posed by multimodal named entity recognition (MNER) is mainly\ntwo-fold: (1) bridging the semantic gap between text and image and (2) matching\nthe entity with its associated object in image. Existing methods fail to\ncapture the implicit entity-object relations, due to the lack of corresponding\nannotation. In this paper, we propose a bidirectional generative alignment\nmethod named BGA-MNER to tackle these issues. Our BGA-MNER consists of\n\\texttt{image2text} and \\texttt{text2image} generation with respect to\nentity-salient content in two modalities. It jointly optimizes the\nbidirectional reconstruction objectives, leading to aligning the implicit\nentity-object relations under such direct and powerful constraints.\nFurthermore, image-text pairs usually contain unmatched components which are\nnoisy for generation. A stage-refined context sampler is proposed to extract\nthe matched cross-modal content for generation. Extensive experiments on two\nbenchmarks demonstrate that our method achieves state-of-the-art performance\nwithout image input during inference.\n",
        "published": "2023",
        "authors": [
            "Feng Chen",
            "Jiajia Liu",
            "Kaixiang Ji",
            "Wang Ren",
            "Jian Wang",
            "Jingdong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.07706v2",
        "title": "Exploring Transfer Learning in Medical Image Segmentation using\n  Vision-Language Models",
        "abstract": "  Medical image segmentation with deep learning is an important and widely\nstudied topic because segmentation enables quantifying target structure size\nand shape that can help in disease diagnosis, prognosis, surgery planning, and\nunderstanding. Recent advances in the foundation VLMs and their adaptation to\nsegmentation tasks in natural images with VLSMs have opened up a unique\nopportunity to build potentially powerful segmentation models for medical\nimages that enable providing helpful information via language prompt as input,\nleverage the extensive range of other medical imaging datasets by pooled\ndataset training, adapt to new classes, and be robust against\nout-of-distribution data with human-in-the-loop prompting during inference.\nAlthough transfer learning from natural to medical images for image-only\nsegmentation models has been studied, no studies have analyzed how the joint\nrepresentation of vision-language transfers to medical images in segmentation\nproblems and understand gaps in leveraging their full potential. We present the\nfirst benchmark study on transfer learning of VLSMs to 2D medical images with\nthoughtfully collected 11 existing 2D medical image datasets of diverse\nmodalities with carefully presented 9 types of language prompts from 14\nattributes. Our results indicate that VLSMs trained in natural image-text pairs\ntransfer reasonably to the medical domain in zero-shot settings when prompted\nappropriately for non-radiology photographic modalities; when finetuned, they\nobtain comparable performance to conventional architectures, even in X-rays and\nultrasound modalities. However, the additional benefit of language prompts\nduring finetuning may be limited, with image features playing a more dominant\nrole; they can better handle training on pooled datasets combining diverse\nmodalities and are potentially more robust to domain shift than the\nconventional segmentation models.\n",
        "published": "2023",
        "authors": [
            "Kanchan Poudel",
            "Manish Dhakal",
            "Prasiddha Bhandari",
            "Rabin Adhikari",
            "Safal Thapaliya",
            "Bishesh Khanal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.07871v1",
        "title": "Emotion Embeddings $\\unicode{x2014}$ Learning Stable and Homogeneous\n  Abstractions from Heterogeneous Affective Datasets",
        "abstract": "  Human emotion is expressed in many communication modalities and media formats\nand so their computational study is equally diversified into natural language\nprocessing, audio signal analysis, computer vision, etc. Similarly, the large\nvariety of representation formats used in previous research to describe\nemotions (polarity scales, basic emotion categories, dimensional approaches,\nappraisal theory, etc.) have led to an ever proliferating diversity of\ndatasets, predictive models, and software tools for emotion analysis. Because\nof these two distinct types of heterogeneity, at the expressional and\nrepresentational level, there is a dire need to unify previous work on\nincreasingly diverging data and label types. This article presents such a\nunifying computational model. We propose a training procedure that learns a\nshared latent representation for emotions, so-called emotion embeddings,\nindependent of different natural languages, communication modalities, media or\nrepresentation label formats, and even disparate model architectures.\nExperiments on a wide range of heterogeneous affective datasets indicate that\nthis approach yields the desired interoperability for the sake of reusability,\ninterpretability and flexibility, without penalizing prediction quality. Code\nand data are archived under https://doi.org/10.5281/zenodo.7405327 .\n",
        "published": "2023",
        "authors": [
            "Sven Buechel",
            "Udo Hahn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.09804v1",
        "title": "VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity\n  Control",
        "abstract": "  As the model size of pre-trained language models (PLMs) grows rapidly, full\nfine-tuning becomes prohibitively expensive for model training and storage. In\nvision-and-language (VL), parameter-efficient tuning (PET) techniques are\nproposed to integrate modular modifications (e.g., Adapter and LoRA) into\nencoder-decoder PLMs. By tuning a small set of trainable parameters, these\ntechniques perform on par with full fine-tuning. However, excessive modular\nmodifications and neglecting the functionality gap between the encoders and\ndecoders can lead to performance degradation, while existing PET techniques\n(e.g., VL-Adapter) overlook these critical issues. In this paper, we propose a\nVision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose\neffective control over modular modifications via a novel granularity-controlled\nmechanism. Considering different granularity-controlled matrices generated by\nthis mechanism, a variety of model-agnostic VL-PET modules can be instantiated\nfrom our framework for better efficiency and effectiveness trade-offs. We\nfurther propose lightweight PET module designs to enhance VL alignment and\nmodeling for the encoders and maintain text generation for the decoders.\nExtensive experiments conducted on four image-text tasks and four video-text\ntasks demonstrate the efficiency, effectiveness and transferability of our\nVL-PET framework. In particular, our VL-PET-large with lightweight PET module\ndesigns significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37%\n(7.03%) with BART-base (T5-base) on image-text tasks. Furthermore, we validate\nthe enhanced effect of employing our VL-PET designs on existing PET techniques,\nenabling them to achieve significant performance improvements. Our code is\navailable at https://github.com/HenryHZY/VL-PET.\n",
        "published": "2023",
        "authors": [
            "Zi-Yuan Hu",
            "Yanyang Li",
            "Michael R. Lyu",
            "Liwei Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.09936v3",
        "title": "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual\n  Questions",
        "abstract": "  Vision Language Models (VLMs), which extend Large Language Models (LLM) by\nincorporating visual understanding capability, have demonstrated significant\nadvancements in addressing open-ended visual question-answering (VQA) tasks.\nHowever, these models cannot accurately interpret images infused with text, a\ncommon occurrence in real-world scenarios. Standard procedures for extracting\ninformation from images often involve learning a fixed set of query embeddings.\nThese embeddings are designed to encapsulate image contexts and are later used\nas soft prompt inputs in LLMs. Yet, this process is limited to the token count,\npotentially curtailing the recognition of scenes with text-rich context. To\nimprove upon them, the present study introduces BLIVA: an augmented version of\nInstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings\nfrom InstructBLIP and also directly projects encoded patch embeddings into the\nLLM, a technique inspired by LLaVA. This approach assists the model to capture\nintricate details potentially missed during the query decoding process.\nEmpirical evidence demonstrates that our model, BLIVA, significantly enhances\nperformance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA\nbenchmark) and in undertaking general (not particularly text-rich) VQA\nbenchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved\n17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME),\ncomparing to our baseline InstructBLIP. BLIVA demonstrates significant\ncapability in decoding real-world images, irrespective of text presence. To\ndemonstrate the broad industry applications enabled by BLIVA, we evaluate the\nmodel using a new dataset comprising YouTube thumbnails paired with\nquestion-answer sets across 11 diverse categories. Our code and models are\nfreely accessible at https://github.com/mlpc-ucsd/BLIVA.\n",
        "published": "2023",
        "authors": [
            "Wenbo Hu",
            "Yifan Xu",
            "Yi Li",
            "Weiyue Li",
            "Zeyuan Chen",
            "Zhuowen Tu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.11696v3",
        "title": "Efficient Benchmarking (of Language Models)",
        "abstract": "  The increasing versatility of language models LMs has given rise to a new\nclass of benchmarks that comprehensively assess a broad range of capabilities.\nSuch benchmarks are associated with massive computational costs reaching\nthousands of GPU hours per model. However the efficiency aspect of these\nevaluation efforts had raised little discussion in the literature. In this work\nwe present the problem of Efficient Benchmarking namely intelligently reducing\nthe computation costs of LM evaluation without compromising reliability. Using\nthe HELM benchmark as a test case we investigate how different benchmark design\nchoices affect the computation-reliability tradeoff. We propose to evaluate the\nreliability of such decisions by using a new measure Decision Impact on\nReliability DIoR for short. We find for example that the current leader on HELM\nmay change by merely removing a low-ranked model from the benchmark and observe\nthat a handful of examples suffice to obtain the correct benchmark ranking.\nConversely a slightly different choice of HELM scenarios varies ranking widely.\nBased on our findings we outline a set of concrete recommendations for more\nefficient benchmark design and utilization practices leading to dramatic cost\nsavings with minimal loss of benchmark reliability often reducing computation\nby x100 or more.\n",
        "published": "2023",
        "authors": [
            "Yotam Perlitz",
            "Elron Bandel",
            "Ariel Gera",
            "Ofir Arviv",
            "Liat Ein-Dor",
            "Eyal Shnarch",
            "Noam Slonim",
            "Michal Shmueli-Scheuer",
            "Leshem Choshen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.13566v2",
        "title": "MLLM-DataEngine: An Iterative Refinement Approach for MLLM",
        "abstract": "  Despite the great advance of Multimodal Large Language Models (MLLMs) in both\ninstruction dataset building and benchmarking, the independence of training and\nevaluation makes current MLLMs hard to further improve their capability under\nthe guidance of evaluation results with a relatively low human cost. In this\npaper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data\ngeneration, model training, and evaluation. Within each loop iteration, the\nMLLM-DataEngine first analyze the weakness of the model based on the evaluation\nresults, then generate a proper incremental dataset for the next training\niteration and enhance the model capability iteratively. Compared with previous\ndata collection methods which are separate from the benchmarking, the data\ngenerated by MLLM-DataEngine shows better targeting, quality, and correctness.\nFor targeting, we propose an Adaptive Bad-case Sampling module, which adjusts\nthe ratio of different types of data within each incremental dataset based on\nthe benchmarking results. For quality, we resort to GPT-4 to generate\nhigh-quality data with each given data type. For correctness, prompt design is\ncritical for the data generation results. Rather than previous hand-crafted\nprompt, we propose an Interactive Prompt Optimization strategy, which optimizes\nthe prompt with the multi-round interaction between human and GPT, and improve\nthe correctness of generated data greatly. Through extensive experiments, we\nfind our MLLM-DataEngine could boost the MLLM capability in a targeted and\nautomatic manner, with only a few human participation. We hope it could be a\ngeneral solution for the following MLLMs building. The MLLM-DataEngine has been\nopen-sourced and is now available at\nhttps://github.com/opendatalab/MLLM-DataEngine.\n",
        "published": "2023",
        "authors": [
            "Zhiyuan Zhao",
            "Linke Ouyang",
            "Bin Wang",
            "Siyuan Huang",
            "Pan Zhang",
            "Xiaoyi Dong",
            "Jiaqi Wang",
            "Conghui He"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.05173v3",
        "title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning",
        "abstract": "  Prompt tuning (PT), where a small amount of trainable soft (continuous)\nprompt vectors is affixed to the input of language models (LM), has shown\npromising results across various tasks and models for parameter-efficient\nfine-tuning (PEFT). PT stands out from other PEFT approaches because it\nmaintains competitive performance with fewer trainable parameters and does not\ndrastically scale up its parameters as the model size expands. However, PT\nintroduces additional soft prompt tokens, leading to longer input sequences,\nwhich significantly impacts training and inference time and memory usage due to\nthe Transformer's quadratic complexity. Particularly concerning for Large\nLanguage Models (LLMs) that face heavy daily querying. To address this issue,\nwe propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt\ninto a shorter soft prompt and a pair of low-rank matrices that are then\noptimised with two different learning rates. This allows DePT to achieve better\nperformance while saving over 20% memory and time costs compared to vanilla PT\nand its variants, without changing trainable parameter sizes. Through extensive\nexperiments on 23 natural language processing (NLP) and vision-language (VL)\ntasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches,\nincluding the full fine-tuning baseline in some scenarios. Additionally, we\nempirically show that DEPT grows more efficient as the model size increases.\nOur further study reveals that DePT integrates seamlessly with\nparameter-efficient transfer learning in the few-shot learning setting and\nhighlights its adaptability to various model architectures and sizes.\n",
        "published": "2023",
        "authors": [
            "Zhengxiang Shi",
            "Aldo Lipani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.05569v1",
        "title": "ITI-GEN: Inclusive Text-to-Image Generation",
        "abstract": "  Text-to-image generative models often reflect the biases of the training\ndata, leading to unequal representations of underrepresented groups. This study\ninvestigates inclusive text-to-image generative models that generate images\nbased on human-written prompts and ensure the resulting images are uniformly\ndistributed across attributes of interest. Unfortunately, directly expressing\nthe desired attributes in the prompt often leads to sub-optimal results due to\nlinguistic ambiguity or model misrepresentation. Hence, this paper proposes a\ndrastically different approach that adheres to the maxim that \"a picture is\nworth a thousand words\". We show that, for some attributes, images can\nrepresent concepts more expressively than text. For instance, categories of\nskin tones are typically hard to specify by text but can be easily represented\nby example images. Building upon these insights, we propose a novel approach,\nITI-GEN, that leverages readily available reference images for Inclusive\nText-to-Image GENeration. The key idea is learning a set of prompt embeddings\nto generate images that can effectively represent all desired attribute\ncategories. More importantly, ITI-GEN requires no model fine-tuning, making it\ncomputationally efficient to augment existing text-to-image models. Extensive\nexperiments demonstrate that ITI-GEN largely improves over state-of-the-art\nmodels to generate inclusive images from a prompt. Project page:\nhttps://czhang0528.github.io/iti-gen.\n",
        "published": "2023",
        "authors": [
            "Cheng Zhang",
            "Xuanbai Chen",
            "Siqi Chai",
            "Chen Henry Wu",
            "Dmitry Lagun",
            "Thabo Beeler",
            "Fernando De la Torre"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.10091v1",
        "title": "Unified Coarse-to-Fine Alignment for Video-Text Retrieval",
        "abstract": "  The canonical approach to video-text retrieval leverages a coarse-grained or\nfine-grained alignment between visual and textual information. However,\nretrieving the correct video according to the text query is often challenging\nas it requires the ability to reason about both high-level (scene) and\nlow-level (object) visual clues and how they relate to the text query. To this\nend, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA.\nSpecifically, our model captures the cross-modal similarity information at\ndifferent granularity levels. To alleviate the effect of irrelevant visual\nclues, we also apply an Interactive Similarity Aggregation module (ISA) to\nconsider the importance of different visual features while aggregating the\ncross-modal similarity to obtain a similarity score for each granularity.\nFinally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of\neach level before summing them, alleviating over- and under-representation\nissues at different levels. By jointly considering the crossmodal similarity of\ndifferent granularity, UCoFiA allows the effective unification of multi-grained\nalignments. Empirically, UCoFiA outperforms previous state-of-the-art\nCLIP-based methods on multiple video-text retrieval benchmarks, achieving 2.4%,\n1.4% and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT,\nActivity-Net, and DiDeMo, respectively. Our code is publicly available at\nhttps://github.com/Ziyang412/UCoFiA.\n",
        "published": "2023",
        "authors": [
            "Ziyang Wang",
            "Yi-Lin Sung",
            "Feng Cheng",
            "Gedas Bertasius",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.12829v1",
        "title": "Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language\n  Segmentation in Echocardiography",
        "abstract": "  Accurate segmentation is essential for echocardiography-based assessment of\ncardiovascular diseases (CVDs). However, the variability among sonographers and\nthe inherent challenges of ultrasound images hinder precise segmentation. By\nleveraging the joint representation of image and text modalities,\nVision-Language Segmentation Models (VLSMs) can incorporate rich contextual\ninformation, potentially aiding in accurate and explainable segmentation.\nHowever, the lack of readily available data in echocardiography hampers the\ntraining of VLSMs. In this study, we explore using synthetic datasets from\nSemantic Diffusion Models (SDMs) to enhance VLSMs for echocardiography\nsegmentation. We evaluate results for two popular VLSMs (CLIPSeg and CRIS)\nusing seven different kinds of language prompts derived from several\nattributes, automatically extracted from echocardiography images, segmentation\nmasks, and their metadata. Our results show improved metrics and faster\nconvergence when pretraining VLSMs on SDM-generated synthetic images before\nfinetuning on real images. The code, configs, and prompts are available at\nhttps://github.com/naamiinepal/synthetic-boost.\n",
        "published": "2023",
        "authors": [
            "Rabin Adhikari",
            "Manish Dhakal",
            "Safal Thapaliya",
            "Kanchan Poudel",
            "Prasiddha Bhandari",
            "Bishesh Khanal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.13952v1",
        "title": "VidChapters-7M: Video Chapters at Scale",
        "abstract": "  Segmenting long videos into chapters enables users to quickly navigate to the\ninformation of their interest. This important topic has been understudied due\nto the lack of publicly released datasets. To address this issue, we present\nVidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters\nin total. VidChapters-7M is automatically created from videos online in a\nscalable manner by scraping user-annotated chapters and hence without any\nadditional manual annotation. We introduce the following three tasks based on\nthis data. First, the video chapter generation task consists of temporally\nsegmenting the video and generating a chapter title for each segment. To\nfurther dissect the problem, we also define two variants of this task: video\nchapter generation given ground-truth boundaries, which requires generating a\nchapter title given an annotated video segment, and video chapter grounding,\nwhich requires temporally localizing a chapter given its annotated title. We\nbenchmark both simple baselines and state-of-the-art video-language models for\nthese three tasks. We also show that pretraining on VidChapters-7M transfers\nwell to dense video captioning tasks in both zero-shot and finetuning settings,\nlargely improving the state of the art on the YouCook2 and ViTT benchmarks.\nFinally, our experiments reveal that downstream performance scales well with\nthe size of the pretraining dataset. Our dataset, code, and models are publicly\navailable at https://antoyang.github.io/vidchapters.html.\n",
        "published": "2023",
        "authors": [
            "Antoine Yang",
            "Arsha Nagrani",
            "Ivan Laptev",
            "Josef Sivic",
            "Cordelia Schmid"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.15091v1",
        "title": "VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided\n  Planning",
        "abstract": "  Although recent text-to-video (T2V) generation methods have seen significant\nadvancements, most of these works focus on producing short video clips of a\nsingle event with a single background (i.e., single-scene videos). Meanwhile,\nrecent large language models (LLMs) have demonstrated their capability in\ngenerating layouts and programs to control downstream visual modules such as\nimage generation models. This raises an important question: can we leverage the\nknowledge embedded in these LLMs for temporally consistent long video\ngeneration? In this paper, we propose VideoDirectorGPT, a novel framework for\nconsistent multi-scene video generation that uses the knowledge of LLMs for\nvideo content planning and grounded video generation. Specifically, given a\nsingle text prompt, we first ask our video planner LLM (GPT-4) to expand it\ninto a 'video plan', which involves generating the scene descriptions, the\nentities with their respective layouts, the background for each scene, and\nconsistency groupings of the entities and backgrounds. Next, guided by this\noutput from the video planner, our video generator, Layout2Vid, has explicit\ncontrol over spatial layouts and can maintain temporal consistency of\nentities/backgrounds across scenes, while only trained with image-level\nannotations. Our experiments demonstrate that VideoDirectorGPT framework\nsubstantially improves layout and movement control in both single- and\nmulti-scene video generation and can generate multi-scene videos with visual\nconsistency across scenes, while achieving competitive performance with SOTAs\nin open-domain single-scene T2V generation. We also demonstrate that our\nframework can dynamically control the strength for layout guidance and can also\ngenerate videos with user-provided images. We hope our framework can inspire\nfuture work on better integrating the planning ability of LLMs into consistent\nlong video generation.\n",
        "published": "2023",
        "authors": [
            "Han Lin",
            "Abhay Zala",
            "Jaemin Cho",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.00811v1",
        "title": "Sparse Backpropagation for MoE Training",
        "abstract": "  One defining characteristic of Mixture-of-Expert (MoE) models is their\ncapacity for conducting sparse computation via expert routing, leading to\nremarkable scalability. However, backpropagation, the cornerstone of deep\nlearning, requires dense computation, thereby posting challenges in MoE\ngradient computations. Here, we introduce SparseMixer, a scalable gradient\nestimator that bridges the gap between backpropagation and sparse expert\nrouting. Unlike typical MoE training which strategically neglects certain\ngradient terms for the sake of sparse computation and scalability, SparseMixer\nprovides scalable gradient approximations for these terms, enabling reliable\ngradient estimation in MoE training. Grounded in a numerical ODE framework,\nSparseMixer harnesses the mid-point method, a second-order ODE solver, to\ndeliver precise gradient approximations with negligible computational overhead.\nApplying SparseMixer to Switch Transformer on both pre-training and machine\ntranslation tasks, SparseMixer showcases considerable performance gain,\naccelerating training convergence up to 2 times.\n",
        "published": "2023",
        "authors": [
            "Liyuan Liu",
            "Jianfeng Gao",
            "Weizhu Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.01825v2",
        "title": "Empirical Study of PEFT techniques for Winter Wheat Segmentation",
        "abstract": "  Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced\nsignificant growth and have been extensively employed to adapt large vision and\nlanguage models to various domains, enabling satisfactory model performance\nwith minimal computational needs. Despite these advances, more research has yet\nto delve into potential PEFT applications in real-life scenarios, particularly\nin the critical domains of remote sensing and crop monitoring. The diversity of\nclimates across different regions and the need for comprehensive large-scale\ndatasets have posed significant obstacles to accurately identify crop types\nacross varying geographic locations and changing growing seasons. This study\nseeks to bridge this gap by comprehensively exploring the feasibility of\ncross-area and cross-year out-of-distribution generalization using the\nState-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to\nexplore PEFT approaches for crop monitoring. Specifically, we focus on adapting\nthe SOTA TSViT model to address winter wheat field segmentation, a critical\ntask for crop monitoring and food security. This adaptation process involves\nintegrating different PEFT techniques, including BigFit, LoRA, Adaptformer, and\nprompt tuning. Using PEFT techniques, we achieved notable results comparable to\nthose achieved using full fine-tuning methods while training only a mere 0.7%\nparameters of the whole TSViT architecture. The in-house labeled data-set,\nreferred to as the Beqaa-Lebanon dataset, comprises high-quality annotated\npolygons for wheat and non-wheat classes with a total surface of 170 kmsq, over\nfive consecutive years. Using Sentinel-2 images, our model achieved a 84%\nF1-score. We intend to publicly release the Lebanese winter wheat data set,\ncode repository, and model weights.\n",
        "published": "2023",
        "authors": [
            "Mohamad Hasan Zahweh",
            "Hasan Nasrallah",
            "Mustafa Shukor",
            "Ghaleb Faour",
            "Ali J. Ghandour"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.01828v2",
        "title": "Trainable Noise Model as an XAI evaluation method: application on Sobol\n  for remote sensing image segmentation",
        "abstract": "  eXplainable Artificial Intelligence (XAI) has emerged as an essential\nrequirement when dealing with mission-critical applications, ensuring\ntransparency and interpretability of the employed black box AI models. The\nsignificance of XAI spans various domains, from healthcare to finance, where\nunderstanding the decision-making process of deep learning algorithms is\nessential. Most AI-based computer vision models are often black boxes; hence,\nproviding explainability of deep neural networks in image processing is crucial\nfor their wide adoption and deployment in medical image analysis, autonomous\ndriving, and remote sensing applications. Recently, several XAI methods for\nimage classification tasks have been introduced. On the contrary, image\nsegmentation has received comparatively less attention in the context of\nexplainability, although it is a fundamental task in computer vision\napplications, especially in remote sensing. Only some research proposes\ngradient-based XAI algorithms for image segmentation. This paper adapts the\nrecent gradient-free Sobol XAI method for semantic segmentation. To measure the\nperformance of the Sobol method for segmentation, we propose a quantitative XAI\nevaluation method based on a learnable noise model. The main objective of this\nmodel is to induce noise on the explanation maps, where higher induced noise\nsignifies low accuracy and vice versa. A benchmark analysis is conducted to\nevaluate and compare performance of three XAI methods, including Seg-Grad-CAM,\nSeg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation\ntechnique. This constitutes the first attempt to run and evaluate XAI methods\nusing high-resolution satellite images.\n",
        "published": "2023",
        "authors": [
            "Hossein Shreim",
            "Abdul Karim Gizzini",
            "Ali J. Ghandour"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.01837v2",
        "title": "Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation",
        "abstract": "  Current AI-based methods do not provide comprehensible physical\ninterpretations of the utilized data, extracted features, and\npredictions/inference operations. As a result, deep learning models trained\nusing high-resolution satellite imagery lack transparency and explainability\nand can be merely seen as a black box, which limits their wide-level adoption.\nExperts need help understanding the complex behavior of AI models and the\nunderlying decision-making process. The explainable artificial intelligence\n(XAI) field is an emerging field providing means for robust, practical, and\ntrustworthy deployment of AI models. Several XAI techniques have been proposed\nfor image classification tasks, whereas the interpretation of image\nsegmentation remains largely unexplored. This paper offers to bridge this gap\nby adapting the recent XAI classification algorithms and making them usable for\nmuti-class image segmentation, where we mainly focus on buildings' segmentation\nfrom high-resolution satellite images. To benchmark and compare the performance\nof the proposed approaches, we introduce a new XAI evaluation methodology and\nmetric based on \"Entropy\" to measure the model uncertainty. Conventional XAI\nevaluation methods rely mainly on feeding area-of-interest regions from the\nimage back to the pre-trained (utility) model and then calculating the average\nchange in the probability of the target class. Those evaluation metrics lack\nthe needed robustness, and we show that using Entropy to monitor the model\nuncertainty in segmenting the pixels within the target class is more suitable.\nWe hope this work will pave the way for additional XAI research for image\nsegmentation and applications in the remote sensing discipline.\n",
        "published": "2023",
        "authors": [
            "Abdul Karim Gizzini",
            "Mustafa Shukor",
            "Ali J. Ghandour"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.01845v1",
        "title": "Zero-Shot Refinement of Buildings' Segmentation Models using SAM",
        "abstract": "  Foundation models have excelled in various tasks but are often evaluated on\ngeneral benchmarks. The adaptation of these models for specific domains, such\nas remote sensing imagery, remains an underexplored area. In remote sensing,\nprecise building instance segmentation is vital for applications like urban\nplanning. While Convolutional Neural Networks (CNNs) perform well, their\ngeneralization can be limited. For this aim, we present a novel approach to\nadapt foundation models to address existing models' generalization dropback.\nAmong several models, our focus centers on the Segment Anything Model (SAM), a\npotent foundation model renowned for its prowess in class-agnostic image\nsegmentation capabilities. We start by identifying the limitations of SAM,\nrevealing its suboptimal performance when applied to remote sensing imagery.\nMoreover, SAM does not offer recognition abilities and thus fails to classify\nand tag localized objects. To address these limitations, we introduce different\nprompting strategies, including integrating a pre-trained CNN as a prompt\ngenerator. This novel approach augments SAM with recognition abilities, a first\nof its kind. We evaluated our method on three remote sensing datasets,\nincluding the WHU Buildings dataset, the Massachusetts Buildings dataset, and\nthe AICrowd Mapping Challenge. For out-of-distribution performance on the WHU\ndataset, we achieve a 5.47% increase in IoU and a 4.81% improvement in\nF1-score. For in-distribution performance on the WHU dataset, we observe a\n2.72% and 1.58% increase in True-Positive-IoU and True-Positive-F1 score,\nrespectively. We intend to release our code repository, hoping to inspire\nfurther exploration of foundation models for domain-specific tasks within the\nremote sensing community.\n",
        "published": "2023",
        "authors": [
            "Ali Mayladan",
            "Hasan Nasrallah",
            "Hasan Moughnieh",
            "Mustafa Shukor",
            "Ali J. Ghandour"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.02255v2",
        "title": "MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V,\n  Bard, and Other Large Multimodal Models",
        "abstract": "  Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit\nimpressive problem-solving skills in many tasks and domains, but their ability\nin mathematical reasoning in visual contexts has not been systematically\nstudied. To bridge this gap, we present MathVista, a benchmark designed to\ncombine challenges from diverse mathematical and visual tasks. It consists of\n6,141 examples, derived from 28 existing multimodal datasets involving\nmathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and\nPaperQA). Completing these tasks requires fine-grained, deep visual\nunderstanding and compositional reasoning, which all state-of-the-art\nfoundation models find challenging. With MathVista, we have conducted a\ncomprehensive, quantitative evaluation of 12 prominent foundation models. The\nbest-performing GPT-4V model achieves an overall accuracy of 49.9%,\nsubstantially outperforming Bard, the second-best performer, by 15.1%. Our\nin-depth analysis reveals that the superiority of GPT-4V is mainly attributed\nto its enhanced visual perception and mathematical reasoning. However, GPT-4V\nstill falls short of human performance by 10.4%, as it often struggles to\nunderstand complex figures and perform rigorous reasoning. This significant gap\nunderscores the critical role that MathVista will play in the development of\ngeneral-purpose AI agents capable of tackling mathematically intensive and\nvisually rich real-world tasks. We further explore the new ability of\nself-verification, the application of self-consistency, and the interactive\nchatbot capabilities of GPT-4V, highlighting its promising potential for future\nresearch. The project is available at https://mathvista.github.io/.\n",
        "published": "2023",
        "authors": [
            "Pan Lu",
            "Hritik Bansal",
            "Tony Xia",
            "Jiacheng Liu",
            "Chunyuan Li",
            "Hannaneh Hajishirzi",
            "Hao Cheng",
            "Kai-Wei Chang",
            "Michel Galley",
            "Jianfeng Gao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.02567v2",
        "title": "Improving Automatic VQA Evaluation Using Large Language Models",
        "abstract": "  8 years after the visual question answering (VQA) task was proposed, accuracy\nremains the primary metric for automatic evaluation. VQA Accuracy has been\neffective so far in the IID evaluation setting. However, our community is\nundergoing a shift towards open-ended generative models and OOD evaluation. In\nthis new paradigm, the existing VQA Accuracy metric is overly stringent and\nunderestimates the performance of VQA systems. Thus, there is a need to develop\nmore robust automatic VQA metrics that serve as a proxy for human judgment. In\nthis work, we propose to leverage the in-context learning capabilities of\ninstruction-tuned large language models (LLMs) to build a better VQA metric. We\nformulate VQA evaluation as an answer-rating task where the LLM is instructed\nto score the accuracy of a candidate answer given a set of reference answers.\nWe demonstrate the proposed metric better correlates with human judgment\ncompared to existing metrics across several VQA models and benchmarks. We hope\nwide adoption of our metric will contribute to better estimating the research\nprogress on the VQA task. We plan to release the evaluation code and collected\nhuman judgments.\n",
        "published": "2023",
        "authors": [
            "Oscar Ma\u00f1as",
            "Benno Krojer",
            "Aishwarya Agrawal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.03744v1",
        "title": "Improved Baselines with Visual Instruction Tuning",
        "abstract": "  Large multimodal models (LMM) have recently shown encouraging progress with\nvisual instruction tuning. In this note, we show that the fully-connected\nvision-language cross-modal connector in LLaVA is surprisingly powerful and\ndata-efficient. With simple modifications to LLaVA, namely, using\nCLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA\ndata with simple response formatting prompts, we establish stronger baselines\nthat achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint\nuses merely 1.2M publicly available data, and finishes full training in ~1 day\non a single 8-A100 node. We hope this can make state-of-the-art LMM research\nmore accessible. Code and model will be publicly available.\n",
        "published": "2023",
        "authors": [
            "Haotian Liu",
            "Chunyuan Li",
            "Yuheng Li",
            "Yong Jae Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.04406v2",
        "title": "Language Agent Tree Search Unifies Reasoning Acting and Planning in\n  Language Models",
        "abstract": "  While large language models (LLMs) have demonstrated impressive performance\non a range of decision-making tasks, they rely on simple acting processes and\nfall short of broad deployment as autonomous agents. We introduce LATS\n(Language Agent Tree Search), a general framework that synergizes the\ncapabilities of LLMs in planning, acting, and reasoning. Drawing inspiration\nfrom Monte Carlo tree search in model-based reinforcement learning, LATS\nemploys LLMs as agents, value functions, and optimizers, repurposing their\nlatent strengths for enhanced decision-making. What is crucial in this method\nis the use of an environment for external feedback, which offers a more\ndeliberate and adaptive problem-solving mechanism that moves beyond the\nlimitations of existing techniques. Our experimental evaluation across diverse\ndomains, such as programming, HotPotQA, and WebShop, illustrates the\napplicability of LATS for both reasoning and acting. In particular, LATS\nachieves 94.4% for programming on HumanEval with GPT-4 and an average score of\n75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness\nand generality of our method.\n",
        "published": "2023",
        "authors": [
            "Andy Zhou",
            "Kai Yan",
            "Michal Shlapentokh-Rothman",
            "Haohan Wang",
            "Yu-Xiong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.05861v1",
        "title": "Rephrase, Augment, Reason: Visual Grounding of Questions for\n  Vision-Language Models",
        "abstract": "  An increasing number of vision-language tasks can be handled with little to\nno training, i.e., in a zero and few-shot manner, by marrying large language\nmodels (LLMs) to vision encoders, resulting in large vision-language models\n(LVLMs). While this has huge upsides, such as not requiring training data or\ncustom architectures, how an input is presented to a LVLM can have a major\nimpact on zero-shot model performance. In particular, inputs phrased in an\nunderspecified way can result in incorrect answers due to factors like missing\nvisual information, complex implicit reasoning, or linguistic ambiguity.\nTherefore, adding visually grounded information to the input as a preemptive\nclarification should improve model performance by reducing underspecification,\ne.g., by localizing objects and disambiguating references. Similarly, in the\nVQA setting, changing the way questions are framed can make them easier for\nmodels to answer. To this end, we present Rephrase, Augment and Reason\n(RepARe), a gradient-free framework that extracts salient details about the\nimage using the underlying LVLM as a captioner and reasoner, in order to\npropose modifications to the original question. We then use the LVLM's\nconfidence over a generated answer as an unsupervised scoring function to\nselect the rephrased question most likely to improve zero-shot performance.\nFocusing on two visual question answering tasks, we show that RepARe can result\nin a 3.85% (absolute) increase in zero-shot performance on VQAv2 and a 6.41%\npoint increase on A-OKVQA. Additionally, we find that using gold answers for\noracle question candidate selection achieves a substantial gain in VQA accuracy\nby up to 14.41%. Through extensive analysis, we demonstrate that outputs from\nRepARe increase syntactic complexity, and effectively utilize vision-language\ninteraction and the frozen language model in LVLMs.\n",
        "published": "2023",
        "authors": [
            "Archiki Prasad",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.12128v1",
        "title": "DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM\n  Planning",
        "abstract": "  Text-to-image (T2I) generation has seen significant growth over the past few\nyears. Despite this, there has been little work on generating diagrams with T2I\nmodels. A diagram is a symbolic/schematic representation that explains\ninformation using structurally rich and spatially complex visualizations (e.g.,\na dense combination of related objects, text labels, directional arrows,\nconnection lines, etc.). Existing state-of-the-art T2I models often fail at\ndiagram generation because they lack fine-grained object layout control when\nmany objects are densely connected via complex relations such as arrows/lines\nand also often fail to render comprehensible text labels. To address this gap,\nwe present DiagrammerGPT, a novel two-stage text-to-diagram generation\nframework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4)\nto generate more accurate open-domain, open-platform diagrams. In the first\nstage, we use LLMs to generate and iteratively refine 'diagram plans' (in a\nplanner-auditor feedback loop) which describe all the entities (objects and\ntext labels), their relationships (arrows or lines), and their bounding box\nlayouts. In the second stage, we use a diagram generator, DiagramGLIGEN, and a\ntext label rendering module to generate diagrams following the diagram plans.\nTo benchmark the text-to-diagram generation task, we introduce AI2D-Caption, a\ndensely annotated diagram dataset built on top of the AI2D dataset. We show\nquantitatively and qualitatively that our DiagrammerGPT framework produces more\naccurate diagrams, outperforming existing T2I models. We also provide\ncomprehensive analysis including open-domain diagram generation, vector graphic\ndiagram generation in different platforms, human-in-the-loop diagram plan\nediting, and multimodal planner/auditor LLMs (e.g., GPT-4Vision). We hope our\nwork can inspire further research on diagram generation via T2I models and\nLLMs.\n",
        "published": "2023",
        "authors": [
            "Abhay Zala",
            "Han Lin",
            "Jaemin Cho",
            "Mohit Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.12973v1",
        "title": "Frozen Transformers in Language Models Are Effective Visual Encoder\n  Layers",
        "abstract": "  This paper reveals that large language models (LLMs), despite being trained\nsolely on textual data, are surprisingly strong encoders for purely visual\ntasks in the absence of language. Even more intriguingly, this can be achieved\nby a simple yet previously overlooked strategy -- employing a frozen\ntransformer block from pre-trained LLMs as a constituent encoder layer to\ndirectly process visual tokens. Our work pushes the boundaries of leveraging\nLLMs for computer vision tasks, significantly departing from conventional\npractices that typically necessitate a multi-modal vision-language setup with\nassociated language prompts, inputs, or outputs. We demonstrate that our\napproach consistently enhances performance across a diverse range of tasks,\nencompassing pure 2D and 3D visual recognition tasks (e.g., image and point\ncloud classification), temporal modeling tasks (e.g., action recognition),\nnon-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g.,\n2D/3D visual question answering and image-text retrieval). Such improvements\nare a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and\nOPT) and different LLM transformer blocks. We additionally propose the\ninformation filtering hypothesis to explain the effectiveness of pre-trained\nLLMs in visual encoding -- the pre-trained LLM transformer blocks discern\ninformative visual tokens and further amplify their effect. This hypothesis is\nempirically supported by the observation that the feature activation, after\ntraining with LLM transformer blocks, exhibits a stronger focus on relevant\nregions. We hope that our work inspires new perspectives on utilizing LLMs and\ndeepening our understanding of their underlying mechanisms. Code is available\nat https://github.com/ziqipang/LM4VisualEncoding.\n",
        "published": "2023",
        "authors": [
            "Ziqi Pang",
            "Ziyang Xie",
            "Yunze Man",
            "Yu-Xiong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.17770v1",
        "title": "GROOViST: A Metric for Grounding Objects in Visual Storytelling",
        "abstract": "  A proper evaluation of stories generated for a sequence of images -- the task\ncommonly referred to as visual storytelling -- must consider multiple aspects,\nsuch as coherence, grammatical correctness, and visual grounding. In this work,\nwe focus on evaluating the degree of grounding, that is, the extent to which a\nstory is about the entities shown in the images. We analyze current metrics,\nboth designed for this purpose and for general vision-text alignment. Given\ntheir observed shortcomings, we propose a novel evaluation tool, GROOViST, that\naccounts for cross-modal dependencies, temporal misalignments (the fact that\nthe order in which entities appear in the story and the image sequence may not\nmatch), and human intuitions on visual grounding. An additional advantage of\nGROOViST is its modular design, where the contribution of each component can be\nassessed and interpreted individually.\n",
        "published": "2023",
        "authors": [
            "Aditya K Surikuchi",
            "Sandro Pezzelle",
            "Raquel Fern\u00e1ndez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.18804v1",
        "title": "Open Visual Knowledge Extraction via Relation-Oriented Multimodality\n  Model Prompting",
        "abstract": "  Images contain rich relational knowledge that can help machines understand\nthe world. Existing methods on visual knowledge extraction often rely on the\npre-defined format (e.g., sub-verb-obj tuples) or vocabulary (e.g., relation\ntypes), restricting the expressiveness of the extracted knowledge. In this\nwork, we take a first exploration to a new paradigm of open visual knowledge\nextraction. To achieve this, we present OpenVik which consists of an open\nrelational region detector to detect regions potentially containing relational\nknowledge and a visual knowledge generator that generates format-free knowledge\nby prompting the large multimodality model with the detected region of\ninterest. We also explore two data enhancement techniques for diversifying the\ngenerated format-free visual knowledge. Extensive knowledge quality evaluations\nhighlight the correctness and uniqueness of the extracted open visual knowledge\nby OpenVik. Moreover, integrating our extracted knowledge across various visual\nreasoning applications shows consistent improvements, indicating the real-world\napplicability of OpenVik.\n",
        "published": "2023",
        "authors": [
            "Hejie Cui",
            "Xinyu Fang",
            "Zihan Zhang",
            "Ran Xu",
            "Xuan Kan",
            "Xin Liu",
            "Yue Yu",
            "Manling Li",
            "Yangqiu Song",
            "Carl Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.20550v2",
        "title": "CapsFusion: Rethinking Image-Text Data at Scale",
        "abstract": "  Large multimodal models demonstrate remarkable generalist ability to perform\ndiverse multimodal tasks in a zero-shot manner. Large-scale web-based\nimage-text pairs contribute fundamentally to this success, but suffer from\nexcessive noise. Recent studies use alternative captions synthesized by\ncaptioning models and have achieved notable benchmark performance. However, our\nexperiments reveal significant Scalability Deficiency and World Knowledge Loss\nissues in models trained with synthetic captions, which have been largely\nobscured by their initial benchmark success. Upon closer examination, we\nidentify the root cause as the overly-simplified language structure and lack of\nknowledge details in existing synthetic captions. To provide higher-quality and\nmore scalable multimodal pretraining data, we propose CapsFusion, an advanced\nframework that leverages large language models to consolidate and refine\ninformation from both web-based image-text pairs and synthetic captions.\nExtensive experiments show that CapsFusion captions exhibit remarkable\nall-round superiority over existing captions in terms of model performance\n(e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample\nefficiency (requiring 11-16 times less computation than baselines), world\nknowledge depth, and scalability. These effectiveness, efficiency and\nscalability advantages position CapsFusion as a promising candidate for future\nscaling of LMM training.\n",
        "published": "2023",
        "authors": [
            "Qiying Yu",
            "Quan Sun",
            "Xiaosong Zhang",
            "Yufeng Cui",
            "Fan Zhang",
            "Yue Cao",
            "Xinlong Wang",
            "Jingjing Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.00047v1",
        "title": "Grounding Visual Illusions in Language: Do Vision-Language Models\n  Perceive Illusions Like Humans?",
        "abstract": "  Vision-Language Models (VLMs) are trained on vast amounts of data captured by\nhumans emulating our understanding of the world. However, known as visual\nillusions, human's perception of reality isn't always faithful to the physical\nworld. This raises a key question: do VLMs have the similar kind of illusions\nas humans do, or do they faithfully learn to represent reality? To investigate\nthis question, we build a dataset containing five types of visual illusions and\nformulate four tasks to examine visual illusions in state-of-the-art VLMs. Our\nfindings have shown that although the overall alignment is low, larger models\nare closer to human perception and more susceptible to visual illusions. Our\ndataset and initial findings will promote a better understanding of visual\nillusions in humans and machines and provide a stepping stone for future\ncomputational models that can better align humans and machines in perceiving\nand communicating about the shared visual world. The code and data are\navailable at https://github.com/vl-illusion/dataset.\n",
        "published": "2023",
        "authors": [
            "Yichi Zhang",
            "Jiayi Pan",
            "Yuchen Zhou",
            "Rui Pan",
            "Joyce Chai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.00059v1",
        "title": "The Generative AI Paradox: \"What It Can Create, It May Not Understand\"",
        "abstract": "  The recent wave of generative AI has sparked unprecedented global attention,\nwith both excitement and concern over potentially superhuman levels of\nartificial intelligence: models now take only seconds to produce outputs that\nwould challenge or exceed the capabilities even of expert humans. At the same\ntime, models still show basic errors in understanding that would not be\nexpected even in non-expert humans. This presents us with an apparent paradox:\nhow do we reconcile seemingly superhuman capabilities with the persistence of\nerrors that few humans would make? In this work, we posit that this tension\nreflects a divergence in the configuration of intelligence in today's\ngenerative models relative to intelligence in humans. Specifically, we propose\nand test the Generative AI Paradox hypothesis: generative models, having been\ntrained directly to reproduce expert-like outputs, acquire generative\ncapabilities that are not contingent upon -- and can therefore exceed -- their\nability to understand those same types of outputs. This contrasts with humans,\nfor whom basic understanding almost always precedes the ability to generate\nexpert-level outputs. We test this hypothesis through controlled experiments\nanalyzing generation vs. understanding in generative models, across both\nlanguage and image modalities. Our results show that although models can\noutperform humans in generation, they consistently fall short of human\ncapabilities in measures of understanding, as well as weaker correlation\nbetween generation and understanding performance, and more brittleness to\nadversarial inputs. Our findings support the hypothesis that models' generative\ncapability may not be contingent upon understanding capability, and call for\ncaution in interpreting artificial intelligence by analogy to human\nintelligence.\n",
        "published": "2023",
        "authors": [
            "Peter West",
            "Ximing Lu",
            "Nouha Dziri",
            "Faeze Brahman",
            "Linjie Li",
            "Jena D. Hwang",
            "Liwei Jiang",
            "Jillian Fisher",
            "Abhilasha Ravichander",
            "Khyathi Chandu",
            "Benjamin Newman",
            "Pang Wei Koh",
            "Allyson Ettinger",
            "Yejin Choi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.01623v1",
        "title": "VQPy: An Object-Oriented Approach to Modern Video Analytics",
        "abstract": "  Video analytics is widely used in contemporary systems and services. At the\nforefront of video analytics are video queries that users develop to find\nobjects of particular interest. Building upon the insight that video objects\n(e.g., human, animals, cars, etc.), the center of video analytics, are similar\nin spirit to objects modeled by traditional object-oriented languages, we\npropose to develop an object-oriented approach to video analytics. This\napproach, named VQPy, consists of a frontend$\\unicode{x2015}$a Python variant\nwith constructs that make it easy for users to express video objects and their\ninteractions$\\unicode{x2015}$as well as an extensible backend that can\nautomatically construct and optimize pipelines based on video objects. We have\nimplemented and open-sourced VQPy, which has been productized in Cisco as part\nof its DeepVision framework.\n",
        "published": "2023",
        "authors": [
            "Shan Yu",
            "Zhenting Zhu",
            "Yu Chen",
            "Hanchen Xu",
            "Pengzhan Zhao",
            "Yang Wang",
            "Arthi Padmanabhan",
            "Hugo Latapie",
            "Harry Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.02236v1",
        "title": "Robust Fine-Tuning of Vision-Language Models for Domain Generalization",
        "abstract": "  Transfer learning enables the sharing of common knowledge among models for a\nvariety of downstream tasks, but traditional methods suffer in limited training\ndata settings and produce narrow models incapable of effectively generalizing\nunder distribution shifts. Foundation models have recently demonstrated\nimpressive zero-shot inference capabilities and robustness under distribution\nshifts. However, zero-shot evaluation for these models has been predominantly\nconfined to benchmarks with simple distribution shifts, limiting our\nunderstanding of their effectiveness under the more realistic shifts found in\npractice. Moreover, common fine-tuning methods for these models have yet to be\nevaluated against vision models in few-shot scenarios where training data is\nlimited. To address these gaps, we present a new recipe for few-shot\nfine-tuning of the popular vision-language foundation model CLIP and evaluate\nits performance on challenging benchmark datasets with realistic distribution\nshifts from the WILDS collection. Our experimentation demonstrates that, while\nzero-shot CLIP fails to match performance of trained vision models on more\ncomplex benchmarks, few-shot CLIP fine-tuning outperforms its vision-only\ncounterparts in terms of in-distribution and out-of-distribution accuracy at\nall levels of training data availability. This provides a strong incentive for\nadoption of foundation models within few-shot learning applications operating\nwith real-world data. Code is available at\nhttps://github.com/mit-ll/robust-vision-language-finetuning\n",
        "published": "2023",
        "authors": [
            "Kevin Vogt-Lowell",
            "Noah Lee",
            "Theodoros Tsiligkaridis",
            "Marc Vaillant"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.06243v1",
        "title": "Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization",
        "abstract": "  Large foundation models are becoming ubiquitous, but training them from\nscratch is prohibitively expensive. Thus, efficiently adapting these powerful\nmodels to downstream tasks is increasingly important. In this paper, we study a\nprincipled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream\ntask adaptation. Despite demonstrating good generalizability, OFT still uses a\nfairly large number of trainable parameters due to the high dimensionality of\northogonal matrices. To address this, we start by examining OFT from an\ninformation transmission perspective, and then identify a few key desiderata\nthat enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast\nFourier transform algorithm enables efficient information transmission, we\npropose an efficient orthogonal parameterization using butterfly structures. We\napply this parameterization to OFT, creating a novel parameter-efficient\nfinetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a\nspecial case, BOFT introduces a generalized orthogonal finetuning framework.\nFinally, we conduct an extensive empirical study of adapting large vision\ntransformers, large language models, and text-to-image diffusion models to\nvarious downstream tasks in vision and language.\n",
        "published": "2023",
        "authors": [
            "Weiyang Liu",
            "Zeju Qiu",
            "Yao Feng",
            "Yuliang Xiu",
            "Yuxuan Xue",
            "Longhui Yu",
            "Haiwen Feng",
            "Zhen Liu",
            "Juyeon Heo",
            "Songyou Peng",
            "Yandong Wen",
            "Michael J. Black",
            "Adrian Weller",
            "Bernhard Sch\u00f6lkopf"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.07575v1",
        "title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for\n  Multi-modal Large Language Models",
        "abstract": "  We present SPHINX, a versatile multi-modal large language model (MLLM) with a\njoint mixing of model weights, tuning tasks, and visual embeddings. First, for\nstronger vision-language alignment, we unfreeze the large language model (LLM)\nduring pre-training, and introduce a weight mix strategy between LLMs trained\nby real-world and synthetic data. By directly integrating the weights from two\ndomains, the mixed LLM can efficiently incorporate diverse semantics with\nfavorable robustness. Then, to enable multi-purpose capabilities, we mix a\nvariety of tasks for joint visual instruction tuning, and design task-specific\ninstructions to avoid inter-task conflict. In addition to the basic visual\nquestion answering, we include more challenging tasks such as region-level\nunderstanding, caption grounding, document layout detection, and human pose\nestimation, contributing to mutual enhancement over different scenarios.\nAdditionally, we propose to extract comprehensive visual embeddings from\nvarious network architectures, pre-training paradigms, and information\ngranularity, providing language models with more robust image representations.\nBased on our proposed joint mixing, SPHINX exhibits superior multi-modal\nunderstanding capabilities on a wide range of applications. On top of this, we\nfurther propose an efficient strategy aiming to better capture fine-grained\nappearances of high-resolution images. With a mixing of different scales and\nhigh-resolution sub-images, SPHINX attains exceptional visual parsing and\nreasoning performance on existing evaluation benchmarks. We hope our work may\ncast a light on the exploration of joint mixing in future MLLM research. Code\nis released at https://github.com/Alpha-VLLM/LLaMA2-Accessory.\n",
        "published": "2023",
        "authors": [
            "Ziyi Lin",
            "Chris Liu",
            "Renrui Zhang",
            "Peng Gao",
            "Longtian Qiu",
            "Han Xiao",
            "Han Qiu",
            "Chen Lin",
            "Wenqi Shao",
            "Keqin Chen",
            "Jiaming Han",
            "Siyuan Huang",
            "Yichi Zhang",
            "Xuming He",
            "Hongsheng Li",
            "Yu Qiao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.07766v1",
        "title": "Vision-Language Integration in Multimodal Video Transformers (Partially)\n  Aligns with the Brain",
        "abstract": "  Integrating information from multiple modalities is arguably one of the\nessential prerequisites for grounding artificial intelligence systems with an\nunderstanding of the real world. Recent advances in video transformers that\njointly learn from vision, text, and sound over time have made some progress\ntoward this goal, but the degree to which these models integrate information\nfrom modalities still remains unclear. In this work, we present a promising\napproach for probing a pre-trained multimodal video transformer model by\nleveraging neuroscientific evidence of multimodal information processing in the\nbrain. Using brain recordings of participants watching a popular TV show, we\nanalyze the effects of multi-modal connections and interactions in a\npre-trained multi-modal video transformer on the alignment with uni- and\nmulti-modal brain regions. We find evidence that vision enhances masked\nprediction performance during language processing, providing support that\ncross-modal representations in models can benefit individual modalities.\nHowever, we don't find evidence of brain-relevant information captured by the\njoint multi-modal transformer representations beyond that captured by all of\nthe individual modalities. We finally show that the brain alignment of the\npre-trained joint representation can be improved by fine-tuning using a task\nthat requires vision-language inferences. Overall, our results paint an\noptimistic picture of the ability of multi-modal transformers to integrate\nvision and language in partially brain-relevant ways but also show that\nimproving the brain alignment of these models may require new approaches.\n",
        "published": "2023",
        "authors": [
            "Dota Tianai Dong",
            "Mariya Toneva"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.10111v1",
        "title": "VideoCon: Robust Video-Language Alignment via Contrast Captions",
        "abstract": "  Despite being (pre)trained on a massive amount of data, state-of-the-art\nvideo-language alignment models are not robust to semantically-plausible\ncontrastive changes in the video captions. Our work addresses this by\nidentifying a broad spectrum of contrast misalignments, such as replacing\nentities, actions, and flipping event order, which alignment models should be\nrobust against. To this end, we introduce the VideoCon, a video-language\nalignment dataset constructed by a large language model that generates\nplausible contrast video captions and explanations for differences between\noriginal and contrast video captions. Then, a generative video-language model\nis finetuned with VideoCon to assess video-language entailment and generate\nexplanations. Our VideoCon-based alignment model significantly outperforms\ncurrent models. It exhibits a 12-point increase in AUC for the video-language\nalignment task on human-generated contrast captions. Finally, our model sets\nnew state of the art zero-shot performance in temporally-extensive\nvideo-language tasks such as text-to-video retrieval (SSv2-Temporal) and video\nquestion answering (ATP-Hard). Moreover, our model shows superior performance\non novel videos and human-crafted captions and explanations. Our code and data\nare available at https://github.com/Hritikbansal/videocon.\n",
        "published": "2023",
        "authors": [
            "Hritik Bansal",
            "Yonatan Bitton",
            "Idan Szpektor",
            "Kai-Wei Chang",
            "Aditya Grover"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.11090v1",
        "title": "Beyond Images: An Integrative Multi-modal Approach to Chest X-Ray Report\n  Generation",
        "abstract": "  Image-to-text radiology report generation aims to automatically produce\nradiology reports that describe the findings in medical images. Most existing\nmethods focus solely on the image data, disregarding the other patient\ninformation accessible to radiologists. In this paper, we present a novel\nmulti-modal deep neural network framework for generating chest X-rays reports\nby integrating structured patient data, such as vital signs and symptoms,\nalongside unstructured clinical notes.We introduce a conditioned\ncross-multi-head attention module to fuse these heterogeneous data modalities,\nbridging the semantic gap between visual and textual data. Experiments\ndemonstrate substantial improvements from using additional modalities compared\nto relying on images alone. Notably, our model achieves the highest reported\nperformance on the ROUGE-L metric compared to relevant state-of-the-art models\nin the literature. Furthermore, we employed both human evaluation and clinical\nsemantic similarity measurement alongside word-overlap metrics to improve the\ndepth of quantitative analysis. A human evaluation, conducted by a\nboard-certified radiologist, confirms the model's accuracy in identifying\nhigh-level findings, however, it also highlights that more improvement is\nneeded to capture nuanced details and clinical context.\n",
        "published": "2023",
        "authors": [
            "Nurbanu Aksoy",
            "Serge Sharoff",
            "Selcuk Baser",
            "Nishant Ravikumar",
            "Alejandro F Frangi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.11097v1",
        "title": "Radiology Report Generation Using Transformers Conditioned with\n  Non-imaging Data",
        "abstract": "  Medical image interpretation is central to most clinical applications such as\ndisease diagnosis, treatment planning, and prognostication. In clinical\npractice, radiologists examine medical images and manually compile their\nfindings into reports, which can be a time-consuming process. Automated\napproaches to radiology report generation, therefore, can reduce radiologist\nworkload and improve efficiency in the clinical pathway. While recent\ndeep-learning approaches for automated report generation from medical images\nhave seen some success, most studies have relied on image-derived features\nalone, ignoring non-imaging patient data. Although a few studies have included\nthe word-level contexts along with the image, the use of patient demographics\nis still unexplored. This paper proposes a novel multi-modal transformer\nnetwork that integrates chest x-ray (CXR) images and associated patient\ndemographic information, to synthesise patient-specific radiology reports. The\nproposed network uses a convolutional neural network to extract visual features\nfrom CXRs and a transformer-based encoder-decoder network that combines the\nvisual features with semantic text embeddings of patient demographic\ninformation, to synthesise full-text radiology reports. Data from two public\ndatabases were used to train and evaluate the proposed approach. CXRs and\nreports were extracted from the MIMIC-CXR database and combined with\ncorresponding patients' data MIMIC-IV. Based on the evaluation metrics used\nincluding patient demographic information was found to improve the quality of\nreports generated using the proposed approach, relative to a baseline network\ntrained using CXRs alone. The proposed approach shows potential for enhancing\nradiology report generation by leveraging rich patient metadata and combining\nsemantic text embeddings derived thereof, with medical image-derived visual\nfeatures.\n",
        "published": "2023",
        "authors": [
            "Nurbanu Aksoy",
            "Nishant Ravikumar",
            "Alejandro F Frangi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.12866v1",
        "title": "Modular Blended Attention Network for Video Question Answering",
        "abstract": "  In multimodal machine learning tasks, it is due to the complexity of the\nassignments that the network structure, in most cases, is assembled in a\nsophisticated way. The holistic architecture can be separated into several\nlogical parts according to the respective ends that the modules are devised to\nachieve. As the number of modalities of information representation increases,\nconstructing ad hoc subnetworks for processing the data from divergent\nmodalities while mediating the fusion of different information types has become\na cumbersome and expensive problem. In this paper, we present an approach to\nfacilitate the question with a reusable and composable neural unit; by\nconnecting the units in series or parallel, the arduous network constructing of\nmultimodal machine learning tasks will be accomplished in a much\nstraightforward way. Additionally, through parameter sharing (weights\nreplication) among the units, the space complexity will be significantly\nreduced. We have conducted experiments on three commonly used datasets; our\nmethod achieves impressive performance compared to several video QA baselines.\n",
        "published": "2023",
        "authors": [
            "Mingjie Zhou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.16201v1",
        "title": "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image\n  Generation",
        "abstract": "  Recent advances in image tokenizers, such as VQ-VAE, have enabled\ntext-to-image generation using auto-regressive methods, similar to language\nmodeling. However, these methods have yet to leverage pre-trained language\nmodels, despite their adaptability to various downstream tasks. In this work,\nwe explore this gap by adapting a pre-trained language model for\nauto-regressive text-to-image generation, and find that pre-trained language\nmodels offer limited help. We provide a two-fold explanation by analyzing\ntokens from each modality. First, we demonstrate that image tokens possess\nsignificantly different semantics compared to text tokens, rendering\npre-trained language models no more effective in modeling them than randomly\ninitialized ones. Second, the text tokens in the image-text datasets are too\nsimple compared to normal language model pre-training data, which causes the\ncatastrophic degradation of language models' capability.\n",
        "published": "2023",
        "authors": [
            "Yuhui Zhang",
            "Brandon McKinzie",
            "Zhe Gan",
            "Vaishaal Shankar",
            "Alexander Toshev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.17076v1",
        "title": "Compositional Chain-of-Thought Prompting for Large Multimodal Models",
        "abstract": "  The combination of strong visual backbones and Large Language Model (LLM)\nreasoning has led to Large Multimodal Models (LMMs) becoming the current\nstandard for a wide range of vision and language (VL) tasks. However, recent\nresearch has shown that even the most advanced LMMs still struggle to capture\naspects of compositional visual reasoning, such as attributes and relationships\nbetween objects. One solution is to utilize scene graphs (SGs)--a formalization\nof objects and their relations and attributes that has been extensively used as\na bridge between the visual and textual domains. Yet, scene graph data requires\nscene graph annotations, which are expensive to collect and thus not easily\nscalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic\nforgetting of the pretraining objective. To overcome this, inspired by\nchain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a\nnovel zero-shot Chain-of-Thought prompting method that utilizes SG\nrepresentations in order to extract compositional knowledge from an LMM.\nSpecifically, we first generate an SG using the LMM, and then use that SG in\nthe prompt to produce a response. Through extensive experiments, we find that\nthe proposed CCoT approach not only improves LMM performance on several vision\nand language VL compositional benchmarks but also improves the performance of\nseveral popular LMMs on general multimodal benchmarks, without the need for\nfine-tuning or annotated ground-truth SGs.\n",
        "published": "2023",
        "authors": [
            "Chancharik Mitra",
            "Brandon Huang",
            "Trevor Darrell",
            "Roei Herzig"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.18765v2",
        "title": "MLLMs-Augmented Visual-Language Representation Learning",
        "abstract": "  Visual-language pre-training (VLP) has achieved remarkable success in\nmulti-modal tasks, largely attributed to the availability of large-scale\nimage-text datasets. In this work, we demonstrate that multi-modal large\nlanguage models (MLLMs) can enhance visual-language representation learning by\nimproving data quality. Our approach is simple, utilizing MLLMs to extend\nmultiple captions for each image. To prevent the bias introduced by MLLMs'\nhallucinations and intrinsic caption styles, we propose \"text shearing\" to\nmaintain the same length for extended captions as that of the original\ncaptions. In image-text retrieval, our method consistently obtains 5.6 ~ 35.0%\nand 16.8 ~ 46.1% improvement on R@1 under the fine-tuning and zero-shot\nsettings, respectively. Notably, we obtain zero-shot results that are\ncomparable to fine-tuning on target datasets, which encourages more exploration\nof the versatile use of MLLMs.\n",
        "published": "2023",
        "authors": [
            "Yanqing Liu",
            "Kai Wang",
            "Wenqi Shao",
            "Ping Luo",
            "Yu Qiao",
            "Mike Zheng Shou",
            "Kaipeng Zhang",
            "Yang You"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.00079v1",
        "title": "HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion\n  Models",
        "abstract": "  This paper explores advancements in high-fidelity personalized image\ngeneration through the utilization of pre-trained text-to-image diffusion\nmodels. While previous approaches have made significant strides in generating\nversatile scenes based on text descriptions and a few input images, challenges\npersist in maintaining the subject fidelity within the generated images. In\nthis work, we introduce an innovative algorithm named HiFi Tuner to enhance the\nappearance preservation of objects during personalized image generation. Our\nproposed method employs a parameter-efficient fine-tuning framework, comprising\na denoising process and a pivotal inversion process. Key enhancements include\nthe utilization of mask guidance, a novel parameter regularization technique,\nand the incorporation of step-wise subject representations to elevate the\nsample fidelity. Additionally, we propose a reference-guided generation\napproach that leverages the pivotal inversion of a reference image to mitigate\nunwanted subject variations and artifacts. We further extend our method to a\nnovel image editing task: substituting the subject in an image through textual\nmanipulations. Experimental evaluations conducted on the DreamBooth dataset\nusing the Stable Diffusion model showcase promising results. Fine-tuning solely\non textual embeddings improves CLIP-T score by 3.6 points and improves DINO\nscore by 9.6 points over Textual Inversion. When fine-tuning all parameters,\nHiFi Tuner improves CLIP-T score by 1.2 points and improves DINO score by 1.2\npoints over DreamBooth, establishing a new state of the art.\n",
        "published": "2023",
        "authors": [
            "Zhonghao Wang",
            "Wei Wei",
            "Yang Zhao",
            "Zhisheng Xiao",
            "Mark Hasegawa-Johnson",
            "Humphrey Shi",
            "Tingbo Hou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.00784v1",
        "title": "Making Large Multimodal Models Understand Arbitrary Visual Prompts",
        "abstract": "  While existing large vision-language multimodal models focus on whole image\nunderstanding, there is a prominent gap in achieving region-specific\ncomprehension. Current approaches that use textual coordinates or spatial\nencodings often fail to provide a user-friendly interface for visual prompting.\nTo address this challenge, we introduce a novel multimodal model capable of\ndecoding arbitrary visual prompts. This allows users to intuitively mark images\nand interact with the model using natural cues like a \"red bounding box\" or\n\"pointed arrow\". Our simple design directly overlays visual markers onto the\nRGB image, eliminating the need for complex region encodings, yet achieves\nstate-of-the-art performance on region-understanding tasks like Visual7W,\nPointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present\nViP-Bench, a comprehensive benchmark to assess the capability of models in\nunderstanding visual prompts across multiple dimensions, enabling future\nresearch in this domain. Code, data, and model are publicly available.\n",
        "published": "2023",
        "authors": [
            "Mu Cai",
            "Haotian Liu",
            "Siva Karthik Mustikovela",
            "Gregory P. Meyer",
            "Yuning Chai",
            "Dennis Park",
            "Yong Jae Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.01504v1",
        "title": "Effectively Fine-tune to Improve Large Multimodal Models for Radiology\n  Report Generation",
        "abstract": "  Writing radiology reports from medical images requires a high level of domain\nexpertise. It is time-consuming even for trained radiologists and can be\nerror-prone for inexperienced radiologists. It would be appealing to automate\nthis task by leveraging generative AI, which has shown drastic progress in\nvision and language understanding. In particular, Large Language Models (LLM)\nhave demonstrated impressive capabilities recently and continued to set new\nstate-of-the-art performance on almost all natural language tasks. While many\nhave proposed architectures to combine vision models with LLMs for multimodal\ntasks, few have explored practical fine-tuning strategies. In this work, we\nproposed a simple yet effective two-stage fine-tuning protocol to align visual\nfeatures to LLM's text embedding space as soft visual prompts. Our framework\nwith OpenLLaMA-7B achieved state-of-the-art level performance without\ndomain-specific pretraining. Moreover, we provide detailed analyses of soft\nvisual prompts and attention mechanisms, shedding light on future research\ndirections.\n",
        "published": "2023",
        "authors": [
            "Yuzhe Lu",
            "Sungmin Hong",
            "Yash Shah",
            "Panpan Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.01564v1",
        "title": "APoLLo: Unified Adapter and Prompt Learning for Vision Language Models",
        "abstract": "  The choice of input text prompt plays a critical role in the performance of\nVision-Language Pretrained (VLP) models such as CLIP. We present APoLLo, a\nunified multi-modal approach that combines Adapter and Prompt learning for\nVision-Language models. Our method is designed to substantially improve the\ngeneralization capabilities of VLP models when they are fine-tuned in a\nfew-shot setting. We introduce trainable cross-attention-based adapter layers\nin conjunction with vision and language encoders to strengthen the alignment\nbetween the two modalities. We enforce consistency between the respective\nencoder branches (receiving augmented inputs) to prevent overfitting in\ndownstream tasks. Our method is evaluated on three representative tasks:\ngeneralization to novel classes, cross-dataset evaluation, and unseen domain\nshifts. In practice, APoLLo achieves a relative gain up to 6.03% over MaPLe\n(SOTA) on novel classes for 10 diverse image recognition datasets.\n",
        "published": "2023",
        "authors": [
            "Sanjoy Chowdhury",
            "Sayan Nag",
            "Dinesh Manocha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.02310v1",
        "title": "VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding",
        "abstract": "  Recent advancements in language-model-based video understanding have been\nprogressing at a remarkable pace, spurred by the introduction of Large Language\nModels (LLMs). However, the focus of prior research has been predominantly on\ndevising a projection layer that maps video features to tokens, an approach\nthat is both rudimentary and inefficient. In our study, we introduce a\ncutting-edge framework, VaQuitA, designed to refine the synergy between video\nand textual information. At the data level, instead of sampling frames\nuniformly, we implement a sampling method guided by CLIP-score rankings, which\nenables a more aligned selection of frames with the given question. At the\nfeature level, we integrate a trainable Video Perceiver alongside a\nVisual-Query Transformer (abbreviated as VQ-Former), which bolsters the\ninterplay between the input question and the video features. We also discover\nthat incorporating a simple prompt, \"Please be critical\", into the LLM input\ncan substantially enhance its video comprehension capabilities. Our\nexperimental results indicate that VaQuitA consistently sets a new benchmark\nfor zero-shot video question-answering tasks and is adept at producing\nhigh-quality, multi-turn video dialogues with users.\n",
        "published": "2023",
        "authors": [
            "Yizhou Wang",
            "Ruiyi Zhang",
            "Haoliang Wang",
            "Uttaran Bhattacharya",
            "Yun Fu",
            "Gang Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.03818v2",
        "title": "Alpha-CLIP: A CLIP Model Focusing on Wherever You Want",
        "abstract": "  Contrastive Language-Image Pre-training (CLIP) plays an essential role in\nextracting valuable content information from images across diverse tasks. It\naligns textual and visual modalities to comprehend the entire image, including\nall the details, even those irrelevant to specific tasks. However, for a finer\nunderstanding and controlled editing of images, it becomes crucial to focus on\nspecific regions of interest, which can be indicated as points, masks, or boxes\nby humans or perception models. To fulfill the requirements, we introduce\nAlpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to\nsuggest attentive regions and fine-tuned with constructed millions of RGBA\nregion-text pairs. Alpha-CLIP not only preserves the visual recognition ability\nof CLIP but also enables precise control over the emphasis of image contents.\nIt demonstrates effectiveness in various tasks, including but not limited to\nopen-world recognition, multimodal large language models, and conditional 2D /\n3D generation. It has a strong potential to serve as a versatile tool for\nimage-related tasks.\n",
        "published": "2023",
        "authors": [
            "Zeyi Sun",
            "Ye Fang",
            "Tong Wu",
            "Pan Zhang",
            "Yuhang Zang",
            "Shu Kong",
            "Yuanjun Xiong",
            "Dahua Lin",
            "Jiaqi Wang"
        ]
    }
]