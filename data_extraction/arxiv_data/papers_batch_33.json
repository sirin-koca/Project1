[
    {
        "id": "http://arxiv.org/abs/2106.09749v1",
        "title": "Optimizing robotic swarm based construction tasks",
        "abstract": "  Social insects in nature such as ants, termites and bees construct their\ncolonies collaboratively in a very efficient process. In these swarms, each\ninsect contributes to the construction task individually showing redundant and\nparallel behavior of individual entities. But the robotics adaptations of these\nswarm's behaviors haven't yet made it to the real world at a large enough scale\nof commonly being used due to the limitations in the existing approaches to the\nswarm robotics construction. This paper presents an approach that combines the\nexisting swarm construction approaches which results in a swarm robotic system,\ncapable of constructing a given 2 dimensional shape in an optimized manner.\n",
        "published": "2021",
        "authors": [
            "Teshan Liyanage",
            "Subha Fernando"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.11365v1",
        "title": "Distributed Heuristic Multi-Agent Path Finding with Communication",
        "abstract": "  Multi-Agent Path Finding (MAPF) is essential to large-scale robotic systems.\nRecent methods have applied reinforcement learning (RL) to learn decentralized\npolices in partially observable environments. A fundamental challenge of\nobtaining collision-free policy is that agents need to learn cooperation to\nhandle congested situations. This paper combines communication with deep\nQ-learning to provide a novel learning based method for MAPF, where agents\nachieve cooperation via graph convolution. To guide RL algorithm on\nlong-horizon goal-oriented tasks, we embed the potential choices of shortest\npaths from single source as heuristic guidance instead of using a specific path\nas in most existing works. Our method treats each agent independently and\ntrains the model from a single agent's perspective. The final trained policy is\napplied to each agent for decentralized execution. The whole system is\ndistributed during training and is trained under a curriculum learning\nstrategy. Empirical evaluation in obstacle-rich environment indicates the high\nsuccess rate with low average step of our method.\n",
        "published": "2021",
        "authors": [
            "Ziyuan Ma",
            "Yudong Luo",
            "Hang Ma"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.11454v1",
        "title": "A Competitive Analysis of Online Multi-Agent Path Finding",
        "abstract": "  We study online Multi-Agent Path Finding (MAPF), where new agents are\nconstantly revealed over time and all agents must find collision-free paths to\ntheir given goal locations. We generalize existing complexity results of\n(offline) MAPF to online MAPF. We classify online MAPF algorithms into\ndifferent categories based on (1) controllability (the set of agents that they\ncan plan paths for at each time) and (2) rationality (the quality of paths they\nplan) and study the relationships between them. We perform a competitive\nanalysis for each category of online MAPF algorithms with respect to\ncommonly-used objective functions. We show that a naive algorithm that routes\nnewly-revealed agents one at a time in sequence achieves a competitive ratio\nthat is asymptotically bounded from both below and above by the number of\nagents with respect to flowtime and makespan. We then show a counter-intuitive\nresult that, if rerouting of previously-revealed agents is not allowed, any\nrational online MAPF algorithms, including ones that plan optimal paths for all\nnewly-revealed agents, have the same asymptotic competitive ratio as the naive\nalgorithm, even on 2D 4-neighbor grids. We also derive constant lower bounds on\nthe competitive ratio of any rational online MAPF algorithms that allow\nrerouting. The results thus provide theoretical insights into the effectiveness\nof using MAPF algorithms in an online setting for the first time.\n",
        "published": "2021",
        "authors": [
            "Hang Ma"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.00366v1",
        "title": "Agent-aware State Estimation in Autonomous Vehicles",
        "abstract": "  Autonomous systems often operate in environments where the behavior of\nmultiple agents is coordinated by a shared global state. Reliable estimation of\nthe global state is thus critical for successfully operating in a multi-agent\nsetting. We introduce agent-aware state estimation -- a framework for\ncalculating indirect estimations of state given observations of the behavior of\nother agents in the environment. We also introduce transition-independent\nagent-aware state estimation -- a tractable class of agent-aware state\nestimation -- and show that it allows the speed of inference to scale linearly\nwith the number of agents in the environment. As an example, we model traffic\nlight classification in instances of complete loss of direct observation. By\ntaking into account observations of vehicular behavior from multiple directions\nof traffic, our approach exhibits accuracy higher than that of existing traffic\nlight-only HMM methods on a real-world autonomous vehicle data set under a\nvariety of simulated occlusion scenarios.\n",
        "published": "2021",
        "authors": [
            "Shane Parr",
            "Ishan Khatri",
            "Justin Svegliato",
            "Shlomo Zilberstein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.03383v2",
        "title": "Artificial Intelligence-Driven Customized Manufacturing Factory: Key\n  Technologies, Applications, and Challenges",
        "abstract": "  The traditional production paradigm of large batch production does not offer\nflexibility towards satisfying the requirements of individual customers. A new\ngeneration of smart factories is expected to support new multi-variety and\nsmall-batch customized production modes. For that, Artificial Intelligence (AI)\nis enabling higher value-added manufacturing by accelerating the integration of\nmanufacturing and information communication technologies, including computing,\ncommunication, and control. The characteristics of a customized smart factory\nare to include self-perception, operations optimization, dynamic\nreconfiguration, and intelligent decision-making. The AI technologies will\nallow manufacturing systems to perceive the environment, adapt to external\nneeds, and extract the processed knowledge, including business models, such as\nintelligent production, networked collaboration, and extended service models.\n  This paper focuses on the implementation of AI in customized manufacturing\n(CM). The architecture of an AI-driven customized smart factory is presented.\nDetails of intelligent manufacturing devices, intelligent information\ninteraction, and the construction of a flexible manufacturing line are\nshowcased. The state-of-the-art AI technologies of potential use in CM, i.e.,\nmachine learning, multi-agent systems, Internet of Things, big data, and\ncloud-edge computing are surveyed. The AI-enabled technologies in a customized\nsmart factory are validated with a case study of customized packaging. The\nexperimental results have demonstrated that the AI-assisted CM offers the\npossibility of higher production flexibility and efficiency. Challenges and\nsolutions related to AI in CM are also discussed.\n",
        "published": "2021",
        "authors": [
            "Jiafu Wan",
            "Xiaomin Li",
            "Hong-Ning Dai",
            "Andrew Kusiak",
            "Miguel Mart\u00ednez-Garc\u00eda",
            "Di Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.05145v1",
        "title": "Prioritized SIPP for Multi-Agent Path Finding With Kinematic Constraints",
        "abstract": "  Multi-Agent Path Finding (MAPF) is a long-standing problem in Robotics and\nArtificial Intelligence in which one needs to find a set of collision-free\npaths for a group of mobile agents (robots) operating in the shared workspace.\nDue to its importance, the problem is well-studied and multiple optimal and\napproximate algorithms are known. However, many of them abstract away from the\nkinematic constraints and assume that the agents can accelerate/decelerate\ninstantaneously. This complicates the application of the algorithms on the real\nrobots. In this paper, we present a method that mitigates this issue to a\ncertain extent. The suggested solver is essentially, a prioritized planner\nbased on the well-known Safe Interval Path Planning (SIPP) algorithm. Within\nSIPP we explicitly reason about the speed and the acceleration thus the\nconstructed plans directly take kinematic constraints of agents into account.\nWe suggest a range of heuristic functions for that setting and conduct a\nthorough empirical evaluation of the suggested algorithm.\n",
        "published": "2021",
        "authors": [
            "Zain Alabedeen Ali",
            "Konstantin Yakovlev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.11885v1",
        "title": "Human operator cognitive availability aware Mixed-Initiative control",
        "abstract": "  This paper presents a Cognitive Availability Aware Mixed-Initiative\nController for remotely operated mobile robots. The controller enables dynamic\nswitching between different levels of autonomy (LOA), initiated by either the\nAI or the human operator. The controller leverages a state-of-the-art computer\nvision method and an off-the-shelf web camera to infer the cognitive\navailability of the operator and inform the AI-initiated LOA switching. This\nconstitutes a qualitative advancement over previous Mixed-Initiative (MI)\ncontrollers. The controller is evaluated in a disaster response experiment, in\nwhich human operators have to conduct an exploration task with a remote robot.\nMI systems are shown to effectively assist the operators, as demonstrated by\nquantitative and qualitative results in performance and workload. Additionally,\nsome insights into the experimental difficulties of evaluating complex MI\ncontrollers are presented.\n",
        "published": "2021",
        "authors": [
            "Giannis Petousakis",
            "Manolis Chiou",
            "Grigoris Nikolaou",
            "Rustam Stolkin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.12934v1",
        "title": "Distributed Swarm Collision Avoidance Based on Angular Calculations",
        "abstract": "  Collision avoidance is one of the most important topics in the robotics\nfield. The goal is to move the robots from initial locations to target\nlocations such that they follow shortest non-colliding paths in the shortest\ntime and with the least amount of energy. In this paper, a distributed and\nreal-time algorithm for dense and complex 2D and 3D environments is proposed.\nThis algorithm uses angular calculations to select the optimal direction for\nthe movement of each robot and it has been shown that these separate\ncalculations lead to a form of cooperative behavior among agents. We evaluated\nthe proposed approach on various simulation and experimental scenarios and\ncompared the results with FMP and ORCA, two important algorithms in this field.\nThe results show that the proposed approach is at least 25% faster than ORCA\nand at least 7% faster than FMP and also more reliable than both methods. The\nproposed method is shown to enable fully autonomous navigation of a swarm of\ncrazyflies.\n",
        "published": "2021",
        "authors": [
            "SeyedZahir Qazavi",
            "Samaneh Hosseini Semnani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.04205v2",
        "title": "DAN: Decentralized Attention-based Neural Network for the MinMax\n  Multiple Traveling Salesman Problem",
        "abstract": "  The multiple traveling salesman problem (mTSP) is a well-known NP-hard\nproblem with numerous real-world applications. In particular, this work\naddresses MinMax mTSP, where the objective is to minimize the max tour length\namong all agents. Many robotic deployments require recomputing potentially\nlarge mTSP instances frequently, making the natural trade-off between computing\ntime and solution quality of great importance. However, exact and heuristic\nalgorithms become inefficient as the number of cities increases, due to their\ncomputational complexity. Encouraged by the recent developments in deep\nreinforcement learning (dRL), this work approaches the mTSP as a cooperative\ntask and introduces DAN, a decentralized attention-based neural method that\naims at tackling this key trade-off. In DAN, agents learn fully decentralized\npolicies to collaboratively construct a tour, by predicting each other's future\ndecisions. Our model relies on the Transformer architecture and is trained\nusing multi-agent RL with parameter sharing, providing natural scalability to\nthe numbers of agents and cities. Our experimental results on small- to\nlarge-scale mTSP instances ($50$ to $1000$ cities and $5$ to $20$ agents) show\nthat DAN is able to match or outperform state-of-the-art solvers while keeping\nplanning times low. In particular, given the same computation time budget, DAN\noutperforms all conventional and dRL-based baselines on larger-scale instances\n(more than 100 cities, more than 5 agents), and exhibits enhanced agent\ncollaboration. A video explaining our approach and presenting our results is\navailable at \\url{https://youtu.be/xi3cLsDsLvs}.\n",
        "published": "2021",
        "authors": [
            "Yuhong Cao",
            "Zhanhong Sun",
            "Guillaume Sartoretti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.05413v2",
        "title": "Learning Selective Communication for Multi-Agent Path Finding",
        "abstract": "  Learning communication via deep reinforcement learning (RL) or imitation\nlearning (IL) has recently been shown to be an effective way to solve\nMulti-Agent Path Finding (MAPF). However, existing communication based MAPF\nsolvers focus on broadcast communication, where an agent broadcasts its message\nto all other or predefined agents. It is not only impractical but also leads to\nredundant information that could even impair the multi-agent cooperation. A\nsuccinct communication scheme should learn which information is relevant and\ninfluential to each agent's decision making process. To address this problem,\nwe consider a request-reply scenario and propose Decision Causal Communication\n(DCC), a simple yet efficient model to enable agents to select neighbors to\nconduct communication during both training and execution. Specifically, a\nneighbor is determined as relevant and influential only when the presence of\nthis neighbor causes the decision adjustment on the central agent. This\njudgment is learned only based on agent's local observation and thus suitable\nfor decentralized execution to handle large scale problems. Empirical\nevaluation in obstacle-rich environment indicates the high success rate with\nlow communication overhead of our method.\n",
        "published": "2021",
        "authors": [
            "Ziyuan Ma",
            "Yudong Luo",
            "Jia Pan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.08299v1",
        "title": "Flexible and Explainable Solutions for Multi-Agent Path Finding Problems",
        "abstract": "  The multi-agent path finding (MAPF) problem is a combinatorial search problem\nthat aims at finding paths for multiple agents (e.g., robots) in an environment\n(e.g., an autonomous warehouse) such that no two agents collide with each\nother, and subject to some constraints on the lengths of paths. The real-world\napplications of MAPF require flexibility (e.g., solving variations of MAPF) as\nwell as explainability. In this study, both of these challenges are addressed\nand some flexible and explainable solutions for MAPF and its variants are\nintroduced.\n",
        "published": "2021",
        "authors": [
            "Aysu Bogatarkan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.09807v1",
        "title": "I Know You Can't See Me: Dynamic Occlusion-Aware Safety Validation of\n  Strategic Planners for Autonomous Vehicles Using Hypergames",
        "abstract": "  A particular challenge for both autonomous and human driving is dealing with\nrisk associated with dynamic occlusion, i.e., occlusion caused by other\nvehicles in traffic. Based on the theory of hypergames, we develop a novel\nmulti-agent dynamic occlusion risk (DOR) measure for assessing situational risk\nin dynamic occlusion scenarios. Furthermore, we present a white-box,\nscenario-based, accelerated safety validation framework for assessing safety of\nstrategic planners in AV. Based on evaluation over a large naturalistic\ndatabase, our proposed validation method achieves a 4000% speedup compared to\ndirect validation on naturalistic data, a more diverse coverage, and ability to\ngeneralize beyond the dataset and generate commonly observed dynamic occlusion\ncrashes in traffic in an automated manner.\n",
        "published": "2021",
        "authors": [
            "Maximilian Kahn",
            "Atrisha Sarkar",
            "Krzysztof Czarnecki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.09861v2",
        "title": "Generalized dynamic cognitive hierarchy models for strategic driving\n  behavior",
        "abstract": "  While there has been an increasing focus on the use of game theoretic models\nfor autonomous driving, empirical evidence shows that there are still open\nquestions around dealing with the challenges of common knowledge assumptions as\nwell as modeling bounded rationality. To address some of these practical\nchallenges, we develop a framework of generalized dynamic cognitive hierarchy\nfor both modelling naturalistic human driving behavior as well as behavior\nplanning for autonomous vehicles (AV). This framework is built upon a rich\nmodel of level-0 behavior through the use of automata strategies, an\ninterpretable notion of bounded rationality through safety and maneuver\nsatisficing, and a robust response for planning. Based on evaluation on two\nlarge naturalistic datasets as well as simulation of critical traffic\nscenarios, we show that i) automata strategies are well suited for level-0\nbehavior in a dynamic level-k framework, and ii) the proposed robust response\nto a heterogeneous population of strategic and non-strategic reasoners can be\nan effective approach for game theoretic planning in AV.\n",
        "published": "2021",
        "authors": [
            "Atrisha Sarkar",
            "Kate Larson",
            "Krzysztof Czarnecki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.08802v2",
        "title": "Coordinated Multi-Agent Pathfinding for Drones and Trucks over Road\n  Networks",
        "abstract": "  We address the problem of routing a team of drones and trucks over\nlarge-scale urban road networks. To conserve their limited flight energy,\ndrones can use trucks as temporary modes of transit en route to their own\ndestinations. Such coordination can yield significant savings in total vehicle\ndistance traveled, i.e., truck travel distance and drone flight distance,\ncompared to operating drones and trucks independently. But it comes at the\npotentially prohibitive computational cost of deciding which trucks and drones\nshould coordinate and when and where it is most beneficial to do so. We tackle\nthis fundamental trade-off by decoupling our overall intractable problem into\ntractable sub-problems that we solve stage-wise. The first stage solves only\nfor trucks, by computing paths that make them more likely to be useful transit\noptions for drones. The second stage solves only for drones, by routing them\nover a composite of the road network and the transit network defined by truck\npaths from the first stage. We design a comprehensive algorithmic framework\nthat frames each stage as a multi-agent path-finding problem and implement two\ndistinct methods for solving them. We evaluate our approach on extensive\nsimulations with up to $100$ agents on the real-world Manhattan road network\ncontaining nearly $4500$ vertices and $10000$ edges. Our framework saves on\nmore than $50\\%$ of vehicle distance traveled compared to independently solving\nfor trucks and drones, and computes solutions for all settings within $5$\nminutes on commodity hardware.\n",
        "published": "2021",
        "authors": [
            "Shushman Choudhury",
            "Kiril Solovey",
            "Mykel Kochenderfer",
            "Marco Pavone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.07441v2",
        "title": "A distributed, plug-n-play algorithm for multi-robot applications with a\n  priori non-computable objective functions",
        "abstract": "  This paper presents a distributed algorithm applicable to a wide range of\npractical multi-robot applications. In such multi-robot applications, the\nuser-defined objectives of the mission can be cast as a general optimization\nproblem, without explicit guidelines of the subtasks per different robot. Owing\nto the unknown environment, unknown robot dynamics, sensor nonlinearities,\netc., the analytic form of the optimization cost function is not available a\npriori. Therefore, standard gradient-descent-like algorithms are not applicable\nto these problems. To tackle this, we introduce a new algorithm that carefully\ndesigns each robot's subcost function, the optimization of which can accomplish\nthe overall team objective. Upon this transformation, we propose a distributed\nmethodology based on the cognitive-based adaptive optimization (CAO) algorithm,\nthat is able to approximate the evolution of each robot's cost function and to\nadequately optimize its decision variables (robot actions). The latter can be\nachieved by online learning only the problem-specific characteristics that\naffect the accomplishment of mission objectives. The overall, low-complexity\nalgorithm can straightforwardly incorporate any kind of operational constraint,\nis fault-tolerant, and can appropriately tackle time-varying cost functions. A\ncornerstone of this approach is that it shares the same convergence\ncharacteristics as those of block coordinate descent algorithms. The proposed\nalgorithm is evaluated in three heterogeneous simulation set-ups under multiple\nscenarios, against both general-purpose and problem-specific algorithms. Source\ncode is available at\nhttps://github.com/athakapo/A-distributed-plug-n-play-algorithm-for-multi-robot-applications.\n",
        "published": "2021",
        "authors": [
            "Athanasios Ch. Kapoutsis",
            "Savvas A. Chatzichristofis",
            "Elias B. Kosmatopoulos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.14706v2",
        "title": "Intersection focused Situation Coverage-based Verification and\n  Validation Framework for Autonomous Vehicles Implemented in CARLA",
        "abstract": "  Autonomous Vehicles (AVs) i.e., self-driving cars, operate in a safety\ncritical domain, since errors in the autonomous driving software can lead to\nhuge losses. Statistically, road intersections which are a part of the AVs\noperational design domain (ODD), have some of the highest accident rates.\nHence, testing AVs to the limits on road intersections and assuring their\nsafety on road intersections is pertinent, and thus the focus of this paper. We\npresent a situation coverage-based (SitCov) AV-testing framework for the\nverification and validation (V&V) and safety assurance of AVs, developed in an\nopen-source AV simulator named CARLA. The SitCov AV-testing framework focuses\non vehicle-to-vehicle interaction on a road intersection under different\nenvironmental and intersection configuration situations, using situation\ncoverage criteria for automatic test suite generation for safety assurance of\nAVs. We have developed an ontology for intersection situations, and used it to\ngenerate a situation hyperspace i.e., the space of all possible situations\narising from that ontology. For the evaluation of our SitCov AV-testing\nframework, we have seeded multiple faults in our ego AV, and compared situation\ncoverage based and random situation generation. We have found that both\ngeneration methodologies trigger around the same number of seeded faults, but\nthe situation coverage-based generation tells us a lot more about the\nweaknesses of the autonomous driving algorithm of our ego AV, especially in\nedge-cases. Our code is publicly available online, anyone can use our SitCov\nAV-testing framework and use it or build further on top of it. This paper aims\nto contribute to the domain of V&V and development of AVs, not only from a\ntheoretical point of view, but also from the viewpoint of an open-source\nsoftware contribution and releasing a flexible/effective tool for V&V and\ndevelopment of AVs.\n",
        "published": "2021",
        "authors": [
            "Zaid Tahir",
            "Rob Alexander"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.06014v2",
        "title": "Standby-Based Deadlock Avoidance Method for Multi-Agent Pickup and\n  Delivery Tasks",
        "abstract": "  The multi-agent pickup and delivery (MAPD) problem, in which multiple agents\niteratively carry materials without collisions, has received significant\nattention. However, many conventional MAPD algorithms assume a specifically\ndesigned grid-like environment, such as an automated warehouse. Therefore, they\nhave many pickup and delivery locations where agents can stay for a lengthy\nperiod, as well as plentiful detours to avoid collisions owing to the freedom\nof movement in a grid. By contrast, because a maze-like environment such as a\nsearch-and-rescue or construction site has fewer pickup/delivery locations and\ntheir numbers may be unbalanced, many agents concentrate on such locations\nresulting in inefficient operations, often becoming stuck or deadlocked. Thus,\nto improve the transportation efficiency even in a maze-like restricted\nenvironment, we propose a deadlock avoidance method, called standby-based\ndeadlock avoidance (SBDA). SBDA uses standby nodes determined in real-time\nusing the articulation-point-finding algorithm, and the agent is guaranteed to\nstay there for a finite amount of time. We demonstrated that our proposed\nmethod outperforms a conventional approach. We also analyzed how the parameters\nused for selecting standby nodes affect the performance.\n",
        "published": "2022",
        "authors": [
            "Tomoki Yamauchi",
            "Yuki Miyashita",
            "Toshiharu Sugawara"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.10918v2",
        "title": "Behavior Tree-Based Task Planning for Multiple Mobile Robots using a\n  Data Distribution Service",
        "abstract": "  In this study, we propose task planning framework for multiple robots that\nbuilds on a behavior tree (BT). BTs communicate with a data distribution\nservice (DDS) to send and receive data. Since the standard BT derived from one\nroot node with a single tick is unsuitable for multiple robots, a novel type of\nBT action and improved nodes are proposed to control multiple robots through a\nDDS asynchronously. To plan tasks for robots efficiently, a single task\nplanning unit is implemented with the proposed task types. The task planning\nunit assigns tasks to each robot simultaneously through a single coalesced BT.\nIf any robot falls into a fault while performing its assigned task, another BT\nembedded in the robot is executed; the robot enters the recovery mode in order\nto overcome the fault. To perform this function, the action in the BT\ncorresponding to the task is defined as a variable, which is shared with the\nDDS so that any action can be exchanged between the task planning unit and\nrobots. To show the feasibility of our framework in a real-world application,\nthree mobile robots were experimentally coordinated for them to travel\nalternately to four goal positions by the proposed single task planning unit\nvia a DDS.\n",
        "published": "2022",
        "authors": [
            "Seungwoo Jeong",
            "Taekwon Ga",
            "Inhwan Jeong",
            "Jongeun Choi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.04452v3",
        "title": "Cooperative Trajectory Planning in Uncertain Environments with Monte\n  Carlo Tree Search and Risk Metrics",
        "abstract": "  Automated vehicles require the ability to cooperate with humans for smooth\nintegration into today's traffic. While the concept of cooperation is well\nknown, developing a robust and efficient cooperative trajectory planning method\nis still a challenge. One aspect of this challenge is the uncertainty\nsurrounding the state of the environment due to limited sensor accuracy. This\nuncertainty can be represented by a Partially Observable Markov Decision\nProcess. Our work addresses this problem by extending an existing cooperative\ntrajectory planning approach based on Monte Carlo Tree Search for continuous\naction spaces. It does so by explicitly modeling uncertainties in the form of a\nroot belief state, from which start states for trees are sampled. After the\ntrees have been constructed with Monte Carlo Tree Search, their results are\naggregated into return distributions using kernel regression. We apply two risk\nmetrics for the final selection, namely a Lower Confidence Bound and a\nConditional Value at Risk. It can be demonstrated that the integration of risk\nmetrics in the final selection policy consistently outperforms a baseline in\nuncertain environments, generating considerably safer trajectories.\n",
        "published": "2022",
        "authors": [
            "Philipp Stegmaier",
            "Karl Kurzer",
            "J. Marius Z\u00f6llner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.06631v2",
        "title": "A ROS Architecture for Personalised HRI with a Bartender Social Robot",
        "abstract": "  BRILLO (Bartending Robot for Interactive Long-Lasting Operations) project has\nthe overall goal of creating an autonomous robotic bartender that can interact\nwith customers while accomplishing its bartending tasks. In such a scenario,\npeople's novelty effect connected to the use of an attractive technology is\ndestined to wear off and, consequently, it negatively affects the success of\nthe service robotics application. For this reason, providing personalised\nnatural interaction while accessing its services is of paramount importance for\nincreasing users' engagement and, consequently, their loyalty. In this paper,\nwe present the developed three-layers ROS architecture integrating a perception\nlayer managing the processing of different social signals, a decision-making\nlayer for handling multi-party interactions, and an execution layer controlling\nthe behaviour of a complex robot composed of arms and a face. Finally, user\nmodelling through a beliefs layer allows for personalised interaction.\n",
        "published": "2022",
        "authors": [
            "Alessandra Rossi",
            "Maria Di Maro",
            "Antonio Origlia",
            "Agostino Palmiero",
            "Silvia Rossi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.11618v3",
        "title": "Distributing Collaborative Multi-Robot Planning with Gaussian Belief\n  Propagation",
        "abstract": "  Precise coordinated planning over a forward time window enables safe and\nhighly efficient motion when many robots must work together in tight spaces,\nbut this would normally require centralised control of all devices which is\ndifficult to scale. We demonstrate GBP Planning, a new purely distributed\ntechnique based on Gaussian Belief Propagation for multi-robot planning\nproblems, formulated by a generic factor graph defining dynamics and collision\nconstraints over a forward time window. In simulations, we show that our method\nallows high performance collaborative planning where robots are able to cross\neach other in busy, intricate scenarios. They maintain shorter, quicker and\nsmoother trajectories than alternative distributed planning techniques even in\ncases of communication failure. We encourage the reader to view the\naccompanying video demonstration at https://youtu.be/8VSrEUjH610.\n",
        "published": "2022",
        "authors": [
            "Aalok Patwardhan",
            "Riku Murai",
            "Andrew J. Davison"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.02392v1",
        "title": "Deep Interactive Motion Prediction and Planning: Playing Games with\n  Motion Prediction Models",
        "abstract": "  In most classical Autonomous Vehicle (AV) stacks, the prediction and planning\nlayers are separated, limiting the planner to react to predictions that are not\ninformed by the planned trajectory of the AV. This work presents a module that\ntightly couples these layers via a game-theoretic Model Predictive Controller\n(MPC) that uses a novel interactive multi-agent neural network policy as part\nof its predictive model. In our setting, the MPC planner considers all the\nsurrounding agents by informing the multi-agent policy with the planned state\nsequence. Fundamental to the success of our method is the design of a novel\nmulti-agent policy network that can steer a vehicle given the state of the\nsurrounding agents and the map information. The policy network is trained\nimplicitly with ground-truth observation data using backpropagation through\ntime and a differentiable dynamics model to roll out the trajectory forward in\ntime. Finally, we show that our multi-agent policy network learns to drive\nwhile interacting with the environment, and, when combined with the\ngame-theoretic MPC planner, can successfully generate interactive behaviors.\n",
        "published": "2022",
        "authors": [
            "Jose L. Vazquez",
            "Alexander Liniger",
            "Wilko Schwarting",
            "Daniela Rus",
            "Luc Van Gool"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.00432v1",
        "title": "Drone Flocking Optimization using NSGA-II and Principal Component\n  Analysis",
        "abstract": "  Individual agents in natural systems like flocks of birds or schools of fish\ndisplay a remarkable ability to coordinate and communicate in local groups and\nexecute a variety of tasks efficiently. Emulating such natural systems into\ndrone swarms to solve problems in defence, agriculture, industry automation and\nhumanitarian relief is an emerging technology. However, flocking of aerial\nrobots while maintaining multiple objectives, like collision avoidance, high\nspeed etc. is still a challenge. In this paper, optimized flocking of drones in\na confined environment with multiple conflicting objectives is proposed. The\nconsidered objectives are collision avoidance (with each other and the wall),\nspeed, correlation, and communication (connected and disconnected agents).\nPrincipal Component Analysis (PCA) is applied for dimensionality reduction, and\nunderstanding the collective dynamics of the swarm. The control model is\ncharacterised by 12 parameters which are then optimized using a multi-objective\nsolver (NSGA-II). The obtained results are reported and compared with that of\nthe CMA-ES algorithm. The study is particularly useful as the proposed\noptimizer outputs a Pareto Front representing different types of swarms which\ncan applied to different scenarios in the real world.\n",
        "published": "2022",
        "authors": [
            "Jagdish Chand Bansal",
            "Nikhil Sethi",
            "Ogbonnaya Anicho",
            "Atulya Nagar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.04548v1",
        "title": "Informed Steiner Trees: Sampling and Pruning for Multi-Goal Path Finding\n  in High Dimensions",
        "abstract": "  We interleave sampling based motion planning methods with pruning ideas from\nminimum spanning tree algorithms to develop a new approach for solving a\nMulti-Goal Path Finding (MGPF) problem in high dimensional spaces. The approach\nalternates between sampling points from selected regions in the search space\nand de-emphasizing regions that may not lead to good solutions for MGPF. Our\napproach provides an asymptotic, 2-approximation guarantee for MGPF. We also\npresent extensive numerical results to illustrate the advantages of our\nproposed approach over uniform sampling in terms of the quality of the\nsolutions found and computation speed.\n",
        "published": "2022",
        "authors": [
            "Nikhil Chandak",
            "Kenny Chour",
            "Sivakumar Rathinam",
            "R. Ravi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.11624v4",
        "title": "Effective Integration of Weighted Cost-to-go and Conflict Heuristic\n  within Suboptimal CBS",
        "abstract": "  Conflict-Based Search (CBS) is a popular multi-agent path finding (MAPF)\nsolver that employs a low-level single agent planner and a high-level\nconstraint tree to resolve conflicts. The vast majority of modern MAPF solvers\nfocus on improving CBS by reducing the size of this tree through various\nstrategies with few methods modifying the low level planner. Typically low\nlevel planners in existing CBS methods use an unweighted cost-to-go heuristic,\nwith suboptimal CBS methods also using a conflict heuristic to help the high\nlevel search. In this paper, we show that, contrary to prevailing CBS beliefs,\na weighted cost-to-go heuristic can be used effectively alongside the conflict\nheuristic in two possible variants. In particular, one of these variants can\nobtain large speedups, 2-100x, across several scenarios and suboptimal CBS\nmethods. Importantly, we discover that performance is related not to the\nweighted cost-to-go heuristic but rather to the relative conflict heuristic\nweight's ability to effectively balance low-level and high-level work, implying\nthat existing suboptimal CBS work misses this subtlety. Additionally, to the\nbest of our knowledge, we show the first theoretical relation of prioritized\nplanning and bounded suboptimal CBS and demonstrate that our methods are their\nnatural generalization.\n",
        "published": "2022",
        "authors": [
            "Rishi Veerapaneni",
            "Tushar Kusnur",
            "Maxim Likhachev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.09590v1",
        "title": "From Multi-agent to Multi-robot: A Scalable Training and Evaluation\n  Platform for Multi-robot Reinforcement Learning",
        "abstract": "  Multi-agent reinforcement learning (MARL) has been gaining extensive\nattention from academia and industries in the past few decades. One of the\nfundamental problems in MARL is how to evaluate different approaches\ncomprehensively. Most existing MARL methods are evaluated in either video games\nor simplistic simulated scenarios. It remains unknown how these methods perform\nin real-world scenarios, especially multi-robot systems. This paper introduces\na scalable emulation platform for multi-robot reinforcement learning (MRRL)\ncalled SMART to meet this need. Precisely, SMART consists of two components: 1)\na simulation environment that provides a variety of complex interaction\nscenarios for training and 2) a real-world multi-robot system for realistic\nperformance evaluation. Besides, SMART offers agent-environment APIs that are\nplug-and-play for algorithm implementation. To illustrate the practicality of\nour platform, we conduct a case study on the cooperative driving lane change\nscenario. Building off the case study, we summarize several unique challenges\nof MRRL, which are rarely considered previously. Finally, we open-source the\nsimulation environments, associated benchmark tasks, and state-of-the-art\nbaselines to encourage and empower MRRL research.\n",
        "published": "2022",
        "authors": [
            "Zhiuxan Liang",
            "Jiannong Cao",
            "Shan Jiang",
            "Divya Saxena",
            "Jinlin Chen",
            "Huafeng Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.11319v1",
        "title": "Graph-Based Multi-Robot Path Finding and Planning",
        "abstract": "  Purpose of Review\n  Planning collision-free paths for multiple robots is important for real-world\nmulti-robot systems and has been studied as an optimization problem on graphs,\ncalled Multi-Agent Path Finding (MAPF). This review surveys different\ncategories of classic and state-of-the-art MAPF algorithms and different\nresearch attempts to tackle the challenges of generalizing MAPF techniques to\nreal-world scenarios.\n  Recent Findings\n  Solving MAPF problems optimally is computationally challenging. Recent\nadvances have resulted in MAPF algorithms that can compute collision-free paths\nfor hundreds of robots and thousands of navigation tasks in seconds of runtime.\nMany variants of MAPF have been formalized to adapt MAPF techniques to\ndifferent real-world requirements, such as considerations of robot kinematics,\nonline optimization for real-time systems, and the integration of task\nassignment and path planning.\n  Summary\n  Algorithmic techniques for MAPF problems have addressed important aspects of\nseveral multi-robot applications, including automated warehouse fulfillment and\nsortation, automated train scheduling, and navigation of non-holonomic robots\nand quadcopters. This showcases their potential for real-world applications of\nlarge-scale multi-robot systems.\n",
        "published": "2022",
        "authors": [
            "Hang Ma"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.12718v1",
        "title": "Hierarchical Reinforcement Learning with Opponent Modeling for\n  Distributed Multi-agent Cooperation",
        "abstract": "  Many real-world applications can be formulated as multi-agent cooperation\nproblems, such as network packet routing and coordination of autonomous\nvehicles. The emergence of deep reinforcement learning (DRL) provides a\npromising approach for multi-agent cooperation through the interaction of the\nagents and environments. However, traditional DRL solutions suffer from the\nhigh dimensions of multiple agents with continuous action space during policy\nsearch. Besides, the dynamicity of agents' policies makes the training\nnon-stationary. To tackle the issues, we propose a hierarchical reinforcement\nlearning approach with high-level decision-making and low-level individual\ncontrol for efficient policy search. In particular, the cooperation of multiple\nagents can be learned in high-level discrete action space efficiently. At the\nsame time, the low-level individual control can be reduced to single-agent\nreinforcement learning. In addition to hierarchical reinforcement learning, we\npropose an opponent modeling network to model other agents' policies during the\nlearning process. In contrast to end-to-end DRL approaches, our approach\nreduces the learning complexity by decomposing the overall task into sub-tasks\nin a hierarchical way. To evaluate the efficiency of our approach, we conduct a\nreal-world case study in the cooperative lane change scenario. Both simulation\nand real-world experiments show the superiority of our approach in the\ncollision rate and convergence speed.\n",
        "published": "2022",
        "authors": [
            "Zhixuan Liang",
            "Jiannong Cao",
            "Shan Jiang",
            "Divya Saxena",
            "Huafeng Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.08279v1",
        "title": "Task Allocation with Load Management in Multi-Agent Teams",
        "abstract": "  In operations of multi-agent teams ranging from homogeneous robot swarms to\nheterogeneous human-autonomy teams, unexpected events might occur. While\nefficiency of operation for multi-agent task allocation problems is the primary\nobjective, it is essential that the decision-making framework is intelligent\nenough to manage unexpected task load with limited resources. Otherwise,\noperation effectiveness would drastically plummet with overloaded agents facing\nunforeseen risks. In this work, we present a decision-making framework for\nmulti-agent teams to learn task allocation with the consideration of load\nmanagement through decentralized reinforcement learning, where idling is\nencouraged and unnecessary resource usage is avoided. We illustrate the effect\nof load management on team performance and explore agent behaviors in example\nscenarios. Furthermore, a measure of agent importance in collaboration is\ndeveloped to infer team resilience when facing handling potential overload\nsituations.\n",
        "published": "2022",
        "authors": [
            "Haochen Wu",
            "Amin Ghadami",
            "Alparslan Emrah Bayrak",
            "Jonathon M. Smereka",
            "Bogdan I. Epureanu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.08860v1",
        "title": "Optimizing Indoor Navigation Policies For Spatial Distancing",
        "abstract": "  In this paper, we focus on the modification of policies that can lead to\nmovement patterns and directional guidance of occupants, which are represented\nas agents in a 3D simulation engine. We demonstrate an optimization method that\nimproves a spatial distancing metric by modifying the navigation graph by\nintroducing a measure of spatial distancing of agents as a function of agent\ndensity (i.e., occupancy). Our optimization framework utilizes such metrics as\nthe target function, using a hybrid approach of combining genetic algorithm and\nsimulated annealing. We show that within our framework, the\nsimulation-optimization process can help to improve spatial distancing between\nagents by optimizing the navigation policies for a given indoor environment.\n",
        "published": "2022",
        "authors": [
            "Xun Zhang",
            "Mathew Schwartz",
            "Muhammad Usman",
            "Petros Faloutsos",
            "Mubbasir Kapadia"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.09650v1",
        "title": "Task Allocation using a Team of Robots",
        "abstract": "  Task allocation using a team or coalition of robots is one of the most\nimportant problems in robotics, computer science, operational research, and\nartificial intelligence. In recent work, research has focused on handling\ncomplex objectives and feasibility constraints amongst other variations of the\nmulti-robot task allocation problem. There are many examples of important\nresearch progress in these directions. We present a general formulation of the\ntask allocation problem that generalizes several versions that are\nwell-studied. Our formulation includes the states of robots, tasks, and the\nsurrounding environment in which they operate. We describe how the problem can\nvary depending on the feasibility constraints, objective functions, and the\nlevel of dynamically changing information. In addition, we discuss existing\nsolution approaches for the problem including optimization-based approaches,\nand market-based approaches.\n",
        "published": "2022",
        "authors": [
            "Haris Aziz",
            "Arindam Pal",
            "Ali Pourmiri",
            "Fahimeh Ramezani",
            "Brendan Sims"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.00268v1",
        "title": "Unified Automatic Control of Vehicular Systems with Reinforcement\n  Learning",
        "abstract": "  Emerging vehicular systems with increasing proportions of automated\ncomponents present opportunities for optimal control to mitigate congestion and\nincrease efficiency. There has been a recent interest in applying deep\nreinforcement learning (DRL) to these nonlinear dynamical systems for the\nautomatic design of effective control strategies. Despite conceptual advantages\nof DRL being model-free, studies typically nonetheless rely on training setups\nthat are painstakingly specialized to specific vehicular systems. This is a key\nchallenge to efficient analysis of diverse vehicular and mobility systems. To\nthis end, this article contributes a streamlined methodology for vehicular\nmicrosimulation and discovers high performance control strategies with minimal\nmanual design. A variable-agent, multi-task approach is presented for\noptimization of vehicular Partially Observed Markov Decision Processes. The\nmethodology is experimentally validated on mixed autonomy traffic systems,\nwhere fractions of vehicles are automated; empirical improvement, typically\n15-60% over a human driving baseline, is observed in all configurations of six\ndiverse open or closed traffic systems. The study reveals numerous emergent\nbehaviors resembling wave mitigation, traffic signaling, and ramp metering.\nFinally, the emergent behaviors are analyzed to produce interpretable control\nstrategies, which are validated against the learned control strategies.\n",
        "published": "2022",
        "authors": [
            "Zhongxia Yan",
            "Abdul Rahman Kreidieh",
            "Eugene Vinitsky",
            "Alexandre M. Bayen",
            "Cathy Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.01222v1",
        "title": "Optimal and Bounded-Suboptimal Multi-Goal Task Assignment and Path\n  Finding",
        "abstract": "  We formalize and study the multi-goal task assignment and path finding\n(MG-TAPF) problem from theoretical and algorithmic perspectives. The MG-TAPF\nproblem is to compute an assignment of tasks to agents, where each task\nconsists of a sequence of goal locations, and collision-free paths for the\nagents that visit all goal locations of their assigned tasks in sequence.\nTheoretically, we prove that the MG-TAPF problem is NP-hard to solve optimally.\nWe present algorithms that build upon algorithmic techniques for the\nmulti-agent path finding problem and solve the MG-TAPF problem optimally and\nbounded-suboptimally. We experimentally compare these algorithms on a variety\nof different benchmark domains.\n",
        "published": "2022",
        "authors": [
            "Xinyi Zhong",
            "Jiaoyang Li",
            "Sven Koenig",
            "Hang Ma"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.01223v1",
        "title": "Multi-Goal Multi-Agent Pickup and Delivery",
        "abstract": "  In this work, we consider the Multi-Agent Pickup-and-Delivery (MAPD) problem,\nwhere agents constantly engage with new tasks and need to plan collision-free\npaths to execute them. To execute a task, an agent needs to visit a pair of\ngoal locations, consisting of a pickup location and a delivery location. We\npropose two variants of an algorithm that assigns a sequence of tasks to each\nagent using the anytime algorithm Large Neighborhood Search (LNS) and plans\npaths using the Multi-Agent Path Finding (MAPF) algorithm Priority-Based Search\n(PBS). LNS-PBS is complete for well-formed MAPD instances, a realistic subclass\nof MAPD instances, and empirically more effective than the existing complete\nMAPD algorithm CENTRAL. LNS-wPBS provides no completeness guarantee but is\nempirically more efficient and stable than LNS-PBS. It scales to thousands of\nagents and thousands of tasks in a large warehouse and is empirically more\neffective than the existing scalable MAPD algorithm HBH+MLA*. LNS-PBS and\nLNS-wPBS also apply to a more general variant of MAPD, namely the Multi-Goal\nMAPD (MG-MAPD) problem, where tasks can have different numbers of goal\nlocations.\n",
        "published": "2022",
        "authors": [
            "Qinghong Xu",
            "Jiaoyang Li",
            "Sven Koenig",
            "Hang Ma"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.02993v2",
        "title": "Learning to Coordinate for a Worker-Station Multi-robot System in Planar\n  Coverage Tasks",
        "abstract": "  For massive large-scale tasks, a multi-robot system (MRS) can effectively\nimprove efficiency by utilizing each robot's different capabilities, mobility,\nand functionality. In this paper, we focus on the multi-robot coverage path\nplanning (mCPP) problem in large-scale planar areas with random dynamic\ninterferers in the environment, where the robots have limited resources. We\nintroduce a worker-station MRS consisting of multiple workers with limited\nresources for actual work, and one station with enough resources for resource\nreplenishment. We aim to solve the mCPP problem for the worker-station MRS by\nformulating it as a fully cooperative multi-agent reinforcement learning\nproblem. Then we propose an end-to-end decentralized online planning method,\nwhich simultaneously solves coverage planning for workers and rendezvous\nplanning for station. Our method manages to reduce the influence of random\ndynamic interferers on planning, while the robots can avoid collisions with\nthem. We conduct simulation and real robot experiments, and the comparison\nresults show that our method has competitive performance in solving the mCPP\nproblem for worker-station MRS in metric of task finish time.\n",
        "published": "2022",
        "authors": [
            "Jingtao Tang",
            "Yuan Gao",
            "Tin Lun Lam"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.05540v2",
        "title": "Augmented Driver Behavior Models for High-Fidelity Simulation Study of\n  Crash Detection Algorithms",
        "abstract": "  Developing safety and efficiency applications for Connected and Automated\nVehicles (CAVs) require a great deal of testing and evaluation. The need for\nthe operation of these systems in critical and dangerous situations makes the\nburden of their evaluation very costly, possibly dangerous, and time-consuming.\nAs an alternative, researchers attempt to study and evaluate their algorithms\nand designs using simulation platforms. Modeling the behavior of drivers or\nhuman operators in CAVs or other vehicles interacting with them is one of the\nmain challenges of such simulations. While developing a perfect model for human\nbehavior is a challenging task and an open problem, we present a significant\naugmentation of the current models used in simulators for driver behavior. In\nthis paper, we present a simulation platform for a hybrid transportation system\nthat includes both human-driven and automated vehicles. In addition, we\ndecompose the human driving task and offer a modular approach to simulating a\nlarge-scale traffic scenario, allowing for a thorough investigation of\nautomated and active safety systems. Such representation through Interconnected\nmodules offers a human-interpretable system that can be tuned to represent\ndifferent classes of drivers. Additionally, we analyze a large driving dataset\nto extract expressive parameters that would best describe different driving\ncharacteristics. Finally, we recreate a similarly dense traffic scenario within\nour simulator and conduct a thorough analysis of various human-specific and\nsystem-specific factors, studying their effect on traffic network performance\nand safety.\n",
        "published": "2022",
        "authors": [
            "Ahura Jami",
            "Mahdi Razzaghpour",
            "Hussein Alnuweiri",
            "Yaser P. Fallah"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.02672v1",
        "title": "Bayesian Statistical Model Checking for Multi-agent Systems using\n  HyperPCTL*",
        "abstract": "  In this paper, we present a Bayesian method for statistical model checking\n(SMC) of probabilistic hyperproperties specified in the logic HyperPCTL* on\ndiscrete-time Markov chains (DTMCs). While SMC of HyperPCTL* using sequential\nprobability ratio test (SPRT) has been explored before, we develop an\nalternative SMC algorithm based on Bayesian hypothesis testing. In comparison\nto PCTL*, verifying HyperPCTL* formulae is complex owing to their simultaneous\ninterpretation on multiple paths of the DTMC. In addition, extending the\nbottom-up model-checking algorithm of the non-probabilistic setting is not\nstraight forward due to the fact that SMC does not return exact answers to the\nsatisfiability problems of subformulae, instead, it only returns correct\nanswers with high-confidence. We propose a recursive algorithm for SMC of\nHyperPCTL* based on a modified Bayes' test that factors in the uncertainty in\nthe recursive satisfiability results. We have implemented our algorithm in a\nPython toolbox, HyProVer, and compared our approach with the SPRT based SMC.\nOur experimental evaluation demonstrates that our Bayesian SMC algorithm\nperforms better both in terms of the verification time and the number of\nsamples required to deduce satisfiability of a given HyperPCTL* formula.\n",
        "published": "2022",
        "authors": [
            "Spandan Das",
            "Pavithra Prabhakar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.02886v1",
        "title": "KT-BT: A Framework for Knowledge Transfer Through Behavior Trees in\n  Multi-Robot Systems",
        "abstract": "  Multi-Robot and Multi-Agent Systems demonstrate collective (swarm)\nintelligence through systematic and distributed integration of local behaviors\nin a group. Agents sharing knowledge about the mission and environment can\nenhance performance at individual and mission levels. However, this is\ndifficult to achieve, partly due to the lack of a generic framework for\ntransferring part of the known knowledge (behaviors) between agents. This paper\npresents a new knowledge representation framework and a transfer strategy\ncalled KT-BT: Knowledge Transfer through Behavior Trees. The KT-BT framework\nfollows a query-response-update mechanism through an online Behavior Tree\nframework, where agents broadcast queries for unknown conditions and respond\nwith appropriate knowledge using a condition-action-control sub-flow. We embed\na novel grammar structure called stringBT that encodes knowledge, enabling\nbehavior sharing. We theoretically investigate the properties of the KT-BT\nframework in achieving homogeneity of high knowledge across the entire group\ncompared to a heterogeneous system without the capability of sharing their\nknowledge. We extensively verify our framework in a simulated multi-robot\nsearch and rescue problem. The results show successful knowledge transfers and\nimproved group performance in various scenarios. We further study the effects\nof opportunities and communication range on group performance, knowledge\nspread, and functional heterogeneity in a group of agents, presenting\ninteresting insights.\n",
        "published": "2022",
        "authors": [
            "Sanjay Sarma Oruganti Venkata",
            "Ramviyas Parasuraman",
            "Ramana Pidaparti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.04696v2",
        "title": "Cooperation and Competition: Flocking with Evolutionary Multi-Agent\n  Reinforcement Learning",
        "abstract": "  Flocking is a very challenging problem in a multi-agent system; traditional\nflocking methods also require complete knowledge of the environment and a\nprecise model for control. In this paper, we propose Evolutionary Multi-Agent\nReinforcement Learning (EMARL) in flocking tasks, a hybrid algorithm that\ncombines cooperation and competition with little prior knowledge. As for\ncooperation, we design the agents' reward for flocking tasks according to the\nboids model. While for competition, agents with high fitness are designed as\nsenior agents, and those with low fitness are designed as junior, letting\njunior agents inherit the parameters of senior agents stochastically. To\nintensify competition, we also design an evolutionary selection mechanism that\nshows effectiveness on credit assignment in flocking tasks. Experimental\nresults in a range of challenging and self-contrast benchmarks demonstrate that\nEMARL significantly outperforms the full competition or cooperation methods.\n",
        "published": "2022",
        "authors": [
            "Yunxiao Guo",
            "Xinjia Xie",
            "Runhao Zhao",
            "Chenglan Zhu",
            "Jiangting Yin",
            "Han Long"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.10003v2",
        "title": "Macro-Action-Based Multi-Agent/Robot Deep Reinforcement Learning under\n  Partial Observability",
        "abstract": "  The state-of-the-art multi-agent reinforcement learning (MARL) methods have\nprovided promising solutions to a variety of complex problems. Yet, these\nmethods all assume that agents perform synchronized primitive-action executions\nso that they are not genuinely scalable to long-horizon real-world\nmulti-agent/robot tasks that inherently require agents/robots to asynchronously\nreason about high-level action selection at varying time durations. The\nMacro-Action Decentralized Partially Observable Markov Decision Process\n(MacDec-POMDP) is a general formalization for asynchronous decision-making\nunder uncertainty in fully cooperative multi-agent tasks. In this thesis, we\nfirst propose a group of value-based RL approaches for MacDec-POMDPs, where\nagents are allowed to perform asynchronous learning and decision-making with\nmacro-action-value functions in three paradigms: decentralized learning and\ncontrol, centralized learning and control, and centralized training for\ndecentralized execution (CTDE). Building on the above work, we formulate a set\nof macro-action-based policy gradient algorithms under the three training\nparadigms, where agents are allowed to directly optimize their parameterized\npolicies in an asynchronous manner. We evaluate our methods both in simulation\nand on real robots over a variety of realistic domains. Empirical results\ndemonstrate the superiority of our approaches in large multi-agent problems and\nvalidate the effectiveness of our algorithms for learning high-quality and\nasynchronous solutions with macro-actions.\n",
        "published": "2022",
        "authors": [
            "Yuchen Xiao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.12091v1",
        "title": "Graph Neural Networks for Multi-Robot Active Information Acquisition",
        "abstract": "  This paper addresses the Multi-Robot Active Information Acquisition (AIA)\nproblem, where a team of mobile robots, communicating through an underlying\ngraph, estimates a hidden state expressing a phenomenon of interest.\nApplications like target tracking, coverage and SLAM can be expressed in this\nframework. Existing approaches, though, are either not scalable, unable to\nhandle dynamic phenomena or not robust to changes in the communication graph.\nTo counter these shortcomings, we propose an Information-aware Graph Block\nNetwork (I-GBNet), an AIA adaptation of Graph Neural Networks, that aggregates\ninformation over the graph representation and provides sequential-decision\nmaking in a distributed manner. The I-GBNet, trained via imitation learning\nwith a centralized sampling-based expert solver, exhibits permutation\nequivariance and time invariance, while harnessing the superior scalability,\nrobustness and generalizability to previously unseen environments and robot\nconfigurations. Experiments on significantly larger graphs and dimensionality\nof the hidden state and more complex environments than those seen in training\nvalidate the properties of the proposed architecture and its efficacy in the\napplication of localization and tracking of dynamic targets.\n",
        "published": "2022",
        "authors": [
            "Mariliza Tzes",
            "Nikolaos Bousias",
            "Evangelos Chatzipantazis",
            "George J. Pappas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.02300v3",
        "title": "Spatial-Temporal-Aware Safe Multi-Agent Reinforcement Learning of\n  Connected Autonomous Vehicles in Challenging Scenarios",
        "abstract": "  Communication technologies enable coordination among connected and autonomous\nvehicles (CAVs). However, it remains unclear how to utilize shared information\nto improve the safety and efficiency of the CAV system in dynamic and\ncomplicated driving scenarios. In this work, we propose a framework of\nconstrained multi-agent reinforcement learning (MARL) with a parallel Safety\nShield for CAVs in challenging driving scenarios that includes unconnected\nhazard vehicles. The coordination mechanisms of the proposed MARL include\ninformation sharing and cooperative policy learning, with Graph Convolutional\nNetwork (GCN)-Transformer as a spatial-temporal encoder that enhances the\nagent's environment awareness. The Safety Shield module with Control Barrier\nFunctions (CBF)-based safety checking protects the agents from taking unsafe\nactions. We design a constrained multi-agent advantage actor-critic (CMAA2C)\nalgorithm to train safe and cooperative policies for CAVs. With the experiment\ndeployed in the CARLA simulator, we verify the performance of the safety\nchecking, spatial-temporal encoder, and coordination mechanisms designed in our\nmethod by comparative experiments in several challenging scenarios with\nunconnected hazard vehicles. Results show that our proposed methodology\nsignificantly increases system safety and efficiency in challenging scenarios.\n",
        "published": "2022",
        "authors": [
            "Zhili Zhang",
            "Songyang Han",
            "Jiangwei Wang",
            "Fei Miao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.04333v2",
        "title": "Hypergraph-based Multi-Robot Task and Motion Planning",
        "abstract": "  We present a multi-robot task and motion planning method that, when applied\nto the rearrangement of objects by manipulators, results in solution times up\nto three orders of magnitude faster than existing methods and successfully\nplans for problems with up to twenty objects, more than three times as many\nobjects as comparable methods. We achieve this improvement by decomposing the\nplanning space to consider manipulators alone, objects, and manipulators\nholding objects. We represent this decomposition with a hypergraph where\nvertices are decomposed elements of the planning spaces and hyperarcs are\ntransitions between elements. Existing methods use graph-based representations\nwhere vertices are full composite spaces and edges are transitions between\nthese. Using the hypergraph reduces the representation size of the planning\nspace-for multi-manipulator object rearrangement, the number of hypergraph\nvertices scales linearly with the number of either robots or objects, while the\nnumber of hyperarcs scales quadratically with the number of robots and linearly\nwith the number of objects. In contrast, the number of vertices and edges in\ngraph-based representations scales exponentially in the number of robots and\nobjects. We show that similar gains can be achieved for other multi-robot task\nand motion planning problems.\n",
        "published": "2022",
        "authors": [
            "James Motes",
            "Tan Chen",
            "Timothy Bretl",
            "Marco Morales",
            "Nancy M. Amato"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.07141v2",
        "title": "Scalable Multi-robot Motion Planning for Congested Environments With\n  Topological Guidance",
        "abstract": "  Multi-robot motion planning (MRMP) is the problem of finding collision-free\npaths for a set of robots in a continuous state space. The difficulty of MRMP\nincreases with the number of robots and is exacerbated in environments with\nnarrow passages that robots must pass through, like warehouse aisles where\ncoordination between robots is required. In single-robot settings,\ntopology-guided motion planning methods have shown improved performance in\nthese constricted environments. In this work, we extend an existing\ntopology-guided single-robot motion planning method to the multi-robot domain\nto leverage the improved efficiency provided by topological guidance. We\ndemonstrate our method's ability to efficiently plan paths in complex\nenvironments with many narrow passages, scaling to robot teams of size up to 25\ntimes larger than existing methods in this class of problems. By leveraging\nknowledge of the topology of the environment, we also find higher-quality\nsolutions than other methods.\n",
        "published": "2022",
        "authors": [
            "Courtney McBeth",
            "James Motes",
            "Diane Uwacu",
            "Marco Morales",
            "Nancy M. Amato"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.08390v1",
        "title": "SOCIALMAPF: Optimal and Efficient Multi-Agent Path Finding with\n  Strategic Agents for Social Navigation",
        "abstract": "  We propose an extension to the MAPF formulation, called SocialMAPF, to\naccount for private incentives of agents in constrained environments such as\ndoorways, narrow hallways, and corridor intersections. SocialMAPF is able to,\nfor instance, accurately reason about the urgent incentive of an agent rushing\nto the hospital over another agent's less urgent incentive of going to a\ngrocery store; MAPF ignores such agent-specific incentives. Our proposed\nformulation addresses the open problem of optimal and efficient path planning\nfor agents with private incentives. To solve SocialMAPF, we propose a new class\nof algorithms that use mechanism design during conflict resolution to\nsimultaneously optimize agents' private local utilities and the global system\nobjective. We perform an extensive array of experiments that show that optimal\nsearch-based MAPF techniques lead to collisions and increased time-to-goal in\nSocialMAPF compared to our proposed method using mechanism design. Furthermore,\nwe empirically demonstrate that mechanism design results in models that\nmaximizes agent utility and minimizes the overall time-to-goal of the entire\nsystem. We further showcase the capabilities of mechanism design-based planning\nby successfully deploying it in environments with static obstacles. To\nconclude, we briefly list several research directions using the SocialMAPF\nformulation, such as exploring motion planning in the continuous domain for\nagents with private incentives.\n",
        "published": "2022",
        "authors": [
            "Rohan Chandra",
            "Rahul Maligi",
            "Arya Anantula",
            "Joydeep Biswas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.08672v1",
        "title": "Decision-Making Among Bounded Rational Agents",
        "abstract": "  When robots share the same workspace with other intelligent agents (e.g.,\nother robots or humans), they must be able to reason about the behaviors of\ntheir neighboring agents while accomplishing the designated tasks. In practice,\nfrequently, agents do not exhibit absolutely rational behavior due to their\nlimited computational resources. Thus, predicting the optimal agent behaviors\nis undesirable (because it demands prohibitive computational resources) and\nundesirable (because the prediction may be wrong). Motivated by this\nobservation, we remove the assumption of perfectly rational agents and propose\nincorporating the concept of bounded rationality from an information-theoretic\nview into the game-theoretic framework. This allows the robots to reason other\nagents' sub-optimal behaviors and act accordingly under their computational\nconstraints. Specifically, bounded rationality directly models the agent's\ninformation processing ability, which is represented as the KL-divergence\nbetween nominal and optimized stochastic policies, and the solution to the\nbounded-optimal policy can be obtained by an efficient importance sampling\napproach. Using both simulated and real-world experiments in multi-robot\nnavigation tasks, we demonstrate that the resulting framework allows the robots\nto reason about different levels of rational behaviors of other agents and\ncompute a reasonable strategy under its computational constraint.\n",
        "published": "2022",
        "authors": [
            "Junhong Xu",
            "Durgakant Pushp",
            "Kai Yin",
            "Lantao Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.14774v1",
        "title": "Unknown area exploration for robots with energy constraints using a\n  modified Butterfly Optimization Algorithm",
        "abstract": "  Butterfly Optimization Algorithm (BOA) is a recent metaheuristic that has\nbeen used in several optimization problems. In this paper, we propose a new\nversion of the algorithm (xBOA) based on the crossover operator and compare its\nresults to the original BOA and 3 other variants recently introduced in the\nliterature. We also proposed a framework for solving the unknown area\nexploration problem with energy constraints using metaheuristics in both\nsingle- and multi-robot scenarios. This framework allowed us to benchmark the\nperformances of different metaheuristics for the robotics exploration problem.\nWe conducted several experiments to validate this framework and used it to\ncompare the effectiveness of xBOA with wellknown metaheuristics used in the\nliterature through 5 evaluation criteria. Although BOA and xBOA are not optimal\nin all these criteria, we found that BOA can be a good alternative to many\nmetaheuristics in terms of the exploration time, while xBOA is more robust to\nlocal optima; has better fitness convergence; and achieves better exploration\nrates than the original BOA and its other variants.\n",
        "published": "2022",
        "authors": [
            "Amine Bendahmane",
            "Redouane Tlemsani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.02127v3",
        "title": "Scalable Multi-Agent Reinforcement Learning through Intelligent\n  Information Aggregation",
        "abstract": "  We consider the problem of multi-agent navigation and collision avoidance\nwhen observations are limited to the local neighborhood of each agent. We\npropose InforMARL, a novel architecture for multi-agent reinforcement learning\n(MARL) which uses local information intelligently to compute paths for all the\nagents in a decentralized manner. Specifically, InforMARL aggregates\ninformation about the local neighborhood of agents for both the actor and the\ncritic using a graph neural network and can be used in conjunction with any\nstandard MARL algorithm. We show that (1) in training, InforMARL has better\nsample efficiency and performance than baseline approaches, despite using less\ninformation, and (2) in testing, it scales well to environments with arbitrary\nnumbers of agents and obstacles. We illustrate these results using four task\nenvironments, including one with predetermined goals for each agent, and one in\nwhich the agents collectively try to cover all goals. Code available at\nhttps://github.com/nsidn98/InforMARL.\n",
        "published": "2022",
        "authors": [
            "Siddharth Nayak",
            "Kenneth Choi",
            "Wenqi Ding",
            "Sydney Dolan",
            "Karthik Gopalakrishnan",
            "Hamsa Balakrishnan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.03408v5",
        "title": "RITA: Boost Driving Simulators with Realistic Interactive Traffic Flow",
        "abstract": "  High-quality traffic flow generation is the core module in building\nsimulators for autonomous driving. However, the majority of available\nsimulators are incapable of replicating traffic patterns that accurately\nreflect the various features of real-world data while also simulating\nhuman-like reactive responses to the tested autopilot driving strategies.\nTaking one step forward to addressing such a problem, we propose Realistic\nInteractive TrAffic flow (RITA) as an integrated component of existing driving\nsimulators to provide high-quality traffic flow for the evaluation and\noptimization of the tested driving strategies. RITA is developed with\nconsideration of three key features, i.e., fidelity, diversity, and\ncontrollability, and consists of two core modules called RITABackend and\nRITAKit. RITABackend is built to support vehicle-wise control and provide\ntraffic generation models from real-world datasets, while RITAKit is developed\nwith easy-to-use interfaces for controllable traffic generation via\nRITABackend. We demonstrate RITA's capacity to create diversified and\nhigh-fidelity traffic simulations in several highly interactive highway\nscenarios. The experimental findings demonstrate that our produced RITA traffic\nflows exhibit all three key features, hence enhancing the completeness of\ndriving strategy evaluation. Moreover, we showcase the possibility for further\nimprovement of baseline strategies through online fine-tuning with RITA traffic\nflows.\n",
        "published": "2022",
        "authors": [
            "Zhengbang Zhu",
            "Shenyu Zhang",
            "Yuzheng Zhuang",
            "Yuecheng Liu",
            "Minghuan Liu",
            "Liyuan Mao",
            "Ziqin Gong",
            "Shixiong Kai",
            "Qiang Gu",
            "Bin Wang",
            "Siyuan Cheng",
            "Xinyu Wang",
            "Jianye Hao",
            "Yong Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.12885v1",
        "title": "Cost Splitting for Multi-Objective Conflict-Based Search",
        "abstract": "  The Multi-Objective Multi-Agent Path Finding (MO-MAPF) problem is the problem\nof finding the Pareto-optimal frontier of collision-free paths for a team of\nagents while minimizing multiple cost metrics. Examples of such cost metrics\ninclude arrival times, travel distances, and energy consumption.In this paper,\nwe focus on the Multi-Objective Conflict-Based Search (MO-CBS) algorithm, a\nstate-of-the-art MO-MAPF algorithm. We show that the standard splitting\nstrategy used by MO-CBS can lead to duplicate search nodes and hence can\nduplicate the search effort that MO-CBS needs to make. To address this issue,\nwe propose two new splitting strategies for MO-CBS, namely cost splitting and\ndisjoint cost splitting. Our theoretical results show that, when combined with\neither of these two new splitting strategies, MO-CBS maintains its completeness\nand optimality guarantees. Our experimental results show that disjoint cost\nsplitting, our best splitting strategy, speeds up MO-CBS by up to two orders of\nmagnitude and substantially improves its success rates in various settings.\n",
        "published": "2022",
        "authors": [
            "Cheng Ge",
            "Han Zhang",
            "Jiaoyang Li",
            "Sven Koenig"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.13432v1",
        "title": "LaCAM: Search-Based Algorithm for Quick Multi-Agent Pathfinding",
        "abstract": "  We propose a novel complete algorithm for multi-agent pathfinding (MAPF)\ncalled lazy constraints addition search for MAPF (LaCAM). MAPF is a problem of\nfinding collision-free paths for multiple agents on graphs and is the\nfoundation of multi-robot coordination. LaCAM uses a two-level search to find\nsolutions quickly, even with hundreds of agents or more. At the low-level, it\nsearches constraints about agents' locations. At the high-level, it searches a\nsequence of all agents' locations, following the constraints specified by the\nlow-level. Our exhaustive experiments reveal that LaCAM is comparable to or\noutperforms state-of-the-art sub-optimal MAPF algorithms in a variety of\nscenarios, regarding success rate, planning time, and solution quality of\nsum-of-costs.\n",
        "published": "2022",
        "authors": [
            "Keisuke Okumura"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.13908v1",
        "title": "Fault-Tolerant Offline Multi-Agent Path Planning",
        "abstract": "  We study a novel graph path planning problem for multiple agents that may\ncrash at runtime, and block part of the workspace. In our setting, agents can\ndetect neighboring crashed agents, and change followed paths at runtime. The\nobjective is then to prepare a set of paths and switching rules for each agent,\nensuring that all correct agents reach their destinations without collisions or\ndeadlocks, despite unforeseen crashes of other agents. Such planning is\nattractive to build reliable multi-robot systems. We present problem\nformalization, theoretical analysis such as computational complexities, and how\nto solve this offline planning problem.\n",
        "published": "2022",
        "authors": [
            "Keisuke Okumura",
            "S\u00e9bastien Tixeuil"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.14095v1",
        "title": "A Hierarchical Variable Autonomy Mixed-Initiative Framework for\n  Human-Robot Teaming in Mobile Robotics",
        "abstract": "  This paper presents a Mixed-Initiative (MI) framework for addressing the\nproblem of control authority transfer between a remote human operator and an AI\nagent when cooperatively controlling a mobile robot. Our Hierarchical\nExpert-guided Mixed-Initiative Control Switcher (HierEMICS) leverages\ninformation on the human operator's state and intent. The control switching\npolicies are based on a criticality hierarchy. An experimental evaluation was\nconducted in a high-fidelity simulated disaster response and remote inspection\nscenario, comparing HierEMICS with a state-of-the-art Expert-guided\nMixed-Initiative Control Switcher (EMICS) in the context of mobile robot\nnavigation. Results suggest that HierEMICS reduces conflicts for control\nbetween the human and the AI agent, which is a fundamental challenge in both\nthe MI control paradigm and also in the related shared control paradigm.\nAdditionally, we provide statistically significant evidence of improved,\nnavigational safety (i.e., fewer collisions), LOA switching efficiency, and\nconflict for control reduction.\n",
        "published": "2022",
        "authors": [
            "Dimitris Panagopoulos",
            "Giannis Petousakis",
            "Aniketh Ramesh",
            "Tianshu Ruan",
            "Grigoris Nikolaou",
            "Rustam Stolkin",
            "Manolis Chiou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.02909v1",
        "title": "Scalable Planning and Learning Framework Development for Swarm-to-Swarm\n  Engagement Problems",
        "abstract": "  Development of guidance, navigation and control frameworks/algorithms for\nswarms attracted significant attention in recent years. That being said,\nalgorithms for planning swarm allocations/trajectories for engaging with enemy\nswarms is largely an understudied problem. Although small-scale scenarios can\nbe addressed with tools from differential game theory, existing approaches fail\nto scale for large-scale multi-agent pursuit evasion (PE) scenarios. In this\nwork, we propose a reinforcement learning (RL) based framework to decompose to\nlarge-scale swarm engagement problems into a number of independent multi-agent\npursuit-evasion games. We simulate a variety of multi-agent PE scenarios, where\nfinite time capture is guaranteed under certain conditions. The calculated PE\nstatistics are provided as a reward signal to the high level allocation layer,\nwhich uses an RL algorithm to allocate controlled swarm units to eliminate\nenemy swarm units with maximum efficiency. We verify our approach in\nlarge-scale swarm-to-swarm engagement simulations.\n",
        "published": "2022",
        "authors": [
            "Umut Demir",
            "A. Sadik Satir",
            "Gulay Goktas Sever",
            "Cansu Yikilmaz",
            "Nazim Kemal Ure"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.08659v1",
        "title": "A Hierarchical Framework for Collaborative Artificial Intelligence",
        "abstract": "  We propose a hierarchical framework for collaborative intelligent systems.\nThis framework organizes research challenges based on the nature of the\ncollaborative activity and the information that must be shared, with each level\nbuilding on capabilities provided by lower levels. We review research paradigms\nat each level, with a description of classical engineering-based approaches and\nmodern alternatives based on machine learning, illustrated with a running\nexample using a hypothetical personal service robot. We discuss cross-cutting\nissues that occur at all levels, focusing on the problem of communicating and\nsharing comprehension, the role of explanation and the social nature of\ncollaboration. We conclude with a summary of research challenges and a\ndiscussion of the potential for economic and societal impact provided by\ntechnologies that enhance human abilities and empower people and society\nthrough collaboration with Intelligent Systems.\n",
        "published": "2022",
        "authors": [
            "James L. Crowley",
            "Jo\u00eblle L Coutaz",
            "Jasmin Grosinger",
            "Javier V\u00e1zquez-Salceda",
            "Cecilio Angulo",
            "Alberto Sanfeliu",
            "Luca Iocchi",
            "Anthony G. Cohn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.14124v1",
        "title": "Joint Action is a Framework for Understanding Partnerships Between\n  Humans and Upper Limb Prostheses",
        "abstract": "  Recent advances in upper limb prostheses have led to significant improvements\nin the number of movements provided by the robotic limb. However, the method\nfor controlling multiple degrees of freedom via user-generated signals remains\nchallenging. To address this issue, various machine learning controllers have\nbeen developed to better predict movement intent. As these controllers become\nmore intelligent and take on more autonomy in the system, the traditional\napproach of representing the human-machine interface as a human controlling a\ntool becomes limiting. One possible approach to improve the understanding of\nthese interfaces is to model them as collaborative, multi-agent systems through\nthe lens of joint action. The field of joint action has been commonly applied\nto two human partners who are trying to work jointly together to achieve a\ntask, such as singing or moving a table together, by effecting coordinated\nchange in their shared environment. In this work, we compare different\nprosthesis controllers (proportional electromyography with sequential\nswitching, pattern recognition, and adaptive switching) in terms of how they\npresent the hallmarks of joint action. The results of the comparison lead to a\nnew perspective for understanding how existing myoelectric systems relate to\neach other, along with recommendations for how to improve these systems by\nincreasing the collaborative communication between each partner.\n",
        "published": "2022",
        "authors": [
            "Michael R. Dawson",
            "Adam S. R. Parker",
            "Heather E. Williams",
            "Ahmed W. Shehata",
            "Jacqueline S. Hebert",
            "Craig S. Chapman",
            "Patrick M. Pilarski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.04446v1",
        "title": "An Efficient Approach to the Online Multi-Agent Path Finding Problem by\n  Using Sustainable Information",
        "abstract": "  Multi-agent path finding (MAPF) is the problem of moving agents to the goal\nvertex without collision. In the online MAPF problem, new agents may be added\nto the environment at any time, and the current agents have no information\nabout future agents. The inability of existing online methods to reuse previous\nplanning contexts results in redundant computation and reduces algorithm\nefficiency. Hence, we propose a three-level approach to solve online MAPF\nutilizing sustainable information, which can decrease its redundant\ncalculations. The high-level solver, the Sustainable Replan algorithm (SR),\nmanages the planning context and simulates the environment. The middle-level\nsolver, the Sustainable Conflict-Based Search algorithm (SCBS), builds a\nconflict tree and maintains the planning context. The low-level solver, the\nSustainable Reverse Safe Interval Path Planning algorithm (SRSIPP), is an\nefficient single-agent solver that uses previous planning context to reduce\nduplicate calculations. Experiments show that our proposed method has\nsignificant improvement in terms of computational efficiency. In one of the\ntest scenarios, our algorithm can be 1.48 times faster than SOTA on average\nunder different agent number settings.\n",
        "published": "2023",
        "authors": [
            "Mingkai Tang",
            "Boyi Liu",
            "Yuanhang Li",
            "Hongji Liu",
            "Ming Liu",
            "Lujia Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.08038v2",
        "title": "A Unified Architecture for Dynamic Role Allocation and Collaborative\n  Task Planning in Mixed Human-Robot Teams",
        "abstract": "  The growing deployment of human-robot collaborative processes in several\nindustrial applications, such as handling, welding, and assembly, unfolds the\npursuit of systems which are able to manage large heterogeneous teams and, at\nthe same time, monitor the execution of complex tasks. In this paper, we\npresent a novel architecture for dynamic role allocation and collaborative task\nplanning in a mixed human-robot team of arbitrary size. The architecture\ncapitalizes on a centralized reactive and modular task-agnostic planning method\nbased on Behavior Trees (BTs), in charge of actions scheduling, while the\nallocation problem is formulated through a Mixed-Integer Linear Program (MILP),\nthat assigns dynamically individual roles or collaborations to the agents of\nthe team. Different metrics used as MILP cost allow the architecture to favor\nvarious aspects of the collaboration (e.g. makespan, ergonomics, human\npreferences). Human preference are identified through a negotiation phase, in\nwhich, an human agent can accept/refuse to execute the assigned task.In\naddition, bilateral communication between humans and the system is achieved\nthrough an Augmented Reality (AR) custom user interface that provides intuitive\nfunctionalities to assist and coordinate workers in different action phases.\nThe computational complexity of the proposed methodology outperforms literature\napproaches in industrial sized jobs and teams (problems up to 50 actions and 20\nagents in the team with collaborations are solved within 1 s). The different\nallocated roles, as the cost functions change, highlights the flexibility of\nthe architecture to several production requirements. Finally, the subjective\nevaluation demonstrating the high usability level and the suitability for the\ntargeted scenario.\n",
        "published": "2023",
        "authors": [
            "Edoardo Lamon",
            "Fabio Fusaro",
            "Elena De Momi",
            "Arash Ajoudani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.04094v1",
        "title": "Learning Graph-Enhanced Commander-Executor for Multi-Agent Navigation",
        "abstract": "  This paper investigates the multi-agent navigation problem, which requires\nmultiple agents to reach the target goals in a limited time. Multi-agent\nreinforcement learning (MARL) has shown promising results for solving this\nissue. However, it is inefficient for MARL to directly explore the (nearly)\noptimal policy in the large search space, which is exacerbated as the agent\nnumber increases (e.g., 10+ agents) or the environment is more complex (e.g.,\n3D simulator). Goal-conditioned hierarchical reinforcement learning (HRL)\nprovides a promising direction to tackle this challenge by introducing a\nhierarchical structure to decompose the search space, where the low-level\npolicy predicts primitive actions in the guidance of the goals derived from the\nhigh-level policy. In this paper, we propose Multi-Agent Graph-Enhanced\nCommander-Executor (MAGE-X), a graph-based goal-conditioned hierarchical method\nfor multi-agent navigation tasks. MAGE-X comprises a high-level Goal Commander\nand a low-level Action Executor. The Goal Commander predicts the probability\ndistribution of goals and leverages them to assign each agent the most\nappropriate final target. The Action Executor utilizes graph neural networks\n(GNN) to construct a subgraph for each agent that only contains crucial\npartners to improve cooperation. Additionally, the Goal Encoder in the Action\nExecutor captures the relationship between the agent and the designated goal to\nencourage the agent to reach the final target. The results show that MAGE-X\noutperforms the state-of-the-art MARL baselines with a 100% success rate with\nonly 3 million training steps in multi-agent particle environments (MPE) with\n50 agents, and at least a 12% higher success rate and 2x higher data efficiency\nin a more complicated quadrotor 3D navigation task.\n",
        "published": "2023",
        "authors": [
            "Xinyi Yang",
            "Shiyu Huang",
            "Yiwen Sun",
            "Yuxiang Yang",
            "Chao Yu",
            "Wei-Wei Tu",
            "Huazhong Yang",
            "Yu Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.05528v1",
        "title": "Learning cooperative behaviours in adversarial multi-agent systems",
        "abstract": "  This work extends an existing virtual multi-agent platform called RoboSumo to\ncreate TripleSumo -- a platform for investigating multi-agent cooperative\nbehaviors in continuous action spaces, with physical contact in an adversarial\nenvironment. In this paper we investigate a scenario in which two agents,\nnamely `Bug' and `Ant', must team up and push another agent `Spider' out of the\narena. To tackle this goal, the newly added agent `Bug' is trained during an\nongoing match between `Ant' and `Spider'. `Bug' must develop awareness of the\nother agents' actions, infer the strategy of both sides, and eventually learn\nan action policy to cooperate. The reinforcement learning algorithm Deep\nDeterministic Policy Gradient (DDPG) is implemented with a hybrid reward\nstructure combining dense and sparse rewards. The cooperative behavior is\nquantitatively evaluated by the mean probability of winning the match and mean\nnumber of steps needed to win.\n",
        "published": "2023",
        "authors": [
            "Ni Wang",
            "Gautham P. Das",
            "Alan G. Millard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.06872v1",
        "title": "Adaptive Value Decomposition with Greedy Marginal Contribution\n  Computation for Cooperative Multi-Agent Reinforcement Learning",
        "abstract": "  Real-world cooperation often requires intensive coordination among agents\nsimultaneously. This task has been extensively studied within the framework of\ncooperative multi-agent reinforcement learning (MARL), and value decomposition\nmethods are among those cutting-edge solutions. However, traditional methods\nthat learn the value function as a monotonic mixing of per-agent utilities\ncannot solve the tasks with non-monotonic returns. This hinders their\napplication in generic scenarios. Recent methods tackle this problem from the\nperspective of implicit credit assignment by learning value functions with\ncomplete expressiveness or using additional structures to improve cooperation.\nHowever, they are either difficult to learn due to large joint action spaces or\ninsufficient to capture the complicated interactions among agents which are\nessential to solving tasks with non-monotonic returns. To address these\nproblems, we propose a novel explicit credit assignment method to address the\nnon-monotonic problem. Our method, Adaptive Value decomposition with Greedy\nMarginal contribution (AVGM), is based on an adaptive value decomposition that\nlearns the cooperative value of a group of dynamically changing agents. We\nfirst illustrate that the proposed value decomposition can consider the\ncomplicated interactions among agents and is feasible to learn in large-scale\nscenarios. Then, our method uses a greedy marginal contribution computed from\nthe value decomposition as an individual credit to incentivize agents to learn\nthe optimal cooperative policy. We further extend the module with an action\nencoder to guarantee the linear time complexity for computing the greedy\nmarginal contribution. Experimental results demonstrate that our method\nachieves significant performance improvements in several non-monotonic domains.\n",
        "published": "2023",
        "authors": [
            "Shanqi Liu",
            "Yujing Hu",
            "Runze Wu",
            "Dong Xing",
            "Yu Xiong",
            "Changjie Fan",
            "Kun Kuang",
            "Yong Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.07337v3",
        "title": "Graph Attention Multi-Agent Fleet Autonomy for Advanced Air Mobility",
        "abstract": "  Autonomous mobility is emerging as a new disruptive mode of urban\ntransportation for moving cargo and passengers. However, designing scalable\nautonomous fleet coordination schemes to accommodate fast-growing mobility\nsystems is challenging primarily due to the increasing heterogeneity of the\nfleets, time-varying demand patterns, service area expansions, and\ncommunication limitations. We introduce the concept of partially observable\nadvanced air mobility games to coordinate a fleet of aerial vehicles by\naccounting for the heterogeneity of the interacting agents and the\nself-interested nature inherent to commercial mobility fleets. To model the\ncomplex interactions among the agents and the observation uncertainty in the\nmobility networks, we propose a novel heterogeneous graph attention\nencoder-decoder (HetGAT Enc-Dec) neural network-based stochastic policy. We\ntrain the policy by leveraging deep multi-agent reinforcement learning,\nallowing decentralized decision-making for the agents using their local\nobservations. Through extensive experimentation, we show that the learned\npolicy generalizes to various fleet compositions, demand patterns, and\nobservation topologies. Further, fleets operating under the HetGAT Enc-Dec\npolicy outperform other state-of-the-art graph neural network policies by\nachieving the highest fleet reward and fulfillment ratios in on-demand mobility\nnetworks.\n",
        "published": "2023",
        "authors": [
            "Malintha Fernando",
            "Ransalu Senanayake",
            "Heeyoul Choi",
            "Martin Swany"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.13629v2",
        "title": "Estimation of continuous environments by robot swarms: Correlated\n  networks and decision-making",
        "abstract": "  Collective decision-making is an essential capability of large-scale\nmulti-robot systems to establish autonomy on the swarm level. A large portion\nof literature on collective decision-making in swarm robotics focuses on\ndiscrete decisions selecting from a limited number of options. Here we assign a\ndecentralized robot system with the task of exploring an unbounded environment,\nfinding consensus on the mean of a measurable environmental feature, and\naggregating at areas where that value is measured (e.g., a contour line). A\nunique quality of this task is a causal loop between the robots' dynamic\nnetwork topology and their decision-making. For example, the network's mean\nnode degree influences time to convergence while the currently agreed-on mean\nvalue influences the swarm's aggregation location, hence, also the network\nstructure as well as the precision error. We propose a control algorithm and\nstudy it in real-world robot swarm experiments in different environments. We\nshow that our approach is effective and achieves higher precision than a\ncontrol experiment. We anticipate applications, for example, in containing\npollution with surface vehicles.\n",
        "published": "2023",
        "authors": [
            "Mohsen Raoufi",
            "Pawel Romanczuk",
            "Heiko Hamann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.03602v1",
        "title": "Data Games: A Game-Theoretic Approach to Swarm Robotic Data Collection",
        "abstract": "  Fleets of networked autonomous vehicles (AVs) collect terabytes of sensory\ndata, which is often transmitted to central servers (the ''cloud'') for\ntraining machine learning (ML) models. Ideally, these fleets should upload all\ntheir data, especially from rare operating contexts, in order to train robust\nML models. However, this is infeasible due to prohibitive network bandwidth and\ndata labeling costs. Instead, we propose a cooperative data sampling strategy\nwhere geo-distributed AVs collaborate to collect a diverse ML training dataset\nin the cloud. Since the AVs have a shared objective but minimal information\nabout each other's local data distribution and perception model, we can\nnaturally cast cooperative data collection as an $N$-player mathematical game.\nWe show that our cooperative sampling strategy uses minimal information to\nconverge to a centralized oracle policy with complete information about all\nAVs. Moreover, we theoretically characterize the performance benefits of our\ngame-theoretic strategy compared to greedy sampling. Finally, we experimentally\ndemonstrate that our method outperforms standard benchmarks by up to $21.9\\%$\non 4 perception datasets, including for autonomous driving in adverse weather\nconditions. Crucially, our experimental results on real-world datasets closely\nalign with our theoretical guarantees.\n",
        "published": "2023",
        "authors": [
            "Oguzhan Akcin",
            "Po-han Li",
            "Shubhankar Agarwal",
            "Sandeep Chinchali"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.04320v2",
        "title": "SG-LSTM: Social Group LSTM for Robot Navigation Through Dense Crowds",
        "abstract": "  With the increasing availability and affordability of personal robots, they\nwill no longer be confined to large corporate warehouses or factories but will\ninstead be expected to operate in less controlled environments alongside larger\ngroups of people. In addition to ensuring safety and efficiency, it is crucial\nto minimize any negative psychological impact robots may have on humans and\nfollow unwritten social norms in these situations. Our research aims to develop\na model that can predict the movements of pedestrians and perceptually-social\ngroups in crowded environments. We introduce a new Social Group Long Short-term\nMemory (SG-LSTM) model that models human groups and interactions in dense\nenvironments using a socially-aware LSTM to produce more accurate trajectory\npredictions. Our approach enables navigation algorithms to calculate\ncollision-free paths faster and more accurately in crowded environments.\nAdditionally, we also release a large video dataset with labeled pedestrian\ngroups for the broader social navigation community. We show comparisons with\ndifferent metrics on different datasets (ETH, Hotel, MOT15) and different\nprediction approaches (LIN, LSTM, O-LSTM, S-LSTM) as well as runtime\nperformance.\n",
        "published": "2023",
        "authors": [
            "Rashmi Bhaskara",
            "Maurice Chiu",
            "Aniket Bera"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.04322v2",
        "title": "DroNeRF: Real-time Multi-agent Drone Pose Optimization for Computing\n  Neural Radiance Fields",
        "abstract": "  We present a novel optimization algorithm called DroNeRF for the autonomous\npositioning of monocular camera drones around an object for real-time 3D\nreconstruction using only a few images. Neural Radiance Fields or NeRF, is a\nnovel view synthesis technique used to generate new views of an object or scene\nfrom a set of input images. Using drones in conjunction with NeRF provides a\nunique and dynamic way to generate novel views of a scene, especially with\nlimited scene capabilities of restricted movements. Our approach focuses on\ncalculating optimized pose for individual drones while solely depending on the\nobject geometry without using any external localization system. The unique\ncamera positioning during the data-capturing phase significantly impacts the\nquality of the 3D model. To evaluate the quality of our generated novel views,\nwe compute different perceptual metrics like the Peak Signal-to-Noise Ratio\n(PSNR) and Structural Similarity Index Measure(SSIM). Our work demonstrates the\nbenefit of using an optimal placement of various drones with limited mobility\nto generate perceptually better results.\n",
        "published": "2023",
        "authors": [
            "Dipam Patel",
            "Phu Pham",
            "Aniket Bera"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.05584v1",
        "title": "SOCIALGYM 2.0: Simulator for Multi-Agent Social Robot Navigation in\n  Shared Human Spaces",
        "abstract": "  We present SocialGym 2, a multi-agent navigation simulator for social robot\nresearch. Our simulator models multiple autonomous agents, replicating\nreal-world dynamics in complex environments, including doorways, hallways,\nintersections, and roundabouts. Unlike traditional simulators that concentrate\non single robots with basic kinematic constraints in open spaces, SocialGym 2\nemploys multi-agent reinforcement learning (MARL) to develop optimal navigation\npolicies for multiple robots with diverse, dynamic constraints in complex\nenvironments. Built on the PettingZoo MARL library and Stable Baselines3 API,\nSocialGym 2 offers an accessible python interface that integrates with a\nnavigation stack through ROS messaging. SocialGym 2 can be easily installed and\nis packaged in a docker container, and it provides the capability to swap and\nevaluate different MARL algorithms, as well as customize observation and reward\nfunctions. We also provide scripts to allow users to create their own\nenvironments and have conducted benchmarks using various social navigation\nalgorithms, reporting a broad range of social navigation metrics. Projected\nhosted at: https://amrl.cs.utexas.edu/social_gym/index.html\n",
        "published": "2023",
        "authors": [
            "Zayne Sprague",
            "Rohan Chandra",
            "Jarrett Holtz",
            "Joydeep Biswas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.17422v1",
        "title": "Robust Multi-Agent Pickup and Delivery with Delays",
        "abstract": "  Multi-Agent Pickup and Delivery (MAPD) is the problem of computing\ncollision-free paths for a group of agents such that they can safely reach\ndelivery locations from pickup ones. These locations are provided at runtime,\nmaking MAPD a combination between classical Multi-Agent Path Finding (MAPF) and\nonline task assignment. Current algorithms for MAPD do not consider many of the\npractical issues encountered in real applications: real agents often do not\nfollow the planned paths perfectly, and may be subject to delays and failures.\nIn this paper, we study the problem of MAPD with delays, and we present two\nsolution approaches that provide robustness guarantees by planning paths that\nlimit the effects of imperfect execution. In particular, we introduce two\nalgorithms, k-TP and p-TP, both based on a decentralized algorithm typically\nused to solve MAPD, Token Passing (TP), which offer deterministic and\nprobabilistic guarantees, respectively. Experimentally, we compare our\nalgorithms against a version of TP enriched with online replanning. k-TP and\np-TP provide robust solutions, significantly reducing the number of replans\ncaused by delays, with little or no increase in solution cost and running time.\n",
        "published": "2023",
        "authors": [
            "Giacomo Lodigiani",
            "Nicola Basilico",
            "Francesco Amigoni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.00342v1",
        "title": "Factorization of Multi-Agent Sampling-Based Motion Planning",
        "abstract": "  Modern robotics often involves multiple embodied agents operating within a\nshared environment. Path planning in these cases is considerably more\nchallenging than in single-agent scenarios. Although standard Sampling-based\nAlgorithms (SBAs) can be used to search for solutions in the robots' joint\nspace, this approach quickly becomes computationally intractable as the number\nof agents increases. To address this issue, we integrate the concept of\nfactorization into sampling-based algorithms, which requires only minimal\nmodifications to existing methods. During the search for a solution we can\ndecouple (i.e., factorize) different subsets of agents into independent\nlower-dimensional search spaces once we certify that their future solutions\nwill be independent of each other using a factorization heuristic.\nConsequently, we progressively construct a lean hypergraph where certain\n(hyper-)edges split the agents to independent subgraphs. In the best case, this\napproach can reduce the growth in dimensionality of the search space from\nexponential to linear in the number of agents. On average, fewer samples are\nneeded to find high-quality solutions while preserving the optimality,\ncompleteness, and anytime properties of SBAs. We present a general\nimplementation of a factorized SBA, derive an analytical gain in terms of\nsample complexity for PRM*, and showcase empirical results for RRG.\n",
        "published": "2023",
        "authors": [
            "Alessandro Zanardi",
            "Pietro Zullo",
            "Andrea Censi",
            "Emilio Frazzoli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.00367v1",
        "title": "Conveying Autonomous Robot Capabilities through Contrasting Behaviour\n  Summaries",
        "abstract": "  As advances in artificial intelligence enable increasingly capable\nlearning-based autonomous agents, it becomes more challenging for human\nobservers to efficiently construct a mental model of the agent's behaviour. In\norder to successfully deploy autonomous agents, humans should not only be able\nto understand the individual limitations of the agents but also have insight on\nhow they compare against one another. To do so, we need effective methods for\ngenerating human interpretable agent behaviour summaries. Single agent\nbehaviour summarization has been tackled in the past through methods that\ngenerate explanations for why an agent chose to pick a particular action at a\nsingle timestep. However, for complex tasks, a per-action explanation may not\nbe able to convey an agents global strategy. As a result, researchers have\nlooked towards multi-timestep summaries which can better help humans assess an\nagents overall capability. More recently, multi-step summaries have also been\nused for generating contrasting examples to evaluate multiple agents. However,\npast approaches have largely relied on unstructured search methods to generate\nsummaries and require agents to have a discrete action space. In this paper we\npresent an adaptive search method for efficiently generating contrasting\nbehaviour summaries with support for continuous state and action spaces. We\nperform a user study to evaluate the effectiveness of the summaries for helping\nhumans discern the superior autonomous agent for a given task. Our results\nindicate that adaptive search can efficiently identify informative contrasting\nscenarios that enable humans to accurately select the better performing agent\nwith a limited observation time budget.\n",
        "published": "2023",
        "authors": [
            "Peter Du",
            "Surya Murthy",
            "Katherine Driggs-Campbell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.04217v1",
        "title": "The Study of Highway for Lifelong Multi-Agent Path Finding",
        "abstract": "  In modern fulfillment warehouses, agents traverse the map to complete endless\ntasks that arrive on the fly, which is formulated as a lifelong Multi-Agent\nPath Finding (lifelong MAPF) problem. The goal of tackling this challenging\nproblem is to find the path for each agent in a finite runtime while maximizing\nthe throughput. However, existing methods encounter exponential growth of\nruntime and undesirable phenomena of deadlocks and rerouting as the map size or\nagent density grows. To address these challenges in lifelong MAPF, we explore\nthe idea of highways mainly studied for one-shot MAPF (i.e., finding paths at\nonce beforehand), which reduces the complexity of the problem by encouraging\nagents to move in the same direction. We utilize two methods to incorporate the\nhighway idea into the lifelong MAPF framework and discuss the properties that\nminimize the existing problems of deadlocks and rerouting. The experimental\nresults demonstrate that the runtime is considerably reduced and the decay of\nthroughput is gradually insignificant as the map size enlarges under the\nsettings of the highway. Furthermore, when the density of agents increases, the\nphenomena of deadlocks and rerouting are significantly reduced by leveraging\nthe highway.\n",
        "published": "2023",
        "authors": [
            "Ming-Feng Li",
            "Min Sun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.14309v1",
        "title": "Double-Deck Multi-Agent Pickup and Delivery: Multi-Robot Rearrangement\n  in Large-Scale Warehouses",
        "abstract": "  We introduce a new problem formulation, Double-Deck Multi-Agent Pickup and\nDelivery (DD-MAPD), which models the multi-robot shelf rearrangement problem in\nautomated warehouses. DD-MAPD extends both Multi-Agent Pickup and Delivery\n(MAPD) and Multi-Agent Path Finding (MAPF) by allowing agents to move beneath\nshelves or lift and deliver a shelf to an arbitrary location, thereby changing\nthe warehouse layout. We show that solving DD-MAPD is NP-hard. To tackle\nDD-MAPD, we propose MAPF-DECOMP, an algorithmic framework that decomposes a\nDD-MAPD instance into a MAPF instance for coordinating shelf trajectories and a\nsubsequent MAPD instance with task dependencies for computing paths for agents.\nWe also present an optimization technique to improve the performance of\nMAPF-DECOMP and demonstrate how to make MAPF-DECOMP complete for well-formed\nDD-MAPD instances, a realistic subclass of DD-MAPD instances. Our experimental\nresults demonstrate the efficiency and effectiveness of MAPF-DECOMP, with the\nability to compute high-quality solutions for large-scale instances with over\none thousand shelves and hundreds of agents in just minutes of runtime.\n",
        "published": "2023",
        "authors": [
            "Baiyu Li",
            "Hang Ma"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.02058v1",
        "title": "Human Machine Co-adaption Interface via Cooperation Markov Decision\n  Process System",
        "abstract": "  This paper aims to develop a new human-machine interface to improve\nrehabilitation performance from the perspective of both the user (patient) and\nthe machine (robot) by introducing the co-adaption techniques via model-based\nreinforcement learning. Previous studies focus more on robot assistance, i.e.,\nto improve the control strategy so as to fulfill the objective of\nAssist-As-Needed. In this study, we treat the full process of robot-assisted\nrehabilitation as a co-adaptive or mutual learning process and emphasize the\nadaptation of the user to the machine. To this end, we proposed a Co-adaptive\nMDPs (CaMDPs) model to quantify the learning rates based on cooperative\nmulti-agent reinforcement learning (MARL) in the high abstraction layer of the\nsystems. We proposed several approaches to cooperatively adjust the Policy\nImprovement among the two agents in the framework of Policy Iteration. Based on\nthe proposed co-adaptive MDPs, the simulation study indicates the\nnon-stationary problem can be mitigated using various proposed Policy\nImprovement approaches.\n",
        "published": "2023",
        "authors": [
            "Kairui Guo",
            "Adrian Cheng",
            "Yaqi Li",
            "Jun Li",
            "Rob Duffield",
            "Steven W. Su"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.03632v1",
        "title": "Improving LaCAM for Scalable Eventually Optimal Multi-Agent Pathfinding",
        "abstract": "  This study extends the recently-developed LaCAM algorithm for multi-agent\npathfinding (MAPF). LaCAM is a sub-optimal search-based algorithm that uses\nlazy successor generation to dramatically reduce the planning effort. We\npresent two enhancements. First, we propose its anytime version, called LaCAM*,\nwhich eventually converges to optima, provided that solution costs are\naccumulated transition costs. Second, we improve the successor generation to\nquickly obtain initial solutions. Exhaustive experiments demonstrate their\nutility. For instance, LaCAM* sub-optimally solved 99% of the instances\nretrieved from the MAPF benchmark, where the number of agents varied up to a\nthousand, within ten seconds on a standard desktop PC, while ensuring eventual\nconvergence to optima; developing a new horizon of MAPF algorithms.\n",
        "published": "2023",
        "authors": [
            "Keisuke Okumura"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.03735v1",
        "title": "Stackelberg Games for Learning Emergent Behaviors During Competitive\n  Autocurricula",
        "abstract": "  Autocurricular training is an important sub-area of multi-agent reinforcement\nlearning~(MARL) that allows multiple agents to learn emergent skills in an\nunsupervised co-evolving scheme. The robotics community has experimented\nautocurricular training with physically grounded problems, such as robust\ncontrol and interactive manipulation tasks. However, the asymmetric nature of\nthese tasks makes the generation of sophisticated policies challenging. Indeed,\nthe asymmetry in the environment may implicitly or explicitly provide an\nadvantage to a subset of agents which could, in turn, lead to a low-quality\nequilibrium. This paper proposes a novel game-theoretic algorithm, Stackelberg\nMulti-Agent Deep Deterministic Policy Gradient (ST-MADDPG), which formulates a\ntwo-player MARL problem as a Stackelberg game with one player as the `leader'\nand the other as the `follower' in a hierarchical interaction structure wherein\nthe leader has an advantage. We first demonstrate that the leader's advantage\nfrom ST-MADDPG can be used to alleviate the inherent asymmetry in the\nenvironment. By exploiting the leader's advantage, ST-MADDPG improves the\nquality of a co-evolution process and results in more sophisticated and complex\nstrategies that work well even against an unseen strong opponent.\n",
        "published": "2023",
        "authors": [
            "Boling Yang",
            "Liyuan Zheng",
            "Lillian J. Ratliff",
            "Byron Boots",
            "Joshua R. Smith"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.11510v1",
        "title": "Terraforming -- Environment Manipulation during Disruptions for\n  Multi-Agent Pickup and Delivery",
        "abstract": "  In automated warehouses, teams of mobile robots fulfill the packaging process\nby transferring inventory pods to designated workstations while navigating\nnarrow aisles formed by tightly packed pods. This problem is typically modeled\nas a Multi-Agent Pickup and Delivery (MAPD) problem, which is then solved by\nrepeatedly planning collision-free paths for agents on a fixed graph, as in the\nRolling-Horizon Collision Resolution (RHCR) algorithm. However, existing\napproaches make the limiting assumption that agents are only allowed to move\npods that correspond to their current task, while considering the other pods as\nstationary obstacles (even though all pods are movable). This behavior can\nresult in unnecessarily long paths which could otherwise be avoided by opening\nadditional corridors via pod manipulation. To this end, we explore the\nimplications of allowing agents the flexibility of dynamically relocating pods.\nWe call this new problem Terraforming MAPD (tMAPD) and develop an RHCR-based\napproach to tackle it. As the extra flexibility of terraforming comes at a\nsignificant computational cost, we utilize this capability judiciously by\nidentifying situations where it could make a significant impact on the solution\nquality. In particular, we invoke terraforming in response to disruptions that\noften occur in automated warehouses, e.g., when an item is dropped from a pod\nor when agents malfunction. Empirically, using our approach for tMAPD, where\ndisruptions are modeled via a stochastic process, we improve throughput by over\n10%, reduce the maximum service time (the difference between the drop-off time\nand the pickup time of a pod) by more than 50%, without drastically increasing\nthe runtime, compared to the MAPD setting.\n",
        "published": "2023",
        "authors": [
            "David Vainshtein",
            "Yaakov Sherma",
            "Kiril Solovey",
            "Oren Salzman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.16303v1",
        "title": "Fine-Grained Complexity Analysis of Multi-Agent Path Finding on 2D Grids",
        "abstract": "  Multi-Agent Path Finding (MAPF) is a fundamental motion coordination problem\narising in multi-agent systems with a wide range of applications. The problem's\nintractability has led to extensive research on improving the scalability of\nsolvers for it. Since optimal solvers can struggle to scale, a major challenge\nthat arises is understanding what makes MAPF hard. We tackle this challenge\nthrough a fine-grained complexity analysis of time-optimal MAPF on 2D grids,\nthereby closing two gaps and identifying a new tractability frontier. First, we\nshow that 2-colored MAPF, i.e., where the agents are divided into two teams,\neach with its own set of targets, remains NP-hard. Second, for the flowtime\nobjective (also called sum-of-costs), we show that it remains NP-hard to find a\nsolution in which agents have an individually optimal cost, which we call an\nindividually optimal solution. The previously tightest results for these MAPF\nvariants are for (non-grid) planar graphs. We use a single hardness\nconstruction that replaces, strengthens, and unifies previous proofs. We\nbelieve that it is also simpler than previous proofs for the planar case as it\nemploys minimal gadgets that enable its full visualization in one figure.\nFinally, for the flowtime objective, we establish a tractability frontier based\non the number of directions agents can move in. Namely, we complement our\nhardness result, which holds for three directions, with an efficient algorithm\nfor finding an individually optimal solution if only two directions are\nallowed. This result sheds new light on the structure of optimal solutions,\nwhich may help guide algorithm design for the general problem.\n",
        "published": "2023",
        "authors": [
            "Tzvika Geft"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.17018v2",
        "title": "Formal Modelling for Multi-Robot Systems Under Uncertainty",
        "abstract": "  Purpose of Review: To effectively synthesise and analyse multi-robot\nbehaviour, we require formal task-level models which accurately capture\nmulti-robot execution. In this paper, we review modelling formalisms for\nmulti-robot systems under uncertainty, and discuss how they can be used for\nplanning, reinforcement learning, model checking, and simulation.\n  Recent Findings: Recent work has investigated models which more accurately\ncapture multi-robot execution by considering different forms of uncertainty,\nsuch as temporal uncertainty and partial observability, and modelling the\neffects of robot interactions on action execution. Other strands of work have\npresented approaches for reducing the size of multi-robot models to admit more\nefficient solution methods. This can be achieved by decoupling the robots under\nindependence assumptions, or reasoning over higher level macro actions.\n  Summary: Existing multi-robot models demonstrate a trade off between\naccurately capturing robot dependencies and uncertainty, and being small enough\nto tractably solve real world problems. Therefore, future research should\nexploit realistic assumptions over multi-robot behaviour to develop smaller\nmodels which retain accurate representations of uncertainty and robot\ninteractions; and exploit the structure of multi-robot problems, such as\nfactored state spaces, to develop scalable solution methods.\n",
        "published": "2023",
        "authors": [
            "Charlie Street",
            "Masoumeh Mansouri",
            "Bruno Lacerda"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.17087v1",
        "title": "Communication-Efficient Reinforcement Learning in Swarm Robotic Networks\n  for Maze Exploration",
        "abstract": "  Smooth coordination within a swarm robotic system is essential for the\neffective execution of collective robot missions. Having efficient\ncommunication is key to the successful coordination of swarm robots. This paper\nproposes a new communication-efficient decentralized cooperative reinforcement\nlearning algorithm for coordinating swarm robots. It is made efficient by\nhierarchically building on the use of local information exchanges. We consider\na case study application of maze solving through cooperation among a group of\nrobots, where the time and costs are minimized while avoiding inter-robot\ncollisions and path overlaps during exploration. With a solid theoretical\nbasis, we extensively analyze the algorithm with realistic CORE network\nsimulations and evaluate it against state-of-the-art solutions in terms of maze\ncoverage percentage and efficiency under communication-degraded environments.\nThe results demonstrate significantly higher coverage accuracy and efficiency\nwhile reducing costs and overlaps even in high packet loss and low\ncommunication range scenarios.\n",
        "published": "2023",
        "authors": [
            "Ehsan Latif",
            "WenZhan Song",
            "Ramviyas Parasuraman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.06527v1",
        "title": "Contribution \u00e0 l'Optimisation d'un Comportement Collectif pour un\n  Groupe de Robots Autonomes",
        "abstract": "  This thesis studies the domain of collective robotics, and more particularly\nthe optimization problems of multirobot systems in the context of exploration,\npath planning and coordination. It includes two contributions. The first one is\nthe use of the Butterfly Optimization Algorithm (BOA) to solve the Unknown Area\nExploration problem with energy constraints in dynamic environments. This\nalgorithm was never used for solving robotics problems before, as far as we\nknow. We proposed a new version of this algorithm called xBOA based on the\ncrossover operator to improve the diversity of the candidate solutions and\nspeed up the convergence of the algorithm. The second contribution is the\ndevelopment of a new simulation framework for benchmarking dynamic incremental\nproblems in robotics such as exploration tasks. The framework is made in such a\nmanner to be generic to quickly compare different metaheuristics with minimum\nmodifications, and to adapt easily to single and multi-robot scenarios. Also,\nit provides researchers with tools to automate their experiments and generate\nvisuals, which will allow them to focus on more important tasks such as\nmodeling new algorithms. We conducted a series of experiments that showed\npromising results and allowed us to validate our approach and model.\n",
        "published": "2023",
        "authors": [
            "Amine Bendahmane"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.08815v1",
        "title": "Decentralized Social Navigation with Non-Cooperative Robots via Bi-Level\n  Optimization",
        "abstract": "  This paper presents a fully decentralized approach for realtime\nnon-cooperative multi-robot navigation in social mini-games, such as navigating\nthrough a narrow doorway or negotiating right of way at a corridor\nintersection. Our contribution is a new realtime bi-level optimization\nalgorithm, in which the top-level optimization consists of computing a fair and\ncollision-free ordering followed by the bottom-level optimization which plans\noptimal trajectories conditioned on the ordering. We show that, given such a\npriority order, we can impose simple kinodynamic constraints on each robot that\nare sufficient for it to plan collision-free trajectories with minimal\ndeviation from their preferred velocities, similar to how humans navigate in\nthese scenarios.\n  We successfully deploy the proposed algorithm in the real world using F$1/10$\nrobots, a Clearpath Jackal, and a Boston Dynamics Spot as well as in simulation\nusing the SocialGym 2.0 multi-agent social navigation simulator, in the doorway\nand corridor intersection scenarios. We compare with state-of-the-art social\nnavigation methods using multi-agent reinforcement learning, collision\navoidance algorithms, and crowd simulation models. We show that $(i)$ classical\nnavigation performs $44\\%$ better than the state-of-the-art learning-based\nsocial navigation algorithms, $(ii)$ without a scheduling protocol, our\napproach results in collisions in social mini-games $(iii)$ our approach yields\n$2\\times$ and $5\\times$ fewer velocity changes than CADRL in doorways and\nintersections, and finally $(iv)$ bi-level navigation in doorways at a flow\nrate of $2.8 - 3.3$ (ms)$^{-1}$ is comparable to flow rate in human navigation\nat a flow rate of $4$ (ms)$^{-1}$.\n",
        "published": "2023",
        "authors": [
            "Rohan Chandra",
            "Rahul Menon",
            "Zayne Sprague",
            "Arya Anantula",
            "Joydeep Biswas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.12623v1",
        "title": "SEAL: Simultaneous Exploration and Localization in Multi-Robot Systems",
        "abstract": "  The availability of accurate localization is critical for multi-robot\nexploration strategies; noisy or inconsistent localization causes failure in\nmeeting exploration objectives. We aim to achieve high localization accuracy\nwith contemporary exploration map belief and vice versa without needing global\nlocalization information. This paper proposes a novel simultaneous exploration\nand localization (SEAL) approach, which uses Gaussian Processes (GP)-based\ninformation fusion for maximum exploration while performing communication graph\noptimization for relative localization. Both these cross-dependent objectives\nwere integrated through the Rao-Blackwellization technique. Distributed\nlinearized convex hull optimization is used to select the next-best unexplored\nregion for distributed exploration. SEAL outperformed cutting-edge methods on\nexploration and localization performance in extensive ROS-Gazebo simulations,\nillustrating the practicality of the approach in real-world applications.\n",
        "published": "2023",
        "authors": [
            "Ehsan Latif",
            "Ramviyas Parasuraman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.14489v1",
        "title": "Decentralized Multi-Robot Formation Control Using Reinforcement Learning",
        "abstract": "  This paper presents a decentralized leader-follower multi-robot formation\ncontrol based on a reinforcement learning (RL) algorithm applied to a swarm of\nsmall educational Sphero robots. Since the basic Q-learning method is known to\nrequire large memory resources for Q-tables, this work implements the Double\nDeep Q-Network (DDQN) algorithm, which has achieved excellent results in many\nrobotic problems. To enhance the system behavior, we trained two different DDQN\nmodels, one for reaching the formation and the other for maintaining it. The\nmodels use a discrete set of robot motions (actions) to adapt the continuous\nnonlinear system to the discrete nature of RL. The presented approach has been\ntested in simulation and real experiments which show that the multi-robot\nsystem can achieve and maintain a stable formation without the need for complex\nmathematical models and nonlinear control laws.\n",
        "published": "2023",
        "authors": [
            "Juraj Obradovic",
            "Marko Krizmancic",
            "Stjepan Bogdan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.16784v2",
        "title": "A Survey on Datasets for Decision-making of Autonomous Vehicle",
        "abstract": "  Autonomous vehicles (AV) are expected to reshape future transportation\nsystems, and decision-making is one of the critical modules toward high-level\nautomated driving. To overcome those complicated scenarios that rule-based\nmethods could not cope with well, data-driven decision-making approaches have\naroused more and more focus. The datasets to be used in developing data-driven\nmethods dramatically influences the performance of decision-making, hence it is\nnecessary to have a comprehensive insight into the existing datasets. From the\naspects of collection sources, driving data can be divided into vehicle,\nenvironment, and driver related data. This study compares the state-of-the-art\ndatasets of these three categories and summarizes their features including\nsensors used, annotation, and driving scenarios. Based on the characteristics\nof the datasets, this survey also concludes the potential applications of\ndatasets on various aspects of AV decision-making, assisting researchers to\nfind appropriate ones to support their own research. The future trends of AV\ndataset development are summarized.\n",
        "published": "2023",
        "authors": [
            "Yuning Wang",
            "Zeyu Han",
            "Yining Xing",
            "Shaobing Xu",
            "Jianqiang Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.02637v1",
        "title": "Surge Routing: Event-informed Multiagent Reinforcement Learning for\n  Autonomous Rideshare",
        "abstract": "  Large events such as conferences, concerts and sports games, often cause\nsurges in demand for ride services that are not captured in average demand\npatterns, posing unique challenges for routing algorithms. We propose a\nlearning framework for an autonomous fleet of taxis that scrapes event data\nfrom the internet to predict and adapt to surges in demand and generates\ncooperative routing and pickup policies that service a higher number of\nrequests than other routing protocols. We achieve this through a combination of\n(i) an event processing framework that scrapes the internet for event\ninformation and generates dense vector representations that can be used as\ninput features for a neural network that predicts demand; (ii) a two neural\nnetwork system that predicts hourly demand over the entire map, using these\ndense vector representations; (iii) a probabilistic approach that leverages\nlocale occupancy schedules to map publicly available demand data over sectors\nto discretized street intersections; and finally, (iv) a scalable model-based\nreinforcement learning framework that uses the predicted demand over\nintersections to anticipate surges and route taxis using one-agent-at-a-time\nrollout with limited sampling certainty equivalence. We learn routing and\npickup policies using real NYC ride share data for 2022 and information for\nmore than 2000 events across 300 unique venues in Manhattan. We test our\napproach with a fleet of 100 taxis on a map with 38 different sectors (2235\nstreet intersections). Our experimental results demonstrate that our method\nobtains routing policies that service $6$ more requests on average per minute\n(around $360$ more requests per hour) than other model-based RL frameworks and\nother classical algorithms in operations research when dealing with surge\ndemand conditions.\n",
        "published": "2023",
        "authors": [
            "Daniel Garces",
            "Stephanie Gil"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.02691v1",
        "title": "SACHA: Soft Actor-Critic with Heuristic-Based Attention for Partially\n  Observable Multi-Agent Path Finding",
        "abstract": "  Multi-Agent Path Finding (MAPF) is a crucial component for many large-scale\nrobotic systems, where agents must plan their collision-free paths to their\ngiven goal positions. Recently, multi-agent reinforcement learning has been\nintroduced to solve the partially observable variant of MAPF by learning a\ndecentralized single-agent policy in a centralized fashion based on each\nagent's partial observation. However, existing learning-based methods are\nineffective in achieving complex multi-agent cooperation, especially in\ncongested environments, due to the non-stationarity of this setting. To tackle\nthis challenge, we propose a multi-agent actor-critic method called Soft\nActor-Critic with Heuristic-Based Attention (SACHA), which employs novel\nheuristic-based attention mechanisms for both the actors and critics to\nencourage cooperation among agents. SACHA learns a neural network for each\nagent to selectively pay attention to the shortest path heuristic guidance from\nmultiple agents within its field of view, thereby allowing for more scalable\nlearning of cooperation. SACHA also extends the existing multi-agent\nactor-critic framework by introducing a novel critic centered on each agent to\napproximate $Q$-values. Compared to existing methods that use a fully\nobservable critic, our agent-centered multi-agent actor-critic method results\nin more impartial credit assignment and better generalizability of the learned\npolicy to MAPF instances with varying numbers of agents and types of\nenvironments. We also implement SACHA(C), which embeds a communication module\nin the agent's policy network to enable information exchange among agents. We\nevaluate both SACHA and SACHA(C) on a variety of MAPF instances and demonstrate\ndecent improvements over several state-of-the-art learning-based MAPF methods\nwith respect to success rate and solution quality.\n",
        "published": "2023",
        "authors": [
            "Qiushi Lin",
            "Hang Ma"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.09364v1",
        "title": "Local Minima Drive Communications in Cooperative Interaction",
        "abstract": "  An important open question in human-robot interaction (HRI) is precisely when\nan agent should decide to communicate, particularly in a cooperative task.\nPerceptual Control Theory (PCT) tells us that agents are able to cooperate on a\njoint task simply by sharing the same 'intention', thereby distributing the\neffort required to complete the task among the agents. This is even true for\nagents that do not possess the same abilities, so long as the goal is\nobservable, the combined actions are sufficient to complete the task, and there\nis no local minimum in the search space. If these conditions hold, then a\ncooperative task can be accomplished without any communication between the\ncontributing agents. However, for tasks that do contain local minima, the\nglobal solution can only be reached if at least one of the agents adapts its\nintention at the appropriate moments, and this can only be achieved by\nappropriately timed communication. In other words, it is hypothesised that in\ncooperative tasks, the function of communication is to coordinate actions in a\ncomplex search space that contains local minima. These principles have been\nverified in a computer-based simulation environment in which two independent\none-dimensional agents are obliged to cooperate in order to solve a\ntwo-dimensional path-finding task.\n",
        "published": "2023",
        "authors": [
            "Roger K. Moore"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.00937v2",
        "title": "LEMMA: Learning Language-Conditioned Multi-Robot Manipulation",
        "abstract": "  Complex manipulation tasks often require robots with complementary\ncapabilities to collaborate. We introduce a benchmark for LanguagE-Conditioned\nMulti-robot MAnipulation (LEMMA) focused on task allocation and long-horizon\nobject manipulation based on human language instructions in a tabletop setting.\nLEMMA features 8 types of procedurally generated tasks with varying degree of\ncomplexity, some of which require the robots to use tools and pass tools to\neach other. For each task, we provide 800 expert demonstrations and human\ninstructions for training and evaluations. LEMMA poses greater challenges\ncompared to existing benchmarks, as it requires the system to identify each\nmanipulator's limitations and assign sub-tasks accordingly while also handling\nstrong temporal dependencies in each task. To address these challenges, we\npropose a modular hierarchical planning approach as a baseline. Our results\nhighlight the potential of LEMMA for developing future language-conditioned\nmulti-robot systems.\n",
        "published": "2023",
        "authors": [
            "Ran Gong",
            "Xiaofeng Gao",
            "Qiaozi Gao",
            "Suhaila Shakiah",
            "Govind Thattai",
            "Gaurav S. Sukhatme"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.04292v1",
        "title": "Engineering LaCAM$^\\ast$: Towards Real-Time, Large-Scale, and\n  Near-Optimal Multi-Agent Pathfinding",
        "abstract": "  This paper addresses the challenges of real-time, large-scale, and\nnear-optimal multi-agent pathfinding (MAPF) through enhancements to the\nrecently proposed LaCAM* algorithm. LaCAM* is a scalable search-based algorithm\nthat guarantees the eventual finding of optimal solutions for cumulative\ntransition costs. While it has demonstrated remarkable planning success rates,\nsurpassing various state-of-the-art MAPF methods, its initial solution quality\nis far from optimal, and its convergence speed to the optimum is slow. To\novercome these limitations, this paper introduces several improvement\ntechniques, partly drawing inspiration from other MAPF methods. We provide\nempirical evidence that the fusion of these techniques significantly improves\nthe solution quality of LaCAM*, thus further pushing the boundaries of MAPF\nalgorithms.\n",
        "published": "2023",
        "authors": [
            "Keisuke Okumura"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.11234v4",
        "title": "Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding",
        "abstract": "  Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that\nasks us to compute collision-free paths for a team of agents, all moving across\na shared map. Although many works appear on this topic, all current algorithms\nstruggle as the number of agents grows. The principal reason is that existing\napproaches typically plan free-flow optimal paths, which creates congestion. To\ntackle this issue we propose a new approach for MAPF where agents are guided to\ntheir destination by following congestion-avoiding paths. We evaluate the idea\nin two large-scale settings: one-shot MAPF, where each agent has a single\ndestination, and lifelong MAPF, where agents are continuously assigned new\ntasks. For one-shot MAPF we show that our approach substantially improves\nsolution quality. For Lifelong MAPF we report large improvements in overall\nthroughput.\n",
        "published": "2023",
        "authors": [
            "Zhe Chen",
            "Daniel Harabor",
            "Jiaoyang Li",
            "Peter J. Stuckey"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.03387v2",
        "title": "Efficient Baselines for Motion Prediction in Autonomous Driving",
        "abstract": "  Motion Prediction (MP) of multiple surroundings agents is a crucial task in\narbitrarily complex environments, from simple robots to Autonomous Driving\nStacks (ADS). Current techniques tackle this problem using end-to-end\npipelines, where the input data is usually a rendered top-view of the physical\ninformation and the past trajectories of the most relevant agents; leveraging\nthis information is a must to obtain optimal performance. In that sense, a\nreliable ADS must produce reasonable predictions on time. However, despite many\napproaches use simple ConvNets and LSTMs to obtain the social latent features,\nState-Of-The-Art (SOTA) models might be too complex for real-time applications\nwhen using both sources of information (map and past trajectories) as well as\nlittle interpretable, specially considering the physical information. Moreover,\nthe performance of such models highly depends on the number of available inputs\nfor each particular traffic scenario, which are expensive to obtain,\nparticularly, annotated High-Definition (HD) maps.\n  In this work, we propose several efficient baselines for the well-known\nArgoverse 1 Motion Forecasting Benchmark. We aim to develop compact models\nusing SOTA techniques for MP, including attention mechanisms and GNNs. Our\nlightweight models use standard social information and interpretable map\ninformation such as points from the driveable area and plausible centerlines by\nmeans of a novel preprocessing step based on kinematic constraints, in\nopposition to black-box CNN-based or too-complex graphs methods for map\nencoding, to generate plausible multimodal trajectories achieving up-to-pair\naccuracy with less operations and parameters than other SOTA methods. Our code\nis publicly available at https://github.com/Cram3r95/mapfe4mp .\n",
        "published": "2023",
        "authors": [
            "Carlos G\u00f3mez-Hu\u00e9lamo",
            "Marcos V. Conde",
            "Rafael Barea",
            "Manuel Oca\u00f1a",
            "Luis M. Bergasa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.13206v1",
        "title": "Intent-Aware Autonomous Driving: A Case Study on Highway Merging\n  Scenarios",
        "abstract": "  In this work, we use the communication of intent as a means to facilitate\ncooperation between autonomous vehicle agents. Generally speaking, intents can\nbe any reliable information about its future behavior that a vehicle\ncommunicates with another vehicle. We implement this as an intent-sharing task\natop the merging environment in the simulator of highway-env, which provides a\ncollection of environments for learning decision-making strategies for\nautonomous vehicles. Under a simple setting between two agents, we carefully\ninvestigate how intent-sharing can aid the receiving vehicle in adjusting its\nbehavior in highway merging scenarios.\n",
        "published": "2023",
        "authors": [
            "Nishtha Mahajan",
            "Qi Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.13285v1",
        "title": "Collision Avoidance and Navigation for a Quadrotor Swarm Using\n  End-to-end Deep Reinforcement Learning",
        "abstract": "  End-to-end deep reinforcement learning (DRL) for quadrotor control promises\nmany benefits -- easy deployment, task generalization and real-time execution\ncapability. Prior end-to-end DRL-based methods have showcased the ability to\ndeploy learned controllers onto single quadrotors or quadrotor teams\nmaneuvering in simple, obstacle-free environments. However, the addition of\nobstacles increases the number of possible interactions exponentially, thereby\nincreasing the difficulty of training RL policies. In this work, we propose an\nend-to-end DRL approach to control quadrotor swarms in environments with\nobstacles. We provide our agents a curriculum and a replay buffer of the\nclipped collision episodes to improve performance in obstacle-rich\nenvironments. We implement an attention mechanism to attend to the neighbor\nrobots and obstacle interactions - the first successful demonstration of this\nmechanism on policies for swarm behavior deployed on severely\ncompute-constrained hardware. Our work is the first work that demonstrates the\npossibility of learning neighbor-avoiding and obstacle-avoiding control\npolicies trained with end-to-end DRL that transfers zero-shot to real\nquadrotors. Our approach scales to 32 robots with 80% obstacle density in\nsimulation and 8 robots with 20% obstacle density in physical deployment. Video\ndemonstrations are available on the project website at:\nhttps://sites.google.com/view/obst-avoid-swarm-rl.\n",
        "published": "2023",
        "authors": [
            "Zhehui Huang",
            "Zhaojing Yang",
            "Rahul Krupani",
            "Bask\u0131n \u015eenba\u015flar",
            "Sumeet Batra",
            "Gaurav S. Sukhatme"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.17433v1",
        "title": "DREAM: Decentralized Reinforcement Learning for Exploration and\n  Efficient Energy Management in Multi-Robot Systems",
        "abstract": "  Resource-constrained robots often suffer from energy inefficiencies,\nunderutilized computational abilities due to inadequate task allocation, and a\nlack of robustness in dynamic environments, all of which strongly affect their\nperformance. This paper introduces DREAM - Decentralized Reinforcement Learning\nfor Exploration and Efficient Energy Management in Multi-Robot Systems, a\ncomprehensive framework that optimizes the allocation of resources for\nefficient exploration. It advances beyond conventional heuristic-based task\nplanning as observed conventionally. The framework incorporates Operational\nRange Estimation using Reinforcement Learning to perform exploration and\nobstacle avoidance in unfamiliar terrains. DREAM further introduces an Energy\nConsumption Model for goal allocation, thereby ensuring mission completion\nunder constrained resources using a Graph Neural Network. This approach also\nensures that the entire Multi-Robot System can survive for an extended period\nof time for further missions compared to the conventional approach of randomly\nallocating goals, which compromises one or more agents. Our approach adapts to\nprioritizing agents in real-time, showcasing remarkable resilience against\ndynamic environments. This robust solution was evaluated in various simulated\nenvironments, demonstrating adaptability and applicability across diverse\nscenarios. We observed a substantial improvement of about 25% over the baseline\nmethod, leading the way for future research in resource-constrained robotics.\n",
        "published": "2023",
        "authors": [
            "Dipam Patel",
            "Phu Pham",
            "Kshitij Tiwari",
            "Aniket Bera"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.14553v1",
        "title": "Denoising Opponents Position in Partial Observation Environment",
        "abstract": "  The RoboCup competitions hold various leagues, and the Soccer Simulation 2D\nLeague is a major among them. Soccer Simulation 2D (SS2D) match involves two\nteams, including 11 players and a coach for each team, competing against each\nother. The players can only communicate with the Soccer Simulation Server\nduring the game. Several code bases are released publicly to simplify team\ndevelopment. So researchers can easily focus on decision-making and\nimplementing machine learning methods. SS2D actions and behaviors are only\npartially accurate due to different challenges, such as noise and partial\nobservation. Therefore, one strategy is to implement alternative denoising\nmethods to tackle observation inaccuracy. Our idea is to predict opponent\npositions while they have yet to be seen in a finite number of cycles using\nmachine learning methods to make more accurate actions such as pass. We will\nexplain our position prediction idea powered by Long Short-Term Memory models\n(LSTM) and Deep Neural Networks (DNN). The results show that the LSTM and DNN\npredict the opponents' position more accurately than the standard algorithm,\nsuch as the last-seen method.\n",
        "published": "2023",
        "authors": [
            "Aref Sayareh",
            "Aria Sardari",
            "Vahid Khoddami",
            "Nader Zare",
            "Vinicius Prado da Fonseca",
            "Amilcar Soares"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.01534v1",
        "title": "Approximate Multiagent Reinforcement Learning for On-Demand Urban\n  Mobility Problem on a Large Map (extended version)",
        "abstract": "  In this paper, we focus on the autonomous multiagent taxi routing problem for\na large urban environment where the location and number of future ride requests\nare unknown a-priori, but follow an estimated empirical distribution. Recent\ntheory has shown that if a base policy is stable then a rollout-based algorithm\nwith such a base policy produces a near-optimal stable policy. Although,\nrollout-based approaches are well-suited for learning cooperative multiagent\npolicies with considerations for future demand, applying such methods to a\nlarge urban environment can be computationally expensive. Large environments\ntend to have a large volume of requests, and hence require a large fleet of\ntaxis to guarantee stability. In this paper, we aim to address the\ncomputational bottleneck of multiagent (one-at-a-time) rollout, where the\ncomputational complexity grows linearly in the number of agents. We propose an\napproximate one-at-a-time rollout-based two-phase algorithm that reduces the\ncomputational cost, while still achieving a stable near-optimal policy. Our\napproach partitions the graph into sectors based on the predicted demand and an\nuser-defined maximum number of agents that can be planned for using the\none-at-a-time rollout approach. The algorithm then applies instantaneous\nassignment (IA) for re-balancing taxis across sectors and a sector-wide\none-at-a-time rollout algorithm that is executed in parallel for each sector.\nWe characterize the number of taxis $m$ that is sufficient for IA base policy\nto be stable, and derive a necessary condition on $m$ as time goes to infinity.\nOur numerical results show that our approach achieves stability for an $m$ that\nsatisfies the theoretical conditions. We also empirically demonstrate that our\nproposed two-phase algorithm has comparable performance to the one-at-a-time\nrollout over the entire map, but with significantly lower runtimes.\n",
        "published": "2023",
        "authors": [
            "Daniel Garces",
            "Sushmita Bhattacharya",
            "Dimitri Bertsekas",
            "Stephanie Gil"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.04805v1",
        "title": "Development and Assessment of Autonomous Vehicles in Both Fully\n  Automated and Mixed Traffic Conditions",
        "abstract": "  Autonomous Vehicle (AV) technology is advancing rapidly, promising a\nsignificant shift in road transportation safety and potentially resolving\nvarious complex transportation issues. With the increasing deployment of AVs by\nvarious companies, questions emerge about how AVs interact with each other and\nwith human drivers, especially when AVs are prevalent on the roads. Ensuring\ncooperative interaction between AVs and between AVs and human drivers is\ncritical, though there are concerns about possible negative competitive\nbehaviors. This paper presents a multi-stage approach, starting with the\ndevelopment of a single AV and progressing to connected AVs, incorporating\nsharing and caring V2V communication strategy to enhance mutual coordination. A\nsurvey is conducted to validate the driving performance of the AV and will be\nutilized for a mixed traffic case study, which focuses on how the human drivers\nwill react to the AV driving alongside them on the same road. Results show that\nusing deep reinforcement learning, the AV acquired driving behavior that\nreached human driving performance. The adoption of sharing and caring based V2V\ncommunication within AV networks enhances their driving behavior, aids in more\neffective action planning, and promotes collaborative behavior amongst the AVs.\nThe survey shows that safety in mixed traffic cannot be guaranteed, as we\ncannot control human ego-driven actions if they decide to compete with AV.\nConsequently, this paper advocates for enhanced research into the safe\nincorporation of AVs on public roads.\n",
        "published": "2023",
        "authors": [
            "Ahmed Abdelrahman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.10797v1",
        "title": "Large-Scale Multi-Robot Coverage Path Planning via Local Search",
        "abstract": "  We study graph-based Multi-Robot Coverage Path Planning (MCPP) that aims to\ncompute coverage paths for multiple robots to cover all vertices of a given 2D\ngrid terrain graph $G$. Existing graph-based MCPP algorithms first compute a\ntree cover on $G$ -- a forest of multiple trees that cover all vertices -- and\nthen employ the Spanning Tree Coverage (STC) paradigm to generate coverage\npaths on the decomposed graph $D$ of the terrain graph $G$ by circumnavigating\nthe edges of the computed trees, aiming to optimize the makespan (i.e., the\nmaximum coverage path cost among all robots). In this paper, we take a\ndifferent approach by exploring how to systematically search for good coverage\npaths directly on $D$. We introduce a new algorithmic framework, called\nLS-MCPP, which leverages a local search to operate directly on $D$. We propose\na novel standalone paradigm, Extended-STC (ESTC), that extends STC to achieve\ncomplete coverage for MCPP on any decomposed graphs, even those resulting from\nincomplete terrain graphs. Furthermore, we demonstrate how to integrate ESTC\nwith three novel types of neighborhood operators into our framework to\neffectively guide its search process. Our extensive experiments demonstrate the\neffectiveness of LS-MCPP, consistently improving the initial solution returned\nby two state-of-the-art baseline algorithms that compute suboptimal tree covers\non $G$, with a notable reduction in makespan by up to 35.7\\% and 30.3\\%,\nrespectively. Moreover, LS-MCPP consistently matches or surpasses the results\nof optimal tree cover computation, achieving these outcomes with orders of\nmagnitude faster runtime, thereby showcasing its significant benefits for\nlarge-scale real-world coverage tasks.\n",
        "published": "2023",
        "authors": [
            "Jingtao Tang",
            "Hang Ma"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.10887v1",
        "title": "On Computing Makespan-Optimal Solutions for Generalized Sliding-Tile\n  Puzzles",
        "abstract": "  In the $15$-puzzle game, $15$ labeled square tiles are reconfigured on a\n$4\\times 4$ board through an escort, wherein each (time) step, a single tile\nneighboring it may slide into it, leaving the space previously occupied by the\ntile as the new escort. We study a generalized sliding-tile puzzle (GSTP) in\nwhich (1) there are $1+$ escorts and (2) multiple tiles can move synchronously\nin a single time step. Compared with popular discrete multi-agent/robot motion\nmodels, GSTP provides a more accurate model for a broad array of high-utility\napplications, including warehouse automation and autonomous garage parking, but\nis less studied due to the more involved tile interactions. In this work, we\nanalyze optimal GSTP solution structures, establishing that computing\nmakespan-optimal solutions for GSTP is NP-complete and developing polynomial\ntime algorithms yielding makespans approximating the minimum with expected/high\nprobability constant factors, assuming randomized start and goal\nconfigurations.\n",
        "published": "2023",
        "authors": [
            "Marcus Gozon",
            "Jingjin Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.11207v1",
        "title": "Decentralized traffic management of autonomous drones",
        "abstract": "  Coordination of local and global aerial traffic has become a legal and\ntechnological bottleneck as the number of unmanned vehicles in the common\nairspace continues to grow. To meet this challenge, automation and\ndecentralization of control is an unavoidable requirement. In this paper, we\npresent a solution that enables self-organization of cooperating autonomous\nagents into an effective traffic flow state in which the common aerial\ncoordination task - filled with conflicts - is resolved. Using realistic\nsimulations, we show that our algorithm is safe, efficient, and scalable\nregarding the number of drones and their speed range, while it can also handle\nheterogeneous agents and even pairwise priorities between them. The algorithm\nworks in any sparse or dense traffic scenario in two dimensions and can be made\nincreasingly efficient by a layered flight space structure in three dimensions.\nTo support the feasibility of our solution, we experimentally demonstrate\ncoordinated aerial traffic of 100 autonomous drones within a circular area with\na radius of 125 meters.\n",
        "published": "2023",
        "authors": [
            "Boldizs\u00e1r Bal\u00e1zs",
            "Tam\u00e1s Vicsek",
            "Gerg\u0151 Somorjai",
            "Tam\u00e1s Nepusz",
            "G\u00e1bor V\u00e1s\u00e1rhelyi"
        ]
    }
]