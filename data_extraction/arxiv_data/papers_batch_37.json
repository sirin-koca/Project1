[
    {
        "id": "http://arxiv.org/abs/2104.03739v1",
        "title": "CARRNN: A Continuous Autoregressive Recurrent Neural Network for Deep\n  Representation Learning from Sporadic Temporal Data",
        "abstract": "  Learning temporal patterns from multivariate longitudinal data is challenging\nespecially in cases when data is sporadic, as often seen in, e.g., healthcare\napplications where the data can suffer from irregularity and asynchronicity as\nthe time between consecutive data points can vary across features and samples,\nhindering the application of existing deep learning models that are constructed\nfor complete, evenly spaced data with fixed sequence lengths. In this paper, a\nnovel deep learning-based model is developed for modeling multiple temporal\nfeatures in sporadic data using an integrated deep learning architecture based\non a recurrent neural network (RNN) unit and a continuous-time autoregressive\n(CAR) model. The proposed model, called CARRNN, uses a generalized\ndiscrete-time autoregressive model that is trainable end-to-end using neural\nnetworks modulated by time lags to describe the changes caused by the\nirregularity and asynchronicity. It is applied to multivariate time-series\nregression tasks using data provided for Alzheimer's disease progression\nmodeling and intensive care unit (ICU) mortality rate prediction, where the\nproposed model based on a gated recurrent unit (GRU) achieves the lowest\nprediction errors among the proposed RNN-based models and state-of-the-art\nmethods using GRUs and long short-term memory (LSTM) networks in their\narchitecture.\n",
        "published": "2021",
        "authors": [
            "Mostafa Mehdipour Ghazi",
            "Lauge S\u00f8rensen",
            "S\u00e9bastien Ourselin",
            "Mads Nielsen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.09435v2",
        "title": "Deep learning enables reference-free isotropic super-resolution for\n  volumetric fluorescence microscopy",
        "abstract": "  Volumetric imaging by fluorescence microscopy is often limited by anisotropic\nspatial resolution from inferior axial resolution compared to the lateral\nresolution. To address this problem, here we present a deep-learning-enabled\nunsupervised super-resolution technique that enhances anisotropic images in\nvolumetric fluorescence microscopy. In contrast to the existing deep learning\napproaches that require matched high-resolution target volume images, our\nmethod greatly reduces the effort to put into practice as the training of a\nnetwork requires as little as a single 3D image stack, without a priori\nknowledge of the image formation process, registration of training data, or\nseparate acquisition of target data. This is achieved based on the optimal\ntransport driven cycle-consistent generative adversarial network that learns\nfrom an unpaired matching between high-resolution 2D images in lateral image\nplane and low-resolution 2D images in the other planes. Using fluorescence\nconfocal microscopy and light-sheet microscopy, we demonstrate that the trained\nnetwork not only enhances axial resolution, but also restores suppressed visual\ndetails between the imaging planes and removes imaging artifacts.\n",
        "published": "2021",
        "authors": [
            "Hyoungjun Park",
            "Myeongsu Na",
            "Bumju Kim",
            "Soohyun Park",
            "Ki Hean Kim",
            "Sunghoe Chang",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.10093v2",
        "title": "Class-Incremental Learning with Generative Classifiers",
        "abstract": "  Incrementally training deep neural networks to recognize new classes is a\nchallenging problem. Most existing class-incremental learning methods store\ndata or use generative replay, both of which have drawbacks, while\n'rehearsal-free' alternatives such as parameter regularization or\nbias-correction methods do not consistently achieve high performance. Here, we\nput forward a new strategy for class-incremental learning: generative\nclassification. Rather than directly learning the conditional distribution\np(y|x), our proposal is to learn the joint distribution p(x,y), factorized as\np(x|y)p(y), and to perform classification using Bayes' rule. As a\nproof-of-principle, here we implement this strategy by training a variational\nautoencoder for each class to be learned and by using importance sampling to\nestimate the likelihoods p(x|y). This simple approach performs very well on a\ndiverse set of continual learning benchmarks, outperforming generative replay\nand other existing baselines that do not store data.\n",
        "published": "2021",
        "authors": [
            "Gido M. van de Ven",
            "Zhe Li",
            "Andreas S. Tolias"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.01277v1",
        "title": "Data augmentation and pre-trained networks for extremely low data\n  regimes unsupervised visual inspection",
        "abstract": "  The use of deep features coming from pre-trained neural networks for\nunsupervised anomaly detection purposes has recently gathered momentum in the\ncomputer vision field. In particular, industrial inspection applications can\ntake advantage of such features, as demonstrated by the multiple successes of\nrelated methods on the MVTec Anomaly Detection (MVTec AD) dataset. These\nmethods make use of neural networks pre-trained on auxiliary classification\ntasks such as ImageNet. However, to our knowledge, no comparative study of\nrobustness to the low data regimes between these approaches has been conducted\nyet. For quality inspection applications, the handling of limited sample sizes\nmay be crucial as large quantities of images are not available for small\nseries. In this work, we aim to compare three approaches based on deep\npre-trained features when varying the quantity of available data in MVTec AD:\nKNN, Mahalanobis, and PaDiM. We show that although these methods are mostly\nrobust to small sample sizes, they still can benefit greatly from using data\naugmentation in the original image space, which allows to deal with very small\nproduction runs.\n",
        "published": "2021",
        "authors": [
            "Pierre Gutierrez",
            "Antoine Cordier",
            "Tha\u00efs Caldeira",
            "Th\u00e9ophile Sautory"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.02531v1",
        "title": "CAFLOW: Conditional Autoregressive Flows",
        "abstract": "  We introduce CAFLOW, a new diverse image-to-image translation model that\nsimultaneously leverages the power of auto-regressive modeling and the modeling\nefficiency of conditional normalizing flows. We transform the conditioning\nimage into a sequence of latent encodings using a multi-scale normalizing flow\nand repeat the process for the conditioned image. We model the conditional\ndistribution of the latent encodings by modeling the auto-regressive\ndistributions with an efficient multi-scale normalizing flow, where each\nconditioning factor affects image synthesis at its respective resolution scale.\nOur proposed framework performs well on a range of image-to-image translation\ntasks. It outperforms former designs of conditional flows because of its\nexpressive auto-regressive structure.\n",
        "published": "2021",
        "authors": [
            "Georgios Batzolis",
            "Marcello Carioni",
            "Christian Etmann",
            "Soroosh Afyouni",
            "Zoe Kourtzi",
            "Carola Bibiane Sch\u00f6nlieb"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.05658v2",
        "title": "Conditional COT-GAN for Video Prediction with Kernel Smoothing",
        "abstract": "  Causal Optimal Transport (COT) results from imposing a temporal causality\nconstraint on classic optimal transport problems, which naturally generates a\nnew concept of distances between distributions on path spaces. The first\napplication of the COT theory for sequential learning was given in Xu et al.\n(2020), where COT-GAN was introduced as an adversarial algorithm to train\nimplicit generative models optimized for producing sequential data. Relying on\n(Xu et al., 2020), the contribution of the present paper is twofold. First, we\ndevelop a conditional version of COT-GAN suitable for sequence prediction. This\nmeans that the dataset is now used in order to learn how a sequence will evolve\ngiven the observation of its past evolution. Second, we improve on the\nconvergence results by working with modifications of the empirical measures via\nkernel smoothing due to (Pflug and Pichler (2016)). The resulting kernel\nconditional COT-GAN algorithm is illustrated with an application for video\nprediction.\n",
        "published": "2021",
        "authors": [
            "Tianlin Xu",
            "Beatrice Acciaio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.07115v3",
        "title": "Understanding Latent Correlation-Based Multiview Learning and\n  Self-Supervision: An Identifiability Perspective",
        "abstract": "  Multiple views of data, both naturally acquired (e.g., image and audio) and\nartificially produced (e.g., via adding different noise to data samples), have\nproven useful in enhancing representation learning. Natural views are often\nhandled by multiview analysis tools, e.g., (deep) canonical correlation\nanalysis [(D)CCA], while the artificial ones are frequently used in\nself-supervised learning (SSL) paradigms, e.g., BYOL and Barlow Twins. Both\ntypes of approaches often involve learning neural feature extractors such that\nthe embeddings of data exhibit high cross-view correlations. Although\nintuitive, the effectiveness of correlation-based neural embedding is mostly\nempirically validated.\n  This work aims to understand latent correlation maximization-based deep\nmultiview learning from a latent component identification viewpoint. An\nintuitive generative model of multiview data is adopted, where the views are\ndifferent nonlinear mixtures of shared and private components. Since the shared\ncomponents are view/distortion-invariant, representing the data using such\ncomponents is believed to reveal the identity of the samples effectively and\nrobustly. Under this model, latent correlation maximization is shown to\nguarantee the extraction of the shared components across views (up to certain\nambiguities). In addition, it is further shown that the private information in\neach view can be provably disentangled from the shared using proper\nregularization design. A finite sample analysis, which has been rare in\nnonlinear mixture identifiability study, is also presented. The theoretical\nresults and newly designed regularization are tested on a series of tasks.\n",
        "published": "2021",
        "authors": [
            "Qi Lyu",
            "Xiao Fu",
            "Weiran Wang",
            "Songtao Lu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.08320v2",
        "title": "Self-Supervised Learning with Kernel Dependence Maximization",
        "abstract": "  We approach self-supervised learning of image representations from a\nstatistical dependence perspective, proposing Self-Supervised Learning with the\nHilbert-Schmidt Independence Criterion (SSL-HSIC). SSL-HSIC maximizes\ndependence between representations of transformations of an image and the image\nidentity, while minimizing the kernelized variance of those representations.\nThis framework yields a new understanding of InfoNCE, a variational lower bound\non the mutual information (MI) between different transformations. While the MI\nitself is known to have pathologies which can result in learning meaningless\nrepresentations, its bound is much better behaved: we show that it implicitly\napproximates SSL-HSIC (with a slightly different regularizer). Our approach\nalso gives us insight into BYOL, a negative-free SSL method, since SSL-HSIC\nsimilarly learns local neighborhoods of samples. SSL-HSIC allows us to directly\noptimize statistical dependence in time linear in the batch size, without\nrestrictive data assumptions or indirect mutual information estimators. Trained\nwith or without a target network, SSL-HSIC matches the current state-of-the-art\nfor standard linear evaluation on ImageNet, semi-supervised learning and\ntransfer to other classification and vision tasks such as semantic\nsegmentation, depth estimation and object recognition. Code is available at\nhttps://github.com/deepmind/ssl_hsic .\n",
        "published": "2021",
        "authors": [
            "Yazhe Li",
            "Roman Pogodin",
            "Danica J. Sutherland",
            "Arthur Gretton"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.09756v1",
        "title": "PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python",
        "abstract": "  Machine learning is a general-purpose technology holding promises for many\ninterdisciplinary research problems. However, significant barriers exist in\ncrossing disciplinary boundaries when most machine learning tools are developed\nin different areas separately. We present Pykale - a Python library for\nknowledge-aware machine learning on graphs, images, texts, and videos to enable\nand accelerate interdisciplinary research. We formulate new green machine\nlearning guidelines based on standard software engineering practices and\npropose a novel pipeline-based application programming interface (API). PyKale\nfocuses on leveraging knowledge from multiple sources for accurate and\ninterpretable prediction, thus supporting multimodal learning and transfer\nlearning (particularly domain adaptation) with latest deep learning and\ndimensionality reduction models. We build PyKale on PyTorch and leverage the\nrich PyTorch ecosystem. Our pipeline-based API design enforces standardization\nand minimalism, embracing green machine learning concepts via reducing\nrepetitions and redundancy, reusing existing resources, and recycling learning\nmodels across areas. We demonstrate its interdisciplinary nature via examples\nin bioinformatics, knowledge graph, image/video recognition, and medical\nimaging.\n",
        "published": "2021",
        "authors": [
            "Haiping Lu",
            "Xianyuan Liu",
            "Robert Turner",
            "Peizhen Bai",
            "Raivo E Koot",
            "Shuo Zhou",
            "Mustafa Chasmai",
            "Lawrence Schobs"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.09946v2",
        "title": "Universum GANs: Improving GANs through contradictions",
        "abstract": "  Limited availability of labeled-data makes any supervised learning problem\nchallenging. Alternative learning settings like semi-supervised and universum\nlearning alleviate the dependency on labeled data, but still require a large\namount of unlabeled data, which may be unavailable or expensive to acquire.\nGAN-based data generation methods have recently shown promise by generating\nsynthetic samples to improve learning. However, most existing GAN based\napproaches either provide poor discriminator performance under limited labeled\ndata settings; or results in low quality generated data. In this paper, we\npropose a Universum GAN game which provides improved discriminator accuracy\nunder limited data settings, while generating high quality realistic data. We\nfurther propose an evolving discriminator loss which improves its convergence\nand generalization performance. We derive the theoretical guarantees and\nprovide empirical results in support of our approach.\n",
        "published": "2021",
        "authors": [
            "Sauptik Dhar",
            "Javad Heydari",
            "Samarth Tripathi",
            "Unmesh Kurup",
            "Mohak Shah"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.12871v1",
        "title": "DCoM: A Deep Column Mapper for Semantic Data Type Detection",
        "abstract": "  Detection of semantic data types is a very crucial task in data science for\nautomated data cleaning, schema matching, data discovery, semantic data type\nnormalization and sensitive data identification. Existing methods include\nregular expression-based or dictionary lookup-based methods that are not robust\nto dirty as well unseen data and are limited to a very less number of semantic\ndata types to predict. Existing Machine Learning methods extract large number\nof engineered features from data and build logistic regression, random forest\nor feedforward neural network for this purpose. In this paper, we introduce\nDCoM, a collection of multi-input NLP-based deep neural networks to detect\nsemantic data types where instead of extracting large number of features from\nthe data, we feed the raw values of columns (or instances) to the model as\ntexts. We train DCoM on 686,765 data columns extracted from VizNet corpus with\n78 different semantic data types. DCoM outperforms other contemporary results\nwith a quite significant margin on the same dataset.\n",
        "published": "2021",
        "authors": [
            "Subhadip Maji",
            "Swapna Sourav Rout",
            "Sudeep Choudhary"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.13880v1",
        "title": "Self-paced Principal Component Analysis",
        "abstract": "  Principal Component Analysis (PCA) has been widely used for dimensionality\nreduction and feature extraction. Robust PCA (RPCA), under different robust\ndistance metrics, such as l1-norm and l2, p-norm, can deal with noise or\noutliers to some extent. However, real-world data may display structures that\ncan not be fully captured by these simple functions. In addition, existing\nmethods treat complex and simple samples equally. By contrast, a learning\npattern typically adopted by human beings is to learn from simple to complex\nand less to more. Based on this principle, we propose a novel method called\nSelf-paced PCA (SPCA) to further reduce the effect of noise and outliers.\nNotably, the complexity of each sample is calculated at the beginning of each\niteration in order to integrate samples from simple to more complex into\ntraining. Based on an alternating optimization, SPCA finds an optimal\nprojection matrix and filters out outliers iteratively. Theoretical analysis is\npresented to show the rationality of SPCA. Extensive experiments on popular\ndata sets demonstrate that the proposed method can improve the state of-the-art\nresults considerably.\n",
        "published": "2021",
        "authors": [
            "Zhao Kang",
            "Hongfei Liu",
            "Jiangxin Li",
            "Xiaofeng Zhu",
            "Ling Tian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.05839v1",
        "title": "Logit Attenuating Weight Normalization",
        "abstract": "  Over-parameterized deep networks trained using gradient-based optimizers are\na popular choice for solving classification and ranking problems. Without\nappropriately tuned $\\ell_2$ regularization or weight decay, such networks have\nthe tendency to make output scores (logits) and network weights large, causing\ntraining loss to become too small and the network to lose its adaptivity\n(ability to move around) in the parameter space. Although regularization is\ntypically understood from an overfitting perspective, we highlight its role in\nmaking the network more adaptive and enabling it to escape more easily from\nweights that generalize poorly. To provide such a capability, we propose a\nmethod called Logit Attenuating Weight Normalization (LAWN), that can be\nstacked onto any gradient-based optimizer. LAWN controls the logits by\nconstraining the weight norms of layers in the final homogeneous sub-network.\nEmpirically, we show that the resulting LAWN variant of the optimizer makes a\ndeep network more adaptive to finding minimas with superior generalization\nperformance on large-scale image classification and recommender systems. While\nLAWN is particularly impressive in improving Adam, it greatly improves all\noptimizers when used with large batch sizes\n",
        "published": "2021",
        "authors": [
            "Aman Gupta",
            "Rohan Ramanath",
            "Jun Shi",
            "Anika Ramachandran",
            "Sirou Zhou",
            "Mingzhou Zhou",
            "S. Sathiya Keerthi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.08810v2",
        "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
        "abstract": "  Convolutional neural networks (CNNs) have so far been the de-facto model for\nvisual data. Recent work has shown that (Vision) Transformer models (ViT) can\nachieve comparable or even superior performance on image classification tasks.\nThis raises a central question: how are Vision Transformers solving these\ntasks? Are they acting like convolutional networks, or learning entirely\ndifferent visual representations? Analyzing the internal representation\nstructure of ViTs and CNNs on image classification benchmarks, we find striking\ndifferences between the two architectures, such as ViT having more uniform\nrepresentations across all layers. We explore how these differences arise,\nfinding crucial roles played by self-attention, which enables early aggregation\nof global information, and ViT residual connections, which strongly propagate\nfeatures from lower to higher layers. We study the ramifications for spatial\nlocalization, demonstrating ViTs successfully preserve input spatial\ninformation, with noticeable effects from different classification methods.\nFinally, we study the effect of (pretraining) dataset scale on intermediate\nfeatures and transfer learning, and conclude with a discussion on connections\nto new architectures such as the MLP-Mixer.\n",
        "published": "2021",
        "authors": [
            "Maithra Raghu",
            "Thomas Unterthiner",
            "Simon Kornblith",
            "Chiyuan Zhang",
            "Alexey Dosovitskiy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.13093v1",
        "title": "Investigating Vulnerabilities of Deep Neural Policies",
        "abstract": "  Reinforcement learning policies based on deep neural networks are vulnerable\nto imperceptible adversarial perturbations to their inputs, in much the same\nway as neural network image classifiers. Recent work has proposed several\nmethods to improve the robustness of deep reinforcement learning agents to\nadversarial perturbations based on training in the presence of these\nimperceptible perturbations (i.e. adversarial training). In this paper, we\nstudy the effects of adversarial training on the neural policy learned by the\nagent. In particular, we follow two distinct parallel approaches to investigate\nthe outcomes of adversarial training on deep neural policies based on\nworst-case distributional shift and feature sensitivity. For the first\napproach, we compare the Fourier spectrum of minimal perturbations computed for\nboth adversarially trained and vanilla trained neural policies. Via experiments\nin the OpenAI Atari environments we show that minimal perturbations computed\nfor adversarially trained policies are more focused on lower frequencies in the\nFourier domain, indicating a higher sensitivity of these policies to low\nfrequency perturbations. For the second approach, we propose a novel method to\nmeasure the feature sensitivities of deep neural policies and we compare these\nfeature sensitivity differences in state-of-the-art adversarially trained deep\nneural policies and vanilla trained deep neural policies. We believe our\nresults can be an initial step towards understanding the relationship between\nadversarial training and different notions of robustness for neural policies.\n",
        "published": "2021",
        "authors": [
            "Ezgi Korkmaz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.02095v1",
        "title": "Exploring the Limits of Large Scale Pre-training",
        "abstract": "  Recent developments in large-scale machine learning suggest that by scaling\nup data, model size and training time properly, one might observe that\nimprovements in pre-training would transfer favorably to most downstream tasks.\nIn this work, we systematically study this phenomena and establish that, as we\nincrease the upstream accuracy, the performance of downstream tasks saturates.\nIn particular, we investigate more than 4800 experiments on Vision\nTransformers, MLP-Mixers and ResNets with number of parameters ranging from ten\nmillion to ten billion, trained on the largest scale of available image data\n(JFT, ImageNet21K) and evaluated on more than 20 downstream image recognition\ntasks. We propose a model for downstream performance that reflects the\nsaturation phenomena and captures the nonlinear relationship in performance of\nupstream and downstream tasks. Delving deeper to understand the reasons that\ngive rise to these phenomena, we show that the saturation behavior we observe\nis closely related to the way that representations evolve through the layers of\nthe models. We showcase an even more extreme scenario where performance on\nupstream and downstream are at odds with each other. That is, to have a better\ndownstream performance, we need to hurt upstream accuracy.\n",
        "published": "2021",
        "authors": [
            "Samira Abnar",
            "Mostafa Dehghani",
            "Behnam Neyshabur",
            "Hanie Sedghi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.06850v4",
        "title": "Boosting the Certified Robustness of L-infinity Distance Nets",
        "abstract": "  Recently, Zhang et al. (2021) developed a new neural network architecture\nbased on $\\ell_\\infty$-distance functions, which naturally possesses certified\n$\\ell_\\infty$ robustness by its construction. Despite the novel design and\ntheoretical foundation, so far the model only achieved comparable performance\nto conventional networks. In this paper, we make the following two\ncontributions: $\\mathrm{(i)}$ We demonstrate that $\\ell_\\infty$-distance nets\nenjoy a fundamental advantage in certified robustness over conventional\nnetworks (under typical certification approaches); $\\mathrm{(ii)}$ With an\nimproved training process we are able to significantly boost the certified\naccuracy of $\\ell_\\infty$-distance nets. Our training approach largely\nalleviates the optimization problem that arose in the previous training scheme,\nin particular, the unexpected large Lipschitz constant due to the use of a\ncrucial trick called $\\ell_p$-relaxation. The core of our training approach is\na novel objective function that combines scaled cross-entropy loss and clipped\nhinge loss with a decaying mixing coefficient. Experiments show that using the\nproposed training strategy, the certified accuracy of $\\ell_\\infty$-distance\nnet can be dramatically improved from 33.30% to 40.06% on CIFAR-10\n($\\epsilon=8/255$), meanwhile outperforming other approaches in this area by a\nlarge margin. Our results clearly demonstrate the effectiveness and potential\nof $\\ell_\\infty$-distance net for certified robustness. Codes are available at\nhttps://github.com/zbh2047/L_inf-dist-net-v2.\n",
        "published": "2021",
        "authors": [
            "Bohang Zhang",
            "Du Jiang",
            "Di He",
            "Liwei Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.13100v1",
        "title": "Parameter Prediction for Unseen Deep Architectures",
        "abstract": "  Deep learning has been successful in automating the design of features in\nmachine learning pipelines. However, the algorithms optimizing neural network\nparameters remain largely hand-designed and computationally inefficient. We\nstudy if we can use deep learning to directly predict these parameters by\nexploiting the past knowledge of training other networks. We introduce a\nlarge-scale dataset of diverse computational graphs of neural architectures -\nDeepNets-1M - and use it to explore parameter prediction on CIFAR-10 and\nImageNet. By leveraging advances in graph neural networks, we propose a\nhypernetwork that can predict performant parameters in a single forward pass\ntaking a fraction of a second, even on a CPU. The proposed model achieves\nsurprisingly good performance on unseen and diverse networks. For example, it\nis able to predict all 24 million parameters of a ResNet-50 achieving a 60%\naccuracy on CIFAR-10. On ImageNet, top-5 accuracy of some of our networks\napproaches 50%. Our task along with the model and results can potentially lead\nto a new, more computationally efficient paradigm of training networks. Our\nmodel also learns a strong representation of neural architectures enabling\ntheir analysis.\n",
        "published": "2021",
        "authors": [
            "Boris Knyazev",
            "Michal Drozdzal",
            "Graham W. Taylor",
            "Adriana Romero-Soriano"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.15232v1",
        "title": "Guided Evolution for Neural Architecture Search",
        "abstract": "  Neural Architecture Search (NAS) methods have been successfully applied to\nimage tasks with excellent results. However, NAS methods are often complex and\ntend to converge to local minima as soon as generated architectures seem to\nyield good results. In this paper, we propose G-EA, a novel approach for guided\nevolutionary NAS. The rationale behind G-EA, is to explore the search space by\ngenerating and evaluating several architectures in each generation at\ninitialization stage using a zero-proxy estimator, where only the\nhighest-scoring network is trained and kept for the next generation. This\nevaluation at initialization stage allows continuous extraction of knowledge\nfrom the search space without increasing computation, thus allowing the search\nto be efficiently guided. Moreover, G-EA forces exploitation of the most\nperformant networks by descendant generation while at the same time forcing\nexploration by parent mutation and by favouring younger architectures to the\ndetriment of older ones. Experimental results demonstrate the effectiveness of\nthe proposed method, showing that G-EA achieves state-of-the-art results in\nNAS-Bench-201 search space in CIFAR-10, CIFAR-100 and ImageNet16-120, with mean\naccuracies of 93.98%, 72.12% and 45.94% respectively.\n",
        "published": "2021",
        "authors": [
            "Vasco Lopes",
            "Miguel Santos",
            "Bruno Degardin",
            "Lu\u00eds A. Alexandre"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.00743v4",
        "title": "Towards the Generalization of Contrastive Self-Supervised Learning",
        "abstract": "  Recently, self-supervised learning has attracted great attention, since it\nonly requires unlabeled data for model training. Contrastive learning is one\npopular method for self-supervised learning and has achieved promising\nempirical performance. However, the theoretical understanding of its\ngeneralization ability is still limited. To this end, we define a kind of\n$(\\sigma,\\delta)$-measure to mathematically quantify the data augmentation, and\nthen provide an upper bound of the downstream classification error rate based\non the measure. It reveals that the generalization ability of contrastive\nself-supervised learning is related to three key factors: alignment of positive\nsamples, divergence of class centers, and concentration of augmented data. The\nfirst two factors are properties of learned representations, while the third\none is determined by pre-defined data augmentation. We further investigate two\ncanonical contrastive losses, InfoNCE and cross-correlation, to show how they\nprovably achieve the first two factors. Moreover, we conduct experiments to\nstudy the third factor, and observe a strong correlation between downstream\nperformance and the concentration of augmented data.\n",
        "published": "2021",
        "authors": [
            "Weiran Huang",
            "Mingyang Yi",
            "Xuyang Zhao",
            "Zihao Jiang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.10734v4",
        "title": "Deep Probability Estimation",
        "abstract": "  Reliable probability estimation is of crucial importance in many real-world\napplications where there is inherent (aleatoric) uncertainty.\nProbability-estimation models are trained on observed outcomes (e.g. whether it\nhas rained or not, or whether a patient has died or not), because the\nground-truth probabilities of the events of interest are typically unknown. The\nproblem is therefore analogous to binary classification, with the difference\nthat the objective is to estimate probabilities rather than predicting the\nspecific outcome. This work investigates probability estimation from\nhigh-dimensional data using deep neural networks. There exist several methods\nto improve the probabilities generated by these models but they mostly focus on\nmodel (epistemic) uncertainty. For problems with inherent uncertainty, it is\nchallenging to evaluate performance without access to ground-truth\nprobabilities. To address this, we build a synthetic dataset to study and\ncompare different computable metrics. We evaluate existing methods on the\nsynthetic data as well as on three real-world probability estimation tasks, all\nof which involve inherent uncertainty: precipitation forecasting from radar\nimages, predicting cancer patient survival from histopathology images, and\npredicting car crashes from dashcam videos. We also give a theoretical analysis\nof a model for high-dimensional probability estimation which reproduces several\nof the phenomena evinced in our experiments. Finally, we propose a new method\nfor probability estimation using neural networks, which modifies the training\nprocess to promote output probabilities that are consistent with empirical\nprobabilities computed from the data. The method outperforms existing\napproaches on most metrics on the simulated as well as real-world data.\n",
        "published": "2021",
        "authors": [
            "Sheng Liu",
            "Aakash Kaku",
            "Weicheng Zhu",
            "Matan Leibovich",
            "Sreyas Mohan",
            "Boyang Yu",
            "Haoxiang Huang",
            "Laure Zanna",
            "Narges Razavian",
            "Jonathan Niles-Weed",
            "Carlos Fernandez-Granda"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.05090v2",
        "title": "Extending the WILDS Benchmark for Unsupervised Adaptation",
        "abstract": "  Machine learning systems deployed in the wild are often trained on a source\ndistribution but deployed on a different target distribution. Unlabeled data\ncan be a powerful point of leverage for mitigating these distribution shifts,\nas it is frequently much more available than labeled data and can often be\nobtained from distributions beyond the source distribution as well. However,\nexisting distribution shift benchmarks with unlabeled data do not reflect the\nbreadth of scenarios that arise in real-world applications. In this work, we\npresent the WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS\nbenchmark of distribution shifts to include curated unlabeled data that would\nbe realistically obtainable in deployment. These datasets span a wide range of\napplications (from histology to wildlife conservation), tasks (classification,\nregression, and detection), and modalities (photos, satellite images,\nmicroscope slides, text, molecular graphs). The update maintains consistency\nwith the original WILDS benchmark by using identical labeled training,\nvalidation, and test sets, as well as the evaluation metrics. On these\ndatasets, we systematically benchmark state-of-the-art methods that leverage\nunlabeled data, including domain-invariant, self-training, and self-supervised\nmethods, and show that their success on WILDS is limited. To facilitate method\ndevelopment and evaluation, we provide an open-source package that automates\ndata loading and contains all of the model architectures and methods used in\nthis paper. Code and leaderboards are available at https://wilds.stanford.edu.\n",
        "published": "2021",
        "authors": [
            "Shiori Sagawa",
            "Pang Wei Koh",
            "Tony Lee",
            "Irena Gao",
            "Sang Michael Xie",
            "Kendrick Shen",
            "Ananya Kumar",
            "Weihua Hu",
            "Michihiro Yasunaga",
            "Henrik Marklund",
            "Sara Beery",
            "Etienne David",
            "Ian Stavness",
            "Wei Guo",
            "Jure Leskovec",
            "Kate Saenko",
            "Tatsunori Hashimoto",
            "Sergey Levine",
            "Chelsea Finn",
            "Percy Liang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.09645v1",
        "title": "Local contrastive loss with pseudo-label based self-training for\n  semi-supervised medical image segmentation",
        "abstract": "  Supervised deep learning-based methods yield accurate results for medical\nimage segmentation. However, they require large labeled datasets for this, and\nobtaining them is a laborious task that requires clinical expertise.\nSemi/self-supervised learning-based approaches address this limitation by\nexploiting unlabeled data along with limited annotated data. Recent\nself-supervised learning methods use contrastive loss to learn good global\nlevel representations from unlabeled images and achieve high performance in\nclassification tasks on popular natural image datasets like ImageNet. In\npixel-level prediction tasks such as segmentation, it is crucial to also learn\ngood local level representations along with global representations to achieve\nbetter accuracy. However, the impact of the existing local contrastive\nloss-based methods remains limited for learning good local representations\nbecause similar and dissimilar local regions are defined based on random\naugmentations and spatial proximity; not based on the semantic label of local\nregions due to lack of large-scale expert annotations in the\nsemi/self-supervised setting. In this paper, we propose a local contrastive\nloss to learn good pixel level features useful for segmentation by exploiting\nsemantic label information obtained from pseudo-labels of unlabeled images\nalongside limited annotated images. In particular, we define the proposed loss\nto encourage similar representations for the pixels that have the same\npseudo-label/ label while being dissimilar to the representation of pixels with\ndifferent pseudo-label/label in the dataset. We perform pseudo-label based\nself-training and train the network by jointly optimizing the proposed\ncontrastive loss on both labeled and unlabeled sets and segmentation loss on\nonly the limited labeled set. We evaluated on three public cardiac and prostate\ndatasets, and obtain high segmentation performance.\n",
        "published": "2021",
        "authors": [
            "Krishna Chaitanya",
            "Ertunc Erdil",
            "Neerav Karani",
            "Ender Konukoglu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.10859v2",
        "title": "Visualizing the Diversity of Representations Learned by Bayesian Neural\n  Networks",
        "abstract": "  Explainable Artificial Intelligence (XAI) aims to make learning machines less\nopaque, and offers researchers and practitioners various tools to reveal the\ndecision-making strategies of neural networks. In this work, we investigate how\nXAI methods can be used for exploring and visualizing the diversity of feature\nrepresentations learned by Bayesian Neural Networks (BNNs). Our goal is to\nprovide a global understanding of BNNs by making their decision-making\nstrategies a) visible and tangible through feature visualizations and b)\nquantitatively measurable with a distance measure learned by contrastive\nlearning. Our work provides new insights into the \\emph{posterior} distribution\nin terms of human-understandable feature information with regard to the\nunderlying decision making strategies. The main findings of our work are the\nfollowing: 1) global XAI methods can be applied to explain the diversity of\ndecision-making strategies of BNN instances, 2) Monte Carlo dropout with\ncommonly used Dropout rates exhibit increased diversity in feature\nrepresentations compared to the multimodal posterior approximation of\nMultiSWAG, 3) the diversity of learned feature representations highly\ncorrelates with the uncertainty estimate for the output and 4) the inter-mode\ndiversity of the multimodal posterior decreases as the network width increases,\nwhile the intra mode diversity increases. These findings are consistent with\nthe recent Deep Neural Networks theory, providing additional intuitions about\nwhat the theory implies in terms of humanly understandable concepts.\n",
        "published": "2022",
        "authors": [
            "Dennis Grinwald",
            "Kirill Bykov",
            "Shinichi Nakajima",
            "Marina M. -C. H\u00f6hne"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.12803v3",
        "title": "Generalizing similarity in noisy setups: the DIBS phenomenon",
        "abstract": "  This work uncovers an interplay among data density, noise, and the\ngeneralization ability in similarity learning. We consider Siamese Neural\nNetworks (SNNs), which are the basic form of contrastive learning, and explore\ntwo types of noise that can impact SNNs, Pair Label Noise (PLN) and Single\nLabel Noise (SLN). Our investigation reveals that SNNs exhibit double descent\nbehaviour regardless of the training setup and that it is further exacerbated\nby noise. We demonstrate that the density of data pairs is crucial for\ngeneralization. When SNNs are trained on sparse datasets with the same amount\nof PLN or SLN, they exhibit comparable generalization properties. However, when\nusing dense datasets, PLN cases generalize worse than SLN ones in the\noverparametrized region, leading to a phenomenon we call Density-Induced Break\nof Similarity (DIBS). In this regime, PLN similarity violation becomes\nmacroscopical, corrupting the dataset to the point where complete interpolation\ncannot be achieved, regardless of the number of model parameters. Our analysis\nalso delves into the correspondence between online optimization and offline\ngeneralization in similarity learning. The results show that this equivalence\nfails in the presence of label noise in all the scenarios considered.\n",
        "published": "2022",
        "authors": [
            "Nayara Fonseca",
            "Veronica Guidetti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.04694v2",
        "title": "Align-Deform-Subtract: An Interventional Framework for Explaining Object\n  Differences",
        "abstract": "  Given two object images, how can we explain their differences in terms of the\nunderlying object properties? To address this question, we propose\nAlign-Deform-Subtract (ADS) -- an interventional framework for explaining\nobject differences. By leveraging semantic alignments in image-space as\ncounterfactual interventions on the underlying object properties, ADS\niteratively quantifies and removes differences in object properties. The result\nis a set of \"disentangled\" error measures which explain object differences in\nterms of the underlying properties. Experiments on real and synthetic data\nillustrate the efficacy of the framework.\n",
        "published": "2022",
        "authors": [
            "Cian Eastwood",
            "Li Nanbo",
            "Christopher K. I. Williams"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.08414v1",
        "title": "Unsupervised Semantic Segmentation by Distilling Feature Correspondences",
        "abstract": "  Unsupervised semantic segmentation aims to discover and localize semantically\nmeaningful categories within image corpora without any form of annotation. To\nsolve this task, algorithms must produce features for every pixel that are both\nsemantically meaningful and compact enough to form distinct clusters. Unlike\nprevious works which achieve this with a single end-to-end framework, we\npropose to separate feature learning from cluster compactification.\nEmpirically, we show that current unsupervised feature learning frameworks\nalready generate dense features whose correlations are semantically consistent.\nThis observation motivates us to design STEGO ($\\textbf{S}$elf-supervised\n$\\textbf{T}$ransformer with $\\textbf{E}$nergy-based $\\textbf{G}$raph\n$\\textbf{O}$ptimization), a novel framework that distills unsupervised features\ninto high-quality discrete semantic labels. At the core of STEGO is a novel\ncontrastive loss function that encourages features to form compact clusters\nwhile preserving their relationships across the corpora. STEGO yields a\nsignificant improvement over the prior state of the art, on both the CocoStuff\n($\\textbf{+14 mIoU}$) and Cityscapes ($\\textbf{+9 mIoU}$) semantic segmentation\nchallenges.\n",
        "published": "2022",
        "authors": [
            "Mark Hamilton",
            "Zhoutong Zhang",
            "Bharath Hariharan",
            "Noah Snavely",
            "William T. Freeman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.09301v4",
        "title": "One-Shot Adaptation of GAN in Just One CLIP",
        "abstract": "  There are many recent research efforts to fine-tune a pre-trained generator\nwith a few target images to generate images of a novel domain. Unfortunately,\nthese methods often suffer from overfitting or under-fitting when fine-tuned\nwith a single target image. To address this, here we present a novel\nsingle-shot GAN adaptation method through unified CLIP space manipulations.\nSpecifically, our model employs a two-step training strategy: reference image\nsearch in the source generator using a CLIP-guided latent optimization,\nfollowed by generator fine-tuning with a novel loss function that imposes CLIP\nspace consistency between the source and adapted generators. To further improve\nthe adapted model to produce spatially consistent samples with respect to the\nsource generator, we also propose contrastive regularization for patchwise\nrelationships in the CLIP space. Experimental results show that our model\ngenerates diverse outputs with the target texture and outperforms the baseline\nmodels both qualitatively and quantitatively. Furthermore, we show that our\nCLIP space manipulation strategy allows more effective attribute editing.\n",
        "published": "2022",
        "authors": [
            "Gihyun Kwon",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.13909v1",
        "title": "Concept Embedding Analysis: A Review",
        "abstract": "  Deep neural networks (DNNs) have found their way into many applications with\npotential impact on the safety, security, and fairness of\nhuman-machine-systems. Such require basic understanding and sufficient trust by\nthe users. This motivated the research field of explainable artificial\nintelligence (XAI), i.e. finding methods for opening the \"black-boxes\" DNNs\nrepresent. For the computer vision domain in specific, practical assessment of\nDNNs requires a globally valid association of human interpretable concepts with\ninternals of the model. The research field of concept (embedding) analysis (CA)\ntackles this problem: CA aims to find global, assessable associations of\nhumanly interpretable semantic concepts (e.g., eye, bearded) with internal\nrepresentations of a DNN. This work establishes a general definition of CA and\na taxonomy for CA methods, uniting several ideas from literature. That allows\nto easily position and compare CA approaches. Guided by the defined notions,\nthe current state-of-the-art research regarding CA methods and interesting\napplications are reviewed. More than thirty relevant methods are discussed,\ncompared, and categorized. Finally, for practitioners, a survey of fifteen\ndatasets is provided that have been used for supervised concept analysis. Open\nchallenges and research directions are pointed out at the end.\n",
        "published": "2022",
        "authors": [
            "Gesina Schwalbe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.00943v2",
        "title": "CCLF: A Contrastive-Curiosity-Driven Learning Framework for\n  Sample-Efficient Reinforcement Learning",
        "abstract": "  In reinforcement learning (RL), it is challenging to learn directly from\nhigh-dimensional observations, where data augmentation has recently been shown\nto remedy this via encoding invariances from raw pixels. Nevertheless, we\nempirically find that not all samples are equally important and hence simply\ninjecting more augmented inputs may instead cause instability in Q-learning. In\nthis paper, we approach this problem systematically by developing a\nmodel-agnostic Contrastive-Curiosity-Driven Learning Framework (CCLF), which\ncan fully exploit sample importance and improve learning efficiency in a\nself-supervised manner. Facilitated by the proposed contrastive curiosity, CCLF\nis capable of prioritizing the experience replay, selecting the most\ninformative augmented inputs, and more importantly regularizing the Q-function\nas well as the encoder to concentrate more on under-learned data. Moreover, it\nencourages the agent to explore with a curiosity-based reward. As a result, the\nagent can focus on more informative samples and learn representation\ninvariances more efficiently, with significantly reduced augmented inputs. We\napply CCLF to several base RL algorithms and evaluate on the DeepMind Control\nSuite, Atari, and MiniGrid benchmarks, where our approach demonstrates superior\nsample efficiency and learning performances compared with other\nstate-of-the-art methods.\n",
        "published": "2022",
        "authors": [
            "Chenyu Sun",
            "Hangwei Qian",
            "Chunyan Miao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.10490v1",
        "title": "Mapping Emulation for Knowledge Distillation",
        "abstract": "  This paper formalizes the source-blind knowledge distillation problem that is\nessential to federated learning. A new geometric perspective is presented to\nview such a problem as aligning generated distributions between the teacher and\nstudent. With its guidance, a new architecture MEKD is proposed to emulate the\ninverse mapping through generative adversarial training. Unlike mimicking\nlogits and aligning logit distributions, reconstructing the mapping from\nclassifier-logits has a geometric intuition of decreasing empirical distances,\nand theoretical guarantees using the universal function approximation and\noptimal mass transportation theories. A new algorithm is also proposed to train\nthe student model that reaches the teacher's performance source-blindly. On\nvarious benchmarks, MEKD outperforms existing source-blind KD methods,\nexplainable with ablation studies and visualized results.\n",
        "published": "2022",
        "authors": [
            "Jing Ma",
            "Xiang Xiang",
            "Zihan Zhang",
            "Yuwen Tan",
            "Yiming Wan",
            "Zhigang Zeng",
            "Dacheng Tao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.14546v2",
        "title": "The Missing Invariance Principle Found -- the Reciprocal Twin of\n  Invariant Risk Minimization",
        "abstract": "  Machine learning models often generalize poorly to out-of-distribution (OOD)\ndata as a result of relying on features that are spuriously correlated with the\nlabel during training. Recently, the technique of Invariant Risk Minimization\n(IRM) was proposed to learn predictors that only use invariant features by\nconserving the feature-conditioned label expectation $\\mathbb{E}_e[y|f(x)]$\nacross environments. However, more recent studies have demonstrated that\nIRM-v1, a practical version of IRM, can fail in various settings. Here, we\nidentify a fundamental flaw of IRM formulation that causes the failure. We then\nintroduce a complementary notion of invariance, MRI, based on conserving the\nlabel-conditioned feature expectation $\\mathbb{E}_e[f(x)|y]$, which is free of\nthis flaw. Further, we introduce a simplified, practical version of the MRI\nformulation called MRI-v1. We prove that for general linear problems, MRI-v1\nguarantees invariant predictors given sufficient number of environments. We\nalso empirically demonstrate that MRI-v1 strongly out-performs IRM-v1 and\nconsistently achieves near-optimal OOD generalization in image-based nonlinear\nproblems.\n",
        "published": "2022",
        "authors": [
            "Dongsung Huh",
            "Avinash Baidya"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.15239v2",
        "title": "Conformal Credal Self-Supervised Learning",
        "abstract": "  In semi-supervised learning, the paradigm of self-training refers to the idea\nof learning from pseudo-labels suggested by the learner itself. Across various\ndomains, corresponding methods have proven effective and achieve\nstate-of-the-art performance. However, pseudo-labels typically stem from ad-hoc\nheuristics, relying on the quality of the predictions though without\nguaranteeing their validity. One such method, so-called credal self-supervised\nlearning, maintains pseudo-supervision in the form of sets of (instead of\nsingle) probability distributions over labels, thereby allowing for a flexible\nyet uncertainty-aware labeling. Again, however, there is no justification\nbeyond empirical effectiveness. To address this deficiency, we make use of\nconformal prediction, an approach that comes with guarantees on the validity of\nset-valued predictions. As a result, the construction of credal sets of labels\nis supported by a rigorous theoretical foundation, leading to better calibrated\nand less error-prone supervision for unlabeled data. Along with this, we\npresent effective algorithms for learning from credal self-supervision. An\nempirical study demonstrates excellent calibration properties of the\npseudo-supervision, as well as the competitiveness of our method on several\nbenchmark datasets.\n",
        "published": "2022",
        "authors": [
            "Julian Lienen",
            "Caglar Demir",
            "Eyke H\u00fcllermeier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.00941v2",
        "title": "Improving Diffusion Models for Inverse Problems using Manifold\n  Constraints",
        "abstract": "  Recently, diffusion models have been used to solve various inverse problems\nin an unsupervised manner with appropriate modifications to the sampling\nprocess. However, the current solvers, which recursively apply a reverse\ndiffusion step followed by a projection-based measurement consistency step,\noften produce suboptimal results. By studying the generative sampling path,\nhere we show that current solvers throw the sample path off the data manifold,\nand hence the error accumulates. To address this, we propose an additional\ncorrection term inspired by the manifold constraint, which can be used\nsynergistically with the previous solvers to make the iterations close to the\nmanifold. The proposed manifold constraint is straightforward to implement\nwithin a few lines of code, yet boosts the performance by a surprisingly large\nmargin. With extensive experiments, we show that our method is superior to the\nprevious methods both theoretically and empirically, producing promising\nresults in many applications such as image inpainting, colorization, and\nsparse-view computed tomography. Code available\nhttps://github.com/HJ-harry/MCG_diffusion\n",
        "published": "2022",
        "authors": [
            "Hyungjin Chung",
            "Byeongsu Sim",
            "Dohoon Ryu",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.04779v3",
        "title": "Challenges and Opportunities in Offline Reinforcement Learning from\n  Visual Observations",
        "abstract": "  Offline reinforcement learning has shown great promise in leveraging large\npre-collected datasets for policy learning, allowing agents to forgo\noften-expensive online data collection. However, offline reinforcement learning\nfrom visual observations with continuous action spaces remains under-explored,\nwith a limited understanding of the key challenges in this complex domain. In\nthis paper, we establish simple baselines for continuous control in the visual\ndomain and introduce a suite of benchmarking tasks for offline reinforcement\nlearning from visual observations designed to better represent the data\ndistributions present in real-world offline RL problems and guided by a set of\ndesiderata for offline RL from visual observations, including robustness to\nvisual distractions and visually identifiable changes in dynamics. Using this\nsuite of benchmarking tasks, we show that simple modifications to two popular\nvision-based online reinforcement learning algorithms, DreamerV2 and DrQ-v2,\nsuffice to outperform existing offline RL methods and establish competitive\nbaselines for continuous control in the visual domain. We rigorously evaluate\nthese algorithms and perform an empirical evaluation of the differences between\nstate-of-the-art model-based and model-free offline RL methods for continuous\ncontrol from visual observations. All code and data used in this evaluation are\nopen-sourced to facilitate progress in this domain.\n",
        "published": "2022",
        "authors": [
            "Cong Lu",
            "Philip J. Ball",
            "Tim G. J. Rudner",
            "Jack Parker-Holder",
            "Michael A. Osborne",
            "Yee Whye Teh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.13872v5",
        "title": "When are Post-hoc Conceptual Explanations Identifiable?",
        "abstract": "  Interest in understanding and factorizing learned embedding spaces through\nconceptual explanations is steadily growing. When no human concept labels are\navailable, concept discovery methods search trained embedding spaces for\ninterpretable concepts like object shape or color that can provide post-hoc\nexplanations for decisions. Unlike previous work, we argue that concept\ndiscovery should be identifiable, meaning that a number of known concepts can\nbe provably recovered to guarantee reliability of the explanations. As a\nstarting point, we explicitly make the connection between concept discovery and\nclassical methods like Principal Component Analysis and Independent Component\nAnalysis by showing that they can recover independent concepts under\nnon-Gaussian distributions. For dependent concepts, we propose two novel\napproaches that exploit functional compositionality properties of\nimage-generating processes. Our provably identifiable concept discovery methods\nsubstantially outperform competitors on a battery of experiments including\nhundreds of trained models and dependent concepts, where they exhibit up to 29\n% better alignment with the ground truth. Our results highlight the strict\nconditions under which reliable concept discovery without human labels can be\nguaranteed and provide a formal foundation for the domain. Our code is\navailable online.\n",
        "published": "2022",
        "authors": [
            "Tobias Leemann",
            "Michael Kirchhof",
            "Yao Rong",
            "Enkelejda Kasneci",
            "Gjergji Kasneci"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.14486v6",
        "title": "Beyond neural scaling laws: beating power law scaling via data pruning",
        "abstract": "  Widely observed neural scaling laws, in which error falls off as a power of\nthe training set size, model size, or both, have driven substantial performance\nimprovements in deep learning. However, these improvements through scaling\nalone require considerable costs in compute and energy. Here we focus on the\nscaling of error with dataset size and show how in theory we can break beyond\npower law scaling and potentially even reduce it to exponential scaling instead\nif we have access to a high-quality data pruning metric that ranks the order in\nwhich training examples should be discarded to achieve any pruned dataset size.\nWe then test this improved scaling prediction with pruned dataset size\nempirically, and indeed observe better than power law scaling in practice on\nResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of\nfinding high-quality pruning metrics, we perform the first large-scale\nbenchmarking study of ten different data pruning metrics on ImageNet. We find\nmost existing high performing metrics scale poorly to ImageNet, while the best\nare computationally intensive and require labels for every image. We therefore\ndeveloped a new simple, cheap and scalable self-supervised pruning metric that\ndemonstrates comparable performance to the best supervised metrics. Overall,\nour work suggests that the discovery of good data-pruning metrics may provide a\nviable path forward to substantially improved neural scaling laws, thereby\nreducing the resource costs of modern deep learning.\n",
        "published": "2022",
        "authors": [
            "Ben Sorscher",
            "Robert Geirhos",
            "Shashank Shekhar",
            "Surya Ganguli",
            "Ari S. Morcos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.02036v1",
        "title": "PRoA: A Probabilistic Robustness Assessment against Functional\n  Perturbations",
        "abstract": "  In safety-critical deep learning applications robustness measurement is a\nvital pre-deployment phase. However, existing robustness verification methods\nare not sufficiently practical for deploying machine learning systems in the\nreal world. On the one hand, these methods attempt to claim that no\nperturbations can ``fool'' deep neural networks (DNNs), which may be too\nstringent in practice. On the other hand, existing works rigorously consider\n$L_p$ bounded additive perturbations on the pixel space, although\nperturbations, such as colour shifting and geometric transformations, are more\npractically and frequently occurring in the real world. Thus, from the\npractical standpoint, we present a novel and general {\\it probabilistic\nrobustness assessment method} (PRoA) based on the adaptive concentration, and\nit can measure the robustness of deep learning models against functional\nperturbations. PRoA can provide statistical guarantees on the probabilistic\nrobustness of a model, \\textit{i.e.}, the probability of failure encountered by\nthe trained model after deployment. Our experiments demonstrate the\neffectiveness and flexibility of PRoA in terms of evaluating the probabilistic\nrobustness against a broad range of functional perturbations, and PRoA can\nscale well to various large-scale deep neural networks compared to existing\nstate-of-the-art baselines. For the purpose of reproducibility, we release our\ntool on GitHub: \\url{ https://github.com/TrustAI/PRoA}.\n",
        "published": "2022",
        "authors": [
            "Tianle Zhang",
            "Wenjie Ruan",
            "Jonathan E. Fieldsend"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.06569v2",
        "title": "Benign, Tempered, or Catastrophic: A Taxonomy of Overfitting",
        "abstract": "  The practical success of overparameterized neural networks has motivated the\nrecent scientific study of interpolating methods, which perfectly fit their\ntraining data. Certain interpolating methods, including neural networks, can\nfit noisy training data without catastrophically bad test performance, in\ndefiance of standard intuitions from statistical learning theory. Aiming to\nexplain this, a body of recent work has studied benign overfitting, a\nphenomenon where some interpolating methods approach Bayes optimality, even in\nthe presence of noise. In this work we argue that while benign overfitting has\nbeen instructive and fruitful to study, many real interpolating methods like\nneural networks do not fit benignly: modest noise in the training set causes\nnonzero (but non-infinite) excess risk at test time, implying these models are\nneither benign nor catastrophic but rather fall in an intermediate regime. We\ncall this intermediate regime tempered overfitting, and we initiate its\nsystematic study. We first explore this phenomenon in the context of kernel\n(ridge) regression (KR) by obtaining conditions on the ridge parameter and\nkernel eigenspectrum under which KR exhibits each of the three behaviors. We\nfind that kernels with powerlaw spectra, including Laplace kernels and ReLU\nneural tangent kernels, exhibit tempered overfitting. We then empirically study\ndeep neural networks through the lens of our taxonomy, and find that those\ntrained to interpolation are tempered, while those stopped early are benign. We\nhope our work leads to a more refined understanding of overfitting in modern\nlearning.\n",
        "published": "2022",
        "authors": [
            "Neil Mallinar",
            "James B. Simon",
            "Amirhesam Abedsoltan",
            "Parthe Pandit",
            "Mikhail Belkin",
            "Preetum Nakkiran"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.09944v4",
        "title": "Probable Domain Generalization via Quantile Risk Minimization",
        "abstract": "  Domain generalization (DG) seeks predictors which perform well on unseen test\ndistributions by leveraging data drawn from multiple related training\ndistributions or domains. To achieve this, DG is commonly formulated as an\naverage- or worst-case problem over the set of possible domains. However,\npredictors that perform well on average lack robustness while predictors that\nperform well in the worst case tend to be overly-conservative. To address this,\nwe propose a new probabilistic framework for DG where the goal is to learn\npredictors that perform well with high probability. Our key idea is that\ndistribution shifts seen during training should inform us of probable shifts at\ntest time, which we realize by explicitly relating training and test domains as\ndraws from the same underlying meta-distribution. To achieve probable DG, we\npropose a new optimization problem called Quantile Risk Minimization (QRM). By\nminimizing the $\\alpha$-quantile of predictor's risk distribution over domains,\nQRM seeks predictors that perform well with probability $\\alpha$. To solve QRM\nin practice, we propose the Empirical QRM (EQRM) algorithm and provide: (i) a\ngeneralization bound for EQRM; and (ii) the conditions under which EQRM\nrecovers the causal predictor as $\\alpha \\to 1$. In our experiments, we\nintroduce a more holistic quantile-focused evaluation protocol for DG and\ndemonstrate that EQRM outperforms state-of-the-art baselines on datasets from\nWILDS and DomainBed.\n",
        "published": "2022",
        "authors": [
            "Cian Eastwood",
            "Alexander Robey",
            "Shashank Singh",
            "Julius von K\u00fcgelgen",
            "Hamed Hassani",
            "George J. Pappas",
            "Bernhard Sch\u00f6lkopf"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.10074v2",
        "title": "Semantic uncertainty intervals for disentangled latent spaces",
        "abstract": "  Meaningful uncertainty quantification in computer vision requires reasoning\nabout semantic information -- say, the hair color of the person in a photo or\nthe location of a car on the street. To this end, recent breakthroughs in\ngenerative modeling allow us to represent semantic information in disentangled\nlatent spaces, but providing uncertainties on the semantic latent variables has\nremained challenging. In this work, we provide principled uncertainty intervals\nthat are guaranteed to contain the true semantic factors for any underlying\ngenerative model. The method does the following: (1) it uses quantile\nregression to output a heuristic uncertainty interval for each element in the\nlatent space (2) calibrates these uncertainties such that they contain the true\nvalue of the latent for a new, unseen input. The endpoints of these calibrated\nintervals can then be propagated through the generator to produce interpretable\nuncertainty visualizations for each semantic factor. This technique reliably\ncommunicates semantically meaningful, principled, and instance-adaptive\nuncertainty in inverse problems like image super-resolution and image\ncompletion.\n",
        "published": "2022",
        "authors": [
            "Swami Sankaranarayanan",
            "Anastasios N. Angelopoulos",
            "Stephen Bates",
            "Yaniv Romano",
            "Phillip Isola"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.11146v1",
        "title": "VTrackIt: A Synthetic Self-Driving Dataset with Infrastructure and\n  Pooled Vehicle Information",
        "abstract": "  Artificial intelligence solutions for Autonomous Vehicles (AVs) have been\ndeveloped using publicly available datasets such as Argoverse, ApolloScape,\nLevel5, and NuScenes. One major limitation of these datasets is the absence of\ninfrastructure and/or pooled vehicle information like lane line type, vehicle\nspeed, traffic signs, and intersections. Such information is necessary and not\ncomplementary to eliminating high-risk edge cases. The rapid advancements in\nVehicle-to-Infrastructure and Vehicle-to-Vehicle technologies show promise that\ninfrastructure and pooled vehicle information will soon be accessible in near\nreal-time. Taking a leap in the future, we introduce the first comprehensive\nsynthetic dataset with intelligent infrastructure and pooled vehicle\ninformation for advancing the next generation of AVs, named VTrackIt. We also\nintroduce the first deep learning model (InfraGAN) for trajectory predictions\nthat considers such information. Our experiments with InfraGAN show that the\ncomprehensive information offered by VTrackIt reduces the number of high-risk\nedge cases. The VTrackIt dataset is available upon request under the Creative\nCommons CC BY-NC-SA 4.0 license at http://vtrackit.irda.club.\n",
        "published": "2022",
        "authors": [
            "Mayuresh Savargaonkar",
            "Abdallah Chehade"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.10967v5",
        "title": "The Value of Out-of-Distribution Data",
        "abstract": "  We expect the generalization error to improve with more samples from a\nsimilar task, and to deteriorate with more samples from an out-of-distribution\n(OOD) task. In this work, we show a counter-intuitive phenomenon: the\ngeneralization error of a task can be a non-monotonic function of the number of\nOOD samples. As the number of OOD samples increases, the generalization error\non the target task improves before deteriorating beyond a threshold. In other\nwords, there is value in training on small amounts of OOD data. We use Fisher's\nLinear Discriminant on synthetic datasets and deep networks on computer vision\nbenchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate\nand analyze this phenomenon. In the idealistic setting where we know which\nsamples are OOD, we show that these non-monotonic trends can be exploited using\nan appropriately weighted objective of the target and OOD empirical risk. While\nits practical utility is limited, this does suggest that if we can detect OOD\nsamples, then there may be ways to benefit from them. When we do not know which\nsamples are OOD, we show how a number of go-to strategies such as\ndata-augmentation, hyper-parameter optimization, and pre-training are not\nenough to ensure that the target generalization error does not deteriorate with\nthe number of OOD samples in the dataset.\n",
        "published": "2022",
        "authors": [
            "Ashwin De Silva",
            "Rahul Ramesh",
            "Carey E. Priebe",
            "Pratik Chaudhari",
            "Joshua T. Vogelstein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.14125v3",
        "title": "A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images",
        "abstract": "  Diffusion models are a special type of generative model, capable of\nsynthesising new data from a learnt distribution. We introduce DISPR, a\ndiffusion-based model for solving the inverse problem of three-dimensional (3D)\ncell shape prediction from two-dimensional (2D) single cell microscopy images.\nUsing the 2D microscopy image as a prior, DISPR is conditioned to predict\nrealistic 3D shape reconstructions. To showcase the applicability of DISPR as a\ndata augmentation tool in a feature-based single cell classification task, we\nextract morphological features from the red blood cells grouped into six highly\nimbalanced classes. Adding features from the DISPR predictions to the three\nminority classes improved the macro F1 score from $F1_\\text{macro} = 55.2 \\pm\n4.6\\%$ to $F1_\\text{macro} = 72.2 \\pm 4.9\\%$. We thus demonstrate that\ndiffusion models can be successfully applied to inverse biomedical problems,\nand that they learn to reconstruct 3D shapes with realistic morphological\nfeatures from 2D microscopy images.\n",
        "published": "2022",
        "authors": [
            "Dominik J. E. Waibel",
            "Ernst R\u00f6ell",
            "Bastian Rieck",
            "Raja Giryes",
            "Carsten Marr"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.14362v2",
        "title": "AutoWS-Bench-101: Benchmarking Automated Weak Supervision with 100\n  Labels",
        "abstract": "  Weak supervision (WS) is a powerful method to build labeled datasets for\ntraining supervised models in the face of little-to-no labeled data. It\nreplaces hand-labeling data with aggregating multiple noisy-but-cheap label\nestimates expressed by labeling functions (LFs). While it has been used\nsuccessfully in many domains, weak supervision's application scope is limited\nby the difficulty of constructing labeling functions for domains with complex\nor high-dimensional features. To address this, a handful of methods have\nproposed automating the LF design process using a small set of ground truth\nlabels. In this work, we introduce AutoWS-Bench-101: a framework for evaluating\nautomated WS (AutoWS) techniques in challenging WS settings -- a set of diverse\napplication domains on which it has been previously difficult or impossible to\napply traditional WS techniques. While AutoWS is a promising direction toward\nexpanding the application-scope of WS, the emergence of powerful methods such\nas zero-shot foundation models reveals the need to understand how AutoWS\ntechniques compare or cooperate with modern zero-shot or few-shot learners.\nThis informs the central question of AutoWS-Bench-101: given an initial set of\n100 labels for each task, we ask whether a practitioner should use an AutoWS\nmethod to generate additional labels or use some simpler baseline, such as\nzero-shot predictions from a foundation model or supervised learning. We\nobserve that in many settings, it is necessary for AutoWS methods to\nincorporate signal from foundation models if they are to outperform simple\nfew-shot baselines, and AutoWS-Bench-101 promotes future research in this\ndirection. We conclude with a thorough ablation study of AutoWS methods.\n",
        "published": "2022",
        "authors": [
            "Nicholas Roberts",
            "Xintong Li",
            "Tzu-Heng Huang",
            "Dyah Adila",
            "Spencer Schoenberg",
            "Cheng-Yu Liu",
            "Lauren Pick",
            "Haotian Ma",
            "Aws Albarghouthi",
            "Frederic Sala"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.09090v2",
        "title": "Uncertainty-aware Efficient Subgraph Isomorphism using Graph Topology",
        "abstract": "  Subgraph isomorphism or subgraph matching is generally considered as an\nNP-complete problem, made more complex in practical applications where the edge\nweights take real values and are subject to measurement noise and possible\nanomalies. To the best of our knowledge, almost all subgraph matching methods\nutilize node labels to perform node-node matching. In the absence of such\nlabels (in applications such as image matching and map matching among others),\nthese subgraph matching methods do not work. We propose a method for\nidentifying the node correspondence between a subgraph and a full graph in the\ninexact case without node labels in two steps - (a) extract the minimal unique\ntopology preserving subset from the subgraph and find its feasible matching in\nthe full graph, and (b) implement a consensus-based algorithm to expand the\nmatched node set by pairing unique paths based on boundary commutativity. Going\nbeyond the existing subgraph matching approaches, the proposed method is shown\nto have realistically sub-linear computational efficiency, robustness to random\nmeasurement noise, and good statistical properties. Our method is also readily\napplicable to the exact matching case without loss of generality. To\ndemonstrate the effectiveness of the proposed method, a simulation and a case\nstudy is performed on the Erdos-Renyi random graphs and the image-based affine\ncovariant features dataset respectively.\n",
        "published": "2022",
        "authors": [
            "Arpan Kusari",
            "Wenbo Sun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.13575v1",
        "title": "Efficient Non-Parametric Optimizer Search for Diverse Tasks",
        "abstract": "  Efficient and automated design of optimizers plays a crucial role in\nfull-stack AutoML systems. However, prior methods in optimizer search are often\nlimited by their scalability, generability, or sample efficiency. With the goal\nof democratizing research and application of optimizer search, we present the\nfirst efficient, scalable and generalizable framework that can directly search\non the tasks of interest. We first observe that optimizer updates are\nfundamentally mathematical expressions applied to the gradient. Inspired by the\ninnate tree structure of the underlying math expressions, we re-arrange the\nspace of optimizers into a super-tree, where each path encodes an optimizer.\nThis way, optimizer search can be naturally formulated as a path-finding\nproblem, allowing a variety of well-established tree traversal methods to be\nused as the search algorithm. We adopt an adaptation of the Monte Carlo method\nto tree search, equipped with rejection sampling and equivalent-form detection\nthat leverage the characteristics of optimizer update rules to further boost\nthe sample efficiency. We provide a diverse set of tasks to benchmark our\nalgorithm and demonstrate that, with only 128 evaluations, the proposed\nframework can discover optimizers that surpass both human-designed counterparts\nand prior optimizer search methods.\n",
        "published": "2022",
        "authors": [
            "Ruochen Wang",
            "Yuanhao Xiong",
            "Minhao Cheng",
            "Cho-Jui Hsieh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.14440v3",
        "title": "GeONet: a neural operator for learning the Wasserstein geodesic",
        "abstract": "  Optimal transport (OT) offers a versatile framework to compare complex data\ndistributions in a geometrically meaningful way. Traditional methods for\ncomputing the Wasserstein distance and geodesic between probability measures\nrequire mesh-dependent domain discretization and suffer from the\ncurse-of-dimensionality. We present GeONet, a mesh-invariant deep neural\noperator network that learns the non-linear mapping from the input pair of\ninitial and terminal distributions to the Wasserstein geodesic connecting the\ntwo endpoint distributions. In the offline training stage, GeONet learns the\nsaddle point optimality conditions for the dynamic formulation of the OT\nproblem in the primal and dual spaces that are characterized by a coupled PDE\nsystem. The subsequent inference stage is instantaneous and can be deployed for\nreal-time predictions in the online learning setting. We demonstrate that\nGeONet achieves comparable testing accuracy to the standard OT solvers on\nsimulation examples and the MNIST dataset with considerably reduced\ninference-stage computational cost by orders of magnitude.\n",
        "published": "2022",
        "authors": [
            "Andrew Gracyk",
            "Xiaohui Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.14687v3",
        "title": "Diffusion Posterior Sampling for General Noisy Inverse Problems",
        "abstract": "  Diffusion models have been recently studied as powerful generative inverse\nproblem solvers, owing to their high quality reconstructions and the ease of\ncombining existing iterative solvers. However, most works focus on solving\nsimple linear inverse problems in noiseless settings, which significantly\nunder-represents the complexity of real-world problems. In this work, we extend\ndiffusion solvers to efficiently handle general noisy (non)linear inverse\nproblems via approximation of the posterior sampling. Interestingly, the\nresulting posterior sampling scheme is a blended version of diffusion sampling\nwith the manifold constrained gradient without a strict measurement consistency\nprojection step, yielding a more desirable generative path in noisy settings\ncompared to the previous studies. Our method demonstrates that diffusion models\ncan incorporate various measurement noise statistics such as Gaussian and\nPoisson, and also efficiently handle noisy nonlinear inverse problems such as\nFourier phase retrieval and non-uniform deblurring. Code available at\nhttps://github.com/DPS2022/diffusion-posterior-sampling\n",
        "published": "2022",
        "authors": [
            "Hyungjin Chung",
            "Jeongsol Kim",
            "Michael T. Mccann",
            "Marc L. Klasky",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.15264v2",
        "title": "Diffusion-based Image Translation using Disentangled Style and Content\n  Representation",
        "abstract": "  Diffusion-based image translation guided by semantic texts or a single target\nimage has enabled flexible style transfer which is not limited to the specific\ndomains. Unfortunately, due to the stochastic nature of diffusion models, it is\noften difficult to maintain the original content of the image during the\nreverse diffusion. To address this, here we present a novel diffusion-based\nunsupervised image translation method using disentangled style and content\nrepresentation.\n  Specifically, inspired by the splicing Vision Transformer, we extract\nintermediate keys of multihead self attention layer from ViT model and used\nthem as the content preservation loss. Then, an image guided style transfer is\nperformed by matching the [CLS] classification token from the denoised samples\nand target image, whereas additional CLIP loss is used for the text-driven\nstyle transfer. To further accelerate the semantic change during the reverse\ndiffusion, we also propose a novel semantic divergence loss and resampling\nstrategy. Our experimental results show that the proposed method outperforms\nstate-of-the-art baseline models in both text-guided and image-guided\ntranslation tasks.\n",
        "published": "2022",
        "authors": [
            "Gihyun Kwon",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.15398v1",
        "title": "Evaluation of importance estimators in deep learning classifiers for\n  Computed Tomography",
        "abstract": "  Deep learning has shown superb performance in detecting objects and\nclassifying images, ensuring a great promise for analyzing medical imaging.\nTranslating the success of deep learning to medical imaging, in which doctors\nneed to understand the underlying process, requires the capability to interpret\nand explain the prediction of neural networks. Interpretability of deep neural\nnetworks often relies on estimating the importance of input features (e.g.,\npixels) with respect to the outcome (e.g., class probability). However, a\nnumber of importance estimators (also known as saliency maps) have been\ndeveloped and it is unclear which ones are more relevant for medical imaging\napplications. In the present work, we investigated the performance of several\nimportance estimators in explaining the classification of computed tomography\n(CT) images by a convolutional deep network, using three distinct evaluation\nmetrics. First, the model-centric fidelity measures a decrease in the model\naccuracy when certain inputs are perturbed. Second, concordance between\nimportance scores and the expert-defined segmentation masks is measured on a\npixel level by a receiver operating characteristic (ROC) curves. Third, we\nmeasure a region-wise overlap between a XRAI-based map and the segmentation\nmask by Dice Similarity Coefficients (DSC). Overall, two versions of SmoothGrad\ntopped the fidelity and ROC rankings, whereas both Integrated Gradients and\nSmoothGrad excelled in DSC evaluation. Interestingly, there was a critical\ndiscrepancy between model-centric (fidelity) and human-centric (ROC and DSC)\nevaluation. Expert expectation and intuition embedded in segmentation maps does\nnot necessarily align with how the model arrived at its prediction.\nUnderstanding this difference in interpretability would help harnessing the\npower of deep learning in medicine.\n",
        "published": "2022",
        "authors": [
            "Lennart Brocki",
            "Wistan Marchadour",
            "Jonas Maison",
            "Bogdan Badic",
            "Panagiotis Papadimitroulas",
            "Mathieu Hatt",
            "Franck Vermet",
            "Neo Christopher Chung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.01964v2",
        "title": "The Calibration Generalization Gap",
        "abstract": "  Calibration is a fundamental property of a good predictive model: it requires\nthat the model predicts correctly in proportion to its confidence. Modern\nneural networks, however, provide no strong guarantees on their calibration --\nand can be either poorly calibrated or well-calibrated depending on the\nsetting. It is currently unclear which factors contribute to good calibration\n(architecture, data augmentation, overparameterization, etc), though various\nclaims exist in the literature.\n  We propose a systematic way to study the calibration error: by decomposing it\ninto (1) calibration error on the train set, and (2) the calibration\ngeneralization gap. This mirrors the fundamental decomposition of\ngeneralization. We then investigate each of these terms, and give empirical\nevidence that (1) DNNs are typically always calibrated on their train set, and\n(2) the calibration generalization gap is upper-bounded by the standard\ngeneralization gap. Taken together, this implies that models with small\ngeneralization gap (|Test Error - Train Error|) are well-calibrated. This\nperspective unifies many results in the literature, and suggests that\ninterventions which reduce the generalization gap (such as adding data, using\nheavy augmentation, or smaller model size) also improve calibration. We thus\nhope our initial study lays the groundwork for a more systematic and\ncomprehensive understanding of the relation between calibration,\ngeneralization, and optimization.\n",
        "published": "2022",
        "authors": [
            "A. Michael Carrell",
            "Neil Mallinar",
            "James Lucas",
            "Preetum Nakkiran"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.02984v1",
        "title": "The Lie Derivative for Measuring Learned Equivariance",
        "abstract": "  Equivariance guarantees that a model's predictions capture key symmetries in\ndata. When an image is translated or rotated, an equivariant model's\nrepresentation of that image will translate or rotate accordingly. The success\nof convolutional neural networks has historically been tied to translation\nequivariance directly encoded in their architecture. The rising success of\nvision transformers, which have no explicit architectural bias towards\nequivariance, challenges this narrative and suggests that augmentations and\ntraining data might also play a significant role in their performance. In order\nto better understand the role of equivariance in recent vision models, we\nintroduce the Lie derivative, a method for measuring equivariance with strong\nmathematical foundations and minimal hyperparameters. Using the Lie derivative,\nwe study the equivariance properties of hundreds of pretrained models, spanning\nCNNs, transformers, and Mixer architectures. The scale of our analysis allows\nus to separate the impact of architecture from other factors like model size or\ntraining method. Surprisingly, we find that many violations of equivariance can\nbe linked to spatial aliasing in ubiquitous network layers, such as pointwise\nnon-linearities, and that as models get larger and more accurate they tend to\ndisplay more equivariance, regardless of architecture. For example,\ntransformers can be more equivariant than convolutional neural networks after\ntraining.\n",
        "published": "2022",
        "authors": [
            "Nate Gruver",
            "Marc Finzi",
            "Micah Goldblum",
            "Andrew Gordon Wilson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.09298v1",
        "title": "What Makes Convolutional Models Great on Long Sequence Modeling?",
        "abstract": "  Convolutional models have been widely used in multiple domains. However, most\nexisting models only use local convolution, making the model unable to handle\nlong-range dependency efficiently. Attention overcomes this problem by\naggregating global information but also makes the computational complexity\nquadratic to the sequence length. Recently, Gu et al. [2021] proposed a model\ncalled S4 inspired by the state space model. S4 can be efficiently implemented\nas a global convolutional model whose kernel size equals the input sequence\nlength. S4 can model much longer sequences than Transformers and achieve\nsignificant gains over SoTA on several long-range tasks. Despite its empirical\nsuccess, S4 is involved. It requires sophisticated parameterization and\ninitialization schemes. As a result, S4 is less intuitive and hard to use. Here\nwe aim to demystify S4 and extract basic principles that contribute to the\nsuccess of S4 as a global convolutional model. We focus on the structure of the\nconvolution kernel and identify two critical but intuitive principles enjoyed\nby S4 that are sufficient to make up an effective global convolutional model:\n1) The parameterization of the convolutional kernel needs to be efficient in\nthe sense that the number of parameters should scale sub-linearly with sequence\nlength. 2) The kernel needs to satisfy a decaying structure that the weights\nfor convolving with closer neighbors are larger than the more distant ones.\nBased on the two principles, we propose a simple yet effective convolutional\nmodel called Structured Global Convolution (SGConv). SGConv exhibits strong\nempirical performance over several tasks: 1) With faster speed, SGConv\nsurpasses S4 on Long Range Arena and Speech Command datasets. 2) When plugging\nSGConv into standard language and vision models, it shows the potential to\nimprove both efficiency and performance.\n",
        "published": "2022",
        "authors": [
            "Yuhong Li",
            "Tianle Cai",
            "Yi Zhang",
            "Deming Chen",
            "Debadeepta Dey"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.17426v3",
        "title": "Trade-off Between Efficiency and Consistency for Removal-based\n  Explanations",
        "abstract": "  In the current landscape of explanation methodologies, most predominant\napproaches, such as SHAP and LIME, employ removal-based techniques to evaluate\nthe impact of individual features by simulating various scenarios with specific\nfeatures omitted. Nonetheless, these methods primarily emphasize efficiency in\nthe original context, often resulting in general inconsistencies. In this\npaper, we demonstrate that such inconsistency is an inherent aspect of these\napproaches by establishing the Impossible Trinity Theorem, which posits that\ninterpretability, efficiency, and consistency cannot hold simultaneously.\nRecognizing that the attainment of an ideal explanation remains elusive, we\npropose the utilization of interpretation error as a metric to gauge\ninefficiencies and inconsistencies. To this end, we present two novel\nalgorithms founded on the standard polynomial basis, aimed at minimizing\ninterpretation error. Our empirical findings indicate that the proposed methods\nachieve a substantial reduction in interpretation error, up to 31.8 times lower\nwhen compared to alternative techniques. Code is available at\nhttps://github.com/trusty-ai/efficient-consistent-explanations.\n",
        "published": "2022",
        "authors": [
            "Yifan Zhang",
            "Haowei He",
            "Zhiquan Tan",
            "Yang Yuan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.01842v3",
        "title": "Construction of Hierarchical Neural Architecture Search Spaces based on\n  Context-free Grammars",
        "abstract": "  The discovery of neural architectures from simple building blocks is a\nlong-standing goal of Neural Architecture Search (NAS). Hierarchical search\nspaces are a promising step towards this goal but lack a unifying search space\ndesign framework and typically only search over some limited aspect of\narchitectures. In this work, we introduce a unifying search space design\nframework based on context-free grammars that can naturally and compactly\ngenerate expressive hierarchical search spaces that are 100s of orders of\nmagnitude larger than common spaces from the literature. By enhancing and using\ntheir properties, we effectively enable search over the complete architecture\nand can foster regularity. Further, we propose an efficient hierarchical kernel\ndesign for a Bayesian Optimization search strategy to efficiently search over\nsuch huge spaces. We demonstrate the versatility of our search space design\nframework and show that our search strategy can be superior to existing NAS\napproaches. Code is available at\nhttps://github.com/automl/hierarchical_nas_construction.\n",
        "published": "2022",
        "authors": [
            "Simon Schrodi",
            "Danny Stoll",
            "Binxin Ru",
            "Rhea Sukthanker",
            "Thomas Brox",
            "Frank Hutter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.05039v2",
        "title": "Active Acquisition for Multimodal Temporal Data: A Challenging\n  Decision-Making Task",
        "abstract": "  We introduce a challenging decision-making task that we call active\nacquisition for multimodal temporal data (A2MT). In many real-world scenarios,\ninput features are not readily available at test time and must instead be\nacquired at significant cost. With A2MT, we aim to learn agents that actively\nselect which modalities of an input to acquire, trading off acquisition cost\nand predictive performance. A2MT extends a previous task called active feature\nacquisition to temporal decision making about high-dimensional inputs. We\npropose a method based on the Perceiver IO architecture to address A2MT in\npractice. Our agents are able to solve a novel synthetic scenario requiring\npractically relevant cross-modal reasoning skills. On two large-scale,\nreal-world datasets, Kinetics-700 and AudioSet, our agents successfully learn\ncost-reactive acquisition behavior. However, an ablation reveals they are\nunable to learn adaptive acquisition strategies, emphasizing the difficulty of\nthe task even for state-of-the-art models. Applications of A2MT may be\nimpactful in domains like medicine, robotics, or finance, where modalities\ndiffer in acquisition cost and informativeness.\n",
        "published": "2022",
        "authors": [
            "Jannik Kossen",
            "C\u0103t\u0103lina Cangea",
            "Eszter V\u00e9rtes",
            "Andrew Jaegle",
            "Viorica Patraucean",
            "Ira Ktena",
            "Nenad Tomasev",
            "Danielle Belgrave"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.07245v1",
        "title": "Assessing Performance and Fairness Metrics in Face Recognition -\n  Bootstrap Methods",
        "abstract": "  The ROC curve is the major tool for assessing not only the performance but\nalso the fairness properties of a similarity scoring function in Face\nRecognition. In order to draw reliable conclusions based on empirical ROC\nanalysis, evaluating accurately the uncertainty related to statistical versions\nof the ROC curves of interest is necessary. For this purpose, we explain in\nthis paper that, because the True/False Acceptance Rates are of the form of\nU-statistics in the case of similarity scoring, the naive bootstrap approach is\nnot valid here and that a dedicated recentering technique must be used instead.\nThis is illustrated on real data of face images, when applied to several\nROC-based metrics such as popular fairness metrics.\n",
        "published": "2022",
        "authors": [
            "Jean-R\u00e9my Conti",
            "St\u00e9phan Cl\u00e9men\u00e7on"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.08403v3",
        "title": "REPAIR: REnormalizing Permuted Activations for Interpolation Repair",
        "abstract": "  In this paper we look into the conjecture of Entezari et al. (2021) which\nstates that if the permutation invariance of neural networks is taken into\naccount, then there is likely no loss barrier to the linear interpolation\nbetween SGD solutions. First, we observe that neuron alignment methods alone\nare insufficient to establish low-barrier linear connectivity between SGD\nsolutions due to a phenomenon we call variance collapse: interpolated deep\nnetworks suffer a collapse in the variance of their activations, causing poor\nperformance. Next, we propose REPAIR (REnormalizing Permuted Activations for\nInterpolation Repair) which mitigates variance collapse by rescaling the\npreactivations of such interpolated networks. We explore the interaction\nbetween our method and the choice of normalization layer, network width, and\ndepth, and demonstrate that using REPAIR on top of neuron alignment methods\nleads to 60%-100% relative barrier reduction across a wide variety of\narchitecture families and tasks. In particular, we report a 74% barrier\nreduction for ResNet50 on ImageNet and 90% barrier reduction for ResNet18 on\nCIFAR10.\n",
        "published": "2022",
        "authors": [
            "Keller Jordan",
            "Hanie Sedghi",
            "Olga Saukh",
            "Rahim Entezari",
            "Behnam Neyshabur"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.11255v2",
        "title": "Diffusion Denoising Process for Perceptron Bias in Out-of-distribution\n  Detection",
        "abstract": "  Out-of-distribution (OOD) detection is a crucial task for ensuring the\nreliability and safety of deep learning. Currently, discriminator models\noutperform other methods in this regard. However, the feature extraction\nprocess used by discriminator models suffers from the loss of critical\ninformation, leaving room for bad cases and malicious attacks. In this paper,\nwe introduce a new perceptron bias assumption that suggests discriminator\nmodels are more sensitive to certain features of the input, leading to the\noverconfidence problem. To address this issue, we propose a novel framework\nthat combines discriminator and generation models and integrates diffusion\nmodels (DMs) into OOD detection. We demonstrate that the diffusion denoising\nprocess (DDP) of DMs serves as a novel form of asymmetric interpolation, which\nis well-suited to enhance the input and mitigate the overconfidence problem.\nThe discriminator model features of OOD data exhibit sharp changes under DDP,\nand we utilize the norm of this change as the indicator score. Our experiments\non CIFAR10, CIFAR100, and ImageNet show that our method outperforms SOTA\napproaches. Notably, for the challenging InD ImageNet and OOD species datasets,\nour method achieves an AUROC of 85.7, surpassing the previous SOTA method's\nscore of 77.4. Our implementation is available at\n\\url{https://github.com/luping-liu/DiffOOD}.\n",
        "published": "2022",
        "authors": [
            "Luping Liu",
            "Yi Ren",
            "Xize Cheng",
            "Rongjie Huang",
            "Chongxuan Li",
            "Zhou Zhao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.12717v1",
        "title": "Benchmarking Bayesian Deep Learning on Diabetic Retinopathy Detection\n  Tasks",
        "abstract": "  Bayesian deep learning seeks to equip deep neural networks with the ability\nto precisely quantify their predictive uncertainty, and has promised to make\ndeep learning more reliable for safety-critical real-world applications. Yet,\nexisting Bayesian deep learning methods fall short of this promise; new methods\ncontinue to be evaluated on unrealistic test beds that do not reflect the\ncomplexities of downstream real-world tasks that would benefit most from\nreliable uncertainty quantification. We propose the RETINA Benchmark, a set of\nreal-world tasks that accurately reflect such complexities and are designed to\nassess the reliability of predictive models in safety-critical scenarios.\nSpecifically, we curate two publicly available datasets of high-resolution\nhuman retina images exhibiting varying degrees of diabetic retinopathy, a\nmedical condition that can lead to blindness, and use them to design a suite of\nautomated diagnosis tasks that require reliable predictive uncertainty\nquantification. We use these tasks to benchmark well-established and\nstate-of-the-art Bayesian deep learning methods on task-specific evaluation\nmetrics. We provide an easy-to-use codebase for fast and easy benchmarking\nfollowing reproducibility and software design principles. We provide\nimplementations of all methods included in the benchmark as well as results\ncomputed over 100 TPU days, 20 GPU days, 400 hyperparameter configurations, and\nevaluation on at least 6 random seeds each.\n",
        "published": "2022",
        "authors": [
            "Neil Band",
            "Tim G. J. Rudner",
            "Qixuan Feng",
            "Angelos Filos",
            "Zachary Nado",
            "Michael W. Dusenberry",
            "Ghassen Jerfel",
            "Dustin Tran",
            "Yarin Gal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.17084v1",
        "title": "High-Fidelity Guided Image Synthesis with Latent Diffusion Models",
        "abstract": "  Controllable image synthesis with user scribbles has gained huge public\ninterest with the recent advent of text-conditioned latent diffusion models.\nThe user scribbles control the color composition while the text prompt provides\ncontrol over the overall image semantics. However, we note that prior works in\nthis direction suffer from an intrinsic domain shift problem, wherein the\ngenerated outputs often lack details and resemble simplistic representations of\nthe target domain. In this paper, we propose a novel guided image synthesis\nframework, which addresses this problem by modeling the output image as the\nsolution of a constrained optimization problem. We show that while computing an\nexact solution to the optimization is infeasible, an approximation of the same\ncan be achieved while just requiring a single pass of the reverse diffusion\nprocess. Additionally, we show that by simply defining a cross-attention based\ncorrespondence between the input text tokens and the user stroke-painting, the\nuser is also able to control the semantics of different painted regions without\nrequiring any conditional training or finetuning. Human user study results show\nthat the proposed approach outperforms the previous state-of-the-art by over\n85.32% on the overall user satisfaction scores. Project page for our paper is\navailable at https://1jsingh.github.io/gradop.\n",
        "published": "2022",
        "authors": [
            "Jaskirat Singh",
            "Stephen Gould",
            "Liang Zheng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.06461v2",
        "title": "A Statistical Model for Predicting Generalization in Few-Shot\n  Classification",
        "abstract": "  The estimation of the generalization error of classifiers often relies on a\nvalidation set. Such a set is hardly available in few-shot learning scenarios,\na highly disregarded shortcoming in the field. In these scenarios, it is common\nto rely on features extracted from pre-trained neural networks combined with\ndistance-based classifiers such as nearest class mean. In this work, we\nintroduce a Gaussian model of the feature distribution. By estimating the\nparameters of this model, we are able to predict the generalization error on\nnew classification tasks with few samples. We observe that accurate distance\nestimates between class-conditional densities are the key to accurate estimates\nof the generalization performance. Therefore, we propose an unbiased estimator\nfor these distances and integrate it in our numerical analysis. We empirically\nshow that our approach outperforms alternatives such as the leave-one-out\ncross-validation strategy.\n",
        "published": "2022",
        "authors": [
            "Yassir Bendou",
            "Vincent Gripon",
            "Bastien Pasdeloup",
            "Lukas Mauch",
            "Stefan Uhlich",
            "Fabien Cardinaux",
            "Ghouthi Boukli Hacene",
            "Javier Alonso Garcia"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.13621v2",
        "title": "Annealing Double-Head: An Architecture for Online Calibration of Deep\n  Neural Networks",
        "abstract": "  Model calibration, which is concerned with how frequently the model predicts\ncorrectly, not only plays a vital part in statistical model design, but also\nhas substantial practical applications, such as optimal decision-making in the\nreal world. However, it has been discovered that modern deep neural networks\nare generally poorly calibrated due to the overestimation (or underestimation)\nof predictive confidence, which is closely related to overfitting. In this\npaper, we propose Annealing Double-Head, a simple-to-implement but highly\neffective architecture for calibrating the DNN during training. To be precise,\nwe construct an additional calibration head-a shallow neural network that\ntypically has one latent layer-on top of the last latent layer in the normal\nmodel to map the logits to the aligned confidence. Furthermore, a simple\nAnnealing technique that dynamically scales the logits by calibration head in\ntraining procedure is developed to improve its performance. Under both the\nin-distribution and distributional shift circumstances, we exhaustively\nevaluate our Annealing Double-Head architecture on multiple pairs of\ncontemporary DNN architectures and vision and speech datasets. We demonstrate\nthat our method achieves state-of-the-art model calibration performance without\npost-processing while simultaneously providing comparable predictive accuracy\nin comparison to other recently proposed calibration methods on a range of\nlearning tasks.\n",
        "published": "2022",
        "authors": [
            "Erdong Guo",
            "David Draper",
            "Maria De Iorio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.01520v2",
        "title": "Towards Explainable Land Cover Mapping: a Counterfactual-based Strategy",
        "abstract": "  Counterfactual explanations are an emerging tool to enhance interpretability\nof deep learning models. Given a sample, these methods seek to find and display\nto the user similar samples across the decision boundary. In this paper, we\npropose a generative adversarial counterfactual approach for satellite image\ntime series in a multi-class setting for the land cover classification task.\nOne of the distinctive features of the proposed approach is the lack of prior\nassumption on the targeted class for a given counterfactual explanation. This\ninherent flexibility allows for the discovery of interesting information on the\nrelationship between land cover classes. The other feature consists of\nencouraging the counterfactual to differ from the original sample only in a\nsmall and compact temporal segment. These time-contiguous perturbations allow\nfor a much sparser and, thus, interpretable solution. Furthermore,\nplausibility/realism of the generated counterfactual explanations is enforced\nvia the proposed adversarial learning strategy.\n",
        "published": "2023",
        "authors": [
            "Cassio F. Dantas",
            "Diego Marcos",
            "Dino Ienco"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.12003v3",
        "title": "Minimizing Trajectory Curvature of ODE-based Generative Models",
        "abstract": "  Recent ODE/SDE-based generative models, such as diffusion models, rectified\nflows, and flow matching, define a generative process as a time reversal of a\nfixed forward process. Even though these models show impressive performance on\nlarge-scale datasets, numerical simulation requires multiple evaluations of a\nneural network, leading to a slow sampling speed. We attribute the reason to\nthe high curvature of the learned generative trajectories, as it is directly\nrelated to the truncation error of a numerical solver. Based on the\nrelationship between the forward process and the curvature, here we present an\nefficient method of training the forward process to minimize the curvature of\ngenerative trajectories without any ODE/SDE simulation. Experiments show that\nour method achieves a lower curvature than previous models and, therefore,\ndecreased sampling costs while maintaining competitive performance. Code is\navailable at https://github.com/sangyun884/fast-ode.\n",
        "published": "2023",
        "authors": [
            "Sangyun Lee",
            "Beomsu Kim",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.12171v2",
        "title": "ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts",
        "abstract": "  Recent success of large-scale Contrastive Language-Image Pre-training (CLIP)\nhas led to great promise in zero-shot semantic segmentation by transferring\nimage-text aligned knowledge to pixel-level classification. However, existing\nmethods usually require an additional image encoder or retraining/tuning the\nCLIP module. Here, we propose a novel Zero-shot segmentation with Optimal\nTransport (ZegOT) method that matches multiple text prompts with frozen image\nembeddings through optimal transport. In particular, we introduce a novel\nMultiple Prompt Optimal Transport Solver (MPOT), which is designed to learn an\noptimal mapping between multiple text prompts and visual feature maps of the\nfrozen image encoder hidden layers. This unique mapping method facilitates each\nof the multiple text prompts to effectively focus on distinct visual semantic\nattributes. Through extensive experiments on benchmark datasets, we show that\nour method achieves the state-of-the-art (SOTA) performance over existing\nZero-shot Semantic Segmentation (ZS3) approaches.\n",
        "published": "2023",
        "authors": [
            "Kwanyoung Kim",
            "Yujin Oh",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.12334v1",
        "title": "Don't Play Favorites: Minority Guidance for Diffusion Models",
        "abstract": "  We explore the problem of generating minority samples using diffusion models.\nThe minority samples are instances that lie on low-density regions of a data\nmanifold. Generating sufficient numbers of such minority instances is\nimportant, since they often contain some unique attributes of the data.\nHowever, the conventional generation process of the diffusion models mostly\nyields majority samples (that lie on high-density regions of the manifold) due\nto their high likelihoods, making themselves highly ineffective and\ntime-consuming for the task. In this work, we present a novel framework that\ncan make the generation process of the diffusion models focus on the minority\nsamples. We first provide a new insight on the majority-focused nature of the\ndiffusion models: they denoise in favor of the majority samples. The\nobservation motivates us to introduce a metric that describes the uniqueness of\na given sample. To address the inherent preference of the diffusion models\nw.r.t. the majority samples, we further develop minority guidance, a sampling\ntechnique that can guide the generation process toward regions with desired\nlikelihood levels. Experiments on benchmark real datasets demonstrate that our\nminority guidance can greatly improve the capability of generating the\nlow-likelihood minority samples over existing generative frameworks including\nthe standard diffusion sampler.\n",
        "published": "2023",
        "authors": [
            "Soobin Um",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.02155v1",
        "title": "Guaranteed Tensor Recovery Fused Low-rankness and Smoothness",
        "abstract": "  The tensor data recovery task has thus attracted much research attention in\nrecent years. Solving such an ill-posed problem generally requires to explore\nintrinsic prior structures underlying tensor data, and formulate them as\ncertain forms of regularization terms for guiding a sound estimate of the\nrestored tensor. Recent research have made significant progress by adopting two\ninsightful tensor priors, i.e., global low-rankness (L) and local smoothness\n(S) across different tensor modes, which are always encoded as a sum of two\nseparate regularization terms into the recovery models. However, unlike the\nprimary theoretical developments on low-rank tensor recovery, these joint L+S\nmodels have no theoretical exact-recovery guarantees yet, making the methods\nlack reliability in real practice. To this crucial issue, in this work, we\nbuild a unique regularization term, which essentially encodes both L and S\npriors of a tensor simultaneously. Especially, by equipping this single\nregularizer into the recovery models, we can rigorously prove the exact\nrecovery guarantees for two typical tensor recovery tasks, i.e., tensor\ncompletion (TC) and tensor robust principal component analysis (TRPCA). To the\nbest of our knowledge, this should be the first exact-recovery results among\nall related L+S methods for tensor recovery. Significant recovery accuracy\nimprovements over many other SOTA methods in several TC and TRPCA tasks with\nvarious kinds of visual tensor data are observed in extensive experiments.\nTypically, our method achieves a workable performance when the missing rate is\nextremely large, e.g., 99.5%, for the color image inpainting task, while all\nits peers totally fail in such challenging case.\n",
        "published": "2023",
        "authors": [
            "Hailin Wang",
            "Jiangjun Peng",
            "Wenjin Qin",
            "Jianjun Wang",
            "Deyu Meng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.02622v1",
        "title": "Uncertainty Calibration and its Application to Object Detection",
        "abstract": "  Image-based environment perception is an important component especially for\ndriver assistance systems or autonomous driving. In this scope, modern neuronal\nnetworks are used to identify multiple objects as well as the according\nposition and size information within a single frame. The performance of such an\nobject detection model is important for the overall performance of the whole\nsystem. However, a detection model might also predict these objects under a\ncertain degree of uncertainty. [...]\n  In this work, we examine the semantic uncertainty (which object type?) as\nwell as the spatial uncertainty (where is the object and how large is it?). We\nevaluate if the predicted uncertainties of an object detection model match with\nthe observed error that is achieved on real-world data. In the first part of\nthis work, we introduce the definition for confidence calibration of the\nsemantic uncertainty in the context of object detection, instance segmentation,\nand semantic segmentation. We integrate additional position information in our\nexaminations to evaluate the effect of the object's position on the semantic\ncalibration properties. Besides measuring calibration, it is also possible to\nperform a post-hoc recalibration of semantic uncertainty that might have turned\nout to be miscalibrated. [...]\n  The second part of this work deals with the spatial uncertainty obtained by a\nprobabilistic detection model. [...] We review and extend common calibration\nmethods so that it is possible to obtain parametric uncertainty distributions\nfor the position information in a more flexible way.\n  In the last part, we demonstrate a possible use-case for our derived\ncalibration methods in the context of object tracking. [...] We integrate our\npreviously proposed calibration techniques and demonstrate the usefulness of\nsemantic and spatial uncertainty calibration in a subsequent process. [...]\n",
        "published": "2023",
        "authors": [
            "Fabian K\u00fcppers"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.03900v1",
        "title": "Zero-shot Generation of Coherent Storybook from Plain Text Story using\n  Diffusion Models",
        "abstract": "  Recent advancements in large scale text-to-image models have opened new\npossibilities for guiding the creation of images through human-devised natural\nlanguage. However, while prior literature has primarily focused on the\ngeneration of individual images, it is essential to consider the capability of\nthese models to ensure coherency within a sequence of images to fulfill the\ndemands of real-world applications such as storytelling. To address this, here\nwe present a novel neural pipeline for generating a coherent storybook from the\nplain text of a story. Specifically, we leverage a combination of a pre-trained\nLarge Language Model and a text-guided Latent Diffusion Model to generate\ncoherent images. While previous story synthesis frameworks typically require a\nlarge-scale text-to-image model trained on expensive image-caption pairs to\nmaintain the coherency, we employ simple textual inversion techniques along\nwith detector-based semantic image editing which allows zero-shot generation of\nthe coherent storybook. Experimental results show that our proposed method\noutperforms state-of-the-art image editing baselines.\n",
        "published": "2023",
        "authors": [
            "Hyeonho Jeong",
            "Gihyun Kwon",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.05294v1",
        "title": "MoreauGrad: Sparse and Robust Interpretation of Neural Networks via\n  Moreau Envelope",
        "abstract": "  Explaining the predictions of deep neural nets has been a topic of great\ninterest in the computer vision literature. While several gradient-based\ninterpretation schemes have been proposed to reveal the influential variables\nin a neural net's prediction, standard gradient-based interpretation frameworks\nhave been commonly observed to lack robustness to input perturbations and\nflexibility for incorporating prior knowledge of sparsity and group-sparsity\nstructures. In this work, we propose MoreauGrad as an interpretation scheme\nbased on the classifier neural net's Moreau envelope. We demonstrate that\nMoreauGrad results in a smooth and robust interpretation of a multi-layer\nneural network and can be efficiently computed through first-order optimization\nmethods. Furthermore, we show that MoreauGrad can be naturally combined with\n$L_1$-norm regularization techniques to output a sparse or group-sparse\nexplanation which are prior conditions applicable to a wide range of deep\nlearning applications. We empirically evaluate the proposed MoreauGrad scheme\non standard computer vision datasets, showing the qualitative and quantitative\nsuccess of the MoreauGrad approach in comparison to standard gradient-based\ninterpretation methods.\n",
        "published": "2023",
        "authors": [
            "Jingwei Zhang",
            "Farzan Farnia"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.11552v4",
        "title": "Reduce, Reuse, Recycle: Compositional Generation with Energy-Based\n  Diffusion Models and MCMC",
        "abstract": "  Since their introduction, diffusion models have quickly become the prevailing\napproach to generative modeling in many domains. They can be interpreted as\nlearning the gradients of a time-varying sequence of log-probability density\nfunctions. This interpretation has motivated classifier-based and\nclassifier-free guidance as methods for post-hoc control of diffusion models.\nIn this work, we build upon these ideas using the score-based interpretation of\ndiffusion models, and explore alternative ways to condition, modify, and reuse\ndiffusion models for tasks involving compositional generation and guidance. In\nparticular, we investigate why certain types of composition fail using current\ntechniques and present a number of solutions. We conclude that the sampler (not\nthe model) is responsible for this failure and propose new samplers, inspired\nby MCMC, which enable successful compositional generation. Further, we propose\nan energy-based parameterization of diffusion models which enables the use of\nnew compositional operators and more sophisticated, Metropolis-corrected\nsamplers. Intriguingly we find these samplers lead to notable improvements in\ncompositional generation across a wide set of problems such as\nclassifier-guided ImageNet modeling and compositional text-to-image generation.\n",
        "published": "2023",
        "authors": [
            "Yilun Du",
            "Conor Durkan",
            "Robin Strudel",
            "Joshua B. Tenenbaum",
            "Sander Dieleman",
            "Rob Fergus",
            "Jascha Sohl-Dickstein",
            "Arnaud Doucet",
            "Will Grathwohl"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.00972v1",
        "title": "Practical Network Acceleration with Tiny Sets: Hypothesis, Theory, and\n  Algorithm",
        "abstract": "  Due to data privacy issues, accelerating networks with tiny training sets has\nbecome a critical need in practice. Previous methods achieved promising results\nempirically by filter-level pruning. In this paper, we both study this problem\ntheoretically and propose an effective algorithm aligning well with our\ntheoretical results. First, we propose the finetune convexity hypothesis to\nexplain why recent few-shot compression algorithms do not suffer from\noverfitting problems. Based on it, a theory is further established to explain\nthese methods for the first time. Compared to naively finetuning a pruned\nnetwork, feature mimicking is proved to achieve a lower variance of parameters\nand hence enjoys easier optimization. With our theoretical conclusions, we\nclaim dropping blocks is a fundamentally superior few-shot compression scheme\nin terms of more convex optimization and a higher acceleration ratio. To choose\nwhich blocks to drop, we propose a new metric, recoverability, to effectively\nmeasure the difficulty of recovering the compressed network. Finally, we\npropose an algorithm named PRACTISE to accelerate networks using only tiny\ntraining sets. PRACTISE outperforms previous methods by a significant margin.\nFor 22% latency reduction, it surpasses previous methods by on average 7\npercentage points on ImageNet-1k. It also works well under data-free or\nout-of-domain data settings. Our code is at\nhttps://github.com/DoctorKey/Practise\n",
        "published": "2023",
        "authors": [
            "Guo-Hua Wang",
            "Jianxin Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.04143v2",
        "title": "Can We Scale Transformers to Predict Parameters of Diverse ImageNet\n  Models?",
        "abstract": "  Pretraining a neural network on a large dataset is becoming a cornerstone in\nmachine learning that is within the reach of only a few communities with\nlarge-resources. We aim at an ambitious goal of democratizing pretraining.\nTowards that goal, we train and release a single neural network that can\npredict high quality ImageNet parameters of other neural networks. By using\npredicted parameters for initialization we are able to boost training of\ndiverse ImageNet models available in PyTorch. When transferred to other\ndatasets, models initialized with predicted parameters also converge faster and\nreach competitive final performance.\n",
        "published": "2023",
        "authors": [
            "Boris Knyazev",
            "Doha Hwang",
            "Simon Lacoste-Julien"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.05754v2",
        "title": "Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse\n  Problems",
        "abstract": "  Krylov subspace, which is generated by multiplying a given vector by the\nmatrix of a linear transformation and its successive powers, has been\nextensively studied in classical optimization literature to design algorithms\nthat converge quickly for large linear inverse problems. For example, the\nconjugate gradient method (CG), one of the most popular Krylov subspace\nmethods, is based on the idea of minimizing the residual error in the Krylov\nsubspace. However, with the recent advancement of high-performance diffusion\nsolvers for inverse problems, it is not clear how classical wisdom can be\nsynergistically combined with modern diffusion models. In this study, we\npropose a novel and efficient diffusion sampling strategy that synergistically\ncombine the diffusion sampling and Krylov subspace methods. Specifically, we\nprove that if the tangent space at a denoised sample by Tweedie's formula forms\na Krylov subspace, then the CG initialized with the denoised data ensures the\ndata consistency update to remain in the tangent space. This negates the need\nto compute the manifold-constrained gradient (MCG), leading to a more efficient\ndiffusion sampling method. Our method is applicable regardless of the\nparametrization and setting (i.e., VE, VP). Notably, we achieve\nstate-of-the-art reconstruction quality on challenging real-world medical\ninverse imaging problems, including multi-coil MRI reconstruction and 3D CT\nreconstruction. Moreover, our proposed method achieves more than 80 times\nfaster inference time than the previous state-of-the-art method.\n",
        "published": "2023",
        "authors": [
            "Hyungjin Chung",
            "Suhyeon Lee",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.08622v2",
        "title": "Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style\n  Transfer",
        "abstract": "  Diffusion models have shown great promise in text-guided image style\ntransfer, but there is a trade-off between style transformation and content\npreservation due to their stochastic nature. Existing methods require\ncomputationally expensive fine-tuning of diffusion models or additional neural\nnetwork. To address this, here we propose a zero-shot contrastive loss for\ndiffusion models that doesn't require additional fine-tuning or auxiliary\nnetworks. By leveraging patch-wise contrastive loss between generated samples\nand original image embeddings in the pre-trained diffusion model, our method\ncan generate images with the same semantic content as the source image in a\nzero-shot manner. Our approach outperforms existing methods while preserving\ncontent and requiring no additional training, not only for image style transfer\nbut also for image-to-image translation and manipulation. Our experimental\nresults validate the effectiveness of our proposed method.\n",
        "published": "2023",
        "authors": [
            "Serin Yang",
            "Hyunmin Hwang",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.12964v1",
        "title": "Continuous Indeterminate Probability Neural Network",
        "abstract": "  This paper introduces a general model called CIPNN - Continuous Indeterminate\nProbability Neural Network, and this model is based on IPNN, which is used for\ndiscrete latent random variables. Currently, posterior of continuous latent\nvariables is regarded as intractable, with the new theory proposed by IPNN this\nproblem can be solved. Our contributions are Four-fold. First, we derive the\nanalytical solution of the posterior calculation of continuous latent random\nvariables and propose a general classification model (CIPNN). Second, we\npropose a general auto-encoder called CIPAE - Continuous Indeterminate\nProbability Auto-Encoder, the decoder part is not a neural network and uses a\nfully probabilistic inference model for the first time. Third, we propose a new\nmethod to visualize the latent random variables, we use one of N dimensional\nlatent variables as a decoder to reconstruct the input image, which can work\neven for classification tasks, in this way, we can see what each latent\nvariable has learned. Fourth, IPNN has shown great classification capability,\nCIPNN has pushed this classification capability to infinity. Theoretical\nadvantages are reflected in experimental results.\n",
        "published": "2023",
        "authors": [
            "Tao Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.15342v1",
        "title": "Exploring Continual Learning of Diffusion Models",
        "abstract": "  Diffusion models have achieved remarkable success in generating high-quality\nimages thanks to their novel training procedures applied to unprecedented\namounts of data. However, training a diffusion model from scratch is\ncomputationally expensive. This highlights the need to investigate the\npossibility of training these models iteratively, reusing computation while the\ndata distribution changes. In this study, we take the first step in this\ndirection and evaluate the continual learning (CL) properties of diffusion\nmodels. We begin by benchmarking the most common CL methods applied to\nDenoising Diffusion Probabilistic Models (DDPMs), where we note the strong\nperformance of the experience replay with the reduced rehearsal coefficient.\nFurthermore, we provide insights into the dynamics of forgetting, which exhibit\ndiverse behavior across diffusion timesteps. We also uncover certain pitfalls\nof using the bits-per-dimension metric for evaluating CL.\n",
        "published": "2023",
        "authors": [
            "Micha\u0142 Zaj\u0105c",
            "Kamil Deja",
            "Anna Kuzina",
            "Jakub M. Tomczak",
            "Tomasz Trzci\u0144ski",
            "Florian Shkurti",
            "Piotr Mi\u0142o\u015b"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.16938v1",
        "title": "Are Neural Architecture Search Benchmarks Well Designed? A Deeper Look\n  Into Operation Importance",
        "abstract": "  Neural Architecture Search (NAS) benchmarks significantly improved the\ncapability of developing and comparing NAS methods while at the same time\ndrastically reduced the computational overhead by providing meta-information\nabout thousands of trained neural networks. However, tabular benchmarks have\nseveral drawbacks that can hinder fair comparisons and provide unreliable\nresults. These usually focus on providing a small pool of operations in heavily\nconstrained search spaces -- usually cell-based neural networks with\npre-defined outer-skeletons. In this work, we conducted an empirical analysis\nof the widely used NAS-Bench-101, NAS-Bench-201 and TransNAS-Bench-101\nbenchmarks in terms of their generability and how different operations\ninfluence the performance of the generated architectures. We found that only a\nsubset of the operation pool is required to generate architectures close to the\nupper-bound of the performance range. Also, the performance distribution is\nnegatively skewed, having a higher density of architectures in the upper-bound\nrange. We consistently found convolution layers to have the highest impact on\nthe architecture's performance, and that specific combination of operations\nfavors top-scoring architectures. These findings shed insights on the correct\nevaluation and comparison of NAS methods using NAS benchmarks, showing that\ndirectly searching on NAS-Bench-201, ImageNet16-120 and TransNAS-Bench-101\nproduces more reliable results than searching only on CIFAR-10. Furthermore,\nwith this work we provide suggestions for future benchmark evaluations and\ndesign. The code used to conduct the evaluations is available at\nhttps://github.com/VascoLopes/NAS-Benchmark-Evaluation.\n",
        "published": "2023",
        "authors": [
            "Vasco Lopes",
            "Bruno Degardin",
            "Lu\u00eds A. Alexandre"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.07613v1",
        "title": "Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training",
        "abstract": "  Training Generative adversarial networks (GANs) stably is a challenging task.\nThe generator in GANs transform noise vectors, typically Gaussian distributed,\ninto realistic data such as images. In this paper, we propose a novel approach\nfor training GANs with images as inputs, but without enforcing any pairwise\nconstraints. The intuition is that images are more structured than noise, which\nthe generator can leverage to learn a more robust transformation. The process\ncan be made efficient by identifying closely related datasets, or a ``friendly\nneighborhood'' of the target distribution, inspiring the moniker, Spider GAN.\nTo define friendly neighborhoods leveraging proximity between datasets, we\npropose a new measure called the signed inception distance (SID), inspired by\nthe polyharmonic kernel. We show that the Spider GAN formulation results in\nfaster convergence, as the generator can discover correspondence even between\nseemingly unrelated datasets, for instance, between Tiny-ImageNet and CelebA\nfaces. Further, we demonstrate cascading Spider GAN, where the output\ndistribution from a pre-trained GAN generator is used as the input to the\nsubsequent network. Effectively, transporting one distribution to another in a\ncascaded fashion until the target is learnt -- a new flavor of transfer\nlearning. We demonstrate the efficacy of the Spider approach on DCGAN,\nconditional GAN, PGGAN, StyleGAN2 and StyleGAN3. The proposed approach achieves\nstate-of-the-art Frechet inception distance (FID) values, with one-fifth of the\ntraining iterations, in comparison to their baseline counterparts on\nhigh-resolution small datasets such as MetFaces, Ukiyo-E Faces and AFHQ-Cats.\n",
        "published": "2023",
        "authors": [
            "Siddarth Asokan",
            "Chandra Sekhar Seelamantula"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.07642v1",
        "title": "The ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge 2023:\n  Intracranial Meningioma",
        "abstract": "  Meningiomas are the most common primary intracranial tumor in adults and can\nbe associated with significant morbidity and mortality. Radiologists,\nneurosurgeons, neuro-oncologists, and radiation oncologists rely on\nmultiparametric MRI (mpMRI) for diagnosis, treatment planning, and longitudinal\ntreatment monitoring; yet automated, objective, and quantitative tools for\nnon-invasive assessment of meningiomas on mpMRI are lacking. The BraTS\nmeningioma 2023 challenge will provide a community standard and benchmark for\nstate-of-the-art automated intracranial meningioma segmentation models based on\nthe largest expert annotated multilabel meningioma mpMRI dataset to date.\nChallenge competitors will develop automated segmentation models to predict\nthree distinct meningioma sub-regions on MRI including enhancing tumor,\nnon-enhancing tumor core, and surrounding nonenhancing T2/FLAIR hyperintensity.\nModels will be evaluated on separate validation and held-out test datasets\nusing standardized metrics utilized across the BraTS 2023 series of challenges\nincluding the Dice similarity coefficient and Hausdorff distance. The models\ndeveloped during the course of this challenge will aid in incorporation of\nautomated meningioma MRI segmentation into clinical practice, which will\nultimately improve care of patients with meningioma.\n",
        "published": "2023",
        "authors": [
            "Dominic LaBella",
            "Maruf Adewole",
            "Michelle Alonso-Basanta",
            "Talissa Altes",
            "Syed Muhammad Anwar",
            "Ujjwal Baid",
            "Timothy Bergquist",
            "Radhika Bhalerao",
            "Sully Chen",
            "Verena Chung",
            "Gian-Marco Conte",
            "Farouk Dako",
            "James Eddy",
            "Ivan Ezhov",
            "Devon Godfrey",
            "Fathi Hilal",
            "Ariana Familiar",
            "Keyvan Farahani",
            "Juan Eugenio Iglesias",
            "Zhifan Jiang",
            "Elaine Johanson",
            "Anahita Fathi Kazerooni",
            "Collin Kent",
            "John Kirkpatrick",
            "Florian Kofler",
            "Koen Van Leemput",
            "Hongwei Bran Li",
            "Xinyang Liu",
            "Aria Mahtabfar",
            "Shan McBurney-Lin",
            "Ryan McLean",
            "Zeke Meier",
            "Ahmed W Moawad",
            "John Mongan",
            "Pierre Nedelec",
            "Maxence Pajot",
            "Marie Piraud",
            "Arif Rashid",
            "Zachary Reitman",
            "Russell Takeshi Shinohara",
            "Yury Velichko",
            "Chunhao Wang",
            "Pranav Warman",
            "Walter Wiggins",
            "Mariam Aboian",
            "Jake Albrecht",
            "Udunna Anazodo",
            "Spyridon Bakas",
            "Adam Flanders",
            "Anastasia Janas",
            "Goldey Khanna",
            "Marius George Linguraru",
            "Bjoern Menze",
            "Ayman Nada",
            "Andreas M Rauschecker",
            "Jeff Rudie",
            "Nourel Hoda Tahon",
            "Javier Villanueva-Meyer",
            "Benedikt Wiestler",
            "Evan Calabrese"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.09828v1",
        "title": "Mimetic Initialization of Self-Attention Layers",
        "abstract": "  It is notoriously difficult to train Transformers on small datasets;\ntypically, large pre-trained models are instead used as the starting point. We\nexplore the weights of such pre-trained Transformers (particularly for vision)\nto attempt to find reasons for this discrepancy. Surprisingly, we find that\nsimply initializing the weights of self-attention layers so that they \"look\"\nmore like their pre-trained counterparts allows us to train vanilla\nTransformers faster and to higher final accuracies, particularly on vision\ntasks such as CIFAR-10 and ImageNet classification, where we see gains in\naccuracy of over 5% and 4%, respectively. Our initialization scheme is closed\nform, learning-free, and very simple: we set the product of the query and key\nweights to be approximately the identity, and the product of the value and\nprojection weights to approximately the negative identity. As this mimics the\npatterns we saw in pre-trained Transformers, we call the technique \"mimetic\ninitialization\".\n",
        "published": "2023",
        "authors": [
            "Asher Trockman",
            "J. Zico Kolter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.15086v2",
        "title": "Unpaired Image-to-Image Translation via Neural Schr\u00f6dinger Bridge",
        "abstract": "  Diffusion models are a powerful class of generative models which simulate\nstochastic differential equations (SDEs) to generate data from noise. Although\ndiffusion models have achieved remarkable progress in recent years, they have\nlimitations in the unpaired image-to-image translation tasks due to the\nGaussian prior assumption. Schr\\\"odinger Bridge (SB), which learns an SDE to\ntranslate between two arbitrary distributions, have risen as an attractive\nsolution to this problem. However, none of SB models so far have been\nsuccessful at unpaired translation between high-resolution images. In this\nwork, we propose the Unpaired Neural Schr\\\"odinger Bridge (UNSB), which\nexpresses SB problem as a sequence of adversarial learning problems. This\nallows us to incorporate advanced discriminators and regularization to learn a\nSB between unpaired data. We demonstrate that UNSB is scalable and successfully\nsolves various unpaired image-to-image translation tasks. Code:\n\\url{https://github.com/cyclomon/UNSB}\n",
        "published": "2023",
        "authors": [
            "Beomsu Kim",
            "Gihyun Kwon",
            "Kwanyoung Kim",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.18806v1",
        "title": "Prediction Error-based Classification for Class-Incremental Learning",
        "abstract": "  Class-incremental learning (CIL) is a particularly challenging variant of\ncontinual learning, where the goal is to learn to discriminate between all\nclasses presented in an incremental fashion. Existing approaches often suffer\nfrom excessive forgetting and imbalance of the scores assigned to classes that\nhave not been seen together during training. In this study, we introduce a\nnovel approach, Prediction Error-based Classification (PEC), which differs from\ntraditional discriminative and generative classification paradigms. PEC\ncomputes a class score by measuring the prediction error of a model trained to\nreplicate the outputs of a frozen random neural network on data from that\nclass. The method can be interpreted as approximating a classification rule\nbased on Gaussian Process posterior variance. PEC offers several practical\nadvantages, including sample efficiency, ease of tuning, and effectiveness even\nwhen data are presented one class at a time. Our empirical results show that\nPEC performs strongly in single-pass-through-data CIL, outperforming other\nrehearsal-free baselines in all cases and rehearsal-based methods with moderate\nreplay buffer size in most cases across multiple benchmarks.\n",
        "published": "2023",
        "authors": [
            "Micha\u0142 Zaj\u0105c",
            "Tinne Tuytelaars",
            "Gido M. van de Ven"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.04396v1",
        "title": "Improving Diffusion-based Image Translation using Asymmetric Gradient\n  Guidance",
        "abstract": "  Diffusion models have shown significant progress in image translation tasks\nrecently. However, due to their stochastic nature, there's often a trade-off\nbetween style transformation and content preservation. Current strategies aim\nto disentangle style and content, preserving the source image's structure while\nsuccessfully transitioning from a source to a target domain under text or\none-shot image conditions. Yet, these methods often require computationally\nintense fine-tuning of diffusion models or additional neural networks. To\naddress these challenges, here we present an approach that guides the reverse\nprocess of diffusion sampling by applying asymmetric gradient guidance. This\nresults in quicker and more stable image manipulation for both text-guided and\nimage-guided image translation. Our model's adaptability allows it to be\nimplemented with both image- and latent-diffusion models. Experiments show that\nour method outperforms various state-of-the-art models in image translation\ntasks.\n",
        "published": "2023",
        "authors": [
            "Gihyun Kwon",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.09933v2",
        "title": "Spuriosity Didn't Kill the Classifier: Using Invariant Predictions to\n  Harness Spurious Features",
        "abstract": "  To avoid failures on out-of-distribution data, recent works have sought to\nextract features that have an invariant or stable relationship with the label\nacross domains, discarding \"spurious\" or unstable features whose relationship\nwith the label changes across domains. However, unstable features often carry\ncomplementary information that could boost performance if used correctly in the\ntest domain. In this work, we show how this can be done without test-domain\nlabels. In particular, we prove that pseudo-labels based on stable features\nprovide sufficient guidance for doing so, provided that stable and unstable\nfeatures are conditionally independent given the label. Based on this\ntheoretical insight, we propose Stable Feature Boosting (SFB), an algorithm\nfor: (i) learning a predictor that separates stable and\nconditionally-independent unstable features; and (ii) using the stable-feature\npredictions to adapt the unstable-feature predictions in the test domain.\nTheoretically, we prove that SFB can learn an asymptotically-optimal predictor\nwithout test-domain labels. Empirically, we demonstrate the effectiveness of\nSFB on real and synthetic data.\n",
        "published": "2023",
        "authors": [
            "Cian Eastwood",
            "Shashank Singh",
            "Andrei Liviu Nicolicioiu",
            "Marin Vlastelica",
            "Julius von K\u00fcgelgen",
            "Bernhard Sch\u00f6lkopf"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.12673v2",
        "title": "On Sparse Modern Hopfield Model",
        "abstract": "  We introduce the sparse modern Hopfield model as a sparse extension of the\nmodern Hopfield model. Like its dense counterpart, the sparse modern Hopfield\nmodel equips a memory-retrieval dynamics whose one-step approximation\ncorresponds to the sparse attention mechanism. Theoretically, our key\ncontribution is a principled derivation of a closed-form sparse Hopfield energy\nusing the convex conjugate of the sparse entropic regularizer. Building upon\nthis, we derive the sparse memory retrieval dynamics from the sparse energy\nfunction and show its one-step approximation is equivalent to the\nsparse-structured attention. Importantly, we provide a sparsity-dependent\nmemory retrieval error bound which is provably tighter than its dense analog.\nThe conditions for the benefits of sparsity to arise are therefore identified\nand discussed. In addition, we show that the sparse modern Hopfield model\nmaintains the robust theoretical properties of its dense counterpart, including\nrapid fixed point convergence and exponential memory capacity. Empirically, we\nuse both synthetic and real-world datasets to demonstrate that the sparse\nHopfield model outperforms its dense counterpart in many situations.\n",
        "published": "2023",
        "authors": [
            "Jerry Yao-Chieh Hu",
            "Donglin Yang",
            "Dennis Wu",
            "Chenwei Xu",
            "Bo-Yu Chen",
            "Han Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.16143v1",
        "title": "Generative Semi-supervised Learning with Meta-Optimized Synthetic\n  Samples",
        "abstract": "  Semi-supervised learning (SSL) is a promising approach for training deep\nclassification models using labeled and unlabeled datasets. However, existing\nSSL methods rely on a large unlabeled dataset, which may not always be\navailable in many real-world applications due to legal constraints (e.g.,\nGDPR). In this paper, we investigate the research question: Can we train SSL\nmodels without real unlabeled datasets? Instead of using real unlabeled\ndatasets, we propose an SSL method using synthetic datasets generated from\ngenerative foundation models trained on datasets containing millions of samples\nin diverse domains (e.g., ImageNet). Our main concepts are identifying\nsynthetic samples that emulate unlabeled samples from generative foundation\nmodels and training classifiers using these synthetic samples. To achieve this,\nour method is formulated as an alternating optimization problem: (i)\nmeta-learning of generative foundation models and (ii) SSL of classifiers using\nreal labeled and synthetic unlabeled samples. For (i), we propose a\nmeta-learning objective that optimizes latent variables to generate samples\nthat resemble real labeled samples and minimize the validation loss. For (ii),\nwe propose a simple unsupervised loss function that regularizes the feature\nextractors of classifiers to maximize the performance improvement obtained from\nsynthetic samples. We confirm that our method outperforms baselines using\ngenerative foundation models on SSL. We also demonstrate that our methods\noutperform SSL using real unlabeled datasets in scenarios with extremely small\namounts of labeled datasets. This suggests that synthetic samples have the\npotential to provide improvement gains more efficiently than real unlabeled\ndata.\n",
        "published": "2023",
        "authors": [
            "Shin'ya Yamaguchi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.01107v1",
        "title": "Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image\n  Diffusion Models",
        "abstract": "  Recent endeavors in video editing have showcased promising results in\nsingle-attribute editing or style transfer tasks, either by training\ntext-to-video (T2V) models on text-video data or adopting training-free\nmethods. However, when confronted with the complexities of multi-attribute\nediting scenarios, they exhibit shortcomings such as omitting or overlooking\nintended attribute changes, modifying the wrong elements of the input video,\nand failing to preserve regions of the input video that should remain intact.\nTo address this, here we present a novel grounding-guided video-to-video\ntranslation framework called Ground-A-Video for multi-attribute video editing.\nGround-A-Video attains temporally consistent multi-attribute editing of input\nvideos in a training-free manner without aforementioned shortcomings. Central\nto our method is the introduction of Cross-Frame Gated Attention which\nincorporates groundings information into the latent representations in a\ntemporally consistent fashion, along with Modulated Cross-Attention and optical\nflow guided inverted latents smoothing. Extensive experiments and applications\ndemonstrate that Ground-A-Video's zero-shot capacity outperforms other baseline\nmethods in terms of edit-accuracy and frame consistency. Further results and\ncodes are provided at our project page (http://ground-a-video.github.io).\n",
        "published": "2023",
        "authors": [
            "Hyeonho Jeong",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.01110v1",
        "title": "Prompt-tuning latent diffusion models for inverse problems",
        "abstract": "  We propose a new method for solving imaging inverse problems using\ntext-to-image latent diffusion models as general priors. Existing methods using\nlatent diffusion models for inverse problems typically rely on simple null text\nprompts, which can lead to suboptimal performance. To address this limitation,\nwe introduce a method for prompt tuning, which jointly optimizes the text\nembedding on-the-fly while running the reverse diffusion process. This allows\nus to generate images that are more faithful to the diffusion prior. In\naddition, we propose a method to keep the evolution of latent variables within\nthe range space of the encoder, by projection. This helps to reduce image\nartifacts, a major problem when using latent diffusion models instead of\npixel-based diffusion models. Our combined method, called P2L, outperforms both\nimage- and latent-diffusion model-based inverse problem solvers on a variety of\ntasks, such as super-resolution, deblurring, and inpainting.\n",
        "published": "2023",
        "authors": [
            "Hyungjin Chung",
            "Jong Chul Ye",
            "Peyman Milanfar",
            "Mauricio Delbracio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.02279v1",
        "title": "Consistency Trajectory Models: Learning Probability Flow ODE Trajectory\n  of Diffusion",
        "abstract": "  Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion\nmodel sampling at the cost of sample quality but lack a natural way to\ntrade-off quality for speed. To address this limitation, we propose Consistency\nTrajectory Model (CTM), a generalization encompassing CM and score-based models\nas special cases. CTM trains a single neural network that can -- in a single\nforward pass -- output scores (i.e., gradients of log-density) and enables\nunrestricted traversal between any initial and final time along the Probability\nFlow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables\nthe efficient combination of adversarial training and denoising score matching\nloss to enhance performance and achieves new state-of-the-art FIDs for\nsingle-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at\n64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes,\nboth deterministic and stochastic, involving long jumps along the ODE solution\ntrajectories. It consistently improves sample quality as computational budgets\nincrease, avoiding the degradation seen in CM. Furthermore, CTM's access to the\nscore accommodates all diffusion model inference techniques, including exact\nlikelihood computation.\n",
        "published": "2023",
        "authors": [
            "Dongjun Kim",
            "Chieh-Hsin Lai",
            "Wei-Hsiang Liao",
            "Naoki Murata",
            "Yuhta Takida",
            "Toshimitsu Uesaka",
            "Yutong He",
            "Yuki Mitsufuji",
            "Stefano Ermon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.02712v1",
        "title": "ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space\n  NeRF",
        "abstract": "  Recently, there has been a significant advancement in text-to-image diffusion\nmodels, leading to groundbreaking performance in 2D image generation. These\nadvancements have been extended to 3D models, enabling the generation of novel\n3D objects from textual descriptions. This has evolved into NeRF editing\nmethods, which allow the manipulation of existing 3D objects through textual\nconditioning. However, existing NeRF editing techniques have faced limitations\nin their performance due to slow training speeds and the use of loss functions\nthat do not adequately consider editing. To address this, here we present a\nnovel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding\nreal-world scenes into the latent space of the latent diffusion model (LDM)\nthrough a unique refinement layer. This approach enables us to obtain a NeRF\nbackbone that is not only faster but also more amenable to editing compared to\ntraditional image space NeRF editing. Furthermore, we propose an improved loss\nfunction tailored for editing by migrating the delta denoising score (DDS)\ndistillation loss, originally used in 2D image editing to the three-dimensional\ndomain. This novel loss function surpasses the well-known score distillation\nsampling (SDS) loss in terms of suitability for editing purposes. Our\nexperimental results demonstrate that ED-NeRF achieves faster editing speed\nwhile producing improved output quality compared to state-of-the-art 3D editing\nmodels.\n",
        "published": "2023",
        "authors": [
            "Jangho Park",
            "Gihyun Kwon",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.04285v1",
        "title": "Assessing Robustness via Score-Based Adversarial Image Generation",
        "abstract": "  Most adversarial attacks and defenses focus on perturbations within small\n$\\ell_p$-norm constraints. However, $\\ell_p$ threat models cannot capture all\nrelevant semantic-preserving perturbations, and hence, the scope of robustness\nevaluations is limited. In this work, we introduce Score-Based Adversarial\nGeneration (ScoreAG), a novel framework that leverages the advancements in\nscore-based generative models to generate adversarial examples beyond\n$\\ell_p$-norm constraints, so-called unrestricted adversarial examples,\novercoming their limitations. Unlike traditional methods, ScoreAG maintains the\ncore semantics of images while generating realistic adversarial examples,\neither by transforming existing images or synthesizing new ones entirely from\nscratch. We further exploit the generative capability of ScoreAG to purify\nimages, empirically enhancing the robustness of classifiers. Our extensive\nempirical evaluation demonstrates that ScoreAG matches the performance of\nstate-of-the-art attacks and defenses across multiple benchmarks. This work\nhighlights the importance of investigating adversarial examples bounded by\nsemantics rather than $\\ell_p$-norm constraints. ScoreAG represents an\nimportant step towards more encompassing robustness assessments.\n",
        "published": "2023",
        "authors": [
            "Marcel Kollovieh",
            "Lukas Gosch",
            "Yan Scholten",
            "Marten Lienen",
            "Stephan G\u00fcnnemann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.06823v2",
        "title": "NECO: NEural Collapse Based Out-of-distribution detection",
        "abstract": "  Detecting out-of-distribution (OOD) data is a critical challenge in machine\nlearning due to model overconfidence, often without awareness of their\nepistemological limits. We hypothesize that ``neural collapse'', a phenomenon\naffecting in-distribution data for models trained beyond loss convergence, also\ninfluences OOD data. To benefit from this interplay, we introduce NECO, a novel\npost-hoc method for OOD detection, which leverages the geometric properties of\n``neural collapse'' and of principal component spaces to identify OOD data. Our\nextensive experiments demonstrate that NECO achieves state-of-the-art results\non both small and large-scale OOD detection tasks while exhibiting strong\ngeneralization capabilities across different network architectures.\nFurthermore, we provide a theoretical explanation for the effectiveness of our\nmethod in OOD detection. We plan to release the code after the anonymity\nperiod.\n",
        "published": "2023",
        "authors": [
            "Mou\u00efn Ben Ammar",
            "Nacim Belkhir",
            "Sebastian Popescu",
            "Antoine Manzanera",
            "Gianni Franchi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.07894v1",
        "title": "Efficient Integrators for Diffusion Generative Models",
        "abstract": "  Diffusion models suffer from slow sample generation at inference time.\nTherefore, developing a principled framework for fast deterministic/stochastic\nsampling for a broader class of diffusion models is a promising direction. We\npropose two complementary frameworks for accelerating sample generation in\npre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate\nintegrators generalize DDIM, mapping the reverse diffusion dynamics to a more\namenable space for sampling. In contrast, splitting-based integrators, commonly\nused in molecular dynamics, reduce the numerical simulation error by cleverly\nalternating between numerical updates involving the data and auxiliary\nvariables. After extensively studying these methods empirically and\ntheoretically, we present a hybrid method that leads to the best-reported\nperformance for diffusion models in augmented spaces. Applied to Phase Space\nLangevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and\nstochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network\nfunction evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing\nbaselines, respectively. Our code and model checkpoints will be made publicly\navailable at \\url{https://github.com/mandt-lab/PSLD}.\n",
        "published": "2023",
        "authors": [
            "Kushagra Pandey",
            "Maja Rudolph",
            "Stephan Mandt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.10375v1",
        "title": "GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers",
        "abstract": "  As transformers are equivariant to the permutation of input tokens, encoding\nthe positional information of tokens is necessary for many tasks. However,\nsince existing positional encoding schemes have been initially designed for NLP\ntasks, their suitability for vision tasks, which typically exhibit different\nstructural properties in their data, is questionable. We argue that existing\npositional encoding schemes are suboptimal for 3D vision tasks, as they do not\nrespect their underlying 3D geometric structure. Based on this hypothesis, we\npropose a geometry-aware attention mechanism that encodes the geometric\nstructure of tokens as relative transformation determined by the geometric\nrelationship between queries and key-value pairs. By evaluating on multiple\nnovel view synthesis (NVS) datasets in the sparse wide-baseline multi-view\nsetting, we show that our attention, called Geometric Transform Attention\n(GTA), improves learning efficiency and performance of state-of-the-art\ntransformer-based NVS models without any additional learned parameters and only\nminor computational overhead.\n",
        "published": "2023",
        "authors": [
            "Takeru Miyato",
            "Bernhard Jaeger",
            "Max Welling",
            "Andreas Geiger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.16221v4",
        "title": "Hierarchical Randomized Smoothing",
        "abstract": "  Real-world data is complex and often consists of objects that can be\ndecomposed into multiple entities (e.g. images into pixels, graphs into\ninterconnected nodes). Randomized smoothing is a powerful framework for making\nmodels provably robust against small changes to their inputs - by guaranteeing\nrobustness of the majority vote when randomly adding noise before\nclassification. Yet, certifying robustness on such complex data via randomized\nsmoothing is challenging when adversaries do not arbitrarily perturb entire\nobjects (e.g. images) but only a subset of their entities (e.g. pixels). As a\nsolution, we introduce hierarchical randomized smoothing: We partially smooth\nobjects by adding random noise only on a randomly selected subset of their\nentities. By adding noise in a more targeted manner than existing methods we\nobtain stronger robustness guarantees while maintaining high accuracy. We\ninitialize hierarchical smoothing using different noising distributions,\nyielding novel robustness certificates for discrete and continuous domains. We\nexperimentally demonstrate the importance of hierarchical smoothing in image\nand node classification, where it yields superior robustness-accuracy\ntrade-offs. Overall, hierarchical smoothing is an important contribution\ntowards models that are both - certifiably robust to perturbations and\naccurate.\n",
        "published": "2023",
        "authors": [
            "Yan Scholten",
            "Jan Schuchardt",
            "Aleksandar Bojchevski",
            "Stephan G\u00fcnnemann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.19360v1",
        "title": "Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from\n  a Minimax Game Perspective",
        "abstract": "  Adversarial Training (AT) has become arguably the state-of-the-art algorithm\nfor extracting robust features. However, researchers recently notice that AT\nsuffers from severe robust overfitting problems, particularly after learning\nrate (LR) decay. In this paper, we explain this phenomenon by viewing\nadversarial training as a dynamic minimax game between the model trainer and\nthe attacker. Specifically, we analyze how LR decay breaks the balance between\nthe minimax game by empowering the trainer with a stronger memorization\nability, and show such imbalance induces robust overfitting as a result of\nmemorizing non-robust features. We validate this understanding with extensive\nexperiments, and provide a holistic view of robust overfitting from the\ndynamics of both the two game players. This understanding further inspires us\nto alleviate robust overfitting by rebalancing the two players by either\nregularizing the trainer's capacity or improving the attack strength.\nExperiments show that the proposed ReBalanced Adversarial Training (ReBAT) can\nattain good robustness and does not suffer from robust overfitting even after\nvery long training. Code is available at https://github.com/PKU-ML/ReBAT.\n",
        "published": "2023",
        "authors": [
            "Yifei Wang",
            "Liangchen Li",
            "Jiansheng Yang",
            "Zhouchen Lin",
            "Yisen Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.04163v1",
        "title": "Outliers with Opposing Signals Have an Outsized Effect on Neural Network\n  Optimization",
        "abstract": "  We identify a new phenomenon in neural network optimization which arises from\nthe interaction of depth and a particular heavy-tailed structure in natural\ndata. Our result offers intuitive explanations for several previously reported\nobservations about network training dynamics. In particular, it implies a\nconceptually new cause for progressive sharpening and the edge of stability; we\nalso highlight connections to other concepts in optimization and generalization\nincluding grokking, simplicity bias, and Sharpness-Aware Minimization.\n  Experimentally, we demonstrate the significant influence of paired groups of\noutliers in the training data with strong opposing signals: consistent, large\nmagnitude features which dominate the network output throughout training and\nprovide gradients which point in opposite directions. Due to these outliers,\nearly optimization enters a narrow valley which carefully balances the opposing\ngroups; subsequent sharpening causes their loss to rise rapidly, oscillating\nbetween high on one group and then the other, until the overall loss spikes. We\ndescribe how to identify these groups, explore what sets them apart, and\ncarefully study their effect on the network's optimization and behavior. We\ncomplement these experiments with a mechanistic explanation on a toy example of\nopposing signals and a theoretical analysis of a two-layer linear network on a\nsimple model. Our finding enables new qualitative predictions of training\nbehavior which we confirm experimentally. It also provides a new lens through\nwhich to study and improve modern training practices for stochastic\noptimization, which we highlight via a case study of Adam versus SGD.\n",
        "published": "2023",
        "authors": [
            "Elan Rosenfeld",
            "Andrej Risteski"
        ]
    }
]