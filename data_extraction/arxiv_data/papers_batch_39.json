[
    {
        "id": "http://arxiv.org/abs/2112.15075v1",
        "title": "Pose Estimation of Specific Rigid Objects",
        "abstract": "  In this thesis, we address the problem of estimating the 6D pose of rigid\nobjects from a single RGB or RGB-D input image, assuming that 3D models of the\nobjects are available. This problem is of great importance to many application\nfields such as robotic manipulation, augmented reality, and autonomous driving.\nFirst, we propose EPOS, a method for 6D object pose estimation from an RGB\nimage. The key idea is to represent an object by compact surface fragments and\npredict the probability distribution of corresponding fragments at each pixel\nof the input image by a neural network. Each pixel is linked with a\ndata-dependent number of fragments, which allows systematic handling of\nsymmetries, and the 6D poses are estimated from the links by a RANSAC-based\nfitting method. EPOS outperformed all RGB and most RGB-D and D methods on\nseveral standard datasets. Second, we present HashMatch, an RGB-D method that\nslides a window over the input image and searches for a match against\ntemplates, which are pre-generated by rendering 3D object models in different\norientations. The method applies a cascade of evaluation stages to each window\nlocation, which avoids exhaustive matching against all templates. Third, we\npropose ObjectSynth, an approach to synthesize photorealistic images of 3D\nobject models for training methods based on neural networks. The images yield\nsubstantial improvements compared to commonly used images of objects rendered\non top of random photographs. Fourth, we introduce T-LESS, the first dataset\nfor 6D object pose estimation that includes 3D models and RGB-D images of\nindustry-relevant objects. Fifth, we define BOP, a benchmark that captures the\nstatus quo in the field. BOP comprises eleven datasets in a unified format, an\nevaluation methodology, an online evaluation system, and public challenges held\nat international workshops organized at the ICCV and ECCV conferences.\n",
        "published": "2021",
        "authors": [
            "Tomas Hodan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.10266v1",
        "title": "Combining Commonsense Reasoning and Knowledge Acquisition to Guide Deep\n  Learning in Robotics",
        "abstract": "  Algorithms based on deep network models are being used for many pattern\nrecognition and decision-making tasks in robotics and AI. Training these models\nrequires a large labeled dataset and considerable computational resources,\nwhich are not readily available in many domains. Also, it is difficult to\nexplore the internal representations and reasoning mechanisms of these models.\nAs a step towards addressing the underlying knowledge representation,\nreasoning, and learning challenges, the architecture described in this paper\ndraws inspiration from research in cognitive systems. As a motivating example,\nwe consider an assistive robot trying to reduce clutter in any given scene by\nreasoning about the occlusion of objects and stability of object configurations\nin an image of the scene. In this context, our architecture incrementally\nlearns and revises a grounding of the spatial relations between objects and\nuses this grounding to extract spatial information from input images.\nNon-monotonic logical reasoning with this information and incomplete\ncommonsense domain knowledge is used to make decisions about stability and\nocclusion. For images that cannot be processed by such reasoning, regions\nrelevant to the tasks at hand are automatically identified and used to train\ndeep network models to make the desired decisions. Image regions used to train\nthe deep networks are also used to incrementally acquire previously unknown\nstate constraints that are merged with the existing knowledge for subsequent\nreasoning. Experimental evaluation performed using simulated and real-world\nimages indicates that in comparison with baselines based just on deep networks,\nour architecture improves reliability of decision making and reduces the effort\ninvolved in training data-driven deep network models.\n",
        "published": "2022",
        "authors": [
            "Mohan Sridharan",
            "Tiago Mota"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.02959v1",
        "title": "A Perspective on Robotic Telepresence and Teleoperation using Cognition:\n  Are we there yet?",
        "abstract": "  Telepresence and teleoperation robotics have attracted a great amount of\nattention in the last 10 years. With the Artificial Intelligence (AI)\nrevolution already being started, we can see a wide range of robotic\napplications being realized. Intelligent robotic systems are being deployed\nboth in industrial and domestic environments. Telepresence is the idea of being\npresent in a remote location virtually or via robotic avatars. Similarly, the\nidea of operating a robot from a remote location for various tasks is called\nteleoperation. These technologies find significant application in health care,\neducation, surveillance, disaster recovery, and corporate/government sectors.\nBut question still remains about their maturity, security and safety levels. We\nalso need to think about enhancing the user experience and trust in such\ntechnologies going into the next generation of computing.\n",
        "published": "2022",
        "authors": [
            "Hrishav Bakul Barua",
            "Ashis Sau",
            "Ruddra dev Roychoudhury"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.09138v1",
        "title": "RangeUDF: Semantic Surface Reconstruction from 3D Point Clouds",
        "abstract": "  We present RangeUDF, a new implicit representation based framework to recover\nthe geometry and semantics of continuous 3D scene surfaces from point clouds.\nUnlike occupancy fields or signed distance fields which can only model closed\n3D surfaces, our approach is not restricted to any type of topology. Being\ndifferent from the existing unsigned distance fields, our framework does not\nsuffer from any surface ambiguity. In addition, our RangeUDF can jointly\nestimate precise semantics for continuous surfaces. The key to our approach is\na range-aware unsigned distance function together with a surface-oriented\nsemantic segmentation module. Extensive experiments show that RangeUDF clearly\nsurpasses state-of-the-art approaches for surface reconstruction on four point\ncloud datasets. Moreover, RangeUDF demonstrates superior generalization\ncapability across multiple unseen datasets, which is nearly impossible for all\nexisting approaches.\n",
        "published": "2022",
        "authors": [
            "Bing Wang",
            "Zhengdi Yu",
            "Bo Yang",
            "Jie Qin",
            "Toby Breckon",
            "Ling Shao",
            "Niki Trigoni",
            "Andrew Markham"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.02834v1",
        "title": "Fixing Malfunctional Objects With Learned Physical Simulation and\n  Functional Prediction",
        "abstract": "  This paper studies the problem of fixing malfunctional 3D objects. While\nprevious works focus on building passive perception models to learn the\nfunctionality from static 3D objects, we argue that functionality is reckoned\nwith respect to the physical interactions between the object and the user.\nGiven a malfunctional object, humans can perform mental simulations to reason\nabout its functionality and figure out how to fix it. Inspired by this, we\npropose FixIt, a dataset that contains about 5k poorly-designed 3D physical\nobjects paired with choices to fix them. To mimic humans' mental simulation\nprocess, we present FixNet, a novel framework that seamlessly incorporates\nperception and physical dynamics. Specifically, FixNet consists of a perception\nmodule to extract the structured representation from the 3D point cloud, a\nphysical dynamics prediction module to simulate the results of interactions on\n3D objects, and a functionality prediction module to evaluate the functionality\nand choose the correct fix. Experimental results show that our framework\noutperforms baseline models by a large margin, and can generalize well to\nobjects with similar interaction types.\n",
        "published": "2022",
        "authors": [
            "Yining Hong",
            "Kaichun Mo",
            "Li Yi",
            "Leonidas J. Guibas",
            "Antonio Torralba",
            "Joshua B. Tenenbaum",
            "Chuang Gan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.05678v1",
        "title": "RISP: Rendering-Invariant State Predictor with Differentiable Simulation\n  and Rendering for Cross-Domain Parameter Estimation",
        "abstract": "  This work considers identifying parameters characterizing a physical system's\ndynamic motion directly from a video whose rendering configurations are\ninaccessible. Existing solutions require massive training data or lack\ngeneralizability to unknown rendering configurations. We propose a novel\napproach that marries domain randomization and differentiable rendering\ngradients to address this problem. Our core idea is to train a\nrendering-invariant state-prediction (RISP) network that transforms image\ndifferences into state differences independent of rendering configurations,\ne.g., lighting, shadows, or material reflectance. To train this predictor, we\nformulate a new loss on rendering variances using gradients from differentiable\nrendering. Moreover, we present an efficient, second-order method to compute\nthe gradients of this loss, allowing it to be integrated seamlessly into modern\ndeep learning frameworks. We evaluate our method in rigid-body and\ndeformable-body simulation environments using four tasks: state estimation,\nsystem identification, imitation learning, and visuomotor control. We further\ndemonstrate the efficacy of our approach on a real-world example: inferring the\nstate and action sequences of a quadrotor from a video of its motion sequences.\nCompared with existing methods, our approach achieves significantly lower\nreconstruction errors and has better generalizability among unknown rendering\nconfigurations.\n",
        "published": "2022",
        "authors": [
            "Pingchuan Ma",
            "Tao Du",
            "Joshua B. Tenenbaum",
            "Wojciech Matusik",
            "Chuang Gan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.11053v2",
        "title": "Surgical-VQA: Visual Question Answering in Surgical Scenes using\n  Transformer",
        "abstract": "  Visual question answering (VQA) in surgery is largely unexplored. Expert\nsurgeons are scarce and are often overloaded with clinical and academic\nworkloads. This overload often limits their time answering questionnaires from\npatients, medical students or junior residents related to surgical procedures.\nAt times, students and junior residents also refrain from asking too many\nquestions during classes to reduce disruption. While computer-aided simulators\nand recording of past surgical procedures have been made available for them to\nobserve and improve their skills, they still hugely rely on medical experts to\nanswer their questions. Having a Surgical-VQA system as a reliable 'second\nopinion' could act as a backup and ease the load on the medical experts in\nanswering these questions. The lack of annotated medical data and the presence\nof domain-specific terms has limited the exploration of VQA for surgical\nprocedures. In this work, we design a Surgical-VQA task that answers\nquestionnaires on surgical procedures based on the surgical scene. Extending\nthe MICCAI endoscopic vision challenge 2018 dataset and workflow recognition\ndataset further, we introduce two Surgical-VQA datasets with classification and\nsentence-based answers. To perform Surgical-VQA, we employ vision-text\ntransformers models. We further introduce a residual MLP-based VisualBert\nencoder model that enforces interaction between visual and text tokens,\nimproving performance in classification-based answering. Furthermore, we study\nthe influence of the number of input image patches and temporal visual features\non the model performance in both classification and sentence-based answering.\n",
        "published": "2022",
        "authors": [
            "Lalithkumar Seenivasan",
            "Mobarakol Islam",
            "Adithya K Krishna",
            "Hongliang Ren"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.11623v1",
        "title": "Waypoint Generation in Row-based Crops with Deep Learning and\n  Contrastive Clustering",
        "abstract": "  The development of precision agriculture has gradually introduced automation\nin the agricultural process to support and rationalize all the activities\nrelated to field management. In particular, service robotics plays a\npredominant role in this evolution by deploying autonomous agents able to\nnavigate in fields while executing different tasks without the need for human\nintervention, such as monitoring, spraying and harvesting. In this context,\nglobal path planning is the first necessary step for every robotic mission and\nensures that the navigation is performed efficiently and with complete field\ncoverage. In this paper, we propose a learning-based approach to tackle\nwaypoint generation for planning a navigation path for row-based crops,\nstarting from a top-view map of the region-of-interest. We present a novel\nmethodology for waypoint clustering based on a contrastive loss, able to\nproject the points to a separable latent space. The proposed deep neural\nnetwork can simultaneously predict the waypoint position and cluster assignment\nwith two specialized heads in a single forward pass. The extensive\nexperimentation on simulated and real-world images demonstrates that the\nproposed approach effectively solves the waypoint generation problem for both\nstraight and curved row-based crops, overcoming the limitations of previous\nstate-of-the-art methodologies.\n",
        "published": "2022",
        "authors": [
            "Francesco Salvetti",
            "Simone Angarano",
            "Mauro Martini",
            "Simone Cerrato",
            "Marcello Chiaberge"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.03081v1",
        "title": "DRL-ISP: Multi-Objective Camera ISP with Deep Reinforcement Learning",
        "abstract": "  In this paper, we propose a multi-objective camera ISP framework that\nutilizes Deep Reinforcement Learning (DRL) and camera ISP toolbox that consist\nof network-based and conventional ISP tools. The proposed DRL-based camera ISP\nframework iteratively selects a proper tool from the toolbox and applies it to\nthe image to maximize a given vision task-specific reward function. For this\npurpose, we implement total 51 ISP tools that include exposure correction,\ncolor-and-tone correction, white balance, sharpening, denoising, and the\nothers. We also propose an efficient DRL network architecture that can extract\nthe various aspects of an image and make a rigid mapping relationship between\nimages and a large number of actions. Our proposed DRL-based ISP framework\neffectively improves the image quality according to each vision task such as\nRAW-to-RGB image restoration, 2D object detection, and monocular depth\nestimation.\n",
        "published": "2022",
        "authors": [
            "Ukcheol Shin",
            "Kyunghyun Lee",
            "In So Kweon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.03444v2",
        "title": "Fairness and Bias in Robot Learning",
        "abstract": "  Machine learning has significantly enhanced the abilities of robots, enabling\nthem to perform a wide range of tasks in human environments and adapt to our\nuncertain real world. Recent works in various machine learning domains have\nhighlighted the importance of accounting for fairness to ensure that these\nalgorithms do not reproduce human biases and consequently lead to\ndiscriminatory outcomes. With robot learning systems increasingly performing\nmore and more tasks in our everyday lives, it is crucial to understand the\ninfluence of such biases to prevent unintended behavior toward certain groups\nof people. In this work, we present the first survey on fairness in robot\nlearning from an interdisciplinary perspective spanning technical, ethical, and\nlegal challenges. We propose a taxonomy for sources of bias and the resulting\ntypes of discrimination due to them. Using examples from different robot\nlearning domains, we examine scenarios of unfair outcomes and strategies to\nmitigate them. We present early advances in the field by covering different\nfairness definitions, ethical and legal considerations, and methods for fair\nrobot learning. With this work, we aim to pave the road for groundbreaking\ndevelopments in fair robot learning.\n",
        "published": "2022",
        "authors": [
            "Laura Londo\u00f1o",
            "Juana Valeria Hurtado",
            "Nora Hertz",
            "Philipp Kellmeyer",
            "Silja Voeneky",
            "Abhinav Valada"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.11569v1",
        "title": "Robots Enact Malignant Stereotypes",
        "abstract": "  Stereotypes, bias, and discrimination have been extensively documented in\nMachine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural\nLanguage Processing (NLP) [6], or both, in the case of large image and caption\nmodels such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias\nmanifests in robots that physically and autonomously act within the world. We\naudit one of several recently published CLIP-powered robotic manipulation\nmethods, presenting it with objects that have pictures of human faces on the\nsurface which vary across race and gender, alongside task descriptions that\ncontain terms associated with common stereotypes. Our experiments definitively\nshow robots acting out toxic stereotypes with respect to gender, race, and\nscientifically-discredited physiognomy, at scale. Furthermore, the audited\nmethods are less likely to recognize Women and People of Color. Our\ninterdisciplinary sociotechnical analysis synthesizes across fields and\napplications such as Science Technology and Society (STS), Critical Studies,\nHistory, Safety, Robotics, and AI. We find that robots powered by large\ndatasets and Dissolution Models (sometimes called \"foundation models\", e.g.\nCLIP) that contain humans risk physically amplifying malignant stereotypes in\ngeneral; and that merely correcting disparities will be insufficient for the\ncomplexity and scale of the problem. Instead, we recommend that robot learning\nmethods that physically manifest stereotypes or other harmful outcomes be\npaused, reworked, or even wound down when appropriate, until outcomes can be\nproven safe, effective, and just. Finally, we discuss comprehensive policy\nchanges and the potential of new interdisciplinary research on topics like\nIdentity Safety Assessment Frameworks and Design Justice to better understand\nand address these harms.\n",
        "published": "2022",
        "authors": [
            "Andrew Hundt",
            "William Agnew",
            "Vicky Zeng",
            "Severin Kacianka",
            "Matthew Gombolay"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.07227v2",
        "title": "DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images",
        "abstract": "  In this paper, we study the problem of 3D scene geometry decomposition and\nmanipulation from 2D views. By leveraging the recent implicit neural\nrepresentation techniques, particularly the appealing neural radiance fields,\nwe introduce an object field component to learn unique codes for all individual\nobjects in 3D space only from 2D supervision. The key to this component is a\nseries of carefully designed loss functions to enable every 3D point,\nespecially in non-occupied space, to be effectively optimized even without 3D\nlabels. In addition, we introduce an inverse query algorithm to freely\nmanipulate any specified 3D object shape in the learned scene representation.\nNotably, our manipulation algorithm can explicitly tackle key issues such as\nobject collisions and visual occlusions. Our method, called DM-NeRF, is among\nthe first to simultaneously reconstruct, decompose, manipulate and render\ncomplex 3D scenes in a single pipeline. Extensive experiments on three datasets\nclearly show that our method can accurately decompose all 3D objects from 2D\nviews, allowing any interested object to be freely manipulated in 3D space such\nas translation, rotation, size adjustment, and deformation.\n",
        "published": "2022",
        "authors": [
            "Bing Wang",
            "Lu Chen",
            "Bo Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.10481v1",
        "title": "BARReL: Bottleneck Attention for Adversarial Robustness in Vision-Based\n  Reinforcement Learning",
        "abstract": "  Robustness to adversarial perturbations has been explored in many areas of\ncomputer vision. This robustness is particularly relevant in vision-based\nreinforcement learning, as the actions of autonomous agents might be\nsafety-critic or impactful in the real world. We investigate the susceptibility\nof vision-based reinforcement learning agents to gradient-based adversarial\nattacks and evaluate a potential defense. We observe that Bottleneck Attention\nModules (BAM) included in CNN architectures can act as potential tools to\nincrease robustness against adversarial attacks. We show how learned attention\nmaps can be used to recover activations of a convolutional layer by restricting\nthe spatial activations to salient regions. Across a number of RL environments,\nBAM-enhanced architectures show increased robustness during inference. Finally,\nwe discuss potential future research directions.\n",
        "published": "2022",
        "authors": [
            "Eugene Bykovets",
            "Yannick Metz",
            "Mennatallah El-Assady",
            "Daniel A. Keim",
            "Joachim M. Buhmann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.14660v1",
        "title": "Unifying Evaluation of Machine Learning Safety Monitors",
        "abstract": "  With the increasing use of Machine Learning (ML) in critical autonomous\nsystems, runtime monitors have been developed to detect prediction errors and\nkeep the system in a safe state during operations. Monitors have been proposed\nfor different applications involving diverse perception tasks and ML models,\nand specific evaluation procedures and metrics are used for different contexts.\nThis paper introduces three unified safety-oriented metrics, representing the\nsafety benefits of the monitor (Safety Gain), the remaining safety gaps after\nusing it (Residual Hazard), and its negative impact on the system's performance\n(Availability Cost). To compute these metrics, one requires to define two\nreturn functions, representing how a given ML prediction will impact expected\nfuture rewards and hazards. Three use-cases (classification, drone landing, and\nautonomous driving) are used to demonstrate how metrics from the literature can\nbe expressed in terms of the proposed metrics. Experimental results on these\nexamples show how different evaluation choices impact the perceived performance\nof a monitor. As our formalism requires us to formulate explicit safety\nassumptions, it allows us to ensure that the evaluation conducted matches the\nhigh-level system requirements.\n",
        "published": "2022",
        "authors": [
            "Joris Guerin",
            "Raul Sena Ferreira",
            "Kevin Delmas",
            "J\u00e9r\u00e9mie Guiochet"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.03355v2",
        "title": "Generative Adversarial Super-Resolution at the Edge with Knowledge\n  Distillation",
        "abstract": "  Single-Image Super-Resolution can support robotic tasks in environments where\na reliable visual stream is required to monitor the mission, handle\nteleoperation or study relevant visual details. In this work, we propose an\nefficient Generative Adversarial Network model for real-time Super-Resolution,\ncalled EdgeSRGAN (code available at https://github.com/PIC4SeR/EdgeSRGAN). We\nadopt a tailored architecture of the original SRGAN and model quantization to\nboost the execution on CPU and Edge TPU devices, achieving up to 200 fps\ninference. We further optimize our model by distilling its knowledge to a\nsmaller version of the network and obtain remarkable improvements compared to\nthe standard training approach. Our experiments show that our fast and\nlightweight model preserves considerably satisfying image quality compared to\nheavier state-of-the-art models. Finally, we conduct experiments on image\ntransmission with bandwidth degradation to highlight the advantages of the\nproposed system for mobile robotic applications.\n",
        "published": "2022",
        "authors": [
            "Simone Angarano",
            "Francesco Salvetti",
            "Mauro Martini",
            "Marcello Chiaberge"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.14435v1",
        "title": "Out-of-Distribution Detection for LiDAR-based 3D Object Detection",
        "abstract": "  3D object detection is an essential part of automated driving, and deep\nneural networks (DNNs) have achieved state-of-the-art performance for this\ntask. However, deep models are notorious for assigning high confidence scores\nto out-of-distribution (OOD) inputs, that is, inputs that are not drawn from\nthe training distribution. Detecting OOD inputs is challenging and essential\nfor the safe deployment of models. OOD detection has been studied extensively\nfor the classification task, but it has not received enough attention for the\nobject detection task, specifically LiDAR-based 3D object detection. In this\npaper, we focus on the detection of OOD inputs for LiDAR-based 3D object\ndetection. We formulate what OOD inputs mean for object detection and propose\nto adapt several OOD detection methods for object detection. We accomplish this\nby our proposed feature extraction method. To evaluate OOD detection methods,\nwe develop a simple but effective technique of generating OOD objects for a\ngiven object detection model. Our evaluation based on the KITTI dataset shows\nthat different OOD detection methods have biases toward detecting specific OOD\nobjects. It emphasizes the importance of combined OOD detection methods and\nmore research in this direction.\n",
        "published": "2022",
        "authors": [
            "Chengjie Huang",
            "Van Duong Nguyen",
            "Vahdat Abdelzad",
            "Christopher Gus Mannes",
            "Luke Rowe",
            "Benjamin Therien",
            "Rick Salay",
            "Krzysztof Czarnecki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.02324v1",
        "title": "Promising or Elusive? Unsupervised Object Segmentation from Real-world\n  Single Images",
        "abstract": "  In this paper, we study the problem of unsupervised object segmentation from\nsingle images. We do not introduce a new algorithm, but systematically\ninvestigate the effectiveness of existing unsupervised models on challenging\nreal-world images. We firstly introduce four complexity factors to\nquantitatively measure the distributions of object- and scene-level biases in\nappearance and geometry for datasets with human annotations. With the aid of\nthese factors, we empirically find that, not surprisingly, existing\nunsupervised models catastrophically fail to segment generic objects in\nreal-world images, although they can easily achieve excellent performance on\nnumerous simple synthetic datasets, due to the vast gap in objectness biases\nbetween synthetic and real images. By conducting extensive experiments on\nmultiple groups of ablated real-world datasets, we ultimately find that the key\nfactors underlying the colossal failure of existing unsupervised models on\nreal-world images are the challenging distributions of object- and scene-level\nbiases in appearance and geometry. Because of this, the inductive biases\nintroduced in existing unsupervised models can hardly capture the diverse\nobject distributions. Our research results suggest that future work should\nexploit more explicit objectness biases in the network design.\n",
        "published": "2022",
        "authors": [
            "Yafei Yang",
            "Bo Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.06463v1",
        "title": "Holo-Dex: Teaching Dexterity with Immersive Mixed Reality",
        "abstract": "  A fundamental challenge in teaching robots is to provide an effective\ninterface for human teachers to demonstrate useful skills to a robot. This\nchallenge is exacerbated in dexterous manipulation, where teaching\nhigh-dimensional, contact-rich behaviors often require esoteric teleoperation\ntools. In this work, we present Holo-Dex, a framework for dexterous\nmanipulation that places a teacher in an immersive mixed reality through\ncommodity VR headsets. The high-fidelity hand pose estimator onboard the\nheadset is used to teleoperate the robot and collect demonstrations for a\nvariety of general-purpose dexterous tasks. Given these demonstrations, we use\npowerful feature learning combined with non-parametric imitation to train\ndexterous skills. Our experiments on six common dexterous tasks, including\nin-hand rotation, spinning, and bottle opening, indicate that Holo-Dex can both\ncollect high-quality demonstration data and train skills in a matter of hours.\nFinally, we find that our trained skills can exhibit generalization on objects\nnot seen in training. Videos of Holo-Dex are available at\nhttps://holo-dex.github.io.\n",
        "published": "2022",
        "authors": [
            "Sridhar Pandian Arunachalam",
            "Irmak G\u00fczey",
            "Soumith Chintala",
            "Lerrel Pinto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.09678v2",
        "title": "Virtual Reality via Object Pose Estimation and Active Learning:\n  Realizing Telepresence Robots with Aerial Manipulation Capabilities",
        "abstract": "  This article presents a novel telepresence system for advancing aerial\nmanipulation in dynamic and unstructured environments. The proposed system not\nonly features a haptic device, but also a virtual reality (VR) interface that\nprovides real-time 3D displays of the robot's workspace as well as a haptic\nguidance to its remotely located operator. To realize this, multiple sensors\nnamely a LiDAR, cameras and IMUs are utilized. For processing of the acquired\nsensory data, pose estimation pipelines are devised for industrial objects of\nboth known and unknown geometries. We further propose an active learning\npipeline in order to increase the sample efficiency of a pipeline component\nthat relies on Deep Neural Networks (DNNs) based object detection. All these\nalgorithms jointly address various challenges encountered during the execution\nof perception tasks in industrial scenarios. In the experiments, exhaustive\nablation studies are provided to validate the proposed pipelines.\nMethodologically, these results commonly suggest how an awareness of the\nalgorithms' own failures and uncertainty (`introspection') can be used tackle\nthe encountered problems. Moreover, outdoor experiments are conducted to\nevaluate the effectiveness of the overall system in enhancing aerial\nmanipulation capabilities. In particular, with flight campaigns over days and\nnights, from spring to winter, and with different users and locations, we\ndemonstrate over 70 robust executions of pick-and-place, force application and\npeg-in-hole tasks with the DLR cable-Suspended Aerial Manipulator (SAM). As a\nresult, we show the viability of the proposed system in future industrial\napplications.\n",
        "published": "2022",
        "authors": [
            "Jongseok Lee",
            "Ribin Balachandran",
            "Konstantin Kondak",
            "Andre Coelho",
            "Marco De Stefano",
            "Matthias Humt",
            "Jianxiang Feng",
            "Tamim Asfour",
            "Rudolph Triebel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.13540v2",
        "title": "Video based Object 6D Pose Estimation using Transformers",
        "abstract": "  We introduce a Transformer based 6D Object Pose Estimation framework\nVideoPose, comprising an end-to-end attention based modelling architecture,\nthat attends to previous frames in order to estimate accurate 6D Object Poses\nin videos. Our approach leverages the temporal information from a video\nsequence for pose refinement, along with being computationally efficient and\nrobust. Compared to existing methods, our architecture is able to capture and\nreason from long-range dependencies efficiently, thus iteratively refining over\nvideo sequences. Experimental evaluation on the YCB-Video dataset shows that\nour approach is on par with the state-of-the-art Transformer methods, and\nperforms significantly better relative to CNN based approaches. Further, with a\nspeed of 33 fps, it is also more efficient and therefore applicable to a\nvariety of applications that require real-time object pose estimation. Training\ncode and pretrained models are available at\nhttps://github.com/ApoorvaBeedu/VideoPose\n",
        "published": "2022",
        "authors": [
            "Apoorva Beedu",
            "Huda Alamri",
            "Irfan Essa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.15559v1",
        "title": "Robust Monocular Localization of Drones by Adapting Domain Maps to Depth\n  Prediction Inaccuracies",
        "abstract": "  We present a novel monocular localization framework by jointly training deep\nlearning-based depth prediction and Bayesian filtering-based pose reasoning.\nThe proposed cross-modal framework significantly outperforms deep learning-only\npredictions with respect to model scalability and tolerance to environmental\nvariations. Specifically, we show little-to-no degradation of pose accuracy\neven with extremely poor depth estimates from a lightweight depth predictor.\nOur framework also maintains high pose accuracy in extreme lighting variations\ncompared to standard deep learning, even without explicit domain adaptation. By\nopenly representing the map and intermediate feature maps (such as depth\nestimates), our framework also allows for faster updates and reusing\nintermediate predictions for other tasks, such as obstacle avoidance, resulting\nin much higher resource efficiency.\n",
        "published": "2022",
        "authors": [
            "Priyesh Shukla",
            "Sureshkumar S.",
            "Alex C. Stutts",
            "Sathya Ravi",
            "Theja Tulabandhula",
            "Amit R. Trivedi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.05237v1",
        "title": "SimuShips -- A High Resolution Simulation Dataset for Ship Detection\n  with Precise Annotations",
        "abstract": "  Obstacle detection is a fundamental capability of an autonomous maritime\nsurface vessel (AMSV). State-of-the-art obstacle detection algorithms are based\non convolutional neural networks (CNNs). While CNNs provide higher detection\naccuracy and fast detection speed, they require enormous amounts of data for\ntheir training. In particular, the availability of domain-specific datasets is\na challenge for obstacle detection. The difficulty in conducting onsite\nexperiments limits the collection of maritime datasets. Owing to the logistic\ncost of conducting on-site operations, simulation tools provide a safe and\ncost-efficient alternative for data collection. In this work, we introduce\nSimuShips, a publicly available simulation-based dataset for maritime\nenvironments. Our dataset consists of 9471 high-resolution (1920x1080) images\nwhich include a wide range of obstacle types, atmospheric and illumination\nconditions along with occlusion, scale and visible proportion variations. We\nprovide annotations in the form of bounding boxes. In addition, we conduct\nexperiments with YOLOv5 to test the viability of simulation data. Our\nexperiments indicate that the combination of real and simulated images improves\nthe recall for all classes by 2.9%.\n",
        "published": "2022",
        "authors": [
            "Minahil Raza",
            "Hanna Prokopova",
            "Samir Huseynzade",
            "Sepinoud Azimi",
            "Sebastien Lafond"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.05335v2",
        "title": "Scalable Modular Synthetic Data Generation for Advancing Aerial Autonomy",
        "abstract": "  One major barrier to advancing aerial autonomy has been collecting\nlarge-scale aerial datasets for training machine learning models. Due to costly\nand time-consuming real-world data collection through deploying drones, there\nhas been an increasing shift towards using synthetic data for training models\nin drone applications. However, to increase widespread generalization and\ntransferring models to real-world, increasing the diversity of simulation\nenvironments to train a model over all the varieties and augmenting the\ntraining data, has been proved to be essential. Current synthetic aerial data\ngeneration tools either lack data augmentation or rely heavily on manual\nworkload or real samples for configuring and generating diverse realistic\nsimulation scenes for data collection. These dependencies limit scalability of\nthe data generation workflow. Accordingly, there is a major challenge in\nbalancing generalizability and scalability in synthetic data generation. To\naddress these gaps, we introduce a scalable Aerial Synthetic Data Augmentation\n(ASDA) framework tailored to aerial autonomy applications. ASDA extends a\ncentral data collection engine with two scriptable pipelines that automatically\nperform scene and data augmentations to generate diverse aerial datasets for\ndifferent training tasks. ASDA improves data generation workflow efficiency by\nproviding a unified prompt-based interface over integrated pipelines for\nflexible control. The procedural generative approach of our data augmentation\nis performant and adaptable to different simulation environments, training\ntasks and data collection needs. We demonstrate the effectiveness of our method\nin automatically generating diverse datasets and show its potential for\ndownstream performance optimization.\n",
        "published": "2022",
        "authors": [
            "Mehrnaz Sabet",
            "Praveen Palanisamy",
            "Sakshi Mishra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.02051v2",
        "title": "A Distance-Geometric Method for Recovering Robot Joint Angles From an\n  RGB Image",
        "abstract": "  Autonomous manipulation systems operating in domains where human intervention\nis difficult or impossible (e.g., underwater, extraterrestrial or hazardous\nenvironments) require a high degree of robustness to sensing and communication\nfailures. Crucially, motion planning and control algorithms require a stream of\naccurate joint angle data provided by joint encoders, the failure of which may\nresult in an unrecoverable loss of functionality. In this paper, we present a\nnovel method for retrieving the joint angles of a robot manipulator using only\na single RGB image of its current configuration, opening up an avenue for\nrecovering system functionality when conventional proprioceptive sensing is\nunavailable. Our approach, based on a distance-geometric representation of the\nconfiguration space, exploits the knowledge of a robot's kinematic model with\nthe goal of training a shallow neural network that performs a 2D-to-3D\nregression of distances associated with detected structural keypoints. It is\nshown that the resulting Euclidean distance matrix uniquely corresponds to the\nobserved configuration, where joint angles can be recovered via\nmultidimensional scaling and a simple inverse kinematics procedure. We evaluate\nthe performance of our approach on real RGB images of a Franka Emika Panda\nmanipulator, showing that the proposed method is efficient and exhibits solid\ngeneralization ability. Furthermore, we show that our method can be easily\ncombined with a dense refinement technique to obtain superior results.\n",
        "published": "2023",
        "authors": [
            "Ivan Bili\u0107",
            "Filip Mari\u0107",
            "Ivan Markovi\u0107",
            "Ivan Petrovi\u0107"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.02207v1",
        "title": "Lightweight, Uncertainty-Aware Conformalized Visual Odometry",
        "abstract": "  Data-driven visual odometry (VO) is a critical subroutine for autonomous edge\nrobotics, and recent progress in the field has produced highly accurate point\npredictions in complex environments. However, emerging autonomous edge robotics\ndevices like insect-scale drones and surgical robots lack a computationally\nefficient framework to estimate VO's predictive uncertainties. Meanwhile, as\nedge robotics continue to proliferate into mission-critical application spaces,\nawareness of model's the predictive uncertainties has become crucial for\nrisk-aware decision-making. This paper addresses this challenge by presenting a\nnovel, lightweight, and statistically robust framework that leverages conformal\ninference (CI) to extract VO's uncertainty bands. Our approach represents the\nuncertainties using flexible, adaptable, and adjustable prediction intervals\nthat, on average, guarantee the inclusion of the ground truth across all\ndegrees of freedom (DOF) of pose estimation. We discuss the architectures of\ngenerative deep neural networks for estimating multivariate uncertainty bands\nalong with point (mean) prediction. We also present techniques to improve the\nuncertainty estimation accuracy, such as leveraging Monte Carlo dropout\n(MC-dropout) for data augmentation. Finally, we propose a novel training loss\nfunction that combines interval scoring and calibration loss with traditional\ntraining metrics--mean-squared error and KL-divergence--to improve\nuncertainty-aware learning. Our simulation results demonstrate that the\npresented framework consistently captures true uncertainty in pose estimations\nacross different datasets, estimation models, and applied noise types,\nindicating its wide applicability.\n",
        "published": "2023",
        "authors": [
            "Alex C. Stutts",
            "Danilo Erricolo",
            "Theja Tulabandhula",
            "Amit Ranjan Trivedi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.05512v1",
        "title": "PAC-NeRF: Physics Augmented Continuum Neural Radiance Fields for\n  Geometry-Agnostic System Identification",
        "abstract": "  Existing approaches to system identification (estimating the physical\nparameters of an object) from videos assume known object geometries. This\nprecludes their applicability in a vast majority of scenes where object\ngeometries are complex or unknown. In this work, we aim to identify parameters\ncharacterizing a physical system from a set of multi-view videos without any\nassumption on object geometry or topology. To this end, we propose \"Physics\nAugmented Continuum Neural Radiance Fields\" (PAC-NeRF), to estimate both the\nunknown geometry and physical parameters of highly dynamic objects from\nmulti-view videos. We design PAC-NeRF to only ever produce physically plausible\nstates by enforcing the neural radiance field to follow the conservation laws\nof continuum mechanics. For this, we design a hybrid Eulerian-Lagrangian\nrepresentation of the neural radiance field, i.e., we use the Eulerian grid\nrepresentation for NeRF density and color fields, while advecting the neural\nradiance fields via Lagrangian particles. This hybrid Eulerian-Lagrangian\nrepresentation seamlessly blends efficient neural rendering with the material\npoint method (MPM) for robust differentiable physics simulation. We validate\nthe effectiveness of our proposed framework on geometry and physical parameter\nestimation over a vast range of materials, including elastic bodies,\nplasticine, sand, Newtonian and non-Newtonian fluids, and demonstrate\nsignificant performance gain on most tasks.\n",
        "published": "2023",
        "authors": [
            "Xuan Li",
            "Yi-Ling Qiao",
            "Peter Yichen Chen",
            "Krishna Murthy Jatavallabhula",
            "Ming Lin",
            "Chenfanfu Jiang",
            "Chuang Gan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.09555v1",
        "title": "SoftZoo: A Soft Robot Co-design Benchmark For Locomotion In Diverse\n  Environments",
        "abstract": "  While significant research progress has been made in robot learning for\ncontrol, unique challenges arise when simultaneously co-optimizing morphology.\nExisting work has typically been tailored for particular environments or\nrepresentations. In order to more fully understand inherent design and\nperformance tradeoffs and accelerate the development of new breeds of soft\nrobots, a comprehensive virtual platform with well-established tasks,\nenvironments, and evaluation metrics is needed. In this work, we introduce\nSoftZoo, a soft robot co-design platform for locomotion in diverse\nenvironments. SoftZoo supports an extensive, naturally-inspired material set,\nincluding the ability to simulate environments such as flat ground, desert,\nwetland, clay, ice, snow, shallow water, and ocean. Further, it provides a\nvariety of tasks relevant for soft robotics, including fast locomotion, agile\nturning, and path following, as well as differentiable design representations\nfor morphology and control. Combined, these elements form a feature-rich\nplatform for analysis and development of soft robot co-design algorithms. We\nbenchmark prevalent representations and co-design algorithms, and shed light on\n1) the interplay between environment, morphology, and behavior; 2) the\nimportance of design space representations; 3) the ambiguity in muscle\nformation and controller synthesis; and 4) the value of differentiable physics.\nWe envision that SoftZoo will serve as a standard platform and template an\napproach toward the development of novel representations and algorithms for\nco-designing soft robots' behavioral and morphological intelligence.\n",
        "published": "2023",
        "authors": [
            "Tsun-Hsuan Wang",
            "Pingchuan Ma",
            "Andrew Everett Spielberg",
            "Zhou Xian",
            "Hao Zhang",
            "Joshua B. Tenenbaum",
            "Daniela Rus",
            "Chuang Gan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.00947v2",
        "title": "RePAST: Relative Pose Attention Scene Representation Transformer",
        "abstract": "  The Scene Representation Transformer (SRT) is a recent method to render novel\nviews at interactive rates. Since SRT uses camera poses with respect to an\narbitrarily chosen reference camera, it is not invariant to the order of the\ninput views. As a result, SRT is not directly applicable to large-scale scenes\nwhere the reference frame would need to be changed regularly. In this work, we\npropose Relative Pose Attention SRT (RePAST): Instead of fixing a reference\nframe at the input, we inject pairwise relative camera pose information\ndirectly into the attention mechanism of the Transformers. This leads to a\nmodel that is by definition invariant to the choice of any global reference\nframe, while still retaining the full capabilities of the original method.\nEmpirical results show that adding this invariance to the model does not lead\nto a loss in quality. We believe that this is a step towards applying fully\nlatent transformer-based rendering methods to large-scale scenes.\n",
        "published": "2023",
        "authors": [
            "Aleksandr Safin",
            "Daniel Duckworth",
            "Mehdi S. M. Sajjadi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.09788v1",
        "title": "Codesign of Edge Intelligence and Automated Guided Vehicle Control",
        "abstract": "  This work presents a harmonic design of autonomous guided vehicle (AGV)\ncontrol, edge intelligence, and human input to enable autonomous transportation\nin industrial environments. The AGV has the capability to navigate between a\nsource and destinations and pick/place objects. The human input implicitly\nprovides preferences of the destination and exact drop point, which are derived\nfrom an artificial intelligence (AI) module at the network edge and shared with\nthe AGV over a wireless network. The demonstration indicates that the proposed\nintegrated design of hardware, software, and AI design achieve a technology\nreadiness level (TRL) of range 4-5\n",
        "published": "2023",
        "authors": [
            "Malith Gallage",
            "Rafaela Scaciota",
            "Sumudu Samarakoon",
            "Mehdi Bennis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.05682v1",
        "title": "Lightweight Monocular Depth Estimation via Token-Sharing Transformer",
        "abstract": "  Depth estimation is an important task in various robotics systems and\napplications. In mobile robotics systems, monocular depth estimation is\ndesirable since a single RGB camera can be deployable at a low cost and compact\nsize. Due to its significant and growing needs, many lightweight monocular\ndepth estimation networks have been proposed for mobile robotics systems. While\nmost lightweight monocular depth estimation methods have been developed using\nconvolution neural networks, the Transformer has been gradually utilized in\nmonocular depth estimation recently. However, massive parameters and large\ncomputational costs in the Transformer disturb the deployment to embedded\ndevices. In this paper, we present a Token-Sharing Transformer (TST), an\narchitecture using the Transformer for monocular depth estimation, optimized\nespecially in embedded devices. The proposed TST utilizes global token sharing,\nwhich enables the model to obtain an accurate depth prediction with high\nthroughput in embedded devices. Experimental results show that TST outperforms\nthe existing lightweight monocular depth estimation methods. On the NYU Depth\nv2 dataset, TST can deliver depth maps up to 63.4 FPS in NVIDIA Jetson nano and\n142.6 FPS in NVIDIA Jetson TX2, with lower errors than the existing methods.\nFurthermore, TST achieves real-time depth estimation of high-resolution images\non Jetson TX2 with competitive results.\n",
        "published": "2023",
        "authors": [
            "Dong-Jae Lee",
            "Jae Young Lee",
            "Hyounguk Shon",
            "Eojindl Yi",
            "Yeong-Hun Park",
            "Sung-Sik Cho",
            "Junmo Kim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.02751v2",
        "title": "NeRFs: The Search for the Best 3D Representation",
        "abstract": "  Neural Radiance Fields or NeRFs have become the representation of choice for\nproblems in view synthesis or image-based rendering, as well as in many other\napplications across computer graphics and vision, and beyond. At their core,\nNeRFs describe a new representation of 3D scenes or 3D geometry. Instead of\nmeshes, disparity maps, multiplane images or even voxel grids, they represent\nthe scene as a continuous volume, with volumetric parameters like\nview-dependent radiance and volume density obtained by querying a neural\nnetwork. The NeRF representation has now been widely used, with thousands of\npapers extending or building on it every year, multiple authors and websites\nproviding overviews and surveys, and numerous industrial applications and\nstartup companies. In this article, we briefly review the NeRF representation,\nand describe the three decades-long quest to find the best 3D representation\nfor view synthesis and related problems, culminating in the NeRF papers. We\nthen describe new developments in terms of NeRF representations and make some\nobservations and insights regarding the future of 3D representations.\n",
        "published": "2023",
        "authors": [
            "Ravi Ramamoorthi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.06173v1",
        "title": "Physical Adversarial Attacks For Camera-based Smart Systems: Current\n  Trends, Categorization, Applications, Research Challenges, and Future Outlook",
        "abstract": "  In this paper, we present a comprehensive survey of the current trends\nfocusing specifically on physical adversarial attacks. We aim to provide a\nthorough understanding of the concept of physical adversarial attacks,\nanalyzing their key characteristics and distinguishing features. Furthermore,\nwe explore the specific requirements and challenges associated with executing\nattacks in the physical world. Our article delves into various physical\nadversarial attack methods, categorized according to their target tasks in\ndifferent applications, including classification, detection, face recognition,\nsemantic segmentation and depth estimation. We assess the performance of these\nattack methods in terms of their effectiveness, stealthiness, and robustness.\nWe examine how each technique strives to ensure the successful manipulation of\nDNNs while mitigating the risk of detection and withstanding real-world\ndistortions. Lastly, we discuss the current challenges and outline potential\nfuture research directions in the field of physical adversarial attacks. We\nhighlight the need for enhanced defense mechanisms, the exploration of novel\nattack strategies, the evaluation of attacks in different application domains,\nand the establishment of standardized benchmarks and evaluation criteria for\nphysical adversarial attacks. Through this comprehensive survey, we aim to\nprovide a valuable resource for researchers, practitioners, and policymakers to\ngain a holistic understanding of physical adversarial attacks in computer\nvision and facilitate the development of robust and secure DNN-based systems.\n",
        "published": "2023",
        "authors": [
            "Amira Guesmi",
            "Muhammad Abdullah Hanif",
            "Bassem Ouni",
            "Muhammed Shafique"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.10748v1",
        "title": "SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction",
        "abstract": "  Recent hand-object interaction datasets show limited real object variability\nand rely on fitting the MANO parametric model to obtain groundtruth hand\nshapes. To go beyond these limitations and spur further research, we introduce\nthe SHOWMe dataset which consists of 96 videos, annotated with real and\ndetailed hand-object 3D textured meshes. Following recent work, we consider a\nrigid hand-object scenario, in which the pose of the hand with respect to the\nobject remains constant during the whole video sequence. This assumption allows\nus to register sub-millimetre-precise groundtruth 3D scans to the image\nsequences in SHOWMe. Although simpler, this hypothesis makes sense in terms of\napplications where the required accuracy and level of detail is important eg.,\nobject hand-over in human-robot collaboration, object scanning, or manipulation\nand contact point analysis. Importantly, the rigidity of the hand-object\nsystems allows to tackle video-based 3D reconstruction of unknown hand-held\nobjects using a 2-stage pipeline consisting of a rigid registration step\nfollowed by a multi-view reconstruction (MVR) part. We carefully evaluate a set\nof non-trivial baselines for these two stages and show that it is possible to\nachieve promising object-agnostic 3D hand-object reconstructions employing an\nSfM toolbox or a hand pose estimator to recover the rigid transforms and\noff-the-shelf MVR algorithms. However, these methods remain sensitive to the\ninitial camera pose estimates which might be imprecise due to lack of textures\non the objects or heavy occlusions of the hands, leaving room for improvements\nin the reconstruction. Code and dataset are available at\nhttps://europe.naverlabs.com/research/showme\n",
        "published": "2023",
        "authors": [
            "Anilkumar Swamy",
            "Vincent Leroy",
            "Philippe Weinzaepfel",
            "Fabien Baradel",
            "Salma Galaaoui",
            "Romain Bregier",
            "Matthieu Armando",
            "Jean-Sebastien Franco",
            "Gregory Rogez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.06020v1",
        "title": "DyST: Towards Dynamic Neural Scene Representations on Real-World Videos",
        "abstract": "  Visual understanding of the world goes beyond the semantics and flat\nstructure of individual images. In this work, we aim to capture both the 3D\nstructure and dynamics of real-world scenes from monocular real-world videos.\nOur Dynamic Scene Transformer (DyST) model leverages recent work in neural\nscene representation to learn a latent decomposition of monocular real-world\nvideos into scene content, per-view scene dynamics, and camera pose. This\nseparation is achieved through a novel co-training scheme on monocular videos\nand our new synthetic dataset DySO. DyST learns tangible latent representations\nfor dynamic scenes that enable view generation with separate control over the\ncamera and the content of the scene.\n",
        "published": "2023",
        "authors": [
            "Maximilian Seitzer",
            "Sjoerd van Steenkiste",
            "Thomas Kipf",
            "Klaus Greff",
            "Mehdi S. M. Sajjadi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.06138v1",
        "title": "Layout Sequence Prediction From Noisy Mobile Modality",
        "abstract": "  Trajectory prediction plays a vital role in understanding pedestrian movement\nfor applications such as autonomous driving and robotics. Current trajectory\nprediction models depend on long, complete, and accurately observed sequences\nfrom visual modalities. Nevertheless, real-world situations often involve\nobstructed cameras, missed objects, or objects out of sight due to\nenvironmental factors, leading to incomplete or noisy trajectories. To overcome\nthese limitations, we propose LTrajDiff, a novel approach that treats objects\nobstructed or out of sight as equally important as those with fully visible\ntrajectories. LTrajDiff utilizes sensor data from mobile phones to surmount\nout-of-sight constraints, albeit introducing new challenges such as modality\nfusion, noisy data, and the absence of spatial layout and object size\ninformation. We employ a denoising diffusion model to predict precise layout\nsequences from noisy mobile data using a coarse-to-fine diffusion strategy,\nincorporating the RMS, Siamese Masked Encoding Module, and MFM. Our model\npredicts layout sequences by implicitly inferring object size and projection\nstatus from a single reference timestamp or significantly obstructed sequences.\nAchieving SOTA results in randomly obstructed experiments and extremely short\ninput experiments, our model illustrates the effectiveness of leveraging noisy\nmobile data. In summary, our approach offers a promising solution to the\nchallenges faced by layout sequence and trajectory prediction models in\nreal-world settings, paving the way for utilizing sensor data from mobile\nphones to accurately predict pedestrian bounding box trajectories. To the best\nof our knowledge, this is the first work that addresses severely obstructed and\nextremely short layout sequences by combining vision with noisy mobile\nmodality, making it the pioneering work in the field of layout sequence\ntrajectory prediction.\n",
        "published": "2023",
        "authors": [
            "Haichao Zhang",
            "Yi Xu",
            "Hongsheng Lu",
            "Takayuki Shimizu",
            "Yun Fu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.19629v2",
        "title": "RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency",
        "abstract": "  In this paper, we study the problem of continuous 3D shape representations.\nThe majority of existing successful methods are coordinate-based implicit\nneural representations. However, they are inefficient to render novel views or\nrecover explicit surface points. A few works start to formulate 3D shapes as\nray-based neural functions, but the learned structures are inferior due to the\nlack of multi-view geometry consistency. To tackle these challenges, we propose\na new framework called RayDF. It consists of three major components: 1) the\nsimple ray-surface distance field, 2) the novel dual-ray visibility classifier,\nand 3) a multi-view consistency optimization module to drive the learned\nray-surface distances to be multi-view geometry consistent. We extensively\nevaluate our method on three public datasets, demonstrating remarkable\nperformance in 3D surface point reconstruction on both synthetic and\nchallenging real-world 3D scenes, clearly surpassing existing coordinate-based\nand ray-based baselines. Most notably, our method achieves a 1000x faster speed\nthan coordinate-based methods to render an 800x800 depth image, showing the\nsuperiority of our method for 3D shape representation. Our code and data are\navailable at https://github.com/vLAR-group/RayDF\n",
        "published": "2023",
        "authors": [
            "Zhuoman Liu",
            "Bo Yang",
            "Yan Luximon",
            "Ajay Kumar",
            "Jinxi Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.02502v1",
        "title": "MAAIP: Multi-Agent Adversarial Interaction Priors for imitation from\n  fighting demonstrations for physics-based characters",
        "abstract": "  Simulating realistic interaction and motions for physics-based characters is\nof great interest for interactive applications, and automatic secondary\ncharacter animation in the movie and video game industries. Recent works in\nreinforcement learning have proposed impressive results for single character\nsimulation, especially the ones that use imitation learning based techniques.\nHowever, imitating multiple characters interactions and motions requires to\nalso model their interactions. In this paper, we propose a novel Multi-Agent\nGenerative Adversarial Imitation Learning based approach that generalizes the\nidea of motion imitation for one character to deal with both the interaction\nand the motions of the multiple physics-based characters. Two unstructured\ndatasets are given as inputs: 1) a single-actor dataset containing motions of a\nsingle actor performing a set of motions linked to a specific application, and\n2) an interaction dataset containing a few examples of interactions between\nmultiple actors. Based on these datasets, our system trains control policies\nallowing each character to imitate the interactive skills associated with each\nactor, while preserving the intrinsic style. This approach has been tested on\ntwo different fighting styles, boxing and full-body martial art, to demonstrate\nthe ability of the method to imitate different styles.\n",
        "published": "2023",
        "authors": [
            "Mohamed Younes",
            "Ewa Kijak",
            "Richard Kulpa",
            "Simon Malinowski",
            "Franck Multon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.05992v1",
        "title": "Robust Adversarial Attacks Detection for Deep Learning based Relative\n  Pose Estimation for Space Rendezvous",
        "abstract": "  Research on developing deep learning techniques for autonomous spacecraft\nrelative navigation challenges is continuously growing in recent years.\nAdopting those techniques offers enhanced performance. However, such approaches\nalso introduce heightened apprehensions regarding the trustability and security\nof such deep learning methods through their susceptibility to adversarial\nattacks. In this work, we propose a novel approach for adversarial attack\ndetection for deep neural network-based relative pose estimation schemes based\non the explainability concept. We develop for an orbital rendezvous scenario an\ninnovative relative pose estimation technique adopting our proposed\nConvolutional Neural Network (CNN), which takes an image from the chaser's\nonboard camera and outputs accurately the target's relative position and\nrotation. We perturb seamlessly the input images using adversarial attacks that\nare generated by the Fast Gradient Sign Method (FGSM). The adversarial attack\ndetector is then built based on a Long Short Term Memory (LSTM) network which\ntakes the explainability measure namely SHapley Value from the CNN-based pose\nestimator and flags the detection of adversarial attacks when acting.\nSimulation results show that the proposed adversarial attack detector achieves\na detection accuracy of 99.21%. Both the deep relative pose estimator and\nadversarial attack detector are then tested on real data captured from our\nlaboratory-designed setup. The experimental results from our\nlaboratory-designed setup demonstrate that the proposed adversarial attack\ndetector achieves an average detection accuracy of 96.29%.\n",
        "published": "2023",
        "authors": [
            "Ziwei Wang",
            "Nabil Aouf",
            "Jose Pizarro",
            "Christophe Honvault"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.06398v1",
        "title": "NVFi: Neural Velocity Fields for 3D Physics Learning from Dynamic Videos",
        "abstract": "  In this paper, we aim to model 3D scene dynamics from multi-view videos.\nUnlike the majority of existing works which usually focus on the common task of\nnovel view synthesis within the training time period, we propose to\nsimultaneously learn the geometry, appearance, and physical velocity of 3D\nscenes only from video frames, such that multiple desirable applications can be\nsupported, including future frame extrapolation, unsupervised 3D semantic scene\ndecomposition, and dynamic motion transfer. Our method consists of three major\ncomponents, 1) the keyframe dynamic radiance field, 2) the interframe velocity\nfield, and 3) a joint keyframe and interframe optimization module which is the\ncore of our framework to effectively train both networks. To validate our\nmethod, we further introduce two dynamic 3D datasets: 1) Dynamic Object\ndataset, and 2) Dynamic Indoor Scene dataset. We conduct extensive experiments\non multiple datasets, demonstrating the superior performance of our method over\nall baselines, particularly in the critical tasks of future frame extrapolation\nand unsupervised 3D semantic scene decomposition.\n",
        "published": "2023",
        "authors": [
            "Jinxi Li",
            "Ziyang Song",
            "Bo Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.00094v1",
        "title": "The Impact of Coevolution and Abstention on the Emergence of Cooperation",
        "abstract": "  This paper explores the Coevolutionary Optional Prisoner's Dilemma (COPD)\ngame, which is a simple model to coevolve game strategy and link weights of\nagents playing the Optional Prisoner's Dilemma game. We consider a population\nof agents placed in a lattice grid with boundary conditions. A number of Monte\nCarlo simulations are performed to investigate the impacts of the COPD game on\nthe emergence of cooperation. Results show that the coevolutionary rules enable\ncooperators to survive and even dominate, with the presence of abstainers in\nthe population playing a key role in the protection of cooperators against\nexploitation from defectors. We observe that in adverse conditions such as when\nthe initial population of abstainers is too scarce/abundant, or when the\ntemptation to defect is very high, cooperation has no chance of emerging.\nHowever, when the simple coevolutionary rules are applied, cooperators\nflourish.\n",
        "published": "2017",
        "authors": [
            "Marcos Cardinot",
            "Colm O'Riordan",
            "Josephine Griffith"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.00742v2",
        "title": "Autocurricula and the Emergence of Innovation from Social Interaction: A\n  Manifesto for Multi-Agent Intelligence Research",
        "abstract": "  Evolution has produced a multi-scale mosaic of interacting adaptive units.\nInnovations arise when perturbations push parts of the system away from stable\nequilibria into new regimes where previously well-adapted solutions no longer\nwork. Here we explore the hypothesis that multi-agent systems sometimes display\nintrinsic dynamics arising from competition and cooperation that provide a\nnaturally emergent curriculum, which we term an autocurriculum. The solution of\none social task often begets new social tasks, continually generating novel\nchallenges, and thereby promoting innovation. Under certain conditions these\nchallenges may become increasingly complex over time, demanding that agents\naccumulate ever more innovations.\n",
        "published": "2019",
        "authors": [
            "Joel Z. Leibo",
            "Edward Hughes",
            "Marc Lanctot",
            "Thore Graepel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.00917v1",
        "title": "Evaluation of the general applicability of Dragoon for the k-center\n  problem",
        "abstract": "  The k-center problem is a fundamental problem we often face when considering\ncomplex service systems. Typical challenges include the placement of warehouses\nin logistics or positioning of servers for content delivery networks. We\npreviously have proposed Dragoon as an effective algorithm to approach the\nk-center problem. This paper evaluates Dragoon with a focus on potential worst\ncase behavior in comparison to other techniques. We use an evolutionary\nalgorithm to generate instances of the k-center problem that are especially\nchallenging for Dragoon. Ultimately, our experiments confirm the previous good\nresults of Dragoon, however, we also can reliably find scenarios where it is\nclearly outperformed by other approaches.\n",
        "published": "2020",
        "authors": [
            "Tobias Uhlig",
            "Peter Hillmann",
            "Oliver Rose"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.08884v3",
        "title": "Inequity aversion improves cooperation in intertemporal social dilemmas",
        "abstract": "  Groups of humans are often able to find ways to cooperate with one another in\ncomplex, temporally extended social dilemmas. Models based on behavioral\neconomics are only able to explain this phenomenon for unrealistic stateless\nmatrix games. Recently, multi-agent reinforcement learning has been applied to\ngeneralize social dilemma problems to temporally and spatially extended Markov\ngames. However, this has not yet generated an agent that learns to cooperate in\nsocial dilemmas as humans do. A key insight is that many, but not all, human\nindividuals have inequity averse social preferences. This promotes a particular\nresolution of the matrix game social dilemma wherein inequity-averse\nindividuals are personally pro-social and punish defectors. Here we extend this\nidea to Markov games and show that it promotes cooperation in several types of\nsequential social dilemma, via a profitable interaction with policy\nlearnability. In particular, we find that inequity aversion improves temporal\ncredit assignment for the important class of intertemporal social dilemmas.\nThese results help explain how large-scale cooperation may emerge and persist.\n",
        "published": "2018",
        "authors": [
            "Edward Hughes",
            "Joel Z. Leibo",
            "Matthew G. Phillips",
            "Karl Tuyls",
            "Edgar A. Du\u00e9\u00f1ez-Guzm\u00e1n",
            "Antonio Garc\u00eda Casta\u00f1eda",
            "Iain Dunning",
            "Tina Zhu",
            "Kevin R. McKee",
            "Raphael Koster",
            "Heather Roff",
            "Thore Graepel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.10450v1",
        "title": "Understanding of Normal and Abnormal Hearts by Phase Space Analysis and\n  Convolutional Neural Networks",
        "abstract": "  Cardiac diseases are one of the leading mortality factors in modern,\nindustrialized societies, which cause high expenses in public health systems.\nDue to high costs, developing analytical methods to improve cardiac diagnostics\nis essential. The heart's electric activity was first modeled using a set of\nnonlinear differential equations. Following this, variations of cardiac spectra\noriginating from deterministic dynamics are investigated. Analyzing a normal\nhuman heart's power spectra offers His-Purkinje network, which possesses a\nfractal-like structure. Phase space trajectories are extracted from the time\nseries electrocardiogram (ECG) graph with third-order derivate Taylor Series.\nHere in this study, phase space analysis and Convolutional Neural Networks\n(CNNs) method are applied to 44 records via the MIT-BIH database recorded with\nMLII. In order to increase accuracy, a straight line is drawn between the\nhighest Q-R distance in the phase space images of the records. Binary CNN\nclassification is used to determine healthy or unhealthy hearts. With a 90.90%\naccuracy rate, this model could classify records according to their heart\nstatus.\n",
        "published": "2023",
        "authors": [
            "Bekir Yavuz Koc",
            "Taner Arsan",
            "Onder Pekcan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.03700v2",
        "title": "A Comparison of Lattice-free Discriminative Training Criteria for Purely\n  Sequence-Trained Neural Network Acoustic Models",
        "abstract": "  In this work, three lattice-free (LF) discriminative training criteria for\npurely sequence-trained neural network acoustic models are compared on LVCSR\ntasks, namely maximum mutual information (MMI), boosted maximum mutual\ninformation (bMMI) and state-level minimum Bayes risk (sMBR). We demonstrate\nthat, analogous to LF-MMI, a neural network acoustic model can also be trained\nfrom scratch using LF-bMMI or LF-sMBR criteria respectively without the need of\ncross-entropy pre-training. Furthermore, experimental results on\nSwitchboard-300hrs and Switchboard+Fisher-2100hrs datasets show that models\ntrained with LF-bMMI consistently outperform those trained with plain LF-MMI\nand achieve a relative word error rate (WER) reduction of 5% over competitive\ntemporal convolution projected LSTM (TDNN-LSTMP) LF-MMI baselines.\n",
        "published": "2018",
        "authors": [
            "Chao Weng",
            "Dong Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.14171v1",
        "title": "SE-KGE: A Location-Aware Knowledge Graph Embedding Model for Geographic\n  Question Answering and Spatial Semantic Lifting",
        "abstract": "  Learning knowledge graph (KG) embeddings is an emerging technique for a\nvariety of downstream tasks such as summarization, link prediction, information\nretrieval, and question answering. However, most existing KG embedding models\nneglect space and, therefore, do not perform well when applied to (geo)spatial\ndata and tasks. For those models that consider space, most of them primarily\nrely on some notions of distance. These models suffer from higher computational\ncomplexity during training while still losing information beyond the relative\ndistance between entities. In this work, we propose a location-aware KG\nembedding model called SE-KGE. It directly encodes spatial information such as\npoint coordinates or bounding boxes of geographic entities into the KG\nembedding space. The resulting model is capable of handling different types of\nspatial reasoning. We also construct a geographic knowledge graph as well as a\nset of geographic query-answer pairs called DBGeo to evaluate the performance\nof SE-KGE in comparison to multiple baselines. Evaluation results show that\nSE-KGE outperforms these baselines on the DBGeo dataset for geographic logic\nquery answering task. This demonstrates the effectiveness of our\nspatially-explicit model and the importance of considering the scale of\ndifferent geographic entities. Finally, we introduce a novel downstream task\ncalled spatial semantic lifting which links an arbitrary location in the study\narea to entities in the KG via some relations. Evaluation on DBGeo shows that\nour model outperforms the baseline by a substantial margin.\n",
        "published": "2020",
        "authors": [
            "Gengchen Mai",
            "Krzysztof Janowicz",
            "Ling Cai",
            "Rui Zhu",
            "Blake Regalia",
            "Bo Yan",
            "Meilin Shi",
            "Ni Lao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1512.05742v3",
        "title": "A Survey of Available Corpora for Building Data-Driven Dialogue Systems",
        "abstract": "  During the past decade, several areas of speech and language understanding\nhave witnessed substantial breakthroughs from the use of data-driven models. In\nthe area of dialogue systems, the trend is less obvious, and most practical\nsystems are still built through significant engineering and expert knowledge.\nNevertheless, several recent results suggest that data-driven approaches are\nfeasible and quite promising. To facilitate research in this area, we have\ncarried out a wide survey of publicly available datasets suitable for\ndata-driven learning of dialogue systems. We discuss important characteristics\nof these datasets, how they can be used to learn diverse dialogue strategies,\nand their other potential uses. We also examine methods for transfer learning\nbetween datasets and the use of external knowledge. Finally, we discuss\nappropriate choice of evaluation metrics for the learning objective.\n",
        "published": "2015",
        "authors": [
            "Iulian Vlad Serban",
            "Ryan Lowe",
            "Peter Henderson",
            "Laurent Charlin",
            "Joelle Pineau"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.13544v2",
        "title": "I-AID: Identifying Actionable Information from Disaster-related Tweets",
        "abstract": "  Social media plays a significant role in disaster management by providing\nvaluable data about affected people, donations and help requests. Recent\nstudies highlight the need to filter information on social media into\nfine-grained content labels. However, identifying useful information from\nmassive amounts of social media posts during a crisis is a challenging task. In\nthis paper, we propose I-AID, a multimodel approach to automatically categorize\ntweets into multi-label information types and filter critical information from\nthe enormous volume of social media data. I-AID incorporates three main\ncomponents: i) a BERT-based encoder to capture the semantics of a tweet and\nrepresent as a low-dimensional vector, ii) a graph attention network (GAT) to\napprehend correlations between tweets' words/entities and the corresponding\ninformation types, and iii) a Relation Network as a learnable distance metric\nto compute the similarity between tweets and their corresponding information\ntypes in a supervised way. We conducted several experiments on two real\npublicly-available datasets. Our results indicate that I-AID outperforms\nstate-of-the-art approaches in terms of weighted average F1 score by +6% and\n+4% on the TREC-IS dataset and COVID-19 Tweets, respectively.\n",
        "published": "2020",
        "authors": [
            "Hamada M. Zahera",
            "Rricha Jalota",
            "Mohamed A. Sherif",
            "Axel N. Ngomo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.00486v1",
        "title": "Towards a Theoretical Understanding of Word and Relation Representation",
        "abstract": "  Representing words by vectors, or embeddings, enables computational reasoning\nand is foundational to automating natural language tasks. For example, if word\nembeddings of similar words contain similar values, word similarity can be\nreadily assessed, whereas judging that from their spelling is often impossible\n(e.g. cat /feline) and to predetermine and store similarities between all words\nis prohibitively time-consuming, memory intensive and subjective. We focus on\nword embeddings learned from text corpora and knowledge graphs. Several\nwell-known algorithms learn word embeddings from text on an unsupervised basis\nby learning to predict those words that occur around each word, e.g. word2vec\nand GloVe. Parameters of such word embeddings are known to reflect word\nco-occurrence statistics, but how they capture semantic meaning has been\nunclear. Knowledge graph representation models learn representations both of\nentities (words, people, places, etc.) and relations between them, typically by\ntraining a model to predict known facts in a supervised manner. Despite steady\nimprovements in fact prediction accuracy, little is understood of the latent\nstructure that enables this.\n  The limited understanding of how latent semantic structure is encoded in the\ngeometry of word embeddings and knowledge graph representations makes a\nprincipled means of improving their performance, reliability or\ninterpretability unclear. To address this:\n  1. we theoretically justify the empirical observation that particular\ngeometric relationships between word embeddings learned by algorithms such as\nword2vec and GloVe correspond to semantic relations between words; and\n  2. we extend this correspondence between semantics and geometry to the\nentities and relations of knowledge graphs, providing a model for the latent\nstructure of knowledge graph representation linked to that of word embeddings.\n",
        "published": "2022",
        "authors": [
            "Carl Allen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1704.07751v1",
        "title": "Fine-Grained Entity Typing with High-Multiplicity Assignments",
        "abstract": "  As entity type systems become richer and more fine-grained, we expect the\nnumber of types assigned to a given entity to increase. However, most\nfine-grained typing work has focused on datasets that exhibit a low degree of\ntype multiplicity. In this paper, we consider the high-multiplicity regime\ninherent in data sources such as Wikipedia that have semi-open type systems. We\nintroduce a set-prediction approach to this problem and show that our model\noutperforms unstructured baselines on a new Wikipedia-based fine-grained typing\ncorpus.\n",
        "published": "2017",
        "authors": [
            "Maxim Rabinovich",
            "Dan Klein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.08067v5",
        "title": "Text Classification Algorithms: A Survey",
        "abstract": "  In recent years, there has been an exponential growth in the number of\ncomplex documents and texts that require a deeper understanding of machine\nlearning methods to be able to accurately classify texts in many applications.\nMany machine learning approaches have achieved surpassing results in natural\nlanguage processing. The success of these learning algorithms relies on their\ncapacity to understand complex models and non-linear relationships within data.\nHowever, finding suitable structures, architectures, and techniques for text\nclassification is a challenge for researchers. In this paper, a brief overview\nof text classification algorithms is discussed. This overview covers different\ntext feature extractions, dimensionality reduction methods, existing algorithms\nand techniques, and evaluations methods. Finally, the limitations of each\ntechnique and their application in the real-world problem are discussed.\n",
        "published": "2019",
        "authors": [
            "Kamran Kowsari",
            "Kiana Jafari Meimandi",
            "Mojtaba Heidarysafa",
            "Sanjana Mendu",
            "Laura E. Barnes",
            "Donald E. Brown"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.10816v4",
        "title": "Program Synthesis and Semantic Parsing with Learned Code Idioms",
        "abstract": "  Program synthesis of general-purpose source code from natural language\nspecifications is challenging due to the need to reason about high-level\npatterns in the target program and low-level implementation details at the same\ntime. In this work, we present PATOIS, a system that allows a neural program\nsynthesizer to explicitly interleave high-level and low-level reasoning at\nevery generation step. It accomplishes this by automatically mining common code\nidioms from a given corpus, incorporating them into the underlying language for\nneural synthesis, and training a tree-based neural synthesizer to use these\nidioms during code generation. We evaluate PATOIS on two complex semantic\nparsing datasets and show that using learned code idioms improves the\nsynthesizer's accuracy.\n",
        "published": "2019",
        "authors": [
            "Richard Shin",
            "Miltiadis Allamanis",
            "Marc Brockschmidt",
            "Oleksandr Polozov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.04039v2",
        "title": "Resolving the Human Subjects Status of Machine Learning's Crowdworkers",
        "abstract": "  In recent years, machine learning (ML) has relied heavily on crowdworkers\nboth for building datasets and for addressing research questions requiring\nhuman interaction or judgment. The diverse tasks performed and uses of the data\nproduced render it difficult to determine when crowdworkers are best thought of\nas workers (versus human subjects). These difficulties are compounded by\nconflicting policies, with some institutions and researchers regarding all ML\ncrowdworkers as human subjects and others holding that they rarely constitute\nhuman subjects. Notably few ML papers involving crowdwork mention IRB\noversight, raising the prospect of non-compliance with ethical and regulatory\nrequirements. We investigate the appropriate designation of ML crowdsourcing\nstudies, focusing our inquiry on natural language processing to expose unique\nchallenges for research oversight. Crucially, under the U.S. Common Rule, these\njudgments hinge on determinations of aboutness, concerning both whom (or what)\nthe collected data is about and whom (or what) the analysis is about. We\nhighlight two challenges posed by ML: the same set of workers can serve\nmultiple roles and provide many sorts of information; and ML research tends to\nembrace a dynamic workflow, where research questions are seldom stated ex ante\nand data sharing opens the door for future studies to aim questions at\ndifferent targets. Our analysis exposes a potential loophole in the Common\nRule, where researchers can elude research ethics oversight by splitting data\ncollection and analysis into distinct studies. Finally, we offer several policy\nrecommendations to address these concerns.\n",
        "published": "2022",
        "authors": [
            "Divyansh Kaushik",
            "Zachary C. Lipton",
            "Alex John London"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.17608v1",
        "title": "Reward Collapse in Aligning Large Language Models",
        "abstract": "  The extraordinary capabilities of large language models (LLMs) such as\nChatGPT and GPT-4 are in part unleashed by aligning them with reward models\nthat are trained on human preferences, which are often represented as rankings\nof responses to prompts. In this paper, we document the phenomenon of\n\\textit{reward collapse}, an empirical observation where the prevailing\nranking-based approach results in an \\textit{identical} reward distribution\n\\textit{regardless} of the prompts during the terminal phase of training. This\noutcome is undesirable as open-ended prompts like ``write a short story about\nyour best friend'' should yield a continuous range of rewards for their\ncompletions, while specific prompts like ``what is the capital of New Zealand''\nshould generate either high or low rewards. Our theoretical investigation\nreveals that reward collapse is primarily due to the insufficiency of the\nranking-based objective function to incorporate prompt-related information\nduring optimization. This insight allows us to derive closed-form expressions\nfor the reward distribution associated with a set of utility functions in an\nasymptotic regime. To overcome reward collapse, we introduce a prompt-aware\noptimization scheme that provably admits a prompt-dependent reward distribution\nwithin the interpolating regime. Our experimental results suggest that our\nproposed prompt-aware utility functions significantly alleviate reward collapse\nduring the training of reward models.\n",
        "published": "2023",
        "authors": [
            "Ziang Song",
            "Tianle Cai",
            "Jason D. Lee",
            "Weijie J. Su"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.14115v2",
        "title": "Towards Trustworthy Explanation: On Causal Rationalization",
        "abstract": "  With recent advances in natural language processing, rationalization becomes\nan essential self-explaining diagram to disentangle the black box by selecting\na subset of input texts to account for the major variation in prediction. Yet,\nexisting association-based approaches on rationalization cannot identify true\nrationales when two or more snippets are highly inter-correlated and thus\nprovide a similar contribution to prediction accuracy, so-called spuriousness.\nTo address this limitation, we novelly leverage two causal desiderata,\nnon-spuriousness and efficiency, into rationalization from the causal inference\nperspective. We formally define a series of probabilities of causation based on\na newly proposed structural causal model of rationalization, with its\ntheoretical identification established as the main component of learning\nnecessary and sufficient rationales. The superior performance of the proposed\ncausal rationalization is demonstrated on real-world review and medical\ndatasets with extensive experiments compared to state-of-the-art methods.\n",
        "published": "2023",
        "authors": [
            "Wenbo Zhang",
            "Tong Wu",
            "Yunlong Wang",
            "Yong Cai",
            "Hengrui Cai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.02119v1",
        "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
        "abstract": "  While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an LLM to\niteratively refine candidate (attack) prompts using tree-of-thoughts reasoning\nuntil one of the generated prompts jailbreaks the target. Crucially, before\nsending prompts to the target, TAP assesses them and prunes the ones unlikely\nto result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate\na large search space of prompts and pruning reduces the total number of queries\nsent to the target. In empirical evaluations, we observe that TAP generates\nprompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo)\nfor more than 80% of the prompts using only a small number of queries. This\nsignificantly improves upon the previous state-of-the-art black-box method for\ngenerating jailbreaks.\n",
        "published": "2023",
        "authors": [
            "Anay Mehrotra",
            "Manolis Zampetakis",
            "Paul Kassianik",
            "Blaine Nelson",
            "Hyrum Anderson",
            "Yaron Singer",
            "Amin Karbasi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.05415v4",
        "title": "Learning from Dialogue after Deployment: Feed Yourself, Chatbot!",
        "abstract": "  The majority of conversations a dialogue agent sees over its lifetime occur\nafter it has already been trained and deployed, leaving a vast store of\npotential training signal untapped. In this work, we propose the self-feeding\nchatbot, a dialogue agent with the ability to extract new training examples\nfrom the conversations it participates in. As our agent engages in\nconversation, it also estimates user satisfaction in its responses. When the\nconversation appears to be going well, the user's responses become new training\nexamples to imitate. When the agent believes it has made a mistake, it asks for\nfeedback; learning to predict the feedback that will be given improves the\nchatbot's dialogue abilities further. On the PersonaChat chit-chat dataset with\nover 131k training examples, we find that learning from dialogue with a\nself-feeding chatbot significantly improves performance, regardless of the\namount of traditional supervision.\n",
        "published": "2019",
        "authors": [
            "Braden Hancock",
            "Antoine Bordes",
            "Pierre-Emmanuel Mazar\u00e9",
            "Jason Weston"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.06518v3",
        "title": "Gender Detection on Social Networks using Ensemble Deep Learning",
        "abstract": "  Analyzing the ever-increasing volume of posts on social media sites such as\nFacebook and Twitter requires improved information processing methods for\nprofiling authorship. Document classification is central to this task, but the\nperformance of traditional supervised classifiers has degraded as the volume of\nsocial media has increased. This paper addresses this problem in the context of\ngender detection through ensemble classification that employs multi-model deep\nlearning architectures to generate specialized understanding from different\nfeature spaces.\n",
        "published": "2020",
        "authors": [
            "Kamran Kowsari",
            "Mojtaba Heidarysafa",
            "Tolu Odukoya",
            "Philip Potter",
            "Laura E. Barnes",
            "Donald E. Brown"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.00286v1",
        "title": "Reinforcement Learning for Personalized Dialogue Management",
        "abstract": "  Language systems have been of great interest to the research community and\nhave recently reached the mass market through various assistant platforms on\nthe web. Reinforcement Learning methods that optimize dialogue policies have\nseen successes in past years and have recently been extended into methods that\npersonalize the dialogue, e.g. take the personal context of users into account.\nThese works, however, are limited to personalization to a single user with whom\nthey require multiple interactions and do not generalize the usage of context\nacross users. This work introduces a problem where a generalized usage of\ncontext is relevant and proposes two Reinforcement Learning (RL)-based\napproaches to this problem. The first approach uses a single learner and\nextends the traditional POMDP formulation of dialogue state with features that\ndescribe the user context. The second approach segments users by context and\nthen employs a learner per context. We compare these approaches in a benchmark\nof existing non-RL and RL-based methods in three established and one novel\napplication domain of financial product recommendation. We compare the\ninfluence of context and training experiences on performance and find that\nlearning approaches generally outperform a handcrafted gold standard.\n",
        "published": "2019",
        "authors": [
            "Floris den Hengst",
            "Mark Hoogendoorn",
            "Frank van Harmelen",
            "Joost Bosman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1604.01272v1",
        "title": "Feature extraction using Latent Dirichlet Allocation and Neural\n  Networks: A case study on movie synopses",
        "abstract": "  Feature extraction has gained increasing attention in the field of machine\nlearning, as in order to detect patterns, extract information, or predict\nfuture observations from big data, the urge of informative features is crucial.\nThe process of extracting features is highly linked to dimensionality reduction\nas it implies the transformation of the data from a sparse high-dimensional\nspace, to higher level meaningful abstractions. This dissertation employs\nNeural Networks for distributed paragraph representations, and Latent Dirichlet\nAllocation to capture higher level features of paragraph vectors. Although\nNeural Networks for distributed paragraph representations are considered the\nstate of the art for extracting paragraph vectors, we show that a quick topic\nanalysis model such as Latent Dirichlet Allocation can provide meaningful\nfeatures too. We evaluate the two methods on the CMU Movie Summary Corpus, a\ncollection of 25,203 movie plot summaries extracted from Wikipedia. Finally,\nfor both approaches, we use K-Nearest Neighbors to discover similar movies, and\nplot the projected representations using T-Distributed Stochastic Neighbor\nEmbedding to depict the context similarities. These similarities, expressed as\nmovie distances, can be used for movies recommendation. The recommended movies\nof this approach are compared with the recommended movies from IMDB, which use\na collaborative filtering recommendation approach, to show that our two models\ncould constitute either an alternative or a supplementary recommendation\napproach.\n",
        "published": "2016",
        "authors": [
            "Despoina Christou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.07743v2",
        "title": "A Benchmarking Study of Embedding-based Entity Alignment for Knowledge\n  Graphs",
        "abstract": "  Entity alignment seeks to find entities in different knowledge graphs (KGs)\nthat refer to the same real-world object. Recent advancement in KG embedding\nimpels the advent of embedding-based entity alignment, which encodes entities\nin a continuous embedding space and measures entity similarities based on the\nlearned embeddings. In this paper, we conduct a comprehensive experimental\nstudy of this emerging field. We survey 23 recent embedding-based entity\nalignment approaches and categorize them based on their techniques and\ncharacteristics. We also propose a new KG sampling algorithm, with which we\ngenerate a set of dedicated benchmark datasets with various heterogeneity and\ndistributions for a realistic evaluation. We develop an open-source library\nincluding 12 representative embedding-based entity alignment approaches, and\nextensively evaluate these approaches, to understand their strengths and\nlimitations. Additionally, for several directions that have not been explored\nin current approaches, we perform exploratory experiments and report our\npreliminary findings for future studies. The benchmark datasets, open-source\nlibrary and experimental results are all accessible online and will be duly\nmaintained.\n",
        "published": "2020",
        "authors": [
            "Zequn Sun",
            "Qingheng Zhang",
            "Wei Hu",
            "Chengming Wang",
            "Muhao Chen",
            "Farahnaz Akrami",
            "Chengkai Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.08557v1",
        "title": "Unsupervised Text Generation by Learning from Search",
        "abstract": "  In this work, we present TGLS, a novel framework to unsupervised Text\nGeneration by Learning from Search. We start by applying a strong search\nalgorithm (in particular, simulated annealing) towards a heuristically defined\nobjective that (roughly) estimates the quality of sentences. Then, a\nconditional generative model learns from the search results, and meanwhile\nsmooth out the noise of search. The alternation between search and learning can\nbe repeated for performance bootstrapping. We demonstrate the effectiveness of\nTGLS on two real-world natural language generation tasks, paraphrase generation\nand text formalization. Our model significantly outperforms unsupervised\nbaseline methods in both tasks. Especially, it achieves comparable performance\nwith the state-of-the-art supervised methods in paraphrase generation.\n",
        "published": "2020",
        "authors": [
            "Jingjing Li",
            "Zichao Li",
            "Lili Mou",
            "Xin Jiang",
            "Michael R. Lyu",
            "Irwin King"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.02979v2",
        "title": "Generative Topic Embedding: a Continuous Representation of Documents\n  (Extended Version with Proofs)",
        "abstract": "  Word embedding maps words into a low-dimensional continuous embedding space\nby exploiting the local word collocation patterns in a small context window. On\nthe other hand, topic modeling maps documents onto a low-dimensional topic\nspace, by utilizing the global word collocation patterns in the same document.\nThese two types of patterns are complementary. In this paper, we propose a\ngenerative topic embedding model to combine the two types of patterns. In our\nmodel, topics are represented by embedding vectors, and are shared across\ndocuments. The probability of each word is influenced by both its local context\nand its topic. A variational inference method yields the topic embeddings as\nwell as the topic mixing proportions for each document. Jointly they represent\nthe document in a low-dimensional continuous space. In two document\nclassification tasks, our method performs better than eight existing methods,\nwith fewer features. In addition, we illustrate with an example that our method\ncan generate coherent topics even based on only one document.\n",
        "published": "2016",
        "authors": [
            "Shaohua Li",
            "Tat-Seng Chua",
            "Jun Zhu",
            "Chunyan Miao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.06879v1",
        "title": "Neural language representations predict outcomes of scientific research",
        "abstract": "  Many research fields codify their findings in standard formats, often by\nreporting correlations between quantities of interest. But the space of all\ntestable correlates is far larger than scientific resources can currently\naddress, so the ability to accurately predict correlations would be useful to\nplan research and allocate resources. Using a dataset of approximately 170,000\ncorrelational findings extracted from leading social science journals, we show\nthat a trained neural network can accurately predict the reported correlations\nusing only the text descriptions of the correlates. Accurate predictive models\nsuch as these can guide scientists towards promising untested correlates,\nbetter quantify the information gained from new findings, and has implications\nfor moving artificial intelligence systems from predicting structures to\npredicting relationships in the real world.\n",
        "published": "2018",
        "authors": [
            "James P. Bagrow",
            "Daniel Berenberg",
            "Joshua Bongard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.12316v1",
        "title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for\n  Discrete Data",
        "abstract": "  We present a probabilistic framework for studying adversarial attacks on\ndiscrete data. Based on this framework, we derive a perturbation-based method,\nGreedy Attack, and a scalable learning-based method, Gumbel Attack, that\nillustrate various tradeoffs in the design of attacks. We demonstrate the\neffectiveness of these methods using both quantitative metrics and human\nevaluation on various state-of-the-art models for text classification,\nincluding a word-based CNN, a character-based CNN and an LSTM. As as example of\nour results, we show that the accuracy of character-based convolutional\nnetworks drops to the level of random selection by modifying only five\ncharacters through Greedy Attack.\n",
        "published": "2018",
        "authors": [
            "Puyudi Yang",
            "Jianbo Chen",
            "Cho-Jui Hsieh",
            "Jane-Ling Wang",
            "Michael I. Jordan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.01485v1",
        "title": "Assessing Social and Intersectional Biases in Contextualized Word\n  Representations",
        "abstract": "  Social bias in machine learning has drawn significant attention, with work\nranging from demonstrations of bias in a multitude of applications, curating\ndefinitions of fairness for different contexts, to developing algorithms to\nmitigate bias. In natural language processing, gender bias has been shown to\nexist in context-free word embeddings. Recently, contextual word\nrepresentations have outperformed word embeddings in several downstream NLP\ntasks. These word representations are conditioned on their context within a\nsentence, and can also be used to encode the entire sentence. In this paper, we\nanalyze the extent to which state-of-the-art models for contextual word\nrepresentations, such as BERT and GPT-2, encode biases with respect to gender,\nrace, and intersectional identities. Towards this, we propose assessing bias at\nthe contextual word level. This novel approach captures the contextual effects\nof bias missing in context-free word embeddings, yet avoids confounding effects\nthat underestimate bias at the sentence encoding level. We demonstrate evidence\nof bias at the corpus level, find varying evidence of bias in embedding\nassociation tests, show in particular that racial bias is strongly encoded in\ncontextual word models, and observe that bias effects for intersectional\nminorities are exacerbated beyond their constituent minority identities.\nFurther, evaluating bias effects at the contextual word level captures biases\nthat are not captured at the sentence level, confirming the need for our novel\napproach.\n",
        "published": "2019",
        "authors": [
            "Yi Chern Tan",
            "L. Elisa Celis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.04615v3",
        "title": "Beyond the Imitation Game: Quantifying and extrapolating the\n  capabilities of language models",
        "abstract": "  Language models demonstrate both quantitative improvement and new qualitative\ncapabilities with increasing scale. Despite their potentially transformative\nimpact, these new capabilities are as yet poorly characterized. In order to\ninform future research, prepare for disruptive new model capabilities, and\nameliorate socially harmful effects, it is vital that we understand the present\nand near-future capabilities and limitations of language models. To address\nthis challenge, we introduce the Beyond the Imitation Game benchmark\n(BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450\nauthors across 132 institutions. Task topics are diverse, drawing problems from\nlinguistics, childhood development, math, common-sense reasoning, biology,\nphysics, social bias, software development, and beyond. BIG-bench focuses on\ntasks that are believed to be beyond the capabilities of current language\nmodels. We evaluate the behavior of OpenAI's GPT models, Google-internal dense\ntransformer architectures, and Switch-style sparse transformers on BIG-bench,\nacross model sizes spanning millions to hundreds of billions of parameters. In\naddition, a team of human expert raters performed all tasks in order to provide\na strong baseline. Findings include: model performance and calibration both\nimprove with scale, but are poor in absolute terms (and when compared with\nrater performance); performance is remarkably similar across model classes,\nthough with benefits from sparsity; tasks that improve gradually and\npredictably commonly involve a large knowledge or memorization component,\nwhereas tasks that exhibit \"breakthrough\" behavior at a critical scale often\ninvolve multiple steps or components, or brittle metrics; social bias typically\nincreases with scale in settings with ambiguous context, but this can be\nimproved with prompting.\n",
        "published": "2022",
        "authors": [
            "Aarohi Srivastava",
            "Abhinav Rastogi",
            "Abhishek Rao",
            "Abu Awal Md Shoeb",
            "Abubakar Abid",
            "Adam Fisch",
            "Adam R. Brown",
            "Adam Santoro",
            "Aditya Gupta",
            "Adri\u00e0 Garriga-Alonso",
            "Agnieszka Kluska",
            "Aitor Lewkowycz",
            "Akshat Agarwal",
            "Alethea Power",
            "Alex Ray",
            "Alex Warstadt",
            "Alexander W. Kocurek",
            "Ali Safaya",
            "Ali Tazarv",
            "Alice Xiang",
            "Alicia Parrish",
            "Allen Nie",
            "Aman Hussain",
            "Amanda Askell",
            "Amanda Dsouza",
            "Ambrose Slone",
            "Ameet Rahane",
            "Anantharaman S. Iyer",
            "Anders Andreassen",
            "Andrea Madotto",
            "Andrea Santilli",
            "Andreas Stuhlm\u00fcller",
            "Andrew Dai",
            "Andrew La",
            "Andrew Lampinen",
            "Andy Zou",
            "Angela Jiang",
            "Angelica Chen",
            "Anh Vuong",
            "Animesh Gupta",
            "Anna Gottardi",
            "Antonio Norelli",
            "Anu Venkatesh",
            "Arash Gholamidavoodi",
            "Arfa Tabassum",
            "Arul Menezes",
            "Arun Kirubarajan",
            "Asher Mullokandov",
            "Ashish Sabharwal",
            "Austin Herrick",
            "Avia Efrat",
            "Aykut Erdem",
            "Ayla Karaka\u015f",
            "B. Ryan Roberts",
            "Bao Sheng Loe",
            "Barret Zoph",
            "Bart\u0142omiej Bojanowski",
            "Batuhan \u00d6zyurt",
            "Behnam Hedayatnia",
            "Behnam Neyshabur",
            "Benjamin Inden",
            "Benno Stein",
            "Berk Ekmekci",
            "Bill Yuchen Lin",
            "Blake Howald",
            "Bryan Orinion",
            "Cameron Diao",
            "Cameron Dour",
            "Catherine Stinson",
            "Cedrick Argueta",
            "C\u00e9sar Ferri Ram\u00edrez",
            "Chandan Singh",
            "Charles Rathkopf",
            "Chenlin Meng",
            "Chitta Baral",
            "Chiyu Wu",
            "Chris Callison-Burch",
            "Chris Waites",
            "Christian Voigt",
            "Christopher D. Manning",
            "Christopher Potts",
            "Cindy Ramirez",
            "Clara E. Rivera",
            "Clemencia Siro",
            "Colin Raffel",
            "Courtney Ashcraft",
            "Cristina Garbacea",
            "Damien Sileo",
            "Dan Garrette",
            "Dan Hendrycks",
            "Dan Kilman",
            "Dan Roth",
            "Daniel Freeman",
            "Daniel Khashabi",
            "Daniel Levy",
            "Daniel Mosegu\u00ed Gonz\u00e1lez",
            "Danielle Perszyk",
            "Danny Hernandez",
            "Danqi Chen",
            "Daphne Ippolito",
            "Dar Gilboa",
            "David Dohan",
            "David Drakard",
            "David Jurgens",
            "Debajyoti Datta",
            "Deep Ganguli",
            "Denis Emelin",
            "Denis Kleyko",
            "Deniz Yuret",
            "Derek Chen",
            "Derek Tam",
            "Dieuwke Hupkes",
            "Diganta Misra",
            "Dilyar Buzan",
            "Dimitri Coelho Mollo",
            "Diyi Yang",
            "Dong-Ho Lee",
            "Dylan Schrader",
            "Ekaterina Shutova",
            "Ekin Dogus Cubuk",
            "Elad Segal",
            "Eleanor Hagerman",
            "Elizabeth Barnes",
            "Elizabeth Donoway",
            "Ellie Pavlick",
            "Emanuele Rodola",
            "Emma Lam",
            "Eric Chu",
            "Eric Tang",
            "Erkut Erdem",
            "Ernie Chang",
            "Ethan A. Chi",
            "Ethan Dyer",
            "Ethan Jerzak",
            "Ethan Kim",
            "Eunice Engefu Manyasi",
            "Evgenii Zheltonozhskii",
            "Fanyue Xia",
            "Fatemeh Siar",
            "Fernando Mart\u00ednez-Plumed",
            "Francesca Happ\u00e9",
            "Francois Chollet",
            "Frieda Rong",
            "Gaurav Mishra",
            "Genta Indra Winata",
            "Gerard de Melo",
            "Germ\u00e1n Kruszewski",
            "Giambattista Parascandolo",
            "Giorgio Mariani",
            "Gloria Wang",
            "Gonzalo Jaimovitch-L\u00f3pez",
            "Gregor Betz",
            "Guy Gur-Ari",
            "Hana Galijasevic",
            "Hannah Kim",
            "Hannah Rashkin",
            "Hannaneh Hajishirzi",
            "Harsh Mehta",
            "Hayden Bogar",
            "Henry Shevlin",
            "Hinrich Sch\u00fctze",
            "Hiromu Yakura",
            "Hongming Zhang",
            "Hugh Mee Wong",
            "Ian Ng",
            "Isaac Noble",
            "Jaap Jumelet",
            "Jack Geissinger",
            "Jackson Kernion",
            "Jacob Hilton",
            "Jaehoon Lee",
            "Jaime Fern\u00e1ndez Fisac",
            "James B. Simon",
            "James Koppel",
            "James Zheng",
            "James Zou",
            "Jan Koco\u0144",
            "Jana Thompson",
            "Janelle Wingfield",
            "Jared Kaplan",
            "Jarema Radom",
            "Jascha Sohl-Dickstein",
            "Jason Phang",
            "Jason Wei",
            "Jason Yosinski",
            "Jekaterina Novikova",
            "Jelle Bosscher",
            "Jennifer Marsh",
            "Jeremy Kim",
            "Jeroen Taal",
            "Jesse Engel",
            "Jesujoba Alabi",
            "Jiacheng Xu",
            "Jiaming Song",
            "Jillian Tang",
            "Joan Waweru",
            "John Burden",
            "John Miller",
            "John U. Balis",
            "Jonathan Batchelder",
            "Jonathan Berant",
            "J\u00f6rg Frohberg",
            "Jos Rozen",
            "Jose Hernandez-Orallo",
            "Joseph Boudeman",
            "Joseph Guerr",
            "Joseph Jones",
            "Joshua B. Tenenbaum",
            "Joshua S. Rule",
            "Joyce Chua",
            "Kamil Kanclerz",
            "Karen Livescu",
            "Karl Krauth",
            "Karthik Gopalakrishnan",
            "Katerina Ignatyeva",
            "Katja Markert",
            "Kaustubh D. Dhole",
            "Kevin Gimpel",
            "Kevin Omondi",
            "Kory Mathewson",
            "Kristen Chiafullo",
            "Ksenia Shkaruta",
            "Kumar Shridhar",
            "Kyle McDonell",
            "Kyle Richardson",
            "Laria Reynolds",
            "Leo Gao",
            "Li Zhang",
            "Liam Dugan",
            "Lianhui Qin",
            "Lidia Contreras-Ochando",
            "Louis-Philippe Morency",
            "Luca Moschella",
            "Lucas Lam",
            "Lucy Noble",
            "Ludwig Schmidt",
            "Luheng He",
            "Luis Oliveros Col\u00f3n",
            "Luke Metz",
            "L\u00fctfi Kerem \u015eenel",
            "Maarten Bosma",
            "Maarten Sap",
            "Maartje ter Hoeve",
            "Maheen Farooqi",
            "Manaal Faruqui",
            "Mantas Mazeika",
            "Marco Baturan",
            "Marco Marelli",
            "Marco Maru",
            "Maria Jose Ram\u00edrez Quintana",
            "Marie Tolkiehn",
            "Mario Giulianelli",
            "Martha Lewis",
            "Martin Potthast",
            "Matthew L. Leavitt",
            "Matthias Hagen",
            "M\u00e1ty\u00e1s Schubert",
            "Medina Orduna Baitemirova",
            "Melody Arnaud",
            "Melvin McElrath",
            "Michael A. Yee",
            "Michael Cohen",
            "Michael Gu",
            "Michael Ivanitskiy",
            "Michael Starritt",
            "Michael Strube",
            "Micha\u0142 Sw\u0119drowski",
            "Michele Bevilacqua",
            "Michihiro Yasunaga",
            "Mihir Kale",
            "Mike Cain",
            "Mimee Xu",
            "Mirac Suzgun",
            "Mitch Walker",
            "Mo Tiwari",
            "Mohit Bansal",
            "Moin Aminnaseri",
            "Mor Geva",
            "Mozhdeh Gheini",
            "Mukund Varma T",
            "Nanyun Peng",
            "Nathan A. Chi",
            "Nayeon Lee",
            "Neta Gur-Ari Krakover",
            "Nicholas Cameron",
            "Nicholas Roberts",
            "Nick Doiron",
            "Nicole Martinez",
            "Nikita Nangia",
            "Niklas Deckers",
            "Niklas Muennighoff",
            "Nitish Shirish Keskar",
            "Niveditha S. Iyer",
            "Noah Constant",
            "Noah Fiedel",
            "Nuan Wen",
            "Oliver Zhang",
            "Omar Agha",
            "Omar Elbaghdadi",
            "Omer Levy",
            "Owain Evans",
            "Pablo Antonio Moreno Casares",
            "Parth Doshi",
            "Pascale Fung",
            "Paul Pu Liang",
            "Paul Vicol",
            "Pegah Alipoormolabashi",
            "Peiyuan Liao",
            "Percy Liang",
            "Peter Chang",
            "Peter Eckersley",
            "Phu Mon Htut",
            "Pinyu Hwang",
            "Piotr Mi\u0142kowski",
            "Piyush Patil",
            "Pouya Pezeshkpour",
            "Priti Oli",
            "Qiaozhu Mei",
            "Qing Lyu",
            "Qinlang Chen",
            "Rabin Banjade",
            "Rachel Etta Rudolph",
            "Raefer Gabriel",
            "Rahel Habacker",
            "Ramon Risco",
            "Rapha\u00ebl Milli\u00e8re",
            "Rhythm Garg",
            "Richard Barnes",
            "Rif A. Saurous",
            "Riku Arakawa",
            "Robbe Raymaekers",
            "Robert Frank",
            "Rohan Sikand",
            "Roman Novak",
            "Roman Sitelew",
            "Ronan LeBras",
            "Rosanne Liu",
            "Rowan Jacobs",
            "Rui Zhang",
            "Ruslan Salakhutdinov",
            "Ryan Chi",
            "Ryan Lee",
            "Ryan Stovall",
            "Ryan Teehan",
            "Rylan Yang",
            "Sahib Singh",
            "Saif M. Mohammad",
            "Sajant Anand",
            "Sam Dillavou",
            "Sam Shleifer",
            "Sam Wiseman",
            "Samuel Gruetter",
            "Samuel R. Bowman",
            "Samuel S. Schoenholz",
            "Sanghyun Han",
            "Sanjeev Kwatra",
            "Sarah A. Rous",
            "Sarik Ghazarian",
            "Sayan Ghosh",
            "Sean Casey",
            "Sebastian Bischoff",
            "Sebastian Gehrmann",
            "Sebastian Schuster",
            "Sepideh Sadeghi",
            "Shadi Hamdan",
            "Sharon Zhou",
            "Shashank Srivastava",
            "Sherry Shi",
            "Shikhar Singh",
            "Shima Asaadi",
            "Shixiang Shane Gu",
            "Shubh Pachchigar",
            "Shubham Toshniwal",
            "Shyam Upadhyay",
            " Shyamolima",
            " Debnath",
            "Siamak Shakeri",
            "Simon Thormeyer",
            "Simone Melzi",
            "Siva Reddy",
            "Sneha Priscilla Makini",
            "Soo-Hwan Lee",
            "Spencer Torene",
            "Sriharsha Hatwar",
            "Stanislas Dehaene",
            "Stefan Divic",
            "Stefano Ermon",
            "Stella Biderman",
            "Stephanie Lin",
            "Stephen Prasad",
            "Steven T. Piantadosi",
            "Stuart M. Shieber",
            "Summer Misherghi",
            "Svetlana Kiritchenko",
            "Swaroop Mishra",
            "Tal Linzen",
            "Tal Schuster",
            "Tao Li",
            "Tao Yu",
            "Tariq Ali",
            "Tatsu Hashimoto",
            "Te-Lin Wu",
            "Th\u00e9o Desbordes",
            "Theodore Rothschild",
            "Thomas Phan",
            "Tianle Wang",
            "Tiberius Nkinyili",
            "Timo Schick",
            "Timofei Kornev",
            "Titus Tunduny",
            "Tobias Gerstenberg",
            "Trenton Chang",
            "Trishala Neeraj",
            "Tushar Khot",
            "Tyler Shultz",
            "Uri Shaham",
            "Vedant Misra",
            "Vera Demberg",
            "Victoria Nyamai",
            "Vikas Raunak",
            "Vinay Ramasesh",
            "Vinay Uday Prabhu",
            "Vishakh Padmakumar",
            "Vivek Srikumar",
            "William Fedus",
            "William Saunders",
            "William Zhang",
            "Wout Vossen",
            "Xiang Ren",
            "Xiaoyu Tong",
            "Xinran Zhao",
            "Xinyi Wu",
            "Xudong Shen",
            "Yadollah Yaghoobzadeh",
            "Yair Lakretz",
            "Yangqiu Song",
            "Yasaman Bahri",
            "Yejin Choi",
            "Yichi Yang",
            "Yiding Hao",
            "Yifu Chen",
            "Yonatan Belinkov",
            "Yu Hou",
            "Yufang Hou",
            "Yuntao Bai",
            "Zachary Seid",
            "Zhuoye Zhao",
            "Zijian Wang",
            "Zijie J. Wang",
            "Zirui Wang",
            "Ziyi Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.01848v2",
        "title": "Explaining Patterns in Data with Language Models via Interpretable\n  Autoprompting",
        "abstract": "  Large language models (LLMs) have displayed an impressive ability to harness\nnatural language to perform complex tasks. In this work, we explore whether we\ncan leverage this learned ability to find and explain patterns in data.\nSpecifically, given a pre-trained LLM and data examples, we introduce\ninterpretable autoprompting (iPrompt), an algorithm that generates a\nnatural-language string explaining the data. iPrompt iteratively alternates\nbetween generating explanations with an LLM and reranking them based on their\nperformance when used as a prompt. Experiments on a wide range of datasets,\nfrom synthetic mathematics to natural-language understanding, show that iPrompt\ncan yield meaningful insights by accurately finding groundtruth dataset\ndescriptions. Moreover, the prompts produced by iPrompt are simultaneously\nhuman-interpretable and highly effective for generalization: on real-world\nsentiment classification datasets, iPrompt produces prompts that match or even\nimprove upon human-written prompts for GPT-3. Finally, experiments with an fMRI\ndataset show the potential for iPrompt to aid in scientific discovery. All code\nfor using the methods and data here is made available on Github.\n",
        "published": "2022",
        "authors": [
            "Chandan Singh",
            "John X. Morris",
            "Jyoti Aneja",
            "Alexander M. Rush",
            "Jianfeng Gao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.02403v1",
        "title": "The Path to Autonomous Learners",
        "abstract": "  In this paper, we present a new theoretical approach for enabling domain\nknowledge acquisition by intelligent systems. We introduce a hybrid model that\nstarts with minimal input knowledge in the form of an upper ontology of\nconcepts, stores and reasons over this knowledge through a knowledge graph\ndatabase and learns new information through a Logic Neural Network. We study\nthe behavior of this architecture when handling new data and show that the\nfinal system is capable of enriching its current knowledge as well as extending\nit to new domains.\n",
        "published": "2022",
        "authors": [
            "Hanna Abi Akl"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.04054v7",
        "title": "Towards Inferential Reproducibility of Machine Learning Research",
        "abstract": "  Reliability of machine learning evaluation -- the consistency of observed\nevaluation scores across replicated model training runs -- is affected by\nseveral sources of nondeterminism which can be regarded as measurement noise.\nCurrent tendencies to remove noise in order to enforce reproducibility of\nresearch results neglect inherent nondeterminism at the implementation level\nand disregard crucial interaction effects between algorithmic noise factors and\ndata properties. This limits the scope of conclusions that can be drawn from\nsuch experiments. Instead of removing noise, we propose to incorporate several\nsources of variance, including their interaction with data properties, into an\nanalysis of significance and reliability of machine learning evaluation, with\nthe aim to draw inferences beyond particular instances of trained models. We\nshow how to use linear mixed effects models (LMEMs) to analyze performance\nevaluation scores, and to conduct statistical inference with a generalized\nlikelihood ratio test (GLRT). This allows us to incorporate arbitrary sources\nof noise like meta-parameter variations into statistical significance testing,\nand to assess performance differences conditional on data properties.\nFurthermore, a variance component analysis (VCA) enables the analysis of the\ncontribution of noise sources to overall variance and the computation of a\nreliability coefficient by the ratio of substantial to total variance.\n",
        "published": "2023",
        "authors": [
            "Michael Hagmann",
            "Philipp Meier",
            "Stefan Riezler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.08072v2",
        "title": "Max-plus Operators Applied to Filter Selection and Model Pruning in\n  Neural Networks",
        "abstract": "  Following recent advances in morphological neural networks, we propose to\nstudy in more depth how Max-plus operators can be exploited to define\nmorphological units and how they behave when incorporated in layers of\nconventional neural networks. Besides showing that they can be easily\nimplemented with modern machine learning frameworks , we confirm and extend the\nobservation that a Max-plus layer can be used to select important filters and\nreduce redundancy in its previous layer, without incurring performance loss.\nExperimental results demonstrate that the filter selection strategy enabled by\na Max-plus is highly efficient and robust, through which we successfully\nperformed model pruning on different neural network architectures. We also\npoint out that there is a close connection between Maxout networks and our\npruned Max-plus networks by comparing their respective characteristics. The\ncode for reproducing our experiments is available online.\n",
        "published": "2019",
        "authors": [
            "Yunxiang Zhang",
            "Samy Blusseau",
            "Santiago Velasco-Forero",
            "Isabelle Bloch",
            "Jesus Angulo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1503.07077v1",
        "title": "Rotation-invariant convolutional neural networks for galaxy morphology\n  prediction",
        "abstract": "  Measuring the morphological parameters of galaxies is a key requirement for\nstudying their formation and evolution. Surveys such as the Sloan Digital Sky\nSurvey (SDSS) have resulted in the availability of very large collections of\nimages, which have permitted population-wide analyses of galaxy morphology.\nMorphological analysis has traditionally been carried out mostly via visual\ninspection by trained experts, which is time-consuming and does not scale to\nlarge ($\\gtrsim10^4$) numbers of images.\n  Although attempts have been made to build automated classification systems,\nthese have not been able to achieve the desired level of accuracy. The Galaxy\nZoo project successfully applied a crowdsourcing strategy, inviting online\nusers to classify images by answering a series of questions. Unfortunately,\neven this approach does not scale well enough to keep up with the increasing\navailability of galaxy images.\n  We present a deep neural network model for galaxy morphology classification\nwhich exploits translational and rotational symmetry. It was developed in the\ncontext of the Galaxy Challenge, an international competition to build the best\nmodel for morphology classification based on annotated images from the Galaxy\nZoo project.\n  For images with high agreement among the Galaxy Zoo participants, our model\nis able to reproduce their consensus with near-perfect accuracy ($> 99\\%$) for\nmost questions. Confident model predictions are highly accurate, which makes\nthe model suitable for filtering large collections of images and forwarding\nchallenging images to experts for manual annotation. This approach greatly\nreduces the experts' workload without affecting accuracy. The application of\nthese algorithms to larger sets of training data will be critical for analysing\nresults from future surveys such as the LSST.\n",
        "published": "2015",
        "authors": [
            "Sander Dieleman",
            "Kyle W. Willett",
            "Joni Dambre"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1312.6120v3",
        "title": "Exact solutions to the nonlinear dynamics of learning in deep linear\n  neural networks",
        "abstract": "  Despite the widespread practical success of deep learning methods, our\ntheoretical understanding of the dynamics of learning in deep neural networks\nremains quite sparse. We attempt to bridge the gap between the theory and\npractice of deep learning by systematically analyzing learning dynamics for the\nrestricted case of deep linear neural networks. Despite the linearity of their\ninput-output map, such networks have nonlinear gradient descent dynamics on\nweights that change with the addition of each new hidden layer. We show that\ndeep linear networks exhibit nonlinear learning phenomena similar to those seen\nin simulations of nonlinear networks, including long plateaus followed by rapid\ntransitions to lower error solutions, and faster convergence from greedy\nunsupervised pretraining initial conditions than from random initial\nconditions. We provide an analytical description of these phenomena by finding\nnew exact solutions to the nonlinear dynamics of deep learning. Our theoretical\nanalysis also reveals the surprising finding that as the depth of a network\napproaches infinity, learning speed can nevertheless remain finite: for a\nspecial class of initial conditions on the weights, very deep networks incur\nonly a finite, depth independent, delay in learning speed relative to shallow\nnetworks. We show that, under certain conditions on the training data,\nunsupervised pretraining can find this special class of initial conditions,\nwhile scaled random Gaussian initializations cannot. We further exhibit a new\nclass of random orthogonal initial conditions on weights that, like\nunsupervised pre-training, enjoys depth independent learning times. We further\nshow that these initial conditions also lead to faithful propagation of\ngradients even in deep nonlinear networks, as long as they operate in a special\nregime known as the edge of chaos.\n",
        "published": "2013",
        "authors": [
            "Andrew M. Saxe",
            "James L. McClelland",
            "Surya Ganguli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1407.4420v2",
        "title": "Kernel Nonnegative Matrix Factorization Without the Curse of the\n  Pre-image - Application to Unmixing Hyperspectral Images",
        "abstract": "  The nonnegative matrix factorization (NMF) is widely used in signal and image\nprocessing, including bio-informatics, blind source separation and\nhyperspectral image analysis in remote sensing. A great challenge arises when\ndealing with a nonlinear formulation of the NMF. Within the framework of kernel\nmachines, the models suggested in the literature do not allow the\nrepresentation of the factorization matrices, which is a fallout of the curse\nof the pre-image. In this paper, we propose a novel kernel-based model for the\nNMF that does not suffer from the pre-image problem, by investigating the\nestimation of the factorization matrices directly in the input space. For\ndifferent kernel functions, we describe two schemes for iterative algorithms:\nan additive update rule based on a gradient descent scheme and a multiplicative\nupdate rule in the same spirit as in the Lee and Seung algorithm. Within the\nproposed framework, we develop several extensions to incorporate constraints,\nincluding sparseness, smoothness, and spatial regularization with a\ntotal-variation-like penalty. The effectiveness of the proposed method is\ndemonstrated with the problem of unmixing hyperspectral images, using\nwell-known real images and results with state-of-the-art techniques.\n",
        "published": "2014",
        "authors": [
            "Fei Zhu",
            "Paul Honeine",
            "Maya Kallas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1409.6046v1",
        "title": "Approximation errors of online sparsification criteria",
        "abstract": "  Many machine learning frameworks, such as resource-allocating networks,\nkernel-based methods, Gaussian processes, and radial-basis-function networks,\nrequire a sparsification scheme in order to address the online learning\nparadigm. For this purpose, several online sparsification criteria have been\nproposed to restrict the model definition on a subset of samples. The most\nknown criterion is the (linear) approximation criterion, which discards any\nsample that can be well represented by the already contributing samples, an\noperation with excessive computational complexity. Several computationally\nefficient sparsification criteria have been introduced in the literature, such\nas the distance, the coherence and the Babel criteria. In this paper, we\nprovide a framework that connects these sparsification criteria to the issue of\napproximating samples, by deriving theoretical bounds on the approximation\nerrors. Moreover, we investigate the error of approximating any feature, by\nproposing upper-bounds on the approximation error for each of the\naforementioned sparsification criteria. Two classes of features are described\nin detail, the empirical mean and the principal axes in the kernel principal\ncomponent analysis.\n",
        "published": "2014",
        "authors": [
            "Paul Honeine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1411.0161v1",
        "title": "Entropy of Overcomplete Kernel Dictionaries",
        "abstract": "  In signal analysis and synthesis, linear approximation theory considers a\nlinear decomposition of any given signal in a set of atoms, collected into a\nso-called dictionary. Relevant sparse representations are obtained by relaxing\nthe orthogonality condition of the atoms, yielding overcomplete dictionaries\nwith an extended number of atoms. More generally than the linear decomposition,\novercomplete kernel dictionaries provide an elegant nonlinear extension by\ndefining the atoms through a mapping kernel function (e.g., the gaussian\nkernel). Models based on such kernel dictionaries are used in neural networks,\ngaussian processes and online learning with kernels.\n  The quality of an overcomplete dictionary is evaluated with a diversity\nmeasure the distance, the approximation, the coherence and the Babel measures.\nIn this paper, we develop a framework to examine overcomplete kernel\ndictionaries with the entropy from information theory. Indeed, a higher value\nof the entropy is associated to a further uniform spread of the atoms over the\nspace. For each of the aforementioned diversity measures, we derive lower\nbounds on the entropy. Several definitions of the entropy are examined, with an\nextensive analysis in both the input space and the mapped feature space.\n",
        "published": "2014",
        "authors": [
            "Paul Honeine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1605.08283v1",
        "title": "Discrete Deep Feature Extraction: A Theory and New Architectures",
        "abstract": "  First steps towards a mathematical theory of deep convolutional neural\nnetworks for feature extraction were made---for the continuous-time case---in\nMallat, 2012, and Wiatowski and B\\\"olcskei, 2015. This paper considers the\ndiscrete case, introduces new convolutional neural network architectures, and\nproposes a mathematical framework for their analysis. Specifically, we\nestablish deformation and translation sensitivity results of local and global\nnature, and we investigate how certain structural properties of the input\nsignal are reflected in the corresponding feature vectors. Our theory applies\nto general filters and general Lipschitz-continuous non-linearities and pooling\noperators. Experiments on handwritten digit classification and facial landmark\ndetection---including feature importance evaluation---complement the\ntheoretical findings.\n",
        "published": "2016",
        "authors": [
            "Thomas Wiatowski",
            "Michael Tschannen",
            "Aleksandar Stani\u0107",
            "Philipp Grohs",
            "Helmut B\u00f6lcskei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.07656v2",
        "title": "Survey on Deep Neural Networks in Speech and Vision Systems",
        "abstract": "  This survey presents a review of state-of-the-art deep neural network\narchitectures, algorithms, and systems in vision and speech applications.\nRecent advances in deep artificial neural network algorithms and architectures\nhave spurred rapid innovation and development of intelligent vision and speech\nsystems. With availability of vast amounts of sensor data and cloud computing\nfor processing and training of deep neural networks, and with increased\nsophistication in mobile and embedded technology, the next-generation\nintelligent systems are poised to revolutionize personal and commercial\ncomputing. This survey begins by providing background and evolution of some of\nthe most successful deep learning models for intelligent vision and speech\nsystems to date. An overview of large-scale industrial research and development\nefforts is provided to emphasize future trends and prospects of intelligent\nvision and speech systems. Robust and efficient intelligent systems demand\nlow-latency and high fidelity in resource-constrained hardware platforms such\nas mobile devices, robots, and automobiles. Therefore, this survey also\nprovides a summary of key challenges and recent successes in running deep\nneural networks on hardware-restricted platforms, i.e. within limited memory,\nbattery life, and processing capabilities. Finally, emerging applications of\nvision and speech across disciplines such as affective computing, intelligent\ntransportation, and precision medicine are discussed. To our knowledge, this\npaper provides one of the most comprehensive surveys on the latest developments\nin intelligent vision and speech applications from the perspectives of both\nsoftware and hardware systems. Many of these emerging technologies using deep\nneural networks show tremendous promise to revolutionize research and\ndevelopment for future vision and speech systems.\n",
        "published": "2019",
        "authors": [
            "Mahbubul Alam",
            "Manar D. Samad",
            "Lasitha Vidyaratne",
            "Alexander Glandon",
            "Khan M. Iftekharuddin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.00063v2",
        "title": "Bio-Inspired Modality Fusion for Active Speaker Detection",
        "abstract": "  Human beings have developed fantastic abilities to integrate information from\nvarious sensory sources exploring their inherent complementarity. Perceptual\ncapabilities are therefore heightened, enabling, for instance, the well-known\n\"cocktail party\" and McGurk effects, i.e., speech disambiguation from a panoply\nof sound signals. This fusion ability is also key in refining the perception of\nsound source location, as in distinguishing whose voice is being heard in a\ngroup conversation. Furthermore, neuroscience has successfully identified the\nsuperior colliculus region in the brain as the one responsible for this\nmodality fusion, with a handful of biological models having been proposed to\napproach its underlying neurophysiological process. Deriving inspiration from\none of these models, this paper presents a methodology for effectively fusing\ncorrelated auditory and visual information for active speaker detection. Such\nan ability can have a wide range of applications, from teleconferencing systems\nto social robotics. The detection approach initially routes auditory and visual\ninformation through two specialized neural network structures. The resulting\nembeddings are fused via a novel layer based on the superior colliculus, whose\ntopological structure emulates spatial neuron cross-mapping of unimodal\nperceptual fields. The validation process employed two publicly available\ndatasets, with achieved results confirming and greatly surpassing initial\nexpectations.\n",
        "published": "2020",
        "authors": [
            "Gustavo Assun\u00e7\u00e3o",
            "Nuno Gon\u00e7alves",
            "Paulo Menezes"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.08127v2",
        "title": "The Deep Bootstrap Framework: Good Online Learners are Good Offline\n  Generalizers",
        "abstract": "  We propose a new framework for reasoning about generalization in deep\nlearning. The core idea is to couple the Real World, where optimizers take\nstochastic gradient steps on the empirical loss, to an Ideal World, where\noptimizers take steps on the population loss. This leads to an alternate\ndecomposition of test error into: (1) the Ideal World test error plus (2) the\ngap between the two worlds. If the gap (2) is universally small, this reduces\nthe problem of generalization in offline learning to the problem of\noptimization in online learning. We then give empirical evidence that this gap\nbetween worlds can be small in realistic deep learning settings, in particular\nsupervised image classification. For example, CNNs generalize better than MLPs\non image distributions in the Real World, but this is \"because\" they optimize\nfaster on the population loss in the Ideal World. This suggests our framework\nis a useful tool for understanding generalization in deep learning, and lays a\nfoundation for future research in the area.\n",
        "published": "2020",
        "authors": [
            "Preetum Nakkiran",
            "Behnam Neyshabur",
            "Hanie Sedghi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.02076v2",
        "title": "On the use of Pairwise Distance Learning for Brain Signal Classification\n  with Limited Observations",
        "abstract": "  The increasing access to brain signal data using electroencephalography\ncreates new opportunities to study electrophysiological brain activity and\nperform ambulatory diagnoses of neuronal diseases. This work proposes a\npairwise distance learning approach for Schizophrenia classification relying on\nthe spectral properties of the signal. Given the limited number of observations\n(i.e. the case and/or control individuals) in clinical trials, we propose a\nSiamese neural network architecture to learn a discriminative feature space\nfrom pairwise combinations of observations per channel. In this way, the\nmultivariate order of the signal is used as a form of data augmentation,\nfurther supporting the network generalization ability. Convolutional layers\nwith parameters learned under a cosine contrastive loss are proposed to\nadequately explore spectral images derived from the brain signal. Results on a\ncase-control population show that the features extracted using the proposed\nneural network lead to an improved Schizophrenia diagnosis (+10pp in accuracy\nand sensitivity) against spectral features, thus suggesting the existence of\nnon-trivial, discriminative electrophysiological brain patterns.\n",
        "published": "2019",
        "authors": [
            "David Calhas",
            "Enrique Romero",
            "Rui Henriques"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.11730v1",
        "title": "Deep learning based dictionary learning and tomographic image\n  reconstruction",
        "abstract": "  This work presents an approach for image reconstruction in clinical low-dose\ntomography that combines principles from sparse signal processing with ideas\nfrom deep learning. First, we describe sparse signal representation in terms of\ndictionaries from a statistical perspective and interpret dictionary learning\nas a process of aligning distribution that arises from a generative model with\nempirical distribution of true signals. As a result we can see that sparse\ncoding with learned dictionaries resembles a specific variational autoencoder,\nwhere the decoder is a linear function and the encoder is a sparse coding\nalgorithm. Next, we show that dictionary learning can also benefit from\ncomputational advancements introduced in the context of deep learning, such as\nparallelism and as stochastic optimization. Finally, we show that\nregularization by dictionaries achieves competitive performance in computed\ntomography (CT) reconstruction comparing to state-of-the-art model based and\ndata driven approaches.\n",
        "published": "2021",
        "authors": [
            "Jevgenija Rudzusika",
            "Thomas Koehler",
            "Ozan \u00d6ktem"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1306.1491v1",
        "title": "Gaussian Process-Based Decentralized Data Fusion and Active Sensing for\n  Mobility-on-Demand System",
        "abstract": "  Mobility-on-demand (MoD) systems have recently emerged as a promising\nparadigm of one-way vehicle sharing for sustainable personal urban mobility in\ndensely populated cities. In this paper, we enhance the capability of a MoD\nsystem by deploying robotic shared vehicles that can autonomously cruise the\nstreets to be hailed by users. A key challenge to managing the MoD system\neffectively is that of real-time, fine-grained mobility demand sensing and\nprediction. This paper presents a novel decentralized data fusion and active\nsensing algorithm for real-time, fine-grained mobility demand sensing and\nprediction with a fleet of autonomous robotic vehicles in a MoD system. Our\nGaussian process (GP)-based decentralized data fusion algorithm can achieve a\nfine balance between predictive power and time efficiency. We theoretically\nguarantee its predictive performance to be equivalent to that of a\nsophisticated centralized sparse approximation for the GP model: The\ncomputation of such a sparse approximate GP model can thus be distributed among\nthe MoD vehicles, hence achieving efficient and scalable demand prediction.\nThough our decentralized active sensing strategy is devised to gather the most\ninformative demand data for demand prediction, it can achieve a dual effect of\nfleet rebalancing to service the mobility demands. Empirical evaluation on\nreal-world mobility demand data shows that our proposed algorithm can achieve a\nbetter balance between predictive accuracy and time efficiency than\nstate-of-the-art algorithms.\n",
        "published": "2013",
        "authors": [
            "Jie Chen",
            "Kian Hsiang Low",
            "Colin Keng-Yan Tan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1408.0058v1",
        "title": "A Framework for learning multi-agent dynamic formation strategy in\n  real-time applications",
        "abstract": "  Formation strategy is one of the most important parts of many multi-agent\nsystems with many applications in real world problems. In this paper, a\nframework for learning this task in a limited domain (restricted environment)\nis proposed. In this framework, agents learn either directly by observing an\nexpert behavior or indirectly by observing other agents or objects behavior.\nFirst, a group of algorithms for learning formation strategy based on limited\nfeatures will be presented. Due to distributed and complex nature of many\nmulti-agent systems, it is impossible to include all features directly in the\nlearning process; thus, a modular scheme is proposed in order to reduce the\nnumber of features. In this method, some important features have indirect\ninfluence in learning instead of directly involving them as input features.\nThis framework has the ability to dynamically assign a group of positions to a\ngroup of agents to improve system performance. In addition, it can change the\nformation strategy when the context changes. Finally, this framework is able to\nautomatically produce many complex and flexible formation strategy algorithms\nwithout directly involving an expert to present and implement such complex\nalgorithms.\n",
        "published": "2014",
        "authors": [
            "Mehrab Norouzitallab",
            "Valiallah Monajjemi",
            "Saeed Shiry Ghidary",
            "Mohammad Bagher Menhaj"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.05933v2",
        "title": "Distributed Iterative Learning Control for a Team of Quadrotors",
        "abstract": "  The goal of this work is to enable a team of quadrotors to learn how to\naccurately track a desired trajectory while holding a given formation. We solve\nthis problem in a distributed manner, where each vehicle has only access to the\ninformation of its neighbors. The desired trajectory is only available to one\n(or few) vehicles. We present a distributed iterative learning control (ILC)\napproach where each vehicle learns from the experience of its own and its\nneighbors' previous task repetitions, and adapts its feedforward input to\nimprove performance. Existing algorithms are extended in theory to make them\nmore applicable to real-world experiments. In particular, we prove stability\nfor any causal learning function with gains chosen according to a simple scalar\ncondition. Previous proofs were restricted to a specific learning function that\nonly depends on the tracking error derivative (D-type ILC). Our extension\nprovides more degrees of freedom in the ILC design and, as a result, better\nperformance can be achieved. We also show that stability is not affected by a\nlinear dynamic coupling between neighbors. This allows us to use an additional\nconsensus feedback controller to compensate for non-repetitive disturbances.\nExperiments with two quadrotors attest the effectiveness of the proposed\ndistributed multi-agent ILC approach. This is the first work to show\ndistributed ILC in experiment.\n",
        "published": "2016",
        "authors": [
            "Andreas Hock",
            "Angela P. Schoellig"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.07399v2",
        "title": "Learning for Multi-robot Cooperation in Partially Observable Stochastic\n  Environments with Macro-actions",
        "abstract": "  This paper presents a data-driven approach for multi-robot coordination in\npartially-observable domains based on Decentralized Partially Observable Markov\nDecision Processes (Dec-POMDPs) and macro-actions (MAs). Dec-POMDPs provide a\ngeneral framework for cooperative sequential decision making under uncertainty\nand MAs allow temporally extended and asynchronous action execution. To date,\nmost methods assume the underlying Dec-POMDP model is known a priori or a full\nsimulator is available during planning time. Previous methods which aim to\naddress these issues suffer from local optimality and sensitivity to initial\nconditions. Additionally, few hardware demonstrations involving a large team of\nheterogeneous robots and with long planning horizons exist. This work addresses\nthese gaps by proposing an iterative sampling based Expectation-Maximization\nalgorithm (iSEM) to learn polices using only trajectory data containing\nobservations, MAs, and rewards. Our experiments show the algorithm is able to\nachieve better solution quality than the state-of-the-art learning-based\nmethods. We implement two variants of multi-robot Search and Rescue (SAR)\ndomains (with and without obstacles) on hardware to demonstrate the learned\npolicies can effectively control a team of distributed robots to cooperate in a\npartially observable stochastic environment.\n",
        "published": "2017",
        "authors": [
            "Miao Liu",
            "Kavinayan Sivakumar",
            "Shayegan Omidshafiei",
            "Christopher Amato",
            "Jonathan P. How"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.09540v1",
        "title": "Distributed Reinforcement Learning for Cooperative Multi-Robot Object\n  Manipulation",
        "abstract": "  We consider solving a cooperative multi-robot object manipulation task using\nreinforcement learning (RL). We propose two distributed multi-agent RL\napproaches: distributed approximate RL (DA-RL), where each agent applies\nQ-learning with individual reward functions; and game-theoretic RL (GT-RL),\nwhere the agents update their Q-values based on the Nash equilibrium of a\nbimatrix Q-value game. We validate the proposed approaches in the setting of\ncooperative object manipulation with two simulated robot arms. Although we\nfocus on a small system of two agents in this paper, both DA-RL and GT-RL apply\nto general multi-agent systems, and are expected to scale well to large\nsystems.\n",
        "published": "2020",
        "authors": [
            "Guohui Ding",
            "Joewie J. Koh",
            "Kelly Merckaert",
            "Bram Vanderborght",
            "Marco M. Nicotra",
            "Christoffer Heckman",
            "Alessandro Roncone",
            "Lijun Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.06343v2",
        "title": "AirCapRL: Autonomous Aerial Human Motion Capture using Deep\n  Reinforcement Learning",
        "abstract": "  In this letter, we introduce a deep reinforcement learning (RL) based\nmulti-robot formation controller for the task of autonomous aerial human motion\ncapture (MoCap). We focus on vision-based MoCap, where the objective is to\nestimate the trajectory of body pose and shape of a single moving person using\nmultiple micro aerial vehicles. State-of-the-art solutions to this problem are\nbased on classical control methods, which depend on hand-crafted system and\nobservation models. Such models are difficult to derive and generalize across\ndifferent systems. Moreover, the non-linearity and non-convexities of these\nmodels lead to sub-optimal controls. In our work, we formulate this problem as\na sequential decision making task to achieve the vision-based motion capture\nobjectives, and solve it using a deep neural network-based RL method. We\nleverage proximal policy optimization (PPO) to train a stochastic decentralized\ncontrol policy for formation control. The neural network is trained in a\nparallelized setup in synthetic environments. We performed extensive simulation\nexperiments to validate our approach. Finally, real-robot experiments\ndemonstrate that our policies generalize to real world conditions. Video Link:\nhttps://bit.ly/38SJfjo Supplementary: https://bit.ly/3evfo1O\n",
        "published": "2020",
        "authors": [
            "Rahul Tallamraju",
            "Nitin Saini",
            "Elia Bonetto",
            "Michael Pabst",
            "Yu Tang Liu",
            "Michael J. Black",
            "Aamir Ahmad"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.00679v1",
        "title": "Cooperative Control of Mobile Robots with Stackelberg Learning",
        "abstract": "  Multi-robot cooperation requires agents to make decisions that are consistent\nwith the shared goal without disregarding action-specific preferences that\nmight arise from asymmetry in capabilities and individual objectives. To\naccomplish this goal, we propose a method named SLiCC: Stackelberg Learning in\nCooperative Control. SLiCC models the problem as a partially observable\nstochastic game composed of Stackelberg bimatrix games, and uses deep\nreinforcement learning to obtain the payoff matrices associated with these\ngames. Appropriate cooperative actions are then selected with the derived\nStackelberg equilibria. Using a bi-robot cooperative object transportation\nproblem, we validate the performance of SLiCC against centralized multi-agent\nQ-learning and demonstrate that SLiCC achieves better combined utility.\n",
        "published": "2020",
        "authors": [
            "Joewie J. Koh",
            "Guohui Ding",
            "Christoffer Heckman",
            "Lijun Chen",
            "Alessandro Roncone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.12610v1",
        "title": "Collaborative Multi-Robot Systems for Search and Rescue: Coordination\n  and Perception",
        "abstract": "  Autonomous or teleoperated robots have been playing increasingly important\nroles in civil applications in recent years. Across the different civil domains\nwhere robots can support human operators, one of the areas where they can have\nmore impact is in search and rescue (SAR) operations. In particular,\nmulti-robot systems have the potential to significantly improve the efficiency\nof SAR personnel with faster search of victims, initial assessment and mapping\nof the environment, real-time monitoring and surveillance of SAR operations, or\nestablishing emergency communication networks, among other possibilities. SAR\noperations encompass a wide variety of environments and situations, and\ntherefore heterogeneous and collaborative multi-robot systems can provide the\nmost advantages. In this paper, we review and analyze the existing approaches\nto multi-robot SAR support, from an algorithmic perspective and putting an\nemphasis on the methods enabling collaboration among the robots as well as\nadvanced perception through machine vision and multi-agent active perception.\nFurthermore, we put these algorithms in the context of the different challenges\nand constraints that various types of robots (ground, aerial, surface or\nunderwater) encounter in different SAR environments (maritime, urban,\nwilderness or other post-disaster scenarios). This is, to the best of our\nknowledge, the first review considering heterogeneous SAR robots across\ndifferent environments, while giving two complimentary points of view: control\nmechanisms and machine perception. Based on our review of the state-of-the-art,\nwe discuss the main open research questions, and outline our insights on the\ncurrent approaches that have potential to improve the real-world performance of\nmulti-robot SAR systems.\n",
        "published": "2020",
        "authors": [
            "Jorge Pe\u00f1a Queralta",
            "Jussi Taipalmaa",
            "Bilge Can Pullinen",
            "Victor Kathan Sarker",
            "Tuan Nguyen Gia",
            "Hannu Tenhunen",
            "Moncef Gabbouj",
            "Jenni Raitoharju",
            "Tomi Westerlund"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.00993v1",
        "title": "MADRaS : Multi Agent Driving Simulator",
        "abstract": "  In this work, we present MADRaS, an open-source multi-agent driving simulator\nfor use in the design and evaluation of motion planning algorithms for\nautonomous driving. MADRaS provides a platform for constructing a wide variety\nof highway and track driving scenarios where multiple driving agents can train\nfor motion planning tasks using reinforcement learning and other machine\nlearning algorithms. MADRaS is built on TORCS, an open-source car-racing\nsimulator. TORCS offers a variety of cars with different dynamic properties and\ndriving tracks with different geometries and surface properties. MADRaS\ninherits these functionalities from TORCS and introduces support for\nmulti-agent training, inter-vehicular communication, noisy observations,\nstochastic actions, and custom traffic cars whose behaviours can be programmed\nto simulate challenging traffic conditions encountered in the real world.\nMADRaS can be used to create driving tasks whose complexities can be tuned\nalong eight axes in well-defined steps. This makes it particularly suited for\ncurriculum and continual learning. MADRaS is lightweight and it provides a\nconvenient OpenAI Gym interface for independent control of each car. Apart from\nthe primitive steering-acceleration-brake control mode of TORCS, MADRaS offers\na hierarchical track-position -- speed control that can potentially be used to\nachieve better generalization. MADRaS uses multiprocessing to run each agent as\na parallel process for efficiency and integrates well with popular\nreinforcement learning libraries like RLLib.\n",
        "published": "2020",
        "authors": [
            "Anirban Santara",
            "Sohan Rudra",
            "Sree Aditya Buridi",
            "Meha Kaushik",
            "Abhishek Naik",
            "Bharat Kaul",
            "Balaraman Ravindran"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.08595v1",
        "title": "Flow-FL: Data-Driven Federated Learning for Spatio-Temporal Predictions\n  in Multi-Robot Systems",
        "abstract": "  In this paper, we show how the Federated Learning (FL) framework enables\nlearning collectively from distributed data in connected robot teams. This\nframework typically works with clients collecting data locally, updating neural\nnetwork weights of their model, and sending updates to a server for aggregation\ninto a global model. We explore the design space of FL by comparing two\nvariants of this concept. The first variant follows the traditional FL approach\nin which a server aggregates the local models. In the second variant, that we\ncall Flow-FL, the aggregation process is serverless thanks to the use of a\ngossip-based shared data structure. In both variants, we use a data-driven\nmechanism to synchronize the learning process in which robots contribute model\nupdates when they collect sufficient data. We validate our approach with an\nagent trajectory forecasting problem in a multi-agent setting. Using a\ncentralized implementation as a baseline, we study the effects of staggered\nonline data collection, and variations in data flow, number of participating\nrobots, and time delays introduced by the decentralization of the framework in\na multi-robot setting.\n",
        "published": "2020",
        "authors": [
            "Nathalie Majcherczyk",
            "Nishan Srishankar",
            "Carlo Pinciroli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.12903v1",
        "title": "Neural Enhanced Belief Propagation for Cooperative Localization",
        "abstract": "  Location-aware networks will introduce innovative services and applications\nfor modern convenience, applied ocean sciences, and public safety. In this\npaper, we establish a hybrid method for model-based and data-driven inference.\nWe consider a cooperative localization (CL) scenario where the mobile agents in\na wireless network aim to localize themselves by performing pairwise\nobservations with other agents and by exchanging location information. A\ntraditional method for distributed CL in large agent networks is belief\npropagation (BP) which is completely model-based and is known to suffer from\nproviding inconsistent (overconfident) estimates. The proposed approach\naddresses these limitations by complementing BP with learned information\nprovided by a graph neural network (GNN). We demonstrate numerically that our\nmethod can improve estimation accuracy and avoid overconfident beliefs, while\nits computational complexity remains comparable to BP. Notably, more consistent\nbeliefs are obtained by not explicitly addressing overconfidence in the loss\nfunction used for training of the GNN.\n",
        "published": "2021",
        "authors": [
            "Mingchao Liang",
            "Florian Meyer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.08834v1",
        "title": "A Multi-UAV System for Exploration and Target Finding in Cluttered and\n  GPS-Denied Environments",
        "abstract": "  The use of multi-rotor Unmanned Aerial Vehicles (UAVs) for search and rescue\nas well as remote sensing is rapidly increasing. Multi-rotor UAVs, however,\nhave limited endurance. The range of UAV applications can be widened if teams\nof multiple UAVs are used. We propose a framework for a team of UAVs to\ncooperatively explore and find a target in complex GPS-denied environments with\nobstacles. The team of UAVs autonomously navigates, explores, detects, and\nfinds the target in a cluttered environment with a known map. Examples of such\nenvironments include indoor scenarios, urban or natural canyons, caves, and\ntunnels, where the GPS signal is limited or blocked. The framework is based on\na probabilistic decentralised Partially Observable Markov Decision Process\nwhich accounts for the uncertainties in sensing and the environment. The team\ncan cooperate efficiently, with each UAV sharing only limited processed\nobservations and their locations during the mission. The system is simulated\nusing the Robotic Operating System and Gazebo. Performance of the system with\nan increasing number of UAVs in several indoor scenarios with obstacles is\ntested. Results indicate that the proposed multi-UAV system has improvements in\nterms of time-cost, the proportion of search area surveyed, as well as\nsuccessful rates for search and rescue missions.\n",
        "published": "2021",
        "authors": [
            "Xiaolong Zhu",
            "Fernando Vanegas",
            "Felipe Gonzalez",
            "Conrad Sanderson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.12254v1",
        "title": "The Holy Grail of Multi-Robot Planning: Learning to Generate\n  Online-Scalable Solutions from Offline-Optimal Experts",
        "abstract": "  Many multi-robot planning problems are burdened by the curse of\ndimensionality, which compounds the difficulty of applying solutions to\nlarge-scale problem instances. The use of learning-based methods in multi-robot\nplanning holds great promise as it enables us to offload the online\ncomputational burden of expensive, yet optimal solvers, to an offline learning\nprocedure. Simply put, the idea is to train a policy to copy an optimal pattern\ngenerated by a small-scale system, and then transfer that policy to much larger\nsystems, in the hope that the learned strategy scales, while maintaining\nnear-optimal performance. Yet, a number of issues impede us from leveraging\nthis idea to its full potential. This blue-sky paper elaborates some of the key\nchallenges that remain.\n",
        "published": "2021",
        "authors": [
            "Amanda Prorok",
            "Jan Blumenkamp",
            "Qingbiao Li",
            "Ryan Kortvelesy",
            "Zhe Liu",
            "Ethan Stump"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.03634v2",
        "title": "Multi-Agent Path Finding with Prioritized Communication Learning",
        "abstract": "  Multi-agent pathfinding (MAPF) has been widely used to solve large-scale\nreal-world problems, e.g., automation warehouses. The learning-based, fully\ndecentralized framework has been introduced to alleviate real-time problems and\nsimultaneously pursue optimal planning policy. However, existing methods might\ngenerate significantly more vertex conflicts (or collisions), which lead to a\nlow success rate or more makespan. In this paper, we propose a PrIoritized\nCOmmunication learning method (PICO), which incorporates the \\textit{implicit}\nplanning priorities into the communication topology within the decentralized\nmulti-agent reinforcement learning framework. Assembling with the classic\ncoupled planners, the implicit priority learning module can be utilized to form\nthe dynamic communication topology, which also builds an effective\ncollision-avoiding mechanism. PICO performs significantly better in large-scale\nMAPF tasks in success rates and collision rates than state-of-the-art\nlearning-based planners.\n",
        "published": "2022",
        "authors": [
            "Wenhao Li",
            "Hongjun Chen",
            "Bo Jin",
            "Wenzhe Tan",
            "Hongyuan Zha",
            "Xiangfeng Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.03737v2",
        "title": "Graph Neural Networks for Learning Robot Team Coordination",
        "abstract": "  This paper shows how Graph Neural Networks can be used for learning\ndistributed coordination mechanisms in connected teams of robots. We capture\nthe relational aspect of robot coordination by modeling the robot team as a\ngraph, where each robot is a node, and edges represent communication links.\nDuring training, robots learn how to pass messages and update internal states,\nso that a target behavior is reached. As a proxy for more complex problems,\nthis short paper considers the problem where each robot must locally estimate\nthe algebraic connectivity of the team's network topology.\n",
        "published": "2018",
        "authors": [
            "Amanda Prorok"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.09721v3",
        "title": "Safer End-to-End Autonomous Driving via Conditional Imitation Learning\n  and Command Augmentation",
        "abstract": "  Imitation learning is a promising approach to end-to-end training of\nautonomous vehicle controllers. Typically the driving process with such\napproaches is entirely automatic and black-box, although in practice it is\ndesirable to control the vehicle through high-level commands, such as telling\nit which way to go at an intersection. In existing work this has been\naccomplished by the application of a branched neural architecture, since\ndirectly providing the command as an additional input to the controller often\nresults in the command being ignored. In this work we overcome this limitation\nby learning a disentangled probabilistic latent variable model that generates\nthe steering commands. We achieve faithful command-conditional generation\nwithout using a branched architecture and demonstrate improved stability of the\ncontroller, applying only a variational objective without any domain-specific\nadjustments. On top of that, we extend our model with an additional latent\nvariable and augment the dataset to train a controller that is robust to unsafe\ncommands, such as asking it to turn into a wall. The main contribution of this\nwork is a recipe for building controllable imitation driving agents that\nimproves upon multiple aspects of the current state of the art relating to\nrobustness and interpretability.\n",
        "published": "2019",
        "authors": [
            "Renhao Wang",
            "Adam Scibior",
            "Frank Wood"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.10737v4",
        "title": "Multi-agent Interactive Prediction under Challenging Driving Scenarios",
        "abstract": "  In order to drive safely on the road, autonomous vehicle is expected to\npredict future outcomes of its surrounding environment and react properly. In\nfact, many researchers have been focused on solving behavioral prediction\nproblems for autonomous vehicles. However, very few of them consider\nmulti-agent prediction under challenging driving scenarios such as urban\nenvironment. In this paper, we proposed a prediction method that is able to\npredict various complicated driving scenarios where heterogeneous road\nentities, signal lights, and static map information are taken into account.\nMoreover, the proposed multi-agent interactive prediction (MAIP) system is\ncapable of simultaneously predicting any number of road entities while\nconsidering their mutual interactions. A case study of a simulated challenging\nurban intersection scenario is provided to demonstrate the performance and\ncapability of the proposed prediction system.\n",
        "published": "2019",
        "authors": [
            "Weihao Xuan",
            "Ruijie Ren"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.03158v1",
        "title": "Distributed Multi-agent Meta Learning for Trajectory Design in Wireless\n  Drone Networks",
        "abstract": "  In this paper, the problem of the trajectory design for a group of\nenergy-constrained drones operating in dynamic wireless network environments is\nstudied. In the considered model, a team of drone base stations (DBSs) is\ndispatched to cooperatively serve clusters of ground users that have dynamic\nand unpredictable uplink access demands. In this scenario, the DBSs must\ncooperatively navigate in the considered area to maximize coverage of the\ndynamic requests of the ground users. This trajectory design problem is posed\nas an optimization framework whose goal is to find optimal trajectories that\nmaximize the fraction of users served by all DBSs. To find an optimal solution\nfor this non-convex optimization problem under unpredictable environments, a\nvalue decomposition based reinforcement learning (VDRL) solution coupled with a\nmeta-training mechanism is proposed. This algorithm allows the DBSs to\ndynamically learn their trajectories while generalizing their learning to\nunseen environments. Analytical results show that, the proposed VD-RL algorithm\nis guaranteed to converge to a local optimal solution of the non-convex\noptimization problem. Simulation results show that, even without meta-training,\nthe proposed VD-RL algorithm can achieve a 53.2% improvement of the service\ncoverage and a 30.6% improvement in terms of the convergence speed, compared to\nbaseline multi-agent algorithms. Meanwhile, the use of meta-learning improves\nthe convergence speed of the VD-RL algorithm by up to 53.8% when the DBSs must\ndeal with a previously unseen task.\n",
        "published": "2020",
        "authors": [
            "Ye Hu",
            "Mingzhe Chen",
            "Walid Saad",
            "H. Vincent Poor",
            "Shuguang Cui"
        ]
    }
]