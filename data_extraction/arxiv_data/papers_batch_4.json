[
    {
        "id": "http://arxiv.org/abs/1706.01322v2",
        "title": "Deep learning evaluation using deep linguistic processing",
        "abstract": "  We discuss problems with the standard approaches to evaluation for tasks like\nvisual question answering, and argue that artificial data can be used to\naddress these as a complement to current practice. We demonstrate that with the\nhelp of existing 'deep' linguistic processing technology we are able to create\nchallenging abstract datasets, which enable us to investigate the language\nunderstanding abilities of multimodal deep learning models in detail, as\ncompared to a single performance value on a static and monolithic dataset.\n",
        "published": "2017",
        "authors": [
            "Alexander Kuhnle",
            "Ann Copestake"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.10470v2",
        "title": "What deep learning can tell us about higher cognitive functions like\n  mindreading?",
        "abstract": "  Can deep learning (DL) guide our understanding of computations happening in\nbiological brain? We will first briefly consider how DL has contributed to the\nresearch on visual object recognition. In the main part we will assess whether\nDL could also help us to clarify the computations underlying higher cognitive\nfunctions such as Theory of Mind. In addition, we will compare the objectives\nand learning signals of brains and machines, leading us to conclude that simply\nscaling up the current DL algorithms will most likely not lead to human level\nTheory of Mind.\n",
        "published": "2018",
        "authors": [
            "Jaan Aru",
            "Raul Vicente"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.08265v1",
        "title": "Deep learning at the shallow end: Malware classification for non-domain\n  experts",
        "abstract": "  Current malware detection and classification approaches generally rely on\ntime consuming and knowledge intensive processes to extract patterns\n(signatures) and behaviors from malware, which are then used for\nidentification. Moreover, these signatures are often limited to local,\ncontiguous sequences within the data whilst ignoring their context in relation\nto each other and throughout the malware file as a whole. We present a Deep\nLearning based malware classification approach that requires no expert domain\nknowledge and is based on a purely data driven approach for complex pattern and\nfeature identification.\n",
        "published": "2018",
        "authors": [
            "Quan Le",
            "Ois\u00edn Boydell",
            "Brian Mac Namee",
            "Mark Scanlon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.06905v1",
        "title": "User Association and Load Balancing for Massive MIMO through Deep\n  Learning",
        "abstract": "  This work investigates the use of deep learning to perform user cell\nassociation for sum-rate maximization in Massive MIMO networks. It is shown how\na deep neural network can be trained to approach the optimal association rule\nwith a much more limited computational complexity, thus enabling to update the\nassociation rule in real-time, on the basis of the mobility patterns of users.\nIn particular, the proposed neural network design requires as input only the\nusers' geographical positions. Numerical results show that it guarantees the\nsame performance of traditional optimization-oriented methods.\n",
        "published": "2018",
        "authors": [
            "Alessio Zappone",
            "Luca Sanguinetti",
            "Merouane Debbah"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.01192v1",
        "title": "CHISEL: Compression-Aware High-Accuracy Embedded Indoor Localization\n  with Deep Learning",
        "abstract": "  GPS technology has revolutionized the way we localize and navigate outdoors.\nHowever, the poor reception of GPS signals in buildings makes it unsuitable for\nindoor localization. WiFi fingerprinting-based indoor localization is one of\nthe most promising ways to meet this demand. Unfortunately, most work in the\ndomain fails to resolve challenges associated with deployability on\nresource-limited embedded devices. In this work, we propose a compression-aware\nand high-accuracy deep learning framework called CHISEL that outperforms the\nbest-known works in the area while maintaining localization robustness on\nembedded devices.\n",
        "published": "2021",
        "authors": [
            "Liping Wang",
            "Saideep Tiku",
            "Sudeep Pasricha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.06668v1",
        "title": "Thinkback: Task-SpecificOut-of-Distribution Detection",
        "abstract": "  The increased success of Deep Learning (DL) has recently sparked large-scale\ndeployment of DL models in many diverse industry segments. Yet, a crucial\nweakness of supervised model is the inherent difficulty in handling\nout-of-distribution samples, i.e., samples belonging to classes that were not\npresented to the model at training time. We propose in this paper a novel way\nto formulate the out-of-distribution detection problem, tailored for DL models.\nOur method does not require fine tuning process on training data, yet is\nsignificantly more accurate than the state of the art for out-of-distribution\ndetection.\n",
        "published": "2021",
        "authors": [
            "Lixuan Yang",
            "Dario Rossi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.08110v1",
        "title": "Image Captioning based on Deep Learning Methods: A Survey",
        "abstract": "  Image captioning is a challenging task and attracting more and more attention\nin the field of Artificial Intelligence, and which can be applied to efficient\nimage retrieval, intelligent blind guidance and human-computer interaction,\netc. In this paper, we present a survey on advances in image captioning based\non Deep Learning methods, including Encoder-Decoder structure, improved methods\nin Encoder, improved methods in Decoder, and other improvements. Furthermore,\nwe discussed future research directions.\n",
        "published": "2019",
        "authors": [
            "Yiyu Wang",
            "Jungang Xu",
            "Yingfei Sun",
            "Ben He"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.11262v1",
        "title": "DLSpec: A Deep Learning Task Exchange Specification",
        "abstract": "  Deep Learning (DL) innovations are being introduced at a rapid pace. However,\nthe current lack of standard specification of DL tasks makes sharing, running,\nreproducing, and comparing these innovations difficult. To address this\nproblem, we propose DLSpec, a model-, dataset-, software-, and\nhardware-agnostic DL specification that captures the different aspects of DL\ntasks. DLSpec has been tested by specifying and running hundreds of DL tasks.\n",
        "published": "2020",
        "authors": [
            "Abdul Dakkak",
            "Cheng Li",
            "Jinjun Xiong",
            "Wen-Mei Hwu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.02834v1",
        "title": "Augmenting Organizational Decision-Making with Deep Learning Algorithms:\n  Principles, Promises, and Challenges",
        "abstract": "  The current expansion of theory and research on artificial intelligence in\nmanagement and organization studies has revitalized the theory and research on\ndecision-making in organizations. In particular, recent advances in deep\nlearning (DL) algorithms promise benefits for decision-making within\norganizations, such as assisting employees with information processing, thereby\naugment their analytical capabilities and perhaps help their transition to more\ncreative work.\n",
        "published": "2020",
        "authors": [
            "Yash Raj Shrestha",
            "Vaibhav Krishna",
            "Georg von Krogh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.07368v2",
        "title": "Conformer-Kernel with Query Term Independence at TREC 2020 Deep Learning\n  Track",
        "abstract": "  We benchmark Conformer-Kernel models under the strict blind evaluation\nsetting of the TREC 2020 Deep Learning track. In particular, we study the\nimpact of incorporating: (i) Explicit term matching to complement matching\nbased on learned representations (i.e., the \"Duet principle\"), (ii) query term\nindependence (i.e., the \"QTI assumption\") to scale the model to the full\nretrieval setting, and (iii) the ORCAS click data as an additional document\ndescription field. We find evidence which supports that all three\naforementioned strategies can lead to improved retrieval quality.\n",
        "published": "2020",
        "authors": [
            "Bhaskar Mitra",
            "Sebastian Hofstatter",
            "Hamed Zamani",
            "Nick Craswell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.00818v1",
        "title": "Proceedings of the DATE Friday Workshop on System-level Design Methods\n  for Deep Learning on Heterogeneous Architectures (SLOHA 2021)",
        "abstract": "  This volume contains the papers accepted at the first DATE Friday Workshop on\nSystem-level Design Methods for Deep Learning on Heterogeneous Architectures\n(SLOHA 2021), held virtually on February 5, 2021. SLOHA 2021 was co-located\nwith the Conference on Design, Automation and Test in Europe (DATE).\n",
        "published": "2021",
        "authors": [
            "Frank Hannig",
            "Paolo Meloni",
            "Matteo Spallanzani",
            "Matthias Ziegler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.04425v1",
        "title": "Arabic Speech Emotion Recognition Employing Wav2vec2.0 and HuBERT Based\n  on BAVED Dataset",
        "abstract": "  Recently, there have been tremendous research outcomes in the fields of\nspeech recognition and natural language processing. This is due to the\nwell-developed multi-layers deep learning paradigms such as wav2vec2.0,\nWav2vecU, WavBERT, and HuBERT that provide better representation learning and\nhigh information capturing. Such paradigms run on hundreds of unlabeled data,\nthen fine-tuned on a small dataset for specific tasks. This paper introduces a\ndeep learning constructed emotional recognition model for Arabic speech\ndialogues. The developed model employs the state of the art audio\nrepresentations include wav2vec2.0 and HuBERT. The experiment and performance\nresults of our model overcome the previous known outcomes.\n",
        "published": "2021",
        "authors": [
            "Omar Mohamed",
            "Salah A. Aly"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.06679v2",
        "title": "deepstruct -- linking deep learning and graph theory",
        "abstract": "  deepstruct connects deep learning models and graph theory such that different\ngraph structures can be imposed on neural networks or graph structures can be\nextracted from trained neural network models. For this, deepstruct provides\ndeep neural network models with different restrictions which can be created\nbased on an initial graph. Further, tools to extract graph structures from\ntrained models are available. This step of extracting graphs can be\ncomputationally expensive even for models of just a few dozen thousand\nparameters and poses a challenging problem. deepstruct supports research in\npruning, neural architecture search, automated network design and structure\nanalysis of neural networks.\n",
        "published": "2021",
        "authors": [
            "Julian Stier",
            "Michael Granitzer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.04977v1",
        "title": "Bringing Atomistic Deep Learning to Prime Time",
        "abstract": "  Artificial intelligence has not yet revolutionized the design of materials\nand molecules. In this perspective, we identify four barriers preventing the\nintegration of atomistic deep learning, molecular science, and high-performance\ncomputing. We outline focused research efforts to address the opportunities\npresented by these challenges.\n",
        "published": "2021",
        "authors": [
            "Nathan C. Frey",
            "Siddharth Samsi",
            "Bharath Ramsundar",
            "Connor W. Coley",
            "Vijay Gadepally"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.00199v1",
        "title": "The GatedTabTransformer. An enhanced deep learning architecture for\n  tabular modeling",
        "abstract": "  There is an increasing interest in the application of deep learning\narchitectures to tabular data. One of the state-of-the-art solutions is\nTabTransformer which incorporates an attention mechanism to better track\nrelationships between categorical features and then makes use of a standard MLP\nto output its final logits. In this paper we propose multiple modifications to\nthe original TabTransformer performing better on binary classification tasks\nfor three separate datasets with more than 1% AUROC gains. Inspired by gated\nMLP, linear projections are implemented in the MLP block and multiple\nactivation functions are tested. We also evaluate the importance of specific\nhyper parameters during training.\n",
        "published": "2022",
        "authors": [
            "Radostin Cholakov",
            "Todor Kolev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.09199v1",
        "title": "Deep Learning on Attributed Sequences",
        "abstract": "  Recent research in feature learning has been extended to sequence data, where\neach instance consists of a sequence of heterogeneous items with a variable\nlength. However, in many real-world applications, the data exists in the form\nof attributed sequences, which is composed of a set of fixed-size attributes\nand variable-length sequences with dependencies between them. In the attributed\nsequence context, feature learning remains challenging due to the dependencies\nbetween sequences and their associated attributes. In this dissertation, we\nfocus on analyzing and building deep learning models for four new problems on\nattributed sequences. Our extensive experiments on real-world datasets\ndemonstrate that the proposed solutions significantly improve the performance\nof each task over the state-of-the-art methods on attributed sequences.\n",
        "published": "2022",
        "authors": [
            "Zhongfang Zhuang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.09051v1",
        "title": "PR-DAD: Phase Retrieval Using Deep Auto-Decoders",
        "abstract": "  Phase retrieval is a well known ill-posed inverse problem where one tries to\nrecover images given only the magnitude values of their Fourier transform as\ninput. In recent years, new algorithms based on deep learning have been\nproposed, providing breakthrough results that surpass the results of the\nclassical methods. In this work we provide a novel deep learning architecture\nPR-DAD (Phase Retrieval Using Deep Auto- Decoders), whose components are\ncarefully designed based on mathematical modeling of the phase retrieval\nproblem. The architecture provides experimental results that surpass all\ncurrent results.\n",
        "published": "2022",
        "authors": [
            "Leon Gugel",
            "Shai Dekel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.04425v1",
        "title": "The Role Of Biology In Deep Learning",
        "abstract": "  Artificial neural networks took a lot of inspiration from their biological\ncounterparts in becoming our best machine perceptual systems. This work\nsummarizes some of that history and incorporates modern theoretical\nneuroscience into experiments with artificial neural networks from the field of\ndeep learning. Specifically, iterative magnitude pruning is used to train\nsparsely connected networks with 33x fewer weights without loss in performance.\nThese are used to test and ultimately reject the hypothesis that weight\nsparsity alone improves image noise robustness. Recent work mitigated\ncatastrophic forgetting using weight sparsity, activation sparsity, and active\ndendrite modeling. This paper replicates those findings, and extends the method\nto train convolutional neural networks on a more challenging continual learning\ntask. The code has been made publicly available.\n",
        "published": "2022",
        "authors": [
            "Robert Bain"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.03538v1",
        "title": "Evolutionary Deep Nets for Non-Intrusive Load Monitoring",
        "abstract": "  Non-Intrusive Load Monitoring (NILM) is an energy efficiency technique to\ntrack electricity consumption of an individual appliance in a household by one\naggregated single, such as building level meter readings. The goal of NILM is\nto disaggregate the appliance from the aggregated singles by computational\nmethod. In this work, deep learning approaches are implemented to operate the\ndesegregations. Deep neural networks, convolutional neural networks, and\nrecurrent neural networks are employed for this operation. Additionally, sparse\nevolutionary training is applied to accelerate training efficiency of each deep\nlearning model. UK-Dale dataset is used for this work.\n",
        "published": "2023",
        "authors": [
            "Jinsong Wang",
            "Kenneth A. Loparo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.07502v1",
        "title": "Meta-learning approaches for few-shot learning: A survey of recent\n  advances",
        "abstract": "  Despite its astounding success in learning deeper multi-dimensional data, the\nperformance of deep learning declines on new unseen tasks mainly due to its\nfocus on same-distribution prediction. Moreover, deep learning is notorious for\npoor generalization from few samples. Meta-learning is a promising approach\nthat addresses these issues by adapting to new tasks with few-shot datasets.\nThis survey first briefly introduces meta-learning and then investigates\nstate-of-the-art meta-learning methods and recent advances in: (I)\nmetric-based, (II) memory-based, (III), and learning-based methods. Finally,\ncurrent challenges and insights for future researches are discussed.\n",
        "published": "2023",
        "authors": [
            "Hassan Gharoun",
            "Fereshteh Momenifar",
            "Fang Chen",
            "Amir H. Gandomi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.12032v1",
        "title": "The Representational Status of Deep Learning Models",
        "abstract": "  This paper aims to clarify the representational status of Deep Learning\nModels (DLMs). While commonly referred to as 'representations', what this\nentails is ambiguous due to a conflation of functional and relational\nconceptions of representation. This paper argues that while DLMs represent\ntheir targets in a relational sense, they are best understood as highly\nidealized models. This result has immediate implications for explainable AI\n(XAI) and directs philosophical attention toward examining the idealized nature\nof DLM representations and their role in future scientific investigation.\n",
        "published": "2023",
        "authors": [
            "Eamon Duede"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.14498v1",
        "title": "MWaste: A Deep Learning Approach to Manage Household Waste",
        "abstract": "  Computer vision methods have shown to be effective in classifying garbage\ninto recycling categories for waste processing, existing methods are costly,\nimprecise, and unclear. To tackle this issue, we introduce MWaste, a mobile\napplication that uses computer vision and deep learning techniques to classify\nwaste materials as trash, plastic, paper, metal, glass or cardboard. Its\neffectiveness was tested on various neural network architectures and real-world\nimages, achieving an average precision of 92\\% on the test set. This app can\nhelp combat climate change by enabling efficient waste processing and reducing\nthe generation of greenhouse gases caused by incorrect waste disposal.\n",
        "published": "2023",
        "authors": [
            "Suman Kunwar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.06269v1",
        "title": "DeepLCZChange: A Remote Sensing Deep Learning Model Architecture for\n  Urban Climate Resilience",
        "abstract": "  Urban land use structures impact local climate conditions of metropolitan\nareas. To shed light on the mechanism of local climate wrt. urban land use, we\npresent a novel, data-driven deep learning architecture and pipeline,\nDeepLCZChange, to correlate airborne LiDAR data statistics with the Landsat 8\nsatellite's surface temperature product. A proof-of-concept numerical\nexperiment utilizes corresponding remote sensing data for the city of New York\nto verify the cooling effect of urban forests.\n",
        "published": "2023",
        "authors": [
            "Wenlu Sun",
            "Yao Sun",
            "Chenying Liu",
            "Conrad M Albrecht"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.00350v1",
        "title": "Learning Green's Function Efficiently Using Low-Rank Approximations",
        "abstract": "  Learning the Green's function using deep learning models enables to solve\ndifferent classes of partial differential equations. A practical limitation of\nusing deep learning for the Green's function is the repeated computationally\nexpensive Monte-Carlo integral approximations. We propose to learn the Green's\nfunction by low-rank decomposition, which results in a novel architecture to\nremove redundant computations by separate learning with domain data for\nevaluation and Monte-Carlo samples for integral approximation. Using\nexperiments we show that the proposed method improves computational time\ncompared to MOD-Net while achieving comparable accuracy compared to both PINNs\nand MOD-Net.\n",
        "published": "2023",
        "authors": [
            "Kishan Wimalawarne",
            "Taiji Suzuki",
            "Sophie Langer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.02314v1",
        "title": "Thermal Face Image Classification using Deep Learning Techniques",
        "abstract": "  Thermal images have various applications in security, medical and industrial\ndomains. This paper proposes a practical deep-learning approach for thermal\nimage classification. Accurate and efficient classification of thermal images\nposes a significant challenge across various fields due to the complex image\ncontent and the scarcity of annotated datasets. This work uses a convolutional\nneural network (CNN) architecture, specifically ResNet-50 and VGGNet-19, to\nextract features from thermal images. This work also applied Kalman filter on\nthermal input images for image denoising. The experimental results demonstrate\nthe effectiveness of the proposed approach in terms of accuracy and efficiency.\n",
        "published": "2023",
        "authors": [
            "Prosenjit Chatterjee",
            "ANK Zaman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.08170v1",
        "title": "Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning\n  Approach",
        "abstract": "  Lattice reduction is a combinatorial optimization problem aimed at finding\nthe most orthogonal basis in a given lattice. In this work, we address lattice\nreduction via deep learning methods. We design a deep neural model outputting\nfactorized unimodular matrices and train it in a self-supervised manner by\npenalizing non-orthogonal lattice bases. We incorporate the symmetries of\nlattice reduction into the model by making it invariant and equivariant with\nrespect to appropriate continuous and discrete groups.\n",
        "published": "2023",
        "authors": [
            "Giovanni Luca Marchetti",
            "Gabriele Cesa",
            "Kumar Pratik",
            "Arash Behboodi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.15565v3",
        "title": "Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing\n  AI-Generated Text",
        "abstract": "  My research investigates the use of cutting-edge hybrid deep learning models\nto accurately differentiate between AI-generated text and human writing. I\napplied a robust methodology, utilising a carefully selected dataset comprising\nAI and human texts from various sources, each tagged with instructions.\nAdvanced natural language processing techniques facilitated the analysis of\ntextual features. Combining sophisticated neural networks, the custom model\nenabled it to detect nuanced differences between AI and human content.\n",
        "published": "2023",
        "authors": [
            "Abiodun Finbarrs Oketunji"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.10231v1",
        "title": "Proactive and Reactive Engagement of Artificial Intelligence Methods for\n  Education: A Review",
        "abstract": "  Quality education, one of the seventeen sustainable development goals (SDGs)\nidentified by the United Nations General Assembly, stands to benefit enormously\nfrom the adoption of artificial intelligence (AI) driven tools and\ntechnologies. The concurrent boom of necessary infrastructure, digitized data\nand general social awareness has propelled massive research and development\nefforts in the artificial intelligence for education (AIEd) sector. In this\nreview article, we investigate how artificial intelligence, machine learning\nand deep learning methods are being utilized to support students, educators and\nadministrative staff. We do this through the lens of a novel categorization\napproach. We consider the involvement of AI-driven methods in the education\nprocess in its entirety - from students admissions, course scheduling etc. in\nthe proactive planning phase to knowledge delivery, performance assessment etc.\nin the reactive execution phase. We outline and analyze the major research\ndirections under proactive and reactive engagement of AI in education using a\nrepresentative group of 194 original research articles published in the past\ntwo decades i.e., 2003 - 2022. We discuss the paradigm shifts in the solution\napproaches proposed, i.e., in the choice of data and algorithms used over this\ntime. We further dive into how the COVID-19 pandemic challenged and reshaped\nthe education landscape at the fag end of this time period. Finally, we\npinpoint existing limitations in adopting artificial intelligence for education\nand reflect on the path forward.\n",
        "published": "2023",
        "authors": [
            "Sruti Mallik",
            "Ahana Gangopadhyay"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.01301v2",
        "title": "Reliable AI: Does the Next Generation Require Quantum Computing?",
        "abstract": "  In this survey, we aim to explore the fundamental question of whether the\nnext generation of artificial intelligence requires quantum computing.\nArtificial intelligence is increasingly playing a crucial role in many aspects\nof our daily lives and is central to the fourth industrial revolution. It is\ntherefore imperative that artificial intelligence is reliable and trustworthy.\nHowever, there are still many issues with reliability of artificial\nintelligence, such as privacy, responsibility, safety, and security, in areas\nsuch as autonomous driving, healthcare, robotics, and others. These problems\ncan have various causes, including insufficient data, biases, and robustness\nproblems, as well as fundamental issues such as computability problems on\ndigital hardware. The cause of these computability problems is rooted in the\nfact that digital hardware is based on the computing model of the Turing\nmachine, which is inherently discrete. Notably, our findings demonstrate that\ndigital hardware is inherently constrained in solving problems about\noptimization, deep learning, or differential equations. Therefore, these\nlimitations carry substantial implications for the field of artificial\nintelligence, in particular for machine learning. Furthermore, although it is\nwell known that the quantum computer shows a quantum advantage for certain\nclasses of problems, our findings establish that some of these limitations\npersist when employing quantum computing models based on the quantum circuit or\nthe quantum Turing machine paradigm. In contrast, analog computing models, such\nas the Blum-Shub-Smale machine, exhibit the potential to surmount these\nlimitations.\n",
        "published": "2023",
        "authors": [
            "Aras Bacho",
            "Holger Boche",
            "Gitta Kutyniok"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.10041v1",
        "title": "Data Curation and Quality Assurance for Machine Learning-based Cyber\n  Intrusion Detection",
        "abstract": "  Intrusion detection is an essential task in the cyber threat environment.\nMachine learning and deep learning techniques have been applied for intrusion\ndetection. However, most of the existing research focuses on the model work but\nignores the fact that poor data quality has a direct impact on the performance\nof a machine learning system. More attention should be paid to the data work\nwhen building a machine learning-based intrusion detection system. This article\nfirst summarizes existing machine learning-based intrusion detection systems\nand the datasets used for building these systems. Then the data preparation\nworkflow and quality requirements for intrusion detection are discussed. To\nfigure out how data and models affect machine learning performance, we\nconducted experiments on 11 HIDS datasets using seven machine learning models\nand three deep learning models. The experimental results show that BERT and GPT\nwere the best algorithms for HIDS on all of the datasets. However, the\nperformance on different datasets varies, indicating the differences between\nthe data quality of these datasets. We then evaluate the data quality of the 11\ndatasets based on quality dimensions proposed in this paper to determine the\nbest characteristics that a HIDS dataset should possess in order to yield the\nbest possible result. This research initiates a data quality perspective for\nresearchers and practitioners to improve the performance of machine\nlearning-based intrusion detection.\n",
        "published": "2021",
        "authors": [
            "Haihua Chen",
            "Ngan Tran",
            "Anand Sagar Thumati",
            "Jay Bhuyan",
            "Junhua Ding"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.00408v1",
        "title": "Blackbox Trojanising of Deep Learning Models : Using non-intrusive\n  network structure and binary alterations",
        "abstract": "  Recent advancements in Artificial Intelligence namely in Deep Learning has\nheightened its adoption in many applications. Some are playing important roles\nto the extent that we are heavily dependent on them for our livelihood.\nHowever, as with all technologies, there are vulnerabilities that malicious\nactors could exploit. A form of exploitation is to turn these technologies,\nintended for good, to become dual-purposed instruments to support deviant acts\nlike malicious software trojans. As part of proactive defense, researchers are\nproactively identifying such vulnerabilities so that protective measures could\nbe developed subsequently. This research explores a novel blackbox trojanising\napproach using a simple network structure modification to any deep learning\nimage classification model that would transform a benign model into a deviant\none with a simple manipulation of the weights to induce specific types of\nerrors. Propositions to protect the occurrence of such simple exploits are\ndiscussed in this research. This research highlights the importance of\nproviding sufficient safeguards to these models so that the intended good of AI\ninnovation and adoption may be protected.\n",
        "published": "2020",
        "authors": [
            "Jonathan Pan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.14824v2",
        "title": "Explainable Artificial Intelligence for Manufacturing Cost Estimation\n  and Machining Feature Visualization",
        "abstract": "  Studies on manufacturing cost prediction based on deep learning have begun in\nrecent years, but the cost prediction rationale cannot be explained because the\nmodels are still used as a black box. This study aims to propose a\nmanufacturing cost prediction process for 3D computer-aided design (CAD) models\nusing explainable artificial intelligence. The proposed process can visualize\nthe machining features of the 3D CAD model that are influencing the increase in\nmanufacturing costs. The proposed process consists of (1) data collection and\npre-processing, (2) 3D deep learning architecture exploration, and (3)\nvisualization to explain the prediction results. The proposed deep learning\nmodel shows high predictability of manufacturing cost for the computer\nnumerical control (CNC) machined parts. In particular, using 3D\ngradient-weighted class activation mapping proves that the proposed model not\nonly can detect the CNC machining features but also can differentiate the\nmachining difficulty for the same feature. Using the proposed process, we can\nprovide a design guidance to engineering designers in reducing manufacturing\ncosts during the conceptual design phase. We can also provide real-time\nquotations and redesign proposals to online manufacturing platform customers.\n",
        "published": "2020",
        "authors": [
            "Soyoung Yoo",
            "Namwoo Kang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.05276v1",
        "title": "Advances in Natural Language Question Answering: A Review",
        "abstract": "  Question Answering has recently received high attention from artificial\nintelligence communities due to the advancements in learning technologies.\nEarly question answering models used rule-based approaches and moved to the\nstatistical approach to address the vastly available information. However,\nstatistical approaches are shown to underperform in handling the dynamic nature\nand the variation of language. Therefore, learning models have shown the\ncapability of handling the dynamic nature and variations in language. Many deep\nlearning methods have been introduced to question answering. Most of the deep\nlearning approaches have shown to achieve higher results compared to machine\nlearning and statistical methods. The dynamic nature of language has profited\nfrom the nonlinear learning in deep learning. This has created prominent\nsuccess and a spike in work on question answering. This paper discusses the\nsuccesses and challenges in question answering question answering systems and\ntechniques that are used in these challenges.\n",
        "published": "2019",
        "authors": [
            "K. S. D. Ishwari",
            "A. K. R. R. Aneeze",
            "S. Sudheesan",
            "H. J. D. A. Karunaratne",
            "A. Nugaliyadde",
            "Y. Mallawarrachchi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.08225v2",
        "title": "Deep Learning for Community Detection: Progress, Challenges and\n  Opportunities",
        "abstract": "  As communities represent similar opinions, similar functions, similar\npurposes, etc., community detection is an important and extremely useful tool\nin both scientific inquiry and data analytics. However, the classic methods of\ncommunity detection, such as spectral clustering and statistical inference, are\nfalling by the wayside as deep learning techniques demonstrate an increasing\ncapacity to handle high-dimensional graph data with impressive performance.\nThus, a survey of current progress in community detection through deep learning\nis timely. Structured into three broad research streams in this domain - deep\nneural networks, deep graph embedding, and graph neural networks, this article\nsummarizes the contributions of the various frameworks, models, and algorithms\nin each stream along with the current challenges that remain unsolved and the\nfuture research opportunities yet to be explored.\n",
        "published": "2020",
        "authors": [
            "Fanzhen Liu",
            "Shan Xue",
            "Jia Wu",
            "Chuan Zhou",
            "Wenbin Hu",
            "Cecile Paris",
            "Surya Nepal",
            "Jian Yang",
            "Philip S. Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.02395v3",
        "title": "Ensemble deep learning: A review",
        "abstract": "  Ensemble learning combines several individual models to obtain better\ngeneralization performance. Currently, deep learning architectures are showing\nbetter performance compared to the shallow or traditional models. Deep ensemble\nlearning models combine the advantages of both the deep learning models as well\nas the ensemble learning such that the final model has better generalization\nperformance. This paper reviews the state-of-art deep ensemble models and hence\nserves as an extensive summary for the researchers. The ensemble models are\nbroadly categorised into bagging, boosting, stacking, negative correlation\nbased deep ensemble models, explicit/implicit ensembles,\nhomogeneous/heterogeneous ensemble, decision fusion strategies based deep\nensemble models. Applications of deep ensemble models in different domains are\nalso briefly discussed. Finally, we conclude this paper with some potential\nfuture research directions.\n",
        "published": "2021",
        "authors": [
            "M. A. Ganaie",
            "Minghui Hu",
            "A. K. Malik",
            "M. Tanveer",
            "P. N. Suganthan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.09383v1",
        "title": "Neuro-Symbolic Forward Reasoning",
        "abstract": "  Reasoning is an essential part of human intelligence and thus has been a\nlong-standing goal in artificial intelligence research. With the recent success\nof deep learning, incorporating reasoning with deep learning systems, i.e.,\nneuro-symbolic AI has become a major field of interest. We propose the\nNeuro-Symbolic Forward Reasoner (NSFR), a new approach for reasoning tasks\ntaking advantage of differentiable forward-chaining using first-order logic.\nThe key idea is to combine differentiable forward-chaining reasoning with\nobject-centric (deep) learning. Differentiable forward-chaining reasoning\ncomputes logical entailments smoothly, i.e., it deduces new facts from given\nfacts and rules in a differentiable manner. The object-centric learning\napproach factorizes raw inputs into representations in terms of objects. Thus,\nit allows us to provide a consistent framework to perform the forward-chaining\ninference from raw inputs. NSFR factorizes the raw inputs into the\nobject-centric representations, converts them into probabilistic ground atoms,\nand finally performs differentiable forward-chaining inference using weighted\nrules for inference. Our comprehensive experimental evaluations on\nobject-centric reasoning data sets, 2D Kandinsky patterns and 3D CLEVR-Hans,\nand a variety of tasks show the effectiveness and advantage of our approach.\n",
        "published": "2021",
        "authors": [
            "Hikaru Shindo",
            "Devendra Singh Dhami",
            "Kristian Kersting"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.00190v1",
        "title": "Is the use of Deep Learning and Artificial Intelligence an appropriate\n  means to locate debris in the ocean without harming aquatic wildlife?",
        "abstract": "  With the global issue of plastic debris ever expanding, it is about time that\nthe technology industry stepped in. This study aims to assess whether deep\nlearning can successfully distinguish between marine life and man-made debris\nunderwater. The aim is to find if we are safely able to clean up our oceans\nwith Artificial Intelligence without disrupting the delicate balance of the\naquatic ecosystems. The research explores the use of Convolutional Neural\nNetworks from the perspective of protecting the ecosystem, rather than\nprimarily collecting rubbish. We did this by building a custom-built, deep\nlearning model, with an original database including 1,644 underwater images and\nused a binary classification to sort synthesised material from aquatic life. We\nconcluded that although it is possible to safely distinguish between debris and\nlife, further exploration with a larger database and stronger CNN structure has\nthe potential for much more promising results.\n",
        "published": "2021",
        "authors": [
            "Zoe Moorton",
            "Zeyneb Kurt",
            "Wai Lok Woo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.02000v1",
        "title": "Disentangling private classes through regularization",
        "abstract": "  Deep learning models are nowadays broadly deployed to solve an incredibly\nlarge variety of tasks. However, little attention has been devoted to connected\nlegal aspects. In 2016, the European Union approved the General Data Protection\nRegulation which entered into force in 2018. Its main rationale was to protect\nthe privacy and data protection of its citizens by the way of operating of the\nso-called \"Data Economy\". As data is the fuel of modern Artificial\nIntelligence, it is argued that the GDPR can be partly applicable to a series\nof algorithmic decision making tasks before a more structured AI Regulation\nenters into force. In the meantime, AI should not allow undesired information\nleakage deviating from the purpose for which is created. In this work we\npropose DisP, an approach for deep learning models disentangling the\ninformation related to some classes we desire to keep private, from the data\nprocessed by AI. In particular, DisP is a regularization strategy\nde-correlating the features belonging to the same private class at training\ntime, hiding the information of private classes membership. Our experiments on\nstate-of-the-art deep learning models show the effectiveness of DisP,\nminimizing the risk of extraction for the classes we desire to keep private.\n",
        "published": "2022",
        "authors": [
            "Enzo Tartaglione",
            "Francesca Gennari",
            "Marco Grangetto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.03374v1",
        "title": "Deep Causal Learning: Representation, Discovery and Inference",
        "abstract": "  Causal learning has attracted much attention in recent years because\ncausality reveals the essential relationship between things and indicates how\nthe world progresses. However, there are many problems and bottlenecks in\ntraditional causal learning methods, such as high-dimensional unstructured\nvariables, combinatorial optimization problems, unknown intervention,\nunobserved confounders, selection bias and estimation bias. Deep causal\nlearning, that is, causal learning based on deep neural networks, brings new\ninsights for addressing these problems. While many deep learning-based causal\ndiscovery and causal inference methods have been proposed, there is a lack of\nreviews exploring the internal mechanism of deep learning to improve causal\nlearning. In this article, we comprehensively review how deep learning can\ncontribute to causal learning by addressing conventional challenges from three\naspects: representation, discovery, and inference. We point out that deep\ncausal learning is important for the theoretical extension and application\nexpansion of causal science and is also an indispensable part of general\nartificial intelligence. We conclude the article with a summary of open issues\nand potential directions for future work.\n",
        "published": "2022",
        "authors": [
            "Zizhen Deng",
            "Xiaolong Zheng",
            "Hu Tian",
            "Daniel Dajun Zeng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.16236v1",
        "title": "Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning:\n  A Survey",
        "abstract": "  Recently emerged technologies based on Deep Learning (DL) achieved\noutstanding results on a variety of tasks in the field of Artificial\nIntelligence (AI). However, these encounter several challenges related to\nrobustness to adversarial inputs, ecological impact, and the necessity of huge\namounts of training data. In response, researchers are focusing more and more\ninterest on biologically grounded mechanisms, which are appealing due to the\nimpressive capabilities exhibited by biological brains. This survey explores a\nrange of these biologically inspired models of synaptic plasticity, their\napplication in DL scenarios, and the connections with models of plasticity in\nSpiking Neural Networks (SNNs). Overall, Bio-Inspired Deep Learning (BIDL)\nrepresents an exciting research direction, aiming at advancing not only our\ncurrent technologies but also our understanding of intelligence.\n",
        "published": "2023",
        "authors": [
            "Gabriele Lagani",
            "Fabrizio Falchi",
            "Claudio Gennaro",
            "Giuseppe Amato"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.06481v3",
        "title": "A Survey on Machine Learning Approaches for Modelling Intuitive Physics",
        "abstract": "  Research in cognitive science has provided extensive evidence of human\ncognitive ability in performing physical reasoning of objects from noisy\nperceptual inputs. Such a cognitive ability is commonly known as intuitive\nphysics. With advancements in deep learning, there is an increasing interest in\nbuilding intelligent systems that are capable of performing physical reasoning\nfrom a given scene for the purpose of building better AI systems. As a result,\nmany contemporary approaches in modelling intuitive physics for machine\ncognition have been inspired by literature from cognitive science. Despite the\nwide range of work in physical reasoning for machine cognition, there is a\nscarcity of reviews that organize and group these deep learning approaches.\nEspecially at the intersection of intuitive physics and artificial\nintelligence, there is a need to make sense of the diverse range of ideas and\napproaches. Therefore, this paper presents a comprehensive survey of recent\nadvances and techniques in intuitive physics-inspired deep learning approaches\nfor physical reasoning. The survey will first categorize existing deep learning\napproaches into three facets of physical reasoning before organizing them into\nthree general technical approaches and propose six categorical tasks of the\nfield. Finally, we highlight the challenges of the current field and present\nsome future research directions.\n",
        "published": "2022",
        "authors": [
            "Jiafei Duan",
            "Arijit Dasgupta",
            "Jason Fischer",
            "Cheston Tan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.09844v2",
        "title": "Modular Mechanistic Networks: On Bridging Mechanistic and\n  Phenomenological Models with Deep Neural Networks in Natural Language\n  Processing",
        "abstract": "  Natural language processing (NLP) can be done using either top-down (theory\ndriven) and bottom-up (data driven) approaches, which we call mechanistic and\nphenomenological respectively. The approaches are frequently considered to\nstand in opposition to each other. Examining some recent approaches in deep\nlearning we argue that deep neural networks incorporate both perspectives and,\nfurthermore, that leveraging this aspect of deep learning may help in solving\ncomplex problems within language technology, such as modelling language and\nperception in the domain of spatial cognition.\n",
        "published": "2018",
        "authors": [
            "Simon Dobnik",
            "John D. Kelleher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.02256v2",
        "title": "Artificial Intelligence and Machine Learning to Predict and Improve\n  Efficiency in Manufacturing Industry",
        "abstract": "  The overall equipment effectiveness (OEE) is a performance measurement metric\nwidely used. Its calculation provides to the managers the possibility to\nidentify the main losses that reduce the machine effectiveness and then take\nthe necessary decisions in order to improve the situation. However, this\ncalculation is done a-posterior which is often too late. In the present\nresearch, we implemented different Machine Learning algorithms namely; Support\nvector machine, Optimized Support vector Machine (using Genetic Algorithm),\nRandom Forest, XGBoost and Deep Learning to predict the estimate OEE value. The\ndata used to train our models was provided by an automotive cable production\nindustry. The results show that the Deep Learning and Random Forest are more\naccurate and present better performance for the prediction of the overall\nequipment effectiveness in our case study.\n",
        "published": "2019",
        "authors": [
            "Ibtissam El Hassani",
            "Choumicha El Mazgualdi",
            "Tawfik Masrour"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.10181v1",
        "title": "Operating critical machine learning models in resource constrained\n  regimes",
        "abstract": "  The accelerated development of machine learning methods, primarily deep\nlearning, are causal to the recent breakthroughs in medical image analysis and\ncomputer aided intervention. The resource consumption of deep learning models\nin terms of amount of training data, compute and energy costs are known to be\nmassive. These large resource costs can be barriers in deploying these models\nin clinics, globally. To address this, there are cogent efforts within the\nmachine learning community to introduce notions of resource efficiency. For\ninstance, using quantisation to alleviate memory consumption. While most of\nthese methods are shown to reduce the resource utilisation, they could come at\na cost in performance. In this work, we probe into the trade-off between\nresource consumption and performance, specifically, when dealing with models\nthat are used in critical settings such as in clinics.\n",
        "published": "2023",
        "authors": [
            "Raghavendra Selvan",
            "Julian Sch\u00f6n",
            "Erik B Dam"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.15210v1",
        "title": "Topology combined machine learning for consonant recognition",
        "abstract": "  In artificial-intelligence-aided signal processing, existing deep learning\nmodels often exhibit a black-box structure, and their validity and\ncomprehensibility remain elusive. The integration of topological methods,\ndespite its relatively nascent application, serves a dual purpose of making\nmodels more interpretable as well as extracting structural information from\ntime-dependent data for smarter learning. Here, we provide a transparent and\nbroadly applicable methodology, TopCap, to capture the most salient topological\nfeatures inherent in time series for machine learning. Rooted in\nhigh-dimensional ambient spaces, TopCap is capable of capturing features rarely\ndetected in datasets with low intrinsic dimensionality. Applying time-delay\nembedding and persistent homology, we obtain descriptors which encapsulate\ninformation such as the vibration of a time series, in terms of its variability\nof frequency, amplitude, and average line, demonstrated with simulated data.\nThis information is then vectorised and fed into multiple machine learning\nalgorithms such as k-nearest neighbours and support vector machine. Notably, in\nclassifying voiced and voiceless consonants, TopCap achieves an accuracy\nexceeding 96% and is geared towards designing topological convolutional layers\nfor deep learning of speech and audio signals.\n",
        "published": "2023",
        "authors": [
            "Pingyao Feng",
            "Siheng Yi",
            "Qingrui Qu",
            "Zhiwang Yu",
            "Yifei Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.04726v1",
        "title": "Model-Based Machine Learning for Communications",
        "abstract": "  We present an introduction to model-based machine learning for communication\nsystems. We begin by reviewing existing strategies for combining model-based\nalgorithms and machine learning from a high level perspective, and compare them\nto the conventional deep learning approach which utilizes established deep\nneural network (DNN) architectures trained in an end-to-end manner. Then, we\nfocus on symbol detection, which is one of the fundamental tasks of\ncommunication receivers. We show how the different strategies of conventional\ndeep architectures, deep unfolding, and DNN-aided hybrid algorithms, can be\napplied to this problem. The last two approaches constitute a middle ground\nbetween purely model-based and solely DNN-based receivers. By focusing on this\nspecific task, we highlight the advantages and drawbacks of each strategy, and\npresent guidelines to facilitate the design of future model-based deep learning\nsystems for communications.\n",
        "published": "2021",
        "authors": [
            "Nir Shlezinger",
            "Nariman Farsad",
            "Yonina C. Eldar",
            "Andrea J. Goldsmith"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.12700v2",
        "title": "Towards Transparent Application of Machine Learning in Video Processing",
        "abstract": "  Machine learning techniques for more efficient video compression and video\nenhancement have been developed thanks to breakthroughs in deep learning. The\nnew techniques, considered as an advanced form of Artificial Intelligence (AI),\nbring previously unforeseen capabilities. However, they typically come in the\nform of resource-hungry black-boxes (overly complex with little transparency\nregarding the inner workings). Their application can therefore be unpredictable\nand generally unreliable for large-scale use (e.g. in live broadcast). The aim\nof this work is to understand and optimise learned models in video processing\napplications so systems that incorporate them can be used in a more trustworthy\nmanner. In this context, the presented work introduces principles for\nsimplification of learned models targeting improved transparency in\nimplementing machine learning for video production and distribution\napplications. These principles are demonstrated on video compression examples,\nshowing how bitrate savings and reduced complexity can be achieved by\nsimplifying relevant deep learning models.\n",
        "published": "2021",
        "authors": [
            "Luka Murn",
            "Marc Gorriz Blanch",
            "Maria Santamaria",
            "Fiona Rivera",
            "Marta Mrak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.01017v1",
        "title": "MegazordNet: combining statistical and machine learning standpoints for\n  time series forecasting",
        "abstract": "  Forecasting financial time series is considered to be a difficult task due to\nthe chaotic feature of the series. Statistical approaches have shown solid\nresults in some specific problems such as predicting market direction and\nsingle-price of stocks; however, with the recent advances in deep learning and\nbig data techniques, new promising options have arises to tackle financial time\nseries forecasting. Moreover, recent literature has shown that employing a\ncombination of statistics and machine learning may improve accuracy in the\nforecasts in comparison to single solutions. Taking into consideration the\nmentioned aspects, in this work, we proposed the MegazordNet, a framework that\nexplores statistical features within a financial series combined with a\nstructured deep learning model for time series forecasting. We evaluated our\napproach predicting the closing price of stocks in the S&P 500 using different\nmetrics, and we were able to beat single statistical and machine learning\nmethods.\n",
        "published": "2021",
        "authors": [
            "Angelo Garangau Menezes",
            "Saulo Martiello Mastelini"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.02517v1",
        "title": "An Evaluation of Machine Learning and Deep Learning Models for Drought\n  Prediction using Weather Data",
        "abstract": "  Drought is a serious natural disaster that has a long duration and a wide\nrange of influence. To decrease the drought-caused losses, drought prediction\nis the basis of making the corresponding drought prevention and disaster\nreduction measures. While this problem has been studied in the literature, it\nremains unknown whether drought can be precisely predicted or not with machine\nlearning models using weather data. To answer this question, a real-world\npublic dataset is leveraged in this study and different drought levels are\npredicted using the last 90 days of 18 meteorological indicators as the\npredictors. In a comprehensive approach, 16 machine learning models and 16 deep\nlearning models are evaluated and compared. The results show no single model\ncan achieve the best performance for all evaluation metrics simultaneously,\nwhich indicates the drought prediction problem is still challenging. As\nbenchmarks for further studies, the code and results are publicly available in\na Github repository.\n",
        "published": "2021",
        "authors": [
            "Weiwei Jiang",
            "Jiayun Luo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.07788v3",
        "title": "PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural\n  Machine Learning Models",
        "abstract": "  We present PyTorch Geometric Temporal a deep learning framework combining\nstate-of-the-art machine learning algorithms for neural spatiotemporal signal\nprocessing. The main goal of the library is to make temporal geometric deep\nlearning available for researchers and machine learning practitioners in a\nunified easy-to-use framework. PyTorch Geometric Temporal was created with\nfoundations on existing libraries in the PyTorch eco-system, streamlined neural\nnetwork layer definitions, temporal snapshot generators for batching, and\nintegrated benchmark datasets. These features are illustrated with a\ntutorial-like case study. Experiments demonstrate the predictive performance of\nthe models implemented in the library on real world problems such as\nepidemiological forecasting, ridehail demand prediction and web-traffic\nmanagement. Our sensitivity analysis of runtime shows that the framework can\npotentially operate on web-scale datasets with rich temporal features and\nspatial structure.\n",
        "published": "2021",
        "authors": [
            "Benedek Rozemberczki",
            "Paul Scherer",
            "Yixuan He",
            "George Panagopoulos",
            "Alexander Riedel",
            "Maria Astefanoaei",
            "Oliver Kiss",
            "Ferenc Beres",
            "Guzm\u00e1n L\u00f3pez",
            "Nicolas Collignon",
            "Rik Sarkar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.07005v1",
        "title": "Interpretability of Machine Learning Methods Applied to Neuroimaging",
        "abstract": "  Deep learning methods have become very popular for the processing of natural\nimages, and were then successfully adapted to the neuroimaging field. As these\nmethods are non-transparent, interpretability methods are needed to validate\nthem and ensure their reliability. Indeed, it has been shown that deep learning\nmodels may obtain high performance even when using irrelevant features, by\nexploiting biases in the training set. Such undesirable situations can\npotentially be detected by using interpretability methods. Recently, many\nmethods have been proposed to interpret neural networks. However, this domain\nis not mature yet. Machine learning users face two major issues when aiming to\ninterpret their models: which method to choose, and how to assess its\nreliability? Here, we aim at providing answers to these questions by presenting\nthe most common interpretability methods and metrics developed to assess their\nreliability, as well as their applications and benchmarks in the neuroimaging\ncontext. Note that this is not an exhaustive survey: we aimed to focus on the\nstudies which we found to be the most representative and relevant.\n",
        "published": "2022",
        "authors": [
            "Elina Thibeau-Sutre",
            "Sasha Collin",
            "Ninon Burgos",
            "Olivier Colliot"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.04250v1",
        "title": "DetAIL : A Tool to Automatically Detect and Analyze Drift In Language",
        "abstract": "  Machine learning and deep learning-based decision making has become part of\ntoday's software. The goal of this work is to ensure that machine learning and\ndeep learning-based systems are as trusted as traditional software. Traditional\nsoftware is made dependable by following rigorous practice like static\nanalysis, testing, debugging, verifying, and repairing throughout the\ndevelopment and maintenance life-cycle. Similarly for machine learning systems,\nwe need to keep these models up to date so that their performance is not\ncompromised. For this, current systems rely on scheduled re-training of these\nmodels as new data kicks in. In this work, we propose to measure the data drift\nthat takes place when new data kicks in so that one can adaptively re-train the\nmodels whenever re-training is actually required irrespective of schedules. In\naddition to that, we generate various explanations at sentence level and\ndataset level to capture why a given payload text has drifted.\n",
        "published": "2022",
        "authors": [
            "Nishtha Madaan",
            "Adithya Manjunatha",
            "Hrithik Nambiar",
            "Aviral Kumar Goel",
            "Harivansh Kumar",
            "Diptikalyan Saha",
            "Srikanta Bedathur"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.11122v4",
        "title": "Deep Learning in Cardiology",
        "abstract": "  The medical field is creating large amount of data that physicians are unable\nto decipher and use efficiently. Moreover, rule-based expert systems are\ninefficient in solving complicated medical tasks or for creating insights using\nbig data. Deep learning has emerged as a more accurate and effective technology\nin a wide range of medical problems such as diagnosis, prediction and\nintervention. Deep learning is a representation learning method that consists\nof layers that transform the data non-linearly, thus, revealing hierarchical\nrelationships and structures. In this review we survey deep learning\napplication papers that use structured data, signal and imaging modalities from\ncardiology. We discuss the advantages and limitations of applying deep learning\nin cardiology that also apply in medicine in general, while proposing certain\ndirections as the most viable for clinical use.\n",
        "published": "2019",
        "authors": [
            "Paschalis Bizopoulos",
            "Dimitrios Koutsouris"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.08356v2",
        "title": "New Era of Deeplearning-Based Malware Intrusion Detection: The Malware\n  Detection and Prediction Based On Deep Learning",
        "abstract": "  With the development of artificial intelligence algorithms like deep learning\nmodels and the successful applications in many different fields, further\nsimilar trails of deep learning technology have been made in cyber security\narea. It shows the preferable performance not only in academic security\nresearch but also in industry practices when dealing with part of cyber\nsecurity issues by deep learning methods compared to those conventional rules.\nEspecially for the malware detection and classification tasks, it saves\ngenerous time cost and promotes the accuracy for a total pipeline of malware\ndetection system. In this paper, we construct special deep neural network, ie,\nMalDeepNet (TB-Malnet and IB-Malnet) for malware dynamic behavior\nclassification tasks. Then we build the family clustering algorithm based on\ndeep learning and fulfil related testing. Except that, we also design a novel\nmalware prediction model which could detect the malware coming in future\nthrough the Mal Generative Adversarial Network (Mal-GAN) implementation. All\nthose algorithms present fairly considerable value in related datasets\nafterwards.\n",
        "published": "2019",
        "authors": [
            "Shuqiang Lu",
            "Lingyun Ying",
            "Wenjie Lin",
            "Yu Wang",
            "Meining Nie",
            "Kaiwen Shen",
            "Lu Liu",
            "Haixin Duan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.05888v1",
        "title": "An investigation of a deep learning based malware detection system",
        "abstract": "  We investigate a Deep Learning based system for malware detection. In the\ninvestigation, we experiment with different combination of Deep Learning\narchitectures including Auto-Encoders, and Deep Neural Networks with varying\nlayers over Malicia malware dataset on which earlier studies have obtained an\naccuracy of (98%) with an acceptable False Positive Rates (1.07%). But these\nresults were done using extensive man-made custom domain features and investing\ncorresponding feature engineering and design efforts. In our proposed approach,\nbesides improving the previous best results (99.21% accuracy and a False\nPositive Rate of 0.19%) indicates that Deep Learning based systems could\ndeliver an effective defense against malware. Since it is good in automatically\nextracting higher conceptual features from the data, Deep Learning based\nsystems could provide an effective, general and scalable mechanism for\ndetection of existing and unknown malware.\n",
        "published": "2018",
        "authors": [
            "Mohit Sewak",
            "Sanjay K. Sahay",
            "Hemant Rathore"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.04238v1",
        "title": "Machine Translation : From Statistical to modern Deep-learning practices",
        "abstract": "  Machine translation (MT) is an area of study in Natural Language processing\nwhich deals with the automatic translation of human language, from one language\nto another by the computer. Having a rich research history spanning nearly\nthree decades, Machine translation is one of the most sought after area of\nresearch in the linguistics and computational community. In this paper, we\ninvestigate the models based on deep learning that have achieved substantial\nprogress in recent years and becoming the prominent method in MT. We shall\ndiscuss the two main deep-learning based Machine Translation methods, one at\ncomponent or domain level which leverages deep learning models to enhance the\nefficacy of Statistical Machine Translation (SMT) and end-to-end deep learning\nmodels in MT which uses neural networks to find correspondence between the\nsource and target languages using the encoder-decoder architecture. We conclude\nthis paper by providing a time line of the major research problems solved by\nthe researchers and also provide a comprehensive overview of present areas of\nresearch in Neural Machine Translation.\n",
        "published": "2018",
        "authors": [
            "Siddhant Srivastava",
            "Anupam Shukla",
            "Ritu Tiwari"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.06183v2",
        "title": "Data-driven geophysics: from dictionary learning to deep learning",
        "abstract": "  Understanding the principles of geophysical phenomena is an essential and\nchallenging task. \"Model-driven\" approaches have supported the development of\ngeophysics for a long time; however, such methods suffer from the curse of\ndimensionality and may inaccurately model the subsurface. \"Data-driven\"\ntechniques may overcome these issues with increasingly available geophysical\ndata. In this article, we review the basic concepts of and recent advances in\ndata-driven approaches from dictionary learning to deep learning in a variety\nof geophysical scenarios. Explorational geophysics including data processing,\ninversion and interpretation will be mainly focused. Artificial intelligence\napplications on geoscience involving deep Earth, earthquake, water resource,\natmospheric science, satellite remoe sensing and space sciences are also\nreviewed. We present a coding tutorial and a summary of tips for beginners and\ninterested geophysical readers to rapidly explore deep learning. Some promising\ndirections are provided for future research involving deep learning in\ngeophysics, such as unsupervised learning, transfer learning, multimodal deep\nlearning, federated learning, uncertainty estimation, and activate learning.\n",
        "published": "2020",
        "authors": [
            "Siwei Yu",
            "Jianwei Ma"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.03928v1",
        "title": "Deep Learning For Computer Vision Tasks: A review",
        "abstract": "  Deep learning has recently become one of the most popular sub-fields of\nmachine learning owing to its distributed data representation with multiple\nlevels of abstraction. A diverse range of deep learning algorithms are being\nemployed to solve conventional artificial intelligence problems. This paper\ngives an overview of some of the most widely used deep learning algorithms\napplied in the field of computer vision. It first inspects the various\napproaches of deep learning algorithms, followed by a description of their\napplications in image classification, object identification, image extraction\nand semantic segmentation in the presence of noise. The paper concludes with\nthe discussion of the future scope and challenges for construction and training\nof deep neural networks.\n",
        "published": "2018",
        "authors": [
            "Rajat Kumar Sinha",
            "Ruchi Pandey",
            "Rohan Pattnaik"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.03002v2",
        "title": "Fixed Priority Global Scheduling from a Deep Learning Perspective",
        "abstract": "  Deep Learning has been recently recognized as one of the feasible solutions\nto effectively address combinatorial optimization problems, which are often\nconsidered important yet challenging in various research domains. In this work,\nwe first present how to adopt Deep Learning for real-time task scheduling\nthrough our preliminary work upon fixed priority global scheduling (FPGS)\nproblems. We then briefly discuss possible generalizations of Deep Learning\nadoption for several realistic and complicated FPGS scenarios, e.g., scheduling\ntasks with dependency, mixed-criticality task scheduling. We believe that there\nare many opportunities for leveraging advanced Deep Learning technologies to\nimprove the quality of scheduling in various system configurations and problem\nscenarios.\n",
        "published": "2020",
        "authors": [
            "Hyunsung Lee",
            "Michael Wang",
            "Honguk Woo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.14092v1",
        "title": "Model Optimization for Deep Space Exploration via Simulators and Deep\n  Learning",
        "abstract": "  Machine learning, and eventually true artificial intelligence techniques, are\nextremely important advancements in astrophysics and astronomy. We explore the\napplication of deep learning using neural networks in order to automate the\ndetection of astronomical bodies for future exploration missions, such as\nmissions to search for signatures or suitability of life. The ability to\nacquire images, analyze them, and send back those that are important, as\ndetermined by the deep learning algorithm, is critical in bandwidth-limited\napplications. Our previous foundational work solidified the concept of using\nsimulator images and deep learning in order to detect planets. Optimization of\nthis process is of vital importance, as even a small loss in accuracy might be\nthe difference between capturing and completely missing a possibly-habitable\nnearby planet. Through computer vision, deep learning, and simulators, we\nintroduce methods that optimize the detection of exoplanets. We show that\nmaximum achieved accuracy can hit above 98% for multiple model architectures,\neven with a relatively small training set.\n",
        "published": "2020",
        "authors": [
            "James Bird",
            "Kellan Colburn",
            "Linda Petzold",
            "Philip Lubin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.07004v1",
        "title": "Weight Initialization Techniques for Deep Learning Algorithms in Remote\n  Sensing: Recent Trends and Future Perspectives",
        "abstract": "  During the last decade, several research works have focused on providing\nnovel deep learning methods in many application fields. However, few of them\nhave investigated the weight initialization process for deep learning, although\nits importance is revealed in improving deep learning performance. This can be\njustified by the technical difficulties in proposing new techniques for this\npromising research field. In this paper, a survey related to weight\ninitialization techniques for deep algorithms in remote sensing is conducted.\nThis survey will help practitioners to drive further research in this promising\nfield. To the best of our knowledge, this paper constitutes the first survey\nfocusing on weight initialization for deep learning models.\n",
        "published": "2021",
        "authors": [
            "Wadii Boulila",
            "Maha Driss",
            "Mohamed Al-Sarem",
            "Faisal Saeed",
            "Moez Krichen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.14250v2",
        "title": "Evaluation of deep learning models for multi-step ahead time series\n  prediction",
        "abstract": "  Time series prediction with neural networks has been the focus of much\nresearch in the past few decades. Given the recent deep learning revolution,\nthere has been much attention in using deep learning models for time series\nprediction, and hence it is important to evaluate their strengths and\nweaknesses. In this paper, we present an evaluation study that compares the\nperformance of deep learning models for multi-step ahead time series\nprediction. The deep learning methods comprise simple recurrent neural\nnetworks, long short-term memory (LSTM) networks, bidirectional LSTM networks,\nencoder-decoder LSTM networks, and convolutional neural networks. We provide a\nfurther comparison with simple neural networks that use stochastic gradient\ndescent and adaptive moment estimation (Adam) for training. We focus on\nunivariate time series for multi-step-ahead prediction from benchmark\ntime-series datasets and provide a further comparison of the results with\nrelated methods from the literature. The results show that the bidirectional\nand encoder-decoder LSTM network provides the best performance in accuracy for\nthe given time series problems.\n",
        "published": "2021",
        "authors": [
            "Rohitash Chandra",
            "Shaurya Goyal",
            "Rishabh Gupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.09991v1",
        "title": "Sketch-based Creativity Support Tools using Deep Learning",
        "abstract": "  Sketching is a natural and effective visual communication medium commonly\nused in creative processes. Recent developments in deep-learning models\ndrastically improved machines' ability in understanding and generating visual\ncontent. An exciting area of development explores deep-learning approaches used\nto model human sketches, opening opportunities for creative applications. This\nchapter describes three fundamental steps in developing deep-learning-driven\ncreativity support tools that consumes and generates sketches: 1) a data\ncollection effort that generated a new paired dataset between sketches and\nmobile user interfaces; 2) a sketch-based user interface retrieval system\nadapted from state-of-the-art computer vision techniques; and, 3) a\nconversational sketching system that supports the novel interaction of a\nnatural-language-based sketch/critique authoring process. In this chapter, we\nsurvey relevant prior work in both the deep-learning and\nhuman-computer-interaction communities, document the data collection process\nand the systems' architectures in detail, present qualitative and quantitative\nresults, and paint the landscape of several future research directions in this\nexciting area.\n",
        "published": "2021",
        "authors": [
            "Forrest Huang",
            "Eldon Schoop",
            "David Ha",
            "Jeffrey Nichols",
            "John Canny"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.10847v2",
        "title": "Calibrated Diffusion Tensor Estimation",
        "abstract": "  It is highly desirable to know how uncertain a model's predictions are,\nespecially for models that are complex and hard to understand as in deep\nlearning. Although there has been a growing interest in using deep learning\nmethods in diffusion-weighted MRI, prior works have not addressed the issue of\nmodel uncertainty. Here, we propose a deep learning method to estimate the\ndiffusion tensor and compute the estimation uncertainty. Data-dependent\nuncertainty is computed directly by the network and learned via loss\nattenuation. Model uncertainty is computed using Monte Carlo dropout. We also\npropose a new method for evaluating the quality of predicted uncertainties. We\ncompare the new method with the standard least-squares tensor estimation and\nbootstrap-based uncertainty computation techniques. Our experiments show that\nwhen the number of measurements is small the deep learning method is more\naccurate and its uncertainty predictions are better calibrated than the\nstandard methods. We show that the estimation uncertainties computed by the new\nmethod can highlight the model's biases, detect domain shift, and reflect the\nstrength of noise in the measurements. Our study shows the importance and\npractical value of modeling prediction uncertainties in deep learning-based\ndiffusion MRI analysis.\n",
        "published": "2021",
        "authors": [
            "Davood Karimi",
            "Simon K. Warfield",
            "Ali Gholipour"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.03364v1",
        "title": "Scalable Geometric Deep Learning on Molecular Graphs",
        "abstract": "  Deep learning in molecular and materials sciences is limited by the lack of\nintegration between applied science, artificial intelligence, and\nhigh-performance computing. Bottlenecks with respect to the amount of training\ndata, the size and complexity of model architectures, and the scale of the\ncompute infrastructure are all key factors limiting the scaling of deep\nlearning for molecules and materials. Here, we present $\\textit{LitMatter}$, a\nlightweight framework for scaling molecular deep learning methods. We train\nfour graph neural network architectures on over 400 GPUs and investigate the\nscaling behavior of these methods. Depending on the model architecture,\ntraining time speedups up to $60\\times$ are seen. Empirical neural scaling\nrelations quantify the model-dependent scaling and enable optimal compute\nresource allocation and the identification of scalable molecular geometric deep\nlearning model implementations.\n",
        "published": "2021",
        "authors": [
            "Nathan C. Frey",
            "Siddharth Samsi",
            "Joseph McDonald",
            "Lin Li",
            "Connor W. Coley",
            "Vijay Gadepally"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.11196v2",
        "title": "Performance of Deep Learning models with transfer learning for\n  multiple-step-ahead forecasts in monthly time series",
        "abstract": "  Deep Learning and transfer learning models are being used to generate time\nseries forecasts; however, there is scarce evidence about their performance\nprediction that it is more evident for monthly time series. The purpose of this\npaper is to compare Deep Learning models with transfer learning and without\ntransfer learning and other traditional methods used for monthly forecasts to\nanswer three questions about the suitability of Deep Learning and Transfer\nLearning to generate predictions of time series. Time series of M4 and M3\ncompetitions were used for the experiments. The results suggest that deep\nlearning models based on TCN, LSTM, and CNN with transfer learning tend to\nsurpass the performance prediction of other traditional methods. On the other\nhand, TCN and LSTM, trained directly on the target time series, got similar or\nbetter performance than traditional methods for some forecast horizons.\n",
        "published": "2022",
        "authors": [
            "Mart\u00edn Sol\u00eds",
            "Luis-Alexander Calvo-Valverde"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.07372v1",
        "title": "Effect of Batch Normalization on Noise Resistant Property of Deep\n  Learning Models",
        "abstract": "  The fast execution speed and energy efficiency of analog hardware has made\nthem a strong contender for deployment of deep learning model at the edge.\nHowever, there are concerns about the presence of analog noise which causes\nchanges to the weight of the models, leading to performance degradation of deep\nlearning model, despite their inherent noise resistant characteristics. The\neffect of the popular batch normalization layer on the noise resistant ability\nof deep learning model is investigated in this work. This systematic study has\nbeen carried out by first training different models with and without batch\nnormalization layer on CIFAR10 and CIFAR100 dataset. The weights of the\nresulting models are then injected with analog noise and the performance of the\nmodels on the test dataset is obtained and compared. The results show that the\npresence of batch normalization layer negatively impacts noise resistant\nproperty of deep learning model and the impact grows with the increase of the\nnumber of batch normalization layers.\n",
        "published": "2022",
        "authors": [
            "Omobayode Fagbohungbe",
            "Lijun Qian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.07856v1",
        "title": "Impact of Learning Rate on Noise Resistant Property of Deep Learning\n  Models",
        "abstract": "  The interest in analog computation has grown tremendously in recent years due\nto its fast computation speed and excellent energy efficiency, which is very\nimportant for edge and IoT devices in the sub-watt power envelope for deep\nlearning inferencing. However, significant performance degradation suffered by\ndeep learning models due to the inherent noise present in the analog\ncomputation can limit their use in mission-critical applications. Hence, there\nis a need to understand the impact of critical model hyperparameters choice on\nthe resulting model noise-resistant property. This need is critical as the\ninsight obtained can be used to design deep learning models that are robust to\nanalog noise. In this paper, the impact of the learning rate, a critical design\nchoice, on the noise-resistant property is investigated. The study is achieved\nby first training deep learning models using different learning rates.\nThereafter, the models are injected with analog noise and the noise-resistant\nproperty of the resulting models is examined by measuring the performance\ndegradation due to the analog noise. The results showed there exists a sweet\nspot of learning rate values that achieves a good balance between model\nprediction performance and model noise-resistant property. Furthermore, the\ntheoretical justification of the observed phenomenon is provided.\n",
        "published": "2022",
        "authors": [
            "Omobayode Fagbohungbe",
            "Lijun Qian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.14976v1",
        "title": "Rethinking Saliency Map: An Context-aware Perturbation Method to Explain\n  EEG-based Deep Learning Model",
        "abstract": "  Deep learning is widely used to decode the electroencephalogram (EEG) signal.\nHowever, there are few attempts to specifically investigate how to explain the\nEEG-based deep learning models. We conduct a review to summarize the existing\nworks explaining the EEG-based deep learning model. Unfortunately, we find that\nthere is no appropriate method to explain them. Based on the characteristic of\nEEG data, we suggest a context-aware perturbation method to generate a saliency\nmap from the perspective of the raw EEG signal. Moreover, we also justify that\nthe context information can be used to suppress the artifacts in the EEG-based\ndeep learning model. In practice, some users might want a simple version of the\nexplanation, which only indicates a few features as salient points. To this\nend, we propose an optional area limitation strategy to restrict the\nhighlighted region. To validate our idea and make a comparison with the other\nmethods, we select three representative EEG-based models to implement\nexperiments on the emotional EEG dataset DEAP. The results of the experiments\nsupport the advantages of our method.\n",
        "published": "2022",
        "authors": [
            "Hanqi Wang",
            "Xiaoguang Zhu",
            "Tao Chen",
            "Chengfang Li",
            "Liang Song"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.00002v1",
        "title": "Calibrated Bagging Deep Learning for Image Semantic Segmentation: A Case\n  Study on COVID-19 Chest X-ray Image",
        "abstract": "  Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) causes\ncoronavirus disease 2019 (COVID-19). Imaging tests such as chest X-ray (CXR)\nand computed tomography (CT) can provide useful information to clinical staff\nfor facilitating a diagnosis of COVID-19 in a more efficient and comprehensive\nmanner. As a breakthrough of artificial intelligence (AI), deep learning has\nbeen applied to perform COVID-19 infection region segmentation and disease\nclassification by analyzing CXR and CT data. However, prediction uncertainty of\ndeep learning models for these tasks, which is very important to\nsafety-critical applications like medical image processing, has not been\ncomprehensively investigated. In this work, we propose a novel ensemble deep\nlearning model through integrating bagging deep learning and model calibration\nto not only enhance segmentation performance, but also reduce prediction\nuncertainty. The proposed method has been validated on a large dataset that is\nassociated with CXR image segmentation. Experimental results demonstrate that\nthe proposed method can improve the segmentation performance, as well as\ndecrease prediction uncertainties.\n",
        "published": "2022",
        "authors": [
            "Lucy Nwosu",
            "Xiangfang Li",
            "Lijun Qian",
            "Seungchan Kim",
            "Xishuang Dong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.13295v1",
        "title": "Structure-based drug discovery with deep learning",
        "abstract": "  Artificial intelligence (AI) in the form of deep learning bears promise for\ndrug discovery and chemical biology, $\\textit{e.g.}$, to predict protein\nstructure and molecular bioactivity, plan organic synthesis, and design\nmolecules $\\textit{de novo}$. While most of the deep learning efforts in drug\ndiscovery have focused on ligand-based approaches, structure-based drug\ndiscovery has the potential to tackle unsolved challenges, such as affinity\nprediction for unexplored protein targets, binding-mechanism elucidation, and\nthe rationalization of related chemical kinetic properties. Advances in deep\nlearning methodologies and the availability of accurate predictions for protein\ntertiary structure advocate for a $\\textit{renaissance}$ in structure-based\napproaches for drug discovery guided by AI. This review summarizes the most\nprominent algorithmic concepts in structure-based deep learning for drug\ndiscovery, and forecasts opportunities, applications, and challenges ahead.\n",
        "published": "2022",
        "authors": [
            "R\u0131za \u00d6z\u00e7elik",
            "Derek van Tilborg",
            "Jos\u00e9 Jim\u00e9nez-Luna",
            "Francesca Grisoni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.02694v2",
        "title": "Loss Functions and Metrics in Deep Learning",
        "abstract": "  One of the essential components of deep learning is the choice of the loss\nfunction and performance metrics used to train and evaluate models. This paper\nreviews the most prevalent loss functions and performance measurements in deep\nlearning. We examine the benefits and limits of each technique and illustrate\ntheir application to various deep-learning problems. Our review aims to give a\ncomprehensive picture of the different loss functions and performance\nindicators used in the most common deep learning tasks and help practitioners\nchoose the best method for their specific task.\n",
        "published": "2023",
        "authors": [
            "Juan Terven",
            "Diana M. Cordova-Esparza",
            "Alfonso Ramirez-Pedraza",
            "Edgar A. Chavez-Urbiola"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.05017v1",
        "title": "Feature Activation Map: Visual Explanation of Deep Learning Models for\n  Image Classification",
        "abstract": "  Decisions made by convolutional neural networks(CNN) can be understood and\nexplained by visualizing discriminative regions on images. To this end, Class\nActivation Map (CAM) based methods were proposed as powerful interpretation\ntools, making the prediction of deep learning models more explainable,\ntransparent, and trustworthy. However, all the CAM-based methods (e.g., CAM,\nGrad-CAM, and Relevance-CAM) can only be used for interpreting CNN models with\nfully-connected (FC) layers as a classifier. It is worth noting that many deep\nlearning models classify images without FC layers, e.g., few-shot learning\nimage classification, contrastive learning image classification, and image\nretrieval tasks. In this work, a post-hoc interpretation tool named feature\nactivation map (FAM) is proposed, which can interpret deep learning models\nwithout FC layers as a classifier. In the proposed FAM algorithm, the\nchannel-wise contribution weights are derived from the similarity scores\nbetween two image embeddings. The activation maps are linearly combined with\nthe corresponding normalized contribution weights, forming the explanation map\nfor visualization. The quantitative and qualitative experiments conducted on\nten deep learning models for few-shot image classification, contrastive\nlearning image classification and image retrieval tasks demonstrate the\neffectiveness of the proposed FAM algorithm.\n",
        "published": "2023",
        "authors": [
            "Yi Liao",
            "Yongsheng Gao",
            "Weichuan Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.10506v1",
        "title": "Is Grad-CAM Explainable in Medical Images?",
        "abstract": "  Explainable Deep Learning has gained significant attention in the field of\nartificial intelligence (AI), particularly in domains such as medical imaging,\nwhere accurate and interpretable machine learning models are crucial for\neffective diagnosis and treatment planning. Grad-CAM is a baseline that\nhighlights the most critical regions of an image used in a deep learning\nmodel's decision-making process, increasing interpretability and trust in the\nresults. It is applied in many computer vision (CV) tasks such as\nclassification and explanation. This study explores the principles of\nExplainable Deep Learning and its relevance to medical imaging, discusses\nvarious explainability techniques and their limitations, and examines medical\nimaging applications of Grad-CAM. The findings highlight the potential of\nExplainable Deep Learning and Grad-CAM in improving the accuracy and\ninterpretability of deep learning models in medical imaging. The code is\navailable in (will be available).\n",
        "published": "2023",
        "authors": [
            "Subhashis Suara",
            "Aayush Jha",
            "Pratik Sinha",
            "Arif Ahmed Sekh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.02485v1",
        "title": "Uncertainty Quantification of Deep Learning for Spatiotemporal Data:\n  Challenges and Opportunities",
        "abstract": "  With the advancement of GPS, remote sensing, and computational simulations,\nlarge amounts of geospatial and spatiotemporal data are being collected at an\nincreasing speed. Such emerging spatiotemporal big data assets, together with\nthe recent progress of deep learning technologies, provide unique opportunities\nto transform society. However, it is widely recognized that deep learning\nsometimes makes unexpected and incorrect predictions with unwarranted\nconfidence, causing severe consequences in high-stake decision-making\napplications (e.g., disaster management, medical diagnosis, autonomous\ndriving). Uncertainty quantification (UQ) aims to estimate a deep learning\nmodel's confidence. This paper provides a brief overview of UQ of deep learning\nfor spatiotemporal data, including its unique challenges and existing methods.\nWe particularly focus on the importance of uncertainty sources. We identify\nseveral future research directions for spatiotemporal data.\n",
        "published": "2023",
        "authors": [
            "Wenchong He",
            "Zhe Jiang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.15332v1",
        "title": "ASI: Accuracy-Stability Index for Evaluating Deep Learning Models",
        "abstract": "  In the context of deep learning research, where model introductions\ncontinually occur, the need for effective and efficient evaluation remains\nparamount. Existing methods often emphasize accuracy metrics, overlooking\nstability. To address this, the paper introduces the Accuracy-Stability Index\n(ASI), a quantitative measure incorporating both accuracy and stability for\nassessing deep learning models. Experimental results demonstrate the\napplication of ASI, and a 3D surface model is presented for visualizing ASI,\nmean accuracy, and coefficient of variation. This paper addresses the important\nissue of quantitative benchmarking metrics for deep learning models, providing\na new approach for accurately evaluating accuracy and stability of deep\nlearning models. The paper concludes with discussions on potential weaknesses\nand outlines future research directions.\n",
        "published": "2023",
        "authors": [
            "Wei Dai",
            "Daniel Berleant"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1602.05897v2",
        "title": "Toward Deeper Understanding of Neural Networks: The Power of\n  Initialization and a Dual View on Expressivity",
        "abstract": "  We develop a general duality between neural networks and compositional\nkernels, striving towards a better understanding of deep learning. We show that\ninitial representations generated by common random initializations are\nsufficiently rich to express all functions in the dual kernel space. Hence,\nthough the training objective is hard to optimize in the worst case, the\ninitial weights form a good starting point for optimization. Our dual view also\nreveals a pragmatic and aesthetic perspective of neural networks and\nunderscores their expressive power.\n",
        "published": "2016",
        "authors": [
            "Amit Daniely",
            "Roy Frostig",
            "Yoram Singer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.11225v1",
        "title": "Variational Deep Q Network",
        "abstract": "  We propose a framework that directly tackles the probability distribution of\nthe value function parameters in Deep Q Network (DQN), with powerful\nvariational inference subroutines to approximate the posterior of the\nparameters. We will establish the equivalence between our proposed surrogate\nobjective and variational inference loss. Our new algorithm achieves efficient\nexploration and performs well on large scale chain Markov Decision Process\n(MDP).\n",
        "published": "2017",
        "authors": [
            "Yunhao Tang",
            "Alp Kucukelbir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.07654v1",
        "title": "Expectation Learning for Adaptive Crossmodal Stimuli Association",
        "abstract": "  The human brain is able to learn, generalize, and predict crossmodal stimuli.\nLearning by expectation fine-tunes crossmodal processing at different levels,\nthus enhancing our power of generalization and adaptation in highly dynamic\nenvironments. In this paper, we propose a deep neural architecture trained by\nusing expectation learning accounting for unsupervised learning tasks. Our\nlearning model exhibits a self-adaptable behavior, setting the first steps\ntowards the development of deep learning architectures for crossmodal stimuli\nassociation.\n",
        "published": "2018",
        "authors": [
            "Pablo Barros",
            "German I. Parisi",
            "Di Fu",
            "Xun Liu",
            "Stefan Wermter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.09573v1",
        "title": "Deep Learning Approach for Very Similar Objects Recognition Application\n  on Chihuahua and Muffin Problem",
        "abstract": "  We address the problem to tackle the very similar objects like Chihuahua or\nmuffin problem to recognize at least in human vision level. Our regular deep\nstructured machine learning still does not solve it. We saw many times for\nabout year in our community the problem. Today we proposed the state-of-the-art\nsolution for it. Our approach is quite tricky to get the very high accuracy. We\npropose the deep transfer learning method which could be tackled all this type\nof problems not limited to just Chihuahua or muffin problem. It is the best\nmethod to train with small data set not like require huge amount data.\n",
        "published": "2018",
        "authors": [
            "Enkhtogtokh Togootogtokh",
            "Amarzaya Amartuvshin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.06024v1",
        "title": "Deep Learning Reconstruction of Ultra-Short Pulses",
        "abstract": "  Ultra-short laser pulses with femtosecond to attosecond pulse duration are\nthe shortest systematic events humans can create. Characterization (amplitude\nand phase) of these pulses is a key ingredient in ultrafast science, e.g.,\nexploring chemical reactions and electronic phase transitions. Here, we propose\nand demonstrate, numerically and experimentally, the first deep neural network\ntechnique to reconstruct ultra-short optical pulses. We anticipate that this\napproach will extend the range of ultrashort laser pulses that can be\ncharacterized, e.g., enabling to diagnose very weak attosecond pulses.\n",
        "published": "2018",
        "authors": [
            "Tom Zahavy",
            "Alex Dikopoltsev",
            "Oren Cohen",
            "Shie Mannor",
            "Mordechai Segev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.01235v1",
        "title": "Deep Graphs",
        "abstract": "  We propose an algorithm for deep learning on networks and graphs. It relies\non the notion that many graph algorithms, such as PageRank, Weisfeiler-Lehman,\nor Message Passing can be expressed as iterative vertex updates. Unlike\nprevious methods which rely on the ingenuity of the designer, Deep Graphs are\nadaptive to the estimation problem. Training and deployment are both efficient,\nsince the cost is $O(|E| + |V|)$, where $E$ and $V$ are the sets of edges and\nvertices respectively. In short, we learn the recurrent update functions rather\nthan positing their specific functional form. This yields an algorithm that\nachieves excellent accuracy on both graph labeling and regression tasks.\n",
        "published": "2018",
        "authors": [
            "Emmanouil Antonios Platanios",
            "Alex Smola"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.01122v1",
        "title": "Topological Approaches to Deep Learning",
        "abstract": "  We perform topological data analysis on the internal states of convolutional\ndeep neural networks to develop an understanding of the computations that they\nperform. We apply this understanding to modify the computations so as to (a)\nspeed up computations and (b) improve generalization from one data set of\ndigits to another. One byproduct of the analysis is the production of a\ngeometry on new sets of features on data sets of images, and use this\nobservation to develop a methodology for constructing analogues of CNN's for\nmany other geometries, including the graph structures constructed by\ntopological data analysis.\n",
        "published": "2018",
        "authors": [
            "Gunnar Carlsson",
            "Rickard Br\u00fcel Gabrielsson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.03474v1",
        "title": "Fine-grained Sentiment Classification using BERT",
        "abstract": "  Sentiment classification is an important process in understanding people's\nperception towards a product, service, or topic. Many natural language\nprocessing models have been proposed to solve the sentiment classification\nproblem. However, most of them have focused on binary sentiment classification.\nIn this paper, we use a promising deep learning model called BERT to solve the\nfine-grained sentiment classification task. Experiments show that our model\noutperforms other popular models for this task without sophisticated\narchitecture. We also demonstrate the effectiveness of transfer learning in\nnatural language processing in the process.\n",
        "published": "2019",
        "authors": [
            "Manish Munikar",
            "Sushil Shakya",
            "Aakash Shrestha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.03077v1",
        "title": "Generative Counterfactual Introspection for Explainable Deep Learning",
        "abstract": "  In this work, we propose an introspection technique for deep neural networks\nthat relies on a generative model to instigate salient editing of the input\nimage for model interpretation. Such modification provides the fundamental\ninterventional operation that allows us to obtain answers to counterfactual\ninquiries, i.e., what meaningful change can be made to the input image in order\nto alter the prediction. We demonstrate how to reveal interesting properties of\nthe given classifiers by utilizing the proposed introspection approach on both\nthe MNIST and the CelebA dataset.\n",
        "published": "2019",
        "authors": [
            "Shusen Liu",
            "Bhavya Kailkhura",
            "Donald Loveland",
            "Yong Han"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.04754v1",
        "title": "Gradient Descent Happens in a Tiny Subspace",
        "abstract": "  We show that in a variety of large-scale deep learning scenarios the gradient\ndynamically converges to a very small subspace after a short period of\ntraining. The subspace is spanned by a few top eigenvectors of the Hessian\n(equal to the number of classes in the dataset), and is mostly preserved over\nlong periods of training. A simple argument then suggests that gradient descent\nmay happen mostly in this subspace. We give an example of this effect in a\nsolvable model of classification, and we comment on possible implications for\noptimization and learning.\n",
        "published": "2018",
        "authors": [
            "Guy Gur-Ari",
            "Daniel A. Roberts",
            "Ethan Dyer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.09764v1",
        "title": "An Artificial Intelligence approach to Shadow Rating",
        "abstract": "  We analyse the effectiveness of modern deep learning techniques in predicting\ncredit ratings over a universe of thousands of global corporate entities\nobligations when compared to most popular, traditional machine-learning\napproaches such as linear models and tree-based classifiers. Our results show a\nadequate accuracy over different rating classes when applying categorical\nembeddings to artificial neural networks (ANN) architectures.\n",
        "published": "2019",
        "authors": [
            "Angela Rita Provenzano",
            "Daniele Trifir\u00f2",
            "Nicola Jean",
            "Giacomo Le Pera",
            "Maurizio Spadaccino",
            "Luca Massaron",
            "Claudio Nordio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.11841v2",
        "title": "Learning Perception and Planning with Deep Active Inference",
        "abstract": "  Active inference is a process theory of the brain that states that all living\norganisms infer actions in order to minimize their (expected) free energy.\nHowever, current experiments are limited to predefined, often discrete, state\nspaces. In this paper we use recent advances in deep learning to learn the\nstate space and approximate the necessary probability distributions to engage\nin active inference.\n",
        "published": "2020",
        "authors": [
            "Ozan \u00c7atal",
            "Tim Verbelen",
            "Johannes Nauta",
            "Cedric De Boom",
            "Bart Dhoedt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.05683v1",
        "title": "Batch-level Experience Replay with Review for Continual Learning",
        "abstract": "  Continual learning is a branch of deep learning that seeks to strike a\nbalance between learning stability and plasticity. The CVPR 2020 CLVision\nContinual Learning for Computer Vision challenge is dedicated to evaluating and\nadvancing the current state-of-the-art continual learning methods using the\nCORe50 dataset with three different continual learning scenarios. This paper\npresents our approach, called Batch-level Experience Replay with Review, to\nthis challenge. Our team achieved the 1'st place in all three scenarios out of\n79 participated teams. The codebase of our implementation is publicly available\nat https://github.com/RaptorMai/CVPR20_CLVision_challenge\n",
        "published": "2020",
        "authors": [
            "Zheda Mai",
            "Hyunwoo Kim",
            "Jihwan Jeong",
            "Scott Sanner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.07636v2",
        "title": "DOC3-Deep One Class Classification using Contradictions",
        "abstract": "  This paper introduces the notion of learning from contradictions (a.k.a\nUniversum learning) for deep one class classification problems. We formalize\nthis notion for the widely adopted one class large-margin loss, and propose the\nDeep One Class Classification using Contradictions (DOC3) algorithm. We show\nthat learning from contradictions incurs lower generalization error by\ncomparing the Empirical Rademacher Complexity (ERC) of DOC3 against its\ntraditional inductive learning counterpart. Our empirical results demonstrate\nthe efficacy of DOC3 compared to popular baseline algorithms on several\nreal-life data sets.\n",
        "published": "2021",
        "authors": [
            "Sauptik Dhar",
            "Bernardo Gonzalez Torres"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.13283v2",
        "title": "Deep Ensembles from a Bayesian Perspective",
        "abstract": "  Deep ensembles can be considered as the current state-of-the-art for\nuncertainty quantification in deep learning. While the approach was originally\nproposed as a non-Bayesian technique, arguments supporting its Bayesian footing\nhave been put forward as well. We show that deep ensembles can be viewed as an\napproximate Bayesian method by specifying the corresponding assumptions. Our\nfindings lead to an improved approximation which results in an enlarged\nepistemic part of the uncertainty. Numerical examples suggest that the improved\napproximation can lead to more reliable uncertainties. Analytical derivations\nensure easy calculation of results.\n",
        "published": "2021",
        "authors": [
            "Lara Hoffmann",
            "Clemens Elster"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.03714v2",
        "title": "Foundations of Sequence-to-Sequence Modeling for Time Series",
        "abstract": "  The availability of large amounts of time series data, paired with the\nperformance of deep-learning algorithms on a broad class of problems, has\nrecently led to significant interest in the use of sequence-to-sequence models\nfor time series forecasting. We provide the first theoretical analysis of this\ntime series forecasting framework. We include a comparison of\nsequence-to-sequence modeling to classical time series models, and as such our\ntheory can serve as a quantitative guide for practitioners choosing between\ndifferent modeling methodologies.\n",
        "published": "2018",
        "authors": [
            "Vitaly Kuznetsov",
            "Zelda Mariet"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.00555v1",
        "title": "Probabilistic Meta-Representations Of Neural Networks",
        "abstract": "  Existing Bayesian treatments of neural networks are typically characterized\nby weak prior and approximate posterior distributions according to which all\nthe weights are drawn independently. Here, we consider a richer prior\ndistribution in which units in the network are represented by latent variables,\nand the weights between units are drawn conditionally on the values of the\ncollection of those variables. This allows rich correlations between related\nweights, and can be seen as realizing a function prior with a Bayesian\ncomplexity regularizer ensuring simple solutions. We illustrate the resulting\nmeta-representations and representations, elucidating the power of this prior.\n",
        "published": "2018",
        "authors": [
            "Theofanis Karaletsos",
            "Peter Dayan",
            "Zoubin Ghahramani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.07242v1",
        "title": "Adversarial Attacks on Cognitive Self-Organizing Networks: The Challenge\n  and the Way Forward",
        "abstract": "  Future communications and data networks are expected to be largely cognitive\nself-organizing networks (CSON). Such networks will have the essential property\nof cognitive self-organization, which can be achieved using machine learning\ntechniques (e.g., deep learning). Despite the potential of these techniques,\nthese techniques in their current form are vulnerable to adversarial attacks\nthat can cause cascaded damages with detrimental consequences for the whole\nnetwork. In this paper, we explore the effect of adversarial attacks on CSON.\nOur experiments highlight the level of threat that CSON have to deal with in\norder to meet the challenges of next-generation networks and point out\npromising directions for future work.\n",
        "published": "2018",
        "authors": [
            "Muhammad Usama",
            "Junaid Qadir",
            "Ala Al-Fuqaha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.09028v2",
        "title": "RLgraph: Modular Computation Graphs for Deep Reinforcement Learning",
        "abstract": "  Reinforcement learning (RL) tasks are challenging to implement, execute and\ntest due to algorithmic instability, hyper-parameter sensitivity, and\nheterogeneous distributed communication patterns. We argue for the separation\nof logical component composition, backend graph definition, and distributed\nexecution. To this end, we introduce RLgraph, a library for designing and\nexecuting reinforcement learning tasks in both static graph and define-by-run\nparadigms. The resulting implementations are robust, incrementally testable,\nand yield high performance across different deep learning frameworks and\ndistributed backends.\n",
        "published": "2018",
        "authors": [
            "Michael Schaarschmidt",
            "Sven Mika",
            "Kai Fricke",
            "Eiko Yoneki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.00431v1",
        "title": "An Empirical Study on Hyperparameters and their Interdependence for RL\n  Generalization",
        "abstract": "  Recent results in Reinforcement Learning (RL) have shown that agents with\nlimited training environments are susceptible to a large amount of overfitting\nacross many domains. A key challenge for RL generalization is to quantitatively\nexplain the effects of changing parameters on testing performance. Such\nparameters include architecture, regularization, and RL-dependent variables\nsuch as discount factor and action stochasticity. We provide empirical results\nthat show complex and interdependent relationships between hyperparameters and\ngeneralization. We further show that several empirical metrics such as gradient\ncosine similarity and trajectory-dependent metrics serve to provide intuition\ntowards these results.\n",
        "published": "2019",
        "authors": [
            "Xingyou Song",
            "Yilun Du",
            "Jacob Jackson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.01217v1",
        "title": "Detect Toxic Content to Improve Online Conversations",
        "abstract": "  Social media is filled with toxic content. The aim of this paper is to build\na model that can detect insincere questions. We use the 'Quora Insincere\nQuestions Classification' dataset for our analysis. The dataset is composed of\nsincere and insincere questions, with the majority of sincere questions. The\ndataset is processed and analyzed using Python and its libraries such as\nsklearn, numpy, pandas, keras etc. The dataset is converted to vector form\nusing word embeddings such as GloVe, Wiki-news and TF-IDF. The imbalance in the\ndataset is handled by resampling techniques. We train and compare various\nmachine learning and deep learning models to come up with the best results.\nModels discussed include SVM, Naive Bayes, GRU and LSTM.\n",
        "published": "2019",
        "authors": [
            "Deepshi Mediratta",
            "Nikhil Oswal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.07019v2",
        "title": "Learning to Prove Theorems by Learning to Generate Theorems",
        "abstract": "  We consider the task of automated theorem proving, a key AI task. Deep\nlearning has shown promise for training theorem provers, but there are limited\nhuman-written theorems and proofs available for supervised learning. To address\nthis limitation, we propose to learn a neural generator that automatically\nsynthesizes theorems and proofs for the purpose of training a theorem prover.\nExperiments on real-world tasks demonstrate that synthetic data from our\napproach improves the theorem prover and advances the state of the art of\nautomated theorem proving in Metamath. Code is available at\nhttps://github.com/princeton-vl/MetaGen.\n",
        "published": "2020",
        "authors": [
            "Mingzhe Wang",
            "Jia Deng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.03393v1",
        "title": "Generative Language Modeling for Automated Theorem Proving",
        "abstract": "  We explore the application of transformer-based language models to automated\ntheorem proving. This work is motivated by the possibility that a major\nlimitation of automated theorem provers compared to humans -- the generation of\noriginal mathematical terms -- might be addressable via generation from\nlanguage models. We present an automated prover and proof assistant, GPT-f, for\nthe Metamath formalization language, and analyze its performance. GPT-f found\nnew short proofs that were accepted into the main Metamath library, which is to\nour knowledge, the first time a deep-learning based system has contributed\nproofs that were adopted by a formal mathematics community.\n",
        "published": "2020",
        "authors": [
            "Stanislas Polu",
            "Ilya Sutskever"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.03488v1",
        "title": "Learning with Molecules beyond Graph Neural Networks",
        "abstract": "  We demonstrate a deep learning framework which is inherently based in the\nhighly expressive language of relational logic, enabling to, among other\nthings, capture arbitrarily complex graph structures. We show how Graph Neural\nNetworks and similar models can be easily covered in the framework by\nspecifying the underlying propagation rules in the relational logic. The\ndeclarative nature of the used language then allows to easily modify and extend\nthe propagation schemes into complex structures, such as the molecular rings\nwhich we choose for a short demonstration in this paper.\n",
        "published": "2020",
        "authors": [
            "Gustav Sourek",
            "Filip Zelezny",
            "Ondrej Kuzelka"
        ]
    }
]