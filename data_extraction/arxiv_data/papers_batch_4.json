[
    {
        "id": "http://arxiv.org/abs/1804.03022v1",
        "title": "Learning at the Ends: From Hand to Tool Affordances in Humanoid Robots",
        "abstract": "  One of the open challenges in designing robots that operate successfully in\nthe unpredictable human environment is how to make them able to predict what\nactions they can perform on objects, and what their effects will be, i.e., the\nability to perceive object affordances. Since modeling all the possible world\ninteractions is unfeasible, learning from experience is required, posing the\nchallenge of collecting a large amount of experiences (i.e., training data).\nTypically, a manipulative robot operates on external objects by using its own\nhands (or similar end-effectors), but in some cases the use of tools may be\ndesirable, nevertheless, it is reasonable to assume that while a robot can\ncollect many sensorimotor experiences using its own hands, this cannot happen\nfor all possible human-made tools.\n  Therefore, in this paper we investigate the developmental transition from\nhand to tool affordances: what sensorimotor skills that a robot has acquired\nwith its bare hands can be employed for tool use? By employing a visual and\nmotor imagination mechanism to represent different hand postures compactly, we\npropose a probabilistic model to learn hand affordances, and we show how this\nmodel can generalize to estimate the affordances of previously unseen tools,\nultimately supporting planning, decision-making and tool selection tasks in\nhumanoid robots. We present experimental results with the iCub humanoid robot,\nand we publicly release the collected sensorimotor data in the form of a hand\nposture affordances dataset.\n",
        "published": "2018-04-09",
        "authors": [
            "Giovanni Saponaro",
            "Pedro Vicente",
            "Atabak Dehban",
            "Lorenzo Jamone",
            "Alexandre Bernardino",
            "Jos\u00e9 Santos-Victor"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.08606v1",
        "title": "Zero-Shot Visual Imitation",
        "abstract": "  The current dominant paradigm for imitation learning relies on strong\nsupervision of expert actions to learn both 'what' and 'how' to imitate. We\npursue an alternative paradigm wherein an agent first explores the world\nwithout any expert supervision and then distills its experience into a\ngoal-conditioned skill policy with a novel forward consistency loss. In our\nframework, the role of the expert is only to communicate the goals (i.e., what\nto imitate) during inference. The learned policy is then employed to mimic the\nexpert (i.e., how to imitate) after seeing just a sequence of images\ndemonstrating the desired task. Our method is 'zero-shot' in the sense that the\nagent never has access to expert actions during training or for the task\ndemonstration at inference. We evaluate our zero-shot imitator in two\nreal-world settings: complex rope manipulation with a Baxter robot and\nnavigation in previously unseen office environments with a TurtleBot. Through\nfurther experiments in VizDoom simulation, we provide evidence that better\nmechanisms for exploration lead to learning a more capable policy which in turn\nimproves end task performance. Videos, models, and more details are available\nat https://pathak22.github.io/zeroshot-imitation/\n",
        "published": "2018-04-23",
        "authors": [
            "Deepak Pathak",
            "Parsa Mahmoudieh",
            "Guanghao Luo",
            "Pulkit Agrawal",
            "Dian Chen",
            "Yide Shentu",
            "Evan Shelhamer",
            "Jitendra Malik",
            "Alexei A. Efros",
            "Trevor Darrell"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.04355v1",
        "title": "Large-Scale Study of Curiosity-Driven Learning",
        "abstract": "  Reinforcement learning algorithms rely on carefully engineering environment\nrewards that are extrinsic to the agent. However, annotating each environment\nwith hand-designed, dense rewards is not scalable, motivating the need for\ndeveloping reward functions that are intrinsic to the agent. Curiosity is a\ntype of intrinsic reward function which uses prediction error as reward signal.\nIn this paper: (a) We perform the first large-scale study of purely\ncuriosity-driven learning, i.e. without any extrinsic rewards, across 54\nstandard benchmark environments, including the Atari game suite. Our results\nshow surprisingly good performance, and a high degree of alignment between the\nintrinsic curiosity objective and the hand-designed extrinsic rewards of many\ngame environments. (b) We investigate the effect of using different feature\nspaces for computing prediction error and show that random features are\nsufficient for many popular RL game benchmarks, but learned features appear to\ngeneralize better (e.g. to novel game levels in Super Mario Bros.). (c) We\ndemonstrate limitations of the prediction-based rewards in stochastic setups.\nGame-play videos and code are at\nhttps://pathak22.github.io/large-scale-curiosity/\n",
        "published": "2018-08-13",
        "authors": [
            "Yuri Burda",
            "Harri Edwards",
            "Deepak Pathak",
            "Amos Storkey",
            "Trevor Darrell",
            "Alexei A. Efros"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.10393v1",
        "title": "Learning End-to-end Autonomous Driving using Guided Auxiliary\n  Supervision",
        "abstract": "  Learning to drive faithfully in highly stochastic urban settings remains an\nopen problem. To that end, we propose a Multi-task Learning from Demonstration\n(MT-LfD) framework which uses supervised auxiliary task prediction to guide the\nmain task of predicting the driving commands. Our framework involves an\nend-to-end trainable network for imitating the expert demonstrator's driving\ncommands. The network intermediately predicts visual affordances and action\nprimitives through direct supervision which provide the aforementioned\nauxiliary supervised guidance. We demonstrate that such joint learning and\nsupervised guidance facilitates hierarchical task decomposition, assisting the\nagent to learn faster, achieve better driving performance and increases\ntransparency of the otherwise black-box end-to-end network. We run our\nexperiments to validate the MT-LfD framework in CARLA, an open-source urban\ndriving simulator. We introduce multiple non-player agents in CARLA and induce\ntemporal noise in them for realistic stochasticity.\n",
        "published": "2018-08-30",
        "authors": [
            "Ashish Mehta",
            "Adithya Subramanian",
            "Anbumani Subramanian"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.02274v5",
        "title": "Episodic Curiosity through Reachability",
        "abstract": "  Rewards are sparse in the real world and most of today's reinforcement\nlearning algorithms struggle with such sparsity. One solution to this problem\nis to allow the agent to create rewards for itself - thus making rewards dense\nand more suitable for learning. In particular, inspired by curious behaviour in\nanimals, observing something novel could be rewarded with a bonus. Such bonus\nis summed up with the real task reward - making it possible for RL algorithms\nto learn from the combined reward. We propose a new curiosity method which uses\nepisodic memory to form the novelty bonus. To determine the bonus, the current\nobservation is compared with the observations in memory. Crucially, the\ncomparison is done based on how many environment steps it takes to reach the\ncurrent observation from those in memory - which incorporates rich information\nabout environment dynamics. This allows us to overcome the known \"couch-potato\"\nissues of prior work - when the agent finds a way to instantly gratify itself\nby exploiting actions which lead to hardly predictable consequences. We test\nour approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo. In\nnavigational tasks from ViZDoom and DMLab, our agent outperforms the\nstate-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our\ncuriosity module learns locomotion out of the first-person-view curiosity only.\n",
        "published": "2018-10-04",
        "authors": [
            "Nikolay Savinov",
            "Anton Raichuk",
            "Rapha\u00ebl Marinier",
            "Damien Vincent",
            "Marc Pollefeys",
            "Timothy Lillicrap",
            "Sylvain Gelly"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.06544v4",
        "title": "Deep Imitative Models for Flexible Inference, Planning, and Control",
        "abstract": "  Imitation Learning (IL) is an appealing approach to learn desirable\nautonomous behavior. However, directing IL to achieve arbitrary goals is\ndifficult. In contrast, planning-based algorithms use dynamics models and\nreward functions to achieve goals. Yet, reward functions that evoke desirable\nbehavior are often difficult to specify. In this paper, we propose Imitative\nModels to combine the benefits of IL and goal-directed planning. Imitative\nModels are probabilistic predictive models of desirable behavior able to plan\ninterpretable expert-like trajectories to achieve specified goals. We derive\nfamilies of flexible goal objectives, including constrained goal regions,\nunconstrained goal sets, and energy-based goals. We show that our method can\nuse these objectives to successfully direct behavior. Our method substantially\noutperforms six IL approaches and a planning-based approach in a dynamic\nsimulated autonomous driving task, and is efficiently learned from expert\ndemonstrations without online data collection. We also show our approach is\nrobust to poorly specified goals, such as goals on the wrong side of the road.\n",
        "published": "2018-10-15",
        "authors": [
            "Nicholas Rhinehart",
            "Rowan McAllister",
            "Sergey Levine"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.11043v1",
        "title": "One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks",
        "abstract": "  We consider the problem of learning multi-stage vision-based tasks on a real\nrobot from a single video of a human performing the task, while leveraging\ndemonstration data of subtasks with other objects. This problem presents a\nnumber of major challenges. Video demonstrations without teleoperation are easy\nfor humans to provide, but do not provide any direct supervision. Learning\npolicies from raw pixels enables full generality but calls for large function\napproximators with many parameters to be learned. Finally, compound tasks can\nrequire impractical amounts of demonstration data, when treated as a monolithic\nskill. To address these challenges, we propose a method that learns both how to\nlearn primitive behaviors from video demonstrations and how to dynamically\ncompose these behaviors to perform multi-stage tasks by \"watching\" a human\ndemonstrator. Our results on a simulated Sawyer robot and real PR2 robot\nillustrate our method for learning a variety of order fulfillment and kitchen\nserving tasks with novel objects and raw pixel inputs.\n",
        "published": "2018-10-25",
        "authors": [
            "Tianhe Yu",
            "Pieter Abbeel",
            "Sergey Levine",
            "Chelsea Finn"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.01489v3",
        "title": "WoodScape: A multi-task, multi-camera fisheye dataset for autonomous\n  driving",
        "abstract": "  Fisheye cameras are commonly employed for obtaining a large field of view in\nsurveillance, augmented reality and in particular automotive applications. In\nspite of their prevalence, there are few public datasets for detailed\nevaluation of computer vision algorithms on fisheye images. We release the\nfirst extensive fisheye automotive dataset, WoodScape, named after Robert Wood\nwho invented the fisheye camera in 1906. WoodScape comprises of four surround\nview cameras and nine tasks including segmentation, depth estimation, 3D\nbounding box detection and soiling detection. Semantic annotation of 40 classes\nat the instance level is provided for over 10,000 images and annotation for\nother tasks are provided for over 100,000 images. With WoodScape, we would like\nto encourage the community to adapt computer vision models for fisheye camera\ninstead of using naive rectification.\n",
        "published": "2019-05-04",
        "authors": [
            "Senthil Yogamani",
            "Ciaran Hughes",
            "Jonathan Horgan",
            "Ganesh Sistu",
            "Padraig Varley",
            "Derek O'Dea",
            "Michal Uricar",
            "Stefan Milz",
            "Martin Simon",
            "Karl Amende",
            "Christian Witt",
            "Hazem Rashed",
            "Sumanth Chennupati",
            "Sanjaya Nayak",
            "Saquib Mansoor",
            "Xavier Perroton",
            "Patrick Perez"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.01492v2",
        "title": "SoilingNet: Soiling Detection on Automotive Surround-View Cameras",
        "abstract": "  Cameras are an essential part of sensor suite in autonomous driving.\nSurround-view cameras are directly exposed to external environment and are\nvulnerable to get soiled. Cameras have a much higher degradation in performance\ndue to soiling compared to other sensors. Thus it is critical to accurately\ndetect soiling on the cameras, particularly for higher levels of autonomous\ndriving. We created a new dataset having multiple types of soiling namely\nopaque and transparent. It will be released publicly as part of our WoodScape\ndataset \\cite{yogamani2019woodscape} to encourage further research. We\ndemonstrate high accuracy using a Convolutional Neural Network (CNN) based\narchitecture. We also show that it can be combined with the existing object\ndetection task in a multi-task learning framework. Finally, we make use of\nGenerative Adversarial Networks (GANs) to generate more images for data\naugmentation and show that it works successfully similar to the style transfer.\n",
        "published": "2019-05-04",
        "authors": [
            "Michal Uricar",
            "Pavel Krizek",
            "Ganesh Sistu",
            "Senthil Yogamani"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.01631v2",
        "title": "Conditional Generative Neural System for Probabilistic Trajectory\n  Prediction",
        "abstract": "  Effective understanding of the environment and accurate trajectory prediction\nof surrounding dynamic obstacles are critical for intelligent systems such as\nautonomous vehicles and wheeled mobile robotics navigating in complex scenarios\nto achieve safe and high-quality decision making, motion planning and control.\nDue to the uncertain nature of the future, it is desired to make inference from\na probability perspective instead of deterministic prediction. In this paper,\nwe propose a conditional generative neural system (CGNS) for probabilistic\ntrajectory prediction to approximate the data distribution, with which\nrealistic, feasible and diverse future trajectory hypotheses can be sampled.\nThe system combines the strengths of conditional latent space learning and\nvariational divergence minimization, and leverages both static context and\ninteraction information with soft attention mechanisms. We also propose a\nregularization method for incorporating soft constraints into deep neural\nnetworks with differentiable barrier functions, which can regulate and push the\ngenerated samples into the feasible regions. The proposed system is evaluated\non several public benchmark datasets for pedestrian trajectory prediction and a\nroundabout naturalistic driving dataset collected by ourselves. The\nexperimental results demonstrate that our model achieves better performance\nthan various baseline approaches in terms of prediction accuracy.\n",
        "published": "2019-05-05",
        "authors": [
            "Jiachen Li",
            "Hengbo Ma",
            "Masayoshi Tomizuka"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.04161v1",
        "title": "Self-Supervised Exploration via Disagreement",
        "abstract": "  Efficient exploration is a long-standing problem in sensorimotor learning.\nMajor advances have been demonstrated in noise-free, non-stochastic domains\nsuch as video games and simulation. However, most of these formulations either\nget stuck in environments with stochastic dynamics or are too inefficient to be\nscalable to real robotics setups. In this paper, we propose a formulation for\nexploration inspired by the work in active learning literature. Specifically,\nwe train an ensemble of dynamics models and incentivize the agent to explore\nsuch that the disagreement of those ensembles is maximized. This allows the\nagent to learn skills by exploring in a self-supervised manner without any\nexternal reward. Notably, we further leverage the disagreement objective to\noptimize the agent's policy in a differentiable manner, without using\nreinforcement learning, which results in a sample-efficient exploration. We\ndemonstrate the efficacy of this formulation across a variety of benchmark\nenvironments including stochastic-Atari, Mujoco and Unity. Finally, we\nimplement our differentiable exploration on a real robot which learns to\ninteract with objects completely from scratch. Project videos and code are at\nhttps://pathak22.github.io/exploration-by-disagreement/\n",
        "published": "2019-06-10",
        "authors": [
            "Deepak Pathak",
            "Dhiraj Gandhi",
            "Abhinav Gupta"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.05829v1",
        "title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks\n  via Visual Subgoal Generation",
        "abstract": "  Video prediction models combined with planning algorithms have shown promise\nin enabling robots to learn to perform many vision-based tasks through only\nself-supervision, reaching novel goals in cluttered scenes with unseen objects.\nHowever, due to the compounding uncertainty in long horizon video prediction\nand poor scalability of sampling-based planning optimizers, one significant\nlimitation of these approaches is the ability to plan over long horizons to\nreach distant goals. To that end, we propose a framework for subgoal generation\nand planning, hierarchical visual foresight (HVF), which generates subgoal\nimages conditioned on a goal image, and uses them for planning. The subgoal\nimages are directly optimized to decompose the task into easy to plan segments,\nand as a result, we observe that the method naturally identifies semantically\nmeaningful states as subgoals. Across three out of four simulated vision-based\nmanipulation tasks, we find that our method achieves nearly a 200% performance\nimprovement over planning without subgoals and model-free RL approaches.\nFurther, our experiments illustrate that our approach extends to real,\ncluttered visual scenes. Project page:\nhttps://sites.google.com/stanford.edu/hvf\n",
        "published": "2019-09-12",
        "authors": [
            "Suraj Nair",
            "Chelsea Finn"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.10868v2",
        "title": "End-to-End Model-Free Reinforcement Learning for Urban Driving using\n  Implicit Affordances",
        "abstract": "  Reinforcement Learning (RL) aims at learning an optimal behavior policy from\nits own experiments and not rule-based control methods. However, there is no RL\nalgorithm yet capable of handling a task as difficult as urban driving. We\npresent a novel technique, coined implicit affordances, to effectively leverage\nRL for urban driving thus including lane keeping, pedestrians and vehicles\navoidance, and traffic light detection. To our knowledge we are the first to\npresent a successful RL agent handling such a complex task especially regarding\nthe traffic light detection. Furthermore, we have demonstrated the\neffectiveness of our method by winning the Camera Only track of the CARLA\nchallenge.\n",
        "published": "2019-11-25",
        "authors": [
            "Marin Toromanoff",
            "Emilie Wirbel",
            "Fabien Moutarde"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.02788v1",
        "title": "Neural Dynamic Policies for End-to-End Sensorimotor Learning",
        "abstract": "  The current dominant paradigm in sensorimotor control, whether imitation or\nreinforcement learning, is to train policies directly in raw action spaces such\nas torque, joint angle, or end-effector position. This forces the agent to make\ndecisions individually at each timestep in training, and hence, limits the\nscalability to continuous, high-dimensional, and long-horizon tasks. In\ncontrast, research in classical robotics has, for a long time, exploited\ndynamical systems as a policy representation to learn robot behaviors via\ndemonstrations. These techniques, however, lack the flexibility and\ngeneralizability provided by deep learning or reinforcement learning and have\nremained under-explored in such settings. In this work, we begin to close this\ngap and embed the structure of a dynamical system into deep neural\nnetwork-based policies by reparameterizing action spaces via second-order\ndifferential equations. We propose Neural Dynamic Policies (NDPs) that make\npredictions in trajectory distribution space as opposed to prior policy\nlearning methods where actions represent the raw control space. The embedded\nstructure allows end-to-end policy learning for both reinforcement and\nimitation learning setups. We show that NDPs outperform the prior\nstate-of-the-art in terms of either efficiency or performance across several\nrobotic control tasks for both imitation and reinforcement learning setups.\nProject video and code are available at\nhttps://shikharbahl.github.io/neural-dynamic-policies/\n",
        "published": "2020-12-04",
        "authors": [
            "Shikhar Bahl",
            "Mustafa Mukadam",
            "Abhinav Gupta",
            "Deepak Pathak"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.13588v1",
        "title": "Making Curiosity Explicit in Vision-based RL",
        "abstract": "  Vision-based reinforcement learning (RL) is a promising technique to solve\ncontrol tasks involving images as the main observation. State-of-the-art RL\nalgorithms still struggle in terms of sample efficiency, especially when using\nimage observations. This has led to an increased attention on integrating state\nrepresentation learning (SRL) techniques into the RL pipeline. Work in this\nfield demonstrates a substantial improvement in sample efficiency among other\nbenefits. However, to take full advantage of this paradigm, the quality of\nsamples used for training plays a crucial role. More importantly, the diversity\nof these samples could affect the sample efficiency of vision-based RL, but\nalso its generalization capability. In this work, we present an approach to\nimprove the sample diversity. Our method enhances the exploration capability of\nthe RL algorithms by taking advantage of the SRL setup. Our experiments show\nthat the presented approach outperforms the baseline for all tested\nenvironments. These results are most apparent for environments where the\nbaseline method struggles. Even in simple environments, our method stabilizes\nthe training, reduces the reward variance and boosts sample efficiency.\n",
        "published": "2021-09-28",
        "authors": [
            "Elie Aljalbout",
            "Maximilian Ulmer",
            "Rudolph Triebel"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.09514v1",
        "title": "Discovering and Achieving Goals via World Models",
        "abstract": "  How can artificial agents learn to solve many diverse tasks in complex visual\nenvironments in the absence of any supervision? We decompose this question into\ntwo problems: discovering new goals and learning to reliably achieve them. We\nintroduce Latent Explorer Achiever (LEXA), a unified solution to these that\nlearns a world model from image inputs and uses it to train an explorer and an\nachiever policy from imagined rollouts. Unlike prior methods that explore by\nreaching previously visited states, the explorer plans to discover unseen\nsurprising states through foresight, which are then used as diverse targets for\nthe achiever to practice. After the unsupervised phase, LEXA solves tasks\nspecified as goal images zero-shot without any additional learning. LEXA\nsubstantially outperforms previous approaches to unsupervised goal-reaching,\nboth on prior benchmarks and on a new challenging benchmark with a total of 40\ntest tasks spanning across four standard robotic manipulation and locomotion\ndomains. LEXA further achieves goals that require interacting with multiple\nobjects in sequence. Finally, to demonstrate the scalability and generality of\nLEXA, we train a single general agent across four distinct environments. Code\nand videos at https://orybkin.github.io/lexa/\n",
        "published": "2021-10-18",
        "authors": [
            "Russell Mendonca",
            "Oleh Rybkin",
            "Kostas Daniilidis",
            "Danijar Hafner",
            "Deepak Pathak"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.09297v1",
        "title": "Learning to Compose Visual Relations",
        "abstract": "  The visual world around us can be described as a structured set of objects\nand their associated relations. An image of a room may be conjured given only\nthe description of the underlying objects and their associated relations. While\nthere has been significant work on designing deep neural networks which may\ncompose individual objects together, less work has been done on composing the\nindividual relations between objects. A principal difficulty is that while the\nplacement of objects is mutually independent, their relations are entangled and\ndependent on each other. To circumvent this issue, existing works primarily\ncompose relations by utilizing a holistic encoder, in the form of text or\ngraphs. In this work, we instead propose to represent each relation as an\nunnormalized density (an energy-based model), enabling us to compose separate\nrelations in a factorized manner. We show that such a factorized decomposition\nallows the model to both generate and edit scenes that have multiple sets of\nrelations more faithfully. We further show that decomposition enables our model\nto effectively understand the underlying relational scene structure. Project\npage at: https://composevisualrelations.github.io/.\n",
        "published": "2021-11-17",
        "authors": [
            "Nan Liu",
            "Shuang Li",
            "Yilun Du",
            "Joshua B. Tenenbaum",
            "Antonio Torralba"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.04624v1",
        "title": "KEMP: Keyframe-Based Hierarchical End-to-End Deep Model for Long-Term\n  Trajectory Prediction",
        "abstract": "  Predicting future trajectories of road agents is a critical task for\nautonomous driving. Recent goal-based trajectory prediction methods, such as\nDenseTNT and PECNet, have shown good performance on prediction tasks on public\ndatasets. However, they usually require complicated goal-selection algorithms\nand optimization. In this work, we propose KEMP, a hierarchical end-to-end deep\nlearning framework for trajectory prediction. At the core of our framework is\nkeyframe-based trajectory prediction, where keyframes are representative states\nthat trace out the general direction of the trajectory. KEMP first predicts\nkeyframes conditioned on the road context, and then fills in intermediate\nstates conditioned on the keyframes and the road context. Under our general\nframework, goal-conditioned methods are special cases in which the number of\nkeyframes equal to one. Unlike goal-conditioned methods, our keyframe predictor\nis learned automatically and does not require hand-crafted goal-selection\nalgorithms. We evaluate our model on public benchmarks and our model ranked 1st\non Waymo Open Motion Dataset Leaderboard (as of September 1, 2021).\n",
        "published": "2022-05-10",
        "authors": [
            "Qiujing Lu",
            "Weiqiao Han",
            "Jeffrey Ling",
            "Minfa Wang",
            "Haoyu Chen",
            "Balakrishnan Varadarajan",
            "Paul Covington"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.13443v2",
        "title": "Guided Flows for Generative Modeling and Decision Making",
        "abstract": "  Classifier-free guidance is a key component for enhancing the performance of\nconditional generative models across diverse tasks. While it has previously\ndemonstrated remarkable improvements for the sample quality, it has only been\nexclusively employed for diffusion models. In this paper, we integrate\nclassifier-free guidance into Flow Matching (FM) models, an alternative\nsimulation-free approach that trains Continuous Normalizing Flows (CNFs) based\non regressing vector fields. We explore the usage of \\emph{Guided Flows} for a\nvariety of downstream applications. We show that Guided Flows significantly\nimproves the sample quality in conditional image generation and zero-shot\ntext-to-speech synthesis, boasting state-of-the-art performance. Notably, we\nare the first to apply flow models for plan generation in the offline\nreinforcement learning setting, showcasing a 10x speedup in computation\ncompared to diffusion models while maintaining comparable performance.\n",
        "published": "2023-11-22",
        "authors": [
            "Qinqing Zheng",
            "Matt Le",
            "Neta Shaul",
            "Yaron Lipman",
            "Aditya Grover",
            "Ricky T. Q. Chen"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.09056v1",
        "title": "ReCoRe: Regularized Contrastive Representation Learning of World Model",
        "abstract": "  While recent model-free Reinforcement Learning (RL) methods have demonstrated\nhuman-level effectiveness in gaming environments, their success in everyday\ntasks like visual navigation has been limited, particularly under significant\nappearance variations. This limitation arises from (i) poor sample efficiency\nand (ii) over-fitting to training scenarios. To address these challenges, we\npresent a world model that learns invariant features using (i) contrastive\nunsupervised learning and (ii) an intervention-invariant regularizer. Learning\nan explicit representation of the world dynamics i.e. a world model, improves\nsample efficiency while contrastive learning implicitly enforces learning of\ninvariant features, which improves generalization. However, the naive\nintegration of contrastive loss to world models fails due to a lack of\nsupervisory signals to the visual encoder, as world-model-based RL methods\nindependently optimize representation learning and agent policy. To overcome\nthis issue, we propose an intervention-invariant regularizer in the form of an\nauxiliary task such as depth prediction, image denoising, etc., that explicitly\nenforces invariance to style-interventions. Our method outperforms current\nstate-of-the-art model-based and model-free RL methods and significantly on\nout-of-distribution point navigation task evaluated on the iGibson benchmark.\nWe further demonstrate that our approach, with only visual observations,\noutperforms recent language-guided foundation models for point navigation,\nwhich is essential for deployment on robots with limited computation\ncapabilities. Finally, we demonstrate that our proposed model excels at the\nsim-to-real transfer of its perception module on Gibson benchmark.\n",
        "published": "2023-12-14",
        "authors": [
            "Rudra P. K. Poudel",
            "Harit Pandya",
            "Stephan Liwicki",
            "Roberto Cipolla"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.05363v1",
        "title": "Curiosity-driven Exploration by Self-supervised Prediction",
        "abstract": "  In many real-world scenarios, rewards extrinsic to the agent are extremely\nsparse, or absent altogether. In such cases, curiosity can serve as an\nintrinsic reward signal to enable the agent to explore its environment and\nlearn skills that might be useful later in its life. We formulate curiosity as\nthe error in an agent's ability to predict the consequence of its own actions\nin a visual feature space learned by a self-supervised inverse dynamics model.\nOur formulation scales to high-dimensional continuous state spaces like images,\nbypasses the difficulties of directly predicting pixels, and, critically,\nignores the aspects of the environment that cannot affect the agent. The\nproposed approach is evaluated in two environments: VizDoom and Super Mario\nBros. Three broad settings are investigated: 1) sparse extrinsic reward, where\ncuriosity allows for far fewer interactions with the environment to reach the\ngoal; 2) exploration with no extrinsic reward, where curiosity pushes the agent\nto explore more efficiently; and 3) generalization to unseen scenarios (e.g.\nnew levels of the same game) where the knowledge gained from earlier experience\nhelps the agent explore new places much faster than starting from scratch. Demo\nvideo and code available at https://pathak22.github.io/noreward-rl/\n",
        "published": "2017-05-15",
        "authors": [
            "Deepak Pathak",
            "Pulkit Agrawal",
            "Alexei A. Efros",
            "Trevor Darrell"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.09956v3",
        "title": "Learning Synergies between Pushing and Grasping with Self-supervised\n  Deep Reinforcement Learning",
        "abstract": "  Skilled robotic manipulation benefits from complex synergies between\nnon-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing\ncan help rearrange cluttered objects to make space for arms and fingers;\nlikewise, grasping can help displace objects to make pushing movements more\nprecise and collision-free. In this work, we demonstrate that it is possible to\ndiscover and learn these synergies from scratch through model-free deep\nreinforcement learning. Our method involves training two fully convolutional\nnetworks that map from visual observations to actions: one infers the utility\nof pushes for a dense pixel-wise sampling of end effector orientations and\nlocations, while the other does the same for grasping. Both networks are\ntrained jointly in a Q-learning framework and are entirely self-supervised by\ntrial and error, where rewards are provided from successful grasps. In this\nway, our policy learns pushing motions that enable future grasps, while\nlearning grasps that can leverage past pushes. During picking experiments in\nboth simulation and real-world scenarios, we find that our system quickly\nlearns complex behaviors amid challenging cases of clutter, and achieves better\ngrasping success rates and picking efficiencies than baseline alternatives\nafter only a few hours of training. We further demonstrate that our method is\ncapable of generalizing to novel objects. Qualitative results (videos), code,\npre-trained models, and simulation environments are available at\nhttp://vpg.cs.princeton.edu\n",
        "published": "2018-03-27",
        "authors": [
            "Andy Zeng",
            "Shuran Song",
            "Stefan Welker",
            "Johnny Lee",
            "Alberto Rodriguez",
            "Thomas Funkhouser"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.07822v2",
        "title": "Learning Neural Parsers with Deterministic Differentiable Imitation\n  Learning",
        "abstract": "  We explore the problem of learning to decompose spatial tasks into segments,\nas exemplified by the problem of a painting robot covering a large object.\nInspired by the ability of classical decision tree algorithms to construct\nstructured partitions of their input spaces, we formulate the problem of\ndecomposing objects into segments as a parsing approach. We make the insight\nthat the derivation of a parse-tree that decomposes the object into segments\nclosely resembles a decision tree constructed by ID3, which can be done when\nthe ground-truth available. We learn to imitate an expert parsing oracle, such\nthat our neural parser can generalize to parse natural images without ground\ntruth. We introduce a novel deterministic policy gradient update, DRAG (i.e.,\nDeteRministically AGgrevate) in the form of a deterministic actor-critic\nvariant of AggreVaTeD, to train our neural parser. From another perspective,\nour approach is a variant of the Deterministic Policy Gradient suitable for the\nimitation learning setting. The deterministic policy representation offered by\ntraining our neural parser with DRAG allows it to outperform state of the art\nimitation and reinforcement learning approaches.\n",
        "published": "2018-06-20",
        "authors": [
            "Tanmay Shankar",
            "Nicholas Rhinehart",
            "Katharina Muelling",
            "Kris M. Kitani"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.01001v2",
        "title": "Modular Vehicle Control for Transferring Semantic Information Between\n  Weather Conditions Using GANs",
        "abstract": "  Even though end-to-end supervised learning has shown promising results for\nsensorimotor control of self-driving cars, its performance is greatly affected\nby the weather conditions under which it was trained, showing poor\ngeneralization to unseen conditions. In this paper, we show how knowledge can\nbe transferred using semantic maps to new weather conditions without the need\nto obtain new ground truth data. To this end, we propose to divide the task of\nvehicle control into two independent modules: a control module which is only\ntrained on one weather condition for which labeled steering data is available,\nand a perception module which is used as an interface between new weather\nconditions and the fixed control module. To generate the semantic data needed\nto train the perception module, we propose to use a generative adversarial\nnetwork (GAN)-based model to retrieve the semantic information for the new\nconditions in an unsupervised manner. We introduce a master-servant\narchitecture, where the master model (semantic labels available) trains the\nservant model (semantic labels not available). We show that our proposed method\ntrained with ground truth data for a single weather condition is capable of\nachieving similar results on the task of steering angle prediction as an\nend-to-end model trained with ground truth data of 15 different weather\nconditions.\n",
        "published": "2018-07-03",
        "authors": [
            "Patrick Wenzel",
            "Qadeer Khan",
            "Daniel Cremers",
            "Laura Leal-Taix\u00e9"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.03765v1",
        "title": "Latent Space Reinforcement Learning for Steering Angle Prediction",
        "abstract": "  Model-free reinforcement learning has recently been shown to successfully\nlearn navigation policies from raw sensor data. In this work, we address the\nproblem of learning driving policies for an autonomous agent in a high-fidelity\nsimulator. Building upon recent research that applies deep reinforcement\nlearning to navigation problems, we present a modular deep reinforcement\nlearning approach to predict the steering angle of the car from raw images. The\nfirst module extracts a low-dimensional latent semantic representation of the\nimage. The control module trained with reinforcement learning takes the latent\nvector as input to predict the correct steering angle. The experimental results\nhave showed that our method is capable of learning to maneuver the car without\nany human control signals.\n",
        "published": "2019-02-11",
        "authors": [
            "Qadeer Khan",
            "Torsten Sch\u00f6n",
            "Patrick Wenzel"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.11432v1",
        "title": "HRL4IN: Hierarchical Reinforcement Learning for Interactive Navigation\n  with Mobile Manipulators",
        "abstract": "  Most common navigation tasks in human environments require auxiliary arm\ninteractions, e.g. opening doors, pressing buttons and pushing obstacles away.\nThis type of navigation tasks, which we call Interactive Navigation, requires\nthe use of mobile manipulators: mobile bases with manipulation capabilities.\nInteractive Navigation tasks are usually long-horizon and composed of\nheterogeneous phases of pure navigation, pure manipulation, and their\ncombination. Using the wrong part of the embodiment is inefficient and hinders\nprogress. We propose HRL4IN, a novel Hierarchical RL architecture for\nInteractive Navigation tasks. HRL4IN exploits the exploration benefits of HRL\nover flat RL for long-horizon tasks thanks to temporally extended commitments\ntowards subgoals. Different from other HRL solutions, HRL4IN handles the\nheterogeneous nature of the Interactive Navigation task by creating subgoals in\ndifferent spaces in different phases of the task. Moreover, HRL4IN selects\ndifferent parts of the embodiment to use for each phase, improving energy\nefficiency. We evaluate HRL4IN against flat PPO and HAC, a state-of-the-art HRL\nalgorithm, on Interactive Navigation in two environments - a 2D grid-world\nenvironment and a 3D environment with physics simulation. We show that HRL4IN\nsignificantly outperforms its baselines in terms of task performance and energy\nefficiency. More information is available at\nhttps://sites.google.com/view/hrl4in.\n",
        "published": "2019-10-24",
        "authors": [
            "Chengshu Li",
            "Fei Xia",
            "Roberto Martin-Martin",
            "Silvio Savarese"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.12887v5",
        "title": "End-to-end Recurrent Multi-Object Tracking and Trajectory Prediction\n  with Relational Reasoning",
        "abstract": "  The majority of contemporary object-tracking approaches do not model\ninteractions between objects. This contrasts with the fact that objects' paths\nare not independent: a cyclist might abruptly deviate from a previously planned\ntrajectory in order to avoid colliding with a car. Building upon HART, a neural\nclass-agnostic single-object tracker, we introduce a multi-object tracking\nmethod MOHART capable of relational reasoning. Importantly, the entire system,\nincluding the understanding of interactions and relations between objects, is\nclass-agnostic and learned simultaneously in an end-to-end fashion. We explore\na number of relational reasoning architectures and show that\npermutation-invariant models outperform non-permutation-invariant alternatives.\nWe also find that architectures using a single permutation invariant operation\nlike DeepSets, despite, in theory, being universal function approximators, are\nnonetheless outperformed by a more complex architecture based on multi-headed\nattention. The latter better accounts for complex physical interactions in a\nchallenging toy experiment. Further, we find that modelling interactions leads\nto consistent performance gains in tracking as well as future trajectory\nprediction on three real-world datasets (MOTChallenge, UA-DETRAC, and Stanford\nDrone dataset), particularly in the presence of ego-motion, occlusions, crowded\nscenes, and faulty sensor inputs.\n",
        "published": "2019-07-12",
        "authors": [
            "Fabian B. Fuchs",
            "Adam R. Kosiorek",
            "Li Sun",
            "Oiwi Parker Jones",
            "Ingmar Posner"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.10972v2",
        "title": "Reasoning About Physical Interactions with Object-Oriented Prediction\n  and Planning",
        "abstract": "  Object-based factorizations provide a useful level of abstraction for\ninteracting with the world. Building explicit object representations, however,\noften requires supervisory signals that are difficult to obtain in practice. We\npresent a paradigm for learning object-centric representations for physical\nscene understanding without direct supervision of object properties. Our model,\nObject-Oriented Prediction and Planning (O2P2), jointly learns a perception\nfunction to map from image observations to object representations, a pairwise\nphysics interaction function to predict the time evolution of a collection of\nobjects, and a rendering function to map objects back to pixels. For\nevaluation, we consider not only the accuracy of the physical predictions of\nthe model, but also its utility for downstream tasks that require an actionable\nrepresentation of intuitive physics. After training our model on an image\nprediction task, we can use its learned representations to build block towers\nmore complicated than those observed during training.\n",
        "published": "2018-12-28",
        "authors": [
            "Michael Janner",
            "Sergey Levine",
            "William T. Freeman",
            "Joshua B. Tenenbaum",
            "Chelsea Finn",
            "Jiajun Wu"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.05398v1",
        "title": "Deep Visual Reasoning: Learning to Predict Action Sequences for Task and\n  Motion Planning from an Initial Scene Image",
        "abstract": "  In this paper, we propose a deep convolutional recurrent neural network that\npredicts action sequences for task and motion planning (TAMP) from an initial\nscene image. Typical TAMP problems are formalized by combining reasoning on a\nsymbolic, discrete level (e.g. first-order logic) with continuous motion\nplanning such as nonlinear trajectory optimization. Due to the great\ncombinatorial complexity of possible discrete action sequences, a large number\nof optimization/motion planning problems have to be solved to find a solution,\nwhich limits the scalability of these approaches.\n  To circumvent this combinatorial complexity, we develop a neural network\nwhich, based on an initial image of the scene, directly predicts promising\ndiscrete action sequences such that ideally only one motion planning problem\nhas to be solved to find a solution to the overall TAMP problem. A key aspect\nis that our method generalizes to scenes with many and varying number of\nobjects, although being trained on only two objects at a time. This is possible\nby encoding the objects of the scene in images as input to the neural network,\ninstead of a fixed feature vector. Results show runtime improvements of several\nmagnitudes. Video: https://youtu.be/i8yyEbbvoEk\n",
        "published": "2020-06-09",
        "authors": [
            "Danny Driess",
            "Jung-Su Ha",
            "Marc Toussaint"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.08975v3",
        "title": "Particle Filter Networks with Application to Visual Localization",
        "abstract": "  Particle filtering is a powerful approach to sequential state estimation and\nfinds application in many domains, including robot localization, object\ntracking, etc. To apply particle filtering in practice, a critical challenge is\nto construct probabilistic system models, especially for systems with complex\ndynamics or rich sensory inputs such as camera images. This paper introduces\nthe Particle Filter Network (PFnet), which encodes both a system model and a\nparticle filter algorithm in a single neural network. The PF-net is fully\ndifferentiable and trained end-to-end from data. Instead of learning a generic\nsystem model, it learns a model optimized for the particle filter algorithm. We\napply the PF-net to a visual localization task, in which a robot must localize\nin a rich 3-D world, using only a schematic 2-D floor map. In simulation\nexperiments, PF-net consistently outperforms alternative learning\narchitectures, as well as a traditional model-based method, under a variety of\nsensor inputs. Further, PF-net generalizes well to new, unseen environments.\n",
        "published": "2018-05-23",
        "authors": [
            "Peter Karkus",
            "David Hsu",
            "Wee Sun Lee"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.00482v1",
        "title": "Few-Shot Goal Inference for Visuomotor Learning and Planning",
        "abstract": "  Reinforcement learning and planning methods require an objective or reward\nfunction that encodes the desired behavior. Yet, in practice, there is a wide\nrange of scenarios where an objective is difficult to provide programmatically,\nsuch as tasks with visual observations involving unknown object positions or\ndeformable objects. In these cases, prior methods use engineered\nproblem-specific solutions, e.g., by instrumenting the environment with\nadditional sensors to measure a proxy for the objective. Such solutions require\na significant engineering effort on a per-task basis, and make it impractical\nfor robots to continuously learn complex skills outside of laboratory settings.\nWe aim to find a more general and scalable solution for specifying goals for\nrobot learning in unconstrained environments. To that end, we formulate the\nfew-shot objective learning problem, where the goal is to learn a task\nobjective from only a few example images of successful end states for that\ntask. We propose a simple solution to this problem: meta-learn a classifier\nthat can recognize new goals from a few examples. We show how this approach can\nbe used with both model-free reinforcement learning and visual model-based\nplanning and show results in three domains: rope manipulation from images in\nsimulation, visual navigation in a simulated 3D environment, and object\narrangement into user-specified configurations on a real robot.\n",
        "published": "2018-09-30",
        "authors": [
            "Annie Xie",
            "Avi Singh",
            "Sergey Levine",
            "Chelsea Finn"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.01296v3",
        "title": "PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings",
        "abstract": "  For autonomous vehicles (AVs) to behave appropriately on roads populated by\nhuman-driven vehicles, they must be able to reason about the uncertain\nintentions and decisions of other drivers from rich perceptual information.\nTowards these capabilities, we present a probabilistic forecasting model of\nfuture interactions between a variable number of agents. We perform both\nstandard forecasting and the novel task of conditional forecasting, which\nreasons about how all agents will likely respond to the goal of a controlled\nagent (here, the AV). We train models on real and simulated data to forecast\nvehicle trajectories given past positions and LIDAR. Our evaluation shows that\nour model is substantially more accurate in multi-agent driving scenarios\ncompared to existing state-of-the-art. Beyond its general ability to perform\nconditional forecasting queries, we show that our model's predictions of all\nagents improve when conditioned on knowledge of the AV's goal, further\nillustrating its capability to model agent interactions.\n",
        "published": "2019-05-03",
        "authors": [
            "Nicholas Rhinehart",
            "Rowan McAllister",
            "Kris Kitani",
            "Sergey Levine"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.14932v1",
        "title": "Contrastive Unsupervised Learning of World Model with Invariant Causal\n  Features",
        "abstract": "  In this paper we present a world model, which learns causal features using\nthe invariance principle. In particular, we use contrastive unsupervised\nlearning to learn the invariant causal features, which enforces invariance\nacross augmentations of irrelevant parts or styles of the observation. The\nworld-model-based reinforcement learning methods independently optimize\nrepresentation learning and the policy. Thus naive contrastive loss\nimplementation collapses due to a lack of supervisory signals to the\nrepresentation learning module. We propose an intervention invariant auxiliary\ntask to mitigate this issue. Specifically, we utilize depth prediction to\nexplicitly enforce the invariance and use data augmentation as style\nintervention on the RGB observation space. Our design leverages unsupervised\nrepresentation learning to learn the world model with invariant causal\nfeatures. Our proposed method significantly outperforms current\nstate-of-the-art model-based and model-free reinforcement learning methods on\nout-of-distribution point navigation tasks on the iGibson dataset. Moreover,\nour proposed model excels at the sim-to-real transfer of our perception\nlearning module. Finally, we evaluate our approach on the DeepMind control\nsuite and enforce invariance only implicitly since depth is not available.\nNevertheless, our proposed model performs on par with the state-of-the-art\ncounterpart.\n",
        "published": "2022-09-29",
        "authors": [
            "Rudra P. K. Poudel",
            "Harit Pandya",
            "Roberto Cipolla"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.08584v1",
        "title": "Nahid: AI-based Algorithm for operating fully-automatic surgery",
        "abstract": "  In this paper, for the first time, a method is presented that can provide a\nfully automated surgery based on software and computer vision techniques. Then,\nthe advantages and challenges of computerization of medical surgery are\nexamined. Finally, the surgery related to isolated ovarian endometriosis\ndisease has been examined, and based on the presented method, a more detailed\nalgorithm is presented that is capable of automatically diagnosing and treating\nthis disease during surgery as proof of our proposed method where a U-net is\ntrained to detect the endometriosis during surgery.\n",
        "published": "2023-11-03",
        "authors": [
            "Sina Saadati"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.03353v2",
        "title": "Learning Bidirectional Action-Language Translation with Limited\n  Supervision and Incongruent Input",
        "abstract": "  Human infant learning happens during exploration of the environment, by\ninteraction with objects, and by listening to and repeating utterances\ncasually, which is analogous to unsupervised learning. Only occasionally, a\nlearning infant would receive a matching verbal description of an action it is\ncommitting, which is similar to supervised learning. Such a learning mechanism\ncan be mimicked with deep learning. We model this weakly supervised learning\nparadigm using our Paired Gated Autoencoders (PGAE) model, which combines an\naction and a language autoencoder. After observing a performance drop when\nreducing the proportion of supervised training, we introduce the Paired\nTransformed Autoencoders (PTAE) model, using Transformer-based crossmodal\nattention. PTAE achieves significantly higher accuracy in language-to-action\nand action-to-language translations, particularly in realistic but difficult\ncases when only few supervised training samples are available. We also test\nwhether the trained model behaves realistically with conflicting multimodal\ninput. In accordance with the concept of incongruence in psychology, conflict\ndeteriorates the model output. Conflicting action input has a more severe\nimpact than conflicting language input, and more conflicting features lead to\nlarger interference. PTAE can be trained on mostly unlabelled data where\nlabeled data is scarce, and it behaves plausibly when tested with incongruent\ninput.\n",
        "published": "2023-01-09",
        "authors": [
            "Ozan \u00d6zdemir",
            "Matthias Kerzel",
            "Cornelius Weber",
            "Jae Hee Lee",
            "Muhammad Burhan Hafez",
            "Patrick Bruns",
            "Stefan Wermter"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1101.5632v1",
        "title": "Active Markov Information-Theoretic Path Planning for Robotic\n  Environmental Sensing",
        "abstract": "  Recent research in multi-robot exploration and mapping has focused on\nsampling environmental fields, which are typically modeled using the Gaussian\nprocess (GP). Existing information-theoretic exploration strategies for\nlearning GP-based environmental field maps adopt the non-Markovian problem\nstructure and consequently scale poorly with the length of history of\nobservations. Hence, it becomes computationally impractical to use these\nstrategies for in situ, real-time active sampling. To ease this computational\nburden, this paper presents a Markov-based approach to efficient\ninformation-theoretic path planning for active sampling of GP-based fields. We\nanalyze the time complexity of solving the Markov-based path planning problem,\nand demonstrate analytically that it scales better than that of deriving the\nnon-Markovian strategies with increasing length of planning horizon. For a\nclass of exploration tasks called the transect sampling task, we provide\ntheoretical guarantees on the active sampling performance of our Markov-based\npolicy, from which ideal environmental field conditions and sampling task\nsettings can be established to limit its performance degradation due to\nviolation of the Markov assumption. Empirical evaluation on real-world\ntemperature and plankton density field data shows that our Markov-based policy\ncan generally achieve active sampling performance comparable to that of the\nwidely-used non-Markovian greedy policies under less favorable realistic field\nconditions and task settings while enjoying significant computational gain over\nthem.\n",
        "published": "2011-01-28",
        "authors": [
            "Kian Hsiang Low",
            "John M. Dolan",
            "Pradeep Khosla"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1302.0723v2",
        "title": "Multi-Robot Informative Path Planning for Active Sensing of\n  Environmental Phenomena: A Tale of Two Algorithms",
        "abstract": "  A key problem of robotic environmental sensing and monitoring is that of\nactive sensing: How can a team of robots plan the most informative observation\npaths to minimize the uncertainty in modeling and predicting an environmental\nphenomenon? This paper presents two principled approaches to efficient\ninformation-theoretic path planning based on entropy and mutual information\ncriteria for in situ active sensing of an important broad class of\nwidely-occurring environmental phenomena called anisotropic fields. Our\nproposed algorithms are novel in addressing a trade-off between active sensing\nperformance and time efficiency. An important practical consequence is that our\nalgorithms can exploit the spatial correlation structure of Gaussian\nprocess-based anisotropic fields to improve time efficiency while preserving\nnear-optimal active sensing performance. We analyze the time complexity of our\nalgorithms and prove analytically that they scale better than state-of-the-art\nalgorithms with increasing planning horizon length. We provide theoretical\nguarantees on the active sensing performance of our algorithms for a class of\nexploration tasks called transect sampling, which, in particular, can be\nimproved with longer planning time and/or lower spatial correlation along the\ntransect. Empirical evaluation on real-world anisotropic field data shows that\nour algorithms can perform better or at least as well as the state-of-the-art\nalgorithms while often incurring a few orders of magnitude less computational\ntime, even when the field conditions are less favorable.\n",
        "published": "2013-02-04",
        "authors": [
            "Nannan Cao",
            "Kian Hsiang Low",
            "John M. Dolan"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1305.6129v1",
        "title": "Information-Theoretic Approach to Efficient Adaptive Path Planning for\n  Mobile Robotic Environmental Sensing",
        "abstract": "  Recent research in robot exploration and mapping has focused on sampling\nenvironmental hotspot fields. This exploration task is formalized by Low,\nDolan, and Khosla (2008) in a sequential decision-theoretic planning under\nuncertainty framework called MASP. The time complexity of solving MASP\napproximately depends on the map resolution, which limits its use in\nlarge-scale, high-resolution exploration and mapping. To alleviate this\ncomputational difficulty, this paper presents an information-theoretic approach\nto MASP (iMASP) for efficient adaptive path planning; by reformulating the\ncost-minimizing iMASP as a reward-maximizing problem, its time complexity\nbecomes independent of map resolution and is less sensitive to increasing robot\nteam size as demonstrated both theoretically and empirically. Using the\nreward-maximizing dual, we derive a novel adaptive variant of maximum entropy\nsampling, thus improving the induced exploration policy performance. It also\nallows us to establish theoretical bounds quantifying the performance advantage\nof optimal adaptive over non-adaptive policies and the performance quality of\napproximately optimal vs. optimal adaptive policies. We show analytically and\nempirically the superior performance of iMASP-based policies for sampling the\nlog-Gaussian process to that of policies for the widely-used Gaussian process\nin mapping the hotspot field. Lastly, we provide sufficient conditions that,\nwhen met, guarantee adaptivity has no benefit under an assumed environment\nmodel.\n",
        "published": "2013-05-27",
        "authors": [
            "Kian Hsiang Low",
            "John M. Dolan",
            "Pradeep Khosla"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.02702v1",
        "title": "Robust Adversarial Reinforcement Learning",
        "abstract": "  Deep neural networks coupled with fast simulation and improved computation\nhave led to recent successes in the field of reinforcement learning (RL).\nHowever, most current RL-based approaches fail to generalize since: (a) the gap\nbetween simulation and real world is so large that policy-learning approaches\nfail to transfer; (b) even if policy learning is done in real world, the data\nscarcity leads to failed generalization from training to test scenarios (e.g.,\ndue to different friction or object masses). Inspired from H-infinity control\nmethods, we note that both modeling errors and differences in training and test\nscenarios can be viewed as extra forces/disturbances in the system. This paper\nproposes the idea of robust adversarial reinforcement learning (RARL), where we\ntrain an agent to operate in the presence of a destabilizing adversary that\napplies disturbance forces to the system. The jointly trained adversary is\nreinforced -- that is, it learns an optimal destabilization policy. We\nformulate the policy learning as a zero-sum, minimax objective function.\nExtensive experiments in multiple environments (InvertedPendulum, HalfCheetah,\nSwimmer, Hopper and Walker2d) conclusively demonstrate that our method (a)\nimproves training stability; (b) is robust to differences in training/test\nconditions; and c) outperform the baseline even in the absence of the\nadversary.\n",
        "published": "2017-03-08",
        "authors": [
            "Lerrel Pinto",
            "James Davidson",
            "Rahul Sukthankar",
            "Abhinav Gupta"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.10082v3",
        "title": "Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep\n  Reinforcement Learning",
        "abstract": "  Developing a safe and efficient collision avoidance policy for multiple\nrobots is challenging in the decentralized scenarios where each robot generate\nits paths without observing other robots' states and intents. While other\ndistributed multi-robot collision avoidance systems exist, they often require\nextracting agent-level features to plan a local collision-free action, which\ncan be computationally prohibitive and not robust. More importantly, in\npractice the performance of these methods are much lower than their centralized\ncounterparts.\n  We present a decentralized sensor-level collision avoidance policy for\nmulti-robot systems, which directly maps raw sensor measurements to an agent's\nsteering commands in terms of movement velocity. As a first step toward\nreducing the performance gap between decentralized and centralized methods, we\npresent a multi-scenario multi-stage training framework to find an optimal\npolicy which is trained over a large number of robots on rich, complex\nenvironments simultaneously using a policy gradient based reinforcement\nlearning algorithm. We validate the learned sensor-level collision avoidance\npolicy in a variety of simulated scenarios with thorough performance\nevaluations and show that the final learned policy is able to find time\nefficient, collision-free paths for a large-scale robot system. We also\ndemonstrate that the learned policy can be well generalized to new scenarios\nthat do not appear in the entire training period, including navigating a\nheterogeneous group of robots and a large-scale scenario with 100 robots.\nVideos are available at https://sites.google.com/view/drlmaca\n",
        "published": "2017-09-28",
        "authors": [
            "Pinxin Long",
            "Tingxiang Fan",
            "Xinyi Liao",
            "Wenxi Liu",
            "Hao Zhang",
            "Jia Pan"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.06095v2",
        "title": "Graph Neural Networks for Decentralized Multi-Robot Path Planning",
        "abstract": "  Effective communication is key to successful, decentralized, multi-robot path\nplanning. Yet, it is far from obvious what information is crucial to the task\nat hand, and how and when it must be shared among robots. To side-step these\nissues and move beyond hand-crafted heuristics, we propose a combined model\nthat automatically synthesizes local communication and decision-making policies\nfor robots navigating in constrained workspaces. Our architecture is composed\nof a convolutional neural network (CNN) that extracts adequate features from\nlocal observations, and a graph neural network (GNN) that communicates these\nfeatures among robots. We train the model to imitate an expert algorithm, and\nuse the resulting model online in decentralized planning involving only local\ncommunication and local observations. We evaluate our method in simulations {by\nnavigating teams of robots to their destinations in 2D} cluttered workspaces.\nWe measure the success rates and sum of costs over the planned paths. The\nresults show a performance close to that of our expert algorithm, demonstrating\nthe validity of our approach. In particular, we show our model's capability to\ngeneralize to previously unseen cases (involving larger environments and larger\nrobot teams).\n",
        "published": "2019-12-12",
        "authors": [
            "Qingbiao Li",
            "Fernando Gama",
            "Alejandro Ribeiro",
            "Amanda Prorok"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.00786v3",
        "title": "Intelligent Roundabout Insertion using Deep Reinforcement Learning",
        "abstract": "  An important topic in the autonomous driving research is the development of\nmaneuver planning systems. Vehicles have to interact and negotiate with each\nother so that optimal choices, in terms of time and safety, are taken. For this\npurpose, we present a maneuver planning module able to negotiate the entering\nin busy roundabouts. The proposed module is based on a neural network trained\nto predict when and how entering the roundabout throughout the whole duration\nof the maneuver. Our model is trained with a novel implementation of A3C, which\nwe will call Delayed A3C (D-A3C), in a synthetic environment where vehicles\nmove in a realistic manner with interaction capabilities. In addition, the\nsystem is trained such that agents feature a unique tunable behavior, emulating\nreal world scenarios where drivers have their own driving styles. Similarly,\nthe maneuver can be performed using different aggressiveness levels, which is\nparticularly useful to manage busy scenarios where conservative rule-based\npolicies would result in undefined waits.\n",
        "published": "2020-01-03",
        "authors": [
            "Alessandro Paolo Capasso",
            "Giulio Bacchiani",
            "Daniele Molinari"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.10208v1",
        "title": "Towards Learning Multi-agent Negotiations via Self-Play",
        "abstract": "  Making sophisticated, robust, and safe sequential decisions is at the heart\nof intelligent systems. This is especially critical for planning in complex\nmulti-agent environments, where agents need to anticipate other agents'\nintentions and possible future actions. Traditional methods formulate the\nproblem as a Markov Decision Process, but the solutions often rely on various\nassumptions and become brittle when presented with corner cases. In contrast,\ndeep reinforcement learning (Deep RL) has been very effective at finding\npolicies by simultaneously exploring, interacting, and learning from\nenvironments. Leveraging the powerful Deep RL paradigm, we demonstrate that an\niterative procedure of self-play can create progressively more diverse\nenvironments, leading to the learning of sophisticated and robust multi-agent\npolicies. We demonstrate this in a challenging multi-agent simulation of\nmerging traffic, where agents must interact and negotiate with others in order\nto successfully merge on or off the road. While the environment starts off\nsimple, we increase its complexity by iteratively adding an increasingly\ndiverse set of agents to the agent \"zoo\" as training progresses. Qualitatively,\nwe find that through self-play, our policies automatically learn interesting\nbehaviors such as defensive driving, overtaking, yielding, and the use of\nsignal lights to communicate intentions to other agents. In addition,\nquantitatively, we show a dramatic improvement of the success rate of merging\nmaneuvers from 63% to over 98%.\n",
        "published": "2020-01-28",
        "authors": [
            "Yichuan Charlie Tang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.03168v1",
        "title": "Lane-Merging Using Policy-based Reinforcement Learning and\n  Post-Optimization",
        "abstract": "  Many current behavior generation methods struggle to handle real-world\ntraffic situations as they do not scale well with complexity. However,\nbehaviors can be learned off-line using data-driven approaches. Especially,\nreinforcement learning is promising as it implicitly learns how to behave\nutilizing collected experiences. In this work, we combine policy-based\nreinforcement learning with local optimization to foster and synthesize the\nbest of the two methodologies. The policy-based reinforcement learning\nalgorithm provides an initial solution and guiding reference for the\npost-optimization. Therefore, the optimizer only has to compute a single\nhomotopy class, e.g.\\ drive behind or in front of the other vehicle. By storing\nthe state-history during reinforcement learning, it can be used for constraint\nchecking and the optimizer can account for interactions. The post-optimization\nadditionally acts as a safety-layer and the novel method, thus, can be applied\nin safety-critical applications. We evaluate the proposed method using\nlane-change scenarios with a varying number of vehicles.\n",
        "published": "2020-03-06",
        "authors": [
            "Patrick Hart",
            "Leonard Rychly",
            "Alois Knol"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.06906v2",
        "title": "Model-based Reinforcement Learning for Decentralized Multiagent\n  Rendezvous",
        "abstract": "  Collaboration requires agents to align their goals on the fly. Underlying the\nhuman ability to align goals with other agents is their ability to predict the\nintentions of others and actively update their own plans. We propose\nhierarchical predictive planning (HPP), a model-based reinforcement learning\nmethod for decentralized multiagent rendezvous. Starting with pretrained,\nsingle-agent point to point navigation policies and using noisy,\nhigh-dimensional sensor inputs like lidar, we first learn via self-supervision\nmotion predictions of all agents on the team. Next, HPP uses the prediction\nmodels to propose and evaluate navigation subgoals for completing the\nrendezvous task without explicit communication among agents. We evaluate HPP in\na suite of unseen environments, with increasing complexity and numbers of\nobstacles. We show that HPP outperforms alternative reinforcement learning,\npath planning, and heuristic-based baselines on challenging, unseen\nenvironments. Experiments in the real world demonstrate successful transfer of\nthe prediction models from sim to real world without any additional\nfine-tuning. Altogether, HPP removes the need for a centralized operator in\nmultiagent systems by combining model-based RL and inference methods, enabling\nagents to dynamically align plans.\n",
        "published": "2020-03-15",
        "authors": [
            "Rose E. Wang",
            "J. Chase Kew",
            "Dennis Lee",
            "Tsang-Wei Edward Lee",
            "Tingnan Zhang",
            "Brian Ichter",
            "Jie Tan",
            "Aleksandra Faust"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.15807v2",
        "title": "Using Reinforcement Learning to Herd a Robotic Swarm to a Target\n  Distribution",
        "abstract": "  In this paper, we present a reinforcement learning approach to designing a\ncontrol policy for a \"leader\" agent that herds a swarm of \"follower\" agents,\nvia repulsive interactions, as quickly as possible to a target probability\ndistribution over a strongly connected graph. The leader control policy is a\nfunction of the swarm distribution, which evolves over time according to a\nmean-field model in the form of an ordinary difference equation. The dependence\nof the policy on agent populations at each graph vertex, rather than on\nindividual agent activity, simplifies the observations required by the leader\nand enables the control strategy to scale with the number of agents. Two\nTemporal-Difference learning algorithms, SARSA and Q-Learning, are used to\ngenerate the leader control policy based on the follower agent distribution and\nthe leader's location on the graph. A simulation environment corresponding to a\ngrid graph with 4 vertices was used to train and validate the control policies\nfor follower agent populations ranging from 10 to 100. Finally, the control\npolicies trained on 100 simulated agents were used to successfully redistribute\na physical swarm of 10 small robots to a target distribution among 4 spatial\nregions.\n",
        "published": "2020-06-29",
        "authors": [
            "Zahi M. Kakish",
            "Karthik Elamvazhuthi",
            "Spring Berman"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.15724v1",
        "title": "MAPPER: Multi-Agent Path Planning with Evolutionary Reinforcement\n  Learning in Mixed Dynamic Environments",
        "abstract": "  Multi-agent navigation in dynamic environments is of great industrial value\nwhen deploying a large scale fleet of robot to real-world applications. This\npaper proposes a decentralized partially observable multi-agent path planning\nwith evolutionary reinforcement learning (MAPPER) method to learn an effective\nlocal planning policy in mixed dynamic environments. Reinforcement\nlearning-based methods usually suffer performance degradation on long-horizon\ntasks with goal-conditioned sparse rewards, so we decompose the long-range\nnavigation task into many easier sub-tasks under the guidance of a global\nplanner, which increases agents' performance in large environments. Moreover,\nmost existing multi-agent planning approaches assume either perfect information\nof the surrounding environment or homogeneity of nearby dynamic agents, which\nmay not hold in practice. Our approach models dynamic obstacles' behavior with\nan image-based representation and trains a policy in mixed dynamic environments\nwithout homogeneity assumption. To ensure multi-agent training stability and\nperformance, we propose an evolutionary training approach that can be easily\nscaled to large and complex environments. Experiments show that MAPPER is able\nto achieve higher success rates and more stable performance when exposed to a\nlarge number of non-cooperative dynamic obstacles compared with traditional\nreaction-based planner LRA* and the state-of-the-art learning-based method.\n",
        "published": "2020-07-30",
        "authors": [
            "Zuxin Liu",
            "Baiming Chen",
            "Hongyi Zhou",
            "Guru Koushik",
            "Martial Hebert",
            "Ding Zhao"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.02616v2",
        "title": "The Emergence of Adversarial Communication in Multi-Agent Reinforcement\n  Learning",
        "abstract": "  Many real-world problems require the coordination of multiple autonomous\nagents. Recent work has shown the promise of Graph Neural Networks (GNNs) to\nlearn explicit communication strategies that enable complex multi-agent\ncoordination. These works use models of cooperative multi-agent systems whereby\nagents strive to achieve a shared global goal. When considering agents with\nself-interested local objectives, the standard design choice is to model these\nas separate learning systems (albeit sharing the same environment). Such a\ndesign choice, however, precludes the existence of a single, differentiable\ncommunication channel, and consequently prohibits the learning of inter-agent\ncommunication strategies. In this work, we address this gap by presenting a\nlearning model that accommodates individual non-shared rewards and a\ndifferentiable communication channel that is common among all agents. We focus\non the case where agents have self-interested objectives, and develop a\nlearning algorithm that elicits the emergence of adversarial communications. We\nperform experiments on multi-agent coverage and path planning problems, and\nemploy a post-hoc interpretability technique to visualize the messages that\nagents communicate to each other. We show how a single self-interested agent is\ncapable of learning highly manipulative communication strategies that allows it\nto significantly outperform a cooperative team of agents.\n",
        "published": "2020-08-06",
        "authors": [
            "Jan Blumenkamp",
            "Amanda Prorok"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.04452v1",
        "title": "Multi-Agent Safe Planning with Gaussian Processes",
        "abstract": "  Multi-agent safe systems have become an increasingly important area of study\nas we can now easily have multiple AI-powered systems operating together. In\nsuch settings, we need to ensure the safety of not only each individual agent,\nbut also the overall system. In this paper, we introduce a novel multi-agent\nsafe learning algorithm that enables decentralized safe navigation when there\nare multiple different agents in the environment. This algorithm makes mild\nassumptions about other agents and is trained in a decentralized fashion, i.e.\nwith very little prior knowledge about other agents' policies. Experiments show\nour algorithm performs well with the robots running other algorithms when\noptimizing various objectives.\n",
        "published": "2020-08-10",
        "authors": [
            "Zheqing Zhu",
            "Erdem B\u0131y\u0131k",
            "Dorsa Sadigh"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.00499v1",
        "title": "Curious Exploration and Return-based Memory Restoration for Deep\n  Reinforcement Learning",
        "abstract": "  Reward engineering and designing an incentive reward function are non-trivial\ntasks to train agents in complex environments. Furthermore, an inaccurate\nreward function may lead to a biased behaviour which is far from an efficient\nand optimised behaviour. In this paper, we focus on training a single agent to\nscore goals with binary success/failure reward function in Half Field Offense\ndomain. As the major advantage of this research, the agent has no presumption\nabout the environment which means it only follows the original formulation of\nreinforcement learning agents. The main challenge of using such a reward\nfunction is the high sparsity of positive reward signals. To address this\nproblem, we use a simple prediction-based exploration strategy (called Curious\nExploration) along with a Return-based Memory Restoration (RMR) technique which\ntends to remember more valuable memories. The proposed method can be utilized\nto train agents in environments with fairly complex state and action spaces.\nOur experimental results show that many recent solutions including our baseline\nmethod fail to learn and perform in complex soccer domain. However, the\nproposed method can converge easily to the nearly optimal behaviour. The video\npresenting the performance of our trained agent is available at\nhttp://bit.ly/HFO_Binary_Reward.\n",
        "published": "2021-05-02",
        "authors": [
            "Saeed Tafazzol",
            "Erfan Fathi",
            "Mahdi Rezaei",
            "Ehsan Asali"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.03546v1",
        "title": "Scalable, Decentralized Multi-Agent Reinforcement Learning Methods\n  Inspired by Stigmergy and Ant Colonies",
        "abstract": "  Bolstering multi-agent learning algorithms to tackle complex coordination and\ncontrol tasks has been a long-standing challenge of on-going research. Numerous\nmethods have been proposed to help reduce the effects of non-stationarity and\nunscalability. In this work, we investigate a novel approach to decentralized\nmulti-agent learning and planning that attempts to address these two\nchallenges. In particular, this method is inspired by the cohesion,\ncoordination, and behavior of ant colonies. As a result, these algorithms are\ndesigned to be naturally scalable to systems with numerous agents. While no\noptimality is guaranteed, the method is intended to work well in practice and\nscale better in efficacy with the number of agents present than others. The\napproach combines single-agent RL and an ant-colony-inspired decentralized,\nstigmergic algorithm for multi-agent path planning and environment\nmodification. Specifically, we apply this algorithm in a setting where agents\nmust navigate to a goal location, learning to push rectangular boxes into holes\nto yield new traversable pathways. It is shown that while the approach yields\npromising success in this particular environment, it may not be as easily\ngeneralized to others. The algorithm designed is notably scalable to numerous\nagents but is limited in its performance due to its relatively simplistic,\nrule-based approach. Furthermore, the composability of RL-trained policies is\ncalled into question, where, while policies are successful in their training\nenvironments, applying trained policies to a larger-scale, multi-agent\nframework results in unpredictable behavior.\n",
        "published": "2021-05-08",
        "authors": [
            "Austin Anhkhoi Nguyen"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.08601v3",
        "title": "Graph Neural Networks for Decentralized Multi-Robot Submodular Action\n  Selection",
        "abstract": "  The problem of decentralized multi-robot target tracking asks for jointly\nselecting actions, e.g., motion primitives, for the robots to maximize target\ntracking performance with local communications. One major challenge for\npractical implementations is to make target tracking approaches scalable for\nlarge-scale problem instances. In this work, we propose a general-purpose\nlearning architecture toward collaborative target tracking at scale, with\ndecentralized communications. Particularly, our learning architecture leverages\na graph neural network (GNN) to capture local interactions of the robots and\nlearns decentralized decision-making for the robots. We train the learning\nmodel by imitating an expert solution and implement the resulting model for\ndecentralized action selection involving local observations and communications\nonly. We demonstrate the performance of our GNN-based learning approach in a\nscenario of active target tracking with large networks of robots. The\nsimulation results show our approach nearly matches the tracking performance of\nthe expert algorithm, and yet runs several orders faster with up to 100 robots.\nMoreover, it slightly outperforms a decentralized greedy algorithm but runs\nfaster (especially with more than 20 robots). The results also exhibit our\napproach's generalization capability in previously unseen scenarios, e.g.,\nlarger environments and larger networks of robots.\n",
        "published": "2021-05-18",
        "authors": [
            "Lifeng Zhou",
            "Vishnu D. Sharma",
            "Qingbiao Li",
            "Amanda Prorok",
            "Alejandro Ribeiro",
            "Pratap Tokekar",
            "Vijay Kumar"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.09232v4",
        "title": "Using reinforcement learning to autonomously identify sources of error\n  for agents in group missions",
        "abstract": "  When agents swarm to execute a mission, some of them frequently exhibit\nsudden failure, as observed from the command base. It is generally difficult to\ndetermine whether a failure is caused by actuators (hypothesis, $h_a$) or\nsensors (hypothesis, $h_s$) by solely relying on the communication between the\ncommand base and concerning agent. However, by instigating collusion between\nthe agents, the cause of failure can be identified; in other words, we expect\nto detect corresponding displacements for $h_a$ but not for $h_s$. In this\nstudy, we considered the question as to whether artificial intelligence can\nautonomously generate an action plan $\\boldsymbol{g}$ to pinpoint the cause as\naforedescribed. Because the expected response to $\\boldsymbol{g}$ generally\ndepends upon the adopted hypothesis [let the difference be denoted by\n$D(\\boldsymbol{g})$], a formulation that uses $D\\left(\\boldsymbol{g}\\right)$ to\npinpoint the cause can be made. Although a $\\boldsymbol{g}^*$ that maximizes\n$D(\\boldsymbol{g})$ would be a suitable action plan for this task, such an\noptimization is difficult to achieve using the conventional gradient method, as\n$D(\\boldsymbol{g})$ becomes nonzero in rare events such as collisions with\nother agents, and most swarm actions $\\boldsymbol{g}$ give\n$D(\\boldsymbol{g})=0$. In other words, throughout almost the entire space of\n$\\boldsymbol{g}$, $D(\\boldsymbol{g})$ has zero gradient, and the gradient\nmethod is not applicable. To overcome this problem, we formulated an action\nplan using Q-table reinforcement learning. Surprisingly, the optimal action\nplan generated via reinforcement learning presented a human-like solution to\npinpoint the problem by colliding other agents with the failed agent. Using\nthis simple prototype, we demonstrated the potential of applying Q-table\nreinforcement learning methods to plan autonomous actions to pinpoint the\ncauses of failure.\n",
        "published": "2021-07-20",
        "authors": [
            "Keishu Utimula",
            "Ken-taro Hayaschi",
            "Trevor J. Bihl",
            "Kenta Hongo",
            "Ryo Maezono"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.00584v1",
        "title": "A Perceived Environment Design using a Multi-Modal Variational\n  Autoencoder for learning Active-Sensing",
        "abstract": "  This contribution comprises the interplay between a multi-modal variational\nautoencoder and an environment to a perceived environment, on which an agent\ncan act. Furthermore, we conclude our work with a comparison to\ncuriosity-driven learning.\n",
        "published": "2019-11-01",
        "authors": [
            "Timo Korthals",
            "Malte Schilling",
            "J\u00fcrgen Leitner"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.11699v2",
        "title": "Multi-Vehicle Mixed-Reality Reinforcement Learning for Autonomous\n  Multi-Lane Driving",
        "abstract": "  Autonomous driving promises to transform road transport. Multi-vehicle and\nmulti-lane scenarios, however, present unique challenges due to constrained\nnavigation and unpredictable vehicle interactions. Learning-based\nmethods---such as deep reinforcement learning---are emerging as a promising\napproach to automatically design intelligent driving policies that can cope\nwith these challenges. Yet, the process of safely learning multi-vehicle\ndriving behaviours is hard: while collisions---and their near-avoidance---are\nessential to the learning process, directly executing immature policies on\nautonomous vehicles raises considerable safety concerns. In this article, we\npresent a safe and efficient framework that enables the learning of driving\npolicies for autonomous vehicles operating in a shared workspace, where the\nabsence of collisions cannot be guaranteed. Key to our learning procedure is a\nsim2real approach that uses real-world online policy adaptation in a\nmixed-reality setup, where other vehicles and static obstacles exist in the\nvirtual domain. This allows us to perform safe learning by simulating (and\nlearning from) collisions between the learning agent(s) and other objects in\nvirtual reality. Our results demonstrate that, after only a few runs in\nmixed-reality, collisions are significantly reduced.\n",
        "published": "2019-11-26",
        "authors": [
            "Rupert Mitchell",
            "Jenny Fletcher",
            "Jacopo Panerati",
            "Amanda Prorok"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.05420v2",
        "title": "Mobile Robot Path Planning in Dynamic Environments through Globally\n  Guided Reinforcement Learning",
        "abstract": "  Path planning for mobile robots in large dynamic environments is a\nchallenging problem, as the robots are required to efficiently reach their\ngiven goals while simultaneously avoiding potential conflicts with other robots\nor dynamic objects. In the presence of dynamic obstacles, traditional solutions\nusually employ re-planning strategies, which re-call a planning algorithm to\nsearch for an alternative path whenever the robot encounters a conflict.\nHowever, such re-planning strategies often cause unnecessary detours. To\naddress this issue, we propose a learning-based technique that exploits\nenvironmental spatio-temporal information. Different from existing\nlearning-based methods, we introduce a globally guided reinforcement learning\napproach (G2RL), which incorporates a novel reward structure that generalizes\nto arbitrary environments. We apply G2RL to solve the multi-robot path planning\nproblem in a fully distributed reactive manner. We evaluate our method across\ndifferent map types, obstacle densities, and the number of robots. Experimental\nresults show that G2RL generalizes well, outperforming existing distributed\nmethods, and performing very similarly to fully centralized state-of-the-art\nbenchmarks.\n",
        "published": "2020-05-11",
        "authors": [
            "Binyu Wang",
            "Zhe Liu",
            "Qingbiao Li",
            "Amanda Prorok"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.08807v1",
        "title": "Monte Carlo Tree Search Based Tactical Maneuvering",
        "abstract": "  In this paper we explore the application of simultaneous move Monte Carlo\nTree Search (MCTS) based online framework for tactical maneuvering between two\nunmanned aircrafts. Compared to other techniques, MCTS enables efficient search\nover long horizons and uses self-play to select best maneuver in the current\nstate while accounting for the opponent aircraft tactics. We explore different\nalgorithmic choices in MCTS and demonstrate the framework numerically in a\nsimulated 2D tactical maneuvering application.\n",
        "published": "2020-09-13",
        "authors": [
            "Kunal Srivastava",
            "Amit Surana"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.03894v1",
        "title": "Multimodal Trajectory Prediction via Topological Invariance for\n  Navigation at Uncontrolled Intersections",
        "abstract": "  We focus on decentralized navigation among multiple non-communicating\nrational agents at \\emph{uncontrolled} intersections, i.e., street\nintersections without traffic signs or signals. Avoiding collisions in such\ndomains relies on the ability of agents to predict each others' intentions\nreliably, and react quickly. Multiagent trajectory prediction is NP-hard\nwhereas the sample complexity of existing data-driven approaches limits their\napplicability. Our key insight is that the geometric structure of the\nintersection and the incentive of agents to move efficiently and avoid\ncollisions (rationality) reduces the space of likely behaviors, effectively\nrelaxing the problem of trajectory prediction. In this paper, we collapse the\nspace of multiagent trajectories at an intersection into a set of modes\nrepresenting different classes of multiagent behavior, formalized using a\nnotion of topological invariance. Based on this formalism, we design Multiple\nTopologies Prediction (MTP), a data-driven trajectory-prediction mechanism that\nreconstructs trajectory representations of high-likelihood modes in multiagent\nintersection scenes. We show that MTP outperforms a state-of-the-art multimodal\ntrajectory prediction baseline (MFP) in terms of prediction accuracy by 78.24%\non a challenging simulated dataset. Finally, we show that MTP enables our\noptimization-based planner, MTPnav, to achieve collision-free and\ntime-efficient navigation across a variety of challenging intersection\nscenarios on the CARLA simulator.\n",
        "published": "2020-11-08",
        "authors": [
            "Junha Roh",
            "Christoforos Mavrogiannis",
            "Rishabh Madan",
            "Dieter Fox",
            "Siddhartha S. Srinivasa"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.00508v1",
        "title": "Gaussian Process Based Message Filtering for Robust Multi-Agent\n  Cooperation in the Presence of Adversarial Communication",
        "abstract": "  In this paper, we consider the problem of providing robustness to adversarial\ncommunication in multi-agent systems. Specifically, we propose a solution\ntowards robust cooperation, which enables the multi-agent system to maintain\nhigh performance in the presence of anonymous non-cooperative agents that\ncommunicate faulty, misleading or manipulative information. In pursuit of this\ngoal, we propose a communication architecture based on Graph Neural Networks\n(GNNs), which is amenable to a novel Gaussian Process (GP)-based probabilistic\nmodel characterizing the mutual information between the simultaneous\ncommunications of different agents due to their physical proximity and relative\nposition. This model allows agents to locally compute approximate posterior\nprobabilities, or confidences, that any given one of their communication\npartners is being truthful. These confidences can be used as weights in a\nmessage filtering scheme, thereby suppressing the influence of suspicious\ncommunication on the receiving agent's decisions. In order to assess the\nefficacy of our method, we introduce a taxonomy of non-cooperative agents,\nwhich distinguishes them by the amount of information available to them. We\ndemonstrate in two distinct experiments that our method performs well across\nthis taxonomy, outperforming alternative methods. For all but the best informed\nadversaries, our filtering method is able to reduce the impact that\nnon-cooperative agents cause, reducing it to the point of negligibility, and\nwith negligible cost to performance in the absence of adversaries.\n",
        "published": "2020-12-01",
        "authors": [
            "Rupert Mitchell",
            "Jan Blumenkamp",
            "Amanda Prorok"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.12070v1",
        "title": "Learning to Robustly Negotiate Bi-Directional Lane Usage in\n  High-Conflict Driving Scenarios",
        "abstract": "  Recently, autonomous driving has made substantial progress in addressing the\nmost common traffic scenarios like intersection navigation and lane changing.\nHowever, most of these successes have been limited to scenarios with\nwell-defined traffic rules and require minimal negotiation with other vehicles.\nIn this paper, we introduce a previously unconsidered, yet everyday,\nhigh-conflict driving scenario requiring negotiations between agents of equal\nrights and priorities. There exists no centralized control structure and we do\nnot allow communications. Therefore, it is unknown if other drivers are willing\nto cooperate, and if so to what extent. We train policies to robustly negotiate\nwith opposing vehicles of an unobservable degree of cooperativeness using\nmulti-agent reinforcement learning (MARL). We propose Discrete Asymmetric Soft\nActor-Critic (DASAC), a maximum-entropy off-policy MARL algorithm allowing for\ncentralized training with decentralized execution. We show that using DASAC we\nare able to successfully negotiate and traverse the scenario considered over\n99% of the time. Our agents are robust to an unknown timing of opponent\ndecisions, an unobservable degree of cooperativeness of the opposing vehicle,\nand previously unencountered policies. Furthermore, they learn to exhibit\nhuman-like behaviors such as defensive driving, anticipating solution options\nand interpreting the behavior of other agents.\n",
        "published": "2021-03-22",
        "authors": [
            "Christoph Killing",
            "Adam Villaflor",
            "John M. Dolan"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.04678v1",
        "title": "Incentivizing Efficient Equilibria in Traffic Networks with Mixed\n  Autonomy",
        "abstract": "  Traffic congestion has large economic and social costs. The introduction of\nautonomous vehicles can potentially reduce this congestion by increasing road\ncapacity via vehicle platooning and by creating an avenue for influencing\npeople's choice of routes. We consider a network of parallel roads with two\nmodes of transportation: (i) human drivers, who will choose the quickest route\navailable to them, and (ii) a ride hailing service, which provides an array of\nautonomous vehicle route options, each with different prices, to users. We\nformalize a model of vehicle flow in mixed autonomy and a model of how\nautonomous service users make choices between routes with different prices and\nlatencies. Developing an algorithm to learn the preferences of the users, we\nformulate a planning optimization that chooses prices to maximize a social\nobjective. We demonstrate the benefit of the proposed scheme by comparing the\nresults to theoretical benchmarks which we show can be efficiently calculated.\n",
        "published": "2021-05-06",
        "authors": [
            "Erdem B\u0131y\u0131k",
            "Daniel A. Lazar",
            "Ramtin Pedarsani",
            "Dorsa Sadigh"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.06514v1",
        "title": "Vision Transformer for Learning Driving Policies in Complex Multi-Agent\n  Environments",
        "abstract": "  Driving in a complex urban environment is a difficult task that requires a\ncomplex decision policy. In order to make informed decisions, one needs to gain\nan understanding of the long-range context and the importance of other\nvehicles. In this work, we propose to use Vision Transformer (ViT) to learn a\ndriving policy in urban settings with birds-eye-view (BEV) input images. The\nViT network learns the global context of the scene more effectively than with\nearlier proposed Convolutional Neural Networks (ConvNets). Furthermore, ViT's\nattention mechanism helps to learn an attention map for the scene which allows\nthe ego car to determine which surrounding cars are important to its next\ndecision. We demonstrate that a DQN agent with a ViT backbone outperforms\nbaseline algorithms with ConvNet backbones pre-trained in various ways. In\nparticular, the proposed method helps reinforcement learning algorithms to\nlearn faster, with increased performance and less data than baselines.\n",
        "published": "2021-09-14",
        "authors": [
            "Eshagh Kargar",
            "Ville Kyrki"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.15266v3",
        "title": "Modeling Interactions of Autonomous Vehicles and Pedestrians with Deep\n  Multi-Agent Reinforcement Learning for Collision Avoidance",
        "abstract": "  Reliable pedestrian crash avoidance mitigation (PCAM) systems are crucial\ncomponents of safe autonomous vehicles (AVs). The nature of the\nvehicle-pedestrian interaction where decisions of one agent directly affect the\nother agent's optimal behavior, and vice versa, is a challenging yet often\nneglected aspect of such systems. We address this issue by modeling a Markov\ndecision process (MDP) for a simulated AV-pedestrian interaction at an unmarked\ncrosswalk. The AV's PCAM decision policy is learned through deep reinforcement\nlearning (DRL). Since modeling pedestrians realistically is challenging, we\ncompare two levels of intelligent pedestrian behavior. While the baseline model\nfollows a predefined strategy, our advanced pedestrian model is defined as a\nsecond DRL agent. This model captures continuous learning and the uncertainty\ninherent in human behavior, making the AV-pedestrian interaction a deep\nmulti-agent reinforcement learning (DMARL) problem. We benchmark the developed\nPCAM systems according to the collision rate and the resulting traffic flow\nefficiency with a focus on the influence of observation uncertainty on the\ndecision-making of the agents. The results show that the AV is able to\ncompletely mitigate collisions under the majority of the investigated\nconditions and that the DRL pedestrian model learns an intelligent crossing\nbehavior.\n",
        "published": "2021-09-30",
        "authors": [
            "Raphael Trumpp",
            "Harald Bayerlein",
            "David Gesbert"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.08229v1",
        "title": "Influencing Towards Stable Multi-Agent Interactions",
        "abstract": "  Learning in multi-agent environments is difficult due to the non-stationarity\nintroduced by an opponent's or partner's changing behaviors. Instead of\nreactively adapting to the other agent's (opponent or partner) behavior, we\npropose an algorithm to proactively influence the other agent's strategy to\nstabilize -- which can restrain the non-stationarity caused by the other agent.\nWe learn a low-dimensional latent representation of the other agent's strategy\nand the dynamics of how the latent strategy evolves with respect to our robot's\nbehavior. With this learned dynamics model, we can define an unsupervised\nstability reward to train our robot to deliberately influence the other agent\nto stabilize towards a single strategy. We demonstrate the effectiveness of\nstabilizing in improving efficiency of maximizing the task reward in a variety\nof simulated environments, including autonomous driving, emergent\ncommunication, and robotic manipulation. We show qualitative results on our\nwebsite: https://sites.google.com/view/stable-marl/.\n",
        "published": "2021-10-05",
        "authors": [
            "Woodrow Z. Wang",
            "Andy Shih",
            "Annie Xie",
            "Dorsa Sadigh"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.09012v1",
        "title": "Centralizing State-Values in Dueling Networks for Multi-Robot\n  Reinforcement Learning Mapless Navigation",
        "abstract": "  We study the problem of multi-robot mapless navigation in the popular\nCentralized Training and Decentralized Execution (CTDE) paradigm. This problem\nis challenging when each robot considers its path without explicitly sharing\nobservations with other robots and can lead to non-stationary issues in Deep\nReinforcement Learning (DRL). The typical CTDE algorithm factorizes the joint\naction-value function into individual ones, to favor cooperation and achieve\ndecentralized execution. Such factorization involves constraints (e.g.,\nmonotonicity) that limit the emergence of novel behaviors in an individual as\neach agent is trained starting from a joint action-value. In contrast, we\npropose a novel architecture for CTDE that uses a centralized state-value\nnetwork to compute a joint state-value, which is used to inject global state\ninformation in the value-based updates of the agents. Consequently, each model\ncomputes its gradient update for the weights, considering the overall state of\nthe environment. Our idea follows the insights of Dueling Networks as a\nseparate estimation of the joint state-value has both the advantage of\nimproving sample efficiency, while providing each robot information whether the\nglobal state is (or is not) valuable. Experiments in a robotic navigation task\nwith 2 4, and 8 robots, confirm the superior performance of our approach over\nprior CTDE methods (e.g., VDN, QMIX).\n",
        "published": "2021-12-16",
        "authors": [
            "Enrico Marchesini",
            "Alessandro Farinelli"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.08484v4",
        "title": "Iterated Reasoning with Mutual Information in Cooperative and Byzantine\n  Decentralized Teaming",
        "abstract": "  Information sharing is key in building team cognition and enables\ncoordination and cooperation. High-performing human teams also benefit from\nacting strategically with hierarchical levels of iterated communication and\nrationalizability, meaning a human agent can reason about the actions of their\nteammates in their decision-making. Yet, the majority of prior work in\nMulti-Agent Reinforcement Learning (MARL) does not support iterated\nrationalizability and only encourage inter-agent communication, resulting in a\nsuboptimal equilibrium cooperation strategy. In this work, we show that\nreformulating an agent's policy to be conditional on the policies of its\nneighboring teammates inherently maximizes Mutual Information (MI) lower-bound\nwhen optimizing under Policy Gradient (PG). Building on the idea of\ndecision-making under bounded rationality and cognitive hierarchy theory, we\nshow that our modified PG approach not only maximizes local agent rewards but\nalso implicitly reasons about MI between agents without the need for any\nexplicit ad-hoc regularization terms. Our approach, InfoPG, outperforms\nbaselines in learning emergent collaborative behaviors and sets the\nstate-of-the-art in decentralized cooperative MARL tasks. Our experiments\nvalidate the utility of InfoPG by achieving higher sample efficiency and\nsignificantly larger cumulative reward in several complex cooperative\nmulti-agent domains.\n",
        "published": "2022-01-20",
        "authors": [
            "Sachin Konan",
            "Esmaeil Seraj",
            "Matthew Gombolay"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.11994v2",
        "title": "FCMNet: Full Communication Memory Net for Team-Level Cooperation in\n  Multi-Agent Systems",
        "abstract": "  Decentralized cooperation in partially-observable multi-agent systems\nrequires effective communications among agents. To support this effort, this\nwork focuses on the class of problems where global communications are available\nbut may be unreliable, thus precluding differentiable communication learning\nmethods. We introduce FCMNet, a reinforcement learning based approach that\nallows agents to simultaneously learn a) an effective multi-hop communications\nprotocol and b) a common, decentralized policy that enables team-level\ndecision-making. Specifically, our proposed method utilizes the hidden states\nof multiple directional recurrent neural networks as communication messages\namong agents. Using a simple multi-hop topology, we endow each agent with the\nability to receive information sequentially encoded by every other agent at\neach time step, leading to improved global cooperation. We demonstrate FCMNet\non a challenging set of StarCraft II micromanagement tasks with shared rewards,\nas well as a collaborative multi-agent pathfinding task with individual\nrewards. There, our comparison results show that FCMNet outperforms\nstate-of-the-art communication-based reinforcement learning methods in all\nStarCraft II micromanagement tasks, and value decomposition methods in certain\ntasks. We further investigate the robustness of FCMNet under realistic\ncommunication disturbances, such as random message loss or binarized messages\n(i.e., non-differentiable communication channels), to showcase FMCNet's\npotential applicability to robotic tasks under a variety of real-world\nconditions.\n",
        "published": "2022-01-28",
        "authors": [
            "Yutong Wang",
            "Guillaume Sartoretti"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.10823v1",
        "title": "Long Short-Term Memory for Spatial Encoding in Multi-Agent Path Planning",
        "abstract": "  Reinforcement learning-based path planning for multi-agent systems of varying\nsize constitutes a research topic with increasing significance as progress in\ndomains such as urban air mobility and autonomous aerial vehicles continues.\nReinforcement learning with continuous state and action spaces is used to train\na policy network that accommodates desirable path planning behaviors and can be\nused for time-critical applications. A Long Short-Term Memory module is\nproposed to encode an unspecified number of states for a varying, indefinite\nnumber of agents. The described training strategies and policy architecture\nlead to a guidance that scales to an infinite number of agents and unlimited\nphysical dimensions, although training takes place at a smaller scale. The\nguidance is implemented on a low-cost, off-the-shelf onboard computer. The\nfeasibility of the proposed approach is validated by presenting flight test\nresults of up to four drones, autonomously navigating collision-free in a\nreal-world environment.\n",
        "published": "2022-03-21",
        "authors": [
            "Marc R. Schlichting",
            "Stefan Notter",
            "Walter Fichter"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.15925v3",
        "title": "Asynchronous, Option-Based Multi-Agent Policy Gradient: A Conditional\n  Reasoning Approach",
        "abstract": "  Cooperative multi-agent problems often require coordination between agents,\nwhich can be achieved through a centralized policy that considers the global\nstate. Multi-agent policy gradient (MAPG) methods are commonly used to learn\nsuch policies, but they are often limited to problems with low-level action\nspaces. In complex problems with large state and action spaces, it is\nadvantageous to extend MAPG methods to use higher-level actions, also known as\noptions, to improve the policy search efficiency. However, multi-robot option\nexecutions are often asynchronous, that is, agents may select and complete\ntheir options at different time steps. This makes it difficult for MAPG methods\nto derive a centralized policy and evaluate its gradient, as centralized policy\nalways select new options at the same time. In this work, we propose a novel,\nconditional reasoning approach to address this problem and demonstrate its\neffectiveness on representative option-based multi-agent cooperative tasks\nthrough empirical validation. Find code and videos at:\n\\href{https://sites.google.com/view/mahrlsupp/}{https://sites.google.com/view/mahrlsupp/}\n",
        "published": "2022-03-29",
        "authors": [
            "Xubo Lyu",
            "Amin Banitalebi-Dehkordi",
            "Mo Chen",
            "Yong Zhang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.03516v1",
        "title": "Distributed Reinforcement Learning for Robot Teams: A Review",
        "abstract": "  Purpose of review: Recent advances in sensing, actuation, and computation\nhave opened the door to multi-robot systems consisting of hundreds/thousands of\nrobots, with promising applications to automated manufacturing, disaster\nrelief, harvesting, last-mile delivery, port/airport operations, or search and\nrescue. The community has leveraged model-free multi-agent reinforcement\nlearning (MARL) to devise efficient, scalable controllers for multi-robot\nsystems (MRS). This review aims to provide an analysis of the state-of-the-art\nin distributed MARL for multi-robot cooperation.\n  Recent findings: Decentralized MRS face fundamental challenges, such as\nnon-stationarity and partial observability. Building upon the \"centralized\ntraining, decentralized execution\" paradigm, recent MARL approaches include\nindependent learning, centralized critic, value decomposition, and\ncommunication learning approaches. Cooperative behaviors are demonstrated\nthrough AI benchmarks and fundamental real-world robotic capabilities such as\nmulti-robot motion/path planning.\n  Summary: This survey reports the challenges surrounding decentralized\nmodel-free MARL for multi-robot cooperation and existing classes of approaches.\nWe present benchmarks and robotic applications along with a discussion on\ncurrent open avenues for research.\n",
        "published": "2022-04-07",
        "authors": [
            "Yutong Wang",
            "Mehul Damani",
            "Pamela Wang",
            "Yuhong Cao",
            "Guillaume Sartoretti"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.10759v1",
        "title": "The Boltzmann Policy Distribution: Accounting for Systematic\n  Suboptimality in Human Models",
        "abstract": "  Models of human behavior for prediction and collaboration tend to fall into\ntwo categories: ones that learn from large amounts of data via imitation\nlearning, and ones that assume human behavior to be noisily-optimal for some\nreward function. The former are very useful, but only when it is possible to\ngather a lot of human data in the target environment and distribution. The\nadvantage of the latter type, which includes Boltzmann rationality, is the\nability to make accurate predictions in new environments without extensive data\nwhen humans are actually close to optimal. However, these models fail when\nhumans exhibit systematic suboptimality, i.e. when their deviations from\noptimal behavior are not independent, but instead consistent over time. Our key\ninsight is that systematic suboptimality can be modeled by predicting policies,\nwhich couple action choices over time, instead of trajectories. We introduce\nthe Boltzmann policy distribution (BPD), which serves as a prior over human\npolicies and adapts via Bayesian inference to capture systematic deviations by\nobserving human actions during a single episode. The BPD is difficult to\ncompute and represent because policies lie in a high-dimensional continuous\nspace, but we leverage tools from generative and sequence models to enable\nefficient sampling and inference. We show that the BPD enables prediction of\nhuman behavior and human-AI collaboration equally as well as imitation\nlearning-based human models while using far less data.\n",
        "published": "2022-04-22",
        "authors": [
            "Cassidy Laidlaw",
            "Anca Dragan"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.00233v3",
        "title": "DM$^2$: Decentralized Multi-Agent Reinforcement Learning for\n  Distribution Matching",
        "abstract": "  Current approaches to multi-agent cooperation rely heavily on centralized\nmechanisms or explicit communication protocols to ensure convergence. This\npaper studies the problem of distributed multi-agent learning without resorting\nto centralized components or explicit communication. It examines the use of\ndistribution matching to facilitate the coordination of independent agents. In\nthe proposed scheme, each agent independently minimizes the distribution\nmismatch to the corresponding component of a target visitation distribution.\nThe theoretical analysis shows that under certain conditions, each agent\nminimizing its individual distribution mismatch allows the convergence to the\njoint policy that generated the target distribution. Further, if the target\ndistribution is from a joint policy that optimizes a cooperative task, the\noptimal policy for a combination of this task reward and the distribution\nmatching reward is the same joint policy. This insight is used to formulate a\npractical algorithm (DM$^2$), in which each individual agent matches a target\ndistribution derived from concurrently sampled trajectories from a joint expert\npolicy. Experimental validation on the StarCraft domain shows that combining\n(1) a task reward, and (2) a distribution matching reward for expert\ndemonstrations for the same task, allows agents to outperform a naive\ndistributed baseline. Additional experiments probe the conditions under which\nexpert demonstrations need to be sampled to obtain the learning benefits.\n",
        "published": "2022-06-01",
        "authors": [
            "Caroline Wang",
            "Ishan Durugkar",
            "Elad Liebman",
            "Peter Stone"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.08686v2",
        "title": "Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement\n  Learning",
        "abstract": "  Achieving human-level dexterity is an important open problem in robotics.\nHowever, tasks of dexterous hand manipulation, even at the baby level, are\nchallenging to solve through reinforcement learning (RL). The difficulty lies\nin the high degrees of freedom and the required cooperation among heterogeneous\nagents (e.g., joints of fingers). In this study, we propose the Bimanual\nDexterous Hands Benchmark (Bi-DexHands), a simulator that involves two\ndexterous hands with tens of bimanual manipulation tasks and thousands of\ntarget objects. Specifically, tasks in Bi-DexHands are designed to match\ndifferent levels of human motor skills according to cognitive science\nliterature. We built Bi-DexHands in the Issac Gym; this enables highly\nefficient RL training, reaching 30,000+ FPS by only one single NVIDIA RTX 3090.\nWe provide a comprehensive benchmark for popular RL algorithms under different\nsettings; this includes Single-agent/Multi-agent RL, Offline RL, Multi-task RL,\nand Meta RL. Our results show that the PPO type of on-policy algorithms can\nmaster simple manipulation tasks that are equivalent up to 48-month human\nbabies (e.g., catching a flying object, opening a bottle), while multi-agent RL\ncan further help to master manipulations that require skilled bimanual\ncooperation (e.g., lifting a pot, stacking blocks). Despite the success on each\nsingle task, when it comes to acquiring multiple manipulation skills, existing\nRL algorithms fail to work in most of the multi-task and the few-shot learning\nsettings, which calls for more substantial development from the RL community.\nOur project is open sourced at https://github.com/PKU-MARL/DexterousHands.\n",
        "published": "2022-06-17",
        "authors": [
            "Yuanpei Chen",
            "Tianhao Wu",
            "Shengjie Wang",
            "Xidong Feng",
            "Jiechuang Jiang",
            "Stephen Marcus McAleer",
            "Yiran Geng",
            "Hao Dong",
            "Zongqing Lu",
            "Song-Chun Zhu",
            "Yaodong Yang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.09889v3",
        "title": "Nocturne: a scalable driving benchmark for bringing multi-agent learning\n  one step closer to the real world",
        "abstract": "  We introduce Nocturne, a new 2D driving simulator for investigating\nmulti-agent coordination under partial observability. The focus of Nocturne is\nto enable research into inference and theory of mind in real-world multi-agent\nsettings without the computational overhead of computer vision and feature\nextraction from images. Agents in this simulator only observe an obstructed\nview of the scene, mimicking human visual sensing constraints. Unlike existing\nbenchmarks that are bottlenecked by rendering human-like observations directly\nusing a camera input, Nocturne uses efficient intersection methods to compute a\nvectorized set of visible features in a C++ back-end, allowing the simulator to\nrun at over 2000 steps-per-second. Using open-source trajectory and map data,\nwe construct a simulator to load and replay arbitrary trajectories and scenes\nfrom real-world driving data. Using this environment, we benchmark\nreinforcement-learning and imitation-learning agents and demonstrate that the\nagents are quite far from human-level coordination ability and deviate\nsignificantly from the expert trajectories.\n",
        "published": "2022-06-20",
        "authors": [
            "Eugene Vinitsky",
            "Nathan Lichtl\u00e9",
            "Xiaomeng Yang",
            "Brandon Amos",
            "Jakob Foerster"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.08347v1",
        "title": "Sub-optimal Policy Aided Multi-Agent Reinforcement Learning for Flocking\n  Control",
        "abstract": "  Flocking control is a challenging problem, where multiple agents, such as\ndrones or vehicles, need to reach a target position while maintaining the flock\nand avoiding collisions with obstacles and collisions among agents in the\nenvironment. Multi-agent reinforcement learning has achieved promising\nperformance in flocking control. However, methods based on traditional\nreinforcement learning require a considerable number of interactions between\nagents and the environment. This paper proposes a sub-optimal policy aided\nmulti-agent reinforcement learning algorithm (SPA-MARL) to boost sample\nefficiency. SPA-MARL directly leverages a prior policy that can be manually\ndesigned or solved with a non-learning method to aid agents in learning, where\nthe performance of the policy can be sub-optimal. SPA-MARL recognizes the\ndifference in performance between the sub-optimal policy and itself, and then\nimitates the sub-optimal policy if the sub-optimal policy is better. We\nleverage SPA-MARL to solve the flocking control problem. A traditional control\nmethod based on artificial potential fields is used to generate a sub-optimal\npolicy. Experiments demonstrate that SPA-MARL can speed up the training process\nand outperform both the MARL baseline and the used sub-optimal policy.\n",
        "published": "2022-09-17",
        "authors": [
            "Yunbo Qiu",
            "Yue Jin",
            "Jian Wang",
            "Xudong Zhang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.08351v1",
        "title": "Sample-Efficient Multi-Agent Reinforcement Learning with Demonstrations\n  for Flocking Control",
        "abstract": "  Flocking control is a significant problem in multi-agent systems such as\nmulti-agent unmanned aerial vehicles and multi-agent autonomous underwater\nvehicles, which enhances the cooperativity and safety of agents. In contrast to\ntraditional methods, multi-agent reinforcement learning (MARL) solves the\nproblem of flocking control more flexibly. However, methods based on MARL\nsuffer from sample inefficiency, since they require a huge number of\nexperiences to be collected from interactions between agents and the\nenvironment. We propose a novel method Pretraining with Demonstrations for MARL\n(PwD-MARL), which can utilize non-expert demonstrations collected in advance\nwith traditional methods to pretrain agents. During the process of pretraining,\nagents learn policies from demonstrations by MARL and behavior cloning\nsimultaneously, and are prevented from overfitting demonstrations. By\npretraining with non-expert demonstrations, PwD-MARL improves sample efficiency\nin the process of online MARL with a warm start. Experiments show that PwD-MARL\nimproves sample efficiency and policy performance in the problem of flocking\ncontrol, even with bad or few demonstrations.\n",
        "published": "2022-09-17",
        "authors": [
            "Yunbo Qiu",
            "Yuzhu Zhan",
            "Yue Jin",
            "Jian Wang",
            "Xudong Zhang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.10113v2",
        "title": "Asynchronous Actor-Critic for Multi-Agent Reinforcement Learning",
        "abstract": "  Synchronizing decisions across multiple agents in realistic settings is\nproblematic since it requires agents to wait for other agents to terminate and\ncommunicate about termination reliably. Ideally, agents should learn and\nexecute asynchronously instead. Such asynchronous methods also allow temporally\nextended actions that can take different amounts of time based on the situation\nand action executed. Unfortunately, current policy gradient methods are not\napplicable in asynchronous settings, as they assume that agents synchronously\nreason about action selection at every time step. To allow asynchronous\nlearning and decision-making, we formulate a set of asynchronous multi-agent\nactor-critic methods that allow agents to directly optimize asynchronous\npolicies in three standard training paradigms: decentralized learning,\ncentralized learning, and centralized training for decentralized execution.\nEmpirical results (in simulation and hardware) in a variety of realistic\ndomains demonstrate the superiority of our approaches in large multi-agent\nproblems and validate the effectiveness of our algorithms for learning\nhigh-quality and asynchronous solutions.\n",
        "published": "2022-09-20",
        "authors": [
            "Yuchen Xiao",
            "Weihao Tan",
            "Christopher Amato"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.03649v1",
        "title": "How to Enable Uncertainty Estimation in Proximal Policy Optimization",
        "abstract": "  While deep reinforcement learning (RL) agents have showcased strong results\nacross many domains, a major concern is their inherent opaqueness and the\nsafety of such systems in real-world use cases. To overcome these issues, we\nneed agents that can quantify their uncertainty and detect out-of-distribution\n(OOD) states. Existing uncertainty estimation techniques, like Monte-Carlo\nDropout or Deep Ensembles, have not seen widespread adoption in on-policy deep\nRL. We posit that this is due to two reasons: concepts like uncertainty and OOD\nstates are not well defined compared to supervised learning, especially for\non-policy RL methods. Secondly, available implementations and comparative\nstudies for uncertainty estimation methods in RL have been limited. To overcome\nthe first gap, we propose definitions of uncertainty and OOD for Actor-Critic\nRL algorithms, namely, proximal policy optimization (PPO), and present possible\napplicable measures. In particular, we discuss the concepts of value and policy\nuncertainty. The second point is addressed by implementing different\nuncertainty estimation methods and comparing them across a number of\nenvironments. The OOD detection performance is evaluated via a custom\nevaluation benchmark of in-distribution (ID) and OOD states for various RL\nenvironments. We identify a trade-off between reward and OOD detection\nperformance. To overcome this, we formulate a Pareto optimization problem in\nwhich we simultaneously optimize for reward and OOD detection performance. We\nshow experimentally that the recently proposed method of Masksembles strikes a\nfavourable balance among the survey methods, enabling high-quality uncertainty\nestimation and OOD detection while matching the performance of original RL\nagents.\n",
        "published": "2022-10-07",
        "authors": [
            "Eugene Bykovets",
            "Yannick Metz",
            "Mennatallah El-Assady",
            "Daniel A. Keim",
            "Joachim M. Buhmann"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.07882v3",
        "title": "Explainable Action Advising for Multi-Agent Reinforcement Learning",
        "abstract": "  Action advising is a knowledge transfer technique for reinforcement learning\nbased on the teacher-student paradigm. An expert teacher provides advice to a\nstudent during training in order to improve the student's sample efficiency and\npolicy performance. Such advice is commonly given in the form of state-action\npairs. However, it makes it difficult for the student to reason with and apply\nto novel states. We introduce Explainable Action Advising, in which the teacher\nprovides action advice as well as associated explanations indicating why the\naction was chosen. This allows the student to self-reflect on what it has\nlearned, enabling advice generalization and leading to improved sample\nefficiency and learning performance - even in environments where the teacher is\nsub-optimal. We empirically show that our framework is effective in both\nsingle-agent and multi-agent scenarios, yielding improved policy returns and\nconvergence rates when compared to state-of-the-art methods\n",
        "published": "2022-11-15",
        "authors": [
            "Yue Guo",
            "Joseph Campbell",
            "Simon Stepputtis",
            "Ruiyu Li",
            "Dana Hughes",
            "Fei Fang",
            "Katia Sycara"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.14983v2",
        "title": "Multiagent Reinforcement Learning for Autonomous Routing and Pickup\n  Problem with Adaptation to Variable Demand",
        "abstract": "  We derive a learning framework to generate routing/pickup policies for a\nfleet of autonomous vehicles tasked with servicing stochastically appearing\nrequests on a city map. We focus on policies that 1) give rise to coordination\namongst the vehicles, thereby reducing wait times for servicing requests, 2)\nare non-myopic, and consider a-priori potential future requests, 3) can adapt\nto changes in the underlying demand distribution. Specifically, we are\ninterested in policies that are adaptive to fluctuations of actual demand\nconditions in urban environments, such as on-peak vs. off-peak hours. We\nachieve this through a combination of (i) an online play algorithm that\nimproves the performance of an offline-trained policy, and (ii) an offline\napproximation scheme that allows for adapting to changes in the underlying\ndemand model. In particular, we achieve adaptivity of our learned policy to\ndifferent demand distributions by quantifying a region of validity using the\nq-valid radius of a Wasserstein Ambiguity Set. We propose a mechanism for\nswitching the originally trained offline approximation when the current demand\nis outside the original validity region. In this case, we propose to use an\noffline architecture, trained on a historical demand model that is closer to\nthe current demand in terms of Wasserstein distance. We learn routing and\npickup policies over real taxicab requests in San Francisco with high\nvariability between on-peak and off-peak hours, demonstrating the ability of\nour method to adapt to real fluctuation in demand distributions. Our numerical\nresults demonstrate that our method outperforms alternative rollout-based\nreinforcement learning schemes, as well as other classical methods from\noperations research.\n",
        "published": "2022-11-28",
        "authors": [
            "Daniel Garces",
            "Sushmita Bhattacharya",
            "Stephanie Gil",
            "Dimitri Bertsekas"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.15414v1",
        "title": "Dynamic Collaborative Multi-Agent Reinforcement Learning Communication\n  for Autonomous Drone Reforestation",
        "abstract": "  We approach autonomous drone-based reforestation with a collaborative\nmulti-agent reinforcement learning (MARL) setup. Agents can communicate as part\nof a dynamically changing network. We explore collaboration and communication\non the back of a high-impact problem. Forests are the main resource to control\nrising CO2 conditions. Unfortunately, the global forest volume is decreasing at\nan unprecedented rate. Many areas are too large and hard to traverse to plant\nnew trees. To efficiently cover as much area as possible, here we propose a\nGraph Neural Network (GNN) based communication mechanism that enables\ncollaboration. Agents can share location information on areas needing\nreforestation, which increases viewed area and planted tree count. We compare\nour proposed communication mechanism with a multi-agent baseline without the\nability to communicate. Results show how communication enables collaboration\nand increases collective performance, planting precision and the risk-taking\npropensity of individual agents.\n",
        "published": "2022-11-14",
        "authors": [
            "Philipp Dominic Siedler"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.08230v4",
        "title": "An Energy-aware and Fault-tolerant Deep Reinforcement Learning based\n  approach for Multi-agent Patrolling Problems",
        "abstract": "  Autonomous vehicles are suited for continuous area patrolling problems.\nHowever, finding an optimal patrolling strategy can be challenging for many\nreasons. Firstly, patrolling environments are often complex and can include\nunknown environmental factors, such as wind or landscape. Secondly, autonomous\nvehicles can have failures or hardware constraints, such as limited battery\nlife. Importantly, patrolling large areas often requires multiple agents that\nneed to collectively coordinate their actions. In this work, we consider these\nlimitations and propose an approach based on model-free, deep multi-agent\nreinforcement learning. In this approach, the agents are trained to patrol an\nenvironment with various unknown dynamics and factors. They can automatically\nrecharge themselves to support continuous collective patrolling. A distributed\nhomogeneous multi-agent architecture is proposed, where all patrolling agents\nexecute identical policies locally based on their local observations and shared\nlocation information. This architecture provides a patrolling system that can\ntolerate agent failures and allow supplementary agents to be added to replace\nfailed agents or to increase the overall patrol performance. The solution is\nvalidated through simulation experiments from multiple perspectives, including\nthe overall patrol performance, the efficiency of battery recharging\nstrategies, the overall fault tolerance, and the ability to cooperate with\nsupplementary agents.\n",
        "published": "2022-12-16",
        "authors": [
            "Chenhao Tong",
            "Aaron Harwood",
            "Maria A. Rodriguez",
            "Richard O. Sinnott"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.11498v2",
        "title": "Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with\n  Robotic and Human Co-Workers",
        "abstract": "  We envision a warehouse in which dozens of mobile robots and human pickers\nwork together to collect and deliver items within the warehouse. The\nfundamental problem we tackle, called the order-picking problem, is how these\nworker agents must coordinate their movement and actions in the warehouse to\nmaximise performance (e.g. order throughput). Established industry methods\nusing heuristic approaches require large engineering efforts to optimise for\ninnately variable warehouse configurations. In contrast, multi-agent\nreinforcement learning (MARL) can be flexibly applied to diverse warehouse\nconfigurations (e.g. size, layout, number/types of workers, item replenishment\nfrequency), as the agents learn through experience how to optimally cooperate\nwith one another. We develop hierarchical MARL algorithms in which a manager\nassigns goals to worker agents, and the policies of the manager and workers are\nco-trained toward maximising a global objective (e.g. pick rate). Our\nhierarchical algorithms achieve significant gains in sample efficiency and\noverall pick rates over baseline MARL algorithms in diverse warehouse\nconfigurations, and substantially outperform two established industry\nheuristics for order-picking systems.\n",
        "published": "2022-12-22",
        "authors": [
            "Aleksandar Krnjaic",
            "Raul D. Steleac",
            "Jonathan D. Thomas",
            "Georgios Papoudakis",
            "Lukas Sch\u00e4fer",
            "Andrew Wing Keung To",
            "Kuan-Ho Lao",
            "Murat Cubuktepe",
            "Matthew Haley",
            "Peter B\u00f6rsting",
            "Stefano V. Albrecht"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.01250v1",
        "title": "Decentralized cooperative perception for autonomous vehicles: Learning\n  to value the unknown",
        "abstract": "  Recently, we have been witnesses of accidents involving autonomous vehicles\nand their lack of sufficient information. One way to tackle this issue is to\nbenefit from the perception of different view points, namely cooperative\nperception. We propose here a decentralized collaboration, i.e. peer-to-peer,\nin which the agents are active in their quest for full perception by asking for\nspecific areas in their surroundings on which they would like to know more.\nUltimately, we want to optimize a trade-off between the maximization of\nknowledge about moving objects and the minimization of the total volume of\ninformation received from others, to limit communication costs and message\nprocessing time. For this, we propose a way to learn a communication policy\nthat reverses the usual communication paradigm by only requesting from other\nvehicles what is unknown to the ego-vehicle, instead of filtering on the sender\nside. We tested three different generative models to be taken as base for a\nDeep Reinforcement Learning (DRL) algorithm, and compared them to a\nbroadcasting policy and a policy randomly selecting areas. In particular, we\npropose Locally Predictable VAE (LP-VAE), which appears to be producing better\nbelief states for predictions than state-of-the-art models, both as a\nstandalone model and in the context of DRL. Experiments were conducted in the\ndriving simulator CARLA. Our best models reached on average a gain of 25% of\nthe total complementary information, while only requesting about 5% of the\nego-vehicle's perceptual field. This trade-off is adjustable through the\ninterpretable hyperparameters of our reward function.\n",
        "published": "2022-12-12",
        "authors": [
            "Maxime Chaveroche",
            "Franck Davoine",
            "V\u00e9ronique Cherfaoui"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.05223v1",
        "title": "NOPA: Neurally-guided Online Probabilistic Assistance for Building\n  Socially Intelligent Home Assistants",
        "abstract": "  In this work, we study how to build socially intelligent robots to assist\npeople in their homes. In particular, we focus on assistance with online goal\ninference, where robots must simultaneously infer humans' goals and how to help\nthem achieve those goals. Prior assistance methods either lack the adaptivity\nto adjust helping strategies (i.e., when and how to help) in response to\nuncertainty about goals or the scalability to conduct fast inference in a large\ngoal space. Our NOPA (Neurally-guided Online Probabilistic Assistance) method\naddresses both of these challenges. NOPA consists of (1) an online goal\ninference module combining neural goal proposals with inverse planning and\nparticle filtering for robust inference under uncertainty, and (2) a helping\nplanner that discovers valuable subgoals to help with and is aware of the\nuncertainty in goal inference. We compare NOPA against multiple baselines in a\nnew embodied AI assistance challenge: Online Watch-And-Help, in which a helper\nagent needs to simultaneously watch a main agent's action, infer its goal, and\nhelp perform a common household task faster in realistic virtual home\nenvironments. Experiments show that our helper agent robustly updates its goal\ninference and adapts its helping plans to the changing level of uncertainty.\n",
        "published": "2023-01-12",
        "authors": [
            "Xavier Puig",
            "Tianmin Shu",
            "Joshua B. Tenenbaum",
            "Antonio Torralba"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.07137v1",
        "title": "Heterogeneous Multi-Robot Reinforcement Learning",
        "abstract": "  Cooperative multi-robot tasks can benefit from heterogeneity in the robots'\nphysical and behavioral traits. In spite of this, traditional Multi-Agent\nReinforcement Learning (MARL) frameworks lack the ability to explicitly\naccommodate policy heterogeneity, and typically constrain agents to share\nneural network parameters. This enforced homogeneity limits application in\ncases where the tasks benefit from heterogeneous behaviors. In this paper, we\ncrystallize the role of heterogeneity in MARL policies. Towards this end, we\nintroduce Heterogeneous Graph Neural Network Proximal Policy Optimization\n(HetGPPO), a paradigm for training heterogeneous MARL policies that leverages a\nGraph Neural Network for differentiable inter-agent communication. HetGPPO\nallows communicating agents to learn heterogeneous behaviors while enabling\nfully decentralized training in partially observable environments. We\ncomplement this with a taxonomical overview that exposes more heterogeneity\nclasses than previously identified. To motivate the need for our model, we\npresent a characterization of techniques that homogeneous models can leverage\nto emulate heterogeneous behavior, and show how this \"apparent heterogeneity\"\nis brittle in real-world conditions. Through simulations and real-world\nexperiments, we show that: (i) when homogeneous methods fail due to strong\nheterogeneous requirements, HetGPPO succeeds, and, (ii) when homogeneous\nmethods are able to learn apparently heterogeneous behaviors, HetGPPO achieves\nhigher resilience to both training and deployment noise.\n",
        "published": "2023-01-17",
        "authors": [
            "Matteo Bettini",
            "Ajay Shankar",
            "Amanda Prorok"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.08451v1",
        "title": "Accelerating Multi-Agent Planning Using Graph Transformers with Bounded\n  Suboptimality",
        "abstract": "  Conflict-Based Search is one of the most popular methods for multi-agent path\nfinding. Though it is complete and optimal, it does not scale well. Recent\nworks have been proposed to accelerate it by introducing various heuristics.\nHowever, whether these heuristics can apply to non-grid-based problem settings\nwhile maintaining their effectiveness remains an open question. In this work,\nwe find that the answer is prone to be no. To this end, we propose a\nlearning-based component, i.e., the Graph Transformer, as a heuristic function\nto accelerate the planning. The proposed method is provably complete and\nbounded-suboptimal with any desired factor. We conduct extensive experiments on\ntwo environments with dense graphs. Results show that the proposed Graph\nTransformer can be trained in problem instances with relatively few agents and\ngeneralizes well to a larger number of agents, while achieving better\nperformance than state-of-the-art methods.\n",
        "published": "2023-01-20",
        "authors": [
            "Chenning Yu",
            "Qingbiao Li",
            "Sicun Gao",
            "Amanda Prorok"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.13279v1",
        "title": "Learning Coordination Policies over Heterogeneous Graphs for Human-Robot\n  Teams via Recurrent Neural Schedule Propagation",
        "abstract": "  As human-robot collaboration increases in the workforce, it becomes essential\nfor human-robot teams to coordinate efficiently and intuitively. Traditional\napproaches for human-robot scheduling either utilize exact methods that are\nintractable for large-scale problems and struggle to account for stochastic,\ntime varying human task performance, or application-specific heuristics that\nrequire expert domain knowledge to develop. We propose a deep learning-based\nframework, called HybridNet, combining a heterogeneous graph-based encoder with\na recurrent schedule propagator for scheduling stochastic human-robot teams\nunder upper- and lower-bound temporal constraints. The HybridNet's encoder\nleverages Heterogeneous Graph Attention Networks to model the initial\nenvironment and team dynamics while accounting for the constraints. By\nformulating task scheduling as a sequential decision-making process, the\nHybridNet's recurrent neural schedule propagator leverages Long Short-Term\nMemory (LSTM) models to propagate forward consequences of actions to carry out\nfast schedule generation, removing the need to interact with the environment\nbetween every task-agent pair selection. The resulting scheduling policy\nnetwork provides a computationally lightweight yet highly expressive model that\nis end-to-end trainable via Reinforcement Learning algorithms. We develop a\nvirtual task scheduling environment for mixed human-robot teams in a\nmulti-round setting, capable of modeling the stochastic learning behaviors of\nhuman workers. Experimental results showed that HybridNet outperformed other\nhuman-robot scheduling solutions across problem sizes for both deterministic\nand stochastic human performance, with faster runtime compared to\npure-GNN-based schedulers.\n",
        "published": "2023-01-30",
        "authors": [
            "Batuhan Altundas",
            "Zheyuan Wang",
            "Joshua Bishop",
            "Matthew Gombolay"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.09277v1",
        "title": "Promoting Cooperation in Multi-Agent Reinforcement Learning via Mutual\n  Help",
        "abstract": "  Multi-agent reinforcement learning (MARL) has achieved great progress in\ncooperative tasks in recent years. However, in the local reward scheme, where\nonly local rewards for each agent are given without global rewards shared by\nall the agents, traditional MARL algorithms lack sufficient consideration of\nagents' mutual influence. In cooperative tasks, agents' mutual influence is\nespecially important since agents are supposed to coordinate to achieve better\nperformance. In this paper, we propose a novel algorithm Mutual-Help-based MARL\n(MH-MARL) to instruct agents to help each other in order to promote\ncooperation. MH-MARL utilizes an expected action module to generate expected\nother agents' actions for each particular agent. Then, the expected actions are\ndelivered to other agents for selective imitation during training. Experimental\nresults show that MH-MARL improves the performance of MARL both in success rate\nand cumulative reward.\n",
        "published": "2023-02-18",
        "authors": [
            "Yunbo Qiu",
            "Yue Jin",
            "Lebin Yu",
            "Jian Wang",
            "Xudong Zhang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.01428v1",
        "title": "Learned Tree Search for Long-Horizon Social Robot Navigation in Shared\n  Airspace",
        "abstract": "  The fast-growing demand for fully autonomous aerial operations in shared\nspaces necessitates developing trustworthy agents that can safely and\nseamlessly navigate in crowded, dynamic spaces. In this work, we propose Social\nRobot Tree Search (SoRTS), an algorithm for the safe navigation of mobile\nrobots in social domains. SoRTS aims to augment existing socially-aware\ntrajectory prediction policies with a Monte Carlo Tree Search planner for\nimproved downstream navigation of mobile robots. To evaluate the performance of\nour method, we choose the use case of social navigation for general aviation.\nTo aid this evaluation, within this work, we also introduce X-PlaneROS, a\nhigh-fidelity aerial simulator, to enable more research in full-scale aerial\nautonomy. By conducting a user study based on the assessments of 26 FAA\ncertified pilots, we show that SoRTS performs comparably to a competent human\npilot, significantly outperforming our baseline algorithm. We further\ncomplement these results with self-play experiments in scenarios with\nincreasing complexity.\n",
        "published": "2023-04-04",
        "authors": [
            "Ingrid Navarro",
            "Jay Patrikar",
            "Joao P. A. Dantas",
            "Rohan Baijal",
            "Ian Higgins",
            "Sebastian Scherer",
            "Jean Oh"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.06281v1",
        "title": "Model-based Dynamic Shielding for Safe and Efficient Multi-Agent\n  Reinforcement Learning",
        "abstract": "  Multi-Agent Reinforcement Learning (MARL) discovers policies that maximize\nreward but do not have safety guarantees during the learning and deployment\nphases. Although shielding with Linear Temporal Logic (LTL) is a promising\nformal method to ensure safety in single-agent Reinforcement Learning (RL), it\nresults in conservative behaviors when scaling to multi-agent scenarios.\nAdditionally, it poses computational challenges for synthesizing shields in\ncomplex multi-agent environments. This work introduces Model-based Dynamic\nShielding (MBDS) to support MARL algorithm design. Our algorithm synthesizes\ndistributive shields, which are reactive systems running in parallel with each\nMARL agent, to monitor and rectify unsafe behaviors. The shields can\ndynamically split, merge, and recompute based on agents' states. This design\nenables efficient synthesis of shields to monitor agents in complex\nenvironments without coordination overheads. We also propose an algorithm to\nsynthesize shields without prior knowledge of the dynamics model. The proposed\nalgorithm obtains an approximate world model by interacting with the\nenvironment during the early stage of exploration, making our MBDS enjoy formal\nsafety guarantees with high probability. We demonstrate in simulations that our\nframework can surpass existing baselines in terms of safety guarantees and\nlearning performance.\n",
        "published": "2023-04-13",
        "authors": [
            "Wenli Xiao",
            "Yiwei Lyu",
            "John Dolan"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.08493v1",
        "title": "Coordinated Multi-Agent Reinforcement Learning for Unmanned Aerial\n  Vehicle Swarms in Autonomous Mobile Access Applications",
        "abstract": "  This paper proposes a novel centralized training and distributed execution\n(CTDE)-based multi-agent deep reinforcement learning (MADRL) method for\nmultiple unmanned aerial vehicles (UAVs) control in autonomous mobile access\napplications. For the purpose, a single neural network is utilized in\ncentralized training for cooperation among multiple agents while maximizing the\ntotal quality of service (QoS) in mobile access applications.\n",
        "published": "2022-12-23",
        "authors": [
            "Chanyoung Park",
            "Haemin Lee",
            "Won Joon Yun",
            "Soyi Jung",
            "Joongheon Kim"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.14102v2",
        "title": "SocNavGym: A Reinforcement Learning Gym for Social Navigation",
        "abstract": "  It is essential for autonomous robots to be socially compliant while\nnavigating in human-populated environments. Machine Learning and, especially,\nDeep Reinforcement Learning have recently gained considerable traction in the\nfield of Social Navigation. This can be partially attributed to the resulting\npolicies not being bound by human limitations in terms of code complexity or\nthe number of variables that are handled. Unfortunately, the lack of safety\nguarantees and the large data requirements by DRL algorithms make learning in\nthe real world unfeasible. To bridge this gap, simulation environments are\nfrequently used. We propose SocNavGym, an advanced simulation environment for\nsocial navigation that can generate a wide variety of social navigation\nscenarios and facilitates the development of intelligent social agents.\nSocNavGym is light-weight, fast, easy-to-use, and can be effortlessly\nconfigured to generate different types of social navigation scenarios. It can\nalso be configured to work with different hand-crafted and data-driven social\nreward signals and to yield a variety of evaluation metrics to benchmark\nagents' performance. Further, we also provide a case study where a Dueling-DQN\nagent is trained to learn social-navigation policies using SocNavGym. The\nresults provides evidence that SocNavGym can be used to train an agent from\nscratch to navigate in simple as well as complex social scenarios. Our\nexperiments also show that the agents trained using the data-driven reward\nfunction displays more advanced social compliance in comparison to the\nheuristic-based reward function.\n",
        "published": "2023-04-27",
        "authors": [
            "Aditya Kapoor",
            "Sushant Swamy",
            "Luis Manso",
            "Pilar Bachiller"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.02128v1",
        "title": "System Neural Diversity: Measuring Behavioral Heterogeneity in\n  Multi-Agent Learning",
        "abstract": "  Evolutionary science provides evidence that diversity confers resilience.\nYet, traditional multi-agent reinforcement learning techniques commonly enforce\nhomogeneity to increase training sample efficiency. When a system of learning\nagents is not constrained to homogeneous policies, individual agents may\ndevelop diverse behaviors, resulting in emergent complementarity that benefits\nthe system. Despite this feat, there is a surprising lack of tools that measure\nbehavioral diversity in systems of learning agents. Such techniques would pave\nthe way towards understanding the impact of diversity in collective resilience\nand performance. In this paper, we introduce System Neural Diversity (SND): a\nmeasure of behavioral heterogeneity for multi-agent systems where agents have\nstochastic policies. %over a continuous state space. We discuss and prove its\ntheoretical properties, and compare it with alternate, state-of-the-art\nbehavioral diversity metrics used in cross-disciplinary domains. Through\nsimulations of a variety of multi-agent tasks, we show how our metric\nconstitutes an important diagnostic tool to analyze latent properties of\nbehavioral heterogeneity. By comparing SND with task reward in static tasks,\nwhere the problem does not change during training, we show that it is key to\nunderstanding the effectiveness of heterogeneous vs homogeneous agents. In\ndynamic tasks, where the problem is affected by repeated disturbances during\ntraining, we show that heterogeneous agents are first able to learn specialized\nroles that allow them to cope with the disturbance, and then retain these roles\nwhen the disturbance is removed. SND allows a direct measurement of this latent\nresilience, while other proxies such as task performance (reward) fail to.\n",
        "published": "2023-05-03",
        "authors": [
            "Matteo Bettini",
            "Ajay Shankar",
            "Amanda Prorok"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.08870v1",
        "title": "Evolutionary Curriculum Training for DRL-Based Navigation Systems",
        "abstract": "  In recent years, Deep Reinforcement Learning (DRL) has emerged as a promising\nmethod for robot collision avoidance. However, such DRL models often come with\nlimitations, such as adapting effectively to structured environments containing\nvarious pedestrians. In order to solve this difficulty, previous research has\nattempted a few approaches, including training an end-to-end solution by\nintegrating a waypoint planner with DRL and developing a multimodal solution to\nmitigate the drawbacks of the DRL model. However, these approaches have\nencountered several issues, including slow training times, scalability\nchallenges, and poor coordination among different models. To address these\nchallenges, this paper introduces a novel approach called evolutionary\ncurriculum training to tackle these challenges. The primary goal of\nevolutionary curriculum training is to evaluate the collision avoidance model's\ncompetency in various scenarios and create curricula to enhance its\ninsufficient skills. The paper introduces an innovative evaluation technique to\nassess the DRL model's performance in navigating structured maps and avoiding\ndynamic obstacles. Additionally, an evolutionary training environment generates\nall the curriculum to improve the DRL model's inadequate skills tested in the\nprevious evaluation. We benchmark the performance of our model across five\nstructured environments to validate the hypothesis that this evolutionary\ntraining environment leads to a higher success rate and a lower average number\nof collisions. Further details and results at our project website.\n",
        "published": "2023-06-15",
        "authors": [
            "Max Asselmeier",
            "Zhaoyi Li",
            "Kelin Yu",
            "Danfei Xu"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.12926v2",
        "title": "Decentralized Multi-Agent Reinforcement Learning with Global State\n  Prediction",
        "abstract": "  Deep reinforcement learning (DRL) has seen remarkable success in the control\nof single robots. However, applying DRL to robot swarms presents significant\nchallenges. A critical challenge is non-stationarity, which occurs when two or\nmore robots update individual or shared policies concurrently, thereby engaging\nin an interdependent training process with no guarantees of convergence.\nCircumventing non-stationarity typically involves training the robots with\nglobal information about other agents' states and/or actions. In contrast, in\nthis paper we explore how to remove the need for global information. We pose\nour problem as a Partially Observable Markov Decision Process, due to the\nabsence of global knowledge on other agents. Using collective transport as a\ntestbed scenario, we study two approaches to multi-agent training. In the\nfirst, the robots exchange no messages, and are trained to rely on implicit\ncommunication through push-and-pull on the object to transport. In the second\napproach, we introduce Global State Prediction (GSP), a network trained to\nforma a belief over the swarm as a whole and predict its future states. We\nprovide a comprehensive study over four well-known deep reinforcement learning\nalgorithms in environments with obstacles, measuring performance as the\nsuccessful transport of the object to the goal within a desired time-frame.\nThrough an ablation study, we show that including GSP boosts performance and\nincreases robustness when compared with methods that use global knowledge.\n",
        "published": "2023-06-22",
        "authors": [
            "Joshua Bloom",
            "Pranjal Paliwal",
            "Apratim Mukherjee",
            "Carlo Pinciroli"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.16186v2",
        "title": "ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning",
        "abstract": "  Multi-agent reinforcement learning (MARL) has achieved promising results in\nrecent years. However, most existing reinforcement learning methods require a\nlarge amount of data for model training. In addition, data-efficient\nreinforcement learning requires the construction of strong inductive biases,\nwhich are ignored in the current MARL approaches. Inspired by the symmetry\nphenomenon in multi-agent systems, this paper proposes a framework for\nexploiting prior knowledge by integrating data augmentation and a well-designed\nconsistency loss into the existing MARL methods. In addition, the proposed\nframework is model-agnostic and can be applied to most of the current MARL\nalgorithms. Experimental tests on multiple challenging tasks demonstrate the\neffectiveness of the proposed framework. Moreover, the proposed framework is\napplied to a physical multi-robot testbed to show its superiority.\n",
        "published": "2023-07-30",
        "authors": [
            "Xin Yu",
            "Rongye Shi",
            "Pu Feng",
            "Yongkai Tian",
            "Jie Luo",
            "Wenjun Wu"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.09075v1",
        "title": "Fast Decision Support for Air Traffic Management at Urban Air Mobility\n  Vertiports using Graph Learning",
        "abstract": "  Urban Air Mobility (UAM) promises a new dimension to decongested, safe, and\nfast travel in urban and suburban hubs. These UAM aircraft are conceived to\noperate from small airports called vertiports each comprising multiple\ntake-off/landing and battery-recharging spots. Since they might be situated in\ndense urban areas and need to handle many aircraft landings and take-offs each\nhour, managing this schedule in real-time becomes challenging for a traditional\nair-traffic controller but instead calls for an automated solution. This paper\nprovides a novel approach to this problem of Urban Air Mobility - Vertiport\nSchedule Management (UAM-VSM), which leverages graph reinforcement learning to\ngenerate decision-support policies. Here the designated physical spots within\nthe vertiport's airspace and the vehicles being managed are represented as two\nseparate graphs, with feature extraction performed through a graph\nconvolutional network (GCN). Extracted features are passed onto perceptron\nlayers to decide actions such as continue to hover or cruise, continue idling\nor take-off, or land on an allocated vertiport spot. Performance is measured\nbased on delays, safety (no. of collisions) and battery consumption. Through\nrealistic simulations in AirSim applied to scaled down multi-rotor vehicles,\nour results demonstrate the suitability of using graph reinforcement learning\nto solve the UAM-VSM problem and its superiority to basic reinforcement\nlearning (with graph embeddings) or random choice baselines.\n",
        "published": "2023-08-17",
        "authors": [
            "Prajit KrisshnaKumar",
            "Jhoel Witter",
            "Steve Paul",
            "Hanvit Cho",
            "Karthik Dantu",
            "Souma Chowdhury"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.11071v1",
        "title": "Neural Amortized Inference for Nested Multi-agent Reasoning",
        "abstract": "  Multi-agent interactions, such as communication, teaching, and bluffing,\noften rely on higher-order social inference, i.e., understanding how others\ninfer oneself. Such intricate reasoning can be effectively modeled through\nnested multi-agent reasoning. Nonetheless, the computational complexity\nescalates exponentially with each level of reasoning, posing a significant\nchallenge. However, humans effortlessly perform complex social inferences as\npart of their daily lives. To bridge the gap between human-like inference\ncapabilities and computational limitations, we propose a novel approach:\nleveraging neural networks to amortize high-order social inference, thereby\nexpediting nested multi-agent reasoning. We evaluate our method in two\nchallenging multi-agent interaction domains. The experimental results\ndemonstrate that our method is computationally efficient while exhibiting\nminimal degradation in accuracy.\n",
        "published": "2023-08-21",
        "authors": [
            "Kunal Jha",
            "Tuan Anh Le",
            "Chuanyang Jin",
            "Yen-Ling Kuo",
            "Joshua B. Tenenbaum",
            "Tianmin Shu"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.10007v2",
        "title": "Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive\n  Autonomous Vehicles using AutoDRIVE Ecosystem",
        "abstract": "  This work presents a modular and parallelizable multi-agent deep\nreinforcement learning framework for imbibing cooperative as well as\ncompetitive behaviors within autonomous vehicles. We introduce AutoDRIVE\nEcosystem as an enabler to develop physically accurate and graphically\nrealistic digital twins of Nigel and F1TENTH, two scaled autonomous vehicle\nplatforms with unique qualities and capabilities, and leverage this ecosystem\nto train and deploy multi-agent reinforcement learning policies. We first\ninvestigate an intersection traversal problem using a set of cooperative\nvehicles (Nigel) that share limited state information with each other in single\nas well as multi-agent learning settings using a common policy approach. We\nthen investigate an adversarial head-to-head autonomous racing problem using a\ndifferent set of vehicles (F1TENTH) in a multi-agent learning setting using an\nindividual policy approach. In either set of experiments, a decentralized\nlearning architecture was adopted, which allowed robust training and testing of\nthe approaches in stochastic environments, since the agents were mutually\nindependent and exhibited asynchronous motion behavior. The problems were\nfurther aggravated by providing the agents with sparse observation spaces and\nrequiring them to sample control commands that implicitly satisfied the imposed\nkinodynamic as well as safety constraints. The experimental results for both\nproblem statements are reported in terms of quantitative metrics and\nqualitative remarks for training as well as deployment phases.\n",
        "published": "2023-09-18",
        "authors": [
            "Tanmay Vilas Samak",
            "Chinmay Vilas Samak",
            "Venkat Krovi"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    }
]