[
    {
        "id": "http://arxiv.org/abs/2004.06838v1",
        "title": "Composite Travel Generative Adversarial Networks for Tabular and\n  Sequential Population Synthesis",
        "abstract": "  Agent-based transportation modelling has become the standard to simulate\ntravel behaviour, mobility choices and activity preferences using disaggregate\ntravel demand data for entire populations, data that are not typically readily\navailable. Various methods have been proposed to synthesize population data for\nthis purpose. We present a Composite Travel Generative Adversarial Network\n(CTGAN), a novel deep generative model to estimate the underlying joint\ndistribution of a population, that is capable of reconstructing composite\nsynthetic agents having tabular (e.g. age and sex) as well as sequential\nmobility data (e.g. trip trajectory and sequence). The CTGAN model is compared\nwith other recently proposed methods such as the Variational Autoencoders (VAE)\nmethod, which has shown success in high dimensional tabular population\nsynthesis. We evaluate the performance of the synthesized outputs based on\ndistribution similarity, multi-variate correlations and spatio-temporal\nmetrics. The results show the consistent and accurate generation of synthetic\npopulations and their tabular and spatially sequential attributes, generated\nover varying spatial scales and dimensions.\n",
        "published": "2020",
        "authors": [
            "Godwin Badu-Marfo",
            "Bilal Farooq",
            "Zachary Paterson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.08883v4",
        "title": "Variational Policy Propagation for Multi-agent Reinforcement Learning",
        "abstract": "  We propose a \\emph{collaborative} multi-agent reinforcement learning\nalgorithm named variational policy propagation (VPP) to learn a \\emph{joint}\npolicy through the interactions over agents. We prove that the joint policy is\na Markov Random Field under some mild conditions, which in turn reduces the\npolicy space effectively. We integrate the variational inference as special\ndifferentiable layers in policy such that the actions can be efficiently\nsampled from the Markov Random Field and the overall policy is differentiable.\nWe evaluate our algorithm on several large scale challenging tasks and\ndemonstrate that it outperforms previous state-of-the-arts.\n",
        "published": "2020",
        "authors": [
            "Chao Qu",
            "Hui Li",
            "Chang Liu",
            "Junwu Xiong",
            "James Zhang",
            "Wei Chu",
            "Weiqiang Wang",
            "Yuan Qi",
            "Le Song"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.07902v4",
        "title": "Performance Limits of Stochastic Sub-Gradient Learning, Part I: Single\n  Agent Case",
        "abstract": "  In this work and the supporting Part II, we examine the performance of\nstochastic sub-gradient learning strategies under weaker conditions than\nusually considered in the literature. The new conditions are shown to be\nautomatically satisfied by several important cases of interest including SVM,\nLASSO, and Total-Variation denoising formulations. In comparison, these\nproblems do not satisfy the traditional assumptions used in prior analyses and,\ntherefore, conclusions derived from these earlier treatments are not directly\napplicable to these problems. The results in this article establish that\nstochastic sub-gradient strategies can attain linear convergence rates, as\nopposed to sub-linear rates, to the steady-state regime. A realizable\nexponential-weighting procedure is employed to smooth the intermediate iterates\nand guarantee useful performance bounds in terms of convergence rate and\nexcessive risk performance. Part I of this work focuses on single-agent\nscenarios, which are common in stand-alone learning applications, while Part II\nextends the analysis to networked learners. The theoretical conclusions are\nillustrated by several examples and simulations, including comparisons with the\nFISTA procedure.\n",
        "published": "2015",
        "authors": [
            "Bicheng Ying",
            "Ali H. Sayed"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.03053v1",
        "title": "A Communication-Efficient Multi-Agent Actor-Critic Algorithm for\n  Distributed Reinforcement Learning",
        "abstract": "  This paper considers a distributed reinforcement learning problem in which a\nnetwork of multiple agents aim to cooperatively maximize the globally averaged\nreturn through communication with only local neighbors. A randomized\ncommunication-efficient multi-agent actor-critic algorithm is proposed for\npossibly unidirectional communication relationships depicted by a directed\ngraph. It is shown that the algorithm can solve the problem for strongly\nconnected graphs by allowing each agent to transmit only two scalar-valued\nvariables at one time.\n",
        "published": "2019",
        "authors": [
            "Yixuan Lin",
            "Kaiqing Zhang",
            "Zhuoran Yang",
            "Zhaoran Wang",
            "Tamer Ba\u015far",
            "Romeil Sandhu",
            "Ji Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.06536v2",
        "title": "Federated Reinforcement Distillation with Proxy Experience Memory",
        "abstract": "  In distributed reinforcement learning, it is common to exchange the\nexperience memory of each agent and thereby collectively train their local\nmodels. The experience memory, however, contains all the preceding state\nobservations and their corresponding policies of the host agent, which may\nviolate the privacy of the agent. To avoid this problem, in this work, we\npropose a privacy-preserving distributed reinforcement learning (RL) framework,\ntermed federated reinforcement distillation (FRD). The key idea is to exchange\na proxy experience memory comprising a pre-arranged set of states and\ntime-averaged policies, thereby preserving the privacy of actual experiences.\nBased on an advantage actor-critic RL architecture, we numerically evaluate the\neffectiveness of FRD and investigate how the performance of FRD is affected by\nthe proxy memory structure and different memory exchanging rules.\n",
        "published": "2019",
        "authors": [
            "Han Cha",
            "Jihong Park",
            "Hyesung Kim",
            "Seong-Lyun Kim",
            "Mehdi Bennis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.07847v3",
        "title": "Prioritized Guidance for Efficient Multi-Agent Reinforcement Learning\n  Exploration",
        "abstract": "  Exploration efficiency is a challenging problem in multi-agent reinforcement\nlearning (MARL), as the policy learned by confederate MARL depends on the\ncollaborative approach among multiple agents. Another important problem is the\nless informative reward restricts the learning speed of MARL compared with the\ninformative label in supervised learning. In this work, we leverage on a novel\ncommunication method to guide MARL to accelerate exploration and propose a\npredictive network to forecast the reward of current state-action pair and use\nthe guidance learned by the predictive network to modify the reward function.\nAn improved prioritized experience replay is employed to better take advantage\nof the different knowledge learned by different agents which utilizes\nTime-difference (TD) error more effectively. Experimental results demonstrates\nthat the proposed algorithm outperforms existing methods in cooperative\nmulti-agent environments. We remark that this algorithm can be extended to\nsupervised learning to speed up its training.\n",
        "published": "2019",
        "authors": [
            "Qisheng Wang",
            "Qichao Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.10827v1",
        "title": "Terminal Prediction as an Auxiliary Task for Deep Reinforcement Learning",
        "abstract": "  Deep reinforcement learning has achieved great successes in recent years, but\nthere are still open challenges, such as convergence to locally optimal\npolicies and sample inefficiency. In this paper, we contribute a novel\nself-supervised auxiliary task, i.e., Terminal Prediction (TP), estimating\ntemporal closeness to terminal states for episodic tasks. The intuition is to\nhelp representation learning by letting the agent predict how close it is to a\nterminal state, while learning its control policy. Although TP could be\nintegrated with multiple algorithms, this paper focuses on Asynchronous\nAdvantage Actor-Critic (A3C) and demonstrating the advantages of A3C-TP. Our\nextensive evaluation includes: a set of Atari games, the BipedalWalker domain,\nand a mini version of the recently proposed multi-agent Pommerman game. Our\nresults on Atari games and the BipedalWalker domain suggest that A3C-TP\noutperforms standard A3C in most of the tested domains and in others it has\nsimilar performance. In Pommerman, our proposed method provides significant\nimprovement both in learning efficiency and converging to better policies\nagainst different opponents.\n",
        "published": "2019",
        "authors": [
            "Bilal Kartal",
            "Pablo Hernandez-Leal",
            "Matthew E. Taylor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.11703v1",
        "title": "Action Guidance with MCTS for Deep Reinforcement Learning",
        "abstract": "  Deep reinforcement learning has achieved great successes in recent years,\nhowever, one main challenge is the sample inefficiency. In this paper, we focus\non how to use action guidance by means of a non-expert demonstrator to improve\nsample efficiency in a domain with sparse, delayed, and possibly deceptive\nrewards: the recently-proposed multi-agent benchmark of Pommerman. We propose a\nnew framework where even a non-expert simulated demonstrator, e.g., planning\nalgorithms such as Monte Carlo tree search with a small number rollouts, can be\nintegrated within asynchronous distributed deep reinforcement learning methods.\nCompared to a vanilla deep RL algorithm, our proposed methods both learn faster\nand converge to better policies on a two-player mini version of the Pommerman\ngame.\n",
        "published": "2019",
        "authors": [
            "Bilal Kartal",
            "Pablo Hernandez-Leal",
            "Matthew E. Taylor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.02269v4",
        "title": "Promoting Coordination through Policy Regularization in Multi-Agent Deep\n  Reinforcement Learning",
        "abstract": "  In multi-agent reinforcement learning, discovering successful collective\nbehaviors is challenging as it requires exploring a joint action space that\ngrows exponentially with the number of agents. While the tractability of\nindependent agent-wise exploration is appealing, this approach fails on tasks\nthat require elaborate group strategies. We argue that coordinating the agents'\npolicies can guide their exploration and we investigate techniques to promote\nsuch an inductive bias. We propose two policy regularization methods: TeamReg,\nwhich is based on inter-agent action predictability and CoachReg that relies on\nsynchronized behavior selection. We evaluate each approach on four challenging\ncontinuous control tasks with sparse rewards that require varying levels of\ncoordination as well as on the discrete action Google Research Football\nenvironment. Our experiments show improved performance across many cooperative\nmulti-agent problems. Finally, we analyze the effects of our proposed methods\non the policies that our agents learn and show that our methods successfully\nenforce the qualities that we propose as proxies for coordinated behaviors.\n",
        "published": "2019",
        "authors": [
            "Julien Roy",
            "Paul Barde",
            "F\u00e9lix G. Harvey",
            "Derek Nowrouzezahrai",
            "Christopher Pal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.03761v2",
        "title": "Large-Scale Traffic Signal Control Using a Novel Multi-Agent\n  Reinforcement Learning",
        "abstract": "  Finding the optimal signal timing strategy is a difficult task for the\nproblem of large-scale traffic signal control (TSC). Multi-Agent Reinforcement\nLearning (MARL) is a promising method to solve this problem. However, there is\nstill room for improvement in extending to large-scale problems and modeling\nthe behaviors of other agents for each individual agent. In this paper, a new\nMARL, called Cooperative double Q-learning (Co-DQL), is proposed, which has\nseveral prominent features. It uses a highly scalable independent double\nQ-learning method based on double estimators and the UCB policy, which can\neliminate the over-estimation problem existing in traditional independent\nQ-learning while ensuring exploration. It uses mean field approximation to\nmodel the interaction among agents, thereby making agents learn a better\ncooperative strategy. In order to improve the stability and robustness of the\nlearning process, we introduce a new reward allocation mechanism and a local\nstate sharing method. In addition, we analyze the convergence properties of the\nproposed algorithm. Co-DQL is applied on TSC and tested on a multi-traffic\nsignal simulator. According to the results obtained on several traffic\nscenarios, Co- DQL outperforms several state-of-the-art decentralized MARL\nalgorithms. It can effectively shorten the average waiting time of the vehicles\nin the whole road system.\n",
        "published": "2019",
        "authors": [
            "Xiaoqiang Wang",
            "Liangjun Ke",
            "Zhimin Qiao",
            "Xinghua Chai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1301.3347v1",
        "title": "Multi-agent learning using Fictitious Play and Extended Kalman Filter",
        "abstract": "  Decentralised optimisation tasks are important components of multi-agent\nsystems. These tasks can be interpreted as n-player potential games: therefore\ngame-theoretic learning algorithms can be used to solve decentralised\noptimisation tasks. Fictitious play is the canonical example of these\nalgorithms. Nevertheless fictitious play implicitly assumes that players have\nstationary strategies. We present a novel variant of fictitious play where\nplayers predict their opponents' strategies using Extended Kalman filters and\nuse their predictions to update their strategies.\n  We show that in 2 by 2 games with at least one pure Nash equilibrium and in\npotential games where players have two available actions, the proposed\nalgorithm converges to the pure Nash equilibrium. The performance of the\nproposed algorithm was empirically tested, in two strategic form games and an\nad-hoc sensor network surveillance problem. The proposed algorithm performs\nbetter than the classic fictitious play algorithm in these games and therefore\nimproves the performance of game-theoretical learning in decentralised\noptimisation.\n",
        "published": "2013",
        "authors": [
            "Michalis Smyrnakis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.05188v3",
        "title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement\n  Learning",
        "abstract": "  A variety of cooperative multi-agent control problems require agents to\nachieve individual goals while contributing to collective success. This\nmulti-goal multi-agent setting poses difficulties for recent algorithms, which\nprimarily target settings with a single global reward, due to two new\nchallenges: efficient exploration for learning both individual goal attainment\nand cooperation for others' success, and credit-assignment for interactions\nbetween actions and goals of different agents. To address both challenges, we\nrestructure the problem into a novel two-stage curriculum, in which\nsingle-agent goal attainment is learned prior to learning multi-agent\ncooperation, and we derive a new multi-goal multi-agent policy gradient with a\ncredit function for localized credit assignment. We use a function augmentation\nscheme to bridge value and policy functions across the curriculum. The complete\narchitecture, called CM3, learns significantly faster than direct adaptations\nof existing algorithms on three challenging multi-goal multi-agent problems:\ncooperative navigation in difficult formations, negotiating multi-vehicle lane\nchanges in the SUMO traffic simulator, and strategic cooperation in a Checkers\nenvironment.\n",
        "published": "2018",
        "authors": [
            "Jiachen Yang",
            "Alireza Nakhaei",
            "David Isele",
            "Kikuo Fujimura",
            "Hongyuan Zha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.03239v3",
        "title": "Communication-Efficient Policy Gradient Methods for Distributed\n  Reinforcement Learning",
        "abstract": "  This paper deals with distributed policy optimization in reinforcement\nlearning, which involves a central controller and a group of learners. In\nparticular, two typical settings encountered in several applications are\nconsidered: multi-agent reinforcement learning (RL) and parallel RL, where\nfrequent information exchanges between the learners and the controller are\nrequired. For many practical distributed systems, however, the overhead caused\nby these frequent communication exchanges is considerable, and becomes the\nbottleneck of the overall performance. To address this challenge, a novel\npolicy gradient approach is developed for solving distributed RL. The novel\napproach adaptively skips the policy gradient communication during iterations,\nand can reduce the communication overhead without degrading learning\nperformance. It is established analytically that: i) the novel algorithm has\nconvergence rate identical to that of the plain-vanilla policy gradient; while\nii) if the distributed learners are heterogeneous in terms of their reward\nfunctions, the number of communication rounds needed to achieve a desirable\nlearning accuracy is markedly reduced. Numerical experiments corroborate the\ncommunication reduction attained by the novel algorithm compared to\nalternatives.\n",
        "published": "2018",
        "authors": [
            "Tianyi Chen",
            "Kaiqing Zhang",
            "Georgios B. Giannakis",
            "Tamer Ba\u015far"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.04145v1",
        "title": "Learning Sharing Behaviors with Arbitrary Numbers of Agents",
        "abstract": "  We propose a method for modeling and learning turn-taking behaviors for\naccessing a shared resource. We model the individual behavior for each agent in\nan interaction and then use a multi-agent fusion model to generate a summary\nover the expected actions of the group to render the model independent of the\nnumber of agents. The individual behavior models are weighted finite state\ntransducers (WFSTs) with weights dynamically updated during interactions, and\nthe multi-agent fusion model is a logistic regression classifier.\n  We test our models in a multi-agent tower-building environment, where a\nQ-learning agent learns to interact with rule-based agents. Our approach\naccurately models the underlying behavior patterns of the rule-based agents\nwith accuracy ranging between 0.63 and 1.0 depending on the stochasticity of\nthe other agent behaviors. In addition we show using KL-divergence that the\nmodel accurately captures the distribution of next actions when interacting\nwith both a single agent (KL-divergence < 0.1) and with multiple agents\n(KL-divergence < 0.37). Finally, we demonstrate that our behavior model can be\nused by a Q-learning agent to take turns in an interactive turn-taking\nenvironment.\n",
        "published": "2018",
        "authors": [
            "Katherine Metcalf",
            "Barry-John Theobald",
            "Nicholas Apostoloff"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.00784v1",
        "title": "Neural MMO: A Massively Multiagent Game Environment for Training and\n  Evaluating Intelligent Agents",
        "abstract": "  The emergence of complex life on Earth is often attributed to the arms race\nthat ensued from a huge number of organisms all competing for finite resources.\nWe present an artificial intelligence research environment, inspired by the\nhuman game genre of MMORPGs (Massively Multiplayer Online Role-Playing Games,\na.k.a. MMOs), that aims to simulate this setting in microcosm. As with MMORPGs\nand the real world alike, our environment is persistent and supports a large\nand variable number of agents. Our environment is well suited to the study of\nlarge-scale multiagent interaction: it requires that agents learn robust combat\nand navigation policies in the presence of large populations attempting to do\nthe same. Baseline experiments reveal that population size magnifies and\nincentivizes the development of skillful behaviors and results in agents that\noutcompete agents trained in smaller populations. We further show that the\npolicies of agents with unshared weights naturally diverge to fill different\nniches in order to avoid competition.\n",
        "published": "2019",
        "authors": [
            "Joseph Suarez",
            "Yilun Du",
            "Phillip Isola",
            "Igor Mordatch"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.02580v2",
        "title": "Collective Learning",
        "abstract": "  In this paper, we introduce the concept of collective learning (CL) which\nexploits the notion of collective intelligence in the field of distributed\nsemi-supervised learning. The proposed framework draws inspiration from the\nlearning behavior of human beings, who alternate phases involving\ncollaboration, confrontation and exchange of views with other consisting of\nstudying and learning on their own. On this regard, CL comprises two main\nphases: a self-training phase in which learning is performed on local private\n(labeled) data only and a collective training phase in which proxy-labels are\nassigned to shared (unlabeled) data by means of a consensus-based algorithm. In\nthe considered framework, heterogeneous systems can be connected over the same\nnetwork, each with different computational capabilities and resources and\neveryone in the network may take advantage of the cooperation and will\neventually reach higher performance with respect to those it can reach on its\nown. An extensive experimental campaign on an image classification problem\nemphasizes the properties of CL by analyzing the performance achieved by the\ncooperating agents.\n",
        "published": "2019",
        "authors": [
            "Francesco Farina"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.03558v3",
        "title": "Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill\n  Discovery",
        "abstract": "  Human players in professional team sports achieve high level coordination by\ndynamically choosing complementary skills and executing primitive actions to\nperform these skills. As a step toward creating intelligent agents with this\ncapability for fully cooperative multi-agent settings, we propose a two-level\nhierarchical multi-agent reinforcement learning (MARL) algorithm with\nunsupervised skill discovery. Agents learn useful and distinct skills at the\nlow level via independent Q-learning, while they learn to select complementary\nlatent skill variables at the high level via centralized multi-agent training\nwith an extrinsic team reward. The set of low-level skills emerges from an\nintrinsic reward that solely promotes the decodability of latent skill\nvariables from the trajectory of a low-level skill, without the need for\nhand-crafted rewards for each skill. For scalable decentralized execution, each\nagent independently chooses latent skill variables and primitive actions based\non local observations. Our overall method enables the use of general\ncooperative MARL algorithms for training high level policies and single-agent\nRL for training low level skills. Experiments on a stochastic high dimensional\nteam game show the emergence of useful skills and cooperative team play. The\ninterpretability of the learned skills show the promise of the proposed method\nfor achieving human-AI cooperation in team sports games.\n",
        "published": "2019",
        "authors": [
            "Jiachen Yang",
            "Igor Borovikov",
            "Hongyuan Zha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.03960v6",
        "title": "MetaCI: Meta-Learning for Causal Inference in a Heterogeneous Population",
        "abstract": "  Performing inference on data obtained through observational studies is\nbecoming extremely relevant due to the widespread availability of data in\nfields such as healthcare, education, retail, etc. Furthermore, this data is\naccrued from multiple homogeneous subgroups of a heterogeneous population, and\nhence, generalizing the inference mechanism over such data is essential. We\npropose the MetaCI framework with the goal of answering counterfactual\nquestions in the context of causal inference (CI), where the factual\nobservations are obtained from several homogeneous subgroups. While the CI\nnetwork is designed to generalize from factual to counterfactual distribution\nin order to tackle covariate shift, MetaCI employs the meta-learning paradigm\nto tackle the shift in data distributions between training and test phase due\nto the presence of heterogeneity in the population, and due to drifts in the\ntarget distribution, also known as concept shift. We benchmark the performance\nof the MetaCI algorithm using the mean absolute percentage error over the\naverage treatment effect as the metric, and demonstrate that meta\ninitialization has significant gains compared to randomly initialized networks,\nand other methods.\n",
        "published": "2019",
        "authors": [
            "Ankit Sharma",
            "Garima Gupta",
            "Ranjitha Prasad",
            "Arnab Chatterjee",
            "Lovekesh Vig",
            "Gautam Shroff"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.08465v3",
        "title": "Graph Learning Under Partial Observability",
        "abstract": "  Many optimization, inference and learning tasks can be accomplished\nefficiently by means of decentralized processing algorithms where the network\ntopology (i.e., the graph) plays a critical role in enabling the interactions\namong neighboring nodes. There is a large body of literature examining the\neffect of the graph structure on the performance of decentralized processing\nstrategies. In this article, we examine the inverse problem and consider the\nreverse question: How much information does observing the behavior at the nodes\nof a graph convey about the underlying topology? For large-scale networks, the\ndifficulty in addressing such inverse problems is compounded by the fact that\nusually only a limited fraction of the nodes can be probed, giving rise to a\nsecond important question: Despite the presence of unobserved nodes, can\npartial observations still be sufficient to discover the graph linking the\nprobed nodes? The article surveys recent advances on this challenging learning\nproblem and related questions.\n",
        "published": "2019",
        "authors": [
            "Vincenzo Matta",
            "Augusto Santos",
            "Ali H. Sayed"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.13107v1",
        "title": "Improved Structural Discovery and Representation Learning of Multi-Agent\n  Data",
        "abstract": "  Central to all machine learning algorithms is data representation. For\nmulti-agent systems, selecting a representation which adequately captures the\ninteractions among agents is challenging due to the latent group structure\nwhich tends to vary depending on context. However, in multi-agent systems with\nstrong group structure, we can simultaneously learn this structure and map a\nset of agents to a consistently ordered representation for further learning. In\nthis paper, we present a dynamic alignment method which provides a robust\nordering of structured multi-agent data enabling representation learning to\noccur in a fraction of the time of previous methods. We demonstrate the value\nof this approach using a large amount of soccer tracking data from a\nprofessional league.\n",
        "published": "2019",
        "authors": [
            "Jennifer Hobbs",
            "Matthew Holbrook",
            "Nathan Frank",
            "Long Sha",
            "Patrick Lucey"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.00543v2",
        "title": "Toward Optimal Adversarial Policies in the Multiplicative Learning\n  System with a Malicious Expert",
        "abstract": "  We consider a learning system based on the conventional multiplicative weight\n(MW) rule that combines experts' advice to predict a sequence of true outcomes.\nIt is assumed that one of the experts is malicious and aims to impose the\nmaximum loss on the system. The loss of the system is naturally defined to be\nthe aggregate absolute difference between the sequence of predicted outcomes\nand the true outcomes. We consider this problem under both offline and online\nsettings. In the offline setting where the malicious expert must choose its\nentire sequence of decisions a priori, we show somewhat surprisingly that a\nsimple greedy policy of always reporting false prediction is asymptotically\noptimal with an approximation ratio of $1+O(\\sqrt{\\frac{\\ln N}{N}})$, where $N$\nis the total number of prediction stages. In particular, we describe a policy\nthat closely resembles the structure of the optimal offline policy. For the\nonline setting where the malicious expert can adaptively make its decisions, we\nshow that the optimal online policy can be efficiently computed by solving a\ndynamic program in $O(N^3)$. Our results provide a new direction for\nvulnerability assessment of commonly used learning algorithms to adversarial\nattacks where the threat is an integral part of the system.\n",
        "published": "2020",
        "authors": [
            "S. Rasoul Etesami",
            "Negar Kiyavash",
            "Vincent Leon",
            "H. Vincent Poor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.10122v1",
        "title": "Regret Bounds for Decentralized Learning in Cooperative Multi-Agent\n  Dynamical Systems",
        "abstract": "  Regret analysis is challenging in Multi-Agent Reinforcement Learning (MARL)\nprimarily due to the dynamical environments and the decentralized information\namong agents. We attempt to solve this challenge in the context of\ndecentralized learning in multi-agent linear-quadratic (LQ) dynamical systems.\nWe begin with a simple setup consisting of two agents and two dynamically\ndecoupled stochastic linear systems, each system controlled by an agent. The\nsystems are coupled through a quadratic cost function. When both systems'\ndynamics are unknown and there is no communication among the agents, we show\nthat no learning policy can generate sub-linear in $T$ regret, where $T$ is the\ntime horizon. When only one system's dynamics are unknown and there is\none-directional communication from the agent controlling the unknown system to\nthe other agent, we propose a MARL algorithm based on the construction of an\nauxiliary single-agent LQ problem. The auxiliary single-agent problem in the\nproposed MARL algorithm serves as an implicit coordination mechanism among the\ntwo learning agents. This allows the agents to achieve a regret within\n$O(\\sqrt{T})$ of the regret of the auxiliary single-agent problem.\nConsequently, using existing results for single-agent LQ regret, our algorithm\nprovides a $\\tilde{O}(\\sqrt{T})$ regret bound. (Here $\\tilde{O}(\\cdot)$ hides\nconstants and logarithmic factors). Our numerical experiments indicate that\nthis bound is matched in practice. From the two-agent problem, we extend our\nresults to multi-agent LQ systems with certain communication patterns.\n",
        "published": "2020",
        "authors": [
            "Seyed Mohammad Asghari",
            "Yi Ouyang",
            "Ashutosh Nayyar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.00433v3",
        "title": "Fully Asynchronous Policy Evaluation in Distributed Reinforcement\n  Learning over Networks",
        "abstract": "  This paper proposes a \\emph{fully asynchronous} scheme for the policy\nevaluation problem of distributed reinforcement learning (DisRL) over directed\npeer-to-peer networks. Without waiting for any other node of the network, each\nnode can locally update its value function at any time by using (possibly\ndelayed) information from its neighbors. This is in sharp contrast to the\ngossip-based scheme where a pair of nodes concurrently update. Though the fully\nasynchronous setting involves a difficult multi-timescale decision problem, we\ndesign a novel stochastic average gradient (SAG) based distributed algorithm\nand develop a push-pull augmented graph approach to prove its exact convergence\nat a linear rate of $\\mathcal{O}(c^k)$ where $c\\in(0,1)$ and $k$ increases by\none no matter on which node updates. Finally, numerical experiments validate\nthat our method speeds up linearly with respect to the number of nodes, and is\nrobust to straggler nodes.\n",
        "published": "2020",
        "authors": [
            "Xingyu Sha",
            "Jiaqi Zhang",
            "Keyou You",
            "Kaiqing Zhang",
            "Tamer Ba\u015far"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.00799v1",
        "title": "Learning to Resolve Alliance Dilemmas in Many-Player Zero-Sum Games",
        "abstract": "  Zero-sum games have long guided artificial intelligence research, since they\npossess both a rich strategy space of best-responses and a clear evaluation\nmetric. What's more, competition is a vital mechanism in many real-world\nmulti-agent systems capable of generating intelligent innovations: Darwinian\nevolution, the market economy and the AlphaZero algorithm, to name a few. In\ntwo-player zero-sum games, the challenge is usually viewed as finding Nash\nequilibrium strategies, safeguarding against exploitation regardless of the\nopponent. While this captures the intricacies of chess or Go, it avoids the\nnotion of cooperation with co-players, a hallmark of the major transitions\nleading from unicellular organisms to human civilization. Beyond two players,\nalliance formation often confers an advantage; however this requires trust,\nnamely the promise of mutual cooperation in the face of incentives to defect.\nSuccessful play therefore requires adaptation to co-players rather than the\npursuit of non-exploitability. Here we argue that a systematic study of\nmany-player zero-sum games is a crucial element of artificial intelligence\nresearch. Using symmetric zero-sum matrix games, we demonstrate formally that\nalliance formation may be seen as a social dilemma, and empirically that\nna\\\"ive multi-agent reinforcement learning therefore fails to form alliances.\nWe introduce a toy model of economic competition, and show how reinforcement\nlearning may be augmented with a peer-to-peer contract mechanism to discover\nand enforce alliances. Finally, we generalize our agent model to incorporate\ntemporally-extended contracts, presenting opportunities for further work.\n",
        "published": "2020",
        "authors": [
            "Edward Hughes",
            "Thomas W. Anthony",
            "Tom Eccles",
            "Joel Z. Leibo",
            "David Balduzzi",
            "Yoram Bachrach"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.08839v2",
        "title": "Monotonic Value Function Factorisation for Deep Multi-Agent\n  Reinforcement Learning",
        "abstract": "  In many real-world settings, a team of agents must coordinate its behaviour\nwhile acting in a decentralised fashion. At the same time, it is often possible\nto train the agents in a centralised fashion where global state information is\navailable and communication constraints are lifted. Learning joint\naction-values conditioned on extra state information is an attractive way to\nexploit centralised learning, but the best strategy for then extracting\ndecentralised policies is unclear. Our solution is QMIX, a novel value-based\nmethod that can train decentralised policies in a centralised end-to-end\nfashion. QMIX employs a mixing network that estimates joint action-values as a\nmonotonic combination of per-agent values. We structurally enforce that the\njoint-action value is monotonic in the per-agent values, through the use of\nnon-negative weights in the mixing network, which guarantees consistency\nbetween the centralised and decentralised policies. To evaluate the performance\nof QMIX, we propose the StarCraft Multi-Agent Challenge (SMAC) as a new\nbenchmark for deep multi-agent reinforcement learning. We evaluate QMIX on a\nchallenging set of SMAC scenarios and show that it significantly outperforms\nexisting multi-agent reinforcement learning methods.\n",
        "published": "2020",
        "authors": [
            "Tabish Rashid",
            "Mikayel Samvelyan",
            "Christian Schroeder de Witt",
            "Gregory Farquhar",
            "Jakob Foerster",
            "Shimon Whiteson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.03923v1",
        "title": "Learning to Model Opponent Learning",
        "abstract": "  Multi-Agent Reinforcement Learning (MARL) considers settings in which a set\nof coexisting agents interact with one another and their environment. The\nadaptation and learning of other agents induces non-stationarity in the\nenvironment dynamics. This poses a great challenge for value function-based\nalgorithms whose convergence usually relies on the assumption of a stationary\nenvironment. Policy search algorithms also struggle in multi-agent settings as\nthe partial observability resulting from an opponent's actions not being known\nintroduces high variance to policy training. Modelling an agent's opponent(s)\nis often pursued as a means of resolving the issues arising from the\ncoexistence of learning opponents. An opponent model provides an agent with\nsome ability to reason about other agents to aid its own decision making. Most\nprior works learn an opponent model by assuming the opponent is employing a\nstationary policy or switching between a set of stationary policies. Such an\napproach can reduce the variance of training signals for policy search\nalgorithms. However, in the multi-agent setting, agents have an incentive to\ncontinually adapt and learn. This means that the assumptions concerning\nopponent stationarity are unrealistic. In this work, we develop a novel\napproach to modelling an opponent's learning dynamics which we term Learning to\nModel Opponent Learning (LeMOL). We show our structured opponent model is more\naccurate and stable than naive behaviour cloning baselines. We further show\nthat opponent modelling can improve the performance of algorithmic agents in\nmulti-agent settings.\n",
        "published": "2020",
        "authors": [
            "Ian Davies",
            "Zheng Tian",
            "Jun Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.06051v2",
        "title": "Learning to Incentivize Other Learning Agents",
        "abstract": "  The challenge of developing powerful and general Reinforcement Learning (RL)\nagents has received increasing attention in recent years. Much of this effort\nhas focused on the single-agent setting, in which an agent maximizes a\npredefined extrinsic reward function. However, a long-term question inevitably\narises: how will such independent agents cooperate when they are continually\nlearning and acting in a shared multi-agent environment? Observing that humans\noften provide incentives to influence others' behavior, we propose to equip\neach RL agent in a multi-agent environment with the ability to give rewards\ndirectly to other agents, using a learned incentive function. Each agent learns\nits own incentive function by explicitly accounting for its impact on the\nlearning of recipients and, through them, the impact on its own extrinsic\nobjective. We demonstrate in experiments that such agents significantly\noutperform standard RL and opponent-shaping agents in challenging general-sum\nMarkov games, often by finding a near-optimal division of labor. Our work\npoints toward more opportunities and challenges along the path to ensure the\ncommon good in a multi-agent future.\n",
        "published": "2020",
        "authors": [
            "Jiachen Yang",
            "Ang Li",
            "Mehrdad Farajtabar",
            "Peter Sunehag",
            "Edward Hughes",
            "Hongyuan Zha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.06455v2",
        "title": "Learning Individually Inferred Communication for Multi-Agent Cooperation",
        "abstract": "  Communication lays the foundation for human cooperation. It is also crucial\nfor multi-agent cooperation. However, existing work focuses on broadcast\ncommunication, which is not only impractical but also leads to information\nredundancy that could even impair the learning process. To tackle these\ndifficulties, we propose Individually Inferred Communication (I2C), a simple\nyet effective model to enable agents to learn a prior for agent-agent\ncommunication. The prior knowledge is learned via causal inference and realized\nby a feed-forward neural network that maps the agent's local observation to a\nbelief about who to communicate with. The influence of one agent on another is\ninferred via the joint action-value function in multi-agent reinforcement\nlearning and quantified to label the necessity of agent-agent communication.\nFurthermore, the agent policy is regularized to better exploit communicated\nmessages. Empirically, we show that I2C can not only reduce communication\noverhead but also improve the performance in a variety of multi-agent\ncooperative scenarios, comparing to existing methods. The code is available at\nhttps://github.com/PKU-AI-Edge/I2C.\n",
        "published": "2020",
        "authors": [
            "Ziluo Ding",
            "Tiejun Huang",
            "Zongqing Lu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.06555v3",
        "title": "Multi-Agent Reinforcement Learning in Stochastic Networked Systems",
        "abstract": "  We study multi-agent reinforcement learning (MARL) in a stochastic network of\nagents. The objective is to find localized policies that maximize the\n(discounted) global reward. In general, scalability is a challenge in this\nsetting because the size of the global state/action space can be exponential in\nthe number of agents. Scalable algorithms are only known in cases where\ndependencies are static, fixed and local, e.g., between neighbors in a fixed,\ntime-invariant underlying graph. In this work, we propose a Scalable Actor\nCritic framework that applies in settings where the dependencies can be\nnon-local and stochastic, and provide a finite-time error bound that shows how\nthe convergence rate depends on the speed of information spread in the network.\nAdditionally, as a byproduct of our analysis, we obtain novel finite-time\nconvergence results for a general stochastic approximation scheme and for\ntemporal difference learning with state aggregation, which apply beyond the\nsetting of MARL in networked systems.\n",
        "published": "2020",
        "authors": [
            "Yiheng Lin",
            "Guannan Qu",
            "Longbo Huang",
            "Adam Wierman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.07200v4",
        "title": "Learning to Communicate Using Counterfactual Reasoning",
        "abstract": "  Learning to communicate in order to share state information is an active\nproblem in the area of multi-agent reinforcement learning (MARL). The credit\nassignment problem, the non-stationarity of the communication environment and\nthe creation of influenceable agents are major challenges within this research\nfield which need to be overcome in order to learn a valid communication\nprotocol. This paper introduces the novel multi-agent counterfactual\ncommunication learning (MACC) method which adapts counterfactual reasoning in\norder to overcome the credit assignment problem for communicating agents.\nSecondly, the non-stationarity of the communication environment while learning\nthe communication Q-function is overcome by creating the communication\nQ-function using the action policy of the other agents and the Q-function of\nthe action environment. Additionally, a social loss function is introduced in\norder to create influenceable agents which is required to learn a valid\ncommunication protocol. Our experiments show that MACC is able to outperform\nthe state-of-the-art baselines in four different scenarios in the Particle\nenvironment.\n",
        "published": "2020",
        "authors": [
            "Simon Vanneste",
            "Astrid Vanneste",
            "Kevin Mets",
            "Tom De Schepper",
            "Ali Anwar",
            "Siegfried Mercelis",
            "Steven Latr\u00e9",
            "Peter Hellinckx"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.08212v2",
        "title": "Tight Nonparametric Convergence Rates for Stochastic Gradient Descent\n  under the Noiseless Linear Model",
        "abstract": "  In the context of statistical supervised learning, the noiseless linear model\nassumes that there exists a deterministic linear relation $Y = \\langle\n\\theta_*, X \\rangle$ between the random output $Y$ and the random feature\nvector $\\Phi(U)$, a potentially non-linear transformation of the inputs $U$. We\nanalyze the convergence of single-pass, fixed step-size stochastic gradient\ndescent on the least-square risk under this model. The convergence of the\niterates to the optimum $\\theta_*$ and the decay of the generalization error\nfollow polynomial convergence rates with exponents that both depend on the\nregularities of the optimum $\\theta_*$ and of the feature vectors $\\Phi(u)$. We\ninterpret our result in the reproducing kernel Hilbert space framework. As a\nspecial case, we analyze an online algorithm for estimating a real function on\nthe unit interval from the noiseless observation of its value at randomly\nsampled points; the convergence depends on the Sobolev smoothness of the\nfunction and of a chosen kernel. Finally, we apply our analysis beyond the\nsupervised learning setting to obtain convergence rates for the averaging\nprocess (a.k.a. gossip algorithm) on a graph depending on its spectral\ndimension.\n",
        "published": "2020",
        "authors": [
            "Rapha\u00ebl Berthier",
            "Francis Bach",
            "Pierre Gaillard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.09447v4",
        "title": "Agent Modelling under Partial Observability for Deep Reinforcement\n  Learning",
        "abstract": "  Modelling the behaviours of other agents is essential for understanding how\nagents interact and making effective decisions. Existing methods for agent\nmodelling commonly assume knowledge of the local observations and chosen\nactions of the modelled agents during execution. To eliminate this assumption,\nwe extract representations from the local information of the controlled agent\nusing encoder-decoder architectures. Using the observations and actions of the\nmodelled agents during training, our models learn to extract representations\nabout the modelled agents conditioned only on the local observations of the\ncontrolled agent. The representations are used to augment the controlled\nagent's decision policy which is trained via deep reinforcement learning; thus,\nduring execution, the policy does not require access to other agents'\ninformation. We provide a comprehensive evaluation and ablations studies in\ncooperative, competitive and mixed multi-agent environments, showing that our\nmethod achieves higher returns than baseline methods which do not use the\nlearned representations.\n",
        "published": "2020",
        "authors": [
            "Georgios Papoudakis",
            "Filippos Christianos",
            "Stefano V. Albrecht"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.10412v4",
        "title": "Towards Open Ad Hoc Teamwork Using Graph-based Policy Learning",
        "abstract": "  Ad hoc teamwork is the challenging problem of designing an autonomous agent\nwhich can adapt quickly to collaborate with teammates without prior\ncoordination mechanisms, including joint training. Prior work in this area has\nfocused on closed teams in which the number of agents is fixed. In this work,\nwe consider open teams by allowing agents with different fixed policies to\nenter and leave the environment without prior notification. Our solution builds\non graph neural networks to learn agent models and joint-action value models\nunder varying team compositions. We contribute a novel action-value computation\nthat integrates the agent model and joint-action value model to produce\naction-value estimates. We empirically demonstrate that our approach\nsuccessfully models the effects other agents have on the learner, leading to\npolicies that robustly adapt to dynamic team compositions and significantly\noutperform several alternative methods.\n",
        "published": "2020",
        "authors": [
            "Arrasy Rahman",
            "Niklas H\u00f6pner",
            "Filippos Christianos",
            "Stefano V. Albrecht"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.10611v1",
        "title": "Competitive Policy Optimization",
        "abstract": "  A core challenge in policy optimization in competitive Markov decision\nprocesses is the design of efficient optimization methods with desirable\nconvergence and stability properties. To tackle this, we propose competitive\npolicy optimization (CoPO), a novel policy gradient approach that exploits the\ngame-theoretic nature of competitive games to derive policy updates. Motivated\nby the competitive gradient optimization method, we derive a bilinear\napproximation of the game objective. In contrast, off-the-shelf policy gradient\nmethods utilize only linear approximations, and hence do not capture\ninteractions among the players. We instantiate CoPO in two ways:(i) competitive\npolicy gradient, and (ii) trust-region competitive policy optimization. We\ntheoretically study these methods, and empirically investigate their behavior\non a set of comprehensive, yet challenging, competitive games. We observe that\nthey provide stable optimization, convergence to sophisticated strategies, and\nhigher scores when played against baseline policy gradient methods.\n",
        "published": "2020",
        "authors": [
            "Manish Prajapat",
            "Kamyar Azizzadenesheli",
            "Alexander Liniger",
            "Yisong Yue",
            "Anima Anandkumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.10800v2",
        "title": "Weighted QMIX: Expanding Monotonic Value Function Factorisation for Deep\n  Multi-Agent Reinforcement Learning",
        "abstract": "  QMIX is a popular $Q$-learning algorithm for cooperative MARL in the\ncentralised training and decentralised execution paradigm. In order to enable\neasy decentralisation, QMIX restricts the joint action $Q$-values it can\nrepresent to be a monotonic mixing of each agent's utilities. However, this\nrestriction prevents it from representing value functions in which an agent's\nordering over its actions can depend on other agents' actions. To analyse this\nrepresentational limitation, we first formalise the objective QMIX optimises,\nwhich allows us to view QMIX as an operator that first computes the\n$Q$-learning targets and then projects them into the space representable by\nQMIX. This projection returns a representable $Q$-value that minimises the\nunweighted squared error across all joint actions. We show in particular that\nthis projection can fail to recover the optimal policy even with access to\n$Q^*$, which primarily stems from the equal weighting placed on each joint\naction. We rectify this by introducing a weighting into the projection, in\norder to place more importance on the better joint actions. We propose two\nweighting schemes and prove that they recover the correct maximal action for\nany joint action $Q$-values, and therefore for $Q^*$ as well. Based on our\nanalysis and results in the tabular setting, we introduce two scalable versions\nof our algorithm, Centrally-Weighted (CW) QMIX and Optimistically-Weighted (OW)\nQMIX and demonstrate improved performance on both predator-prey and challenging\nmulti-agent StarCraft benchmark tasks.\n",
        "published": "2020",
        "authors": [
            "Tabish Rashid",
            "Gregory Farquhar",
            "Bei Peng",
            "Shimon Whiteson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.10897v1",
        "title": "Efficient Ridesharing Dispatch Using Multi-Agent Reinforcement Learning",
        "abstract": "  With the advent of ride-sharing services, there is a huge increase in the\nnumber of people who rely on them for various needs. Most of the earlier\napproaches tackling this issue required handcrafted functions for estimating\ntravel times and passenger waiting times. Traditional Reinforcement Learning\n(RL) based methods attempting to solve the ridesharing problem are unable to\naccurately model the complex environment in which taxis operate. Prior\nMulti-Agent Deep RL based methods based on Independent DQN (IDQN) learn\ndecentralized value functions prone to instability due to the concurrent\nlearning and exploring of multiple agents. Our proposed method based on QMIX is\nable to achieve centralized training with decentralized execution. We show that\nour model performs better than the IDQN baseline on a fixed grid size and is\nable to generalize well to smaller or larger grid sizes. Also, our algorithm is\nable to outperform IDQN baseline in the scenario where we have a variable\nnumber of passengers and cars in each episode. Code for our paper is publicly\navailable at: https://github.com/UMich-ML-Group/RL-Ridesharing.\n",
        "published": "2020",
        "authors": [
            "Oscar de Lima",
            "Hansal Shah",
            "Ting-Sheng Chu",
            "Brian Fogelson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.02529v2",
        "title": "Learning Implicit Credit Assignment for Cooperative Multi-Agent\n  Reinforcement Learning",
        "abstract": "  We present a multi-agent actor-critic method that aims to implicitly address\nthe credit assignment problem under fully cooperative settings. Our key\nmotivation is that credit assignment among agents may not require an explicit\nformulation as long as (1) the policy gradients derived from a centralized\ncritic carry sufficient information for the decentralized agents to maximize\ntheir joint action value through optimal cooperation and (2) a sustained level\nof exploration is enforced throughout training. Under the centralized training\nwith decentralized execution (CTDE) paradigm, we achieve the former by\nformulating the centralized critic as a hypernetwork such that a latent state\nrepresentation is integrated into the policy gradients through its\nmultiplicative association with the stochastic policies; to achieve the latter,\nwe derive a simple technique called adaptive entropy regularization where\nmagnitudes of the entropy gradients are dynamically rescaled based on the\ncurrent policy stochasticity to encourage consistent levels of exploration. Our\nalgorithm, referred to as LICA, is evaluated on several benchmarks including\nthe multi-agent particle environments and a set of challenging StarCraft II\nmicromanagement tasks, and we show that LICA significantly outperforms previous\nmethods.\n",
        "published": "2020",
        "authors": [
            "Meng Zhou",
            "Ziyu Liu",
            "Pengwei Sui",
            "Yixuan Li",
            "Yuk Ying Chung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.03155v2",
        "title": "Decentralized policy learning with partial observation and mechanical\n  constraints for multiperson modeling",
        "abstract": "  Extracting the rules of real-world multi-agent behaviors is a current\nchallenge in various scientific and engineering fields. Biological agents\nindependently have limited observation and mechanical constraints; however,\nmost of the conventional data-driven models ignore such assumptions, resulting\nin lack of biological plausibility and model interpretability for behavioral\nanalyses. Here we propose sequential generative models with partial observation\nand mechanical constraints in a decentralized manner, which can model agents'\ncognition and body dynamics, and predict biologically plausible behaviors. We\nformulate this as a decentralized multi-agent imitation-learning problem,\nleveraging binary partial observation and decentralized policy models based on\nhierarchical variational recurrent neural networks with physical and\nbiomechanical penalties. Using real-world basketball and soccer datasets, we\nshow the effectiveness of our method in terms of the constraint violations,\nlong-term trajectory prediction, and partial observation. Our approach can be\nused as a multi-agent simulator to generate realistic trajectories using\nreal-world data.\n",
        "published": "2020",
        "authors": [
            "Keisuke Fujii",
            "Naoya Takeishi",
            "Yoshinobu Kawahara",
            "Kazuya Takeda"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.03562v1",
        "title": "A Distributed Cubic-Regularized Newton Method for Smooth Convex\n  Optimization over Networks",
        "abstract": "  We propose a distributed, cubic-regularized Newton method for large-scale\nconvex optimization over networks. The proposed method requires only local\ncomputations and communications and is suitable for federated learning\napplications over arbitrary network topologies. We show a $O(k^{{-}3})$\nconvergence rate when the cost function is convex with Lipschitz gradient and\nHessian, with $k$ being the number of iterations. We further provide\nnetwork-dependent bounds for the communication required in each step of the\nalgorithm. We provide numerical experiments that validate our theoretical\nresults.\n",
        "published": "2020",
        "authors": [
            "C\u00e9sar A. Uribe",
            "Ali Jadbabaie"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.12322v2",
        "title": "Off-Policy Multi-Agent Decomposed Policy Gradients",
        "abstract": "  Multi-agent policy gradient (MAPG) methods recently witness vigorous\nprogress. However, there is a significant performance discrepancy between MAPG\nmethods and state-of-the-art multi-agent value-based approaches. In this paper,\nwe investigate causes that hinder the performance of MAPG algorithms and\npresent a multi-agent decomposed policy gradient method (DOP). This method\nintroduces the idea of value function decomposition into the multi-agent\nactor-critic framework. Based on this idea, DOP supports efficient off-policy\nlearning and addresses the issue of centralized-decentralized mismatch and\ncredit assignment in both discrete and continuous action spaces. We formally\nshow that DOP critics have sufficient representational capability to guarantee\nconvergence. In addition, empirical evaluations on the StarCraft II\nmicromanagement benchmark and multi-agent particle environments demonstrate\nthat DOP significantly outperforms both state-of-the-art value-based and\npolicy-based multi-agent reinforcement learning algorithms. Demonstrative\nvideos are available at https://sites.google.com/view/dop-mapg/.\n",
        "published": "2020",
        "authors": [
            "Yihan Wang",
            "Beining Han",
            "Tonghan Wang",
            "Heng Dong",
            "Chongjie Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.06220v1",
        "title": "Kernel Methods for Cooperative Multi-Agent Contextual Bandits",
        "abstract": "  Cooperative multi-agent decision making involves a group of agents\ncooperatively solving learning problems while communicating over a network with\ndelays. In this paper, we consider the kernelised contextual bandit problem,\nwhere the reward obtained by an agent is an arbitrary linear function of the\ncontexts' images in the related reproducing kernel Hilbert space (RKHS), and a\ngroup of agents must cooperate to collectively solve their unique decision\nproblems. For this problem, we propose \\textsc{Coop-KernelUCB}, an algorithm\nthat provides near-optimal bounds on the per-agent regret, and is both\ncomputationally and communicatively efficient. For special cases of the\ncooperative problem, we also provide variants of \\textsc{Coop-KernelUCB} that\nprovides optimal per-agent regret. In addition, our algorithm generalizes\nseveral existing results in the multi-agent bandit setting. Finally, on a\nseries of both synthetic and real-world multi-agent network benchmarks, we\ndemonstrate that our algorithm significantly outperforms existing benchmarks.\n",
        "published": "2020",
        "authors": [
            "Abhimanyu Dubey",
            "Alex Pentland"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.06244v1",
        "title": "Cooperative Multi-Agent Bandits with Heavy Tails",
        "abstract": "  We study the heavy-tailed stochastic bandit problem in the cooperative\nmulti-agent setting, where a group of agents interact with a common bandit\nproblem, while communicating on a network with delays. Existing algorithms for\nthe stochastic bandit in this setting utilize confidence intervals arising from\nan averaging-based communication protocol known as~\\textit{running consensus},\nthat does not lend itself to robust estimation for heavy-tailed settings. We\npropose \\textsc{MP-UCB}, a decentralized multi-agent algorithm for the\ncooperative stochastic bandit that incorporates robust estimation with a\nmessage-passing protocol. We prove optimal regret bounds for \\textsc{MP-UCB}\nfor several problem settings, and also demonstrate its superiority to existing\nmethods. Furthermore, we establish the first lower bounds for the cooperative\nbandit problem, in addition to providing efficient algorithms for robust bandit\nestimation of location.\n",
        "published": "2020",
        "authors": [
            "Abhimanyu Dubey",
            "Alex Pentland"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.00161v1",
        "title": "Unknown Delay for Adversarial Bandit Setting with Multiple Play",
        "abstract": "  This paper addresses the problem of unknown delays in adversarial multi-armed\nbandit (MAB) with multiple play. Existing work on similar game setting focused\non only the case where the learner selects an arm in each round. However, there\nare lots of applications in robotics where a learner needs to select more than\none arm per round. It is therefore worthwhile to investigate the effect of\ndelay when multiple arms are chosen. The multiple arms chosen per round in this\nsetting are such that they experience the same amount of delay. There can be an\naggregation of feedback losses from different combinations of arms selected at\ndifferent rounds, and the learner is faced with the challenge of associating\nthe feedback losses to the arms producing them. To address this problem, this\npaper proposes a delayed exponential, exploitation and exploration for multiple\nplay (DEXP3.M) algorithm. The regret bound is only slightly worse than the\nregret of DEXP3 already proposed for the single play setting with unknown\ndelay.\n",
        "published": "2020",
        "authors": [
            "Olusola T. Odeyomi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.01711v2",
        "title": "A Generative Machine Learning Approach to Policy Optimization in\n  Pursuit-Evasion Games",
        "abstract": "  We consider a pursuit-evasion game [11] played between two agents, 'Blue'\n(the pursuer) and 'Red' (the evader), over $T$ time steps. Red aims to attack\nBlue's territory. Blue's objective is to intercept Red by time $T$ and thereby\nlimit the success of Red's attack. Blue must plan its pursuit trajectory by\nchoosing parameters that determine its course of movement (speed and angle in\nour setup) such that it intercepts Red by time $T$. We show that Blue's\npath-planning problem in pursuing Red, can be posed as a sequential decision\nmaking problem under uncertainty. Blue's unawareness of Red's action policy\nrenders the analytic dynamic programming approach intractable for finding the\noptimal action policy for Blue. In this work, we are interested in exploring\ndata-driven approaches to the policy optimization problem that Blue faces. We\napply generative machine learning (ML) approaches to learn optimal action\npolicies for Blue. This highlights the ability of generative ML model to learn\nthe relevant implicit representations for the dynamics of simulated\npursuit-evasion games. We demonstrate the effectiveness of our modeling\napproach via extensive statistical assessments. This work can be viewed as a\npreliminary step towards further adoption of generative modeling approaches for\naddressing policy optimization problems that arise in the context of\nmulti-agent learning and planning [1].\n",
        "published": "2020",
        "authors": [
            "Shiva Navabi",
            "Osonde A. Osoba"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.11425v1",
        "title": "Differentially-Private Federated Linear Bandits",
        "abstract": "  The rapid proliferation of decentralized learning systems mandates the need\nfor differentially-private cooperative learning. In this paper, we study this\nin context of the contextual linear bandit: we consider a collection of agents\ncooperating to solve a common contextual bandit, while ensuring that their\ncommunication remains private. For this problem, we devise \\textsc{FedUCB}, a\nmultiagent private algorithm for both centralized and decentralized\n(peer-to-peer) federated learning. We provide a rigorous technical analysis of\nits utility in terms of regret, improving several results in cooperative bandit\nlearning, and provide rigorous privacy guarantees as well. Our algorithms\nprovide competitive performance both in terms of pseudoregret bounds and\nempirical benchmark performance in various multi-agent settings.\n",
        "published": "2020",
        "authors": [
            "Abhimanyu Dubey",
            "Alex Pentland"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.12797v1",
        "title": "Collaborative Machine Learning with Incentive-Aware Model Rewards",
        "abstract": "  Collaborative machine learning (ML) is an appealing paradigm to build\nhigh-quality ML models by training on the aggregated data from many parties.\nHowever, these parties are only willing to share their data when given enough\nincentives, such as a guaranteed fair reward based on their contributions. This\nmotivates the need for measuring a party's contribution and designing an\nincentive-aware reward scheme accordingly. This paper proposes to value a\nparty's reward based on Shapley value and information gain on model parameters\ngiven its data. Subsequently, we give each party a model as a reward. To\nformally incentivize the collaboration, we define some desirable properties\n(e.g., fairness and stability) which are inspired by cooperative game theory\nbut adapted for our model reward that is uniquely freely replicable. Then, we\npropose a novel model reward scheme to satisfy fairness and trade off between\nthe desirable properties via an adjustable parameter. The value of each party's\nmodel reward determined by our scheme is attained by injecting Gaussian noise\nto the aggregated training data with an optimized noise variance. We\nempirically demonstrate interesting properties of our scheme and evaluate its\nperformance using synthetic and real-world datasets.\n",
        "published": "2020",
        "authors": [
            "Rachael Hwee Ling Sim",
            "Yehong Zhang",
            "Mun Choon Chan",
            "Bryan Kian Hsiang Low"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.01572v1",
        "title": "Sequential Choice Bandits with Feedback for Personalizing users'\n  experience",
        "abstract": "  In this work, we study sequential choice bandits with feedback. We propose\nbandit algorithms for a platform that personalizes users' experience to\nmaximize its rewards. For each action directed to a given user, the platform is\ngiven a positive reward, which is a non-decreasing function of the action, if\nthis action is below the user's threshold. Users are equipped with a patience\nbudget, and actions that are above the threshold decrease the user's patience.\nWhen all patience is lost, the user abandons the platform. The platform\nattempts to learn the thresholds of the users in order to maximize its rewards,\nbased on two different feedback models describing the information pattern\navailable to the platform at each action. We define a notion of regret by\ndetermining the best action to be taken when the platform knows that the user's\nthreshold is in a given interval. We then propose bandit algorithms for the two\nfeedback models and show that upper and lower bounds on the regret are of the\norder of $\\tilde{O}(N^{2/3})$ and $\\tilde\\Omega(N^{2/3})$, respectively, where\n$N$ is the total number of users. Finally, we show that the waiting time of any\nuser before receiving a personalized experience is uniform in $N$.\n",
        "published": "2021",
        "authors": [
            "Anshuka Rangi",
            "Massimo Franceschetti",
            "Long Tran-Thanh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.14586v2",
        "title": "Kolmogorov-Smirnov Test-Based Actively-Adaptive Thompson Sampling for\n  Non-Stationary Bandits",
        "abstract": "  We consider the non-stationary multi-armed bandit (MAB) framework and propose\na Kolmogorov-Smirnov (KS) test based Thompson Sampling (TS) algorithm named\nTS-KS, that actively detects change points and resets the TS parameters once a\nchange is detected. In particular, for the two-armed bandit case, we derive\nbounds on the number of samples of the reward distribution to detect the change\nonce it occurs. Consequently, we show that the proposed algorithm has\nsub-linear regret. Contrary to existing works, our algorithm is able to detect\na change when the underlying reward distribution changes even though the mean\nreward remains the same. Finally, to test the efficacy of the proposed\nalgorithm, we employ it in two case-studies: i) task-offloading scenario in\nwireless edge-computing, and ii) portfolio optimization. Our results show that\nthe proposed TS-KS algorithm outperforms not only the static TS algorithm but\nalso it performs better than other bandit algorithms designed for\nnon-stationary environments. Moreover, the performance of TS-KS is at par with\nthe state-of-the-art forecasting algorithms such as Facebook-PROPHET and ARIMA.\n",
        "published": "2021",
        "authors": [
            "Gourab Ghatak",
            "Hardhik Mohanty",
            "Aniq Ur Rahman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.04050v2",
        "title": "Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning",
        "abstract": "  Learning in multi-agent systems is highly challenging due to several factors\nincluding the non-stationarity introduced by agents' interactions and the\ncombinatorial nature of their state and action spaces. In particular, we\nconsider the Mean-Field Control (MFC) problem which assumes an asymptotically\ninfinite population of identical agents that aim to collaboratively maximize\nthe collective reward. In many cases, solutions of an MFC problem are good\napproximations for large systems, hence, efficient learning for MFC is valuable\nfor the analogous discrete agent setting with many agents. Specifically, we\nfocus on the case of unknown system dynamics where the goal is to\nsimultaneously optimize for the rewards and learn from experience. We propose\nan efficient model-based reinforcement learning algorithm, $M^3-UCRL$, that\nruns in episodes, balances between exploration and exploitation during policy\nlearning, and provably solves this problem. Our main theoretical contributions\nare the first general regret bounds for model-based reinforcement learning for\nMFC, obtained via a novel mean-field type analysis. To learn the system's\ndynamics, $M^3-UCRL$ can be instantiated with various statistical models, e.g.,\nneural networks or Gaussian Processes. Moreover, we provide a practical\nparametrization of the core optimization problem that facilitates\ngradient-based optimization techniques when combined with differentiable\ndynamics approximation methods such as neural networks.\n",
        "published": "2021",
        "authors": [
            "Barna P\u00e1sztor",
            "Ilija Bogunovic",
            "Andreas Krause"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.05812v2",
        "title": "Distributed saddle point problems for strongly concave-convex functions",
        "abstract": "  In this paper, we propose GT-GDA, a distributed optimization method to solve\nsaddle point problems of the form: $\\min_{\\mathbf{x}} \\max_{\\mathbf{y}}\n\\{F(\\mathbf{x},\\mathbf{y}) :=G(\\mathbf{x}) + \\langle \\mathbf{y}, \\overline{P}\n\\mathbf{x} \\rangle - H(\\mathbf{y})\\}$, where the functions $G(\\cdot)$,\n$H(\\cdot)$, and the the coupling matrix $\\overline{P}$ are distributed over a\nstrongly connected network of nodes. GT-GDA is a first-order method that uses\ngradient tracking to eliminate the dissimilarity caused by heterogeneous data\ndistribution among the nodes. In the most general form, GT-GDA includes a\nconsensus over the local coupling matrices to achieve the optimal (unique)\nsaddle point, however, at the expense of increased communication. To avoid\nthis, we propose a more efficient variant GT-GDA-Lite that does not incur the\nadditional communication and analyze its convergence in various scenarios. We\nshow that GT-GDA converges linearly to the unique saddle point solution when\n$G(\\cdot)$ is smooth and convex, $H(\\cdot)$ is smooth and strongly convex, and\nthe global coupling matrix $\\overline{P}$ has full column rank. We further\ncharacterize the regime under which GT-GDA exhibits a network\ntopology-independent convergence behavior. We next show the linear convergence\nof GT-GDA to an error around the unique saddle point, which goes to zero when\nthe coupling cost ${\\langle \\mathbf y, \\overline{P} \\mathbf x \\rangle}$ is\ncommon to all nodes, or when $G(\\cdot)$ and $H(\\cdot)$ are quadratic. Numerical\nexperiments illustrate the convergence properties and importance of GT-GDA and\nGT-GDA-Lite for several applications.\n",
        "published": "2022",
        "authors": [
            "Muhammad I. Qureshi",
            "Usman A. Khan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.09653v2",
        "title": "The Pareto Frontier of Instance-Dependent Guarantees in Multi-Player\n  Multi-Armed Bandits with no Communication",
        "abstract": "  We study the stochastic multi-player multi-armed bandit problem. In this\nproblem, $m$ players cooperate to maximize their total reward from $K > m$\narms. However the players cannot communicate and are penalized (e.g. receive no\nreward) if they pull the same arm at the same time. We ask whether it is\npossible to obtain optimal instance-dependent regret $\\tilde{O}(1/\\Delta)$\nwhere $\\Delta$ is the gap between the $m$-th and $m+1$-st best arms. Such\nguarantees were recently achieved in a model allowing the players to implicitly\ncommunicate through intentional collisions.\n  Surprisingly, we show that with no communication at all, such guarantees are\nnot achievable. In fact, obtaining the optimal $\\tilde{O}(1/\\Delta)$ regret for\nsome values of $\\Delta$ necessarily implies strictly sub-optimal regret in\nother regimes. Our main result is a complete characterization of the Pareto\noptimal instance-dependent trade-offs that are possible with no communication.\nOur algorithm generalizes that of Bubeck, Budzinski, and the second author. As\nthere, our algorithm succeeds even when feedback upon collision can be\ncorrupted by an adaptive adversary, thanks to a strong no-collision property.\nOur lower bound is based on topological obstructions at multiple scales and is\ncompletely new.\n",
        "published": "2022",
        "authors": [
            "Allen Liu",
            "Mark Sellke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.10363v6",
        "title": "Diff-DAC: Distributed Actor-Critic for Average Multitask Deep\n  Reinforcement Learning",
        "abstract": "  We propose a fully distributed actor-critic algorithm approximated by deep\nneural networks, named \\textit{Diff-DAC}, with application to single-task and\nto average multitask reinforcement learning (MRL). Each agent has access to\ndata from its local task only, but it aims to learn a policy that performs well\non average for the whole set of tasks. During the learning process, agents\ncommunicate their value-policy parameters to their neighbors, diffusing the\ninformation across the network, so that they converge to a common policy, with\nno need for a central node. The method is scalable, since the computational and\ncommunication costs per agent grow with its number of neighbors. We derive\nDiff-DAC's from duality theory and provide novel insights into the standard\nactor-critic framework, showing that it is actually an instance of the dual\nascent method that approximates the solution of a linear program. Experiments\nsuggest that Diff-DAC can outperform the single previous distributed MRL\napproach (i.e., Dist-MTLPS) and even the centralized architecture.\n",
        "published": "2017",
        "authors": [
            "Sergio Valcarcel Macua",
            "Aleksi Tukiainen",
            "Daniel Garc\u00eda-Oca\u00f1a Hern\u00e1ndez",
            "David Baldazo",
            "Enrique Munoz de Cote",
            "Santiago Zazo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.02777v2",
        "title": "What game are we playing? End-to-end learning in normal and extensive\n  form games",
        "abstract": "  Although recent work in AI has made great progress in solving large,\nzero-sum, extensive-form games, the underlying assumption in most past work is\nthat the parameters of the game itself are known to the agents. This paper\ndeals with the relatively under-explored but equally important \"inverse\"\nsetting, where the parameters of the underlying game are not known to all\nagents, but must be learned through observations. We propose a differentiable,\nend-to-end learning framework for addressing this task. In particular, we\nconsider a regularized version of the game, equivalent to a particular form of\nquantal response equilibrium, and develop 1) a primal-dual Newton method for\nfinding such equilibrium points in both normal and extensive form games; and 2)\na backpropagation method that lets us analytically compute gradients of all\nrelevant game parameters through the solution itself. This ultimately lets us\nlearn the game by training in an end-to-end fashion, effectively by integrating\na \"differentiable game solver\" into the loop of larger deep network\narchitectures. We demonstrate the effectiveness of the learning method in\nseveral settings including poker and security game tasks.\n",
        "published": "2018",
        "authors": [
            "Chun Kai Ling",
            "Fei Fang",
            "J. Zico Kolter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.04875v1",
        "title": "Multi-user Communication Networks: A Coordinated Multi-armed Bandit\n  Approach",
        "abstract": "  Communication networks shared by many users are a widespread challenge\nnowadays. In this paper we address several aspects of this challenge\nsimultaneously: learning unknown stochastic network characteristics, sharing\nresources with other users while keeping coordination overhead to a minimum.\nThe proposed solution combines Multi-Armed Bandit learning with a lightweight\nsignalling-based coordination scheme, and ensures convergence to a stable\nallocation of resources. Our work considers single-user level algorithms for\ntwo scenarios: an unknown fixed number of users, and a dynamic number of users.\nAnalytic performance guarantees, proving convergence to stable marriage\nconfigurations, are presented for both setups. The algorithms are designed\nbased on a system-wide perspective, rather than focusing on single user\nwelfare. Thus, maximal resource utilization is ensured. An extensive\nexperimental analysis covers convergence to a stable configuration as well as\nreward maximization. Experiments are carried out over a wide range of setups,\ndemonstrating the advantages of our approach over existing state-of-the-art\nmethods.\n",
        "published": "2018",
        "authors": [
            "Orly Avner",
            "Shie Mannor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.06910v2",
        "title": "Scalable Population Synthesis with Deep Generative Modeling",
        "abstract": "  Population synthesis is concerned with the generation of synthetic yet\nrealistic representations of populations. It is a fundamental problem in the\nmodeling of transport where the synthetic populations of micro-agents represent\na key input to most agent-based models. In this paper, a new methodological\nframework for how to 'grow' pools of micro-agents is presented. The model\nframework adopts a deep generative modeling approach from machine learning\nbased on a Variational Autoencoder (VAE). Compared to the previous population\nsynthesis approaches, including Iterative Proportional Fitting (IPF), Gibbs\nsampling and traditional generative models such as Bayesian Networks or Hidden\nMarkov Models, the proposed method allows fitting the full joint distribution\nfor high dimensions. The proposed methodology is compared with a conventional\nGibbs sampler and a Bayesian Network by using a large-scale Danish trip diary.\nIt is shown that, while these two methods outperform the VAE in the\nlow-dimensional case, they both suffer from scalability issues when the number\nof modeled attributes increases. It is also shown that the Gibbs sampler\nessentially replicates the agents from the original sample when the required\nconditional distributions are estimated as frequency tables. In contrast, the\nVAE allows addressing the problem of sampling zeros by generating agents that\nare virtually different from those in the original data but have similar\nstatistical properties. The presented approach can support agent-based modeling\nat all levels by enabling richer synthetic populations with smaller zones and\nmore detailed individual characteristics.\n",
        "published": "2018",
        "authors": [
            "Stanislav S. Borysov",
            "Jeppe Rich",
            "Francisco C. Pereira"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.02423v2",
        "title": "Generalizing the theory of cooperative inference",
        "abstract": "  Cooperation information sharing is important to theories of human learning\nand has potential implications for machine learning. Prior work derived\nconditions for achieving optimal Cooperative Inference given strong, relatively\nrestrictive assumptions. We relax these assumptions by demonstrating\nconvergence for any discrete joint distribution, robustness through equivalence\nclasses and stability under perturbation, and effectiveness by deriving bounds\nfrom structural properties of the original joint distribution. We provide\ngeometric interpretations, connections to and implications for optimal\ntransport, and connections to importance sampling, and conclude by outlining\nopen questions and challenges to realizing the promise of Cooperative\nInference.\n",
        "published": "2018",
        "authors": [
            "Pei Wang",
            "Pushpi Paranamana",
            "Patrick Shafto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.06443v1",
        "title": "Hedging Algorithms and Repeated Matrix Games",
        "abstract": "  Playing repeated matrix games (RMG) while maximizing the cumulative returns\nis a basic method to evaluate multi-agent learning (MAL) algorithms. Previous\nwork has shown that $UCB$, $M3$, $S$ or $Exp3$ algorithms have good behaviours\non average in RMG. Besides, hedging algorithms have been shown to be effective\non prediction problems. An hedging algorithm is made up with a top-level\nalgorithm and a set of basic algorithms. To make its decision, an hedging\nalgorithm uses its top-level algorithm to choose a basic algorithm, and the\nchosen algorithm makes the decision. This paper experimentally shows that\nwell-selected hedging algorithms are better on average than all previous MAL\nalgorithms on the task of playing RMG against various players. $S$ is a very\ngood top-level algorithm, and $UCB$ and $M3$ are very good basic algorithms.\nFurthermore, two-level hedging algorithms are more effective than one-level\nhedging algorithms, and three levels are not better than two levels.\n",
        "published": "2018",
        "authors": [
            "Bruno Bouzy",
            "Marc M\u00e9tivier",
            "Damien Pellier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.08743v5",
        "title": "Quantifying the Burden of Exploration and the Unfairness of Free Riding",
        "abstract": "  We consider the multi-armed bandit setting with a twist. Rather than having\njust one decision maker deciding which arm to pull in each round, we have $n$\ndifferent decision makers (agents). In the simple stochastic setting, we show\nthat a \"free-riding\" agent observing another \"self-reliant\" agent can achieve\njust $O(1)$ regret, as opposed to the regret lower bound of $\\Omega (\\log t)$\nwhen one decision maker is playing in isolation. This result holds whenever the\nself-reliant agent's strategy satisfies either one of two assumptions: (1) each\narm is pulled at least $\\gamma \\ln t$ times in expectation for a constant\n$\\gamma$ that we compute, or (2) the self-reliant agent achieves $o(t)$\nrealized regret with high probability. Both of these assumptions are satisfied\nby standard zero-regret algorithms. Under the second assumption, we further\nshow that the free rider only needs to observe the number of times each arm is\npulled by the self-reliant agent, and not the rewards realized.\n  In the linear contextual setting, each arm has a distribution over parameter\nvectors, each agent has a context vector, and the reward realized when an agent\npulls an arm is the inner product of that agent's context vector with a\nparameter vector sampled from the pulled arm's distribution. We show that the\nfree rider can achieve $O(1)$ regret in this setting whenever the free rider's\ncontext is a small (in $L_2$-norm) linear combination of other agents' contexts\nand all other agents pull each arm $\\Omega (\\log t)$ times with high\nprobability. Again, this condition on the self-reliant players is satisfied by\nstandard zero-regret algorithms like UCB. We also prove a number of lower\nbounds.\n",
        "published": "2018",
        "authors": [
            "Christopher Jung",
            "Sampath Kannan",
            "Neil Lutz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.09162v1",
        "title": "PLOTS: Procedure Learning from Observations using Subtask Structure",
        "abstract": "  In many cases an intelligent agent may want to learn how to mimic a single\nobserved demonstrated trajectory. In this work we consider how to perform such\nprocedural learning from observation, which could help to enable agents to\nbetter use the enormous set of video data on observation sequences. Our\napproach exploits the properties of this setting to incrementally build an open\nloop action plan that can yield the desired subsequence, and can be used in\nboth Markov and partially observable Markov domains. In addition, procedures\ncommonly involve repeated extended temporal action subsequences. Our method\noptimistically explores actions to leverage potential repeated structure in the\nprocedure. In comparing to some state-of-the-art approaches we find that our\nexplicit procedural learning from observation method is about 100 times faster\nthan policy-gradient based approaches that learn a stochastic policy and is\nfaster than model based approaches as well. We also find that performing\noptimistic action selection yields substantial speed ups when latent dynamical\nstructure is present.\n",
        "published": "2019",
        "authors": [
            "Tong Mu",
            "Karan Goel",
            "Emma Brunskill"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.12233v2",
        "title": "Non-Stochastic Multi-Player Multi-Armed Bandits: Optimal Rate With\n  Collision Information, Sublinear Without",
        "abstract": "  We consider the non-stochastic version of the (cooperative) multi-player\nmulti-armed bandit problem. The model assumes no communication at all between\nthe players, and furthermore when two (or more) players select the same action\nthis results in a maximal loss. We prove the first $\\sqrt{T}$-type regret\nguarantee for this problem, under the feedback model where collisions are\nannounced to the colliding players. Such a bound was not known even for the\nsimpler stochastic version. We also prove the first sublinear guarantee for the\nfeedback model where collision information is not available, namely\n$T^{1-\\frac{1}{2m}}$ where $m$ is the number of players.\n",
        "published": "2019",
        "authors": [
            "S\u00e9bastien Bubeck",
            "Yuanzhi Li",
            "Yuval Peres",
            "Mark Sellke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.04121v1",
        "title": "The sharp, the flat and the shallow: Can weakly interacting agents learn\n  to escape bad minima?",
        "abstract": "  An open problem in machine learning is whether flat minima generalize better\nand how to compute such minima efficiently. This is a very challenging problem.\nAs a first step towards understanding this question we formalize it as an\noptimization problem with weakly interacting agents. We review appropriate\nbackground material from the theory of stochastic processes and provide\ninsights that are relevant to practitioners. We propose an algorithmic\nframework for an extended stochastic gradient Langevin dynamics and illustrate\nits potential. The paper is written as a tutorial, and presents an alternative\nuse of multi-agent learning. Our primary focus is on the design of algorithms\nfor machine learning applications; however the underlying mathematical\nframework is suitable for the understanding of large scale systems of agent\nbased models that are popular in the social sciences, economics and finance.\n",
        "published": "2019",
        "authors": [
            "Nikolas Kantas",
            "Panos Parpas",
            "Grigorios A. Pavliotis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.13428v1",
        "title": "Attentional Policies for Cross-Context Multi-Agent Reinforcement\n  Learning",
        "abstract": "  Many potential applications of reinforcement learning in the real world\ninvolve interacting with other agents whose numbers vary over time. We propose\nnew neural policy architectures for these multi-agent problems. In contrast to\nother methods of training an individual, discrete policy for each agent and\nthen enforcing cooperation through some additional inter-policy mechanism, we\nfollow the spirit of recent work on the power of relational inductive biases in\ndeep networks by learning multi-agent relationships at the policy level via an\nattentional architecture. In our method, all agents share the same policy, but\nindependently apply it in their own context to aggregate the other agents'\nstate information when selecting their next action. The structure of our\narchitectures allow them to be applied on environments with varying numbers of\nagents. We demonstrate our architecture on a benchmark multi-agent autonomous\nvehicle coordination problem, obtaining superior results to a full-knowledge,\nfully-centralized reference solution, and significantly outperforming it when\nscaling to large numbers of agents.\n",
        "published": "2019",
        "authors": [
            "Matthew A. Wright",
            "Roberto Horowitz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.01202v1",
        "title": "Learning Transferable Cooperative Behavior in Multi-Agent Teams",
        "abstract": "  While multi-agent interactions can be naturally modeled as a graph, the\nenvironment has traditionally been considered as a black box. We propose to\ncreate a shared agent-entity graph, where agents and environmental entities\nform vertices, and edges exist between the vertices which can communicate with\neach other. Agents learn to cooperate by exchanging messages along the edges of\nthis graph. Our proposed multi-agent reinforcement learning framework is\ninvariant to the number of agents or entities present in the system as well as\npermutation invariance, both of which are desirable properties for any\nmulti-agent system representation. We present state-of-the-art results on\ncoverage, formation and line control tasks for multi-agent teams in a fully\ndecentralized framework and further show that the learned policies quickly\ntransfer to scenarios with different team sizes along with strong zero-shot\ngeneralization performance. This is an important step towards developing\nmulti-agent teams which can be realistically deployed in the real world without\nassuming complete prior knowledge or instantaneous communication at unbounded\ndistances.\n",
        "published": "2019",
        "authors": [
            "Akshat Agarwal",
            "Sumit Kumar",
            "Katia Sycara"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.02330v1",
        "title": "Finding Friend and Foe in Multi-Agent Games",
        "abstract": "  Recent breakthroughs in AI for multi-agent games like Go, Poker, and Dota,\nhave seen great strides in recent years. Yet none of these games address the\nreal-life challenge of cooperation in the presence of unknown and uncertain\nteammates. This challenge is a key game mechanism in hidden role games. Here we\ndevelop the DeepRole algorithm, a multi-agent reinforcement learning agent that\nwe test on The Resistance: Avalon, the most popular hidden role game. DeepRole\ncombines counterfactual regret minimization (CFR) with deep value networks\ntrained through self-play. Our algorithm integrates deductive reasoning into\nvector-form CFR to reason about joint beliefs and deduce partially observable\nactions. We augment deep value networks with constraints that yield\ninterpretable representations of win probabilities. These innovations enable\nDeepRole to scale to the full Avalon game. Empirical game-theoretic methods\nshow that DeepRole outperforms other hand-crafted and learned agents in\nfive-player Avalon. DeepRole played with and against human players on the web\nin hybrid human-agent teams. We find that DeepRole outperforms human players as\nboth a cooperator and a competitor.\n",
        "published": "2019",
        "authors": [
            "Jack Serrino",
            "Max Kleiman-Weiner",
            "David C. Parkes",
            "Joshua B. Tenenbaum"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.05363v2",
        "title": "Competing Bandits in Matching Markets",
        "abstract": "  Stable matching, a classical model for two-sided markets, has long been\nstudied with little consideration for how each side's preferences are learned.\nWith the advent of massive online markets powered by data-driven matching\nplatforms, it has become necessary to better understand the interplay between\nlearning and market objectives. We propose a statistical learning model in\nwhich one side of the market does not have a priori knowledge about its\npreferences for the other side and is required to learn these from stochastic\nrewards. Our model extends the standard multi-armed bandits framework to\nmultiple players, with the added feature that arms have preferences over\nplayers. We study both centralized and decentralized approaches to this problem\nand show surprising exploration-exploitation trade-offs compared to the single\nplayer multi-armed bandits setting.\n",
        "published": "2019",
        "authors": [
            "Lydia T. Liu",
            "Horia Mania",
            "Michael I. Jordan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.09248v2",
        "title": "Privacy Preserving QoE Modeling using Collaborative Learning",
        "abstract": "  Machine Learning based Quality of Experience (QoE) models potentially suffer\nfrom over-fitting due to limitations including low data volume, and limited\nparticipant profiles. This prevents models from becoming generic. Consequently,\nthese trained models may under-perform when tested outside the experimented\npopulation. One reason for the limited datasets, which we refer in this paper\nas small QoE data lakes, is due to the fact that often these datasets\npotentially contain user sensitive information and are only collected\nthroughout expensive user studies with special user consent. Thus, sharing of\ndatasets amongst researchers is often not allowed. In recent years, privacy\npreserving machine learning models have become important and so have techniques\nthat enable model training without sharing datasets but instead relying on\nsecure communication protocols. Following this trend, in this paper, we present\nRound-Robin based Collaborative Machine Learning model training, where the\nmodel is trained in a sequential manner amongst the collaborated partner nodes.\nWe benchmark this work using our customized Federated Learning mechanism as\nwell as conventional Centralized and Isolated Learning methods.\n",
        "published": "2019",
        "authors": [
            "Selim Ickin",
            "Konstantinos Vandikas",
            "Markus Fiedler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.04421v4",
        "title": "Privacy-Preserving Bandits",
        "abstract": "  Contextual bandit algorithms~(CBAs) often rely on personal data to provide\nrecommendations. Centralized CBA agents utilize potentially sensitive data from\nrecent interactions to provide personalization to end-users. Keeping the\nsensitive data locally, by running a local agent on the user's device, protects\nthe user's privacy, however, the agent requires longer to produce useful\nrecommendations, as it does not leverage feedback from other users. This paper\nproposes a technique we call Privacy-Preserving Bandits (P2B); a system that\nupdates local agents by collecting feedback from other local agents in a\ndifferentially-private manner. Comparisons of our proposed approach with a\nnon-private, as well as a fully-private (local) system, show competitive\nperformance on both synthetic benchmarks and real-world data. Specifically, we\nobserved only a decrease of 2.6% and 3.6% in multi-label classification\naccuracy, and a CTR increase of 0.0025 in online advertising for a privacy\nbudget $\\epsilon \\approx 0.693$. These results suggest P2B is an effective\napproach to challenges arising in on-device privacy-preserving personalization.\n",
        "published": "2019",
        "authors": [
            "Mohammad Malekzadeh",
            "Dimitrios Athanasakis",
            "Hamed Haddadi",
            "Benjamin Livshits"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.08540v2",
        "title": "No-Regret Learning in Unknown Games with Correlated Payoffs",
        "abstract": "  We consider the problem of learning to play a repeated multi-agent game with\nan unknown reward function. Single player online learning algorithms attain\nstrong regret bounds when provided with full information feedback, which\nunfortunately is unavailable in many real-world scenarios. Bandit feedback\nalone, i.e., observing outcomes only for the selected action, yields\nsubstantially worse performance. In this paper, we consider a natural model\nwhere, besides a noisy measurement of the obtained reward, the player can also\nobserve the opponents' actions. This feedback model, together with a regularity\nassumption on the reward function, allows us to exploit the correlations among\ndifferent game outcomes by means of Gaussian processes (GPs). We propose a\nnovel confidence-bound based bandit algorithm GP-MW, which utilizes the GP\nmodel for the reward function and runs a multiplicative weight (MW) method. We\nobtain novel kernel-dependent regret bounds that are comparable to the known\nbounds in the full information setting, while substantially improving upon the\nexisting bandit results. We experimentally demonstrate the effectiveness of\nGP-MW in random matrix games, as well as real-world problems of traffic routing\nand movie recommendation. In our experiments, GP-MW consistently outperforms\nseveral baselines, while its performance is often comparable to methods that\nhave access to full information feedback.\n",
        "published": "2019",
        "authors": [
            "Pier Giuseppe Sessa",
            "Ilija Bogunovic",
            "Maryam Kamgarpour",
            "Andreas Krause"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.10651v1",
        "title": "Integrating independent and centralized multi-agent reinforcement\n  learning for traffic signal network optimization",
        "abstract": "  Traffic congestion in metropolitan areas is a world-wide problem that can be\nameliorated by traffic lights that respond dynamically to real-time conditions.\nRecent studies applying deep reinforcement learning (RL) to optimize single\ntraffic lights have shown significant improvement over conventional control.\nHowever, optimization of global traffic condition over a large road network\nfundamentally is a cooperative multi-agent control problem, for which\nsingle-agent RL is not suitable due to environment non-stationarity and\ninfeasibility of optimizing over an exponential joint-action space. Motivated\nby these challenges, we propose QCOMBO, a simple yet effective multi-agent\nreinforcement learning (MARL) algorithm that combines the advantages of\nindependent and centralized learning. We ensure scalability by selecting\nactions from individually optimized utility functions, which are shaped to\nmaximize global performance via a novel consistency regularization loss between\nindividual utility and a global action-value function. Experiments on diverse\nroad topologies and traffic flow conditions in the SUMO traffic simulator show\ncompetitive performance of QCOMBO versus recent state-of-the-art MARL\nalgorithms. We further show that policies trained on small sub-networks can\neffectively generalize to larger networks under different traffic flow\nconditions, providing empirical evidence for the suitability of MARL for\nintelligent traffic control.\n",
        "published": "2019",
        "authors": [
            "Zhi Zhang",
            "Jiachen Yang",
            "Hongyuan Zha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.05438v1",
        "title": "Learning to Communicate in Multi-Agent Reinforcement Learning : A Review",
        "abstract": "  We consider the issue of multiple agents learning to communicate through\nreinforcement learning within partially observable environments, with a focus\non information asymmetry in the second part of our work. We provide a review of\nthe recent algorithms developed to improve the agents' policy by allowing the\nsharing of information between agents and the learning of communication\nstrategies, with a focus on Deep Recurrent Q-Network-based models. We also\ndescribe recent efforts to interpret the languages generated by these agents\nand study their properties in an attempt to generate human-language-like\nsentences. We discuss the metrics used to evaluate the generated communication\nstrategies and propose a novel entropy-based evaluation metric. Finally, we\naddress the issue of the cost of communication and introduce the idea of an\nexperimental setup to expose this cost in cooperative-competitive game.\n",
        "published": "2019",
        "authors": [
            "Mohamed Salah Za\u00efem",
            "Etienne Bennequin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.05706v3",
        "title": "Sequential Cooperative Bayesian Inference",
        "abstract": "  Cooperation is often implicitly assumed when learning from other agents.\nCooperation implies that the agent selecting the data, and the agent learning\nfrom the data, have the same goal, that the learner infer the intended\nhypothesis. Recent models in human and machine learning have demonstrated the\npossibility of cooperation. We seek foundational theoretical results for\ncooperative inference by Bayesian agents through sequential data. We develop\nnovel approaches analyzing consistency, rate of convergence and stability of\nSequential Cooperative Bayesian Inference (SCBI). Our analysis of the\neffectiveness, sample efficiency and robustness show that cooperation is not\nonly possible in specific instances but theoretically well-founded in general.\nWe discuss implications for human-human and human-machine cooperation.\n",
        "published": "2020",
        "authors": [
            "Junqi Wang",
            "Pei Wang",
            "Patrick Shafto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.06723v3",
        "title": "Reward Design for Driver Repositioning Using Multi-Agent Reinforcement\n  Learning",
        "abstract": "  A large portion of passenger requests is reportedly unserviced, partially due\nto vacant for-hire drivers' cruising behavior during the passenger seeking\nprocess. This paper aims to model the multi-driver repositioning task through a\nmean field multi-agent reinforcement learning (MARL) approach that captures\ncompetition among multiple agents. Because the direct application of MARL to\nthe multi-driver system under a given reward mechanism will likely yield a\nsuboptimal equilibrium due to the selfishness of drivers, this study proposes a\nreward design scheme with which a more desired equilibrium can be reached. To\neffectively solve the bilevel optimization problem with upper level as the\nreward design and the lower level as a multi-agent system, a Bayesian\noptimization (BO) algorithm is adopted to speed up the learning process. We\nthen apply the bilevel optimization model to two case studies, namely,\ne-hailing driver repositioning under service charge and multiclass taxi driver\nrepositioning under NYC congestion pricing. In the first case study, the model\nis validated by the agreement between the derived optimal control from BO and\nthat from an analytical solution. With a simple piecewise linear service\ncharge, the objective of the e-hailing platform can be increased by 8.4%. In\nthe second case study, an optimal toll charge of $5.1 is solved using BO, which\nimproves the objective of city planners by 7.9%, compared to that without any\ntoll charge. Under this optimal toll charge, the number of taxis in the NYC\ncentral business district is decreased, indicating a better traffic condition,\nwithout substantially increasing the crowdedness of the subway system.\n",
        "published": "2020",
        "authors": [
            "Zhenyu Shou",
            "Xuan Di"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.07596v2",
        "title": "Coordination without communication: optimal regret in two players\n  multi-armed bandits",
        "abstract": "  We consider two agents playing simultaneously the same stochastic three-armed\nbandit problem. The two agents are cooperating but they cannot communicate. We\npropose a strategy with no collisions at all between the players (with very\nhigh probability), and with near-optimal regret $O(\\sqrt{T \\log(T)})$. We also\nargue that the extra logarithmic term $\\sqrt{\\log(T)}$ should be necessary by\nproving a lower bound for a full information variant of the problem.\n",
        "published": "2020",
        "authors": [
            "S\u00e9bastien Bubeck",
            "Thomas Budzinski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.08567v3",
        "title": "Multi-Agent Meta-Reinforcement Learning for Self-Powered and Sustainable\n  Edge Computing Systems",
        "abstract": "  The stringent requirements of mobile edge computing (MEC) applications and\nfunctions fathom the high capacity and dense deployment of MEC hosts to the\nupcoming wireless networks. However, operating such high capacity MEC hosts can\nsignificantly increase energy consumption. Thus, a base station (BS) unit can\nact as a self-powered BS. In this paper, an effective energy dispatch mechanism\nfor self-powered wireless networks with edge computing capabilities is studied.\nFirst, a two-stage linear stochastic programming problem is formulated with the\ngoal of minimizing the total energy consumption cost of the system while\nfulfilling the energy demand. Second, a semi-distributed data-driven solution\nis proposed by developing a novel multi-agent meta-reinforcement learning\n(MAMRL) framework to solve the formulated problem. In particular, each BS plays\nthe role of a local agent that explores a Markovian behavior for both energy\nconsumption and generation while each BS transfers time-varying features to a\nmeta-agent. Sequentially, the meta-agent optimizes (i.e., exploits) the energy\ndispatch decision by accepting only the observations from each local agent with\nits own state information. Meanwhile, each BS agent estimates its own energy\ndispatch policy by applying the learned parameters from meta-agent. Finally,\nthe proposed MAMRL framework is benchmarked by analyzing deterministic,\nasymmetric, and stochastic environments in terms of non-renewable energy\nusages, energy cost, and accuracy. Experimental results show that the proposed\nMAMRL model can reduce up to 11% non-renewable energy usage and by 22.4% the\nenergy cost (with 95.8% prediction accuracy), compared to other baseline\nmethods.\n",
        "published": "2020",
        "authors": [
            "Md. Shirajum Munir",
            "Nguyen H. Tran",
            "Walid Saad",
            "Choong Seon Hong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.10113v4",
        "title": "Alternating the Population and Control Neural Networks to Solve\n  High-Dimensional Stochastic Mean-Field Games",
        "abstract": "  We present APAC-Net, an alternating population and agent control neural\nnetwork for solving stochastic mean field games (MFGs). Our algorithm is geared\ntoward high-dimensional instances of MFGs that are beyond reach with existing\nsolution methods. We achieve this in two steps. First, we take advantage of the\nunderlying variational primal-dual structure that MFGs exhibit and phrase it as\na convex-concave saddle point problem. Second, we parameterize the value and\ndensity functions by two neural networks, respectively. By phrasing the problem\nin this manner, solving the MFG can be interpreted as a special case of\ntraining a generative adversarial network (GAN). We show the potential of our\nmethod on up to 100-dimensional MFG problems.\n",
        "published": "2020",
        "authors": [
            "Alex Tong Lin",
            "Samy Wu Fung",
            "Wuchen Li",
            "Levon Nurbekyan",
            "Stanley J. Osher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.05441v2",
        "title": "Delay-Aware Multi-Agent Reinforcement Learning for Cooperative and\n  Competitive Environments",
        "abstract": "  Action and observation delays exist prevalently in the real-world\ncyber-physical systems which may pose challenges in reinforcement learning\ndesign. It is particularly an arduous task when handling multi-agent systems\nwhere the delay of one agent could spread to other agents. To resolve this\nproblem, this paper proposes a novel framework to deal with delays as well as\nthe non-stationary training issue of multi-agent tasks with model-free deep\nreinforcement learning. We formally define the Delay-Aware Markov Game that\nincorporates the delays of all agents in the environment. To solve Delay-Aware\nMarkov Games, we apply centralized training and decentralized execution that\nallows agents to use extra information to ease the non-stationarity issue of\nthe multi-agent systems during training, without the need of a centralized\ncontroller during execution. Experiments are conducted in multi-agent particle\nenvironments including cooperative communication, cooperative navigation, and\ncompetitive experiments. We also test the proposed algorithm in traffic\nscenarios that require coordination of all autonomous vehicles to show the\npractical value of delay-awareness. Results show that the proposed delay-aware\nmulti-agent reinforcement learning algorithm greatly alleviates the performance\ndegradation introduced by delay. Codes and demo videos are available at:\nhttps://github.com/baimingc/delay-aware-MARL.\n",
        "published": "2020",
        "authors": [
            "Baiming Chen",
            "Mengdi Xu",
            "Zuxin Liu",
            "Liang Li",
            "Ding Zhao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.04197v5",
        "title": "QR-MIX: Distributional Value Function Factorisation for Cooperative\n  Multi-Agent Reinforcement Learning",
        "abstract": "  In Cooperative Multi-Agent Reinforcement Learning (MARL) and under the\nsetting of Centralized Training with Decentralized Execution (CTDE), agents\nobserve and interact with their environment locally and independently. With\nlocal observation and random sampling, the randomness in rewards and\nobservations leads to randomness in long-term returns. Existing methods such as\nValue Decomposition Network (VDN) and QMIX estimate the value of long-term\nreturns as a scalar that does not contain the information of randomness. Our\nproposed model QR-MIX introduces quantile regression, modeling joint\nstate-action values as a distribution, combining QMIX with Implicit Quantile\nNetwork (IQN). However, the monotonicity in QMIX limits the expression of joint\nstate-action value distribution and may lead to incorrect estimation results in\nnon-monotonic cases. Therefore, we proposed a flexible loss function to\napproximate the monotonicity found in QMIX. Our model is not only more tolerant\nof the randomness of returns, but also more tolerant of the randomness of\nmonotonic constraints. The experimental results demonstrate that QR-MIX\noutperforms the previous state-of-the-art method QMIX in the StarCraft\nMulti-Agent Challenge (SMAC) environment.\n",
        "published": "2020",
        "authors": [
            "Jian Hu",
            "Seth Austin Harding",
            "Haibin Wu",
            "Siyue Hu",
            "Shih-wei Liao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.06227v1",
        "title": "Teaching to Learn: Sequential Teaching of Agents with Inner States",
        "abstract": "  In sequential machine teaching, a teacher's objective is to provide the\noptimal sequence of inputs to sequential learners in order to guide them\ntowards the best model. In this paper we extend this setting from current\nstatic one-data-set analyses to learners which change their learning algorithm\nor latent state to improve during learning, and to generalize to new datasets.\nWe introduce a multi-agent formulation in which learners' inner state may\nchange with the teaching interaction, which affects the learning performance in\nfuture tasks. In order to teach such learners, we propose an optimal control\napproach that takes the future performance of the learner after teaching into\naccount. This provides tools for modelling learners having inner states, and\nmachine teaching of meta-learning algorithms. Furthermore, we distinguish\nmanipulative teaching, which can be done by effectively hiding data and also\nused for indoctrination, from more general education which aims to help the\nlearner become better at generalization and learning in new datasets in the\nabsence of a teacher.\n",
        "published": "2020",
        "authors": [
            "Mustafa Mert Celikok",
            "Pierre-Alexandre Murena",
            "Samuel Kaski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.09842v4",
        "title": "Energy-based Surprise Minimization for Multi-Agent Value Factorization",
        "abstract": "  Multi-Agent Reinforcement Learning (MARL) has demonstrated significant\nsuccess in training decentralised policies in a centralised manner by making\nuse of value factorization methods. However, addressing surprise across\nspurious states and approximation bias remain open problems for multi-agent\nsettings. Towards this goal, we introduce the Energy-based MIXer (EMIX), an\nalgorithm which minimizes surprise utilizing the energy across agents. Our\ncontributions are threefold; (1) EMIX introduces a novel surprise minimization\ntechnique across multiple agents in the case of multi-agent\npartially-observable settings. (2) EMIX highlights a practical use of energy\nfunctions in MARL with theoretical guarantees and experiment validations of the\nenergy operator. Lastly, (3) EMIX extends Maxmin Q-learning for addressing\noverestimation bias across agents in MARL. In a study of challenging StarCraft\nII micromanagement scenarios, EMIX demonstrates consistent stable performance\nfor multiagent surprise minimization. Moreover, our ablation study highlights\nthe necessity of the energy-based scheme and the need for elimination of\noverestimation bias in MARL. Our implementation of EMIX can be found at\nkarush17.github.io/emix-web/.\n",
        "published": "2020",
        "authors": [
            "Karush Suri",
            "Xiao Qi Shi",
            "Konstantinos Plataniotis",
            "Yuri Lawryshyn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.14471v7",
        "title": "PettingZoo: Gym for Multi-Agent Reinforcement Learning",
        "abstract": "  This paper introduces the PettingZoo library and the accompanying Agent\nEnvironment Cycle (\"AEC\") games model. PettingZoo is a library of diverse sets\nof multi-agent environments with a universal, elegant Python API. PettingZoo\nwas developed with the goal of accelerating research in Multi-Agent\nReinforcement Learning (\"MARL\"), by making work more interchangeable,\naccessible and reproducible akin to what OpenAI's Gym library did for\nsingle-agent reinforcement learning. PettingZoo's API, while inheriting many\nfeatures of Gym, is unique amongst MARL APIs in that it's based around the\nnovel AEC games model. We argue, in part through case studies on major problems\nin popular MARL environments, that the popular game models are poor conceptual\nmodels of games commonly used in MARL and accordingly can promote confusing\nbugs that are hard to detect, and that the AEC games model addresses these\nproblems.\n",
        "published": "2020",
        "authors": [
            "J. K. Terry",
            "Benjamin Black",
            "Nathaniel Grammel",
            "Mario Jayakumar",
            "Ananth Hari",
            "Ryan Sullivan",
            "Luis Santos",
            "Rodrigo Perez",
            "Caroline Horsch",
            "Clemens Dieffendahl",
            "Niall L. Williams",
            "Yashas Lokesh",
            "Praveen Ravi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.03896v1",
        "title": "Cooperative and Stochastic Multi-Player Multi-Armed Bandit: Optimal\n  Regret With Neither Communication Nor Collisions",
        "abstract": "  We consider the cooperative multi-player version of the stochastic\nmulti-armed bandit problem. We study the regime where the players cannot\ncommunicate but have access to shared randomness. In prior work by the first\ntwo authors, a strategy for this regime was constructed for two players and\nthree arms, with regret $\\tilde{O}(\\sqrt{T})$, and with no collisions at all\nbetween the players (with very high probability). In this paper we show that\nthese properties (near-optimal regret and no collisions at all) are achievable\nfor any number of players and arms. At a high level, the previous strategy\nheavily relied on a $2$-dimensional geometric intuition that was difficult to\ngeneralize in higher dimensions, while here we take a more combinatorial route\nto build the new strategy.\n",
        "published": "2020",
        "authors": [
            "S\u00e9bastien Bubeck",
            "Thomas Budzinski",
            "Mark Sellke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.07348v4",
        "title": "Bandit Learning in Decentralized Matching Markets",
        "abstract": "  We study two-sided matching markets in which one side of the market (the\nplayers) does not have a priori knowledge about its preferences for the other\nside (the arms) and is required to learn its preferences from experience. Also,\nwe assume the players have no direct means of communication. This model extends\nthe standard stochastic multi-armed bandit framework to a decentralized\nmultiple player setting with competition. We introduce a new algorithm for this\nsetting that, over a time horizon $T$, attains $\\mathcal{O}(\\log(T))$ stable\nregret when preferences of the arms over players are shared, and\n$\\mathcal{O}(\\log(T)^2)$ regret when there are no assumptions on the\npreferences on either side. Moreover, in the setting where a single player may\ndeviate, we show that the algorithm is incentive compatible whenever the arms'\npreferences are shared, but not necessarily so when preferences are fully\ngeneral.\n",
        "published": "2020",
        "authors": [
            "Lydia T. Liu",
            "Feng Ruan",
            "Horia Mania",
            "Michael I. Jordan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.06246v2",
        "title": "Regret, stability & fairness in matching markets with bandit learners",
        "abstract": "  Making an informed decision -- for example, when choosing a career or housing\n-- requires knowledge about the available options. Such knowledge is generally\nacquired through costly trial and error, but this learning process can be\ndisrupted by competition. In this work, we study how competition affects the\nlong-term outcomes of individuals as they learn. We build on a line of work\nthat models this setting as a two-sided matching market with bandit learners. A\nrecent result in this area states that it is impossible to simultaneously\nguarantee two natural desiderata: stability and low optimal regret for all\nagents. Resource-allocating platforms can point to this result as a\njustification for assigning good long-term outcomes to some agents and poor\nones to others. We show that this impossibility need not hold true. In\nparticular, by modeling two additional components of competition -- namely,\ncosts and transfers -- we prove that it is possible to simultaneously guarantee\nfour desiderata: stability, low optimal regret, fairness in the distribution of\nregret, and high social welfare.\n",
        "published": "2021",
        "authors": [
            "Sarah H. Cen",
            "Devavrat Shah"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.04972v1",
        "title": "Provably Efficient Cooperative Multi-Agent Reinforcement Learning with\n  Function Approximation",
        "abstract": "  Reinforcement learning in cooperative multi-agent settings has recently\nadvanced significantly in its scope, with applications in cooperative\nestimation for advertising, dynamic treatment regimes, distributed control, and\nfederated learning. In this paper, we discuss the problem of cooperative\nmulti-agent RL with function approximation, where a group of agents\ncommunicates with each other to jointly solve an episodic MDP. We demonstrate\nthat via careful message-passing and cooperative value iteration, it is\npossible to achieve near-optimal no-regret learning even with a fixed constant\ncommunication budget. Next, we demonstrate that even in heterogeneous\ncooperative settings, it is possible to achieve Pareto-optimal no-regret\nlearning with limited communication. Our work generalizes several ideas from\nthe multi-agent contextual and multi-armed bandit literature to MDPs and\nreinforcement learning.\n",
        "published": "2021",
        "authors": [
            "Abhimanyu Dubey",
            "Alex Pentland"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.15664v1",
        "title": "Competing Adaptive Networks",
        "abstract": "  Adaptive networks have the capability to pursue solutions of global\nstochastic optimization problems by relying only on local interactions within\nneighborhoods. The diffusion of information through repeated interactions\nallows for globally optimal behavior, without the need for central\ncoordination. Most existing strategies are developed for cooperative learning\nsettings, where the objective of the network is common to all agents. We\nconsider in this work a team setting, where a subset of the agents form a team\nwith a common goal while competing with the remainder of the network. We\ndevelop an algorithm for decentralized competition among teams of adaptive\nagents, analyze its dynamics and present an application in the decentralized\ntraining of generative adversarial neural networks.\n",
        "published": "2021",
        "authors": [
            "Stefan Vlaski",
            "Ali H. Sayed"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.09034v1",
        "title": "Greedy UnMixing for Q-Learning in Multi-Agent Reinforcement Learning",
        "abstract": "  This paper introduces Greedy UnMix (GUM) for cooperative multi-agent\nreinforcement learning (MARL). Greedy UnMix aims to avoid scenarios where MARL\nmethods fail due to overestimation of values as part of the large joint\nstate-action space. It aims to address this through a conservative Q-learning\napproach through restricting the state-marginal in the dataset to avoid\nunobserved joint state action spaces, whilst concurrently attempting to unmix\nor simplify the problem space under the centralized training with decentralized\nexecution paradigm. We demonstrate the adherence to Q-function lower bounds in\nthe Q-learning for MARL scenarios, and demonstrate superior performance to\nexisting Q-learning MARL approaches as well as more general MARL algorithms\nover a set of benchmark MARL tasks, despite its relative simplicity compared\nwith state-of-the-art approaches.\n",
        "published": "2021",
        "authors": [
            "Chapman Siu",
            "Jason Traish",
            "Richard Yi Da Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.09038v1",
        "title": "Regularize! Don't Mix: Multi-Agent Reinforcement Learning without\n  Explicit Centralized Structures",
        "abstract": "  We propose using regularization for Multi-Agent Reinforcement Learning rather\nthan learning explicit cooperative structures called {\\em Multi-Agent\nRegularized Q-learning} (MARQ). Many MARL approaches leverage centralized\nstructures in order to exploit global state information or removing\ncommunication constraints when the agents act in a decentralized manner.\nInstead of learning redundant structures which is removed during agent\nexecution, we propose instead to leverage shared experiences of the agents to\nregularize the individual policies in order to promote structured exploration.\nWe examine several different approaches to how MARQ can either explicitly or\nimplicitly regularize our policies in a multi-agent setting. MARQ aims to\naddress these limitations in the MARL context through applying regularization\nconstraints which can correct bias in off-policy out-of-distribution agent\nexperiences and promote diverse exploration. Our algorithm is evaluated on\nseveral benchmark multi-agent environments and we show that MARQ consistently\noutperforms several baselines and state-of-the-art algorithms; learning in\nfewer steps and converging to higher returns.\n",
        "published": "2021",
        "authors": [
            "Chapman Siu",
            "Jason Traish",
            "Richard Yi Da Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.00076v2",
        "title": "Robust Multi-Agent Bandits Over Undirected Graphs",
        "abstract": "  We consider a multi-agent multi-armed bandit setting in which $n$ honest\nagents collaborate over a network to minimize regret but $m$ malicious agents\ncan disrupt learning arbitrarily. Assuming the network is the complete graph,\nexisting algorithms incur $O( (m + K/n) \\log (T) / \\Delta )$ regret in this\nsetting, where $K$ is the number of arms and $\\Delta$ is the arm gap. For $m\n\\ll K$, this improves over the single-agent baseline regret of\n$O(K\\log(T)/\\Delta)$.\n  In this work, we show the situation is murkier beyond the case of a complete\ngraph. In particular, we prove that if the state-of-the-art algorithm is used\non the undirected line graph, honest agents can suffer (nearly) linear regret\nuntil time is doubly exponential in $K$ and $n$. In light of this negative\nresult, we propose a new algorithm for which the $i$-th agent has regret $O( (\nd_{\\text{mal}}(i) + K/n) \\log(T)/\\Delta)$ on any connected and undirected\ngraph, where $d_{\\text{mal}}(i)$ is the number of $i$'s neighbors who are\nmalicious. Thus, we generalize existing regret bounds beyond the complete graph\n(where $d_{\\text{mal}}(i) = m$), and show the effect of malicious agents is\nentirely local (in the sense that only the $d_{\\text{mal}}(i)$ malicious agents\ndirectly connected to $i$ affect its long-term regret).\n",
        "published": "2022",
        "authors": [
            "Daniel Vial",
            "Sanjay Shakkottai",
            "R. Srikant"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.01696v2",
        "title": "Fail-Safe Adversarial Generative Imitation Learning",
        "abstract": "  For flexible yet safe imitation learning (IL), we propose theory and a\nmodular method, with a safety layer that enables a closed-form probability\ndensity/gradient of the safe generative continuous policy, end-to-end\ngenerative adversarial training, and worst-case safety guarantees. The safety\nlayer maps all actions into a set of safe actions, and uses the\nchange-of-variables formula plus additivity of measures for the density. The\nset of safe actions is inferred by first checking safety of a finite sample of\nactions via adversarial reachability analysis of fallback maneuvers, and then\nconcluding on the safety of these actions' neighborhoods using, e.g., Lipschitz\ncontinuity. We provide theoretical analysis showing the robustness advantage of\nusing the safety layer already during training (imitation error linear in the\nhorizon) compared to only using it at test time (up to quadratic error). In an\nexperiment on real-world driver interaction data, we empirically demonstrate\ntractability, safety and imitation performance of our approach.\n",
        "published": "2022",
        "authors": [
            "Philipp Geiger",
            "Christoph-Nikolas Straehle"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.03021v1",
        "title": "Hierarchically Structured Scheduling and Execution of Tasks in a\n  Multi-Agent Environment",
        "abstract": "  In a warehouse environment, tasks appear dynamically. Consequently, a task\nmanagement system that matches them with the workforce too early (e.g., weeks\nin advance) is necessarily sub-optimal. Also, the rapidly increasing size of\nthe action space of such a system consists of a significant problem for\ntraditional schedulers. Reinforcement learning, however, is suited to deal with\nissues requiring making sequential decisions towards a long-term, often remote,\ngoal. In this work, we set ourselves on a problem that presents itself with a\nhierarchical structure: the task-scheduling, by a centralised agent, in a\ndynamic warehouse multi-agent environment and the execution of one such\nschedule, by decentralised agents with only partial observability thereof. We\npropose to use deep reinforcement learning to solve both the high-level\nscheduling problem and the low-level multi-agent problem of schedule execution.\nFinally, we also conceive the case where centralisation is impossible at test\ntime and workers must learn how to cooperate in executing the tasks in an\nenvironment with no schedule and only partial observability.\n",
        "published": "2022",
        "authors": [
            "Diogo S. Carvalho",
            "Biswa Sengupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.07092v1",
        "title": "The Multi-Agent Pickup and Delivery Problem: MAPF, MARL and Its\n  Warehouse Applications",
        "abstract": "  We study two state-of-the-art solutions to the multi-agent pickup and\ndelivery (MAPD) problem based on different principles -- multi-agent\npath-finding (MAPF) and multi-agent reinforcement learning (MARL).\nSpecifically, a recent MAPF algorithm called conflict-based search (CBS) and a\ncurrent MARL algorithm called shared experience actor-critic (SEAC) are\nstudied. While the performance of these algorithms is measured using quite\ndifferent metrics in their separate lines of work, we aim to benchmark these\ntwo methods comprehensively in a simulated warehouse automation environment.\n",
        "published": "2022",
        "authors": [
            "Tim Tsz-Kit Lau",
            "Biswa Sengupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.00586v1",
        "title": "Robust and Efficient Aggregation for Distributed Learning",
        "abstract": "  Distributed learning paradigms, such as federated and decentralized learning,\nallow for the coordination of models across a collection of agents, and without\nthe need to exchange raw data. Instead, agents compute model updates locally\nbased on their available data, and subsequently share the update model with a\nparameter server or their peers. This is followed by an aggregation step, which\ntraditionally takes the form of a (weighted) average. Distributed learning\nschemes based on averaging are known to be susceptible to outliers. A single\nmalicious agent is able to drive an averaging-based distributed learning\nalgorithm to an arbitrarily poor model. This has motivated the development of\nrobust aggregation schemes, which are based on variations of the median and\ntrimmed mean. While such procedures ensure robustness to outliers and malicious\nbehavior, they come at the cost of significantly reduced sample efficiency.\nThis means that current robust aggregation schemes require significantly higher\nagent participation rates to achieve a given level of performance than their\nmean-based counterparts in non-contaminated settings. In this work we remedy\nthis drawback by developing statistically efficient and robust aggregation\nschemes for distributed learning.\n",
        "published": "2022",
        "authors": [
            "Stefan Vlaski",
            "Christian Schroth",
            "Michael Muma",
            "Abdelhak M. Zoubir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.01155v1",
        "title": "Byzantine-Robust Federated Linear Bandits",
        "abstract": "  In this paper, we study a linear bandit optimization problem in a federated\nsetting where a large collection of distributed agents collaboratively learn a\ncommon linear bandit model. Standard federated learning algorithms applied to\nthis setting are vulnerable to Byzantine attacks on even a small fraction of\nagents. We propose a novel algorithm with a robust aggregation oracle that\nutilizes the geometric median. We prove that our proposed algorithm is robust\nto Byzantine attacks on fewer than half of agents and achieves a sublinear\n$\\tilde{\\mathcal{O}}({T^{3/4}})$ regret with $\\mathcal{O}(\\sqrt{T})$ steps of\ncommunication in $T$ steps. Moreover, we make our algorithm differentially\nprivate via a tree-based mechanism. Finally, if the level of corruption is\nknown to be small, we show that using the geometric median of mean oracle for\nrobust aggregation further improves the regret bound.\n",
        "published": "2022",
        "authors": [
            "Ali Jadbabaie",
            "Haochuan Li",
            "Jian Qian",
            "Yi Tian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.03699v2",
        "title": "Rate-Optimal Contextual Online Matching Bandit",
        "abstract": "  Two-sided online matching platforms have been employed in various markets.\nHowever, agents' preferences in present market are usually implicit and unknown\nand must be learned from data. With the growing availability of side\ninformation involved in the decision process, modern online matching\nmethodology demands the capability to track preference dynamics for agents\nbased on their contextual information. This motivates us to consider a novel\nContextual Online Matching Bandit prOblem (COMBO), which allows dynamic\npreferences in matching decisions. Existing works focus on multi-armed bandit\nwith static preference, but this is insufficient: the two-sided preference\nchanges as along as one-side's contextual information updates, resulting in\nnon-static matching. In this paper, we propose a Centralized Contextual -\nExplore Then Commit (CC-ETC) algorithm to adapt to the COMBO. CC-ETC solves\nonline matching with dynamic preference. In theory, we show that CC-ETC\nachieves a sublinear regret upper bound O(log(T)) and is a rate-optimal\nalgorithm by proving a matching lower bound. In the experiments, we demonstrate\nthat CC-ETC is robust to variant preference schemes, dimensions of contexts,\nreward noise levels, and contexts variation levels.\n",
        "published": "2022",
        "authors": [
            "Yuantong Li",
            "Chi-hua Wang",
            "Guang Cheng",
            "Will Wei Sun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.14174v1",
        "title": "Private and Byzantine-Proof Cooperative Decision-Making",
        "abstract": "  The cooperative bandit problem is a multi-agent decision problem involving a\ngroup of agents that interact simultaneously with a multi-armed bandit, while\ncommunicating over a network with delays. The central idea in this problem is\nto design algorithms that can efficiently leverage communication to obtain\nimprovements over acting in isolation. In this paper, we investigate the\nstochastic bandit problem under two settings - (a) when the agents wish to make\ntheir communication private with respect to the action sequence, and (b) when\nthe agents can be byzantine, i.e., they provide (stochastically) incorrect\ninformation. For both these problem settings, we provide upper-confidence bound\nalgorithms that obtain optimal regret while being (a) differentially-private\nand (b) tolerant to byzantine agents. Our decentralized algorithms require no\ninformation about the network of connectivity between agents, making them\nscalable to large dynamic systems. We test our algorithms on a competitive\nbenchmark of random graphs and demonstrate their superior performance with\nrespect to existing robust algorithms. We hope that our work serves as an\nimportant step towards creating distributed decision-making systems that\nmaintain privacy.\n",
        "published": "2022",
        "authors": [
            "Abhimanyu Dubey",
            "Alex Pentland"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.01880v3",
        "title": "Learning in Congestion Games with Bandit Feedback",
        "abstract": "  In this paper, we investigate Nash-regret minimization in congestion games, a\nclass of games with benign theoretical structure and broad real-world\napplications. We first propose a centralized algorithm based on the optimism in\nthe face of uncertainty principle for congestion games with (semi-)bandit\nfeedback, and obtain finite-sample guarantees. Then we propose a decentralized\nalgorithm via a novel combination of the Frank-Wolfe method and G-optimal\ndesign. By exploiting the structure of the congestion game, we show the sample\ncomplexity of both algorithms depends only polynomially on the number of\nplayers and the number of facilities, but not the size of the action set, which\ncan be exponentially large in terms of the number of facilities. We further\ndefine a new problem class, Markov congestion games, which allows us to model\nthe non-stationarity in congestion games. We propose a centralized algorithm\nfor Markov congestion games, whose sample complexity again has only polynomial\ndependence on all relevant problem parameters, but not the size of the action\nset.\n",
        "published": "2022",
        "authors": [
            "Qiwen Cui",
            "Zhihan Xiong",
            "Maryam Fazel",
            "Simon S. Du"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.07570v1",
        "title": "Calibrating Agent-based Models to Microdata with Graph Neural Networks",
        "abstract": "  Calibrating agent-based models (ABMs) to data is among the most fundamental\nrequirements to ensure the model fulfils its desired purpose. In recent years,\nsimulation-based inference methods have emerged as powerful tools for\nperforming this task when the model likelihood function is intractable, as is\noften the case for ABMs. In some real-world use cases of ABMs, both the\nobserved data and the ABM output consist of the agents' states and their\ninteractions over time. In such cases, there is a tension between the desire to\nmake full use of the rich information content of such granular data on the one\nhand, and the need to reduce the dimensionality of the data to prevent\ndifficulties associated with high-dimensional learning tasks on the other. A\npossible resolution is to construct lower-dimensional time-series through the\nuse of summary statistics describing the macrostate of the system at each time\npoint. However, a poor choice of summary statistics can result in an\nunacceptable loss of information from the original dataset, dramatically\nreducing the quality of the resulting calibration. In this work, we instead\npropose to learn parameter posteriors associated with granular microdata\ndirectly using temporal graph neural networks. We will demonstrate that such an\napproach offers highly compelling inductive biases for Bayesian inference using\nthe raw ABM microstates as output.\n",
        "published": "2022",
        "authors": [
            "Joel Dyer",
            "Patrick Cannon",
            "J. Doyne Farmer",
            "Sebastian M. Schmon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.09845v3",
        "title": "Relational Reasoning via Set Transformers: Provable Efficiency and\n  Applications to MARL",
        "abstract": "  The cooperative Multi-A gent R einforcement Learning (MARL) with permutation\ninvariant agents framework has achieved tremendous empirical successes in\nreal-world applications. Unfortunately, the theoretical understanding of this\nMARL problem is lacking due to the curse of many agents and the limited\nexploration of the relational reasoning in existing works. In this paper, we\nverify that the transformer implements complex relational reasoning, and we\npropose and analyze model-free and model-based offline MARL algorithms with the\ntransformer approximators. We prove that the suboptimality gaps of the\nmodel-free and model-based algorithms are independent of and logarithmic in the\nnumber of agents respectively, which mitigates the curse of many agents. These\nresults are consequences of a novel generalization error bound of the\ntransformer and a novel analysis of the Maximum Likelihood Estimate (MLE) of\nthe system dynamics with the transformer. Our model-based algorithm is the\nfirst provably efficient MARL algorithm that explicitly exploits the\npermutation invariance of the agents. Our improved generalization bound may be\nof independent interest and is applicable to other regression problems related\nto the transformer beyond MARL.\n",
        "published": "2022",
        "authors": [
            "Fengzhuo Zhang",
            "Boyi Liu",
            "Kaixin Wang",
            "Vincent Y. F. Tan",
            "Zhuoran Yang",
            "Zhaoran Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.12812v2",
        "title": "Symmetric (Optimistic) Natural Policy Gradient for Multi-agent Learning\n  with Parameter Convergence",
        "abstract": "  Multi-agent interactions are increasingly important in the context of\nreinforcement learning, and the theoretical foundations of policy gradient\nmethods have attracted surging research interest. We investigate the global\nconvergence of natural policy gradient (NPG) algorithms in multi-agent\nlearning. We first show that vanilla NPG may not have parameter convergence,\ni.e., the convergence of the vector that parameterizes the policy, even when\nthe costs are regularized (which enabled strong convergence guarantees in the\npolicy space in the literature). This non-convergence of parameters leads to\nstability issues in learning, which becomes especially relevant in the function\napproximation setting, where we can only operate on low-dimensional parameters,\ninstead of the high-dimensional policy. We then propose variants of the NPG\nalgorithm, for several standard multi-agent learning scenarios: two-player\nzero-sum matrix and Markov games, and multi-player monotone games, with global\nlast-iterate parameter convergence guarantees. We also generalize the results\nto certain function approximation settings. Note that in our algorithms, the\nagents take symmetric roles. Our results might also be of independent interest\nfor solving nonconvex-nonconcave minimax optimization problems with certain\nstructures. Simulations are also provided to corroborate our theoretical\nfindings.\n",
        "published": "2022",
        "authors": [
            "Sarath Pattathil",
            "Kaiqing Zhang",
            "Asuman Ozdaglar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.10230v2",
        "title": "Double Matching Under Complementary Preferences",
        "abstract": "  In this paper, we propose a new algorithm for addressing the problem of\nmatching markets with complementary preferences, where agents' preferences are\nunknown a priori and must be learned from data. The presence of complementary\npreferences can lead to instability in the matching process, making this\nproblem challenging to solve. To overcome this challenge, we formulate the\nproblem as a bandit learning framework and propose the Multi-agent Multi-type\nThompson Sampling (MMTS) algorithm. The algorithm combines the strengths of\nThompson Sampling for exploration with a double matching technique to achieve a\nstable matching outcome. Our theoretical analysis demonstrates the\neffectiveness of MMTS as it is able to achieve stability at every matching\nstep, satisfies the incentive-compatibility property, and has a sublinear\nBayesian regret over time. Our approach provides a useful method for addressing\ncomplementary preferences in real-world scenarios.\n",
        "published": "2023",
        "authors": [
            "Yuantong Li",
            "Guang Cheng",
            "Xiaowu Dai"
        ]
    }
]