[
    {
        "id": "http://arxiv.org/abs/2011.05373v1",
        "title": "Emergent Reciprocity and Team Formation from Randomized Uncertain Social\n  Preferences",
        "abstract": "  Multi-agent reinforcement learning (MARL) has shown recent success in\nincreasingly complex fixed-team zero-sum environments. However, the real world\nis not zero-sum nor does it have fixed teams; humans face numerous social\ndilemmas and must learn when to cooperate and when to compete. To successfully\ndeploy agents into the human world, it may be important that they be able to\nunderstand and help in our conflicts. Unfortunately, selfish MARL agents\ntypically fail when faced with social dilemmas. In this work, we show evidence\nof emergent direct reciprocity, indirect reciprocity and reputation, and team\nformation when training agents with randomized uncertain social preferences\n(RUSP), a novel environment augmentation that expands the distribution of\nenvironments agents play in. RUSP is generic and scalable; it can be applied to\nany multi-agent environment without changing the original underlying game\ndynamics or objectives. In particular, we show that with RUSP these behaviors\ncan emerge and lead to higher social welfare equilibria in both classic\nabstract social dilemmas like Iterated Prisoner's Dilemma as well in more\ncomplex intertemporal environments.\n",
        "published": "2020",
        "authors": [
            "Bowen Baker"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.07290v1",
        "title": "Opponent Learning Awareness and Modelling in Multi-Objective Normal Form\n  Games",
        "abstract": "  Many real-world multi-agent interactions consider multiple distinct criteria,\ni.e. the payoffs are multi-objective in nature. However, the same\nmulti-objective payoff vector may lead to different utilities for each\nparticipant. Therefore, it is essential for an agent to learn about the\nbehaviour of other agents in the system. In this work, we present the first\nstudy of the effects of such opponent modelling on multi-objective multi-agent\ninteractions with non-linear utilities. Specifically, we consider two-player\nmulti-objective normal form games with non-linear utility functions under the\nscalarised expected returns optimisation criterion. We contribute novel\nactor-critic and policy gradient formulations to allow reinforcement learning\nof mixed strategies in this setting, along with extensions that incorporate\nopponent policy reconstruction and learning with opponent learning awareness\n(i.e., learning while considering the impact of one's policy when anticipating\nthe opponent's learning step). Empirical results in five different MONFGs\ndemonstrate that opponent learning awareness and modelling can drastically\nalter the learning dynamics in this setting. When equilibria are present,\nopponent modelling can confer significant benefits on agents that implement it.\nWhen there are no Nash equilibria, opponent learning awareness and modelling\nallows agents to still converge to meaningful solutions that approximate\nequilibria.\n",
        "published": "2020",
        "authors": [
            "Roxana R\u0103dulescu",
            "Timothy Verstraeten",
            "Yijie Zhang",
            "Patrick Mannion",
            "Diederik M. Roijers",
            "Ann Now\u00e9"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.10753v2",
        "title": "Emergent Road Rules In Multi-Agent Driving Environments",
        "abstract": "  For autonomous vehicles to safely share the road with human drivers,\nautonomous vehicles must abide by specific \"road rules\" that human drivers have\nagreed to follow. \"Road rules\" include rules that drivers are required to\nfollow by law -- such as the requirement that vehicles stop at red lights -- as\nwell as more subtle social rules -- such as the implicit designation of fast\nlanes on the highway. In this paper, we provide empirical evidence that\nsuggests that -- instead of hard-coding road rules into self-driving algorithms\n-- a scalable alternative may be to design multi-agent environments in which\nroad rules emerge as optimal solutions to the problem of maximizing traffic\nflow. We analyze what ingredients in driving environments cause the emergence\nof these road rules and find that two crucial factors are noisy perception and\nagents' spatial density. We provide qualitative and quantitative evidence of\nthe emergence of seven social driving behaviors, ranging from obeying traffic\nsignals to following lanes, all of which emerge from training agents to drive\nquickly to destinations without colliding. Our results add empirical support\nfor the social road rules that countries worldwide have agreed on for safe,\nefficient driving.\n",
        "published": "2020",
        "authors": [
            "Avik Pal",
            "Jonah Philion",
            "Yuan-Hong Liao",
            "Sanja Fidler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.12895v2",
        "title": "TLeague: A Framework for Competitive Self-Play based Distributed\n  Multi-Agent Reinforcement Learning",
        "abstract": "  Competitive Self-Play (CSP) based Multi-Agent Reinforcement Learning (MARL)\nhas shown phenomenal breakthroughs recently. Strong AIs are achieved for\nseveral benchmarks, including Dota 2, Glory of Kings, Quake III, StarCraft II,\nto name a few. Despite the success, the MARL training is extremely data\nthirsty, requiring typically billions of (if not trillions of) frames be seen\nfrom the environment during training in order for learning a high performance\nagent. This poses non-trivial difficulties for researchers or engineers and\nprevents the application of MARL to a broader range of real-world problems. To\naddress this issue, in this manuscript we describe a framework, referred to as\nTLeague, that aims at large-scale training and implements several main-stream\nCSP-MARL algorithms. The training can be deployed in either a single machine or\na cluster of hybrid machines (CPUs and GPUs), where the standard Kubernetes is\nsupported in a cloud native manner. TLeague achieves a high throughput and a\nreasonable scale-up when performing distributed training. Thanks to the modular\ndesign, it is also easy to extend for solving other multi-agent problems or\nimplementing and verifying MARL algorithms. We present experiments over\nStarCraft II, ViZDoom and Pommerman to show the efficiency and effectiveness of\nTLeague. The code is open-sourced and available at\nhttps://github.com/tencent-ailab/tleague_projpage\n",
        "published": "2020",
        "authors": [
            "Peng Sun",
            "Jiechao Xiong",
            "Lei Han",
            "Xinghai Sun",
            "Shuxing Li",
            "Jiawei Xu",
            "Meng Fang",
            "Zhengyou Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.14890v2",
        "title": "Low-Bandwidth Communication Emerges Naturally in Multi-Agent Learning\n  Systems",
        "abstract": "  In this work, we study emergent communication through the lens of cooperative\nmulti-agent behavior in nature. Using insights from animal communication, we\npropose a spectrum from low-bandwidth (e.g. pheromone trails) to high-bandwidth\n(e.g. compositional language) communication that is based on the cognitive,\nperceptual, and behavioral capabilities of social agents. Through a series of\nexperiments with pursuit-evasion games, we identify multi-agent reinforcement\nlearning algorithms as a computational model for the low-bandwidth end of the\ncommunication spectrum.\n",
        "published": "2020",
        "authors": [
            "Niko A. Grupen",
            "Daniel D. Lee",
            "Bart Selman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.01934v1",
        "title": "Adaptable Automation with Modular Deep Reinforcement Learning and Policy\n  Transfer",
        "abstract": "  Recent advances in deep Reinforcement Learning (RL) have created\nunprecedented opportunities for intelligent automation, where a machine can\nautonomously learn an optimal policy for performing a given task. However,\ncurrent deep RL algorithms predominantly specialize in a narrow range of tasks,\nare sample inefficient, and lack sufficient stability, which in turn hinder\ntheir industrial adoption. This article tackles this limitation by developing\nand testing a Hyper-Actor Soft Actor-Critic (HASAC) RL framework based on the\nnotions of task modularization and transfer learning. The goal of the proposed\nHASAC is to enhance the adaptability of an agent to new tasks by transferring\nthe learned policies of former tasks to the new task via a \"hyper-actor\". The\nHASAC framework is tested on a new virtual robotic manipulation benchmark,\nMeta-World. Numerical experiments show superior performance by HASAC over\nstate-of-the-art deep RL algorithms in terms of reward value, success rate, and\ntask completion time.\n",
        "published": "2020",
        "authors": [
            "Zohreh Raziei",
            "Mohsen Moghaddam"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.02096v2",
        "title": "Emergent Complexity and Zero-shot Transfer via Unsupervised Environment\n  Design",
        "abstract": "  A wide range of reinforcement learning (RL) problems - including robustness,\ntransfer learning, unsupervised RL, and emergent complexity - require\nspecifying a distribution of tasks or environments in which a policy will be\ntrained. However, creating a useful distribution of environments is error\nprone, and takes a significant amount of developer time and effort. We propose\nUnsupervised Environment Design (UED) as an alternative paradigm, where\ndevelopers provide environments with unknown parameters, and these parameters\nare used to automatically produce a distribution over valid, solvable\nenvironments. Existing approaches to automatically generating environments\nsuffer from common failure modes: domain randomization cannot generate\nstructure or adapt the difficulty of the environment to the agent's learning\nprogress, and minimax adversarial training leads to worst-case environments\nthat are often unsolvable. To generate structured, solvable environments for\nour protagonist agent, we introduce a second, antagonist agent that is allied\nwith the environment-generating adversary. The adversary is motivated to\ngenerate environments which maximize regret, defined as the difference between\nthe protagonist and antagonist agent's return. We call our technique\nProtagonist Antagonist Induced Regret Environment Design (PAIRED). Our\nexperiments demonstrate that PAIRED produces a natural curriculum of\nincreasingly complex environments, and PAIRED agents achieve higher zero-shot\ntransfer performance when tested in highly novel environments.\n",
        "published": "2020",
        "authors": [
            "Michael Dennis",
            "Natasha Jaques",
            "Eugene Vinitsky",
            "Alexandre Bayen",
            "Stuart Russell",
            "Andrew Critch",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.05672v2",
        "title": "Imitating Interactive Intelligence",
        "abstract": "  A common vision from science fiction is that robots will one day inhabit our\nphysical spaces, sense the world as we do, assist our physical labours, and\ncommunicate with us through natural language. Here we study how to design\nartificial agents that can interact naturally with humans using the\nsimplification of a virtual environment. This setting nevertheless integrates a\nnumber of the central challenges of artificial intelligence (AI) research:\ncomplex visual perception and goal-directed physical control, grounded language\ncomprehension and production, and multi-agent social interaction. To build\nagents that can robustly interact with humans, we would ideally train them\nwhile they interact with humans. However, this is presently impractical.\nTherefore, we approximate the role of the human with another learned agent, and\nuse ideas from inverse reinforcement learning to reduce the disparities between\nhuman-human and agent-agent interactive behaviour. Rigorously evaluating our\nagents poses a great challenge, so we develop a variety of behavioural tests,\nincluding evaluation by humans who watch videos of agents or interact directly\nwith them. These evaluations convincingly demonstrate that interactive training\nand auxiliary losses improve agent behaviour beyond what is achieved by\nsupervised learning of actions alone. Further, we demonstrate that agent\ncapabilities generalise beyond literal experiences in the dataset. Finally, we\ntrain evaluation models whose ratings of agents agree well with human\njudgement, thus permitting the evaluation of new agent models without\nadditional effort. Taken together, our results in this virtual environment\nprovide evidence that large-scale human behavioural imitation is a promising\ntool to create intelligent, interactive agents, and the challenge of reliably\nevaluating such agents is possible to surmount.\n",
        "published": "2020",
        "authors": [
            "Josh Abramson",
            "Arun Ahuja",
            "Iain Barr",
            "Arthur Brussee",
            "Federico Carnevale",
            "Mary Cassin",
            "Rachita Chhaparia",
            "Stephen Clark",
            "Bogdan Damoc",
            "Andrew Dudzik",
            "Petko Georgiev",
            "Aurelia Guy",
            "Tim Harley",
            "Felix Hill",
            "Alden Hung",
            "Zachary Kenton",
            "Jessica Landon",
            "Timothy Lillicrap",
            "Kory Mathewson",
            "So\u0148a Mokr\u00e1",
            "Alistair Muldal",
            "Adam Santoro",
            "Nikolay Savinov",
            "Vikrant Varma",
            "Greg Wayne",
            "Duncan Williams",
            "Nathaniel Wong",
            "Chen Yan",
            "Rui Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.07617v2",
        "title": "Specializing Inter-Agent Communication in Heterogeneous Multi-Agent\n  Reinforcement Learning using Agent Class Information",
        "abstract": "  Inspired by recent advances in agent communication with graph neural\nnetworks, this work proposes the representation of multi-agent communication\ncapabilities as a directed labeled heterogeneous agent graph, in which node\nlabels denote agent classes and edge labels, the communication type between two\nclasses of agents. We also introduce a neural network architecture that\nspecializes communication in fully cooperative heterogeneous multi-agent tasks\nby learning individual transformations to the exchanged messages between each\npair of agent classes. By also employing encoding and action selection modules\nwith parameter sharing for environments with heterogeneous agents, we\ndemonstrate comparable or superior performance in environments where a larger\nnumber of agent classes operates.\n",
        "published": "2020",
        "authors": [
            "Douglas De Rizzo Meneghetti",
            "Reinaldo Augusto da Costa Bianchi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.09421v4",
        "title": "Learning Fair Policies in Decentralized Cooperative Multi-Agent\n  Reinforcement Learning",
        "abstract": "  We consider the problem of learning fair policies in (deep) cooperative\nmulti-agent reinforcement learning (MARL). We formalize it in a principled way\nas the problem of optimizing a welfare function that explicitly encodes two\nimportant aspects of fairness: efficiency and equity. As a solution method, we\npropose a novel neural network architecture, which is composed of two\nsub-networks specifically designed for taking into account the two aspects of\nfairness. In experiments, we demonstrate the importance of the two sub-networks\nfor fair optimization. Our overall approach is general as it can accommodate\nany (sub)differentiable welfare function. Therefore, it is compatible with\nvarious notions of fairness that have been proposed in the literature (e.g.,\nlexicographic maximin, generalized Gini social welfare function, proportional\nfairness). Our solution method is generic and can be implemented in various\nMARL settings: centralized training and decentralized execution, or fully\ndecentralized. Finally, we experimentally validate our approach in various\ndomains and show that it can perform much better than previous methods.\n",
        "published": "2020",
        "authors": [
            "Matthieu Zimmer",
            "Claire Glanois",
            "Umer Siddique",
            "Paul Weng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.11527v1",
        "title": "Can we learn where people come from? Retracing of origins in merging\n  situations",
        "abstract": "  One crucial information for a pedestrian crowd simulation is the number of\nagents moving from an origin to a certain target. While this setup has a large\nimpact on the simulation, it is in most setups challenging to find the number\nof agents that should be spawned at a source in the simulation. Often, number\nare chosen based on surveys and experience of modelers and event organizers.\nThese approaches are important and useful but reach their limits when we want\nto perform real-time predictions. In this case, a static information about the\ninflow is not sufficient. Instead, we need a dynamic information that can be\nretrieved each time the prediction is started. Nowadays, sensor data such as\nvideo footage or GPS tracks of a crowd are often available. If we can estimate\nthe number of pedestrians who stem from a certain origin from this sensor data,\nwe can dynamically initialize the simulation. In this study, we use density\nheatmaps that can be derived from sensor data as input for a random forest\nregressor to predict the origin distributions. We study three different\ndatasets: A simulated dataset, experimental data, and a hybrid approach with\nboth experimental and simulated data. In the hybrid setup, the model is trained\nwith simulated data and then tested on experimental data. The results\ndemonstrate that the random forest model is able to predict the origin\ndistribution based on a single density heatmap for all three configurations.\nThis is especially promising for applying the approach on real data since there\nis often only a limited amount of data available.\n",
        "published": "2020",
        "authors": [
            "Marion G\u00f6del",
            "Luca Spataro",
            "Gerta K\u00f6ster"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.12062v1",
        "title": "QVMix and QVMix-Max: Extending the Deep Quality-Value Family of\n  Algorithms to Cooperative Multi-Agent Reinforcement Learning",
        "abstract": "  This paper introduces four new algorithms that can be used for tackling\nmulti-agent reinforcement learning (MARL) problems occurring in cooperative\nsettings. All algorithms are based on the Deep Quality-Value (DQV) family of\nalgorithms, a set of techniques that have proven to be successful when dealing\nwith single-agent reinforcement learning problems (SARL). The key idea of DQV\nalgorithms is to jointly learn an approximation of the state-value function\n$V$, alongside an approximation of the state-action value function $Q$. We\nfollow this principle and generalise these algorithms by introducing two fully\ndecentralised MARL algorithms (IQV and IQV-Max) and two algorithms that are\nbased on the centralised training with decentralised execution training\nparadigm (QVMix and QVMix-Max). We compare our algorithms with state-of-the-art\nMARL techniques on the popular StarCraft Multi-Agent Challenge (SMAC)\nenvironment. We show competitive results when QVMix and QVMix-Max are compared\nto well-known MARL techniques such as QMIX and MAVEN and show that QVMix can\neven outperform them on some of the tested environments, being the algorithm\nwhich performs best overall. We hypothesise that this is due to the fact that\nQVMix suffers less from the overestimation bias of the $Q$ function.\n",
        "published": "2020",
        "authors": [
            "Pascal Leroy",
            "Damien Ernst",
            "Pierre Geurts",
            "Gilles Louppe",
            "Jonathan Pisane",
            "Matthia Sabatelli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.14137v3",
        "title": "Federated Multi-Agent Actor-Critic Learning for Age Sensitive Mobile\n  Edge Computing",
        "abstract": "  As an emerging technique, mobile edge computing (MEC) introduces a new\nprocessing scheme for various distributed communication-computing systems such\nas industrial Internet of Things (IoT), vehicular communication, smart city,\netc. In this work, we mainly focus on the timeliness of the MEC systems where\nthe freshness of the data and computation tasks is significant. Firstly, we\nformulate a kind of age-sensitive MEC models and define the average age of\ninformation (AoI) minimization problems of interests. Then, a novel policy\nbased multi-agent deep reinforcement learning (RL) framework, called\nheterogeneous multi-agent actor critic (H-MAAC), is proposed as a paradigm for\njoint collaboration in the investigated MEC systems, where edge devices and\ncenter controller learn the interactive strategies through their own\nobservations. To improves the system performance, we develop the corresponding\nonline algorithm by introducing an edge federated learning mode into the\nmulti-agent cooperation whose advantages on learning convergence can be\nguaranteed theoretically. To the best of our knowledge, it's the first joint\nMEC collaboration algorithm that combines the edge federated mode with the\nmulti-agent actor-critic reinforcement learning. Furthermore, we evaluate the\nproposed approach and compare it with classical RL based methods. As a result,\nthe proposed framework not only outperforms the baseline on average system age,\nbut also promotes the stability of training process. Besides, the simulation\nresults provide some innovative perspectives for the system design under the\nedge federated collaboration.\n",
        "published": "2020",
        "authors": [
            "Zheqi Zhu",
            "Shuo Wan",
            "Pingyi Fan",
            "Khaled B. Letaief"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.00582v2",
        "title": "Multi-Agent Reinforcement Learning with Temporal Logic Specifications",
        "abstract": "  In this paper, we study the problem of learning to satisfy temporal logic\nspecifications with a group of agents in an unknown environment, which may\nexhibit probabilistic behaviour. From a learning perspective these\nspecifications provide a rich formal language with which to capture tasks or\nobjectives, while from a logic and automated verification perspective the\nintroduction of learning capabilities allows for practical applications in\nlarge, stochastic, unknown environments. The existing work in this area is,\nhowever, limited. Of the frameworks that consider full linear temporal logic or\nhave correctness guarantees, all methods thus far consider only the case of a\nsingle temporal logic specification and a single agent. In order to overcome\nthis limitation, we develop the first multi-agent reinforcement learning\ntechnique for temporal logic specifications, which is also novel in its ability\nto handle multiple specifications. We provide correctness and convergence\nguarantees for our main algorithm - ALMANAC (Automaton/Logic Multi-Agent\nNatural Actor-Critic) - even when using function approximation. Alongside our\ntheoretical results, we further demonstrate the applicability of our technique\nvia a set of preliminary experiments.\n",
        "published": "2021",
        "authors": [
            "Lewis Hammond",
            "Alessandro Abate",
            "Julian Gutierrez",
            "Michael Wooldridge"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.00824v2",
        "title": "HAMMER: Multi-Level Coordination of Reinforcement Learning Agents via\n  Learned Messaging",
        "abstract": "  Cooperative multi-agent reinforcement learning (MARL) has achieved\nsignificant results, most notably by leveraging the representation-learning\nabilities of deep neural networks. However, large centralized approaches\nquickly become infeasible as the number of agents scale, and fully\ndecentralized approaches can miss important opportunities for information\nsharing and coordination. Furthermore, not all agents are equal -- in some\ncases, individual agents may not even have the ability to send communication to\nother agents or explicitly model other agents. This paper considers the case\nwhere there is a single, powerful, \\emph{central agent} that can observe the\nentire observation space, and there are multiple, low-powered \\emph{local\nagents} that can only receive local observations and are not able to\ncommunicate with each other. The central agent's job is to learn what message\nneeds to be sent to different local agents based on the global observations,\nnot by centrally solving the entire problem and sending action commands, but by\ndetermining what additional information an individual agent should receive so\nthat it can make a better decision. In this work we present our MARL algorithm\n\\algo, describe where it would be most applicable, and implement it in the\ncooperative navigation and multi-agent walker domains. Empirical results show\nthat 1) learned communication does indeed improve system performance, 2)\nresults generalize to heterogeneous local agents, and 3) results generalize to\ndifferent reward structures.\n",
        "published": "2021",
        "authors": [
            "Nikunj Gupta",
            "G Srinivasaraghavan",
            "Swarup Kumar Mohalik",
            "Nishant Kumar",
            "Matthew E. Taylor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.01434v2",
        "title": "An Abstraction-based Method to Check Multi-Agent Deep\n  Reinforcement-Learning Behaviors",
        "abstract": "  Multi-agent reinforcement learning (RL) often struggles to ensure the safe\nbehaviours of the learning agents, and therefore it is generally not adapted to\nsafety-critical applications. To address this issue, we present a methodology\nthat combines formal verification with (deep) RL algorithms to guarantee the\nsatisfaction of formally-specified safety constraints both in training and\ntesting. The approach we propose expresses the constraints to verify in\nProbabilistic Computation Tree Logic (PCTL) and builds an abstract\nrepresentation of the system to reduce the complexity of the verification step.\nThis abstract model allows for model checking techniques to identify a set of\nabstract policies that meet the safety constraints expressed in PCTL. Then, the\nagents' behaviours are restricted according to these safe abstract policies. We\nprovide formal guarantees that by using this method, the actions of the agents\nalways meet the safety constraints, and provide a procedure to generate an\nabstract model automatically. We empirically evaluate and show the\neffectiveness of our method in a multi-agent environment.\n",
        "published": "2021",
        "authors": [
            "Pierre El Mqirmi",
            "Francesco Belardinelli",
            "Borja G. Le\u00f3n"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.01585v2",
        "title": "Approximately Solving Mean Field Games via Entropy-Regularized Deep\n  Reinforcement Learning",
        "abstract": "  The recent mean field game (MFG) formalism facilitates otherwise intractable\ncomputation of approximate Nash equilibria in many-agent settings. In this\npaper, we consider discrete-time finite MFGs subject to finite-horizon\nobjectives. We show that all discrete-time finite MFGs with non-constant fixed\npoint operators fail to be contractive as typically assumed in existing MFG\nliterature, barring convergence via fixed point iteration. Instead, we\nincorporate entropy-regularization and Boltzmann policies into the fixed point\niteration. As a result, we obtain provable convergence to approximate fixed\npoints where existing methods fail, and reach the original goal of approximate\nNash equilibria. All proposed methods are evaluated with respect to their\nexploitability, on both instructive examples with tractable exact solutions and\nhigh-dimensional problems where exact methods become intractable. In\nhigh-dimensional scenarios, we apply established deep reinforcement learning\nmethods and empirically combine fictitious play with our approximations.\n",
        "published": "2021",
        "authors": [
            "Kai Cui",
            "Heinz Koeppl"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.02274v1",
        "title": "Neural Recursive Belief States in Multi-Agent Reinforcement Learning",
        "abstract": "  In multi-agent reinforcement learning, the problem of learning to act is\nparticularly difficult because the policies of co-players may be heavily\nconditioned on information only observed by them. On the other hand, humans\nreadily form beliefs about the knowledge possessed by their peers and leverage\nbeliefs to inform decision-making. Such abilities underlie individual success\nin a wide range of Markov games, from bluffing in Poker to conditional\ncooperation in the Prisoner's Dilemma, to convention-building in Bridge.\nClassical methods are usually not applicable to complex domains due to the\nintractable nature of hierarchical beliefs (i.e. beliefs of other agents'\nbeliefs). We propose a scalable method to approximate these belief structures\nusing recursive deep generative models, and to use the belief models to obtain\nrepresentations useful to acting in complex tasks. Our agents trained with\nbelief models outperform model-free baselines with equivalent representational\ncapacity using common training paradigms. We also show that higher-order belief\nmodels outperform agents with lower-order models.\n",
        "published": "2021",
        "authors": [
            "Pol Moreno",
            "Edward Hughes",
            "Kevin R. McKee",
            "Bernardo Avila Pires",
            "Th\u00e9ophane Weber"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.03022v1",
        "title": "Deceptive Reinforcement Learning for Privacy-Preserving Planning",
        "abstract": "  In this paper, we study the problem of deceptive reinforcement learning to\npreserve the privacy of a reward function. Reinforcement learning is the\nproblem of finding a behaviour policy based on rewards received from\nexploratory behaviour. A key ingredient in reinforcement learning is a reward\nfunction, which determines how much reward (negative or positive) is given and\nwhen. However, in some situations, we may want to keep a reward function\nprivate; that is, to make it difficult for an observer to determine the reward\nfunction used. We define the problem of privacy-preserving reinforcement\nlearning, and present two models for solving it. These models are based on\ndissimulation -- a form of deception that `hides the truth'. We evaluate our\nmodels both computationally and via human behavioural experiments. Results show\nthat the resulting policies are indeed deceptive, and that participants can\ndetermine the true reward function less reliably than that of an honest agent.\n",
        "published": "2021",
        "authors": [
            "Zhengshang Liu",
            "Yue Yang",
            "Tim Miller",
            "Peta Masters"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.03479v19",
        "title": "Rethinking the Implementation Tricks and Monotonicity Constraint in\n  Cooperative Multi-Agent Reinforcement Learning",
        "abstract": "  Many complex multi-agent systems such as robot swarms control and autonomous\nvehicle coordination can be modeled as Multi-Agent Reinforcement Learning\n(MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a\nbaseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge\n(SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX\ntarget relaxing the monotonicity constraint of QMIX, allowing for performance\nimprovement in SMAC. In this paper, we investigate the code-level optimizations\nof these variants and the monotonicity constraint. (1) We find that such\nimprovements of the variants are significantly affected by various code-level\noptimizations. (2) The experiment results show that QMIX with normalized\noptimizations outperforms other works in SMAC; (3) beyond the common wisdom\nfrom these works, the monotonicity constraint can improve sample efficiency in\nSMAC and DEPP. We also discuss why monotonicity constraints work well in purely\ncooperative tasks with a theoretical analysis. We open-source the code at\n\\url{https://github.com/hijkzzz/pymarl2}.\n",
        "published": "2021",
        "authors": [
            "Jian Hu",
            "Siyang Jiang",
            "Seth Austin Harding",
            "Haibin Wu",
            "Shih-wei Liao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.04775v1",
        "title": "Structured Diversification Emergence via Reinforced Organization Control\n  and Hierarchical Consensus Learning",
        "abstract": "  When solving a complex task, humans will spontaneously form teams and to\ncomplete different parts of the whole task, respectively. Meanwhile, the\ncooperation between teammates will improve efficiency. However, for current\ncooperative MARL methods, the cooperation team is constructed through either\nheuristics or end-to-end blackbox optimization. In order to improve the\nefficiency of cooperation and exploration, we propose a structured\ndiversification emergence MARL framework named {\\sc{Rochico}} based on\nreinforced organization control and hierarchical consensus learning.\n{\\sc{Rochico}} first learns an adaptive grouping policy through the\norganization control module, which is established by independent multi-agent\nreinforcement learning. Further, the hierarchical consensus module based on the\nhierarchical intentions with consensus constraint is introduced after team\nformation. Simultaneously, utilizing the hierarchical consensus module and a\nself-supervised intrinsic reward enhanced decision module, the proposed\ncooperative MARL algorithm {\\sc{Rochico}} can output the final diversified\nmulti-agent cooperative policy. All three modules are organically combined to\npromote the structured diversification emergence. Comparative experiments on\nfour large-scale cooperation tasks show that {\\sc{Rochico}} is significantly\nbetter than the current SOTA algorithms in terms of exploration efficiency and\ncooperation strength.\n",
        "published": "2021",
        "authors": [
            "Wenhao Li",
            "Xiangfeng Wang",
            "Bo Jin",
            "Junjie Sheng",
            "Yun Hua",
            "Hongyuan Zha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.05026v1",
        "title": "Multi-Agent Coordination in Adversarial Environments through Signal\n  Mediated Strategies",
        "abstract": "  Many real-world scenarios involve teams of agents that have to coordinate\ntheir actions to reach a shared goal. We focus on the setting in which a team\nof agents faces an opponent in a zero-sum, imperfect-information game. Team\nmembers can coordinate their strategies before the beginning of the game, but\nare unable to communicate during the playing phase of the game. This is the\ncase, for example, in Bridge, collusion in poker, and collusion in bidding. In\nthis setting, model-free RL methods are oftentimes unable to capture\ncoordination because agents' policies are executed in a decentralized fashion.\nOur first contribution is a game-theoretic centralized training regimen to\neffectively perform trajectory sampling so as to foster team coordination. When\nteam members can observe each other actions, we show that this approach\nprovably yields equilibrium strategies. Then, we introduce a signaling-based\nframework to represent team coordinated strategies given a buffer of past\nexperiences. Each team member's policy is parametrized as a neural network\nwhose output is conditioned on a suitable exogenous signal, drawn from a\nlearned probability distribution. By combining these two elements, we\nempirically show convergence to coordinated equilibria in cases where previous\nstate-of-the-art multi-agent RL algorithms did not.\n",
        "published": "2021",
        "authors": [
            "Federico Cacciamani",
            "Andrea Celli",
            "Marco Ciccone",
            "Nicola Gatti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.08462v1",
        "title": "Multi-Agent Multi-Armed Bandits with Limited Communication",
        "abstract": "  We consider the problem where $N$ agents collaboratively interact with an\ninstance of a stochastic $K$ arm bandit problem for $K \\gg N$. The agents aim\nto simultaneously minimize the cumulative regret over all the agents for a\ntotal of $T$ time steps, the number of communication rounds, and the number of\nbits in each communication round. We present Limited Communication\nCollaboration - Upper Confidence Bound (LCC-UCB), a doubling-epoch based\nalgorithm where each agent communicates only after the end of the epoch and\nshares the index of the best arm it knows. With our algorithm, LCC-UCB, each\nagent enjoys a regret of $\\tilde{O}\\left(\\sqrt{({K/N}+ N)T}\\right)$,\ncommunicates for $O(\\log T)$ steps and broadcasts $O(\\log K)$ bits in each\ncommunication step. We extend the work to sparse graphs with maximum degree\n$K_G$, and diameter $D$ and propose LCC-UCB-GRAPH which enjoys a regret bound\nof $\\tilde{O}\\left(D\\sqrt{(K/N+ K_G)DT}\\right)$. Finally, we empirically show\nthat the LCC-UCB and the LCC-UCB-GRAPH algorithm perform well and outperform\nstrategies that communicate through a central node\n",
        "published": "2021",
        "authors": [
            "Mridul Agarwal",
            "Vaneet Aggarwal",
            "Kamyar Azizzadenesheli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.08507v1",
        "title": "Towards an AI Coach to Infer Team Mental Model Alignment in Healthcare",
        "abstract": "  Shared mental models are critical to team success; however, in practice, team\nmembers may have misaligned models due to a variety of factors. In\nsafety-critical domains (e.g., aviation, healthcare), lack of shared mental\nmodels can lead to preventable errors and harm. Towards the goal of mitigating\nsuch preventable errors, here, we present a Bayesian approach to infer\nmisalignment in team members' mental models during complex healthcare task\nexecution. As an exemplary application, we demonstrate our approach using two\nsimulated team-based scenarios, derived from actual teamwork in cardiac\nsurgery. In these simulated experiments, our approach inferred model\nmisalignment with over 75% recall, thereby providing a building block for\nenabling computer-assisted interventions to augment human cognition in the\noperating room and improve teamwork.\n",
        "published": "2021",
        "authors": [
            "Sangwon Seo",
            "Lauren R. Kennedy-Metz",
            "Marco A. Zenati",
            "Julie A. Shah",
            "Roger D. Dias",
            "Vaibhav V. Unhelkar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.10740v1",
        "title": "Communication Efficient Parallel Reinforcement Learning",
        "abstract": "  We consider the problem where $M$ agents interact with $M$ identical and\nindependent environments with $S$ states and $A$ actions using reinforcement\nlearning for $T$ rounds. The agents share their data with a central server to\nminimize their regret. We aim to find an algorithm that allows the agents to\nminimize the regret with infrequent communication rounds. We provide \\NAM\\\nwhich runs at each agent and prove that the total cumulative regret of $M$\nagents is upper bounded as $\\Tilde{O}(DS\\sqrt{MAT})$ for a Markov Decision\nProcess with diameter $D$, number of states $S$, and number of actions $A$. The\nagents synchronize after their visitations to any state-action pair exceeds a\ncertain threshold. Using this, we obtain a bound of $O\\left(MSA\\log(MT)\\right)$\non the total number of communications rounds. Finally, we evaluate the\nalgorithm against multiple environments and demonstrate that the proposed\nalgorithm performs at par with an always communication version of the UCRL2\nalgorithm, while with significantly lower communication.\n",
        "published": "2021",
        "authors": [
            "Mridul Agarwal",
            "Bhargav Ganguly",
            "Vaneet Aggarwal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.11717v4",
        "title": "Greedy-Step Off-Policy Reinforcement Learning",
        "abstract": "  Most of the policy evaluation algorithms are based on the theories of Bellman\nExpectation and Optimality Equation, which derive two popular approaches -\nPolicy Iteration (PI) and Value Iteration (VI). However, multi-step\nbootstrapping is often at cross-purposes with and off-policy learning in\nPI-based methods due to the large variance of multi-step off-policy correction.\nIn contrast, VI-based methods are naturally off-policy but subject to one-step\nlearning.In this paper, we deduce a novel multi-step Bellman Optimality\nEquation by utilizing a latent structure of multi-step bootstrapping with the\noptimal value function. Via this new equation, we derive a new multi-step value\niteration method that converges to the optimal value function with exponential\ncontraction rate $\\mathcal{O}(\\gamma^n)$ but only linear computational\ncomplexity. Moreover, it can naturally derive a suite of multi-step off-policy\nalgorithms that can safely utilize data collected by arbitrary policies without\ncorrection.Experiments reveal that the proposed methods are reliable, easy to\nimplement and achieve state-of-the-art performance on a series of standard\nbenchmark datasets.\n",
        "published": "2021",
        "authors": [
            "Yuhui Wang",
            "Qingyuan Wu",
            "Pengcheng He",
            "Xiaoyang Tan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.11762v2",
        "title": "School of hard knocks: Curriculum analysis for Pommerman with a fixed\n  computational budget",
        "abstract": "  Pommerman is a hybrid cooperative/adversarial multi-agent environment, with\nchallenging characteristics in terms of partial observability, limited or no\ncommunication, sparse and delayed rewards, and restrictive computational time\nlimits. This makes it a challenging environment for reinforcement learning (RL)\napproaches. In this paper, we focus on developing a curriculum for learning a\nrobust and promising policy in a constrained computational budget of 100,000\ngames, starting from a fixed base policy (which is itself trained to imitate a\nnoisy expert policy). All RL algorithms starting from the base policy use\nvanilla proximal-policy optimization (PPO) with the same reward function, and\nthe only difference between their training is the mix and sequence of opponent\npolicies. One expects that beginning training with simpler opponents and then\ngradually increasing the opponent difficulty will facilitate faster learning,\nleading to more robust policies compared against a baseline where all available\nopponent policies are introduced from the start. We test this hypothesis and\nshow that within constrained computational budgets, it is in fact better to\n\"learn in the school of hard knocks\", i.e., against all available opponent\npolicies nearly from the start. We also include ablation studies where we study\nthe effect of modifying the base environment properties of ammo and bomb blast\nstrength on the agent performance.\n",
        "published": "2021",
        "authors": [
            "Omkar Shelke",
            "Hardik Meisheri",
            "Harshad Khadilkar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.12307v1",
        "title": "Balancing Rational and Other-Regarding Preferences in\n  Cooperative-Competitive Environments",
        "abstract": "  Recent reinforcement learning studies extensively explore the interplay\nbetween cooperative and competitive behaviour in mixed environments. Unlike\ncooperative environments where agents strive towards a common goal, mixed\nenvironments are notorious for the conflicts of selfish and social interests.\nAs a consequence, purely rational agents often struggle to achieve and maintain\ncooperation. A prevalent approach to induce cooperative behaviour is to assign\nadditional rewards based on other agents' well-being. However, this approach\nsuffers from the issue of multi-agent credit assignment, which can hinder\nperformance. This issue is efficiently alleviated in cooperative setting with\nsuch state-of-the-art algorithms as QMIX and COMA. Still, when applied to mixed\nenvironments, these algorithms may result in unfair allocation of rewards. We\npropose BAROCCO, an extension of these algorithms capable to balance individual\nand social incentives. The mechanism behind BAROCCO is to train two distinct\nbut interwoven components that jointly affect each agent's decisions. Our\nmeta-algorithm is compatible with both Q-learning and Actor-Critic frameworks.\nWe experimentally confirm the advantages over the existing methods and explore\nthe behavioural aspects of BAROCCO in two mixed multi-agent setups.\n",
        "published": "2021",
        "authors": [
            "Dmitry Ivanov",
            "Vladimir Egorov",
            "Aleksei Shpilman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.12550v1",
        "title": "Learning Emergent Discrete Message Communication for Cooperative\n  Reinforcement Learning",
        "abstract": "  Communication is a important factor that enables agents work cooperatively in\nmulti-agent reinforcement learning (MARL). Most previous work uses continuous\nmessage communication whose high representational capacity comes at the expense\nof interpretability. Allowing agents to learn their own discrete message\ncommunication protocol emerged from a variety of domains can increase the\ninterpretability for human designers and other agents.This paper proposes a\nmethod to generate discrete messages analogous to human languages, and achieve\ncommunication by a broadcast-and-listen mechanism based on self-attention. We\nshow that discrete message communication has performance comparable to\ncontinuous message communication but with much a much smaller vocabulary\nsize.Furthermore, we propose an approach that allows humans to interactively\nsend discrete messages to agents.\n",
        "published": "2021",
        "authors": [
            "Sheng Li",
            "Yutai Zhou",
            "Ross Allen",
            "Mykel J. Kochenderfer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.12957v1",
        "title": "Credit Assignment with Meta-Policy Gradient for Multi-Agent\n  Reinforcement Learning",
        "abstract": "  Reward decomposition is a critical problem in centralized training with\ndecentralized execution~(CTDE) paradigm for multi-agent reinforcement learning.\nTo take full advantage of global information, which exploits the states from\nall agents and the related environment for decomposing Q values into individual\ncredits, we propose a general meta-learning-based Mixing Network with Meta\nPolicy Gradient~(MNMPG) framework to distill the global hierarchy for delicate\nreward decomposition. The excitation signal for learning global hierarchy is\ndeduced from the episode reward difference between before and after \"exercise\nupdates\" through the utility network. Our method is generally applicable to the\nCTDE method using a monotonic mixing network. Experiments on the StarCraft II\nmicromanagement benchmark demonstrate that our method just with a simple\nutility network is able to outperform the current state-of-the-art MARL\nalgorithms on 4 of 5 super hard scenarios. Better performance can be further\nachieved when combined with a role-based utility network.\n",
        "published": "2021",
        "authors": [
            "Jianzhun Shao",
            "Hongchang Zhang",
            "Yuhang Jiang",
            "Shuncheng He",
            "Xiangyang Ji"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.01955v4",
        "title": "The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games",
        "abstract": "  Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement\nlearning algorithm but is significantly less utilized than off-policy learning\nalgorithms in multi-agent settings. This is often due to the belief that PPO is\nsignificantly less sample efficient than off-policy methods in multi-agent\nsystems. In this work, we carefully study the performance of PPO in cooperative\nmulti-agent settings. We show that PPO-based multi-agent algorithms achieve\nsurprisingly strong performance in four popular multi-agent testbeds: the\nparticle-world environments, the StarCraft multi-agent challenge, Google\nResearch Football, and the Hanabi challenge, with minimal hyperparameter tuning\nand without any domain-specific algorithmic modifications or architectures.\nImportantly, compared to competitive off-policy methods, PPO often achieves\ncompetitive or superior results in both final returns and sample efficiency.\nFinally, through ablation studies, we analyze implementation and hyperparameter\nfactors that are critical to PPO's empirical performance, and give concrete\npractical suggestions regarding these factors. Our results show that when using\nthese practices, simple PPO-based methods can be a strong baseline in\ncooperative multi-agent reinforcement learning. Source code is released at\n\\url{https://github.com/marlbenchmark/on-policy}.\n",
        "published": "2021",
        "authors": [
            "Chao Yu",
            "Akash Velu",
            "Eugene Vinitsky",
            "Jiaxuan Gao",
            "Yu Wang",
            "Alexandre Bayen",
            "Yi Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.01991v1",
        "title": "Adversarial Environment Generation for Learning to Navigate the Web",
        "abstract": "  Learning to autonomously navigate the web is a difficult sequential decision\nmaking task. The state and action spaces are large and combinatorial in nature,\nand websites are dynamic environments consisting of several pages. One of the\nbottlenecks of training web navigation agents is providing a learnable\ncurriculum of training environments that can cover the large variety of\nreal-world websites. Therefore, we propose using Adversarial Environment\nGeneration (AEG) to generate challenging web environments in which to train\nreinforcement learning (RL) agents. We provide a new benchmarking environment,\ngMiniWoB, which enables an RL adversary to use compositional primitives to\nlearn to generate arbitrarily complex websites. To train the adversary, we\npropose a new technique for maximizing regret using the difference in the\nscores obtained by a pair of navigator agents. Our results show that our\napproach significantly outperforms prior methods for minimax regret AEG. The\nregret objective trains the adversary to design a curriculum of environments\nthat are \"just-the-right-challenge\" for the navigator agents; our results show\nthat over time, the adversary learns to generate increasingly complex web\nnavigation tasks. The navigator agents trained with our technique learn to\ncomplete challenging, high-dimensional web navigation tasks, such as form\nfilling, booking a flight etc. We show that the navigator agent trained with\nour proposed Flexible b-PAIRED technique significantly outperforms competitive\nautomatic curriculum generation baselines -- including a state-of-the-art RL\nweb navigation approach -- on a set of challenging unseen test environments,\nand achieves more than 80% success rate on some tasks.\n",
        "published": "2021",
        "authors": [
            "Izzeddin Gur",
            "Natasha Jaques",
            "Kevin Malta",
            "Manoj Tiwari",
            "Honglak Lee",
            "Aleksandra Faust"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.03216v2",
        "title": "Continuous Coordination As a Realistic Scenario for Lifelong Learning",
        "abstract": "  Current deep reinforcement learning (RL) algorithms are still highly\ntask-specific and lack the ability to generalize to new environments. Lifelong\nlearning (LLL), however, aims at solving multiple tasks sequentially by\nefficiently transferring and using knowledge between tasks. Despite a surge of\ninterest in lifelong RL in recent years, the lack of a realistic testbed makes\nrobust evaluation of LLL algorithms difficult. Multi-agent RL (MARL), on the\nother hand, can be seen as a natural scenario for lifelong RL due to its\ninherent non-stationarity, since the agents' policies change over time. In this\nwork, we introduce a multi-agent lifelong learning testbed that supports both\nzero-shot and few-shot settings. Our setup is based on Hanabi -- a\npartially-observable, fully cooperative multi-agent game that has been shown to\nbe challenging for zero-shot coordination. Its large strategy space makes it a\ndesirable environment for lifelong RL tasks. We evaluate several recent MARL\nmethods, and benchmark state-of-the-art LLL algorithms in limited memory and\ncomputation regimes to shed light on their strengths and weaknesses. This\ncontinual learning paradigm also provides us with a pragmatic way of going\nbeyond centralized training which is the most commonly used training protocol\nin MARL. We empirically show that the agents trained in our setup are able to\ncoordinate well with unseen agents, without any additional assumptions made by\nprevious works. The code and all pre-trained models are available at\nhttps://github.com/chandar-lab/Lifelong-Hanabi.\n",
        "published": "2021",
        "authors": [
            "Hadi Nekoei",
            "Akilesh Badrinaaraayanan",
            "Aaron Courville",
            "Sarath Chandar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.03786v3",
        "title": "Distributed Dynamic Map Fusion via Federated Learning for Intelligent\n  Networked Vehicles",
        "abstract": "  The technology of dynamic map fusion among networked vehicles has been\ndeveloped to enlarge sensing ranges and improve sensing accuracies for\nindividual vehicles. This paper proposes a federated learning (FL) based\ndynamic map fusion framework to achieve high map quality despite unknown\nnumbers of objects in fields of view (FoVs), various sensing and model\nuncertainties, and missing data labels for online learning. The novelty of this\nwork is threefold: (1) developing a three-stage fusion scheme to predict the\nnumber of objects effectively and to fuse multiple local maps with fidelity\nscores; (2) developing an FL algorithm which fine-tunes feature models (i.e.,\nrepresentation learning networks for feature extraction) distributively by\naggregating model parameters; (3) developing a knowledge distillation method to\ngenerate FL training labels when data labels are unavailable. The proposed\nframework is implemented in the Car Learning to Act (CARLA) simulation\nplatform. Extensive experimental results are provided to verify the superior\nperformance and robustness of the developed map fusion and FL schemes.\n",
        "published": "2021",
        "authors": [
            "Zijian Zhang",
            "Shuai Wang",
            "Yuncong Hong",
            "Liangkai Zhou",
            "Qi Hao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.04555v3",
        "title": "Real-world Ride-hailing Vehicle Repositioning using Deep Reinforcement\n  Learning",
        "abstract": "  We present a new practical framework based on deep reinforcement learning and\ndecision-time planning for real-world vehicle repositioning on ride-hailing (a\ntype of mobility-on-demand, MoD) platforms. Our approach learns the\nspatiotemporal state-value function using a batch training algorithm with deep\nvalue networks. The optimal repositioning action is generated on-demand through\nvalue-based policy search, which combines planning and bootstrapping with the\nvalue networks. For the large-fleet problems, we develop several algorithmic\nfeatures that we incorporate into our framework and that we demonstrate to\ninduce coordination among the algorithmically-guided vehicles. We benchmark our\nalgorithm with baselines in a ride-hailing simulation environment to\ndemonstrate its superiority in improving income efficiency meausred by\nincome-per-hour. We have also designed and run a real-world experiment program\nwith regular drivers on a major ride-hailing platform. We have observed\nsignificantly positive results on key metrics comparing our method with\nexperienced drivers who performed idle-time repositioning based on their own\nexpertise.\n",
        "published": "2021",
        "authors": [
            "Yan Jiao",
            "Xiaocheng Tang",
            "Zhiwei Qin",
            "Shuaiji Li",
            "Fan Zhang",
            "Hongtu Zhu",
            "Jieping Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.04931v4",
        "title": "Monte Carlo Tree Search: A Review of Recent Modifications and\n  Applications",
        "abstract": "  Monte Carlo Tree Search (MCTS) is a powerful approach to designing\ngame-playing bots or solving sequential decision problems. The method relies on\nintelligent tree search that balances exploration and exploitation. MCTS\nperforms random sampling in the form of simulations and stores statistics of\nactions to make more educated choices in each subsequent iteration. The method\nhas become a state-of-the-art technique for combinatorial games, however, in\nmore complex games (e.g. those with high branching factor or real-time ones),\nas well as in various practical domains (e.g. transportation, scheduling or\nsecurity) an efficient MCTS application often requires its problem-dependent\nmodification or integration with other techniques. Such domain-specific\nmodifications and hybrid approaches are the main focus of this survey. The last\nmajor MCTS survey has been published in 2012. Contributions that appeared since\nits release are of particular interest for this review.\n",
        "published": "2021",
        "authors": [
            "Maciej \u015awiechowski",
            "Konrad Godlewski",
            "Bartosz Sawicki",
            "Jacek Ma\u0144dziuk"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.05737v1",
        "title": "The AI Arena: A Framework for Distributed Multi-Agent Reinforcement\n  Learning",
        "abstract": "  Advances in reinforcement learning (RL) have resulted in recent breakthroughs\nin the application of artificial intelligence (AI) across many different\ndomains. An emerging landscape of development environments is making powerful\nRL techniques more accessible for a growing community of researchers. However,\nmost existing frameworks do not directly address the problem of learning in\ncomplex operating environments, such as dense urban settings or defense-related\nscenarios, that incorporate distributed, heterogeneous teams of agents. To help\nenable AI research for this important class of applications, we introduce the\nAI Arena: a scalable framework with flexible abstractions for distributed\nmulti-agent reinforcement learning. The AI Arena extends the OpenAI Gym\ninterface to allow greater flexibility in learning control policies across\nmultiple agents with heterogeneous learning strategies and localized views of\nthe environment. To illustrate the utility of our framework, we present\nexperimental results that demonstrate performance gains due to a distributed\nmulti-agent learning approach over commonly-used RL techniques in several\ndifferent learning environments.\n",
        "published": "2021",
        "authors": [
            "Edward W. Staley",
            "Corban G. Rivera",
            "Ashley J. Llorens"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.06426v2",
        "title": "XDO: A Double Oracle Algorithm for Extensive-Form Games",
        "abstract": "  Policy Space Response Oracles (PSRO) is a reinforcement learning (RL)\nalgorithm for two-player zero-sum games that has been empirically shown to find\napproximate Nash equilibria in large games. Although PSRO is guaranteed to\nconverge to an approximate Nash equilibrium and can handle continuous actions,\nit may take an exponential number of iterations as the number of information\nstates (infostates) grows. We propose Extensive-Form Double Oracle (XDO), an\nextensive-form double oracle algorithm for two-player zero-sum games that is\nguaranteed to converge to an approximate Nash equilibrium linearly in the\nnumber of infostates. Unlike PSRO, which mixes best responses at the root of\nthe game, XDO mixes best responses at every infostate. We also introduce Neural\nXDO (NXDO), where the best response is learned through deep RL. In tabular\nexperiments on Leduc poker, we find that XDO achieves an approximate Nash\nequilibrium in a number of iterations an order of magnitude smaller than PSRO.\nExperiments on a modified Leduc poker game and Oshi-Zumo show that tabular XDO\nachieves a lower exploitability than CFR with the same amount of computation.\nWe also find that NXDO outperforms PSRO and NFSP on a sequential\nmultidimensional continuous-action game. NXDO is the first deep RL method that\ncan find an approximate Nash equilibrium in high-dimensional continuous-action\nsequential games. Experiment code is available at\nhttps://github.com/indylab/nxdo.\n",
        "published": "2021",
        "authors": [
            "Stephen McAleer",
            "John Lanier",
            "Kevin Wang",
            "Pierre Baldi",
            "Roy Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.13026v2",
        "title": "The Gradient Convergence Bound of Federated Multi-Agent Reinforcement\n  Learning with Efficient Communication",
        "abstract": "  The paper considers independent reinforcement learning (IRL) for multi-agent\ncollaborative decision-making in the paradigm of federated learning (FL).\nHowever, FL generates excessive communication overheads between agents and a\nremote central server, especially when it involves a large number of agents or\niterations. Besides, due to the heterogeneity of independent learning\nenvironments, multiple agents may undergo asynchronous Markov decision\nprocesses (MDPs), which will affect the training samples and the model's\nconvergence performance. On top of the variation-aware periodic averaging (VPA)\nmethod and the policy-based deep reinforcement learning (DRL) algorithm (i.e.,\nproximal policy optimization (PPO)), this paper proposes two advanced\noptimization schemes orienting to stochastic gradient descent (SGD): 1) A\ndecay-based scheme gradually decays the weights of a model's local gradients\nwith the progress of successive local updates, and 2) By representing the\nagents as a graph, a consensus-based scheme studies the impact of exchanging a\nmodel's local gradients among nearby agents from an algebraic connectivity\nperspective. This paper also provides novel convergence guarantees for both\ndeveloped schemes, and demonstrates their superior effectiveness and efficiency\nin improving the system's utility value through theoretical analyses and\nsimulation results.\n",
        "published": "2021",
        "authors": [
            "Xing Xu",
            "Rongpeng Li",
            "Zhifeng Zhao",
            "Honggang Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.16985v1",
        "title": "Energy Efficient Edge Computing: When Lyapunov Meets Distributed\n  Reinforcement Learning",
        "abstract": "  In this work, we study the problem of energy-efficient computation offloading\nenabled by edge computing. In the considered scenario, multiple users\nsimultaneously compete for limited radio and edge computing resources to get\noffloaded tasks processed under a delay constraint, with the possibility of\nexploiting low power sleep modes at all network nodes. The radio resource\nallocation takes into account inter- and intra-cell interference, and the duty\ncycles of the radio and computing equipment have to be jointly optimized to\nminimize the overall energy consumption. To address this issue, we formulate\nthe underlying problem as a dynamic long-term optimization. Then, based on\nLyapunov stochastic optimization tools, we decouple the formulated problem into\na CPU scheduling problem and a radio resource allocation problem to be solved\nin a per-slot basis. Whereas the first one can be optimally and efficiently\nsolved using a fast iterative algorithm, the second one is solved using\ndistributed multi-agent reinforcement learning due to its non-convexity and\nNP-hardness. The resulting framework achieves up to 96.5% performance of the\noptimal strategy based on exhaustive search, while drastically reducing\ncomplexity. The proposed solution also allows to increase the network's energy\nefficiency compared to a benchmark heuristic approach.\n",
        "published": "2021",
        "authors": [
            "Mohamed Sana",
            "Mattia Merluzzi",
            "Nicola di Pietro",
            "Emilio Calvanese Strinati"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.03503v1",
        "title": "Learning to Coordinate via Multiple Graph Neural Networks",
        "abstract": "  The collaboration between agents has gradually become an important topic in\nmulti-agent systems. The key is how to efficiently solve the credit assignment\nproblems. This paper introduces MGAN for collaborative multi-agent\nreinforcement learning, a new algorithm that combines graph convolutional\nnetworks and value-decomposition methods. MGAN learns the representation of\nagents from different perspectives through multiple graph networks, and\nrealizes the proper allocation of attention between all agents. We show the\namazing ability of the graph network in representation learning by visualizing\nthe output of the graph network, and therefore improve interpretability for the\nactions of each agent in the multi-agent system.\n",
        "published": "2021",
        "authors": [
            "Zhiwei Xu",
            "Bin Zhang",
            "Yunpeng Bai",
            "Dapeng Li",
            "Guoliang Fan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.08203v1",
        "title": "Why Machine Learning Integrated Patient Flow Simulation?",
        "abstract": "  Patient flow analysis can be studied from a clinical and or operational\nperspective using simulation. Traditional statistical methods such as\nstochastic distribution methods have been used to construct patient flow\nsimulation submodels such as patient inflow, Length of Stay (LoS), Cost of\nTreatment (CoT) and Clinical Pathway (CP) models. However, patient inflow\ndemonstrates seasonality, trend and variation over time. LoS, CoT and CP are\nsignificantly determined by attributes of patients and clinical and laboratory\ntest results. For this reason, patient flow simulation models constructed using\ntraditional statistical methods are criticized for ignoring heterogeneity and\ntheir contribution to personalized and value based healthcare. On the other\nhand, machine learning methods have proven to be efficient to study and predict\nadmission rate, LoS, CoT, and CP. This paper, hence, describes why coupling\nmachine learning with patient flow simulation is important and proposes a\nconceptual architecture that shows how to integrate machine learning with\npatient flow simulation.\n",
        "published": "2021",
        "authors": [
            "Tesfamariam M. Abuhay",
            "Adane Mamuye",
            "Stewart Robinson",
            "Sergey V. Kovalchuk"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.00897v1",
        "title": "Solving Large-Scale Extensive-Form Network Security Games via Neural\n  Fictitious Self-Play",
        "abstract": "  Securing networked infrastructures is important in the real world. The\nproblem of deploying security resources to protect against an attacker in\nnetworked domains can be modeled as Network Security Games (NSGs).\nUnfortunately, existing approaches, including the deep learning-based\napproaches, are inefficient to solve large-scale extensive-form NSGs. In this\npaper, we propose a novel learning paradigm, NSG-NFSP, to solve large-scale\nextensive-form NSGs based on Neural Fictitious Self-Play (NFSP). Our main\ncontributions include: i) reforming the best response (BR) policy network in\nNFSP to be a mapping from action-state pair to action-value, to make the\ncalculation of BR possible in NSGs; ii) converting the average policy network\nof an NFSP agent into a metric-based classifier, helping the agent to assign\ndistributions only on legal actions rather than all actions; iii) enabling NFSP\nwith high-level actions, which can benefit training efficiency and stability in\nNSGs; and iv) leveraging information contained in graphs of NSGs by learning\nefficient graph node embeddings. Our algorithm significantly outperforms\nstate-of-the-art algorithms in both scalability and solution quality.\n",
        "published": "2021",
        "authors": [
            "Wanqi Xue",
            "Youzhi Zhang",
            "Shuxin Li",
            "Xinrun Wang",
            "Bo An",
            "Chai Kiat Yeo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.03215v2",
        "title": "PreferenceNet: Encoding Human Preferences in Auction Design with Deep\n  Learning",
        "abstract": "  The design of optimal auctions is a problem of interest in economics, game\ntheory and computer science. Despite decades of effort, strategyproof,\nrevenue-maximizing auction designs are still not known outside of restricted\nsettings. However, recent methods using deep learning have shown some success\nin approximating optimal auctions, recovering several known solutions and\noutperforming strong baselines when optimal auctions are not known. In addition\nto maximizing revenue, auction mechanisms may also seek to encourage socially\ndesirable constraints such as allocation fairness or diversity. However, these\nphilosophical notions neither have standardization nor do they have widely\naccepted formal definitions. In this paper, we propose PreferenceNet, an\nextension of existing neural-network-based auction mechanisms to encode\nconstraints using (potentially human-provided) exemplars of desirable\nallocations. In addition, we introduce a new metric to evaluate an auction\nallocations' adherence to such socially desirable constraints and demonstrate\nthat our proposed method is competitive with current state-of-the-art\nneural-network based auction designs. We validate our approach through human\nsubject research and show that we are able to effectively capture real human\npreferences. Our code is available at\nhttps://github.com/neeharperi/PreferenceNet\n",
        "published": "2021",
        "authors": [
            "Neehar Peri",
            "Michael J. Curry",
            "Samuel Dooley",
            "John P. Dickerson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.03927v1",
        "title": "Improving Social Welfare While Preserving Autonomy via a Pareto Mediator",
        "abstract": "  Machine learning algorithms often make decisions on behalf of agents with\nvaried and sometimes conflicting interests. In domains where agents can choose\nto take their own action or delegate their action to a central mediator, an\nopen question is how mediators should take actions on behalf of delegating\nagents. The main existing approach uses delegating agents to punish\nnon-delegating agents in an attempt to get all agents to delegate, which tends\nto be costly for all. We introduce a Pareto Mediator which aims to improve\noutcomes for delegating agents without making any of them worse off. Our\nexperiments in random normal form games, a restaurant recommendation game, and\na reinforcement learning sequential social dilemma show that the Pareto\nMediator greatly increases social welfare. Also, even when the Pareto Mediator\nis based on an incorrect model of agent utility, performance gracefully\ndegrades to the pre-intervention level, due to the individual autonomy\npreserved by the voluntary mediator.\n",
        "published": "2021",
        "authors": [
            "Stephen McAleer",
            "John Lanier",
            "Michael Dennis",
            "Pierre Baldi",
            "Roy Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.04219v1",
        "title": "Time-series Imputation of Temporally-occluded Multiagent Trajectories",
        "abstract": "  In multiagent environments, several decision-making individuals interact\nwhile adhering to the dynamics constraints imposed by the environment. These\ninteractions, combined with the potential stochasticity of the agents'\ndecision-making processes, make such systems complex and interesting to study\nfrom a dynamical perspective. Significant research has been conducted on\nlearning models for forward-direction estimation of agent behaviors, for\nexample, pedestrian predictions used for collision-avoidance in self-driving\ncars. However, in many settings, only sporadic observations of agents may be\navailable in a given trajectory sequence. For instance, in football, subsets of\nplayers may come in and out of view of broadcast video footage, while\nunobserved players continue to interact off-screen. In this paper, we study the\nproblem of multiagent time-series imputation, where available past and future\nobservations of subsets of agents are used to estimate missing observations for\nother agents. Our approach, called the Graph Imputer, uses forward- and\nbackward-information in combination with graph networks and variational\nautoencoders to enable learning of a distribution of imputed trajectories. We\nevaluate our approach on a dataset of football matches, using a projective\ncamera module to train and evaluate our model for the off-screen player state\nestimation setting. We illustrate that our method outperforms several\nstate-of-the-art approaches, including those hand-crafted for football.\n",
        "published": "2021",
        "authors": [
            "Shayegan Omidshafiei",
            "Daniel Hennes",
            "Marta Garnelo",
            "Eugene Tarassov",
            "Zhe Wang",
            "Romuald Elie",
            "Jerome T. Connor",
            "Paul Muller",
            "Ian Graham",
            "William Spearman",
            "Karl Tuyls"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.04958v2",
        "title": "Unifying Behavioral and Response Diversity for Open-ended Learning in\n  Zero-sum Games",
        "abstract": "  Measuring and promoting policy diversity is critical for solving games with\nstrong non-transitive dynamics where strategic cycles exist, and there is no\nconsistent winner (e.g., Rock-Paper-Scissors). With that in mind, maintaining a\npool of diverse policies via open-ended learning is an attractive solution,\nwhich can generate auto-curricula to avoid being exploited. However, in\nconventional open-ended learning algorithms, there are no widely accepted\ndefinitions for diversity, making it hard to construct and evaluate the diverse\npolicies. In this work, we summarize previous concepts of diversity and work\ntowards offering a unified measure of diversity in multi-agent open-ended\nlearning to include all elements in Markov games, based on both Behavioral\nDiversity (BD) and Response Diversity (RD). At the trajectory distribution\nlevel, we re-define BD in the state-action space as the discrepancies of\noccupancy measures. For the reward dynamics, we propose RD to characterize\ndiversity through the responses of policies when encountering different\nopponents. We also show that many current diversity measures fall in one of the\ncategories of BD or RD but not both. With this unified diversity measure, we\ndesign the corresponding diversity-promoting objective and population\neffectivity when seeking the best responses in open-ended learning. We validate\nour methods in both relatively simple games like matrix game, non-transitive\nmixture model, and the complex \\textit{Google Research Football} environment.\nThe population found by our methods reveals the lowest exploitability, highest\npopulation effectivity in matrix game and non-transitive mixture model, as well\nas the largest goal difference when interacting with opponents of various\nlevels in \\textit{Google Research Football}.\n",
        "published": "2021",
        "authors": [
            "Xiangyu Liu",
            "Hangtian Jia",
            "Ying Wen",
            "Yaodong Yang",
            "Yujing Hu",
            "Yingfeng Chen",
            "Changjie Fan",
            "Zhipeng Hu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.05727v3",
        "title": "Cooperative Multi-Agent Fairness and Equivariant Policies",
        "abstract": "  We study fairness through the lens of cooperative multi-agent learning. Our\nwork is motivated by empirical evidence that naive maximization of team reward\nyields unfair outcomes for individual team members. To address fairness in\nmulti-agent contexts, we introduce team fairness, a group-based fairness\nmeasure for multi-agent learning. We then prove that it is possible to enforce\nteam fairness during policy optimization by transforming the team's joint\npolicy into an equivariant map. We refer to our multi-agent learning strategy\nas Fairness through Equivariance (Fair-E) and demonstrate its effectiveness\nempirically. We then introduce Fairness through Equivariance Regularization\n(Fair-ER) as a soft-constraint version of Fair-E and show that it reaches\nhigher levels of utility than Fair-E and fairer outcomes than non-equivariant\npolicies. Finally, we present novel findings regarding the fairness-utility\ntrade-off in multi-agent settings; showing that the magnitude of the trade-off\nis dependent on agent skill.\n",
        "published": "2021",
        "authors": [
            "Niko A. Grupen",
            "Bart Selman",
            "Daniel D. Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.05802v2",
        "title": "Metric Policy Representations for Opponent Modeling",
        "abstract": "  In multi-agent reinforcement learning, the inherent non-stationarity of the\nenvironment caused by other agents' actions posed significant difficulties for\nan agent to learn a good policy independently. One way to deal with\nnon-stationarity is opponent modeling, by which the agent takes into\nconsideration the influence of other agents' policies. Most existing work\nrelies on predicting other agents' actions or goals, or discriminating between\ndifferent policies. However, such modeling fails to capture the similarities\nand differences between policies simultaneously and thus cannot provide enough\nuseful information when generalizing to unseen agents. To address this, we\npropose a general method to learn representations of other agents' policies,\nsuch that the distance between policies is deliberately reflected by the\ndistance between representations, while the policy distance is inferred from\nthe sampled joint action distributions during training. We empirically show\nthat the agent conditioned on the learned policy representation can well\ngeneralize to unseen agents in three multi-agent tasks.\n",
        "published": "2021",
        "authors": [
            "Haobin Jiang",
            "Yifan Yu",
            "Zongqing Lu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.06762v2",
        "title": "Solving Graph-based Public Good Games with Tree Search and Imitation\n  Learning",
        "abstract": "  Public goods games represent insightful settings for studying incentives for\nindividual agents to make contributions that, while costly for each of them,\nbenefit the wider society. In this work, we adopt the perspective of a central\nplanner with a global view of a network of self-interested agents and the goal\nof maximizing some desired property in the context of a best-shot public goods\ngame. Existing algorithms for this known NP-complete problem find solutions\nthat are sub-optimal and cannot optimize for criteria other than social\nwelfare.\n  In order to efficiently solve public goods games, our proposed method\ndirectly exploits the correspondence between equilibria and the Maximal\nIndependent Set (mIS) structural property of graphs. In particular, we define a\nMarkov Decision Process which incrementally generates an mIS, and adopt a\nplanning method to search for equilibria, outperforming existing methods.\nFurthermore, we devise a graph imitation learning technique that uses\ndemonstrations of the search to obtain a graph neural network parametrized\npolicy which quickly generalizes to unseen game instances. Our evaluation\nresults show that this policy is able to reach 99.5% of the performance of the\nplanning method while being three orders of magnitude faster to evaluate on the\nlargest graphs tested. The methods presented in this work can be applied to a\nlarge class of public goods games of potentially high societal impact and more\nbroadly to other graph combinatorial optimization problems.\n",
        "published": "2021",
        "authors": [
            "Victor-Alexandru Darvariu",
            "Stephen Hailes",
            "Mirco Musolesi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.06828v1",
        "title": "A Game-Theoretic Approach to Multi-Agent Trust Region Optimization",
        "abstract": "  Trust region methods are widely applied in single-agent reinforcement\nlearning problems due to their monotonic performance-improvement guarantee at\nevery iteration. Nonetheless, when applied in multi-agent settings, the\nguarantee of trust region methods no longer holds because an agent's payoff is\nalso affected by other agents' adaptive behaviors. To tackle this problem, we\nconduct a game-theoretical analysis in the policy space, and propose a\nmulti-agent trust region learning method (MATRL), which enables trust region\noptimization for multi-agent learning. Specifically, MATRL finds a stable\nimprovement direction that is guided by the solution concept of Nash\nequilibrium at the meta-game level. We derive the monotonic improvement\nguarantee in multi-agent settings and empirically show the local convergence of\nMATRL to stable fixed points in the two-player rotational differential game. To\ntest our method, we evaluate MATRL in both discrete and continuous multiplayer\ngeneral-sum games including checker and switch grid worlds, multi-agent MuJoCo,\nand Atari games. Results suggest that MATRL significantly outperforms strong\nmulti-agent reinforcement learning baselines.\n",
        "published": "2021",
        "authors": [
            "Ying Wen",
            "Hui Chen",
            "Yaodong Yang",
            "Zheng Tian",
            "Minne Li",
            "Xu Chen",
            "Jun Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.08413v1",
        "title": "Robust Reinforcement Learning Under Minimax Regret for Green Security",
        "abstract": "  Green security domains feature defenders who plan patrols in the face of\nuncertainty about the adversarial behavior of poachers, illegal loggers, and\nillegal fishers. Importantly, the deterrence effect of patrols on adversaries'\nfuture behavior makes patrol planning a sequential decision-making problem.\nTherefore, we focus on robust sequential patrol planning for green security\nfollowing the minimax regret criterion, which has not been considered in the\nliterature. We formulate the problem as a game between the defender and nature\nwho controls the parameter values of the adversarial behavior and design an\nalgorithm MIRROR to find a robust policy. MIRROR uses two reinforcement\nlearning-based oracles and solves a restricted game considering limited\ndefender strategies and parameter values. We evaluate MIRROR on real-world\npoaching data.\n",
        "published": "2021",
        "authors": [
            "Lily Xu",
            "Andrew Perrault",
            "Fei Fang",
            "Haipeng Chen",
            "Milind Tambe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.09435v2",
        "title": "Multi-Agent Training beyond Zero-Sum with Correlated Equilibrium\n  Meta-Solvers",
        "abstract": "  Two-player, constant-sum games are well studied in the literature, but there\nhas been limited progress outside of this setting. We propose Joint\nPolicy-Space Response Oracles (JPSRO), an algorithm for training agents in\nn-player, general-sum extensive form games, which provably converges to an\nequilibrium. We further suggest correlated equilibria (CE) as promising\nmeta-solvers, and propose a novel solution concept Maximum Gini Correlated\nEquilibrium (MGCE), a principled and computationally efficient family of\nsolutions for solving the correlated equilibrium selection problem. We conduct\nseveral experiments using CE meta-solvers for JPSRO and demonstrate convergence\non n-player, general-sum games.\n",
        "published": "2021",
        "authors": [
            "Luke Marris",
            "Paul Muller",
            "Marc Lanctot",
            "Karl Tuyls",
            "Thore Graepel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.09825v1",
        "title": "Many Agent Reinforcement Learning Under Partial Observability",
        "abstract": "  Recent renewed interest in multi-agent reinforcement learning (MARL) has\ngenerated an impressive array of techniques that leverage deep reinforcement\nlearning, primarily actor-critic architectures, and can be applied to a limited\nrange of settings in terms of observability and communication. However, a\ncontinuing limitation of much of this work is the curse of dimensionality when\nit comes to representations based on joint actions, which grow exponentially\nwith the number of agents. In this paper, we squarely focus on this challenge\nof scalability. We apply the key insight of action anonymity, which leads to\npermutation invariance of joint actions, to two recently presented deep MARL\nalgorithms, MADDPG and IA2C, and compare these instantiations to another recent\ntechnique that leverages action anonymity, viz., mean-field MARL. We show that\nour instantiations can learn the optimal behavior in a broader class of agent\nnetworks than the mean-field method, using a recently introduced pragmatic\ndomain.\n",
        "published": "2021",
        "authors": [
            "Keyang He",
            "Prashant Doshi",
            "Bikramjit Banerjee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.11156v3",
        "title": "Multi-Agent Curricula and Emergent Implicit Signaling",
        "abstract": "  Emergent communication has made strides towards learning communication from\nscratch, but has focused primarily on protocols that resemble human language.\nIn nature, multi-agent cooperation gives rise to a wide range of communication\nthat varies in structure and complexity. In this work, we recognize the full\nspectrum of communication that exists in nature and propose studying\nlower-level communication. Specifically, we study emergent implicit signaling\nin the context of decentralized multi-agent learning in difficult, sparse\nreward environments. However, learning to coordinate in such environments is\nchallenging. We propose a curriculum-driven strategy that combines: (i)\nvelocity-based environment shaping, tailored to the skill level of the\nmulti-agent team; and (ii) a behavioral curriculum that helps agents learn\nsuccessful single-agent behaviors as a precursor to learning multi-agent\nbehaviors. Pursuit-evasion experiments show that our approach learns effective\ncoordination, significantly outperforming sophisticated analytical and learned\npolicies. Our method completes the pursuit-evasion task even when pursuers move\nat half of the evader's speed, whereas the highest-performing baseline fails at\n80% of the evader's speed. Moreover, we examine the use of implicit signals in\ncoordination through position-based social influence. We show that pursuers\ntrained with our strategy exchange more than twice as much information (in\nbits) than baseline methods, indicating that our method has learned, and relies\nheavily on, the exchange of implicit signals.\n",
        "published": "2021",
        "authors": [
            "Niko A. Grupen",
            "Daniel D. Lee",
            "Bart Selman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.11345v1",
        "title": "Cogment: Open Source Framework For Distributed Multi-actor Training,\n  Deployment & Operations",
        "abstract": "  Involving humans directly for the benefit of AI agents' training is getting\ntraction thanks to several advances in reinforcement learning and\nhuman-in-the-loop learning. Humans can provide rewards to the agent,\ndemonstrate tasks, design a curriculum, or act in the environment, but these\nbenefits also come with architectural, functional design and engineering\ncomplexities. We present Cogment, a unifying open-source framework that\nintroduces an actor formalism to support a variety of humans-agents\ncollaboration typologies and training approaches. It is also scalable out of\nthe box thanks to a distributed micro service architecture, and offers\nsolutions to the aforementioned complexities.\n",
        "published": "2021",
        "authors": [
            "AI Redefined",
            "Sai Krishna Gottipati",
            "Sagar Kurandwad",
            "Clod\u00e9ric Mars",
            "Gregory Szriftgiser",
            "Fran\u00e7ois Chabot"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.11652v1",
        "title": "MMD-MIX: Value Function Factorisation with Maximum Mean Discrepancy for\n  Cooperative Multi-Agent Reinforcement Learning",
        "abstract": "  In the real world, many tasks require multiple agents to cooperate with each\nother under the condition of local observations. To solve such problems, many\nmulti-agent reinforcement learning methods based on Centralized Training with\nDecentralized Execution have been proposed. One representative class of work is\nvalue decomposition, which decomposes the global joint Q-value $Q_\\text{jt}$\ninto individual Q-values $Q_a$ to guide individuals' behaviors, e.g. VDN\n(Value-Decomposition Networks) and QMIX. However, these baselines often ignore\nthe randomness in the situation. We propose MMD-MIX, a method that combines\ndistributional reinforcement learning and value decomposition to alleviate the\nabove weaknesses. Besides, to improve data sampling efficiency, we were\ninspired by REM (Random Ensemble Mixture) which is a robust RL algorithm to\nexplicitly introduce randomness into the MMD-MIX. The experiments demonstrate\nthat MMD-MIX outperforms prior baselines in the StarCraft Multi-Agent Challenge\n(SMAC) environment.\n",
        "published": "2021",
        "authors": [
            "Zhiwei Xu",
            "Dapeng Li",
            "Yunpeng Bai",
            "Guoliang Fan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.01466v2",
        "title": "Risk Adversarial Learning System for Connected and Autonomous Vehicle\n  Charging",
        "abstract": "  In this paper, the design of a rational decision support system (RDSS) for a\nconnected and autonomous vehicle charging infrastructure (CAV-CI) is studied.\nIn the considered CAV-CI, the distribution system operator (DSO) deploys\nelectric vehicle supply equipment (EVSE) to provide an EV charging facility for\nhuman-driven connected vehicles (CVs) and autonomous vehicles (AVs). The\ncharging request by the human-driven EV becomes irrational when it demands more\nenergy and charging period than its actual need. Therefore, the scheduling\npolicy of each EVSE must be adaptively accumulated the irrational charging\nrequest to satisfy the charging demand of both CVs and AVs. To tackle this, we\nformulate an RDSS problem for the DSO, where the objective is to maximize the\ncharging capacity utilization by satisfying the laxity risk of the DSO. Thus,\nwe devise a rational reward maximization problem to adapt the irrational\nbehavior by CVs in a data-informed manner. We propose a novel risk adversarial\nmulti-agent learning system (RAMALS) for CAV-CI to solve the formulated RDSS\nproblem. In RAMALS, the DSO acts as a centralized risk adversarial agent (RAA)\nfor informing the laxity risk to each EVSE. Subsequently, each EVSE plays the\nrole of a self-learner agent to adaptively schedule its own EV sessions by\ncoping advice from RAA. Experiment results show that the proposed RAMALS\naffords around 46.6% improvement in charging rate, about 28.6% improvement in\nthe EVSE's active charging time and at least 33.3% more energy utilization, as\ncompared to a currently deployed ACN EVSE system, and other baselines.\n",
        "published": "2021",
        "authors": [
            "Md. Shirajum Munir",
            "Ki Tae Kim",
            "Kyi Thar",
            "Dusit Niyato",
            "Choong Seon Hong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.01843v2",
        "title": "Model-Based Opponent Modeling",
        "abstract": "  When one agent interacts with a multi-agent environment, it is challenging to\ndeal with various opponents unseen before. Modeling the behaviors, goals, or\nbeliefs of opponents could help the agent adjust its policy to adapt to\ndifferent opponents. In addition, it is also important to consider opponents\nwho are learning simultaneously or capable of reasoning. However, existing work\nusually tackles only one of the aforementioned types of opponents. In this\npaper, we propose model-based opponent modeling (MBOM), which employs the\nenvironment model to adapt to all kinds of opponents. MBOM simulates the\nrecursive reasoning process in the environment model and imagines a set of\nimproving opponent policies. To effectively and accurately represent the\nopponent policy, MBOM further mixes the imagined opponent policies according to\nthe similarity with the real behaviors of opponents. Empirically, we show that\nMBOM achieves more effective adaptation than existing methods in a variety of\ntasks, respectively with different types of opponents, i.e., fixed policy,\nna\\\"ive learner, and reasoning learner.\n",
        "published": "2021",
        "authors": [
            "Xiaopeng Yu",
            "Jiechuan Jiang",
            "Wanpeng Zhang",
            "Haobin Jiang",
            "Zongqing Lu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.04087v1",
        "title": "Reinforcement Learning for Intelligent Healthcare Systems: A\n  Comprehensive Survey",
        "abstract": "  The rapid increase in the percentage of chronic disease patients along with\nthe recent pandemic pose immediate threats on healthcare expenditure and\nelevate causes of death. This calls for transforming healthcare systems away\nfrom one-on-one patient treatment into intelligent health systems, to improve\nservices, access and scalability, while reducing costs. Reinforcement Learning\n(RL) has witnessed an intrinsic breakthrough in solving a variety of complex\nproblems for diverse applications and services. Thus, we conduct in this paper\na comprehensive survey of the recent models and techniques of RL that have been\ndeveloped/used for supporting Intelligent-healthcare (I-health) systems. This\npaper can guide the readers to deeply understand the state-of-the-art regarding\nthe use of RL in the context of I-health. Specifically, we first present an\noverview for the I-health systems challenges, architecture, and how RL can\nbenefit these systems. We then review the background and mathematical modeling\nof different RL, Deep RL (DRL), and multi-agent RL models. After that, we\nprovide a deep literature review for the applications of RL in I-health\nsystems. In particular, three main areas have been tackled, i.e., edge\nintelligence, smart core network, and dynamic treatment regimes. Finally, we\nhighlight emerging challenges and outline future research directions in driving\nthe future success of RL in I-health systems, which opens the door for\nexploring some interesting and unsolved problems.\n",
        "published": "2021",
        "authors": [
            "Alaa Awad Abdellatif",
            "Naram Mhaisen",
            "Zina Chkirbene",
            "Amr Mohamed",
            "Aiman Erbad",
            "Mohsen Guizani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.06589v1",
        "title": "A Microscopic Pandemic Simulator for Pandemic Prediction Using Scalable\n  Million-Agent Reinforcement Learning",
        "abstract": "  Microscopic epidemic models are powerful tools for government policy makers\nto predict and simulate epidemic outbreaks, which can capture the impact of\nindividual behaviors on the macroscopic phenomenon. However, existing models\nonly consider simple rule-based individual behaviors, limiting their\napplicability. This paper proposes a deep-reinforcement-learning-powered\nmicroscopic model named Microscopic Pandemic Simulator (MPS). By replacing\nrule-based agents with rational agents whose behaviors are driven to maximize\nrewards, the MPS provides a better approximation of real world dynamics. To\nefficiently simulate with massive amounts of agents in MPS, we propose Scalable\nMillion-Agent DQN (SMADQN). The MPS allows us to efficiently evaluate the\nimpact of different government strategies. This paper first calibrates the MPS\nagainst real-world data in Allegheny, US, then demonstratively evaluates two\ngovernment strategies: information disclosure and quarantine. The results\nvalidate the effectiveness of the proposed method. As a broad impact, this\npaper provides novel insights for the application of DRL in large scale\nagent-based networks such as economic and social networks.\n",
        "published": "2021",
        "authors": [
            "Zhenggang Tang",
            "Kai Yan",
            "Liting Sun",
            "Wei Zhan",
            "Changliu Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.08612v3",
        "title": "Settling the Variance of Multi-Agent Policy Gradients",
        "abstract": "  Policy gradient (PG) methods are popular reinforcement learning (RL) methods\nwhere a baseline is often applied to reduce the variance of gradient estimates.\nIn multi-agent RL (MARL), although the PG theorem can be naturally extended,\nthe effectiveness of multi-agent PG (MAPG) methods degrades as the variance of\ngradient estimates increases rapidly with the number of agents. In this paper,\nwe offer a rigorous analysis of MAPG methods by, firstly, quantifying the\ncontributions of the number of agents and agents' explorations to the variance\nof MAPG estimators. Based on this analysis, we derive the optimal baseline (OB)\nthat achieves the minimal variance. In comparison to the OB, we measure the\nexcess variance of existing MARL algorithms such as vanilla MAPG and COMA.\nConsidering using deep neural networks, we also propose a surrogate version of\nOB, which can be seamlessly plugged into any existing PG methods in MARL. On\nbenchmarks of Multi-Agent MuJoCo and StarCraft challenges, our OB technique\neffectively stabilises training and improves the performance of multi-agent PPO\nand COMA algorithms by a significant margin.\n",
        "published": "2021",
        "authors": [
            "Jakub Grudzien Kuba",
            "Muning Wen",
            "Yaodong Yang",
            "Linghui Meng",
            "Shangding Gu",
            "Haifeng Zhang",
            "David Henry Mguni",
            "Jun Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.01178v1",
        "title": "Multi-Agent Inverse Reinforcement Learning: Suboptimal Demonstrations\n  and Alternative Solution Concepts",
        "abstract": "  Multi-agent inverse reinforcement learning (MIRL) can be used to learn reward\nfunctions from agents in social environments. To model realistic social\ndynamics, MIRL methods must account for suboptimal human reasoning and\nbehavior. Traditional formalisms of game theory provide computationally\ntractable behavioral models, but assume agents have unrealistic cognitive\ncapabilities. This research identifies and compares mechanisms in MIRL methods\nwhich a) handle noise, biases and heuristics in agent decision making and b)\nmodel realistic equilibrium solution concepts. MIRL research is systematically\nreviewed to identify solutions for these challenges. The methods and results of\nthese studies are analyzed and compared based on factors including performance\naccuracy, efficiency, and descriptive quality. We found that the primary\nmethods for handling noise, biases and heuristics in MIRL were extensions of\nMaximum Entropy (MaxEnt) IRL to multi-agent settings. We also found that many\nsuccessful solution concepts are generalizations of the traditional Nash\nEquilibrium (NE). These solutions include the correlated equilibrium, logistic\nstochastic best response equilibrium and entropy regularized mean field NE.\nMethods which use recursive reasoning or updating also perform well, including\nthe feedback NE and archive multi-agent adversarial IRL. Success in modeling\nspecific biases and heuristics in single-agent IRL and promising results using\na Theory of Mind approach in MIRL imply that modeling specific biases and\nheuristics may be useful. Flexibility and unbiased inference in the identified\nalternative solution concepts suggest that a solution concept which has both\nrecursive and generalized characteristics may perform well at modeling\nrealistic social interactions.\n",
        "published": "2021",
        "authors": [
            "Sage Bergerson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.01417v4",
        "title": "Event-Based Communication in Distributed Q-Learning",
        "abstract": "  We present an approach to reduce the communication of information needed on a\nDistributed Q-Learning system inspired by Event Triggered Control (ETC)\ntechniques. We consider a baseline scenario of a distributed Q-learning problem\non a Markov Decision Process (MDP). Following an event-based approach, N agents\nexplore the MDP and communicate experiences to a central learner only when\nnecessary, which performs updates of the actor Q functions. We design an Event\nBased distributed Q learning system (EBd-Q), and derive convergence guarantees\nwith respect to a vanilla Q-learning algorithm. We present experimental results\nshowing that event-based communication results in a substantial reduction of\ndata transmission rates in such distributed systems. Additionally, we discuss\nwhat effects (desired and undesired) these event-based approaches have on the\nlearning processes studied, and how they can be applied to more complex\nmulti-agent systems.\n",
        "published": "2021",
        "authors": [
            "Daniel Jarne Ornia",
            "Manuel Mazo Jr"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.02032v1",
        "title": "Soft Hierarchical Graph Recurrent Networks for Many-Agent Partially\n  Observable Environments",
        "abstract": "  The recent progress in multi-agent deep reinforcement learning(MADRL) makes\nit more practical in real-world tasks, but its relatively poor scalability and\nthe partially observable constraints raise challenges to its performance and\ndeployment. Based on our intuitive observation that the human society could be\nregarded as a large-scale partially observable environment, where each\nindividual has the function of communicating with neighbors and remembering its\nown experience, we propose a novel network structure called hierarchical graph\nrecurrent network(HGRN) for multi-agent cooperation under partial\nobservability. Specifically, we construct the multi-agent system as a graph,\nuse the hierarchical graph attention network(HGAT) to achieve communication\nbetween neighboring agents, and exploit GRU to enable agents to record\nhistorical information. To encourage exploration and improve robustness, we\ndesign a maximum-entropy learning method to learn stochastic policies of a\nconfigurable target action entropy. Based on the above technologies, we\nproposed a value-based MADRL algorithm called Soft-HGRN and its actor-critic\nvariant named SAC-HRGN. Experimental results based on three homogeneous tasks\nand one heterogeneous environment not only show that our approach achieves\nclear improvements compared with four baselines, but also demonstrates the\ninterpretability, scalability, and transferability of the proposed model.\nAblation studies prove the function and necessity of each component.\n",
        "published": "2021",
        "authors": [
            "Zhenhui Ye",
            "Xiaohong Jiang",
            "Guanghua Song",
            "Bowei Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.04024v3",
        "title": "On the Approximation of Cooperative Heterogeneous Multi-Agent\n  Reinforcement Learning (MARL) using Mean Field Control (MFC)",
        "abstract": "  Mean field control (MFC) is an effective way to mitigate the curse of\ndimensionality of cooperative multi-agent reinforcement learning (MARL)\nproblems. This work considers a collection of $N_{\\mathrm{pop}}$ heterogeneous\nagents that can be segregated into $K$ classes such that the $k$-th class\ncontains $N_k$ homogeneous agents. We aim to prove approximation guarantees of\nthe MARL problem for this heterogeneous system by its corresponding MFC\nproblem. We consider three scenarios where the reward and transition dynamics\nof all agents are respectively taken to be functions of $(1)$ joint state and\naction distributions across all classes, $(2)$ individual distributions of each\nclass, and $(3)$ marginal distributions of the entire population. We show that,\nin these cases, the $K$-class MARL problem can be approximated by MFC with\nerrors given as\n$e_1=\\mathcal{O}(\\frac{\\sqrt{|\\mathcal{X}|}+\\sqrt{|\\mathcal{U}|}}{N_{\\mathrm{pop}}}\\sum_{k}\\sqrt{N_k})$,\n$e_2=\\mathcal{O}(\\left[\\sqrt{|\\mathcal{X}|}+\\sqrt{|\\mathcal{U}|}\\right]\\sum_{k}\\frac{1}{\\sqrt{N_k}})$\nand\n$e_3=\\mathcal{O}\\left(\\left[\\sqrt{|\\mathcal{X}|}+\\sqrt{|\\mathcal{U}|}\\right]\\left[\\frac{A}{N_{\\mathrm{pop}}}\\sum_{k\\in[K]}\\sqrt{N_k}+\\frac{B}{\\sqrt{N_{\\mathrm{pop}}}}\\right]\\right)$,\nrespectively, where $A, B$ are some constants and $|\\mathcal{X}|,|\\mathcal{U}|$\nare the sizes of state and action spaces of each agent. Finally, we design a\nNatural Policy Gradient (NPG) based algorithm that, in the three cases stated\nabove, can converge to an optimal MARL policy within $\\mathcal{O}(e_j)$ error\nwith a sample complexity of $\\mathcal{O}(e_j^{-3})$, $j\\in\\{1,2,3\\}$,\nrespectively.\n",
        "published": "2021",
        "authors": [
            "Washim Uddin Mondal",
            "Mridul Agarwal",
            "Vaneet Aggarwal",
            "Satish V. Ukkusuri"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.04149v1",
        "title": "DROP: Deep relocating option policy for optimal ride-hailing vehicle\n  repositioning",
        "abstract": "  In a ride-hailing system, an optimal relocation of vacant vehicles can\nsignificantly reduce fleet idling time and balance the supply-demand\ndistribution, enhancing system efficiency and promoting driver satisfaction and\nretention. Model-free deep reinforcement learning (DRL) has been shown to\ndynamically learn the relocating policy by actively interacting with the\nintrinsic dynamics in large-scale ride-hailing systems. However, the issues of\nsparse reward signals and unbalanced demand and supply distribution place\ncritical barriers in developing effective DRL models. Conventional exploration\nstrategy (e.g., the $\\epsilon$-greedy) may barely work under such an\nenvironment because of dithering in low-demand regions distant from\nhigh-revenue regions. This study proposes the deep relocating option policy\n(DROP) that supervises vehicle agents to escape from oversupply areas and\neffectively relocate to potentially underserved areas. We propose to learn the\nLaplacian embedding of a time-expanded relocation graph, as an approximation\nrepresentation of the system relocation policy. The embedding generates\ntask-agnostic signals, which in combination with task-dependent signals,\nconstitute the pseudo-reward function for generating DROPs. We present a\nhierarchical learning framework that trains a high-level relocation policy and\na set of low-level DROPs. The effectiveness of our approach is demonstrated\nusing a custom-built high-fidelity simulator with real-world trip record data.\nWe report that DROP significantly improves baseline models with 15.7% more\nhourly revenue and can effectively resolve the dithering issue in low-demand\nareas.\n",
        "published": "2021",
        "authors": [
            "Xinwu Qian",
            "Shuocheng Guo",
            "Vaneet Aggarwal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.06668v6",
        "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to\n  Multiagent Domain",
        "abstract": "  Deep Reinforcement Learning (DRL) and Deep Multi-agent Reinforcement Learning\n(MARL) have achieved significant successes across a wide range of domains,\nincluding game AI, autonomous vehicles, robotics, and so on. However, DRL and\ndeep MARL agents are widely known to be sample inefficient that millions of\ninteractions are usually needed even for relatively simple problem settings,\nthus preventing the wide application and deployment in real-industry scenarios.\nOne bottleneck challenge behind is the well-known exploration problem, i.e.,\nhow efficiently exploring the environment and collecting informative\nexperiences that could benefit policy learning towards the optimal ones. This\nproblem becomes more challenging in complex environments with sparse rewards,\nnoisy distractions, long horizons, and non-stationary co-learners. In this\npaper, we conduct a comprehensive survey on existing exploration methods for\nboth single-agent and multi-agent RL. We start the survey by identifying\nseveral key challenges to efficient exploration. Beyond the above two main\nbranches, we also include other notable exploration methods with different\nideas and techniques. In addition to algorithmic analysis, we provide a\ncomprehensive and unified empirical comparison of different exploration methods\nfor DRL on a set of commonly used benchmarks. According to our algorithmic and\nempirical investigation, we finally summarize the open problems of exploration\nin DRL and deep MARL and point out a few future directions.\n",
        "published": "2021",
        "authors": [
            "Jianye Hao",
            "Tianpei Yang",
            "Hongyao Tang",
            "Chenjia Bai",
            "Jinyi Liu",
            "Zhaopeng Meng",
            "Peng Liu",
            "Zhen Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.06689v1",
        "title": "Reactive and Safe Road User Simulations using Neural Barrier\n  Certificates",
        "abstract": "  Reactive and safe agent modelings are important for nowadays traffic\nsimulator designs and safe planning applications. In this work, we proposed a\nreactive agent model which can ensure safety without comprising the original\npurposes, by learning only high-level decisions from expert data and a\nlow-level decentralized controller guided by the jointly learned decentralized\nbarrier certificates. Empirical results show that our learned road user\nsimulation models can achieve a significant improvement in safety comparing to\nstate-of-the-art imitation learning and pure control-based methods, while being\nsimilar to human agents by having smaller errors to the expert data. Moreover,\nour learned reactive agents are shown to generalize better to unseen traffic\nconditions, and react better to other road users and therefore can help\nunderstand challenging planning problems pragmatically.\n",
        "published": "2021",
        "authors": [
            "Yue Meng",
            "Zengyi Qin",
            "Chuchu Fan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.08294v1",
        "title": "A Logic-based Multi-agent System for Ethical Monitoring and Evaluation\n  of Dialogues",
        "abstract": "  Dialogue Systems are tools designed for various practical purposes concerning\nhuman-machine interaction. These systems should be built on ethical foundations\nbecause their behavior may heavily influence a user (think especially about\nchildren). The primary objective of this paper is to present the architecture\nand prototype implementation of a Multi Agent System (MAS) designed for ethical\nmonitoring and evaluation of a dialogue system. A prototype application, for\nmonitoring and evaluation of chatting agents' (human/artificial) ethical\nbehavior in an online customer service chat point w.r.t their\ninstitution/company's codes of ethics and conduct, is developed and presented.\nFuture work and open issues with this research are discussed.\n",
        "published": "2021",
        "authors": [
            "Abeer Dyoub",
            "Stefania Costantini",
            "Ivan Letteri",
            "Francesca A. Lisi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.00304v2",
        "title": "Divergence-Regularized Multi-Agent Actor-Critic",
        "abstract": "  Entropy regularization is a popular method in reinforcement learning (RL).\nAlthough it has many advantages, it alters the RL objective of the original\nMarkov Decision Process (MDP). Though divergence regularization has been\nproposed to settle this problem, it cannot be trivially applied to cooperative\nmulti-agent reinforcement learning (MARL). In this paper, we investigate\ndivergence regularization in cooperative MARL and propose a novel off-policy\ncooperative MARL framework, divergence-regularized multi-agent actor-critic\n(DMAC). Theoretically, we derive the update rule of DMAC which is naturally\noff-policy and guarantees monotonic policy improvement and convergence in both\nthe original MDP and divergence-regularized MDP. We also give a bound of the\ndiscrepancy between the converged policy and optimal policy in the original\nMDP. DMAC is a flexible framework and can be combined with many existing MARL\nalgorithms. Empirically, we evaluate DMAC in a didactic stochastic game and\nStarCraft Multi-Agent Challenge and show that DMAC substantially improves the\nperformance of existing MARL algorithms.\n",
        "published": "2021",
        "authors": [
            "Kefan Su",
            "Zongqing Lu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.01307v1",
        "title": "Collective eXplainable AI: Explaining Cooperative Strategies and Agent\n  Contribution in Multiagent Reinforcement Learning with Shapley Values",
        "abstract": "  While Explainable Artificial Intelligence (XAI) is increasingly expanding\nmore areas of application, little has been applied to make deep Reinforcement\nLearning (RL) more comprehensible. As RL becomes ubiquitous and used in\ncritical and general public applications, it is essential to develop methods\nthat make it better understood and more interpretable. This study proposes a\nnovel approach to explain cooperative strategies in multiagent RL using Shapley\nvalues, a game theory concept used in XAI that successfully explains the\nrationale behind decisions taken by Machine Learning algorithms. Through\ntesting common assumptions of this technique in two cooperation-centered\nsocially challenging multi-agent environments environments, this article argues\nthat Shapley values are a pertinent way to evaluate the contribution of players\nin a cooperative multi-agent RL context. To palliate the high overhead of this\nmethod, Shapley values are approximated using Monte Carlo sampling.\nExperimental results on Multiagent Particle and Sequential Social Dilemmas show\nthat Shapley values succeed at estimating the contribution of each agent. These\nresults could have implications that go beyond games in economics, (e.g., for\nnon-discriminatory decision making, ethical and responsible AI-derived\ndecisions or policy making under fairness constraints). They also expose how\nShapley values only give general explanations about a model and cannot explain\na single run, episode nor justify precise actions taken by agents. Future work\nshould focus on addressing these critical aspects.\n",
        "published": "2021",
        "authors": [
            "Alexandre Heuillet",
            "Fabien Couthouis",
            "Natalia D\u00edaz-Rodr\u00edguez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.02924v1",
        "title": "No-Press Diplomacy from Scratch",
        "abstract": "  Prior AI successes in complex games have largely focused on settings with at\nmost hundreds of actions at each decision point. In contrast, Diplomacy is a\ngame with more than 10^20 possible actions per turn. Previous attempts to\naddress games with large branching factors, such as Diplomacy, StarCraft, and\nDota, used human data to bootstrap the policy or used handcrafted reward\nshaping. In this paper, we describe an algorithm for action exploration and\nequilibrium approximation in games with combinatorial action spaces. This\nalgorithm simultaneously performs value iteration while learning a policy\nproposal network. A double oracle step is used to explore additional actions to\nadd to the policy proposals. At each state, the target state value and policy\nfor the model training are computed via an equilibrium search procedure. Using\nthis algorithm, we train an agent, DORA, completely from scratch for a popular\ntwo-player variant of Diplomacy and show that it achieves superhuman\nperformance. Additionally, we extend our methods to full-scale no-press\nDiplomacy and for the first time train an agent from scratch with no human\ndata. We present evidence that this agent plays a strategy that is incompatible\nwith human-data bootstrapped agents. This presents the first strong evidence of\nmultiple equilibria in Diplomacy and suggests that self play alone may be\ninsufficient for achieving superhuman performance in Diplomacy.\n",
        "published": "2021",
        "authors": [
            "Anton Bakhtin",
            "David Wu",
            "Adam Lerer",
            "Noam Brown"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.03604v3",
        "title": "Online Markov Decision Processes with Non-oblivious Strategic Adversary",
        "abstract": "  We study a novel setting in Online Markov Decision Processes (OMDPs) where\nthe loss function is chosen by a non-oblivious strategic adversary who follows\na no-external regret algorithm. In this setting, we first demonstrate that\nMDP-Expert, an existing algorithm that works well with oblivious adversaries\ncan still apply and achieve a policy regret bound of $\\mathcal{O}(\\sqrt{T\n\\log(L)}+\\tau^2\\sqrt{ T \\log(|A|)})$ where $L$ is the size of adversary's pure\nstrategy set and $|A|$ denotes the size of agent's action space. Considering\nreal-world games where the support size of a NE is small, we further propose a\nnew algorithm: MDP-Online Oracle Expert (MDP-OOE), that achieves a policy\nregret bound of $\\mathcal{O}(\\sqrt{T\\log(L)}+\\tau^2\\sqrt{ T k \\log(k)})$ where\n$k$ depends only on the support size of the NE. MDP-OOE leverages the key\nbenefit of Double Oracle in game theory and thus can solve games with\nprohibitively large action space. Finally, to better understand the learning\ndynamics of no-regret methods, under the same setting of no-external regret\nadversary in OMDPs, we introduce an algorithm that achieves last-round\nconvergence result to a NE. To our best knowledge, this is first work leading\nto the last iteration result in OMDPs.\n",
        "published": "2021",
        "authors": [
            "Le Cong Dinh",
            "David Henry Mguni",
            "Long Tran-Thanh",
            "Jun Wang",
            "Yaodong Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.04321v1",
        "title": "Computing an Optimal Pitching Strategy in a Baseball At-Bat",
        "abstract": "  The field of quantitative analytics has transformed the world of sports over\nthe last decade. To date, these analytic approaches are statistical at their\ncore, characterizing what is and what was, while using this information to\ndrive decisions about what to do in the future. However, as we often view team\nsports, such as soccer, hockey, and baseball, as pairwise win-lose encounters,\nit seems natural to model these as zero-sum games. We propose such a model for\none important class of sports encounters: a baseball at-bat, which is a matchup\nbetween a pitcher and a batter. Specifically, we propose a novel model of this\nencounter as a zero-sum stochastic game, in which the goal of the batter is to\nget on base, an outcome the pitcher aims to prevent. The value of this game is\nthe on-base percentage (i.e., the probability that the batter gets on base). In\nprinciple, this stochastic game can be solved using classical approaches. The\nmain technical challenges lie in predicting the distribution of pitch locations\nas a function of pitcher intention, predicting the distribution of outcomes if\nthe batter decides to swing at a pitch, and characterizing the level of\npatience of a particular batter. We address these challenges by proposing novel\npitcher and batter representations as well as a novel deep neural network\narchitecture for outcome prediction. Our experiments using Kaggle data from the\n2015 to 2018 Major League Baseball seasons demonstrate the efficacy of the\nproposed approach.\n",
        "published": "2021",
        "authors": [
            "Connor Douglas",
            "Everett Witt",
            "Mia Bendy",
            "Yevgeniy Vorobeychik"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.05682v3",
        "title": "Provably Efficient Reinforcement Learning in Decentralized General-Sum\n  Markov Games",
        "abstract": "  This paper addresses the problem of learning an equilibrium efficiently in\ngeneral-sum Markov games through decentralized multi-agent reinforcement\nlearning. Given the fundamental difficulty of calculating a Nash equilibrium\n(NE), we instead aim at finding a coarse correlated equilibrium (CCE), a\nsolution concept that generalizes NE by allowing possible correlations among\nthe agents' strategies. We propose an algorithm in which each agent\nindependently runs optimistic V-learning (a variant of Q-learning) to\nefficiently explore the unknown environment, while using a stabilized online\nmirror descent (OMD) subroutine for policy updates. We show that the agents can\nfind an $\\epsilon$-approximate CCE in at most $\\widetilde{O}( H^6S A\n/\\epsilon^2)$ episodes, where $S$ is the number of states, $A$ is the size of\nthe largest individual action space, and $H$ is the length of an episode. This\nappears to be the first sample complexity result for learning in generic\ngeneral-sum Markov games. Our results rely on a novel investigation of an\nanytime high-probability regret bound for OMD with a dynamic learning rate and\nweighted regret, which would be of independent interest. One key feature of our\nalgorithm is that it is fully \\emph{decentralized}, in the sense that each\nagent has access to only its local information, and is completely oblivious to\nthe presence of others. This way, our algorithm can readily scale up to an\narbitrary number of agents, without suffering from the exponential dependence\non the number of agents.\n",
        "published": "2021",
        "authors": [
            "Weichao Mao",
            "Tamer Ba\u015far"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.05707v2",
        "title": "On Improving Model-Free Algorithms for Decentralized Multi-Agent\n  Reinforcement Learning",
        "abstract": "  Multi-agent reinforcement learning (MARL) algorithms often suffer from an\nexponential sample complexity dependence on the number of agents, a phenomenon\nknown as \\emph{the curse of multiagents}. In this paper, we address this\nchallenge by investigating sample-efficient model-free algorithms in\n\\emph{decentralized} MARL, and aim to improve existing algorithms along this\nline. For learning (coarse) correlated equilibria in general-sum Markov games,\nwe propose \\emph{stage-based} V-learning algorithms that significantly simplify\nthe algorithmic design and analysis of recent works, and circumvent a rather\ncomplicated no-\\emph{weighted}-regret bandit subroutine. For learning Nash\nequilibria in Markov potential games, we propose an independent policy gradient\nalgorithm with a decentralized momentum-based variance reduction technique. All\nour algorithms are decentralized in that each agent can make decisions based on\nonly its local information. Neither communication nor centralized coordination\nis required during learning, leading to a natural generalization to a large\nnumber of agents. We also provide numerical simulations to corroborate our\ntheoretical findings.\n",
        "published": "2021",
        "authors": [
            "Weichao Mao",
            "Lin F. Yang",
            "Kaiqing Zhang",
            "Tamer Ba\u015far"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.06829v2",
        "title": "Towards a fully RL-based Market Simulator",
        "abstract": "  We present a new financial framework where two families of RL-based agents\nrepresenting the Liquidity Providers and Liquidity Takers learn simultaneously\nto satisfy their objective. Thanks to a parametrized reward formulation and the\nuse of Deep RL, each group learns a shared policy able to generalize and\ninterpolate over a wide range of behaviors. This is a step towards a fully\nRL-based market simulator replicating complex market conditions particularly\nsuited to study the dynamics of the financial market under various scenarios.\n",
        "published": "2021",
        "authors": [
            "Leo Ardon",
            "Nelson Vadori",
            "Thomas Spooner",
            "Mengda Xu",
            "Jared Vann",
            "Sumitra Ganesh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.07246v3",
        "title": "HAVEN: Hierarchical Cooperative Multi-Agent Reinforcement Learning with\n  Dual Coordination Mechanism",
        "abstract": "  Recently, some challenging tasks in multi-agent systems have been solved by\nsome hierarchical reinforcement learning methods. Inspired by the intra-level\nand inter-level coordination in the human nervous system, we propose a novel\nvalue decomposition framework HAVEN based on hierarchical reinforcement\nlearning for fully cooperative multi-agent problems. To address the instability\narising from the concurrent optimization of policies between various levels and\nagents, we introduce the dual coordination mechanism of inter-level and\ninter-agent strategies by designing reward functions in a two-level hierarchy.\nHAVEN does not require domain knowledge and pre-training, and can be applied to\nany value decomposition variant. Our method achieves desirable results on\ndifferent decentralized partially observable Markov decision process domains\nand outperforms other popular multi-agent hierarchical reinforcement learning\nalgorithms.\n",
        "published": "2021",
        "authors": [
            "Zhiwei Xu",
            "Yunpeng Bai",
            "Bin Zhang",
            "Dapeng Li",
            "Guoliang Fan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.07594v1",
        "title": "The Neural MMO Platform for Massively Multiagent Research",
        "abstract": "  Neural MMO is a computationally accessible research platform that combines\nlarge agent populations, long time horizons, open-ended tasks, and modular game\nsystems. Existing environments feature subsets of these properties, but Neural\nMMO is the first to combine them all. We present Neural MMO as free and open\nsource software with active support, ongoing development, documentation, and\nadditional training, logging, and visualization tools to help users adapt to\nthis new setting. Initial baselines on the platform demonstrate that agents\ntrained in large populations explore more and learn a progression of skills. We\nraise other more difficult problems such as many-team cooperation as open\nresearch questions which Neural MMO is well-suited to answer. Finally, we\ndiscuss current limitations of the platform, potential mitigations, and plans\nfor continued development.\n",
        "published": "2021",
        "authors": [
            "Joseph Suarez",
            "Yilun Du",
            "Clare Zhu",
            "Igor Mordatch",
            "Phillip Isola"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.08642v3",
        "title": "Local Advantage Actor-Critic for Robust Multi-Agent Deep Reinforcement\n  Learning",
        "abstract": "  Policy gradient methods have become popular in multi-agent reinforcement\nlearning, but they suffer from high variance due to the presence of\nenvironmental stochasticity and exploring agents (i.e., non-stationarity),\nwhich is potentially worsened by the difficulty in credit assignment. As a\nresult, there is a need for a method that is not only capable of efficiently\nsolving the above two problems but also robust enough to solve a variety of\ntasks. To this end, we propose a new multi-agent policy gradient method, called\nRobust Local Advantage (ROLA) Actor-Critic. ROLA allows each agent to learn an\nindividual action-value function as a local critic as well as ameliorating\nenvironment non-stationarity via a novel centralized training approach based on\na centralized critic. By using this local critic, each agent calculates a\nbaseline to reduce variance on its policy gradient estimation, which results in\nan expected advantage action-value over other agents' choices that implicitly\nimproves credit assignment. We evaluate ROLA across diverse benchmarks and show\nits robustness and effectiveness over a number of state-of-the-art multi-agent\npolicy gradient algorithms.\n",
        "published": "2021",
        "authors": [
            "Yuchen Xiao",
            "Xueguang Lyu",
            "Christopher Amato"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.09817v1",
        "title": "State-based Episodic Memory for Multi-Agent Reinforcement Learning",
        "abstract": "  Multi-agent reinforcement learning (MARL) algorithms have made promising\nprogress in recent years by leveraging the centralized training and\ndecentralized execution (CTDE) paradigm. However, existing MARL algorithms\nstill suffer from the sample inefficiency problem. In this paper, we propose a\nsimple yet effective approach, called state-based episodic memory (SEM), to\nimprove sample efficiency in MARL. SEM adopts episodic memory (EM) to supervise\nthe centralized training procedure of CTDE in MARL. To the best of our\nknowledge, SEM is the first work to introduce EM into MARL. We can\ntheoretically prove that, when using for MARL, SEM has lower space complexity\nand time complexity than state and action based EM (SAEM), which is originally\nproposed for single-agent reinforcement learning. Experimental results on\nStarCraft multi-agent challenge (SMAC) show that introducing episodic memory\ninto MARL can improve sample efficiency and SEM can reduce storage cost and\ntime cost compared with SAEM.\n",
        "published": "2021",
        "authors": [
            "Xiao Ma",
            "Wu-Jun Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.11404v1",
        "title": "Statistical discrimination in learning agents",
        "abstract": "  Undesired bias afflicts both human and algorithmic decision making, and may\nbe especially prevalent when information processing trade-offs incentivize the\nuse of heuristics. One primary example is \\textit{statistical discrimination}\n-- selecting social partners based not on their underlying attributes, but on\nreadily perceptible characteristics that covary with their suitability for the\ntask at hand. We present a theoretical model to examine how information\nprocessing influences statistical discrimination and test its predictions using\nmulti-agent reinforcement learning with various agent architectures in a\npartner choice-based social dilemma. As predicted, statistical discrimination\nemerges in agent policies as a function of both the bias in the training\npopulation and of agent architecture. All agents showed substantial statistical\ndiscrimination, defaulting to using the readily available correlates instead of\nthe outcome relevant features. We show that less discrimination emerges with\nagents that use recurrent neural networks, and when their training environment\nhas less bias. However, all agent algorithms we tried still exhibited\nsubstantial bias after learning in biased training populations.\n",
        "published": "2021",
        "authors": [
            "Edgar A. Du\u00e9\u00f1ez-Guzm\u00e1n",
            "Kevin R. McKee",
            "Yiran Mao",
            "Ben Coppin",
            "Silvia Chiappa",
            "Alexander Sasha Vezhnevets",
            "Michiel A. Bakker",
            "Yoram Bachrach",
            "Suzanne Sadedin",
            "William Isaac",
            "Karl Tuyls",
            "Joel Z. Leibo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.13484v3",
        "title": "Applications of Multi-Agent Reinforcement Learning in Future Internet: A\n  Comprehensive Survey",
        "abstract": "  Future Internet involves several emerging technologies such as 5G and beyond\n5G networks, vehicular networks, unmanned aerial vehicle (UAV) networks, and\nInternet of Things (IoTs). Moreover, future Internet becomes heterogeneous and\ndecentralized with a large number of involved network entities. Each entity may\nneed to make its local decision to improve the network performance under\ndynamic and uncertain network environments. Standard learning algorithms such\nas single-agent Reinforcement Learning (RL) or Deep Reinforcement Learning\n(DRL) have been recently used to enable each network entity as an agent to\nlearn an optimal decision-making policy adaptively through interacting with the\nunknown environments. However, such an algorithm fails to model the\ncooperations or competitions among network entities, and simply treats other\nentities as a part of the environment that may result in the non-stationarity\nissue. Multi-agent Reinforcement Learning (MARL) allows each network entity to\nlearn its optimal policy by observing not only the environments, but also other\nentities' policies. As a result, MARL can significantly improve the learning\nefficiency of the network entities, and it has been recently used to solve\nvarious issues in the emerging networks. In this paper, we thus review the\napplications of MARL in the emerging networks. In particular, we provide a\ntutorial of MARL and a comprehensive survey of applications of MARL in next\ngeneration Internet. In particular, we first introduce single-agent RL and\nMARL. Then, we review a number of applications of MARL to solve emerging issues\nin future Internet. The issues consist of network access, transmit power\ncontrol, computation offloading, content caching, packet routing, trajectory\ndesign for UAV-aided networks, and network security issues.\n",
        "published": "2021",
        "authors": [
            "Tianxu Li",
            "Kun Zhu",
            "Nguyen Cong Luong",
            "Dusit Niyato",
            "Qihui Wu",
            "Yang Zhang",
            "Bing Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.01100v1",
        "title": "Investigation of Independent Reinforcement Learning Algorithms in\n  Multi-Agent Environments",
        "abstract": "  Independent reinforcement learning algorithms have no theoretical guarantees\nfor finding the best policy in multi-agent settings. However, in practice,\nprior works have reported good performance with independent algorithms in some\ndomains and bad performance in others. Moreover, a comprehensive study of the\nstrengths and weaknesses of independent algorithms is lacking in the\nliterature. In this paper, we carry out an empirical comparison of the\nperformance of independent algorithms on four PettingZoo environments that span\nthe three main categories of multi-agent environments, i.e., cooperative,\ncompetitive, and mixed. We show that in fully-observable environments,\nindependent algorithms can perform on par with multi-agent algorithms in\ncooperative and competitive settings. For the mixed environments, we show that\nagents trained via independent algorithms learn to perform well individually,\nbut fail to learn to cooperate with allies and compete with enemies. We also\nshow that adding recurrence improves the learning of independent algorithms in\ncooperative partially observable environments.\n",
        "published": "2021",
        "authors": [
            "Ken Ming Lee",
            "Sriram Ganapathi Subramanian",
            "Mark Crowley"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.03431v1",
        "title": "Learning to Cooperate with Unseen Agent via Meta-Reinforcement Learning",
        "abstract": "  Ad hoc teamwork problem describes situations where an agent has to cooperate\nwith previously unseen agents to achieve a common goal. For an agent to be\nsuccessful in these scenarios, it has to have a suitable cooperative skill. One\ncould implement cooperative skills into an agent by using domain knowledge to\ndesign the agent's behavior. However, in complex domains, domain knowledge\nmight not be available. Therefore, it is worthwhile to explore how to directly\nlearn cooperative skills from data. In this work, we apply meta-reinforcement\nlearning (meta-RL) formulation in the context of the ad hoc teamwork problem.\nOur empirical results show that such a method could produce robust cooperative\nagents in two cooperative environments with different cooperative\ncircumstances: social compliance and language interpretation. (This is a full\npaper of the extended abstract version.)\n",
        "published": "2021",
        "authors": [
            "Rujikorn Charakorn",
            "Poramate Manoonpong",
            "Nat Dilokthanakul"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.04086v1",
        "title": "Meta Cross-Modal Hashing on Long-Tailed Data",
        "abstract": "  Due to the advantage of reducing storage while speeding up query time on big\nheterogeneous data, cross-modal hashing has been extensively studied for\napproximate nearest neighbor search of multi-modal data. Most hashing methods\nassume that training data is class-balanced.However, in practice, real world\ndata often have a long-tailed distribution. In this paper, we introduce a\nmeta-learning based cross-modal hashing method (MetaCMH) to handle long-tailed\ndata. Due to the lack of training samples in the tail classes, MetaCMH first\nlearns direct features from data in different modalities, and then introduces\nan associative memory module to learn the memory features of samples of the\ntail classes. It then combines the direct and memory features to obtain meta\nfeatures for each sample. For samples of the head classes of the long tail\ndistribution, the weight of the direct features is larger, because there are\nenough training data to learn them well; while for rare classes, the weight of\nthe memory features is larger. Finally, MetaCMH uses a likelihood loss function\nto preserve the similarity in different modalities and learns hash functions in\nan end-to-end fashion. Experiments on long-tailed datasets show that MetaCMH\nperforms significantly better than state-of-the-art methods, especially on the\ntail classes.\n",
        "published": "2021",
        "authors": [
            "Runmin Wang",
            "Guoxian Yu",
            "Carlotta Domeniconi",
            "Xiangliang Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.09189v2",
        "title": "ToM2C: Target-oriented Multi-agent Communication and Cooperation with\n  Theory of Mind",
        "abstract": "  Being able to predict the mental states of others is a key factor to\neffective social interaction. It is also crucial for distributed multi-agent\nsystems, where agents are required to communicate and cooperate. In this paper,\nwe introduce such an important social-cognitive skill, i.e. Theory of Mind\n(ToM), to build socially intelligent agents who are able to communicate and\ncooperate effectively to accomplish challenging tasks. With ToM, each agent is\ncapable of inferring the mental states and intentions of others according to\nits (local) observation. Based on the inferred states, the agents decide \"when\"\nand with \"whom\" to share their intentions. With the information observed,\ninferred, and received, the agents decide their sub-goals and reach a consensus\namong the team. In the end, the low-level executors independently take\nprimitive actions to accomplish the sub-goals. We demonstrate the idea in two\ntypical target-oriented multi-agent tasks: cooperative navigation and\nmulti-sensor target coverage. The experiments show that the proposed model not\nonly outperforms the state-of-the-art methods on reward and communication\nefficiency, but also shows good generalization across different scales of the\nenvironment.\n",
        "published": "2021",
        "authors": [
            "Yuanfei Wang",
            "Fangwei Zhong",
            "Jing Xu",
            "Yizhou Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.11229v2",
        "title": "Off-Policy Correction For Multi-Agent Reinforcement Learning",
        "abstract": "  Multi-agent reinforcement learning (MARL) provides a framework for problems\ninvolving multiple interacting agents. Despite apparent similarity to the\nsingle-agent case, multi-agent problems are often harder to train and analyze\ntheoretically. In this work, we propose MA-Trace, a new on-policy actor-critic\nalgorithm, which extends V-Trace to the MARL setting. The key advantage of our\nalgorithm is its high scalability in a multi-worker setting. To this end,\nMA-Trace utilizes importance sampling as an off-policy correction method, which\nallows distributing the computations with no impact on the quality of training.\nFurthermore, our algorithm is theoretically grounded - we prove a fixed-point\ntheorem that guarantees convergence. We evaluate the algorithm extensively on\nthe StarCraft Multi-Agent Challenge, a standard benchmark for multi-agent\nalgorithms. MA-Trace achieves high performance on all its tasks and exceeds\nstate-of-the-art results on some of them.\n",
        "published": "2021",
        "authors": [
            "Micha\u0142 Zawalski",
            "B\u0142a\u017cej Osi\u0144ski",
            "Henryk Michalewski",
            "Piotr Mi\u0142o\u015b"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.13872v1",
        "title": "Normative Disagreement as a Challenge for Cooperative AI",
        "abstract": "  Cooperation in settings where agents have both common and conflicting\ninterests (mixed-motive environments) has recently received considerable\nattention in multi-agent learning. However, the mixed-motive environments\ntypically studied have a single cooperative outcome on which all agents can\nagree. Many real-world multi-agent environments are instead bargaining problems\n(BPs): they have several Pareto-optimal payoff profiles over which agents have\nconflicting preferences. We argue that typical cooperation-inducing learning\nalgorithms fail to cooperate in BPs when there is room for normative\ndisagreement resulting in the existence of multiple competing cooperative\nequilibria, and illustrate this problem empirically. To remedy the issue, we\nintroduce the notion of norm-adaptive policies. Norm-adaptive policies are\ncapable of behaving according to different norms in different circumstances,\ncreating opportunities for resolving normative disagreement. We develop a class\nof norm-adaptive policies and show in experiments that these significantly\nincrease cooperation. However, norm-adaptiveness cannot address residual\nbargaining failure arising from a fundamental tradeoff between exploitability\nand cooperative robustness.\n",
        "published": "2021",
        "authors": [
            "Julian Stastny",
            "Maxime Rich\u00e9",
            "Alexander Lyzhov",
            "Johannes Treutlein",
            "Allan Dafoe",
            "Jesse Clifton"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.14177v1",
        "title": "Evaluating Generalization and Transfer Capacity of Multi-Agent\n  Reinforcement Learning Across Variable Number of Agents",
        "abstract": "  Multi-agent Reinforcement Learning (MARL) problems often require cooperation\namong agents in order to solve a task. Centralization and decentralization are\ntwo approaches used for cooperation in MARL. While fully decentralized methods\nare prone to converge to suboptimal solutions due to partial observability and\nnonstationarity, the methods involving centralization suffer from scalability\nlimitations and lazy agent problem. Centralized training decentralized\nexecution paradigm brings out the best of these two approaches; however,\ncentralized training still has an upper limit of scalability not only for\nacquired coordination performance but also for model size and training time. In\nthis work, we adopt the centralized training with decentralized execution\nparadigm and investigate the generalization and transfer capacity of the\ntrained models across variable number of agents. This capacity is assessed by\ntraining variable number of agents in a specific MARL problem and then\nperforming greedy evaluations with variable number of agents for each training\nconfiguration. Thus, we analyze the evaluation performance for each combination\nof agent count for training versus evaluation. We perform experimental\nevaluations on predator prey and traffic junction environments and demonstrate\nthat it is possible to obtain similar or higher evaluation performance by\ntraining with less agents. We conclude that optimal number of agents to perform\ntraining may differ from the target number of agents and argue that transfer\nacross large number of agents can be a more efficient solution to scaling up\nthan directly increasing number of agents during training.\n",
        "published": "2021",
        "authors": [
            "Bengisu Guresti",
            "Nazim Kemal Ure"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.14310v1",
        "title": "How Can Creativity Occur in Multi-Agent Systems?",
        "abstract": "  Complex systems show how surprising and beautiful phenomena can emerge from\nstructures or agents following simple rules. With the recent success of deep\nreinforcement learning (RL), a natural path forward would be to use the\ncapabilities of multiple deep RL agents to produce emergent behavior of greater\nbenefit and sophistication. In general, this has proved to be an unreliable\nstrategy without significant computation due to the difficulties inherent in\nmulti-agent RL training. In this paper, we propose some criteria for creativity\nin multi-agent RL. We hope this proposal will give artists applying multi-agent\nRL a starting point, and provide a catalyst for further investigation guided by\nphilosophical discussion.\n",
        "published": "2021",
        "authors": [
            "Ted Fujimoto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.14833v3",
        "title": "Adversarial Attacks in Cooperative AI",
        "abstract": "  Single-agent reinforcement learning algorithms in a multi-agent environment\nare inadequate for fostering cooperation. If intelligent agents are to interact\nand work together to solve complex problems, methods that counter\nnon-cooperative behavior are needed to facilitate the training of multiple\nagents. This is the goal of cooperative AI. Recent research in adversarial\nmachine learning, however, shows that models (e.g., image classifiers) can be\neasily deceived into making inferior decisions. Meanwhile, an important line of\nresearch in cooperative AI has focused on introducing algorithmic improvements\nthat accelerate learning of optimally cooperative behavior. We argue that\nprominent methods of cooperative AI are exposed to weaknesses analogous to\nthose studied in prior machine learning research. More specifically, we show\nthat three algorithms inspired by human-like social intelligence are, in\nprinciple, vulnerable to attacks that exploit weaknesses introduced by\ncooperative AI's algorithmic improvements and report experimental findings that\nillustrate how these vulnerabilities can be exploited in practice.\n",
        "published": "2021",
        "authors": [
            "Ted Fujimoto",
            "Arthur Paul Pedersen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.15611v3",
        "title": "The Power of Communication in a Distributed Multi-Agent System",
        "abstract": "  Single-Agent (SA) Reinforcement Learning systems have shown outstanding\nre-sults on non-stationary problems. However, Multi-Agent Reinforcement\nLearning(MARL) can surpass SA systems generally and when scaling. Furthermore,\nMAsystems can be super-powered by collaboration, which can happen through\nob-serving others, or a communication system used to share information\nbetweencollaborators. Here, we developed a distributed MA learning mechanism\nwiththe ability to communicate based on decentralised partially observable\nMarkovdecision processes (Dec-POMDPs) and Graph Neural Networks (GNNs).\nMinimis-ing the time and energy consumed by training Machine Learning models\nwhileimproving performance can be achieved by collaborative MA mechanisms.\nWedemonstrate this in a real-world scenario, an offshore wind farm, including a\nset ofdistributed wind turbines, where the objective is to maximise collective\nefficiency.Compared to a SA system, MA collaboration has shown significantly\nreducedtraining time and higher cumulative rewards in unseen and scaled\nscenarios.\n",
        "published": "2021",
        "authors": [
            "Philipp Dominic Siedler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.00424v1",
        "title": "Multi-Agent Transfer Learning in Reinforcement Learning-Based\n  Ride-Sharing Systems",
        "abstract": "  Reinforcement learning (RL) has been used in a range of simulated real-world\ntasks, e.g., sensor coordination, traffic light control, and on-demand mobility\nservices. However, real world deployments are rare, as RL struggles with\ndynamic nature of real world environments, requiring time for learning a task\nand adapting to changes in the environment. Transfer Learning (TL) can help\nlower these adaptation times. In particular, there is a significant potential\nof applying TL in multi-agent RL systems, where multiple agents can share\nknowledge with each other, as well as with new agents that join the system. To\nobtain the most from inter-agent transfer, transfer roles (i.e., determining\nwhich agents act as sources and which as targets), as well as relevant transfer\ncontent parameters (e.g., transfer size) should be selected dynamically in each\nparticular situation. As a first step towards fully dynamic transfers, in this\npaper we investigate the impact of TL transfer parameters with fixed source and\ntarget roles. Specifically, we label every agent-environment interaction with\nagent's epistemic confidence, and we filter the shared examples using varying\nthreshold levels and sample sizes. We investigate impact of these parameters in\ntwo scenarios, a standard predator-prey RL benchmark and a simulation of a\nride-sharing system with 200 vehicle agents and 10,000 ride-requests.\n",
        "published": "2021",
        "authors": [
            "Alberto Castagna",
            "Ivana Dusparic"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.00579v1",
        "title": "Conditional Expectation based Value Decomposition for Scalable On-Demand\n  Ride Pooling",
        "abstract": "  Owing to the benefits for customers (lower prices), drivers (higher\nrevenues), aggregation companies (higher revenues) and the environment (fewer\nvehicles), on-demand ride pooling (e.g., Uber pool, Grab Share) has become\nquite popular. The significant computational complexity of matching vehicles to\ncombinations of requests has meant that traditional ride pooling approaches are\nmyopic in that they do not consider the impact of current matches on future\nvalue for vehicles/drivers. Recently, Neural Approximate Dynamic Programming\n(NeurADP) has employed value decomposition with Approximate Dynamic Programming\n(ADP) to outperform leading approaches by considering the impact of an\nindividual agent's (vehicle) chosen actions on the future value of that agent.\nHowever, in order to ensure scalability and facilitate city-scale ride pooling,\nNeurADP completely ignores the impact of other agents actions on individual\nagent/vehicle value. As demonstrated in our experimental results, ignoring the\nimpact of other agents actions on individual value can have a significant\nimpact on the overall performance when there is increased competition among\nvehicles for demand. Our key contribution is a novel mechanism based on\ncomputing conditional expectations through joint conditional probabilities for\ncapturing dependencies on other agents actions without increasing the\ncomplexity of training or decision making. We show that our new approach,\nConditional Expectation based Value Decomposition (CEVD) outperforms NeurADP by\nup to 9.76% in terms of overall requests served, which is a significant\nimprovement on a city wide benchmark taxi dataset.\n",
        "published": "2021",
        "authors": [
            "Avinandan Bose",
            "Pradeep Varakantham"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.00940v1",
        "title": "Reward-Free Attacks in Multi-Agent Reinforcement Learning",
        "abstract": "  We investigate how effective an attacker can be when it only learns from its\nvictim's actions, without access to the victim's reward. In this work, we are\nmotivated by the scenario where the attacker wants to behave strategically when\nthe victim's motivations are unknown. We argue that one heuristic approach an\nattacker can use is to maximize the entropy of the victim's policy. The policy\nis generally not obfuscated, which implies it may be extracted simply by\npassively observing the victim. We provide such a strategy in the form of a\nreward-free exploration algorithm that maximizes the attacker's entropy during\nthe exploration phase, and then maximizes the victim's empirical entropy during\nthe planning phase. In our experiments, the victim agents are subverted through\npolicy entropy maximization, implying an attacker might not need access to the\nvictim's reward to succeed. Hence, reward-free attacks, which are based only on\nobserving behavior, show the feasibility of an attacker to act strategically\nwithout knowledge of the victim's motives even if the victim's reward\ninformation is protected.\n",
        "published": "2021",
        "authors": [
            "Ted Fujimoto",
            "Timothy Doster",
            "Adam Attarian",
            "Jill Brandenberger",
            "Nathan Hodas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.02845v3",
        "title": "Offline Pre-trained Multi-Agent Decision Transformer: One Big Sequence\n  Model Tackles All SMAC Tasks",
        "abstract": "  Offline reinforcement learning leverages previously-collected offline\ndatasets to learn optimal policies with no necessity to access the real\nenvironment. Such a paradigm is also desirable for multi-agent reinforcement\nlearning (MARL) tasks, given the increased interactions among agents and with\nthe enviroment. Yet, in MARL, the paradigm of offline pre-training with online\nfine-tuning has not been studied, nor datasets or benchmarks for offline MARL\nresearch are available. In this paper, we facilitate the research by providing\nlarge-scale datasets, and use them to examine the usage of the Decision\nTransformer in the context of MARL. We investigate the generalisation of MARL\noffline pre-training in the following three aspects: 1) between single agents\nand multiple agents, 2) from offline pretraining to the online fine-tuning, and\n3) to that of multiple downstream tasks with few-shot and zero-shot\ncapabilities. We start by introducing the first offline MARL dataset with\ndiverse quality levels based on the StarCraftII environment, and then propose\nthe novel architecture of multi-agent decision transformer (MADT) for effective\noffline learning. MADT leverages transformer's modelling ability of sequence\nmodelling and integrates it seamlessly with both offline and online MARL tasks.\nA crucial benefit of MADT is that it learns generalisable policies that can\ntransfer between different types of agents under different task scenarios. On\nStarCraft II offline dataset, MADT outperforms the state-of-the-art offline RL\nbaselines. When applied to online tasks, the pre-trained MADT significantly\nimproves sample efficiency, and enjoys strong performance both few-short and\nzero-shot cases. To our best knowledge, this is the first work that studies and\ndemonstrates the effectiveness of offline pre-trained models in terms of sample\nefficiency and generalisability enhancements in MARL.\n",
        "published": "2021",
        "authors": [
            "Linghui Meng",
            "Muning Wen",
            "Yaodong Yang",
            "Chenyang Le",
            "Xiyun Li",
            "Weinan Zhang",
            "Ying Wen",
            "Haifeng Zhang",
            "Jun Wang",
            "Bo Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.03547v4",
        "title": "Self-Organized Polynomial-Time Coordination Graphs",
        "abstract": "  Coordination graph is a promising approach to model agent collaboration in\nmulti-agent reinforcement learning. It conducts a graph-based value\nfactorization and induces explicit coordination among agents to complete\ncomplicated tasks. However, one critical challenge in this paradigm is the\ncomplexity of greedy action selection with respect to the factorized values. It\nrefers to the decentralized constraint optimization problem (DCOP), which and\nwhose constant-ratio approximation are NP-hard problems. To bypass this\nsystematic hardness, this paper proposes a novel method, named Self-Organized\nPolynomial-time Coordination Graphs (SOP-CG), which uses structured graph\nclasses to guarantee the accuracy and the computational efficiency of\ncollaborated action selection. SOP-CG employs dynamic graph topology to ensure\nsufficient value function expressiveness. The graph selection is unified into\nan end-to-end learning paradigm. In experiments, we show that our approach\nlearns succinct and well-adapted graph topologies, induces effective\ncoordination, and improves performance across a variety of cooperative\nmulti-agent tasks.\n",
        "published": "2021",
        "authors": [
            "Qianlan Yang",
            "Weijun Dong",
            "Zhizhou Ren",
            "Jianhao Wang",
            "Tonghan Wang",
            "Chongjie Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.06771v2",
        "title": "Cooperative Multi-Agent Reinforcement Learning with Hypergraph\n  Convolution",
        "abstract": "  Recent years have witnessed the great success of multi-agent systems (MAS).\nValue decomposition, which decomposes joint action values into individual\naction values, has been an important work in MAS. However, many value\ndecomposition methods ignore the coordination among different agents, leading\nto the notorious \"lazy agents\" problem. To enhance the coordination in MAS,\nthis paper proposes HyperGraph CoNvolution MIX (HGCN-MIX), a method that\nincorporates hypergraph convolution with value decomposition. HGCN-MIX models\nagents as well as their relationships as a hypergraph, where agents are nodes\nand hyperedges among nodes indicate that the corresponding agents can\ncoordinate to achieve larger rewards. Then, it trains a hypergraph that can\ncapture the collaborative relationships among agents. Leveraging the learned\nhypergraph to consider how other agents' observations and actions affect their\ndecisions, the agents in a MAS can better coordinate. We evaluate HGCN-MIX in\nthe StarCraft II multi-agent challenge benchmark. The experimental results\ndemonstrate that HGCN-MIX can train joint policies that outperform or achieve a\nsimilar level of performance as the current state-of-the-art techniques. We\nalso observe that HGCN-MIX has an even more significant improvement of\nperformance in the scenarios with a large amount of agents. Besides, we conduct\nadditional analysis to emphasize that when the hypergraph learns more\nrelationships, HGCN-MIX can train stronger joint policies.\n",
        "published": "2021",
        "authors": [
            "Yunpeng Bai",
            "Chen Gong",
            "Bin Zhang",
            "Guoliang Fan",
            "Xinwen Hou",
            "Yu Liu"
        ]
    }
]