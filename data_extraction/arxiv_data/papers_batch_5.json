[
    {
        "id": "http://arxiv.org/abs/2012.04344v1",
        "title": "An Empirical Study of Explainable AI Techniques on Deep Learning Models\n  For Time Series Tasks",
        "abstract": "  Decision explanations of machine learning black-box models are often\ngenerated by applying Explainable AI (XAI) techniques. However, many proposed\nXAI methods produce unverified outputs. Evaluation and verification are usually\nachieved with a visual interpretation by humans on individual images or text.\nIn this preregistration, we propose an empirical study and benchmark framework\nto apply attribution methods for neural networks developed for images and text\ndata on time series. We present a methodology to automatically evaluate and\nrank attribution techniques on time series using perturbation methods to\nidentify reliable approaches.\n",
        "published": "2020",
        "authors": [
            "Udo Schlegel",
            "Daniela Oelke",
            "Daniel A. Keim",
            "Mennatallah El-Assady"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.14624v2",
        "title": "The Logic of Graph Neural Networks",
        "abstract": "  Graph neural networks (GNNs) are deep learning architectures for machine\nlearning problems on graphs. It has recently been shown that the expressiveness\nof GNNs can be characterised precisely by the combinatorial Weisfeiler-Leman\nalgorithms and by finite variable counting logics. The correspondence has even\nled to new, higher-order GNNs corresponding to the WL algorithm in higher\ndimensions.\n  The purpose of this paper is to explain these descriptive characterisations\nof GNNs.\n",
        "published": "2021",
        "authors": [
            "Martin Grohe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.01976v1",
        "title": "A new method for binary classification of proteins with Machine Learning",
        "abstract": "  In this work we set out to find a method to classify protein structures using\na Deep Learning methodology. Our Artificial Intelligence has been trained to\nrecognize complex biomolecule structures extrapolated from the Protein Data\nBank (PDB) database and reprocessed as images; for this purpose various tests\nhave been conducted with pre-trained Convolutional Neural Networks, such as\nInceptionResNetV2 or InceptionV3, in order to extract significant features from\nthese images and correctly classify the molecule. A comparative analysis of the\nperformances of the various networks will therefore be produced.\n",
        "published": "2021",
        "authors": [
            "Damiano Perri",
            "Marco Simonetti",
            "Andrea Lombardi",
            "Noelia Faginas-Lago",
            "Osvaldo Gervasi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.00101v2",
        "title": "ApacheJIT: A Large Dataset for Just-In-Time Defect Prediction",
        "abstract": "  In this paper, we present ApacheJIT, a large dataset for Just-In-Time defect\nprediction. ApacheJIT consists of clean and bug-inducing software changes in\npopular Apache projects. ApacheJIT has a total of 106,674 commits (28,239\nbug-inducing and 78,435 clean commits). Having a large number of commits makes\nApacheJIT a suitable dataset for machine learning models, especially deep\nlearning models that require large training sets to effectively generalize the\npatterns present in the historical data to future data.\n",
        "published": "2022",
        "authors": [
            "Hossein Keshavarz",
            "Meiyappan Nagappan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.06547v1",
        "title": "Uninorm-like parametric activation functions for human-understandable\n  neural models",
        "abstract": "  We present a deep learning model for finding human-understandable connections\nbetween input features. Our approach uses a parameterized, differentiable\nactivation function, based on the theoretical background of nilpotent fuzzy\nlogic and multi-criteria decision-making (MCDM). The learnable parameter has a\nsemantic meaning indicating the level of compensation between input features.\nThe neural network determines the parameters using gradient descent to find\nhuman-understandable relationships between input features. We demonstrate the\nutility and effectiveness of the model by successfully applying it to\nclassification problems from the UCI Machine Learning Repository.\n",
        "published": "2022",
        "authors": [
            "Orsolya Csisz\u00e1r",
            "Luca S\u00e1ra Pusztah\u00e1zi",
            "Lehel D\u00e9nes-Fazakas",
            "Michael S. Gashler",
            "Vladik Kreinovich",
            "G\u00e1bor Csisz\u00e1r"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.10873v2",
        "title": "Interpretable Scientific Discovery with Symbolic Regression: A Review",
        "abstract": "  Symbolic regression is emerging as a promising machine learning method for\nlearning succinct underlying interpretable mathematical expressions directly\nfrom data. Whereas it has been traditionally tackled with genetic programming,\nit has recently gained a growing interest in deep learning as a data-driven\nmodel discovery method, achieving significant advances in various application\ndomains ranging from fundamental to applied sciences. This survey presents a\nstructured and comprehensive overview of symbolic regression methods and\ndiscusses their strengths and limitations.\n",
        "published": "2022",
        "authors": [
            "Nour Makke",
            "Sanjay Chawla"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.00614v1",
        "title": "Adaptation and Optimization of Automatic Speech Recognition (ASR) for\n  the Maritime Domain in the Field of VHF Communication",
        "abstract": "  This paper introduces a multilingual automatic speech recognizer (ASR) for\nmaritime radio communi-cation that automatically converts received VHF radio\nsignals into text. The challenges of maritime radio communication are described\nat first, and the deep learning architecture of marFM consisting of audio\nprocessing techniques and machine learning algorithms is presented.\nSubsequently, maritime radio data of interest is analyzed and then used to\nevaluate the transcription performance of our ASR model for various maritime\nradio data.\n",
        "published": "2023",
        "authors": [
            "Emin Cagatay Nakilcioglu",
            "Maximilian Reimann",
            "Ole John"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.01242v1",
        "title": "Encoding Binary Events from Continuous Time Series in Rooted Trees using\n  Contrastive Learning",
        "abstract": "  Broadband infrastructure owners do not always know how their customers are\nconnected in the local networks, which are structured as rooted trees. A recent\nstudy is able to infer the topology of a local network using discrete time\nseries data from the leaves of the tree (customers). In this study we propose a\ncontrastive approach for learning a binary event encoder from continuous time\nseries data. As a preliminary result, we show that our approach has some\npotential in learning a valuable encoder.\n",
        "published": "2024",
        "authors": [
            "Tobias Engelhardt Rasmussen",
            "Siv S\u00f8rensen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.02665v1",
        "title": "Zero-shot Microclimate Prediction with Deep Learning",
        "abstract": "  Weather station data is a valuable resource for climate prediction, however,\nits reliability can be limited in remote locations. To compound the issue,\nmaking local predictions often relies on sensor data that may not be accessible\nfor a new, previously unmonitored location. In response to these challenges, we\npropose a novel zero-shot learning approach designed to forecast various\nclimate measurements at new and unmonitored locations. Our method surpasses\nconventional weather forecasting techniques in predicting microclimate\nvariables by leveraging knowledge extracted from other geographic locations.\n",
        "published": "2024",
        "authors": [
            "Iman Deznabi",
            "Peeyush Kumar",
            "Madalina Fiterau"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.12348v1",
        "title": "Bayesian Deep Learning for Graphs",
        "abstract": "  The adaptive processing of structured data is a long-standing research topic\nin machine learning that investigates how to automatically learn a mapping from\na structured input to outputs of various nature. Recently, there has been an\nincreasing interest in the adaptive processing of graphs, which led to the\ndevelopment of different neural network-based methodologies. In this thesis, we\ntake a different route and develop a Bayesian Deep Learning framework for graph\nlearning. The dissertation begins with a review of the principles over which\nmost of the methods in the field are built, followed by a study on graph\nclassification reproducibility issues. We then proceed to bridge the basic\nideas of deep learning for graphs with the Bayesian world, by building our deep\narchitectures in an incremental fashion. This framework allows us to consider\ngraphs with discrete and continuous edge features, producing unsupervised\nembeddings rich enough to reach the state of the art on several classification\ntasks. Our approach is also amenable to a Bayesian nonparametric extension that\nautomatizes the choice of almost all model's hyper-parameters. Two real-world\napplications demonstrate the efficacy of deep learning for graphs. The first\nconcerns the prediction of information-theoretic quantities for molecular\nsimulations with supervised neural models. After that, we exploit our Bayesian\nmodels to solve a malware-classification task while being robust to\nintra-procedural code obfuscation techniques. We conclude the dissertation with\nan attempt to blend the best of the neural and Bayesian worlds together. The\nresulting hybrid model is able to predict multimodal distributions conditioned\non input graphs, with the consequent ability to model stochasticity and\nuncertainty better than most works. Overall, we aim to provide a Bayesian\nperspective into the articulated research field of deep learning for graphs.\n",
        "published": "2022",
        "authors": [
            "Federico Errica"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.14907v1",
        "title": "Patients' Severity States Classification based on Electronic Health\n  Record (EHR) Data using Multiple Machine Learning and Deep Learning\n  Approaches",
        "abstract": "  This research presents an examination of categorizing the severity states of\npatients based on their electronic health records during a certain time range\nusing multiple machine learning and deep learning approaches. The suggested\nmethod uses an EHR dataset collected from an open-source platform to categorize\nseverity. Some tools were used in this research, such as openRefine was used to\npre-process, RapidMiner was used for implementing three algorithms (Fast Large\nMargin, Generalized Linear Model, Multi-layer Feed-forward Neural Network) and\nTableau was used to visualize the data, for implementation of algorithms we\nused Google Colab. Here we implemented several supervised and unsupervised\nalgorithms along with semi-supervised and deep learning algorithms. The\nexperimental results reveal that hyperparameter-tuned Random Forest\noutperformed all the other supervised machine learning algorithms with 76%\naccuracy as well as Generalized Linear algorithm achieved the highest precision\nscore 78%, whereas the hyperparameter-tuned Hierarchical Clustering with 86%\nprecision score and Gaussian Mixture Model with 61% accuracy outperformed other\nunsupervised approaches. Dimensionality Reduction improved results a lot for\nmost unsupervised techniques. For implementing Deep Learning we employed a\nfeed-forward neural network (multi-layer) and the Fast Large Margin approach\nfor semi-supervised learning. The Fast Large Margin performed really well with\na recall score of 84% and an F1 score of 78%. Finally, the Multi-layer\nFeed-forward Neural Network performed admirably with 75% accuracy, 75%\nprecision, 87% recall, 81% F1 score.\n",
        "published": "2022",
        "authors": [
            "A. N. M. Sajedul Alam",
            "Rimi Reza",
            "Asir Abrar",
            "Tanvir Ahmed",
            "Salsabil Ahmed",
            "Shihab Sharar",
            "Annajiat Alim Rasel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.06814v1",
        "title": "Comparative Analysis of Contextual Relation Extraction based on Deep\n  Learning Models",
        "abstract": "  Contextual Relation Extraction (CRE) is mainly used for constructing a\nknowledge graph with a help of ontology. It performs various tasks such as\nsemantic search, query answering, and textual entailment. Relation extraction\nidentifies the entities from raw texts and the relations among them. An\nefficient and accurate CRE system is essential for creating domain knowledge in\nthe biomedical industry. Existing Machine Learning and Natural Language\nProcessing (NLP) techniques are not suitable to predict complex relations from\nsentences that consist of more than two relations and unspecified entities\nefficiently. In this work, deep learning techniques have been used to identify\nthe appropriate semantic relation based on the context from multiple sentences.\nEven though various machine learning models have been used for relation\nextraction, they provide better results only for binary relations, i.e.,\nrelations occurred exactly between the two entities in a sentence. Machine\nlearning models are not suited for complex sentences that consist of the words\nthat have various meanings. To address these issues, hybrid deep learning\nmodels have been used to extract the relations from complex sentence\neffectively. This paper explores the analysis of various deep learning models\nthat are used for relation extraction.\n",
        "published": "2023",
        "authors": [
            "R. Priyadharshini",
            "G. Jeyakodi",
            "P. Shanthi Bala"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.01574v1",
        "title": "Opening the Black Box of Financial AI with CLEAR-Trade: A CLass-Enhanced\n  Attentive Response Approach for Explaining and Visualizing Deep\n  Learning-Driven Stock Market Prediction",
        "abstract": "  Deep learning has been shown to outperform traditional machine learning\nalgorithms across a wide range of problem domains. However, current deep\nlearning algorithms have been criticized as uninterpretable \"black-boxes\" which\ncannot explain their decision making processes. This is a major shortcoming\nthat prevents the widespread application of deep learning to domains with\nregulatory processes such as finance. As such, industries such as finance have\nto rely on traditional models like decision trees that are much more\ninterpretable but less effective than deep learning for complex problems. In\nthis paper, we propose CLEAR-Trade, a novel financial AI visualization\nframework for deep learning-driven stock market prediction that mitigates the\ninterpretability issue of deep learning methods. In particular, CLEAR-Trade\nprovides a effective way to visualize and explain decisions made by deep stock\nmarket prediction models. We show the efficacy of CLEAR-Trade in enhancing the\ninterpretability of stock market prediction by conducting experiments based on\nS&P 500 stock index prediction. The results demonstrate that CLEAR-Trade can\nprovide significant insight into the decision-making process of deep\nlearning-driven financial models, particularly for regulatory processes, thus\nimproving their potential uptake in the financial industry.\n",
        "published": "2017",
        "authors": [
            "Devinder Kumar",
            "Graham W Taylor",
            "Alexander Wong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.12807v2",
        "title": "XOmiVAE: an interpretable deep learning model for cancer classification\n  using high-dimensional omics data",
        "abstract": "  The lack of explainability is one of the most prominent disadvantages of deep\nlearning applications in omics. This \"black box\" problem can undermine the\ncredibility and limit the practical implementation of biomedical deep learning\nmodels. Here we present XOmiVAE, a variational autoencoder (VAE) based\ninterpretable deep learning model for cancer classification using\nhigh-dimensional omics data. XOmiVAE is capable of revealing the contribution\nof each gene and latent dimension for each classification prediction, and the\ncorrelation between each gene and each latent dimension. It is also\ndemonstrated that XOmiVAE can explain not only the supervised classification\nbut the unsupervised clustering results from the deep learning network. To the\nbest of our knowledge, XOmiVAE is one of the first activation level-based\ninterpretable deep learning models explaining novel clusters generated by VAE.\nThe explainable results generated by XOmiVAE were validated by both the\nperformance of downstream tasks and the biomedical knowledge. In our\nexperiments, XOmiVAE explanations of deep learning based cancer classification\nand clustering aligned with current domain knowledge including biological\nannotation and academic literature, which shows great potential for novel\nbiomedical knowledge discovery from deep learning models.\n",
        "published": "2021",
        "authors": [
            "Eloise Withnell",
            "Xiaoyu Zhang",
            "Kai Sun",
            "Yike Guo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.02260v1",
        "title": "Deep Learning for Road Traffic Forecasting: Does it Make a Difference?",
        "abstract": "  Deep Learning methods have been proven to be flexible to model complex\nphenomena. This has also been the case of Intelligent Transportation Systems\n(ITS), in which several areas such as vehicular perception and traffic analysis\nhave widely embraced Deep Learning as a core modeling technology. Particularly\nin short-term traffic forecasting, the capability of Deep Learning to deliver\ngood results has generated a prevalent inertia towards using Deep Learning\nmodels, without examining in depth their benefits and downsides. This paper\nfocuses on critically analyzing the state of the art in what refers to the use\nof Deep Learning for this particular ITS research area. To this end, we\nelaborate on the findings distilled from a review of publications from recent\nyears, based on two taxonomic criteria. A posterior critical analysis is held\nto formulate questions and trigger a necessary debate about the issues of Deep\nLearning for traffic forecasting. The study is completed with a benchmark of\ndiverse short-term traffic forecasting methods over traffic datasets of\ndifferent nature, aimed to cover a wide spectrum of possible scenarios. Our\nexperimentation reveals that Deep Learning could not be the best modeling\ntechnique for every case, which unveils some caveats unconsidered to date that\nshould be addressed by the community in prospective studies. These insights\nreveal new challenges and research opportunities in road traffic forecasting,\nwhich are enumerated and discussed thoroughly, with the intention of inspiring\nand guiding future research efforts in this field.\n",
        "published": "2020",
        "authors": [
            "Eric L. Manibardo",
            "Ibai La\u00f1a",
            "Javier Del Ser"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.07859v3",
        "title": "SenseFi: A Library and Benchmark on Deep-Learning-Empowered WiFi Human\n  Sensing",
        "abstract": "  WiFi sensing has been evolving rapidly in recent years. Empowered by\npropagation models and deep learning methods, many challenging applications are\nrealized such as WiFi-based human activity recognition and gesture recognition.\nHowever, in contrast to deep learning for visual recognition and natural\nlanguage processing, no sufficiently comprehensive public benchmark exists. In\nthis paper, we review the recent progress on deep learning enabled WiFi\nsensing, and then propose a benchmark, SenseFi, to study the effectiveness of\nvarious deep learning models for WiFi sensing. These advanced models are\ncompared in terms of distinct sensing tasks, WiFi platforms, recognition\naccuracy, model size, computational complexity, feature transferability, and\nadaptability of unsupervised learning. It is also regarded as a tutorial for\ndeep learning based WiFi sensing, starting from CSI hardware platform to\nsensing algorithms. The extensive experiments provide us with experiences in\ndeep model design, learning strategy skills and training techniques for\nreal-world applications. To the best of our knowledge, this is the first\nbenchmark with an open-source library for deep learning in WiFi sensing\nresearch. The benchmark codes are available at\nhttps://github.com/xyanchen/WiFi-CSI-Sensing-Benchmark.\n",
        "published": "2022",
        "authors": [
            "Jianfei Yang",
            "Xinyan Chen",
            "Dazhuo Wang",
            "Han Zou",
            "Chris Xiaoxuan Lu",
            "Sumei Sun",
            "Lihua Xie"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.02712v1",
        "title": "Unveiling the frontiers of deep learning: innovations shaping diverse\n  domains",
        "abstract": "  Deep learning (DL) enables the development of computer models that are\ncapable of learning, visualizing, optimizing, refining, and predicting data. In\nrecent years, DL has been applied in a range of fields, including audio-visual\ndata processing, agriculture, transportation prediction, natural language,\nbiomedicine, disaster management, bioinformatics, drug design, genomics, face\nrecognition, and ecology. To explore the current state of deep learning, it is\nnecessary to investigate the latest developments and applications of deep\nlearning in these disciplines. However, the literature is lacking in exploring\nthe applications of deep learning in all potential sectors. This paper thus\nextensively investigates the potential applications of deep learning across all\nmajor fields of study as well as the associated benefits and challenges. As\nevidenced in the literature, DL exhibits accuracy in prediction and analysis,\nmakes it a powerful computational tool, and has the ability to articulate\nitself and optimize, making it effective in processing data with no prior\ntraining. Given its independence from training data, deep learning necessitates\nmassive amounts of data for effective analysis and processing, much like data\nvolume. To handle the challenge of compiling huge amounts of medical,\nscientific, healthcare, and environmental data for use in deep learning, gated\narchitectures like LSTMs and GRUs can be utilized. For multimodal learning,\nshared neurons in the neural network for all activities and specialized neurons\nfor particular tasks are necessary.\n",
        "published": "2023",
        "authors": [
            "Shams Forruque Ahmed",
            "Md. Sakib Bin Alam",
            "Maliha Kabir",
            "Shaila Afrin",
            "Sabiha Jannat Rafa",
            "Aanushka Mehjabin",
            "Amir H. Gandomi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1108.3298v1",
        "title": "A Machine Learning Perspective on Predictive Coding with PAQ",
        "abstract": "  PAQ8 is an open source lossless data compression algorithm that currently\nachieves the best compression rates on many benchmarks. This report presents a\ndetailed description of PAQ8 from a statistical machine learning perspective.\nIt shows that it is possible to understand some of the modules of PAQ8 and use\nthis understanding to improve the method. However, intuitive statistical\nexplanations of the behavior of other modules remain elusive. We hope the\ndescription in this report will be a starting point for discussions that will\nincrease our understanding, lead to improvements to PAQ8, and facilitate a\ntransfer of knowledge from PAQ8 to other machine learning methods, such a\nrecurrent neural networks and stochastic memoizers. Finally, the report\npresents a broad range of new applications of PAQ to machine learning tasks\nincluding language modeling and adaptive text prediction, adaptive game\nplaying, classification, and compression using features from the field of deep\nlearning.\n",
        "published": "2011",
        "authors": [
            "Byron Knoll",
            "Nando de Freitas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.01282v1",
        "title": "Recent Advances in Graph-based Machine Learning for Applications in\n  Smart Urban Transportation Systems",
        "abstract": "  The Intelligent Transportation System (ITS) is an important part of modern\ntransportation infrastructure, employing a combination of communication\ntechnology, information processing and control systems to manage transportation\nnetworks. This integration of various components such as roads, vehicles, and\ncommunication systems, is expected to improve efficiency and safety by\nproviding better information, services, and coordination of transportation\nmodes. In recent years, graph-based machine learning has become an increasingly\nimportant research focus in the field of ITS aiming at the development of\ncomplex, data-driven solutions to address various ITS-related challenges. This\nchapter presents background information on the key technical challenges for ITS\ndesign, along with a review of research methods ranging from classic\nstatistical approaches to modern machine learning and deep learning-based\napproaches. Specifically, we provide an in-depth review of graph-based machine\nlearning methods, including basic concepts of graphs, graph data\nrepresentation, graph neural network architectures and their relation to ITS\napplications. Additionally, two case studies of graph-based ITS applications\nproposed in our recent work are presented in detail to demonstrate the\npotential of graph-based machine learning in the ITS domain.\n",
        "published": "2023",
        "authors": [
            "Hongde Wu",
            "Sen Yan",
            "Mingming Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1712.01626v1",
        "title": "Autonomous development and learning in artificial intelligence and\n  robotics: Scaling up deep learning to human--like learning",
        "abstract": "  Autonomous lifelong development and learning is a fundamental capability of\nhumans, differentiating them from current deep learning systems. However, other\nbranches of artificial intelligence have designed crucial ingredients towards\nautonomous learning: curiosity and intrinsic motivation, social learning and\nnatural interaction with peers, and embodiment. These mechanisms guide\nexploration and autonomous choice of goals, and integrating them with deep\nlearning opens stimulating perspectives. Deep learning (DL) approaches made\ngreat advances in artificial intelligence, but are still far away from human\nlearning. As argued convincingly by Lake et al., differences include human\ncapabilities to learn causal models of the world from very little data,\nleveraging compositional representations and priors like intuitive physics and\npsychology. However, there are other fundamental differences between current DL\nsystems and human learning, as well as technical ingredients to fill this gap,\nthat are either superficially, or not adequately, discussed by Lake et al.\nThese fundamental mechanisms relate to autonomous development and learning.\nThey are bound to play a central role in artificial intelligence in the future.\nCurrent DL systems require engineers to manually specify a task-specific\nobjective function for every new task, and learn through off-line processing of\nlarge training databases. On the contrary, humans learn autonomously open-ended\nrepertoires of skills, deciding for themselves which goals to pursue or value,\nand which skills to explore, driven by intrinsic motivation/curiosity and\nsocial learning through natural interaction with peers. Such learning processes\nare incremental, online, and progressive. Human child development involves a\nprogressive increase of complexity in a curriculum of learning where skills are\nexplored, acquired, and built on each other, through particular ordering and\ntiming. Finally, human learning happens in the physical world, and through\nbodily and physical experimentation, under severe constraints on energy, time,\nand computational resources. In the two last decades, the field of\nDevelopmental and Cognitive Robotics (Cangelosi and Schlesinger, 2015, Asada et\nal., 2009), in strong interaction with developmental psychology and\nneuroscience, has achieved significant advances in computational\n",
        "published": "2017",
        "authors": [
            "Pierre-Yves Oudeyer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.05348v1",
        "title": "From Crystallized Adaptivity to Fluid Adaptivity in Deep Reinforcement\n  Learning -- Insights from Biological Systems on Adaptive Flexibility",
        "abstract": "  Recent developments in machine-learning algorithms have led to impressive\nperformance increases in many traditional application scenarios of artificial\nintelligence research. In the area of deep reinforcement learning, deep\nlearning functional architectures are combined with incremental learning\nschemes for sequential tasks that include interaction-based, but often delayed\nfeedback. Despite their impressive successes, modern machine-learning\napproaches, including deep reinforcement learning, still perform weakly when\ncompared to flexibly adaptive biological systems in certain naturally occurring\nscenarios. Such scenarios include transfers to environments different than the\nones in which the training took place or environments that dynamically change,\nboth of which are often mastered by biological systems through a capability\nthat we here term \"fluid adaptivity\" to contrast it from the much slower\nadaptivity (\"crystallized adaptivity\") of the prior learning from which the\nbehavior emerged. In this article, we derive and discuss research strategies,\nbased on analyzes of fluid adaptivity in biological systems and its neuronal\nmodeling, that might aid in equipping future artificially intelligent systems\nwith capabilities of fluid adaptivity more similar to those seen in some\nbiologically intelligent systems. A key component of this research strategy is\nthe dynamization of the problem space itself and the implementation of this\ndynamization by suitably designed flexibly interacting modules.\n",
        "published": "2019",
        "authors": [
            "Malte Schilling",
            "Helge Ritter",
            "Frank W. Ohl"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.07758v1",
        "title": "Automated Biodesign Engineering by Abductive Meta-Interpretive Learning",
        "abstract": "  The application of Artificial Intelligence (AI) to synthetic biology will\nprovide the foundation for the creation of a high throughput automated platform\nfor genetic design, in which a learning machine is used to iteratively optimise\nthe system through a design-build-test-learn (DBTL) cycle. However, mainstream\nmachine learning techniques represented by deep learning lacks the capability\nto represent relational knowledge and requires prodigious amounts of annotated\ntraining data. These drawbacks strongly restrict AI's role in synthetic biology\nin which experimentation is inherently resource and time intensive. In this\nwork, we propose an automated biodesign engineering framework empowered by\nAbductive Meta-Interpretive Learning ($Meta_{Abd}$), a novel machine learning\napproach that combines symbolic and sub-symbolic machine learning, to further\nenhance the DBTL cycle by enabling the learning machine to 1) exploit domain\nknowledge and learn human-interpretable models that are expressed by formal\nlanguages such as first-order logic; 2) simultaneously optimise the structure\nand parameters of the models to make accurate numerical predictions; 3) reduce\nthe cost of experiments and effort on data annotation by actively generating\nhypotheses and examples. To verify the effectiveness of $Meta_{Abd}$, we have\nmodelled a synthetic dataset for the production of proteins from a three gene\noperon in a microbial host, which represents a common synthetic biology\nproblem.\n",
        "published": "2021",
        "authors": [
            "Wang-Zhou Dai",
            "Liam Hallett",
            "Stephen H. Muggleton",
            "Geoff S. Baldwin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.13995v1",
        "title": "A clustering and graph deep learning-based framework for COVID-19 drug\n  repurposing",
        "abstract": "  Drug repurposing (or repositioning) is the process of finding new therapeutic\nuses for drugs already approved by drug regulatory authorities (e.g., the Food\nand Drug Administration (FDA) and Therapeutic Goods Administration (TGA)) for\nother diseases. This involves analyzing the interactions between different\nbiological entities, such as drug targets (genes/proteins and biological\npathways) and drug properties, to discover novel drug-target or drug-disease\nrelations. Artificial intelligence methods such as machine learning and deep\nlearning have successfully analyzed complex heterogeneous data in the\nbiomedical domain and have also been used for drug repurposing. This study\npresents a novel unsupervised machine learning framework that utilizes a\ngraph-based autoencoder for multi-feature type clustering on heterogeneous drug\ndata. The dataset consists of 438 drugs, of which 224 are under clinical trials\nfor COVID-19 (category A). The rest are systematically filtered to ensure the\nsafety and efficacy of the treatment (category B). The framework solely relies\non reported drug data, including its pharmacological properties,\nchemical/physical properties, interaction with the host, and efficacy in\ndifferent publicly available COVID-19 assays. Our machine-learning framework\nreveals three clusters of interest and provides recommendations featuring the\ntop 15 drugs for COVID-19 drug repurposing, which were shortlisted based on the\npredicted clusters that were dominated by category A drugs. The anti-COVID\nefficacy of the drugs should be verified by experimental studies. Our framework\ncan be extended to support other datasets and drug repurposing studies, given\nopen-source code and data availability.\n",
        "published": "2023",
        "authors": [
            "Chaarvi Bansal",
            "Rohitash Chandra",
            "Vinti Agarwal",
            "P. R. Deepa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.04764v3",
        "title": "Hack The Box: Fooling Deep Learning Abstraction-Based Monitors",
        "abstract": "  Deep learning is a type of machine learning that adapts a deep hierarchy of\nconcepts. Deep learning classifiers link the most basic version of concepts at\nthe input layer to the most abstract version of concepts at the output layer,\nalso known as a class or label. However, once trained over a finite set of\nclasses, some deep learning models do not have the power to say that a given\ninput does not belong to any of the classes and simply cannot be linked.\nCorrectly invalidating the prediction of unrelated classes is a challenging\nproblem that has been tackled in many ways in the literature. Novelty detection\ngives deep learning the ability to output \"do not know\" for novel/unseen\nclasses. Still, no attention has been given to the security aspects of novelty\ndetection. In this paper, we consider the case study of abstraction-based\nnovelty detection and show that it is not robust against adversarial samples.\nMoreover, we show the feasibility of crafting adversarial samples that fool the\ndeep learning classifier and bypass the novelty detection monitoring at the\nsame time. In other words, these monitoring boxes are hackable. We demonstrate\nthat novelty detection itself ends up as an attack surface.\n",
        "published": "2021",
        "authors": [
            "Sara Hajj Ibrahim",
            "Mohamed Nassar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.00698v1",
        "title": "Deep-learning-based upscaling method for geologic models via\n  theory-guided convolutional neural network",
        "abstract": "  Large-scale or high-resolution geologic models usually comprise a huge number\nof grid blocks, which can be computationally demanding and time-consuming to\nsolve with numerical simulators. Therefore, it is advantageous to upscale\ngeologic models (e.g., hydraulic conductivity) from fine-scale (high-resolution\ngrids) to coarse-scale systems. Numerical upscaling methods have been proven to\nbe effective and robust for coarsening geologic models, but their efficiency\nremains to be improved. In this work, a deep-learning-based method is proposed\nto upscale the fine-scale geologic models, which can assist to improve\nupscaling efficiency significantly. In the deep learning method, a deep\nconvolutional neural network (CNN) is trained to approximate the relationship\nbetween the coarse grid of hydraulic conductivity fields and the hydraulic\nheads, which can then be utilized to replace the numerical solvers while\nsolving the flow equations for each coarse block. In addition, physical laws\n(e.g., governing equations and periodic boundary conditions) can also be\nincorporated into the training process of the deep CNN model, which is termed\nthe theory-guided convolutional neural network (TgCNN). With the physical\ninformation considered, dependence on the data volume of training the deep\nlearning models can be reduced greatly. Several subsurface flow cases are\nintroduced to test the performance of the proposed deep-learning-based\nupscaling method, including 2D and 3D cases, and isotropic and anisotropic\ncases. The results show that the deep learning method can provide equivalent\nupscaling accuracy to the numerical method, and efficiency can be improved\nsignificantly compared to numerical upscaling.\n",
        "published": "2021",
        "authors": [
            "Nanzhe Wang",
            "Qinzhuo Liao",
            "Haibin Chang",
            "Dongxiao Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.04356v4",
        "title": "Deep learning for time series classification: a review",
        "abstract": "  Time Series Classification (TSC) is an important and challenging problem in\ndata mining. With the increase of time series data availability, hundreds of\nTSC algorithms have been proposed. Among these methods, only a few have\nconsidered Deep Neural Networks (DNNs) to perform this task. This is surprising\nas deep learning has seen very successful applications in the last years. DNNs\nhave indeed revolutionized the field of computer vision especially with the\nadvent of novel deeper architectures such as Residual and Convolutional Neural\nNetworks. Apart from images, sequential data such as text and audio can also be\nprocessed with DNNs to reach state-of-the-art performance for document\nclassification and speech recognition. In this article, we study the current\nstate-of-the-art performance of deep learning algorithms for TSC by presenting\nan empirical study of the most recent DNN architectures for TSC. We give an\noverview of the most successful deep learning applications in various time\nseries domains under a unified taxonomy of DNNs for TSC. We also provide an\nopen source deep learning framework to the TSC community where we implemented\neach of the compared approaches and evaluated them on a univariate TSC\nbenchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By\ntraining 8,730 deep learning models on 97 time series datasets, we propose the\nmost exhaustive study of DNNs for TSC to date.\n",
        "published": "2018",
        "authors": [
            "Hassan Ismail Fawaz",
            "Germain Forestier",
            "Jonathan Weber",
            "Lhassane Idoumghar",
            "Pierre-Alain Muller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.06365v4",
        "title": "An Overview of Deep Learning Architectures in Few-Shot Learning Domain",
        "abstract": "  Since 2012, Deep learning has revolutionized Artificial Intelligence and has\nachieved state-of-the-art outcomes in different domains, ranging from Image\nClassification to Speech Generation. Though it has many potentials, our current\narchitectures come with the pre-requisite of large amounts of data. Few-Shot\nLearning (also known as one-shot learning) is a sub-field of machine learning\nthat aims to create such models that can learn the desired objective with less\ndata, similar to how humans learn. In this paper, we have reviewed some of the\nwell-known deep learning-based approaches towards few-shot learning. We have\ndiscussed the recent achievements, challenges, and possibilities of improvement\nof few-shot learning based deep learning architectures. Our aim for this paper\nis threefold: (i) Give a brief introduction to deep learning architectures for\nfew-shot learning with pointers to core references. (ii) Indicate how deep\nlearning has been applied to the low-data regime, from data preparation to\nmodel training. and, (iii) Provide a starting point for people interested in\nexperimenting and perhaps contributing to the field of few-shot learning by\npointing out some useful resources and open-source code. Our code is available\nat Github: https://github.com/shruti-jadon/Hands-on-One-Shot-Learning.\n",
        "published": "2020",
        "authors": [
            "Shruti Jadon",
            "Aryan Jadon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.12497v2",
        "title": "Designing ECG Monitoring Healthcare System with Federated Transfer\n  Learning and Explainable AI",
        "abstract": "  Deep learning play a vital role in classifying different arrhythmias using\nthe electrocardiography (ECG) data. Nevertheless, training deep learning models\nnormally requires a large amount of data and it can lead to privacy concerns.\nUnfortunately, a large amount of healthcare data cannot be easily collected\nfrom a single silo. Additionally, deep learning models are like black-box, with\nno explainability of the predicted results, which is often required in clinical\nhealthcare. This limits the application of deep learning in real-world health\nsystems. In this paper, we design a new explainable artificial intelligence\n(XAI) based deep learning framework in a federated setting for ECG-based\nhealthcare applications. The federated setting is used to solve issues such as\ndata availability and privacy concerns. Furthermore, the proposed framework\nsetting effectively classifies arrhythmia's using an autoencoder and a\nclassifier, both based on a convolutional neural network (CNN). Additionally,\nwe propose an XAI-based module on top of the proposed classifier to explain the\nclassification results, which help clinical practitioners make quick and\nreliable decisions. The proposed framework was trained and tested using the\nMIT-BIH Arrhythmia database. The classifier achieved accuracy up to 94% and 98%\nfor arrhythmia detection using noisy and clean data, respectively, with\nfive-fold cross-validation.\n",
        "published": "2021",
        "authors": [
            "Ali Raza",
            "Kim Phuc Tran",
            "Ludovic Koehl",
            "Shujun Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.08244v2",
        "title": "Artificial Intelligence for Automatic Detection and Classification\n  Disease on the X-Ray Images",
        "abstract": "  Detecting and classifying diseases using X-ray images is one of the more\nchallenging core tasks in the medical and research world. Due to the recent\nhigh interest in radiological images and AI, early detection of diseases in\nX-ray images has become notably more essential to prevent further spreading and\nflatten the curve. Innovations and revolutions of Computer Vision with Deep\nlearning methods offer great promise for fast and accurate diagnosis of\nscreening and detection from chest X-ray images (CXR). This work presents rapid\ndetection of diseases in the lung using the efficient Deep learning pre-trained\nRepVGG algorithm for deep feature extraction and classification. We used X-ray\nimages as an example to show the model's efficiency. To perform this task, we\nclassify X-Ray images into Covid-19, Pneumonia, and Normal X-Ray images. Employ\nROI object to improve the detection accuracy for lung extraction, followed by\ndata pre-processing and augmentation. We are applying Artificial Intelligence\ntechnology for automatic highlighted detection of affected areas of people's\nlungs. Based on the X-Ray images, an algorithm was developed that classifies\nX-Ray images with height accuracy and power faster thanks to the architecture\ntransformation of the model. We compared deep learning frameworks' accuracy and\ndetection of disease. The study shows the high power of deep learning methods\nfor X-ray images based on COVID-19 detection utilizing chest X-rays. The\nproposed framework offers better diagnostic accuracy by comparing popular deep\nlearning models, i.e., VGG, ResNet50, inceptionV3, DenseNet, and\nInceptionResnetV2.\n",
        "published": "2022",
        "authors": [
            "Liora Mayats-Alpay"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.03905v3",
        "title": "Security Concerns on Machine Learning Solutions for 6G Networks in\n  mmWave Beam Prediction",
        "abstract": "  6G -- sixth generation -- is the latest cellular technology currently under\ndevelopment for wireless communication systems. In recent years, machine\nlearning algorithms have been applied widely in various fields, such as\nhealthcare, transportation, energy, autonomous car, and many more. Those\nalgorithms have been also using in communication technologies to improve the\nsystem performance in terms of frequency spectrum usage, latency, and security.\nWith the rapid developments of machine learning techniques, especially deep\nlearning, it is critical to take the security concern into account when\napplying the algorithms. While machine learning algorithms offer significant\nadvantages for 6G networks, security concerns on Artificial Intelligent (AI)\nmodels is typically ignored by the scientific community so far. However,\nsecurity is also a vital part of the AI algorithms, this is because the AI\nmodel itself can be poisoned by attackers. This paper proposes a mitigation\nmethod for adversarial attacks against proposed 6G machine learning models for\nthe millimeter-wave (mmWave) beam prediction using adversarial learning. The\nmain idea behind adversarial attacks against machine learning models is to\nproduce faulty results by manipulating trained deep learning models for 6G\napplications for mmWave beam prediction. We also present the adversarial\nlearning mitigation method's performance for 6G security in mmWave beam\nprediction application with fast gradient sign method attack. The mean square\nerrors (MSE) of the defended model under attack are very close to the\nundefended model without attack.\n",
        "published": "2021",
        "authors": [
            "Ferhat Ozgur Catak",
            "Evren Catak",
            "Murat Kuzlu",
            "Umit Cali",
            "Devrim Unal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.03982v2",
        "title": "Quantitative Phase Imaging and Artificial Intelligence: A Review",
        "abstract": "  Recent advances in quantitative phase imaging (QPI) and artificial\nintelligence (AI) have opened up the possibility of an exciting frontier. The\nfast and label-free nature of QPI enables the rapid generation of large-scale\nand uniform-quality imaging data in two, three, and four dimensions.\nSubsequently, the AI-assisted interrogation of QPI data using data-driven\nmachine learning techniques results in a variety of biomedical applications.\nAlso, machine learning enhances QPI itself. Herein, we review the synergy\nbetween QPI and machine learning with a particular focus on deep learning.\nFurther, we provide practical guidelines and perspectives for further\ndevelopment.\n",
        "published": "2018",
        "authors": [
            "YoungJu Jo",
            "Hyungjoo Cho",
            "Sang Yun Lee",
            "Gunho Choi",
            "Geon Kim",
            "Hyun-seok Min",
            "YongKeun Park"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.04521v1",
        "title": "Learning to be Global Optimizer",
        "abstract": "  The advancement of artificial intelligence has cast a new light on the\ndevelopment of optimization algorithm. This paper proposes to learn a two-phase\n(including a minimization phase and an escaping phase) global optimization\nalgorithm for smooth non-convex functions. For the minimization phase, a\nmodel-driven deep learning method is developed to learn the update rule of\ndescent direction, which is formalized as a nonlinear combination of historical\ninformation, for convex functions. We prove that the resultant algorithm with\nthe proposed adaptive direction guarantees convergence for convex functions.\nEmpirical study shows that the learned algorithm significantly outperforms some\nwell-known classical optimization algorithms, such as gradient descent,\nconjugate descent and BFGS, and performs well on ill-posed functions. The\nescaping phase from local optimum is modeled as a Markov decision process with\na fixed escaping policy. We further propose to learn an optimal escaping policy\nby reinforcement learning. The effectiveness of the escaping policies is\nverified by optimizing synthesized functions and training a deep neural network\nfor CIFAR image classification. The learned two-phase global optimization\nalgorithm demonstrates a promising global search capability on some benchmark\nfunctions and machine learning tasks.\n",
        "published": "2020",
        "authors": [
            "Haotian Zhang",
            "Jianyong Sun",
            "Zongben Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.01263v1",
        "title": "Safety design concepts for statistical machine learning components\n  toward accordance with functional safety standards",
        "abstract": "  In recent years, curial incidents and accidents have been reported due to\nun-intended control caused by misjudgment of statistical machine learning\n(SML), which include deep learning. The international functional safety\nstandards for Electric/Electronic/Programmable (E/E/P) systems have been widely\nspread to improve safety. However, most of them do not recom-mended to use SML\nin safety critical systems so far. In practical the new concepts and methods\nare urgently required to enable SML to be safely used in safety critical\nsystems. In this paper, we organize five kinds of technical safety concepts\n(TSCs) for SML components toward accordance with functional safety standards.\nWe discuss not only quantitative evaluation criteria, but also development\nprocess based on XAI (eXplainable Artificial Intelligence) and Automotive SPICE\nto improve explainability and reliability in development phase. Fi-nally, we\nbriefly compare the TSCs in cost and difficulty, and expect to en-courage\nfurther discussion in many communities and domain.\n",
        "published": "2020",
        "authors": [
            "Akihisa Morikawa",
            "Yutaka Matsubara"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1610.09900v2",
        "title": "Inference Compilation and Universal Probabilistic Programming",
        "abstract": "  We introduce a method for using deep neural networks to amortize the cost of\ninference in models from the family induced by universal probabilistic\nprogramming languages, establishing a framework that combines the strengths of\nprobabilistic programming and deep learning methods. We call what we do\n\"compilation of inference\" because our method transforms a denotational\nspecification of an inference problem in the form of a probabilistic program\nwritten in a universal programming language into a trained neural network\ndenoted in a neural network specification language. When at test time this\nneural network is fed observational data and executed, it performs approximate\ninference in the original model specified by the probabilistic program. Our\ntraining objective and learning procedure are designed to allow the trained\nneural network to be used as a proposal distribution in a sequential importance\nsampling inference engine. We illustrate our method on mixture models and\nCaptcha solving and show significant speedups in the efficiency of inference.\n",
        "published": "2016",
        "authors": [
            "Tuan Anh Le",
            "Atilim Gunes Baydin",
            "Frank Wood"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.11709v3",
        "title": "Intelligent Nanophotonics: Merging Photonics and Artificial Intelligence\n  at the Nanoscale",
        "abstract": "  Nanophotonics has been an active research field over the past two decades,\ntriggered by the rising interests in exploring new physics and technologies\nwith light at the nanoscale. As the demands of performance and integration\nlevel keep increasing, the design and optimization of nanophotonic devices\nbecome computationally expensive and time-inefficient. Advanced computational\nmethods and artificial intelligence, especially its subfield of machine\nlearning, have led to revolutionary development in many applications, such as\nweb searches, computer vision, and speech/image recognition. The complex models\nand algorithms help to exploit the enormous parameter space in a highly\nefficient way. In this review, we summarize the recent advances on the emerging\nfield where nanophotonics and machine learning blend. We provide an overview of\ndifferent computational methods, with the focus on deep learning, for the\nnanophotonic inverse design. The implementation of deep neural networks with\nphotonic platforms is also discussed. This review aims at sketching an\nillustration of the nanophotonic design with machine learning and giving a\nperspective on the future tasks.\n",
        "published": "2018",
        "authors": [
            "Kan Yao",
            "Rohit Unni",
            "Yuebing Zheng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.00753v1",
        "title": "Galaxy Learning -- A Position Paper",
        "abstract": "  The recent rapid development of artificial intelligence (AI, mainly driven by\nmachine learning research, especially deep learning) has achieved phenomenal\nsuccess in various applications. However, to further apply AI technologies in\nreal-world context, several significant issues regarding the AI ecosystem\nshould be addressed. We identify the main issues as data privacy, ownership,\nand exchange, which are difficult to be solved with the current centralized\nparadigm of machine learning training methodology. As a result, we propose a\nnovel model training paradigm based on blockchain, named Galaxy Learning, which\naims to train a model with distributed data and to reserve the data ownership\nfor their owners. In this new paradigm, encrypted models are moved around\ninstead, and are federated once trained. Model training, as well as the\ncommunication, is achieved with blockchain and its smart contracts. Pricing of\ntraining data is determined by its contribution, and therefore it is not about\nthe exchange of data ownership. In this position paper, we describe the\nmotivation, paradigm, design, and challenges as well as opportunities of Galaxy\nLearning.\n",
        "published": "2019",
        "authors": [
            "Chao Wu",
            "Jun Xiao",
            "Gang Huang",
            "Fei Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.10513v2",
        "title": "Local Post-Hoc Explanations for Predictive Process Monitoring in\n  Manufacturing",
        "abstract": "  This study proposes an innovative explainable predictive quality analytics\nsolution to facilitate data-driven decision-making for process planning in\nmanufacturing by combining process mining, machine learning, and explainable\nartificial intelligence (XAI) methods. For this purpose, after integrating the\ntop-floor and shop-floor data obtained from various enterprise information\nsystems, a deep learning model was applied to predict the process outcomes.\nSince this study aims to operationalize the delivered predictive insights by\nembedding them into decision-making processes, it is essential to generate\nrelevant explanations for domain experts. To this end, two complementary local\npost-hoc explanation approaches, Shapley values and Individual Conditional\nExpectation (ICE) plots are adopted, which are expected to enhance the\ndecision-making capabilities by enabling experts to examine explanations from\ndifferent perspectives. After assessing the predictive strength of the applied\ndeep neural network with relevant binary classification evaluation measures, a\ndiscussion of the generated explanations is provided.\n",
        "published": "2020",
        "authors": [
            "Nijat Mehdiyev",
            "Peter Fettke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.01938v1",
        "title": "Medical Imaging and Machine Learning",
        "abstract": "  Advances in computing power, deep learning architectures, and expert labelled\ndatasets have spurred the development of medical imaging artificial\nintelligence systems that rival clinical experts in a variety of scenarios. The\nNational Institutes of Health in 2018 identified key focus areas for the future\nof artificial intelligence in medical imaging, creating a foundational roadmap\nfor research in image acquisition, algorithms, data standardization, and\ntranslatable clinical decision support systems. Among the key issues raised in\nthe report: data availability, need for novel computing architectures and\nexplainable AI algorithms, are still relevant despite the tremendous progress\nmade over the past few years alone. Furthermore, translational goals of data\nsharing, validation of performance for regulatory approval, generalizability\nand mitigation of unintended bias must be accounted for early in the\ndevelopment process. In this perspective paper we explore challenges unique to\nhigh dimensional clinical imaging data, in addition to highlighting some of the\ntechnical and ethical considerations in developing high-dimensional,\nmulti-modality, machine learning systems for clinical decision support.\n",
        "published": "2021",
        "authors": [
            "Rohan Shad",
            "John P. Cunningham",
            "Euan A. Ashley",
            "Curtis P. Langlotz",
            "William Hiesinger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.03608v3",
        "title": "Eigen-spectrograms: An interpretable feature space for bearing fault\n  diagnosis based on artificial intelligence and image processing",
        "abstract": "  The Intelligent Fault Diagnosis of rotating machinery currently proposes some\ncaptivating challenges. Although results achieved by artificial intelligence\nand deep learning constantly improve, this field is characterized by several\nopen issues. Models' interpretation is still buried under the foundations of\ndata driven science, thus requiring attention to the development of new\nopportunities also for machine learning theories. This study proposes a machine\nlearning diagnosis model, based on intelligent spectrogram recognition, via\nimage processing. The approach is characterized by the employment of the\neigen-spectrograms and randomized linear algebra in fault diagnosis. Randomized\nalgebra and eigen-spectrograms enable the construction of a significant feature\nspace, which nonetheless emerges as a viable device to explore models'\ninterpretations. The computational efficiency of randomized approaches provides\nreading keys of well-established statistical learning theories such as the\nSupport Vector Machine (SVM). Machine learning applied to spectrogram\nrecognition shows to be extremely accurate and efficient as compared to state\nof the art results.\n",
        "published": "2021",
        "authors": [
            "Eugenio Brusa",
            "Cristiana Delprete",
            "Luigi Gianpio Di Maggio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.00252v1",
        "title": "Interpretable Medical Imagery Diagnosis with Self-Attentive\n  Transformers: A Review of Explainable AI for Health Care",
        "abstract": "  Recent advancements in artificial intelligence (AI) have facilitated its\nwidespread adoption in primary medical services, addressing the demand-supply\nimbalance in healthcare. Vision Transformers (ViT) have emerged as\nstate-of-the-art computer vision models, benefiting from self-attention\nmodules. However, compared to traditional machine-learning approaches,\ndeep-learning models are complex and are often treated as a \"black box\" that\ncan cause uncertainty regarding how they operate. Explainable Artificial\nIntelligence (XAI) refers to methods that explain and interpret machine\nlearning models' inner workings and how they come to decisions, which is\nespecially important in the medical domain to guide the healthcare\ndecision-making process. This review summarises recent ViT advancements and\ninterpretative approaches to understanding the decision-making process of ViT,\nenabling transparency in medical diagnosis applications.\n",
        "published": "2023",
        "authors": [
            "Tin Lai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.02229v1",
        "title": "Synthetic Data Generation Techniques for Developing AI-based Speech\n  Assessments for Parkinson's Disease (A Comparative Study)",
        "abstract": "  Changes in speech and language are among the first signs of Parkinson's\ndisease (PD). Thus, clinicians have tried to identify individuals with PD from\ntheir voices for years. Doctors can leverage AI-based speech assessments to\nspot PD thanks to advancements in artificial intelligence (AI). Such AI systems\ncan be developed using machine learning classifiers that have been trained\nusing individuals' voices. Although several studies have shown reasonable\nresults in developing such AI systems, these systems would need more data\nsamples to achieve promising performance. This paper explores using deep\nlearning-based data generation techniques on the accuracy of machine learning\nclassifiers that are the core of such systems.\n",
        "published": "2023",
        "authors": [
            "Mahboobeh Parsapoor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.00867v2",
        "title": "Tensor Networks for Explainable Machine Learning in Cybersecurity",
        "abstract": "  In this paper we show how tensor networks help in developing explainability\nof machine learning algorithms. Specifically, we develop an unsupervised\nclustering algorithm based on Matrix Product States (MPS) and apply it in the\ncontext of a real use-case of adversary-generated threat intelligence. Our\ninvestigation proves that MPS rival traditional deep learning models such as\nautoencoders and GANs in terms of performance, while providing much richer\nmodel interpretability. Our approach naturally facilitates the extraction of\nfeature-wise probabilities, Von Neumann Entropy, and mutual information,\noffering a compelling narrative for classification of anomalies and fostering\nan unprecedented level of transparency and interpretability, something\nfundamental to understand the rationale behind artificial intelligence\ndecisions.\n",
        "published": "2023",
        "authors": [
            "Borja Aizpurua",
            "Roman Orus"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.03722v1",
        "title": "Challenges in Vessel Behavior and Anomaly Detection: From Classical\n  Machine Learning to Deep Learning",
        "abstract": "  The global expansion of maritime activities and the development of the\nAutomatic Identification System (AIS) have driven the advances in maritime\nmonitoring systems in the last decade. Monitoring vessel behavior is\nfundamental to safeguard maritime operations, protecting other vessels sailing\nthe ocean and the marine fauna and flora. Given the enormous volume of vessel\ndata continually being generated, real-time analysis of vessel behaviors is\nonly possible because of decision support systems provided with event and\nanomaly detection methods. However, current works on vessel event detection are\nad-hoc methods able to handle only a single or a few predefined types of vessel\nbehavior. Most of the existing approaches do not learn from the data and\nrequire the definition of queries and rules for describing each behavior. In\nthis paper, we discuss challenges and opportunities in classical machine\nlearning and deep learning for vessel event and anomaly detection. We hope to\nmotivate the research of novel methods and tools, since addressing these\nchallenges is an essential step towards actual intelligent maritime monitoring\nsystems.\n",
        "published": "2020",
        "authors": [
            "Lucas May Petry",
            "Amilcar Soares",
            "Vania Bogorny",
            "Bruno Brandoli",
            "Stan Matwin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.10663v1",
        "title": "Energy time series forecasting-Analytical and empirical assessment of\n  conventional and machine learning models",
        "abstract": "  Machine learning methods have been adopted in the literature as contenders to\nconventional methods to solve the energy time series forecasting (TSF)\nproblems. Recently, deep learning methods have been emerged in the artificial\nintelligence field attaining astonishing performance in a wide range of\napplications. Yet, the evidence about their performance in to solve the energy\nTSF problems, in terms of accuracy and computational requirements, is scanty.\nMost of the review articles that handle the energy TSF problem are systematic\nreviews, however, a qualitative and quantitative study for the energy TSF\nproblem is not yet available in the literature. The purpose of this paper is\ntwofold, first it provides a comprehensive analytical assessment for\nconventional,machine learning, and deep learning methods that can be utilized\nto solve various energy TSF problems. Second, the paper carries out an\nempirical assessment for many selected methods through three real-world\ndatasets. These datasets related to electrical energy consumption problem,\nnatural gas problem, and electric power consumption of an individual household\nproblem.The first two problems are univariate TSF and the third problem is a\nmultivariate TSF. Com-pared to both conventional and machine learning\ncontenders, the deep learning methods attain a significant improvement in terms\nof accuracy and forecasting horizons examined. In the mean-time, their\ncomputational requirements are notably greater than other contenders.\nEventually,the paper identifies a number of challenges, potential research\ndirections, and recommendations to the research community may serve as a basis\nfor further research in the energy forecasting domain.\n",
        "published": "2021",
        "authors": [
            "Hala Hamdoun",
            "Alaa Sagheer",
            "Hassan Youness"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.09178v1",
        "title": "Alzheimers Disease Diagnosis using Machine Learning: A Review",
        "abstract": "  Alzheimers Disease AD is an acute neuro disease that degenerates the brain\ncells and thus leads to memory loss progressively. It is a fatal brain disease\nthat mostly affects the elderly. It steers the decline of cognitive and\nbiological functions of the brain and shrinks the brain successively, which in\nturn is known as Atrophy. For an accurate diagnosis of Alzheimers disease,\ncutting edge methods like machine learning are essential. Recently, machine\nlearning has gained a lot of attention and popularity in the medical industry.\nAs the illness progresses, those with Alzheimers have a far more difficult time\ndoing even the most basic tasks, and in the worst case, their brain completely\nstops functioning. A persons likelihood of having early-stage Alzheimers\ndisease may be determined using the ML method. In this analysis, papers on\nAlzheimers disease diagnosis based on deep learning techniques and\nreinforcement learning between 2008 and 2023 found in google scholar were\nstudied. Sixty relevant papers obtained after the search was considered for\nthis study. These papers were analysed based on the biomarkers of AD and the\nmachine-learning techniques used. The analysis shows that deep learning methods\nhave an immense ability to extract features and classify AD with good accuracy.\nThe DRL methods have not been used much in the field of image processing. The\ncomparison results of deep learning and reinforcement learning illustrate that\nthe scope of Deep Reinforcement Learning DRL in dementia detection needs to be\nexplored.\n",
        "published": "2023",
        "authors": [
            "Nair Bini Balakrishnan",
            "P. S. Sreeja",
            "Jisha Jose Panackal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.01148v2",
        "title": "Applied Machine Learning for Games: A Graduate School Course",
        "abstract": "  The game industry is moving into an era where old-style game engines are\nbeing replaced by re-engineered systems with embedded machine learning\ntechnologies for the operation, analysis and understanding of game play. In\nthis paper, we describe our machine learning course designed for graduate\nstudents interested in applying recent advances of deep learning and\nreinforcement learning towards gaming. This course serves as a bridge to foster\ninterdisciplinary collaboration among graduate schools and does not require\nprior experience designing or building games. Graduate students enrolled in\nthis course apply different fields of machine learning techniques such as\ncomputer vision, natural language processing, computer graphics, human computer\ninteraction, robotics and data analysis to solve open challenges in gaming.\nStudent projects cover use-cases such as training AI-bots in gaming benchmark\nenvironments and competitions, understanding human decision patterns in gaming,\nand creating intelligent non-playable characters or environments to foster\nengaging gameplay. Projects demos can help students open doors for an industry\ncareer, aim for publications, or lay the foundations of a future product. Our\nstudents gained hands-on experience in applying state of the art machine\nlearning techniques to solve real-life problems in gaming.\n",
        "published": "2020",
        "authors": [
            "Yilei Zeng",
            "Aayush Shah",
            "Jameson Thai",
            "Michael Zyda"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.01661v1",
        "title": "Data science and Machine learning in the Clouds: A Perspective for the\n  Future",
        "abstract": "  As we are fast approaching the beginning of a paradigm shift in the field of\nscience, Data driven science (the so called fourth science paradigm) is going\nto be the driving force in research and innovation. From medicine to\nbiodiversity and astronomy to geology, all these terms are somehow going to be\naffected by this paradigm shift. The huge amount of data to be processed under\nthis new paradigm will be a major concern in the future and one will strongly\nrequire cloud based services in all the aspects of these computations (from\nstorage to compute and other services). Another aspect will be energy\nconsumption and performance of prediction jobs and tasks within such a\nscientific paradigm which will change the way one sees computation. Data\nscience has heavily impacted or rather triggered the emergence of Machine\nLearning, Signal/Image/Video processing related algorithms, Artificial\nintelligence, Robotics, health informatics, geoinformatics, and many more such\nareas of interest. Hence, we envisage an era where Data science can deliver its\npromises with the help of the existing cloud based platforms and services with\nthe addition of new services. In this article, we discuss about data driven\nscience and Machine learning and how they are going to be linked through cloud\nbased services in the future. It also discusses the rise of paradigms like\napproximate computing, quantum computing and many more in recent times and\ntheir applicability in big data processing, data science, analytics, prediction\nand machine learning in the cloud environments.\n",
        "published": "2021",
        "authors": [
            "Hrishav Bakul Barua"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.11371v2",
        "title": "Opportunities and Challenges in Explainable Artificial Intelligence\n  (XAI): A Survey",
        "abstract": "  Nowadays, deep neural networks are widely used in mission critical systems\nsuch as healthcare, self-driving vehicles, and military which have direct\nimpact on human lives. However, the black-box nature of deep neural networks\nchallenges its use in mission critical applications, raising ethical and\njudicial concerns inducing lack of trust. Explainable Artificial Intelligence\n(XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools,\ntechniques, and algorithms that can generate high-quality interpretable,\nintuitive, human-understandable explanations of AI decisions. In addition to\nproviding a holistic view of the current XAI landscape in deep learning, this\npaper provides mathematical summaries of seminal work. We start by proposing a\ntaxonomy and categorizing the XAI techniques based on their scope of\nexplanations, methodology behind the algorithms, and explanation level or usage\nwhich helps build trustworthy, interpretable, and self-explanatory deep\nlearning models. We then describe the main principles used in XAI research and\npresent the historical timeline for landmark studies in XAI from 2007 to 2020.\nAfter explaining each category of algorithms and approaches in detail, we then\nevaluate the explanation maps generated by eight XAI algorithms on image data,\ndiscuss the limitations of this approach, and provide potential future\ndirections to improve XAI evaluation.\n",
        "published": "2020",
        "authors": [
            "Arun Das",
            "Paul Rad"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.06310v1",
        "title": "Artificial Intelligence for COVID-19 Detection -- A state-of-the-art\n  review",
        "abstract": "  The emergence of COVID-19 has necessitated many efforts by the scientific\ncommunity for its proper management. An urgent clinical reaction is required in\nthe face of the unending devastation being caused by the pandemic. These\nefforts include technological innovations for improvement in screening,\ntreatment, vaccine development, contact tracing and, survival prediction. The\nuse of Deep Learning (DL) and Artificial Intelligence (AI) can be sought in all\nof the above-mentioned spheres. This paper aims to review the role of Deep\nLearning and Artificial intelligence in various aspects of the overall COVID-19\nmanagement and particularly for COVID-19 detection and classification. The DL\nmodels are developed to analyze clinical modalities like CT scans and X-Ray\nimages of patients and predict their pathological condition. A DL model aims to\ndetect the COVID-19 pneumonia, classify and distinguish between COVID-19,\nCommunity-Acquired Pneumonia (CAP), Viral and Bacterial pneumonia, and normal\nconditions. Furthermore, sophisticated models can be built to segment the\naffected area in the lungs and quantify the infection volume for a better\nunderstanding of the extent of damage. Many models have been developed either\nindependently or with the help of pre-trained models like VGG19, ResNet50, and\nAlexNet leveraging the concept of transfer learning. Apart from model\ndevelopment, data preprocessing and augmentation are also performed to cope\nwith the challenge of insufficient data samples often encountered in medical\napplications. It can be evaluated that DL and AI can be effectively implemented\nto withstand the challenges posed by the global emergency\n",
        "published": "2020",
        "authors": [
            "Parsa Sarosh",
            "Shabir A. Parah",
            "Romany F Mansur",
            "G. M. Bhat"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.06125v2",
        "title": "Artificial Intelligence Advances for De Novo Molecular Structure\n  Modeling in Cryo-EM",
        "abstract": "  Cryo-electron microscopy (cryo-EM) has become a major experimental technique\nto determine the structures of large protein complexes and molecular\nassemblies, as evidenced by the 2017 Nobel Prize. Although cryo-EM has been\ndrastically improved to generate high-resolution three-dimensional (3D) maps\nthat contain detailed structural information about macromolecules, the\ncomputational methods for using the data to automatically build structure\nmodels are lagging far behind. The traditional cryo-EM model building approach\nis template-based homology modeling. Manual de novo modeling is very\ntime-consuming when no template model is found in the database. In recent\nyears, de novo cryo-EM modeling using machine learning (ML) and deep learning\n(DL) has ranked among the top-performing methods in macromolecular structure\nmodeling. Deep-learning-based de novo cryo-EM modeling is an important\napplication of artificial intelligence, with impressive results and great\npotential for the next generation of molecular biomedicine. Accordingly, we\nsystematically review the representative ML/DL-based de novo cryo-EM modeling\nmethods. And their significances are discussed from both practical and\nmethodological viewpoints. We also briefly describe the background of cryo-EM\ndata processing workflow. Overall, this review provides an introductory guide\nto modern research on artificial intelligence (AI) for de novo molecular\nstructure modeling and future directions in this emerging field.\n",
        "published": "2021",
        "authors": [
            "Dong Si",
            "Andrew Nakamura",
            "Runbang Tang",
            "Haowen Guan",
            "Jie Hou",
            "Ammaar Firozi",
            "Renzhi Cao",
            "Kyle Hippe",
            "Minglei Zhao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.07374v5",
        "title": "A Survey on Explainable Artificial Intelligence (XAI): Towards Medical\n  XAI",
        "abstract": "  Recently, artificial intelligence and machine learning in general have\ndemonstrated remarkable performances in many tasks, from image processing to\nnatural language processing, especially with the advent of deep learning. Along\nwith research progress, they have encroached upon many different fields and\ndisciplines. Some of them require high level of accountability and thus\ntransparency, for example the medical sector. Explanations for machine\ndecisions and predictions are thus needed to justify their reliability. This\nrequires greater interpretability, which often means we need to understand the\nmechanism underlying the algorithms. Unfortunately, the blackbox nature of the\ndeep learning is still unresolved, and many machine decisions are still poorly\nunderstood. We provide a review on interpretabilities suggested by different\nresearch works and categorize them. The different categories show different\ndimensions in interpretability research, from approaches that provide\n\"obviously\" interpretable information to the studies of complex patterns. By\napplying the same categorization to interpretability in medical research, it is\nhoped that (1) clinicians and practitioners can subsequently approach these\nmethods with caution, (2) insights into interpretability will be born with more\nconsiderations for medical practices, and (3) initiatives to push forward\ndata-based, mathematically- and technically-grounded medical education are\nencouraged.\n",
        "published": "2019",
        "authors": [
            "Erico Tjoa",
            "Cuntai Guan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.02098v2",
        "title": "Explainable Artificial Intelligence for Process Mining: A General\n  Overview and Application of a Novel Local Explanation Approach for Predictive\n  Process Monitoring",
        "abstract": "  The contemporary process-aware information systems possess the capabilities\nto record the activities generated during the process execution. To leverage\nthese process specific fine-granular data, process mining has recently emerged\nas a promising research discipline. As an important branch of process mining,\npredictive business process management, pursues the objective to generate\nforward-looking, predictive insights to shape business processes. In this\nstudy, we propose a conceptual framework sought to establish and promote\nunderstanding of decision-making environment, underlying business processes and\nnature of the user characteristics for developing explainable business process\nprediction solutions. Consequently, with regard to the theoretical and\npractical implications of the framework, this study proposes a novel local\npost-hoc explanation approach for a deep learning classifier that is expected\nto facilitate the domain experts in justifying the model decisions. In contrary\nto alternative popular perturbation-based local explanation approaches, this\nstudy defines the local regions from the validation dataset by using the\nintermediate latent space representations learned by the deep neural networks.\nTo validate the applicability of the proposed explanation method, the real-life\nprocess log data delivered by the Volvo IT Belgium's incident management system\nare used.The adopted deep learning classifier achieves a good performance with\nthe Area Under the ROC Curve of 0.94. The generated local explanations are also\nvisualized and presented with relevant evaluation measures that are expected to\nincrease the users' trust in the black-box-model.\n",
        "published": "2020",
        "authors": [
            "Nijat Mehdiyev",
            "Peter Fettke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.03289v1",
        "title": "Future Artificial Intelligence tools and perspectives in medicine",
        "abstract": "  Purpose of review: Artificial intelligence (AI) has become popular in medical\napplications, specifically as a clinical support tool for computer-aided\ndiagnosis. These tools are typically employed on medical data (i.e., image,\nmolecular data, clinical variables, etc.) and used the statistical and machine\nlearning methods to measure the model performance. In this review, we\nsummarized and discussed the most recent radiomic pipeline used for clinical\nanalysis. Recent findings:Currently, limited management of cancers benefits\nfrom artificial intelligence, mostly related to a computer-aided diagnosis that\navoids a biopsy analysis that presents additional risks and costs. Most AI\ntools are based on imaging features, known as radiomic analysis that can be\nrefined into predictive models in non-invasively acquired imaging data. This\nreview explores the progress of AI-based radiomic tools for clinical\napplications with a brief description of necessary technical steps. Explaining\nnew radiomic approaches based on deep learning techniques will explain how the\nnew radiomic models (deep radiomic analysis) can benefit from deep\nconvolutional neural networks and be applied on limited data sets. Summary: To\nconsider the radiomic algorithms, further investigations are recommended to\ninvolve deep learning in radiomic models with additional validation steps on\nvarious cancer types.\n",
        "published": "2022",
        "authors": [
            "Ahmad Chaddad",
            "Yousef Katib",
            "Lama Hassan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.15424v1",
        "title": "Accurate Long-term Air Temperature Prediction with a Fusion of\n  Artificial Intelligence and Data Reduction Techniques",
        "abstract": "  In this paper three customised Artificial Intelligence (AI) frameworks,\nconsidering Deep Learning (convolutional neural networks), Machine Learning\nalgorithms and data reduction techniques are proposed, for a problem of\nlong-term summer air temperature prediction. Specifically, the prediction of\naverage air temperature in the first and second August fortnights, using input\ndata from previous months, at two different locations, Paris (France) and\nC\\'ordoba (Spain), is considered. The target variable, mainly in the first\nAugust fortnight, can contain signals of extreme events such as heatwaves, like\nthe mega-heatwave of 2003, which affected France and the Iberian Peninsula.\nThus, an accurate prediction of long-term air temperature may be valuable also\nfor different problems related to climate change, such as attribution of\nextreme events, and in other problems related to renewable energy. The analysis\ncarried out this work is based on Reanalysis data, which are first processed by\na correlation analysis among different prediction variables and the target\n(average air temperature in August first and second fortnights). An area with\nthe largest correlation is located, and the variables within, after a feature\nselection process, are the input of different deep learning and ML algorithms.\nThe experiments carried out show a very good prediction skill in the three\nproposed AI frameworks, both in Paris and C\\'ordoba regions.\n",
        "published": "2022",
        "authors": [
            "Du\u0161an Fister",
            "Jorge P\u00e9rez-Aracil",
            "C\u00e9sar Pel\u00e1ez-Rodr\u00edguez",
            "Javier Del Ser",
            "Sancho Salcedo-Sanz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.13427v1",
        "title": "Using Deep Learning and Explainable Artificial Intelligence in Patients'\n  Choices of Hospital Levels",
        "abstract": "  In countries that enabled patients to choose their own providers, a common\nproblem is that the patients did not make rational decisions, and hence, fail\nto use healthcare resources efficiently. This might cause problems such as\noverwhelming tertiary facilities with mild condition patients, thus limiting\ntheir capacity of treating acute and critical patients. To address such\nmaldistributed patient volume, it is essential to oversee patients choices\nbefore further evaluation of a policy or resource allocation. This study used\nnationwide insurance data, accumulated possible features discussed in existing\nliterature, and used a deep neural network to predict the patients choices of\nhospital levels. This study also used explainable artificial intelligence\nmethods to interpret the contribution of features for the general public and\nindividuals. In addition, we explored the effectiveness of changing data\nrepresentations. The results showed that the model was able to predict with\nhigh area under the receiver operating characteristics curve (AUC) (0.90),\naccuracy (0.90), sensitivity (0.94), and specificity (0.97) with highly\nimbalanced label. Generally, social approval of the provider by the general\npublic (positive or negative) and the number of practicing physicians serving\nper ten thousand people of the located area are listed as the top effecting\nfeatures. The changing data representation had a positive effect on the\nprediction improvement. Deep learning methods can process highly imbalanced\ndata and achieve high accuracy. The effecting features affect the general\npublic and individuals differently. Addressing the sparsity and discrete nature\nof insurance data leads to better prediction. Applications using deep learning\ntechnology are promising in health policy making. More work is required to\ninterpret models and practice implementation.\n",
        "published": "2020",
        "authors": [
            "Lichin Chen",
            "Yu Tsao",
            "Ji-Tian Sheu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.16910v1",
        "title": "Trusted Artificial Intelligence: Towards Certification of Machine\n  Learning Applications",
        "abstract": "  Artificial Intelligence is one of the fastest growing technologies of the\n21st century and accompanies us in our daily lives when interacting with\ntechnical applications. However, reliance on such technical systems is crucial\nfor their widespread applicability and acceptance. The societal tools to\nexpress reliance are usually formalized by lawful regulations, i.e., standards,\nnorms, accreditations, and certificates. Therefore, the T\\\"UV AUSTRIA Group in\ncooperation with the Institute for Machine Learning at the Johannes Kepler\nUniversity Linz, proposes a certification process and an audit catalog for\nMachine Learning applications. We are convinced that our approach can serve as\nthe foundation for the certification of applications that use Machine Learning\nand Deep Learning, the techniques that drive the current revolution in\nArtificial Intelligence. While certain high-risk areas, such as fully\nautonomous robots in workspaces shared with humans, are still some time away\nfrom certification, we aim to cover low-risk applications with our\ncertification procedure. Our holistic approach attempts to analyze Machine\nLearning applications from multiple perspectives to evaluate and verify the\naspects of secure software development, functional requirements, data quality,\ndata protection, and ethics. Inspired by existing work, we introduce four\ncriticality levels to map the criticality of a Machine Learning application\nregarding the impact of its decisions on people, environment, and\norganizations. Currently, the audit catalog can be applied to low-risk\napplications within the scope of supervised learning as commonly encountered in\nindustry. Guided by field experience, scientific developments, and market\ndemands, the audit catalog will be extended and modified accordingly.\n",
        "published": "2021",
        "authors": [
            "Philip Matthias Winter",
            "Sebastian Eder",
            "Johannes Weissenb\u00f6ck",
            "Christoph Schwald",
            "Thomas Doms",
            "Tom Vogt",
            "Sepp Hochreiter",
            "Bernhard Nessler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.3489v2",
        "title": "Quantum Deep Learning",
        "abstract": "  In recent years, deep learning has had a profound impact on machine learning\nand artificial intelligence. At the same time, algorithms for quantum computers\nhave been shown to efficiently solve some problems that are intractable on\nconventional, classical computers. We show that quantum computing not only\nreduces the time required to train a deep restricted Boltzmann machine, but\nalso provides a richer and more comprehensive framework for deep learning than\nclassical computing and leads to significant improvements in the optimization\nof the underlying objective function. Our quantum methods also permit efficient\ntraining of full Boltzmann machines and multi-layer, fully connected models and\ndo not have well known classical counterparts.\n",
        "published": "2014",
        "authors": [
            "Nathan Wiebe",
            "Ashish Kapoor",
            "Krysta M. Svore"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.03543v1",
        "title": "DLPaper2Code: Auto-generation of Code from Deep Learning Research Papers",
        "abstract": "  With an abundance of research papers in deep learning, reproducibility or\nadoption of the existing works becomes a challenge. This is due to the lack of\nopen source implementations provided by the authors. Further, re-implementing\nresearch papers in a different library is a daunting task. To address these\nchallenges, we propose a novel extensible approach, DLPaper2Code, to extract\nand understand deep learning design flow diagrams and tables available in a\nresearch paper and convert them to an abstract computational graph. The\nextracted computational graph is then converted into execution ready source\ncode in both Keras and Caffe, in real-time. An arXiv-like website is created\nwhere the automatically generated designs is made publicly available for 5,000\nresearch papers. The generated designs could be rated and edited using an\nintuitive drag-and-drop UI framework in a crowdsourced manner. To evaluate our\napproach, we create a simulated dataset with over 216,000 valid design\nvisualizations using a manually defined grammar. Experiments on the simulated\ndataset show that the proposed framework provide more than $93\\%$ accuracy in\nflow diagram content extraction.\n",
        "published": "2017",
        "authors": [
            "Akshay Sethi",
            "Anush Sankaran",
            "Naveen Panwar",
            "Shreya Khare",
            "Senthil Mani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.01258v1",
        "title": "Deep Learning Reconstruction for 9-View Dual Energy CT Baggage Scanner",
        "abstract": "  For homeland and transportation security applications, 2D X-ray explosive\ndetection system (EDS) have been widely used, but they have limitations in\nrecognizing 3D shape of the hidden objects. Among various types of 3D computed\ntomography (CT) systems to address this issue, this paper is interested in a\nstationary CT using fixed X-ray sources and detectors. However, due to the\nlimited number of projection views, analytic reconstruction algorithms produce\nsevere streaking artifacts. Inspired by recent success of deep learning\napproach for sparse view CT reconstruction, here we propose a novel image and\nsinogram domain deep learning architecture for 3D reconstruction from very\nsparse view measurement. The algorithm has been tested with the real data from\na prototype 9-view dual energy stationary CT EDS carry-on baggage scanner\ndeveloped by GEMSS Medical Systems, Korea, which confirms the superior\nreconstruction performance over the existing approaches.\n",
        "published": "2018",
        "authors": [
            "Yoseob Han",
            "Jingu Kang",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.04626v1",
        "title": "Barista - a Graphical Tool for Designing and Training Deep Neural\n  Networks",
        "abstract": "  In recent years, the importance of deep learning has significantly increased\nin pattern recognition, computer vision, and artificial intelligence research,\nas well as in industry. However, despite the existence of multiple deep\nlearning frameworks, there is a lack of comprehensible and easy-to-use\nhigh-level tools for the design, training, and testing of deep neural networks\n(DNNs). In this paper, we introduce Barista, an open-source graphical\nhigh-level interface for the Caffe deep learning framework. While Caffe is one\nof the most popular frameworks for training DNNs, editing prototext files in\norder to specify the net architecture and hyper parameters can become a\ncumbersome and error-prone task. Instead, Barista offers a fully graphical user\ninterface with a graph-based net topology editor and provides an end-to-end\ntraining facility for DNNs, which allows researchers to focus on solving their\nproblems without having to write code, edit text files, or manually parse\nlogged data.\n",
        "published": "2018",
        "authors": [
            "Soeren Klemm",
            "Aaron Scherzinger",
            "Dominik Drees",
            "Xiaoyi Jiang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.06111v1",
        "title": "Vulnerability of Deep Learning",
        "abstract": "  The Renormalisation Group (RG) provides a framework in which it is possible\nto assess whether a deep-learning network is sensitive to small changes in the\ninput data and hence prone to error, or susceptible to adversarial attack.\nDistinct classification outputs are associated with different RG fixed points\nand sensitivity to small changes in the input data is due to the presence of\nrelevant operators at a fixed point. A numerical scheme, based on Monte Carlo\nRG ideas, is proposed for identifying the existence of relevant operators and\nthe corresponding directions of greatest sensitivity in the input data. Thus, a\ntrained deep-learning network may be tested for its robustness and, if it is\nvulnerable to attack, dangerous perturbations of the input data identified.\n",
        "published": "2018",
        "authors": [
            "Richard Kenway"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.12535v1",
        "title": "The Relevance of Bayesian Layer Positioning to Model Uncertainty in Deep\n  Bayesian Active Learning",
        "abstract": "  One of the main challenges of deep learning tools is their inability to\ncapture model uncertainty. While Bayesian deep learning can be used to tackle\nthe problem, Bayesian neural networks often require more time and computational\npower to train than deterministic networks. Our work explores whether fully\nBayesian networks are needed to successfully capture model uncertainty. We vary\nthe number and position of Bayesian layers in a network and compare their\nperformance on active learning with the MNIST dataset. We found that we can\nfully capture the model uncertainty by using only a few Bayesian layers near\nthe output of the network, combining the advantages of deterministic and\nBayesian networks.\n",
        "published": "2018",
        "authors": [
            "Jiaming Zeng",
            "Adam Lesnikowski",
            "Jose M. Alvarez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.02476v2",
        "title": "A Simple Baseline for Bayesian Uncertainty in Deep Learning",
        "abstract": "  We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose\napproach for uncertainty representation and calibration in deep learning.\nStochastic Weight Averaging (SWA), which computes the first moment of\nstochastic gradient descent (SGD) iterates with a modified learning rate\nschedule, has recently been shown to improve generalization in deep learning.\nWith SWAG, we fit a Gaussian using the SWA solution as the first moment and a\nlow rank plus diagonal covariance also derived from the SGD iterates, forming\nan approximate posterior distribution over neural network weights; we then\nsample from this Gaussian distribution to perform Bayesian model averaging. We\nempirically find that SWAG approximates the shape of the true posterior, in\naccordance with results describing the stationary distribution of SGD iterates.\nMoreover, we demonstrate that SWAG performs well on a wide variety of tasks,\nincluding out of sample detection, calibration, and transfer learning, in\ncomparison to many popular alternatives including MC dropout, KFAC Laplace,\nSGLD, and temperature scaling.\n",
        "published": "2019",
        "authors": [
            "Wesley Maddox",
            "Timur Garipov",
            "Pavel Izmailov",
            "Dmitry Vetrov",
            "Andrew Gordon Wilson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.04155v5",
        "title": "GP-VAE: Deep Probabilistic Time Series Imputation",
        "abstract": "  Multivariate time series with missing values are common in areas such as\nhealthcare and finance, and have grown in number and complexity over the years.\nThis raises the question whether deep learning methodologies can outperform\nclassical data imputation methods in this domain. However, naive applications\nof deep learning fall short in giving reliable confidence estimates and lack\ninterpretability. We propose a new deep sequential latent variable model for\ndimensionality reduction and data imputation. Our modeling assumption is simple\nand interpretable: the high dimensional time series has a lower-dimensional\nrepresentation which evolves smoothly in time according to a Gaussian process.\nThe non-linear dimensionality reduction in the presence of missing data is\nachieved using a VAE approach with a novel structured variational\napproximation. We demonstrate that our approach outperforms several classical\nand deep learning-based data imputation methods on high-dimensional data from\nthe domains of computer vision and healthcare, while additionally improving the\nsmoothness of the imputations and providing interpretable uncertainty\nestimates.\n",
        "published": "2019",
        "authors": [
            "Vincent Fortuin",
            "Dmitry Baranchuk",
            "Gunnar R\u00e4tsch",
            "Stephan Mandt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.09750v2",
        "title": "Adaptive Regularization via Residual Smoothing in Deep Learning\n  Optimization",
        "abstract": "  We present an adaptive regularization algorithm that can be effectively\napplied to the optimization problem in deep learning framework. Our\nregularization algorithm aims to take into account the fitness of data to the\ncurrent state of model in the determination of regularity to achieve better\ngeneralization. The degree of regularization at each element in the target\nspace of the neural network architecture is determined based on the residual at\neach optimization iteration in an adaptive way. Our adaptive regularization\nalgorithm is designed to apply a diffusion process driven by the heat equation\nwith spatially varying diffusivity depending on the probability density\nfunction following a certain distribution of residual. Our data-driven\nregularity is imposed by adaptively smoothing a simplified objective function\nin which the explicit regularization term is omitted in an alternating manner\nbetween the evaluation of residual and the determination of the degree of its\nregularity. The effectiveness of our algorithm is empirically demonstrated by\nthe numerical experiments in the application of image classification problems,\nindicating that our algorithm outperforms other commonly used optimization\nalgorithms in terms of generalization using popular deep learning models and\nbenchmark datasets.\n",
        "published": "2019",
        "authors": [
            "Junghee Cho",
            "Junseok Kwon",
            "Byung-Woo Hong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1712.10248v2",
        "title": "Deep Learning Interior Tomography for Region-of-Interest Reconstruction",
        "abstract": "  Interior tomography for the region-of-interest (ROI) imaging has advantages\nof using a small detector and reducing X-ray radiation dose. However, standard\nanalytic reconstruction suffers from severe cupping artifacts due to existence\nof null space in the truncated Radon transform. Existing penalized\nreconstruction methods may address this problem but they require extensive\ncomputations due to the iterative reconstruction. Inspired by the recent deep\nlearning approaches to low-dose and sparse view CT, here we propose a deep\nlearning architecture that removes null space signals from the FBP\nreconstruction. Experimental results have shown that the proposed method\nprovides near-perfect reconstruction with about 7-10 dB improvement in PSNR\nover existing methods in spite of significantly reduced run-time complexity.\n",
        "published": "2017",
        "authors": [
            "Yoseob Han",
            "Jawook Gu",
            "Jong Chul Ye"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.12577v2",
        "title": "Learning Relational Representations with Auto-encoding Logic Programs",
        "abstract": "  Deep learning methods capable of handling relational data have proliferated\nover the last years. In contrast to traditional relational learning methods\nthat leverage first-order logic for representing such data, these deep learning\nmethods aim at re-representing symbolic relational data in Euclidean spaces.\nThey offer better scalability, but can only numerically approximate relational\nstructures and are less flexible in terms of reasoning tasks supported. This\npaper introduces a novel framework for relational representation learning that\ncombines the best of both worlds. This framework, inspired by the auto-encoding\nprinciple, uses first-order logic as a data representation language, and the\nmapping between the original and latent representation is done by means of\nlogic programs instead of neural networks. We show how learning can be cast as\na constraint optimisation problem for which existing solvers can be used. The\nuse of logic as a representation language makes the proposed framework more\naccurate (as the representation is exact, rather than approximate), more\nflexible, and more interpretable than deep learning methods. We experimentally\nshow that these latent representations are indeed beneficial in relational\nlearning tasks.\n",
        "published": "2019",
        "authors": [
            "Sebastijan Dumancic",
            "Tias Guns",
            "Wannes Meert",
            "Hendrik Blockeel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.02894v2",
        "title": "Secure Sum Outperforms Homomorphic Encryption in (Current) Collaborative\n  Deep Learning",
        "abstract": "  Deep learning (DL) approaches are achieving extraordinary results in a wide\nrange of domains, but often require a massive collection of private data.\nHence, methods for training neural networks on the joint data of different data\nowners, that keep each party's input confidential, are called for. We address a\nspecific setting in federated learning, namely that of deep learning from\nhorizontally distributed data with a limited number of parties, where their\nvulnerable intermediate results have to be processed in a privacy-preserving\nmanner. This setting can be found in medical and healthcare as well as\nindustrial applications. The predominant scheme for this is based on\nhomomorphic encryption (HE), and it is widely considered to be without\nalternative. In contrast to this, we demonstrate that a carefully chosen, less\ncomplex and computationally less expensive secure sum protocol in conjunction\nwith default secure channels exhibits superior properties in terms of both\ncollusion-resistance and runtime. Finally, we discuss several open research\nquestions in the context of collaborative DL, especially regarding privacy\nrisks caused by joint intermediate results.\n",
        "published": "2020",
        "authors": [
            "Derian Boer",
            "Stefan Kramer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.07187v2",
        "title": "HMIC: Hierarchical Medical Image Classification, A Deep Learning\n  Approach",
        "abstract": "  Image classification is central to the big data revolution in medicine.\nImproved information processing methods for diagnosis and classification of\ndigital medical images have shown to be successful via deep learning\napproaches. As this field is explored, there are limitations to the performance\nof traditional supervised classifiers. This paper outlines an approach that is\ndifferent from the current medical image classification tasks that view the\nissue as multi-class classification. We performed a hierarchical classification\nusing our Hierarchical Medical Image classification (HMIC) approach. HMIC uses\nstacks of deep learning models to give particular comprehension at each level\nof the clinical picture hierarchy. For testing our performance, we use biopsy\nof the small bowel images that contain three categories in the parent level\n(Celiac Disease, Environmental Enteropathy, and histologically normal\ncontrols). For the child level, Celiac Disease Severity is classified into 4\nclasses (I, IIIa, IIIb, and IIIC).\n",
        "published": "2020",
        "authors": [
            "Kamran Kowsari",
            "Rasoul Sali",
            "Lubaina Ehsan",
            "William Adorno",
            "Asad Ali",
            "Sean Moore",
            "Beatrice Amadi",
            "Paul Kelly",
            "Sana Syed",
            "Donald Brown"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.14166v1",
        "title": "A Comparison of Optimization Algorithms for Deep Learning",
        "abstract": "  In recent years, we have witnessed the rise of deep learning. Deep neural\nnetworks have proved their success in many areas. However, the optimization of\nthese networks has become more difficult as neural networks going deeper and\ndatasets becoming bigger. Therefore, more advanced optimization algorithms have\nbeen proposed over the past years. In this study, widely used optimization\nalgorithms for deep learning are examined in detail. To this end, these\nalgorithms called adaptive gradient methods are implemented for both supervised\nand unsupervised tasks. The behaviour of the algorithms during training and\nresults on four image datasets, namely, MNIST, CIFAR-10, Kaggle Flowers and\nLabeled Faces in the Wild are compared by pointing out their differences\nagainst basic optimization algorithms.\n",
        "published": "2020",
        "authors": [
            "Derya Soydaner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.00824v3",
        "title": "State-of-the-art Techniques in Deep Edge Intelligence",
        "abstract": "  The potential held by the gargantuan volumes of data being generated across\nnetworks worldwide has been truly unlocked by machine learning techniques and\nmore recently Deep Learning. The advantages offered by the latter have seen it\nrapidly becoming a framework of choice for various applications. However, the\ncentralization of computational resources and the need for data aggregation\nhave long been limiting factors in the democratization of Deep Learning\napplications. Edge Computing is an emerging paradigm that aims to utilize the\nhitherto untapped processing resources available at the network periphery. Edge\nIntelligence (EI) has quickly emerged as a powerful alternative to enable\nlearning using the concepts of Edge Computing. Deep Learning-based Edge\nIntelligence or Deep Edge Intelligence (DEI) lies in this rapidly evolving\ndomain. In this article, we provide an overview of the major constraints in\noperationalizing DEI. The major research avenues in DEI have been consolidated\nunder Federated Learning, Distributed Computation, Compression Schemes and\nConditional Computation. We also present some of the prevalent challenges and\nhighlight prospective research avenues.\n",
        "published": "2020",
        "authors": [
            "Ahnaf Hannan Lodhi",
            "Bar\u0131\u015f Akg\u00fcn",
            "\u00d6znur \u00d6zkasap"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.09000v1",
        "title": "Generative chemistry: drug discovery with deep learning generative\n  models",
        "abstract": "  The de novo design of molecular structures using deep learning generative\nmodels introduces an encouraging solution to drug discovery in the face of the\ncontinuously increased cost of new drug development. From the generation of\noriginal texts, images, and videos, to the scratching of novel molecular\nstructures, the incredible creativity of deep learning generative models\nsurprised us about the height machine intelligence can achieve. The purpose of\nthis paper is to review the latest advances in generative chemistry which\nrelies on generative modeling to expedite the drug discovery process. This\nreview starts with a brief history of artificial intelligence in drug discovery\nto outline this emerging paradigm. Commonly used chemical databases, molecular\nrepresentations, and tools in cheminformatics and machine learning are covered\nas the infrastructure for the generative chemistry. The detailed discussions on\nutilizing cutting-edge generative architectures, including recurrent neural\nnetwork, variational autoencoder, adversarial autoencoder, and generative\nadversarial network for compound generation are focused. Challenges and future\nperspectives follow.\n",
        "published": "2020",
        "authors": [
            "Yuemin Bian",
            "Xiang-Qun Xie"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.02075v1",
        "title": "Learned Hardware/Software Co-Design of Neural Accelerators",
        "abstract": "  The use of deep learning has grown at an exponential rate, giving rise to\nnumerous specialized hardware and software systems for deep learning. Because\nthe design space of deep learning software stacks and hardware accelerators is\ndiverse and vast, prior work considers software optimizations separately from\nhardware architectures, effectively reducing the search space. Unfortunately,\nthis bifurcated approach means that many profitable design points are never\nexplored. This paper instead casts the problem as hardware/software co-design,\nwith the goal of automatically identifying desirable points in the joint design\nspace. The key to our solution is a new constrained Bayesian optimization\nframework that avoids invalid solutions by exploiting the highly constrained\nfeatures of this design space, which are semi-continuous/semi-discrete. We\nevaluate our optimization framework by applying it to a variety of neural\nmodels, improving the energy-delay product by 18% (ResNet) and 40% (DQN) over\nhand-tuned state-of-the-art systems, as well as demonstrating strong results on\nother neural network architectures, such as MLPs and Transformers.\n",
        "published": "2020",
        "authors": [
            "Zhan Shi",
            "Chirag Sakhuja",
            "Milad Hashemi",
            "Kevin Swersky",
            "Calvin Lin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.07445v3",
        "title": "Deep Learning Models for Predicting Wildfires from Historical\n  Remote-Sensing Data",
        "abstract": "  Identifying regions that have high likelihood for wildfires is a key\ncomponent of land and forestry management and disaster preparedness. We create\na data set by aggregating nearly a decade of remote-sensing data and historical\nfire records to predict wildfires. This prediction problem is framed as three\nmachine learning tasks. Results are compared and analyzed for four different\ndeep learning models to estimate wildfire likelihood. The results demonstrate\nthat deep learning models can successfully identify areas of high fire\nlikelihood using aggregated data about vegetation, weather, and topography with\nan AUC of 83%.\n",
        "published": "2020",
        "authors": [
            "Fantine Huot",
            "R. Lily Hu",
            "Matthias Ihme",
            "Qing Wang",
            "John Burge",
            "Tianjian Lu",
            "Jason Hickey",
            "Yi-Fan Chen",
            "John Anderson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.07321v1",
        "title": "Classification of Pedagogical content using conventional machine\n  learning and deep learning model",
        "abstract": "  The advent of the Internet and a large number of digital technologies has\nbrought with it many different challenges. A large amount of data is found on\nthe web, which in most cases is unstructured and unorganized, and this\ncontributes to the fact that the use and manipulation of this data is quite a\ndifficult process. Due to this fact, the usage of different machine and deep\nlearning techniques for Text Classification has gained its importance, which\nimproved this discipline and made it more interesting for scientists and\nresearchers for further study. This paper aims to classify the pedagogical\ncontent using two different models, the K-Nearest Neighbor (KNN) from the\nconventional models and the Long short-term memory (LSTM) recurrent neural\nnetwork from the deep learning models. The result indicates that the accuracy\nof classifying the pedagogical content reaches 92.52 % using KNN model and\n87.71 % using LSTM model.\n",
        "published": "2021",
        "authors": [
            "Vedat Apuk",
            "Krenare Pireva Nu\u00e7i"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.13336v5",
        "title": "TENSILE: A Tensor granularity dynamic GPU memory scheduling method\n  toward multiple dynamic workloads system",
        "abstract": "  Recently, deep learning has been an area of intense research. However, as a\nkind of computing-intensive task, deep learning highly relies on the scale of\nGPU memory, which is usually prohibitive and scarce. Although some extensive\nworks have been proposed for dynamic GPU memory management, they are hard to\napply to systems with multiple dynamic workloads, such as in-database machine\nlearning systems.\n  In this paper, we demonstrated TENSILE, a method of managing GPU memory in\ntensor granularity to reduce the GPU memory peak, considering the multiple\ndynamic workloads. TENSILE tackled the cold-starting and across-iteration\nscheduling problem existing in previous works. We implemented TENSILE on a deep\nlearning framework built by ourselves and evaluated its performance. The\nexperiment results show that TENSILE can save more GPU memory with less extra\noverhead than prior works in single and multiple dynamic workloads scenarios.\n",
        "published": "2021",
        "authors": [
            "Kaixin Zhang",
            "Hongzhi Wang",
            "Han Hu",
            "Songling Zou",
            "Jiye Qiu",
            "Tongxin Li",
            "Zhishun Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1607.00136v1",
        "title": "Missing Data Estimation in High-Dimensional Datasets: A Swarm\n  Intelligence-Deep Neural Network Approach",
        "abstract": "  In this paper, we examine the problem of missing data in high-dimensional\ndatasets by taking into consideration the Missing Completely at Random and\nMissing at Random mechanisms, as well as theArbitrary missing pattern.\nAdditionally, this paper employs a methodology based on Deep Learning and Swarm\nIntelligence algorithms in order to provide reliable estimates for missing\ndata. The deep learning technique is used to extract features from the input\ndata via an unsupervised learning approach by modeling the data distribution\nbased on the input. This deep learning technique is then used as part of the\nobjective function for the swarm intelligence technique in order to estimate\nthe missing data after a supervised fine-tuning phase by minimizing an error\nfunction based on the interrelationship and correlation between features in the\ndataset. The investigated methodology in this paper therefore has longer\nrunning times, however, the promising potential outcomes justify the trade-off.\nAlso, basic knowledge of statistics is presumed.\n",
        "published": "2016",
        "authors": [
            "Collins Leke",
            "Tshilidzi Marwala"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1607.03611v2",
        "title": "Characterizing Driving Styles with Deep Learning",
        "abstract": "  Characterizing driving styles of human drivers using vehicle sensor data,\ne.g., GPS, is an interesting research problem and an important real-world\nrequirement from automotive industries. A good representation of driving\nfeatures can be highly valuable for autonomous driving, auto insurance, and\nmany other application scenarios. However, traditional methods mainly rely on\nhandcrafted features, which limit machine learning algorithms to achieve a\nbetter performance. In this paper, we propose a novel deep learning solution to\nthis problem, which could be the first attempt of extending deep learning to\ndriving behavior analysis based on GPS data. The proposed approach can\neffectively extract high level and interpretable features describing complex\ndriving patterns. It also requires significantly less human experience and\nwork. The power of the learned driving style representations are validated\nthrough the driver identification problem using a large real dataset.\n",
        "published": "2016",
        "authors": [
            "Weishan Dong",
            "Jian Li",
            "Renjie Yao",
            "Changsheng Li",
            "Ting Yuan",
            "Lanjun Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.11738v1",
        "title": "Deep-IRT: Make Deep Learning Based Knowledge Tracing Explainable Using\n  Item Response Theory",
        "abstract": "  Deep learning based knowledge tracing model has been shown to outperform\ntraditional knowledge tracing model without the need for human-engineered\nfeatures, yet its parameters and representations have long been criticized for\nnot being explainable. In this paper, we propose Deep-IRT which is a synthesis\nof the item response theory (IRT) model and a knowledge tracing model that is\nbased on the deep neural network architecture called dynamic key-value memory\nnetwork (DKVMN) to make deep learning based knowledge tracing explainable.\nSpecifically, we use the DKVMN model to process the student's learning\ntrajectory and estimate the student ability level and the item difficulty level\nover time. Then, we use the IRT model to estimate the probability that a\nstudent will answer an item correctly using the estimated student ability and\nthe item difficulty. Experiments show that the Deep-IRT model retains the\nperformance of the DKVMN model, while it provides a direct psychological\ninterpretation of both students and items.\n",
        "published": "2019",
        "authors": [
            "Chun-Kit Yeung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.12383v1",
        "title": "Enhancing Prediction Models for One-Year Mortality in Patients with\n  Acute Myocardial Infarction and Post Myocardial Infarction Syndrome",
        "abstract": "  Predicting the risk of mortality for patients with acute myocardial\ninfarction (AMI) using electronic health records (EHRs) data can help identify\nrisky patients who might need more tailored care. In our previous work, we\nbuilt computational models to predict one-year mortality of patients admitted\nto an intensive care unit (ICU) with AMI or post myocardial infarction\nsyndrome. Our prior work only used the structured clinical data from MIMIC-III,\na publicly available ICU clinical database. In this study, we enhanced our work\nby adding the word embedding features from free-text discharge summaries. Using\na richer set of features resulted in significant improvement in the performance\nof our deep learning models. The average accuracy of our deep learning models\nwas 92.89% and the average F-measure was 0.928. We further reported the impact\nof different combinations of features extracted from structured and/or\nunstructured data on the performance of the deep learning models.\n",
        "published": "2019",
        "authors": [
            "Seyedeh Neelufar Payrovnaziri",
            "Laura A. Barrett",
            "Daniel Bis",
            "Jiang Bian",
            "Zhe He"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.09509v1",
        "title": "Leveraging Uncertainty in Deep Learning for Selective Classification",
        "abstract": "  The wide and rapid adoption of deep learning by practitioners brought\nunintended consequences in many situations such as in the infamous case of\nGoogle Photos' racist image recognition algorithm; thus, necessitated the\nutilization of the quantified uncertainty for each prediction. There have been\nrecent efforts towards quantifying uncertainty in conventional deep learning\nmethods (e.g., dropout as Bayesian approximation); however, their optimal use\nin decision making is often overlooked and understudied. In this study, we\npropose a mixed-integer programming framework for classification with reject\noption (also known as selective classification), that investigates and combines\nmodel uncertainty and predictive mean to identify optimal classification and\nrejection regions. Our results indicate superior performance of our framework\nboth in non-rejected accuracy and rejection quality on several publicly\navailable datasets. Moreover, we extend our framework to cost-sensitive\nsettings and show that our approach outperforms industry standard methods\nsignificantly for online fraud management in real-world settings.\n",
        "published": "2019",
        "authors": [
            "Mehmet Yigit Yildirim",
            "Mert Ozer",
            "Hasan Davulcu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.09821v1",
        "title": "Learning Feature Interactions with Lorentzian Factorization Machine",
        "abstract": "  Learning representations for feature interactions to model user behaviors is\ncritical for recommendation system and click-trough rate (CTR) predictions.\nRecent advances in this area are empowered by deep learning methods which could\nlearn sophisticated feature interactions and achieve the state-of-the-art\nresult in an end-to-end manner. These approaches require large number of\ntraining parameters integrated with the low-level representations, and thus are\nmemory and computational inefficient. In this paper, we propose a new model\nnamed \"LorentzFM\" that can learn feature interactions embedded in a hyperbolic\nspace in which the violation of triangle inequality for Lorentz distances is\navailable. To this end, the learned representation is benefited by the peculiar\ngeometric properties of hyperbolic triangles, and result in a significant\nreduction in the number of parameters (20\\% to 80\\%) because all the top deep\nlearning layers are not required. With such a lightweight architecture,\nLorentzFM achieves comparable and even materially better results than the deep\nlearning methods such as DeepFM, xDeepFM and Deep \\& Cross in both\nrecommendation and CTR prediction tasks.\n",
        "published": "2019",
        "authors": [
            "Canran Xu",
            "Ming Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.02891v1",
        "title": "Review: Deep Learning Methods for Cybersecurity and Intrusion Detection\n  Systems",
        "abstract": "  As the number of cyber-attacks is increasing, cybersecurity is evolving to a\nkey concern for any business. Artificial Intelligence (AI) and Machine Learning\n(ML) (in particular Deep Learning - DL) can be leveraged as key enabling\ntechnologies for cyber-defense, since they can contribute in threat detection\nand can even provide recommended actions to cyber analysts. A partnership of\nindustry, academia, and government on a global scale is necessary in order to\nadvance the adoption of AI/ML to cybersecurity and create efficient cyber\ndefense systems. In this paper, we are concerned with the investigation of the\nvarious deep learning techniques employed for network intrusion detection and\nwe introduce a DL framework for cybersecurity applications.\n",
        "published": "2020",
        "authors": [
            "Mayra Macas",
            "Chunming Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.09972v3",
        "title": "Implicit Regularization in Tensor Factorization",
        "abstract": "  Recent efforts to unravel the mystery of implicit regularization in deep\nlearning have led to a theoretical focus on matrix factorization -- matrix\ncompletion via linear neural network. As a step further towards practical deep\nlearning, we provide the first theoretical analysis of implicit regularization\nin tensor factorization -- tensor completion via certain type of non-linear\nneural network. We circumvent the notorious difficulty of tensor problems by\nadopting a dynamical systems perspective, and characterizing the evolution\ninduced by gradient descent. The characterization suggests a form of greedy low\ntensor rank search, which we rigorously prove under certain conditions, and\nempirically demonstrate under others. Motivated by tensor rank capturing the\nimplicit regularization of a non-linear neural network, we empirically explore\nit as a measure of complexity, and find that it captures the essence of\ndatasets on which neural networks generalize. This leads us to believe that\ntensor rank may pave way to explaining both implicit regularization in deep\nlearning, and the properties of real-world data translating this implicit\nregularization to generalization.\n",
        "published": "2021",
        "authors": [
            "Noam Razin",
            "Asaf Maman",
            "Nadav Cohen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.03701v1",
        "title": "Don't Forget to Sign the Gradients!",
        "abstract": "  Engineering a top-notch deep learning model is an expensive procedure that\ninvolves collecting data, hiring human resources with expertise in machine\nlearning, and providing high computational resources. For that reason, deep\nlearning models are considered as valuable Intellectual Properties (IPs) of the\nmodel vendors. To ensure reliable commercialization of deep learning models, it\nis crucial to develop techniques to protect model vendors against IP\ninfringements. One of such techniques that recently has shown great promise is\ndigital watermarking. However, current watermarking approaches can embed very\nlimited amount of information and are vulnerable against watermark removal\nattacks. In this paper, we present GradSigns, a novel watermarking framework\nfor deep neural networks (DNNs). GradSigns embeds the owner's signature into\nthe gradient of the cross-entropy cost function with respect to inputs to the\nmodel. Our approach has a negligible impact on the performance of the protected\nmodel and it allows model vendors to remotely verify the watermark through\nprediction APIs. We evaluate GradSigns on DNNs trained for different image\nclassification tasks using CIFAR-10, SVHN, and YTF datasets. Experimental\nresults show that GradSigns is robust against all known counter-watermark\nattacks and can embed a large amount of information into DNNs.\n",
        "published": "2021",
        "authors": [
            "Omid Aramoon",
            "Pin-Yu Chen",
            "Gang Qu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.12953v1",
        "title": "Exploring Uncertainty in Deep Learning for Construction of Prediction\n  Intervals",
        "abstract": "  Deep learning has achieved impressive performance on many tasks in recent\nyears. However, it has been found that it is still not enough for deep neural\nnetworks to provide only point estimates. For high-risk tasks, we need to\nassess the reliability of the model predictions. This requires us to quantify\nthe uncertainty of model prediction and construct prediction intervals. In this\npaper, We explore the uncertainty in deep learning to construct the prediction\nintervals. In general, We comprehensively consider two categories of\nuncertainties: aleatory uncertainty and epistemic uncertainty. We design a\nspecial loss function, which enables us to learn uncertainty without\nuncertainty label. We only need to supervise the learning of regression task.\nWe learn the aleatory uncertainty implicitly from the loss function. And that\nepistemic uncertainty is accounted for in ensembled form. Our method correlates\nthe construction of prediction intervals with the uncertainty estimation.\nImpressive results on some publicly available datasets show that the\nperformance of our method is competitive with other state-of-the-art methods.\n",
        "published": "2021",
        "authors": [
            "Yuandu Lai",
            "Yucheng Shi",
            "Yahong Han",
            "Yunfeng Shao",
            "Meiyu Qi",
            "Bingshuai Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.14372v1",
        "title": "A neural anisotropic view of underspecification in deep learning",
        "abstract": "  The underspecification of most machine learning pipelines means that we\ncannot rely solely on validation performance to assess the robustness of deep\nlearning systems to naturally occurring distribution shifts. Instead, making\nsure that a neural network can generalize across a large number of different\nsituations requires to understand the specific way in which it solves a task.\nIn this work, we propose to study this problem from a geometric perspective\nwith the aim to understand two key characteristics of neural network solutions\nin underspecified settings: how is the geometry of the learned function related\nto the data representation? And, are deep networks always biased towards\nsimpler solutions, as conjectured in recent literature? We show that the way\nneural networks handle the underspecification of these problems is highly\ndependent on the data representation, affecting both the geometry and the\ncomplexity of the learned predictors. Our results highlight that understanding\nthe architectural inductive bias in deep learning is fundamental to address the\nfairness, robustness, and generalization of these systems.\n",
        "published": "2021",
        "authors": [
            "Guillermo Ortiz-Jimenez",
            "Itamar Franco Salazar-Reque",
            "Apostolos Modas",
            "Seyed-Mohsen Moosavi-Dezfooli",
            "Pascal Frossard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.03591v1",
        "title": "FederatedNILM: A Distributed and Privacy-preserving Framework for\n  Non-intrusive Load Monitoring based on Federated Deep Learning",
        "abstract": "  Non-intrusive load monitoring (NILM), which usually utilizes machine learning\nmethods and is effective in disaggregating smart meter readings from the\nhousehold-level into appliance-level consumptions, can help to analyze\nelectricity consumption behaviours of users and enable practical smart energy\nand smart grid applications. However, smart meters are privately owned and\ndistributed, which make real-world applications of NILM challenging. To this\nend, this paper develops a distributed and privacy-preserving federated deep\nlearning framework for NILM (FederatedNILM), which combines federated learning\nwith a state-of-the-art deep learning architecture to conduct NILM for the\nclassification of typical states of household appliances. Through extensive\ncomparative experiments, the effectiveness of the proposed FederatedNILM\nframework is demonstrated.\n",
        "published": "2021",
        "authors": [
            "Shuang Dai",
            "Fanlin Meng",
            "Qian Wang",
            "Xizhong Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.08009v1",
        "title": "XAI Methods for Neural Time Series Classification: A Brief Review",
        "abstract": "  Deep learning models have recently demonstrated remarkable results in a\nvariety of tasks, which is why they are being increasingly applied in\nhigh-stake domains, such as industry, medicine, and finance. Considering that\nautomatic predictions in these domains might have a substantial impact on the\nwell-being of a person, as well as considerable financial and legal\nconsequences to an individual or a company, all actions and decisions that\nresult from applying these models have to be accountable. Given that a\nsubstantial amount of data that is collected in high-stake domains are in the\nform of time series, in this paper we examine the current state of eXplainable\nAI (XAI) methods with a focus on approaches for opening up deep learning black\nboxes for the task of time series classification. Finally, our contribution\nalso aims at deriving promising directions for future work, to advance XAI for\ndeep learning on time series data.\n",
        "published": "2021",
        "authors": [
            "Ilija \u0160imi\u0107",
            "Vedran Sabol",
            "Eduardo Veas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.03999v1",
        "title": "Graphs as Tools to Improve Deep Learning Methods",
        "abstract": "  In recent years, deep neural networks (DNNs) have known an important rise in\npopularity. However, although they are state-of-the-art in many machine\nlearning challenges, they still suffer from several limitations. For example,\nDNNs require a lot of training data, which might not be available in some\npractical applications. In addition, when small perturbations are added to the\ninputs, DNNs are prone to misclassification errors. DNNs are also viewed as\nblack-boxes and as such their decisions are often criticized for their lack of\ninterpretability.\n  In this chapter, we review recent works that aim at using graphs as tools to\nimprove deep learning methods. These graphs are defined considering a specific\nlayer in a deep learning architecture. Their vertices represent distinct\nsamples, and their edges depend on the similarity of the corresponding\nintermediate representations. These graphs can then be leveraged using various\nmethodologies, many of which built on top of graph signal processing.\n  This chapter is composed of four main parts: tools for visualizing\nintermediate layers in a DNN, denoising data representations, optimizing graph\nobjective functions and regularizing the learning process.\n",
        "published": "2021",
        "authors": [
            "Carlos Lassance",
            "Myriam Bontonou",
            "Mounia Hamidouche",
            "Bastien Pasdeloup",
            "Lucas Drumetz",
            "Vincent Gripon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.15829v5",
        "title": "Holistic Deep Learning",
        "abstract": "  This paper presents a novel holistic deep learning framework that\nsimultaneously addresses the challenges of vulnerability to input\nperturbations, overparametrization, and performance instability from different\ntrain-validation splits. The proposed framework holistically improves accuracy,\nrobustness, sparsity, and stability over standard deep learning models, as\ndemonstrated by extensive experiments on both tabular and image data sets. The\nresults are further validated by ablation experiments and SHAP value analysis,\nwhich reveal the interactions and trade-offs between the different evaluation\nmetrics. To support practitioners applying our framework, we provide a\nprescriptive approach that offers recommendations for selecting an appropriate\ntraining loss function based on their specific objectives. All the code to\nreproduce the results can be found at https://github.com/kimvc7/HDL.\n",
        "published": "2021",
        "authors": [
            "Dimitris Bertsimas",
            "Kimberly Villalobos Carballo",
            "L\u00e9onard Boussioux",
            "Michael Lingzhi Li",
            "Alex Paskov",
            "Ivan Paskov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.06571v1",
        "title": "Convergence Analysis of Deep Residual Networks",
        "abstract": "  Various powerful deep neural network architectures have made great\ncontribution to the exciting successes of deep learning in the past two\ndecades. Among them, deep Residual Networks (ResNets) are of particular\nimportance because they demonstrated great usefulness in computer vision by\nwinning the first place in many deep learning competitions. Also, ResNets were\nthe first class of neural networks in the development history of deep learning\nthat are really deep. It is of mathematical interest and practical meaning to\nunderstand the convergence of deep ResNets. We aim at characterizing the\nconvergence of deep ResNets as the depth tends to infinity in terms of the\nparameters of the networks. Toward this purpose, we first give a matrix-vector\ndescription of general deep neural networks with shortcut connections and\nformulate an explicit expression for the networks by using the notions of\nactivation domains and activation matrices. The convergence is then reduced to\nthe convergence of two series involving infinite products of non-square\nmatrices. By studying the two series, we establish a sufficient condition for\npointwise convergence of ResNets. Our result is able to give justification for\nthe design of ResNets. We also conduct experiments on benchmark machine\nlearning data to verify our results.\n",
        "published": "2022",
        "authors": [
            "Wentao Huang",
            "Haizhang Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.10743v1",
        "title": "Do Deep Learning Models and News Headlines Outperform Conventional\n  Prediction Techniques on Forex Data?",
        "abstract": "  Foreign Exchange (FOREX) is a decentralised global market for exchanging\ncurrencies. The Forex market is enormous, and it operates 24 hours a day. Along\nwith country-specific factors, Forex trading is influenced by cross-country\nties and a variety of global events. Recent pandemic scenarios such as COVID19\nand local elections can also have a significant impact on market pricing. We\ntested and compared various predictions with external elements such as news\nitems in this work. Additionally, we compared classical machine learning\nmethods to deep learning algorithms. We also added sentiment features from news\nheadlines using NLP-based word embeddings and compared the performance. Our\nresults indicate that simple regression model like linear, SGD, and Bagged\nperformed better than deep learning models such as LSTM and RNN for single-step\nforecasting like the next two hours, the next day, and seven days.\nSurprisingly, news articles failed to improve the predictions indicating\ndomain-based and relevant information only adds value. Among the text\nvectorization techniques, Word2Vec and SentenceBERT perform better.\n",
        "published": "2022",
        "authors": [
            "Sucharita Atha",
            "Bharath Kumar Bolla"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.08900v2",
        "title": "Adapting the Linearised Laplace Model Evidence for Modern Deep Learning",
        "abstract": "  The linearised Laplace method for estimating model uncertainty has received\nrenewed attention in the Bayesian deep learning community. The method provides\nreliable error bars and admits a closed-form expression for the model evidence,\nallowing for scalable selection of model hyperparameters. In this work, we\nexamine the assumptions behind this method, particularly in conjunction with\nmodel selection. We show that these interact poorly with some now-standard\ntools of deep learning--stochastic approximation methods and normalisation\nlayers--and make recommendations for how to better adapt this classic method to\nthe modern setting. We provide theoretical support for our recommendations and\nvalidate them empirically on MLPs, classic CNNs, residual networks with and\nwithout normalisation layers, generative autoencoders and transformers.\n",
        "published": "2022",
        "authors": [
            "Javier Antor\u00e1n",
            "David Janz",
            "James Urquhart Allingham",
            "Erik Daxberger",
            "Riccardo Barbano",
            "Eric Nalisnick",
            "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.08815v1",
        "title": "Why do tree-based models still outperform deep learning on tabular data?",
        "abstract": "  While deep learning has enabled tremendous progress on text and image\ndatasets, its superiority on tabular data is not clear. We contribute extensive\nbenchmarks of standard and novel deep learning methods as well as tree-based\nmodels such as XGBoost and Random Forests, across a large number of datasets\nand hyperparameter combinations. We define a standard set of 45 datasets from\nvaried domains with clear characteristics of tabular data and a benchmarking\nmethodology accounting for both fitting models and finding good\nhyperparameters. Results show that tree-based models remain state-of-the-art on\nmedium-sized data ($\\sim$10K samples) even without accounting for their\nsuperior speed. To understand this gap, we conduct an empirical investigation\ninto the differing inductive biases of tree-based models and Neural Networks\n(NNs). This leads to a series of challenges which should guide researchers\naiming to build tabular-specific NNs: 1. be robust to uninformative features,\n2. preserve the orientation of the data, and 3. be able to easily learn\nirregular functions. To stimulate research on tabular architectures, we\ncontribute a standard benchmark and raw data for baselines: every point of a 20\n000 compute hours hyperparameter search for each learner.\n",
        "published": "2022",
        "authors": [
            "L\u00e9o Grinsztajn",
            "Edouard Oyallon",
            "Ga\u00ebl Varoquaux"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.02813v1",
        "title": "Towards Understanding Mixture of Experts in Deep Learning",
        "abstract": "  The Mixture-of-Experts (MoE) layer, a sparsely-activated model controlled by\na router, has achieved great success in deep learning. However, the\nunderstanding of such architecture remains elusive. In this paper, we formally\nstudy how the MoE layer improves the performance of neural network learning and\nwhy the mixture model will not collapse into a single model. Our empirical\nresults suggest that the cluster structure of the underlying problem and the\nnon-linearity of the expert are pivotal to the success of MoE. To further\nunderstand this, we consider a challenging classification problem with\nintrinsic cluster structures, which is hard to learn using a single expert. Yet\nwith the MoE layer, by choosing the experts as two-layer nonlinear\nconvolutional neural networks (CNNs), we show that the problem can be learned\nsuccessfully. Furthermore, our theory shows that the router can learn the\ncluster-center features, which helps divide the input complex problem into\nsimpler linear classification sub-problems that individual experts can conquer.\nTo our knowledge, this is the first result towards formally understanding the\nmechanism of the MoE layer for deep learning.\n",
        "published": "2022",
        "authors": [
            "Zixiang Chen",
            "Yihe Deng",
            "Yue Wu",
            "Quanquan Gu",
            "Yuanzhi Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.10828v1",
        "title": "Instability in clinical risk stratification models using deep learning",
        "abstract": "  While it has been well known in the ML community that deep learning models\nsuffer from instability, the consequences for healthcare deployments are under\ncharacterised. We study the stability of different model architectures trained\non electronic health records, using a set of outpatient prediction tasks as a\ncase study. We show that repeated training runs of the same deep learning model\non the same training data can result in significantly different outcomes at a\npatient level even though global performance metrics remain stable. We propose\ntwo stability metrics for measuring the effect of randomness of model training,\nas well as mitigation strategies for improving model stability.\n",
        "published": "2022",
        "authors": [
            "Daniel Lopez-Martinez",
            "Alex Yakubovich",
            "Martin Seneviratne",
            "Adam D. Lelkes",
            "Akshit Tyagi",
            "Jonas Kemp",
            "Ethan Steinberg",
            "N. Lance Downing",
            "Ron C. Li",
            "Keith E. Morse",
            "Nigam H. Shah",
            "Ming-Jun Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.05427v1",
        "title": "Statistical guarantees for sparse deep learning",
        "abstract": "  Neural networks are becoming increasingly popular in applications, but our\nmathematical understanding of their potential and limitations is still limited.\nIn this paper, we further this understanding by developing statistical\nguarantees for sparse deep learning. In contrast to previous work, we consider\ndifferent types of sparsity, such as few active connections, few active nodes,\nand other norm-based types of sparsity. Moreover, our theories cover important\naspects that previous theories have neglected, such as multiple outputs,\nregularization, and l2-loss. The guarantees have a mild dependence on network\nwidths and depths, which means that they support the application of sparse but\nwide and deep networks from a statistical perspective. Some of the concepts and\ntools that we use in our derivations are uncommon in deep learning and, hence,\nmight be of additional interest.\n",
        "published": "2022",
        "authors": [
            "Johannes Lederer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.05272v1",
        "title": "Inaccessible Neural Language Models Could Reinvigorate Linguistic\n  Nativism",
        "abstract": "  Large Language Models (LLMs) have been making big waves in the machine\nlearning community within the past few years. The impressive scalability of\nLLMs due to the advent of deep learning can be seen as a continuation of\nempiricist lingusitic methods, as opposed to rule-based linguistic methods that\nare grounded in a nativist perspective. Current LLMs are generally inaccessible\nto resource-constrained researchers, due to a variety of factors including\nclosed source code. This work argues that this lack of accessibility could\ninstill a nativist bias in researchers new to computational linguistics, given\nthat new researchers may only have rule-based, nativist approaches to study to\nproduce new work. Also, given that there are numerous critics of deep learning\nclaiming that LLMs and related methods may soon lose their relevancy, we\nspeculate that such an event could trigger a new wave of nativism in the\nlanguage processing community. To prevent such a dramatic shift and placing\nfavor in hybrid methods of rules and deep learning, we call upon researchers to\nopen source their LLM code wherever possible to allow both empircist and hybrid\napproaches to remain accessible.\n",
        "published": "2023",
        "authors": [
            "Patrick Perrine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.02045v3",
        "title": "Uncertainty Estimation by Fisher Information-based Evidential Deep\n  Learning",
        "abstract": "  Uncertainty estimation is a key factor that makes deep learning reliable in\npractical applications. Recently proposed evidential neural networks explicitly\naccount for different uncertainties by treating the network's outputs as\nevidence to parameterize the Dirichlet distribution, and achieve impressive\nperformance in uncertainty estimation. However, for high data uncertainty\nsamples but annotated with the one-hot label, the evidence-learning process for\nthose mislabeled classes is over-penalized and remains hindered. To address\nthis problem, we propose a novel method, Fisher Information-based Evidential\nDeep Learning ($\\mathcal{I}$-EDL). In particular, we introduce Fisher\nInformation Matrix (FIM) to measure the informativeness of evidence carried by\neach sample, according to which we can dynamically reweight the objective loss\nterms to make the network more focused on the representation learning of\nuncertain classes. The generalization ability of our network is further\nimproved by optimizing the PAC-Bayesian bound. As demonstrated empirically, our\nproposed method consistently outperforms traditional EDL-related algorithms in\nmultiple uncertainty estimation tasks, especially in the more challenging\nfew-shot classification settings.\n",
        "published": "2023",
        "authors": [
            "Danruo Deng",
            "Guangyong Chen",
            "Yang Yu",
            "Furui Liu",
            "Pheng-Ann Heng"
        ]
    }
]