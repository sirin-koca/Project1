[
    {
        "id": "http://arxiv.org/abs/2211.11744v3",
        "title": "Visual Dexterity: In-Hand Reorientation of Novel and Complex Object\n  Shapes",
        "abstract": "  In-hand object reorientation is necessary for performing many dexterous\nmanipulation tasks, such as tool use in less structured environments that\nremain beyond the reach of current robots. Prior works built reorientation\nsystems assuming one or many of the following: reorienting only specific\nobjects with simple shapes, limited range of reorientation, slow or quasistatic\nmanipulation, simulation-only results, the need for specialized and costly\nsensor suites, and other constraints which make the system infeasible for\nreal-world deployment. We present a general object reorientation controller\nthat does not make these assumptions. It uses readings from a single commodity\ndepth camera to dynamically reorient complex and new object shapes by any\nrotation in real-time, with the median reorientation time being close to seven\nseconds. The controller is trained using reinforcement learning in simulation\nand evaluated in the real world on new object shapes not used for training,\nincluding the most challenging scenario of reorienting objects held in the air\nby a downward-facing hand that must counteract gravity during reorientation.\nOur hardware platform only uses open-source components that cost less than five\nthousand dollars. Although we demonstrate the ability to overcome assumptions\nin prior work, there is ample scope for improving absolute performance. For\ninstance, the challenging duck-shaped object not used for training was dropped\nin 56 percent of the trials. When it was not dropped, our controller reoriented\nthe object within 0.4 radians (23 degrees) 75 percent of the time. Videos are\navailable at: https://taochenshh.github.io/projects/visual-dexterity.\n",
        "published": "2022",
        "authors": [
            "Tao Chen",
            "Megha Tippur",
            "Siyang Wu",
            "Vikash Kumar",
            "Edward Adelson",
            "Pulkit Agrawal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.04498v1",
        "title": "VideoDex: Learning Dexterity from Internet Videos",
        "abstract": "  To build general robotic agents that can operate in many environments, it is\noften imperative for the robot to collect experience in the real world.\nHowever, this is often not feasible due to safety, time, and hardware\nrestrictions. We thus propose leveraging the next best thing as real-world\nexperience: internet videos of humans using their hands. Visual priors, such as\nvisual features, are often learned from videos, but we believe that more\ninformation from videos can be utilized as a stronger prior. We build a\nlearning algorithm, VideoDex, that leverages visual, action, and physical\npriors from human video datasets to guide robot behavior. These actions and\nphysical priors in the neural network dictate the typical human behavior for a\nparticular robot task. We test our approach on a robot arm and dexterous\nhand-based system and show strong results on various manipulation tasks,\noutperforming various state-of-the-art methods. Videos at\nhttps://video-dex.github.io\n",
        "published": "2022",
        "authors": [
            "Kenneth Shaw",
            "Shikhar Bahl",
            "Deepak Pathak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.08729v1",
        "title": "Distribution-aware Goal Prediction and Conformant Model-based Planning\n  for Safe Autonomous Driving",
        "abstract": "  The feasibility of collecting a large amount of expert demonstrations has\ninspired growing research interests in learning-to-drive settings, where models\nlearn by imitating the driving behaviour from experts. However, exclusively\nrelying on imitation can limit agents' generalisability to novel scenarios that\nare outside the support of the training data. In this paper, we address this\nchallenge by factorising the driving task, based on the intuition that modular\narchitectures are more generalisable and more robust to changes in the\nenvironment compared to monolithic, end-to-end frameworks. Specifically, we\ndraw inspiration from the trajectory forecasting community and reformulate the\nlearning-to-drive task as obstacle-aware perception and grounding,\ndistribution-aware goal prediction, and model-based planning. Firstly, we train\nthe obstacle-aware perception module to extract salient representation of the\nvisual context. Then, we learn a multi-modal goal distribution by performing\nconditional density-estimation using normalising flow. Finally, we ground\ncandidate trajectory predictions road geometry, and plan the actions based on\non vehicle dynamics. Under the CARLA simulator, we report state-of-the-art\nresults on the CARNOVEL benchmark.\n",
        "published": "2022",
        "authors": [
            "Jonathan Francis",
            "Bingqing Chen",
            "Weiran Yao",
            "Eric Nyberg",
            "Jean Oh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.06604v1",
        "title": "ALAN: Autonomously Exploring Robotic Agents in the Real World",
        "abstract": "  Robotic agents that operate autonomously in the real world need to\ncontinuously explore their environment and learn from the data collected, with\nminimal human supervision. While it is possible to build agents that can learn\nin such a manner without supervision, current methods struggle to scale to the\nreal world. Thus, we propose ALAN, an autonomously exploring robotic agent,\nthat can perform tasks in the real world with little training and interaction\ntime. This is enabled by measuring environment change, which reflects object\nmovement and ignores changes in the robot position. We use this metric directly\nas an environment-centric signal, and also maximize the uncertainty of\npredicted environment change, which provides agent-centric exploration signal.\nWe evaluate our approach on two different real-world play kitchen settings,\nenabling a robot to efficiently explore and discover manipulation skills, and\nperform tasks specified via goal images. Website at\nhttps://robo-explorer.github.io/\n",
        "published": "2023",
        "authors": [
            "Russell Mendonca",
            "Shikhar Bahl",
            "Deepak Pathak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.11330v2",
        "title": "Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion",
        "abstract": "  Locomotion has seen dramatic progress for walking or running across\nchallenging terrains. However, robotic quadrupeds are still far behind their\nbiological counterparts, such as dogs, which display a variety of agile skills\nand can use the legs beyond locomotion to perform several basic manipulation\ntasks like interacting with objects and climbing. In this paper, we take a step\ntowards bridging this gap by training quadruped robots not only to walk but\nalso to use the front legs to climb walls, press buttons, and perform object\ninteraction in the real world. To handle this challenging optimization, we\ndecouple the skill learning broadly into locomotion, which involves anything\nthat involves movement whether via walking or climbing a wall, and\nmanipulation, which involves using one leg to interact while balancing on the\nother three legs. These skills are trained in simulation using curriculum and\ntransferred to the real world using our proposed sim2real variant that builds\nupon recent locomotion success. Finally, we combine these skills into a robust\nlong-term plan by learning a behavior tree that encodes a high-level task\nhierarchy from one clean expert demonstration. We evaluate our method in both\nsimulation and real-world showing successful executions of both short as well\nas long-range tasks and how robustness helps confront external perturbations.\nVideos at https://robot-skills.github.io\n",
        "published": "2023",
        "authors": [
            "Xuxin Cheng",
            "Ashish Kumar",
            "Deepak Pathak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.10891v1",
        "title": "Transformer-based models and hardware acceleration analysis in\n  autonomous driving: A survey",
        "abstract": "  Transformer architectures have exhibited promising performance in various\nautonomous driving applications in recent years. On the other hand, its\ndedicated hardware acceleration on portable computational platforms has become\nthe next critical step for practical deployment in real autonomous vehicles.\nThis survey paper provides a comprehensive overview, benchmark, and analysis of\nTransformer-based models specifically tailored for autonomous driving tasks\nsuch as lane detection, segmentation, tracking, planning, and decision-making.\nWe review different architectures for organizing Transformer inputs and\noutputs, such as encoder-decoder and encoder-only structures, and explore their\nrespective advantages and disadvantages. Furthermore, we discuss\nTransformer-related operators and their hardware acceleration schemes in depth,\ntaking into account key factors such as quantization and runtime. We\nspecifically illustrate the operator level comparison between layers from\nconvolutional neural network, Swin-Transformer, and Transformer with 4D\nencoder. The paper also highlights the challenges, trends, and current insights\nin Transformer-based models, addressing their hardware deployment and\nacceleration issues within the context of long-term autonomous driving\napplications.\n",
        "published": "2023",
        "authors": [
            "Juan Zhong",
            "Zheng Liu",
            "Xi Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.06299v1",
        "title": "Defensive Perception: Estimation and Monitoring of Neural Network\n  Performance under Deployment",
        "abstract": "  In this paper, we propose a method for addressing the issue of unnoticed\ncatastrophic deployment and domain shift in neural networks for semantic\nsegmentation in autonomous driving. Our approach is based on the idea that deep\nlearning-based perception for autonomous driving is uncertain and best\nrepresented as a probability distribution. As autonomous vehicles' safety is\nparamount, it is crucial for perception systems to recognize when the vehicle\nis leaving its operational design domain, anticipate hazardous uncertainty, and\nreduce the performance of the perception system. To address this, we propose to\nencapsulate the neural network under deployment within an uncertainty\nestimation envelope that is based on the epistemic uncertainty estimation\nthrough the Monte Carlo Dropout approach. This approach does not require\nmodification of the deployed neural network and guarantees expected model\nperformance. Our defensive perception envelope has the capability to estimate a\nneural network's performance, enabling monitoring and notification of entering\ndomains of reduced neural network performance under deployment. Furthermore,\nour envelope is extended by novel methods to improve the application in\ndeployment settings, including reducing compute expenses and confining\nestimation noise. Finally, we demonstrate the applicability of our method for\nmultiple different potential deployment shifts relevant to autonomous driving,\nsuch as transitions into the night, rainy, or snowy domain. Overall, our\napproach shows great potential for application in deployment settings and\nenables operational design domain recognition via uncertainty, which allows for\ndefensive perception, safe state triggers, warning notifications, and feedback\nfor testing or development and adaptation of the perception stack.\n",
        "published": "2023",
        "authors": [
            "Hendrik Vogt",
            "Stefan Buehler",
            "Mark Schutera"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.06440v1",
        "title": "LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot\n  Learning",
        "abstract": "  Dexterous manipulation has been a long-standing challenge in robotics. While\nmachine learning techniques have shown some promise, results have largely been\ncurrently limited to simulation. This can be mostly attributed to the lack of\nsuitable hardware. In this paper, we present LEAP Hand, a low-cost dexterous\nand anthropomorphic hand for machine learning research. In contrast to previous\nhands, LEAP Hand has a novel kinematic structure that allows maximal dexterity\nregardless of finger pose. LEAP Hand is low-cost and can be assembled in 4\nhours at a cost of 2000 USD from readily available parts. It is capable of\nconsistently exerting large torques over long durations of time. We show that\nLEAP Hand can be used to perform several manipulation tasks in the real world\n-- from visual teleoperation to learning from passive video data and sim2real.\nLEAP Hand significantly outperforms its closest competitor Allegro Hand in all\nour experiments while being 1/8th of the cost. We release detailed assembly\ninstructions, the Sim2Real pipeline and a development platform with useful APIs\non our website at https://leap-hand.github.io/\n",
        "published": "2023",
        "authors": [
            "Kenneth Shaw",
            "Ananye Agarwal",
            "Deepak Pathak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.14341v1",
        "title": "Extreme Parkour with Legged Robots",
        "abstract": "  Humans can perform parkour by traversing obstacles in a highly dynamic\nfashion requiring precise eye-muscle coordination and movement. Getting robots\nto do the same task requires overcoming similar challenges. Classically, this\nis done by independently engineering perception, actuation, and control systems\nto very low tolerances. This restricts them to tightly controlled settings such\nas a predetermined obstacle course in labs. In contrast, humans are able to\nlearn parkour through practice without significantly changing their underlying\nbiology. In this paper, we take a similar approach to developing robot parkour\non a small low-cost robot with imprecise actuation and a single front-facing\ndepth camera for perception which is low-frequency, jittery, and prone to\nartifacts. We show how a single neural net policy operating directly from a\ncamera image, trained in simulation with large-scale RL, can overcome imprecise\nsensing and actuation to output highly precise control behavior end-to-end. We\nshow our robot can perform a high jump on obstacles 2x its height, long jump\nacross gaps 2x its length, do a handstand and run across tilted ramps, and\ngeneralize to novel obstacle courses with different physical properties.\nParkour videos at https://extreme-parkour.github.io/\n",
        "published": "2023",
        "authors": [
            "Xuxin Cheng",
            "Kexin Shi",
            "Ananye Agarwal",
            "Deepak Pathak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.14425v2",
        "title": "Self-Recovery Prompting: Promptable General Purpose Service Robot System\n  with Foundation Models and Self-Recovery",
        "abstract": "  A general-purpose service robot (GPSR), which can execute diverse tasks in\nvarious environments, requires a system with high generalizability and\nadaptability to tasks and environments. In this paper, we first developed a\ntop-level GPSR system for worldwide competition (RoboCup@Home 2023) based on\nmultiple foundation models. This system is both generalizable to variations and\nadaptive by prompting each model. Then, by analyzing the performance of the\ndeveloped system, we found three types of failure in more realistic GPSR\napplication settings: insufficient information, incorrect plan generation, and\nplan execution failure. We then propose the self-recovery prompting pipeline,\nwhich explores the necessary information and modifies its prompts to recover\nfrom failure. We experimentally confirm that the system with the self-recovery\nmechanism can accomplish tasks by resolving various failure cases.\nSupplementary videos are available at https://sites.google.com/view/srgpsr .\n",
        "published": "2023",
        "authors": [
            "Mimo Shirasaka",
            "Tatsuya Matsushima",
            "Soshi Tsunashima",
            "Yuya Ikeda",
            "Aoi Horo",
            "So Ikoma",
            "Chikaha Tsuji",
            "Hikaru Wada",
            "Tsunekazu Omija",
            "Dai Komukai",
            "Yutaka Matsuo Yusuke Iwasawa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.02975v1",
        "title": "Dexterous Functional Grasping",
        "abstract": "  While there have been significant strides in dexterous manipulation, most of\nit is limited to benchmark tasks like in-hand reorientation which are of\nlimited utility in the real world. The main benefit of dexterous hands over\ntwo-fingered ones is their ability to pickup tools and other objects (including\nthin ones) and grasp them firmly to apply force. However, this task requires\nboth a complex understanding of functional affordances as well as precise\nlow-level control. While prior work obtains affordances from human data this\napproach doesn't scale to low-level control. Similarly, simulation training\ncannot give the robot an understanding of real-world semantics. In this paper,\nwe aim to combine the best of both worlds to accomplish functional grasping for\nin-the-wild objects. We use a modular approach. First, affordances are obtained\nby matching corresponding regions of different objects and then a low-level\npolicy trained in sim is run to grasp it. We propose a novel application of\neigengrasps to reduce the search space of RL using a small amount of human data\nand find that it leads to more stable and physically realistic motion. We find\nthat eigengrasp action space beats baselines in simulation and outperforms\nhardcoded grasping in real and matches or outperforms a trained human\nteleoperator. Results visualizations and videos at https://dexfunc.github.io/\n",
        "published": "2023",
        "authors": [
            "Ananye Agarwal",
            "Shagun Uppal",
            "Kenneth Shaw",
            "Deepak Pathak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.04549v1",
        "title": "PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play",
        "abstract": "  Learning from unstructured and uncurated data has become the dominant\nparadigm for generative approaches in language and vision. Such unstructured\nand unguided behavior data, commonly known as play, is also easier to collect\nin robotics but much more difficult to learn from due to its inherently\nmultimodal, noisy, and suboptimal nature. In this paper, we study this problem\nof learning goal-directed skill policies from unstructured play data which is\nlabeled with language in hindsight. Specifically, we leverage advances in\ndiffusion models to learn a multi-task diffusion model to extract robotic\nskills from play data. Using a conditional denoising diffusion process in the\nspace of states and actions, we can gracefully handle the complexity and\nmultimodality of play data and generate diverse and interesting robot\nbehaviors. To make diffusion models more useful for skill learning, we\nencourage robotic agents to acquire a vocabulary of skills by introducing\ndiscrete bottlenecks into the conditional behavior generation process. In our\nexperiments, we demonstrate the effectiveness of our approach across a wide\nvariety of environments in both simulation and the real world. Results\nvisualizations and videos at https://play-fusion.github.io\n",
        "published": "2023",
        "authors": [
            "Lili Chen",
            "Shikhar Bahl",
            "Deepak Pathak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.11460v3",
        "title": "Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated\n  Robot Response",
        "abstract": "  Robust locomotion control depends on accurate state estimations. However, the\nsensors of most legged robots can only provide partial and noisy observations,\nmaking the estimation particularly challenging, especially for external states\nlike terrain frictions and elevation maps. Inspired by the classical Internal\nModel Control principle, we consider these external states as disturbances and\nintroduce Hybrid Internal Model (HIM) to estimate them according to the\nresponse of the robot. The response, which we refer to as the hybrid internal\nembedding, contains the robot's explicit velocity and implicit stability\nrepresentation, corresponding to two primary goals for locomotion tasks:\nexplicitly tracking velocity and implicitly maintaining stability. We use\ncontrastive learning to optimize the embedding to be close to the robot's\nsuccessor state, in which the response is naturally embedded. HIM has several\nappealing benefits: It only needs the robot's proprioceptions, i.e., those from\njoint encoders and IMU as observations. It innovatively maintains consistent\nobservations between simulation reference and reality that avoids information\nloss in mimicking learning. It exploits batch-level information that is more\nrobust to noises and keeps better sample efficiency. It only requires 1 hour of\ntraining on an RTX 4090 to enable a quadruped robot to traverse any terrain\nunder any disturbances. A wealth of real-world experiments demonstrates its\nagility, even in high-difficulty tasks and cases never occurred during the\ntraining process, revealing remarkable open-world generalizability.\n",
        "published": "2023",
        "authors": [
            "Junfeng Long",
            "Zirui Wang",
            "Quanyi Li",
            "Jiawei Gao",
            "Liu Cao",
            "Jiangmiao Pang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.02117v1",
        "title": "Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost\n  Whole-Body Teleoperation",
        "abstract": "  Imitation learning from human demonstrations has shown impressive performance\nin robotics. However, most results focus on table-top manipulation, lacking the\nmobility and dexterity necessary for generally useful tasks. In this work, we\ndevelop a system for imitating mobile manipulation tasks that are bimanual and\nrequire whole-body control. We first present Mobile ALOHA, a low-cost and\nwhole-body teleoperation system for data collection. It augments the ALOHA\nsystem with a mobile base, and a whole-body teleoperation interface. Using data\ncollected with Mobile ALOHA, we then perform supervised behavior cloning and\nfind that co-training with existing static ALOHA datasets boosts performance on\nmobile manipulation tasks. With 50 demonstrations for each task, co-training\ncan increase success rates by up to 90%, allowing Mobile ALOHA to autonomously\ncomplete complex mobile manipulation tasks such as sauteing and serving a piece\nof shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling\nand entering an elevator, and lightly rinsing a used pan using a kitchen\nfaucet. Project website: https://mobile-aloha.github.io\n",
        "published": "2024",
        "authors": [
            "Zipeng Fu",
            "Tony Z. Zhao",
            "Chelsea Finn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.03267v1",
        "title": "Autonomous Navigation in Complex Environments",
        "abstract": "  This paper explores the application of CNN-DNN network fusion to construct a\nrobot navigation controller within a simulated environment. The simulated\nenvironment is constructed to model a subterranean rescue situation, such that\nan autonomous agent is tasked with finding a goal within an unknown cavernous\nsystem. Imitation learning is used to train the control algorithm to use LiDAR\nand camera data to navigate the space and find the goal. The trained model is\nthen tested for robustness using Monte-Carlo.\n",
        "published": "2024",
        "authors": [
            "Andrew Gerstenslager",
            "Jomol Lewis",
            "Liam McKenna",
            "Poorva Patel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.14849v2",
        "title": "Why does CTC result in peaky behavior?",
        "abstract": "  The peaky behavior of CTC models is well known experimentally. However, an\nunderstanding about why peaky behavior occurs is missing, and whether this is a\ngood property. We provide a formal analysis of the peaky behavior and gradient\ndescent convergence properties of the CTC loss and related training criteria.\nOur analysis provides a deep understanding why peaky behavior occurs and when\nit is suboptimal. On a simple example which should be trivial to learn for any\nmodel, we prove that a feed-forward neural network trained with CTC from\nuniform initialization converges towards peaky behavior with a 100% error rate.\nOur analysis further explains why CTC only works well together with the blank\nlabel. We further demonstrate that peaky behavior does not occur on other\nrelated losses including a label prior model, and that this improves\nconvergence.\n",
        "published": "2021",
        "authors": [
            "Albert Zeyer",
            "Ralf Schl\u00fcter",
            "Hermann Ney"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.00120v1",
        "title": "Optimizing Mixed Autonomy Traffic Flow With Decentralized Autonomous\n  Vehicles and Multi-Agent RL",
        "abstract": "  We study the ability of autonomous vehicles to improve the throughput of a\nbottleneck using a fully decentralized control scheme in a mixed autonomy\nsetting. We consider the problem of improving the throughput of a scaled model\nof the San Francisco-Oakland Bay Bridge: a two-stage bottleneck where four\nlanes reduce to two and then reduce to one. Although there is extensive work\nexamining variants of bottleneck control in a centralized setting, there is\nless study of the challenging multi-agent setting where the large number of\ninteracting AVs leads to significant optimization difficulties for\nreinforcement learning methods. We apply multi-agent reinforcement algorithms\nto this problem and demonstrate that significant improvements in bottleneck\nthroughput, from 20\\% at a 5\\% penetration rate to 33\\% at a 40\\% penetration\nrate, can be achieved. We compare our results to a hand-designed feedback\ncontroller and demonstrate that our results sharply outperform the feedback\ncontroller despite extensive tuning. Additionally, we demonstrate that the\nRL-based controllers adopt a robust strategy that works across penetration\nrates whereas the feedback controllers degrade immediately upon penetration\nrate variation. We investigate the feasibility of both action and observation\ndecentralization and demonstrate that effective strategies are possible using\npurely local sensing. Finally, we open-source our code at\nhttps://github.com/eugenevinitsky/decentralized_bottlenecks.\n",
        "published": "2020",
        "authors": [
            "Eugene Vinitsky",
            "Nathan Lichtle",
            "Kanaad Parvate",
            "Alexandre Bayen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.10480v2",
        "title": "Distributed Map Classification using Local Observations",
        "abstract": "  We consider the problem of classifying a map using a team of communicating\nrobots. It is assumed that all robots have localized visual sensing\ncapabilities and can exchange their information with neighboring robots. Using\na graph decomposition technique, we proposed an offline learning structure that\nmakes every robot capable of communicating with and fusing information from its\nneighbors to plan its next move towards the most informative parts of the\nenvironment for map classification purposes. The main idea is to decompose a\ngiven undirected graph into a union of directed star graphs and train robots\nw.r.t a bounded number of star graphs. This will significantly reduce the\ncomputational cost of offline training and makes learning scalable (independent\nof the number of robots). Our approach is particularly useful for fast map\nclassification in large environments using a large number of communicating\nrobots. We validate the usefulness of our proposed methodology through\nextensive simulations.\n",
        "published": "2020",
        "authors": [
            "Guangyi Liu",
            "Arash Amini",
            "Martin Tak\u00e1\u010d",
            "H\u00e9ctor Mu\u00f1oz-Avila",
            "Nader Motee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.07620v2",
        "title": "Collective Iterative Learning Control: Exploiting Diversity in\n  Multi-Agent Systems for Reference Tracking Tasks",
        "abstract": "  Multi-agent systems (MASs) can autonomously learn to solve previously unknown\ntasks by means of each agent's individual intelligence as well as by\ncollaborating and exploiting collective intelligence. This article considers a\ngroup of autonomous agents learning to track the same given reference\ntrajectory in a possibly small number of trials. We propose a novel collective\nlearning control method that combines iterative learning control (ILC) with a\ncollective update strategy. We derive conditions for desirable convergence\nproperties of such systems. We show that the proposed method allows the\ncollective to combine the advantages of the agents' individual learning\nstrategies and thereby overcomes trade-offs and limitations of single-agent\nILC. This benefit is achieved by designing a heterogeneous collective, i.e., a\ndifferent learning law is assigned to each agent. All theoretical results are\nconfirmed in simulations and experiments with two-wheeled-inverted-pendulum\nrobots (TWIPRs) that jointly learn to perform the desired maneuver.\n",
        "published": "2021",
        "authors": [
            "Michael Meindl",
            "Fabio Molinari",
            "Dustin Lehmann",
            "Thomas Seel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.06264v1",
        "title": "Bridging the gap between emotion and joint action",
        "abstract": "  Our daily human life is filled with a myriad of joint action moments, be it\nchildren playing, adults working together (i.e., team sports), or strangers\nnavigating through a crowd. Joint action brings individuals (and embodiment of\ntheir emotions) together, in space and in time. Yet little is known about how\nindividual emotions propagate through embodied presence in a group, and how\njoint action changes individual emotion. In fact, the multi-agent component is\nlargely missing from neuroscience-based approaches to emotion, and reversely\njoint action research has not found a way yet to include emotion as one of the\nkey parameters to model socio-motor interaction. In this review, we first\nidentify the gap and then stockpile evidence showing strong entanglement\nbetween emotion and acting together from various branches of sciences. We\npropose an integrative approach to bridge the gap, highlight five research\navenues to do so in behavioral neuroscience and digital sciences, and address\nsome of the key challenges in the area faced by modern societies.\n",
        "published": "2021",
        "authors": [
            "M. M. N. Bie\u0144kiewicz",
            "A. Smykovskyi",
            "T. Olugbade",
            "S. Janaqi",
            "A. Camurri",
            "N. Bianchi-Berthouze",
            "M. Bj\u00f6rkman",
            "B. G. Bardy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.12343v1",
        "title": "Beyond Robustness: A Taxonomy of Approaches towards Resilient\n  Multi-Robot Systems",
        "abstract": "  Robustness is key to engineering, automation, and science as a whole.\nHowever, the property of robustness is often underpinned by costly requirements\nsuch as over-provisioning, known uncertainty and predictive models, and known\nadversaries. These conditions are idealistic, and often not satisfiable.\nResilience on the other hand is the capability to endure unexpected\ndisruptions, to recover swiftly from negative events, and bounce back to\nnormality. In this survey article, we analyze how resilience is achieved in\nnetworks of agents and multi-robot systems that are able to overcome adversity\nby leveraging system-wide complementarity, diversity, and redundancy - often\ninvolving a reconfiguration of robotic capabilities to provide some key ability\nthat was not present in the system a priori. As society increasingly depends on\nconnected automated systems to provide key infrastructure services (e.g.,\nlogistics, transport, and precision agriculture), providing the means to\nachieving resilient multi-robot systems is paramount. By enumerating the\nconsequences of a system that is not resilient (fragile), we argue that\nresilience must become a central engineering design consideration. Towards this\ngoal, the community needs to gain clarity on how it is defined, measured, and\nmaintained. We address these questions across foundational robotics domains,\nspanning perception, control, planning, and learning. One of our key\ncontributions is a formal taxonomy of approaches, which also helps us discuss\nthe defining factors and stressors for a resilient system. Finally, this survey\narticle gives insight as to how resilience may be achieved. Importantly, we\nhighlight open problems that remain to be tackled in order to reap the benefits\nof resilient robotic systems.\n",
        "published": "2021",
        "authors": [
            "Amanda Prorok",
            "Matthew Malencia",
            "Luca Carlone",
            "Gaurav S. Sukhatme",
            "Brian M. Sadler",
            "Vijay Kumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.08892v3",
        "title": "Distributed Differentiable Dynamic Game for Multi-robot Coordination",
        "abstract": "  This paper develops a Distributed Differentiable Dynamic Game (D3G)\nframework, which can efficiently solve the forward and inverse problems in\nmulti-robot coordination. We formulate multi-robot coordination as a dynamic\ngame, where the behavior of a robot is dictated by its own dynamics and\nobjective that also depends on others' behavior. In the forward problem, D3G\nenables all robots collaboratively to seek the Nash equilibrium of the game in\na distributed manner, by developing a distributed shooting-based Nash solver.\nIn the inverse problem, where each robot aims to find (learn) its objective\n(and dynamics) parameters to mimic given coordination demonstrations, D3G\nproposes a differentiation solver based on Differential Pontryagin's Maximum\nPrinciple, which allows each robot to update its parameters in a distributed\nand coordinated manner. We test the D3G in simulation with two types of robots\ngiven different task configurations. The results demonstrate the effectiveness\nof D3G for solving both forward and inverse problems in comparison with\nexisting methods.\n",
        "published": "2022",
        "authors": [
            "Xuan Wang",
            "Yizhi Zhou",
            "Wanxin Jin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.04472v2",
        "title": "Autonomous Advanced Aerial Mobility -- An End-to-end Autonomy Framework\n  for UAVs and Beyond",
        "abstract": "  Developing aerial robots that can both safely navigate and execute assigned\nmission without any human intervention - i.e., fully autonomous aerial mobility\nof passengers and goods - is the larger vision that guides the research,\ndesign, and development efforts in the aerial autonomy space. However, it is\nhighly challenging to concurrently operationalize all types of aerial vehicles\nthat are operating fully autonomously sharing the airspace. Full autonomy of\nthe aerial transportation sector includes several aspects, such as design of\nthe technology that powers the vehicles, operations of multi-agent fleets, and\nprocess of certification that meets stringent safety requirements of aviation\nsector. Thereby, Autonomous Advanced Aerial Mobility is still a vague term and\nits consequences for researchers and professionals are ambiguous. To address\nthis gap, we present a comprehensive perspective on the emerging field of\nautonomous advanced aerial mobility, which involves the use of unmanned aerial\nvehicles (UAVs) and electric vertical takeoff and landing (eVTOL) aircraft for\nvarious applications, such as urban air mobility, package delivery, and\nsurveillance. The article proposes a scalable and extensible autonomy framework\nconsisting of four main blocks: sensing, perception, planning, and controls.\nFurthermore, the article discusses the challenges and opportunities in\nmulti-agent fleet operations and management, as well as the testing,\nvalidation, and certification aspects of autonomous aerial systems. Finally,\nthe article explores the potential of monolithic models for aerial autonomy and\nanalyzes their advantages and limitations. The perspective aims to provide a\nholistic picture of the autonomous advanced aerial mobility field and its\nfuture directions.\n",
        "published": "2023",
        "authors": [
            "Sakshi Mishra",
            "Praveen Palanisamy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.10658v2",
        "title": "Decentralized Safe Multi-agent Stochastic Optimal Control using Deep\n  FBSDEs and ADMM",
        "abstract": "  In this work, we propose a novel safe and scalable decentralized solution for\nmulti-agent control in the presence of stochastic disturbances. Safety is\nmathematically encoded using stochastic control barrier functions and safe\ncontrols are computed by solving quadratic programs. Decentralization is\nachieved by augmenting to each agent's optimization variables, copy variables,\nfor its neighbors. This allows us to decouple the centralized multi-agent\noptimization problem. However, to ensure safety, neighboring agents must agree\non \"what is safe for both of us\" and this creates a need for consensus. To\nenable safe consensus solutions, we incorporate an ADMM-based approach.\nSpecifically, we propose a Merged CADMM-OSQP implicit neural network layer,\nthat solves a mini-batch of both, local quadratic programs as well as the\noverall consensus problem, as a single optimization problem. This layer is\nembedded within a Deep FBSDEs network architecture at every time step, to\nfacilitate end-to-end differentiable, safe and decentralized stochastic optimal\ncontrol. The efficacy of the proposed approach is demonstrated on several\nchallenging multi-robot tasks in simulation. By imposing requirements on safety\nspecified by collision avoidance constraints, the safe operation of all agents\nis ensured during the entire training process. We also demonstrate superior\nscalability in terms of computational and memory savings as compared to a\ncentralized approach.\n",
        "published": "2022",
        "authors": [
            "Marcus A. Pereira",
            "Augustinos D. Saravanos",
            "Oswin So",
            "Evangelos A. Theodorou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.01777v2",
        "title": "A Framework for Real-World Multi-Robot Systems Running Decentralized\n  GNN-Based Policies",
        "abstract": "  GNNs are a paradigm-shifting neural architecture to facilitate the learning\nof complex multi-agent behaviors. Recent work has demonstrated remarkable\nperformance in tasks such as flocking, multi-agent path planning and\ncooperative coverage. However, the policies derived through GNN-based learning\nschemes have not yet been deployed to the real-world on physical multi-robot\nsystems. In this work, we present the design of a system that allows for fully\ndecentralized execution of GNN-based policies. We create a framework based on\nROS2 and elaborate its details in this paper. We demonstrate our framework on a\ncase-study that requires tight coordination between robots, and present\nfirst-of-a-kind results that show successful real-world deployment of GNN-based\npolicies on a decentralized multi-robot system relying on Adhoc communication.\nA video demonstration of this case-study, as well as the accompanying source\ncode repository, can be found online.\nhttps://www.youtube.com/watch?v=COh-WLn4iO4\nhttps://github.com/proroklab/ros2_multi_agent_passage\nhttps://github.com/proroklab/rl_multi_agent_passage\n",
        "published": "2021",
        "authors": [
            "Jan Blumenkamp",
            "Steven Morad",
            "Jennifer Gielis",
            "Qingbiao Li",
            "Amanda Prorok"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.00759v5",
        "title": "See What the Robot Can't See: Learning Cooperative Perception for Visual\n  Navigation",
        "abstract": "  We consider the problem of navigating a mobile robot towards a target in an\nunknown environment that is endowed with visual sensors, where neither the\nrobot nor the sensors have access to global positioning information and only\nuse first-person-view images. In order to overcome the need for positioning, we\ntrain the sensors to encode and communicate relevant viewpoint information to\nthe mobile robot, whose objective it is to use this information to navigate to\nthe target along the shortest path. We overcome the challenge of enabling all\nthe sensors (even those that cannot directly see the target) to predict the\ndirection along the shortest path to the target by implementing a\nneighborhood-based feature aggregation module using a Graph Neural Network\n(GNN) architecture. In our experiments, we first demonstrate generalizability\nto previously unseen environments with various sensor layouts. Our results show\nthat by using communication between the sensors and the robot, we achieve up to\n2.0x improvement in SPL (Success weighted by Path Length) when compared to a\ncommunication-free baseline. This is done without requiring a global map,\npositioning data, nor pre-calibration of the sensor network. Second, we perform\na zero-shot transfer of our model from simulation to the real world. Laboratory\nexperiments demonstrate the feasibility of our approach in various cluttered\nenvironments. Finally, we showcase examples of successful navigation to the\ntarget while both the sensor network layout as well as obstacles are\ndynamically reconfigured as the robot navigates. We provide a video demo, the\ndataset, trained models, and source code.\n  https://www.youtube.com/watch?v=kcmr6RUgucw\nhttps://github.com/proroklab/sensor-guided-visual-nav\n",
        "published": "2022",
        "authors": [
            "Jan Blumenkamp",
            "Qingbiao Li",
            "Binyu Wang",
            "Zhe Liu",
            "Amanda Prorok"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.08230v2",
        "title": "A Robust and Constrained Multi-Agent Reinforcement Learning Electric\n  Vehicle Rebalancing Method in AMoD Systems",
        "abstract": "  Electric vehicles (EVs) play critical roles in autonomous mobility-on-demand\n(AMoD) systems, but their unique charging patterns increase the model\nuncertainties in AMoD systems (e.g. state transition probability). Since there\nusually exists a mismatch between the training and test/true environments,\nincorporating model uncertainty into system design is of critical importance in\nreal-world applications. However, model uncertainties have not been considered\nexplicitly in EV AMoD system rebalancing by existing literature yet, and the\ncoexistence of model uncertainties and constraints that the decision should\nsatisfy makes the problem even more challenging. In this work, we design a\nrobust and constrained multi-agent reinforcement learning (MARL) framework with\nstate transition kernel uncertainty for EV AMoD systems. We then propose a\nrobust and constrained MARL algorithm (ROCOMA) with robust natural policy\ngradients (RNPG) that trains a robust EV rebalancing policy to balance the\nsupply-demand ratio and the charging utilization rate across the city under\nmodel uncertainty. Experiments show that the ROCOMA can learn an effective and\nrobust rebalancing policy. It outperforms non-robust MARL methods in the\npresence of model uncertainties. It increases the system fairness by 19.6% and\ndecreases the rebalancing costs by 75.8%.\n",
        "published": "2022",
        "authors": [
            "Sihong He",
            "Yue Wang",
            "Shuo Han",
            "Shaofeng Zou",
            "Fei Miao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.09946v1",
        "title": "An Adaptive Fuzzy Reinforcement Learning Cooperative Approach for the\n  Autonomous Control of Flock Systems",
        "abstract": "  The flock-guidance problem enjoys a challenging structure where multiple\noptimization objectives are solved simultaneously. This usually necessitates\ndifferent control approaches to tackle various objectives, such as guidance,\ncollision avoidance, and cohesion. The guidance schemes, in particular, have\nlong suffered from complex tracking-error dynamics. Furthermore, techniques\nthat are based on linear feedback strategies obtained at equilibrium conditions\neither may not hold or degrade when applied to uncertain dynamic environments.\nPre-tuned fuzzy inference architectures lack robustness under such unmodeled\nconditions. This work introduces an adaptive distributed technique for the\nautonomous control of flock systems. Its relatively flexible structure is based\non online fuzzy reinforcement learning schemes which simultaneously target a\nnumber of objectives; namely, following a leader, avoiding collision, and\nreaching a flock velocity consensus. In addition to its resilience in the face\nof dynamic disturbances, the algorithm does not require more than the agent\nposition as a feedback signal. The effectiveness of the proposed method is\nvalidated with two simulation scenarios and benchmarked against a similar\ntechnique from the literature.\n",
        "published": "2023",
        "authors": [
            "Shuzheng Qu",
            "Mohammed Abouheaf",
            "Wail Gueaieb",
            "Davide Spinello"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.10035v1",
        "title": "A Policy Iteration Approach for Flock Motion Control",
        "abstract": "  The flocking motion control is concerned with managing the possible conflicts\nbetween local and team objectives of multi-agent systems. The overall control\nprocess guides the agents while monitoring the flock-cohesiveness and\nlocalization. The underlying mechanisms may degrade due to overlooking the\nunmodeled uncertainties associated with the flock dynamics and formation. On\nanother side, the efficiencies of the various control designs rely on how\nquickly they can adapt to different dynamic situations in real-time. An online\nmodel-free policy iteration mechanism is developed here to guide a flock of\nagents to follow an independent command generator over a time-varying graph\ntopology. The strength of connectivity between any two agents or the graph edge\nweight is decided using a position adjacency dependent function. An online\nrecursive least squares approach is adopted to tune the guidance strategies\nwithout knowing the dynamics of the agents or those of the command generator.\nIt is compared with another reinforcement learning approach from the literature\nwhich is based on a value iteration technique. The simulation results of the\npolicy iteration mechanism revealed fast learning and convergence behaviors\nwith less computational effort.\n",
        "published": "2023",
        "authors": [
            "Shuzheng Qu",
            "Mohammed Abouheaf",
            "Wail Gueaieb",
            "Davide Spinello"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.11260v1",
        "title": "Constrained Environment Optimization for Prioritized Multi-Agent\n  Navigation",
        "abstract": "  Traditional approaches to the design of multi-agent navigation algorithms\nconsider the environment as a fixed constraint, despite the influence of\nspatial constraints on agents' performance. Yet hand-designing conducive\nenvironment layouts is inefficient and potentially expensive. The goal of this\npaper is to consider the environment as a decision variable in a system-level\noptimization problem, where both agent performance and environment cost are\nincorporated. Towards this end, we propose novel problems of unprioritized and\nprioritized environment optimization, where the former considers agents\nunbiasedly and the latter accounts for agent priorities. We show, through\nformal proofs, under which conditions the environment can change while\nguaranteeing completeness (i.e., all agents reach goals), and analyze the role\nof agent priorities in the environment optimization. We proceed to impose\nreal-world constraints on the environment optimization and formulate it\nmathematically as a constrained stochastic optimization problem. Since the\nrelation between agents, environment and performance is challenging to model,\nwe leverage reinforcement learning to develop a model-free solution and a\nprimal-dual mechanism to handle constraints. Distinct information processing\narchitectures are integrated for various implementation scenarios, including\nonline/offline optimization and discrete/continuous environment. Numerical\nresults corroborate the theory and demonstrate the validity and adaptability of\nour approach.\n",
        "published": "2023",
        "authors": [
            "Zhan Gao",
            "Amanda Prorok"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.10657v2",
        "title": "Learning Adaptive Safety for Multi-Agent Systems",
        "abstract": "  Ensuring safety in dynamic multi-agent systems is challenging due to limited\ninformation about the other agents. Control Barrier Functions (CBFs) are\nshowing promise for safety assurance but current methods make strong\nassumptions about other agents and often rely on manual tuning to balance\nsafety, feasibility, and performance. In this work, we delve into the problem\nof adaptive safe learning for multi-agent systems with CBF. We show how\nemergent behavior can be profoundly influenced by the CBF configuration,\nhighlighting the necessity for a responsive and dynamic approach to CBF design.\nWe present ASRL, a novel adaptive safe RL framework, to fully automate the\noptimization of policy and CBF coefficients, to enhance safety and long-term\nperformance through reinforcement learning. By directly interacting with the\nother agents, ASRL learns to cope with diverse agent behaviours and maintains\nthe cost violations below a desired limit. We evaluate ASRL in a multi-robot\nsystem and a competitive multi-agent racing scenario, against learning-based\nand control-theoretic approaches. We empirically demonstrate the efficacy and\nflexibility of ASRL, and assess generalization and scalability to\nout-of-distribution scenarios. Code and supplementary material are public\nonline.\n",
        "published": "2023",
        "authors": [
            "Luigi Berducci",
            "Shuo Yang",
            "Rahul Mangharam",
            "Radu Grosu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/cs/0410058v1",
        "title": "Robust Dialogue Understanding in HERALD",
        "abstract": "  We tackle the problem of robust dialogue processing from the perspective of\nlanguage engineering. We propose an agent-oriented architecture that allows us\na flexible way of composing robust processors. Our approach is based on\nShoham's Agent Oriented Programming (AOP) paradigm. We will show how the AOP\nagent model can be enriched with special features and components that allow us\nto deal with classical problems of dialogue understanding.\n",
        "published": "2004",
        "authors": [
            "Vincenzo Pallotta",
            "Afzal Ballim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/cs/0602018v1",
        "title": "Improving the CSIEC Project and Adapting It to the English Teaching and\n  Learning in China",
        "abstract": "  In this paper after short review of the CSIEC project initialized by us in\n2003 we present the continuing development and improvement of the CSIEC project\nin details, including the design of five new Microsoft agent characters\nrepresenting different virtual chatting partners and the limitation of\nsimulated dialogs in specific practical scenarios like graduate job application\ninterview, then briefly analyze the actual conditions and features of its\napplication field: web-based English education in China. Finally we introduce\nour efforts to adapt this system to the requirements of English teaching and\nlearning in China and point out the work next to do.\n",
        "published": "2006",
        "authors": [
            "Jiyou Jia",
            "Shufen Hou",
            "Weichao Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.03604v1",
        "title": "Conversational AI: The Science Behind the Alexa Prize",
        "abstract": "  Conversational agents are exploding in popularity. However, much work remains\nin the area of social conversation as well as free-form conversation over a\nbroad range of domains and topics. To advance the state of the art in\nconversational AI, Amazon launched the Alexa Prize, a 2.5-million-dollar\nuniversity competition where sixteen selected university teams were challenged\nto build conversational agents, known as socialbots, to converse coherently and\nengagingly with humans on popular topics such as Sports, Politics,\nEntertainment, Fashion and Technology for 20 minutes. The Alexa Prize offers\nthe academic community a unique opportunity to perform research with a live\nsystem used by millions of users. The competition provided university teams\nwith real user conversational data at scale, along with the user-provided\nratings and feedback augmented with annotations by the Alexa team. This enabled\nteams to effectively iterate and make improvements throughout the competition\nwhile being evaluated in real-time through live user interactions. To build\ntheir socialbots, university teams combined state-of-the-art techniques with\nnovel strategies in the areas of Natural Language Understanding, Context\nModeling, Dialog Management, Response Generation, and Knowledge Acquisition. To\nsupport the efforts of participating teams, the Alexa Prize team made\nsignificant scientific and engineering investments to build and improve\nConversational Speech Recognition, Topic Tracking, Dialog Evaluation, Voice\nUser Experience, and tools for traffic management and scalability. This paper\noutlines the advances created by the university teams as well as the Alexa\nPrize team to achieve the common goal of solving the problem of Conversational\nAI.\n",
        "published": "2018",
        "authors": [
            "Ashwin Ram",
            "Rohit Prasad",
            "Chandra Khatri",
            "Anu Venkatesh",
            "Raefer Gabriel",
            "Qing Liu",
            "Jeff Nunn",
            "Behnam Hedayatnia",
            "Ming Cheng",
            "Ashish Nagar",
            "Eric King",
            "Kate Bland",
            "Amanda Wartick",
            "Yi Pan",
            "Han Song",
            "Sk Jayadevan",
            "Gene Hwang",
            "Art Pettigrue"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.03622v1",
        "title": "Topic-based Evaluation for Conversational Bots",
        "abstract": "  Dialog evaluation is a challenging problem, especially for non task-oriented\ndialogs where conversational success is not well-defined. We propose to\nevaluate dialog quality using topic-based metrics that describe the ability of\na conversational bot to sustain coherent and engaging conversations on a topic,\nand the diversity of topics that a bot can handle. To detect conversation\ntopics per utterance, we adopt Deep Average Networks (DAN) and train a topic\nclassifier on a variety of question and query data categorized into multiple\ntopics. We propose a novel extension to DAN by adding a topic-word attention\ntable that allows the system to jointly capture topic keywords in an utterance\nand perform topic classification. We compare our proposed topic based metrics\nwith the ratings provided by users and show that our metrics both correlate\nwith and complement human judgment. Our analysis is performed on tens of\nthousands of real human-bot dialogs from the Alexa Prize competition and\nhighlights user expectations for conversational bots.\n",
        "published": "2018",
        "authors": [
            "Fenfei Guo",
            "Angeliki Metallinou",
            "Chandra Khatri",
            "Anirudh Raju",
            "Anu Venkatesh",
            "Ashwin Ram"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.03625v2",
        "title": "On Evaluating and Comparing Open Domain Dialog Systems",
        "abstract": "  Conversational agents are exploding in popularity. However, much work remains\nin the area of non goal-oriented conversations, despite significant growth in\nresearch interest over recent years. To advance the state of the art in\nconversational AI, Amazon launched the Alexa Prize, a 2.5-million dollar\nuniversity competition where sixteen selected university teams built\nconversational agents to deliver the best social conversational experience.\nAlexa Prize provided the academic community with the unique opportunity to\nperform research with a live system used by millions of users. The subjectivity\nassociated with evaluating conversations is key element underlying the\nchallenge of building non-goal oriented dialogue systems. In this paper, we\npropose a comprehensive evaluation strategy with multiple metrics designed to\nreduce subjectivity by selecting metrics which correlate well with human\njudgement. The proposed metrics provide granular analysis of the conversational\nagents, which is not captured in human ratings. We show that these metrics can\nbe used as a reasonable proxy for human judgment. We provide a mechanism to\nunify the metrics for selecting the top performing agents, which has also been\napplied throughout the Alexa Prize competition. To our knowledge, to date it is\nthe largest setting for evaluating agents with millions of conversations and\nhundreds of thousands of ratings from users. We believe that this work is a\nstep towards an automatic evaluation process for conversational AIs.\n",
        "published": "2018",
        "authors": [
            "Anu Venkatesh",
            "Chandra Khatri",
            "Ashwin Ram",
            "Fenfei Guo",
            "Raefer Gabriel",
            "Ashish Nagar",
            "Rohit Prasad",
            "Ming Cheng",
            "Behnam Hedayatnia",
            "Angeliki Metallinou",
            "Rahul Goel",
            "Shaohua Yang",
            "Anirudh Raju"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.09293v1",
        "title": "DREAMT -- Embodied Motivational Conversational Storytelling",
        "abstract": "  Storytelling is fundamental to language, including culture, conversation and\ncommunication in their broadest senses. It thus emerges as an essential\ncomponent of intelligent systems, including systems where natural language is\nnot a primary focus or where we do not usually think of a story being involved.\nIn this paper we explore the emergence of storytelling as a requirement in\nembodied conversational agents, including its role in educational and health\ninterventions, as well as in a general-purpose computer interface for people\nwith disabilities or other constraints that prevent the use of traditional\nkeyboard and speech interfaces. We further present a characterization of\nstorytelling as an inventive fleshing out of detail according to a particular\npersonal perspective, and propose the DREAMT model to focus attention on the\ndifferent layers that need to be present in a character-driven storytelling\nsystem. Most if not all aspects of the DREAMT model have arisen from or been\nexplored in some aspect of our implemented research systems, but currently only\nat a primitive and relatively unintegrated level. However, this experience\nleads us to formalize and elaborate the DREAMT model mnemonically as follows: -\nDescription/Dialogue/Definition/Denotation - Realization/Representation/Role -\nExplanation/Education/Entertainment - Actualization/Activation -\nMotivation/Modelling - Topicalization/Transformation\n",
        "published": "2019",
        "authors": [
            "David M W Powers"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.02356v1",
        "title": "Multimodal Machine Translation with Reinforcement Learning",
        "abstract": "  Multimodal machine translation is one of the applications that integrates\ncomputer vision and language processing. It is a unique task given that in the\nfield of machine translation, many state-of-the-arts algorithms still only\nemploy textual information. In this work, we explore the effectiveness of\nreinforcement learning in multimodal machine translation. We present a novel\nalgorithm based on the Advantage Actor-Critic (A2C) algorithm that specifically\ncater to the multimodal machine translation task of the EMNLP 2018 Third\nConference on Machine Translation (WMT18). We experiment our proposed algorithm\non the Multi30K multilingual English-German image description dataset and the\nFlickr30K image entity dataset. Our model takes two channels of inputs, image\nand text, uses translation evaluation metrics as training rewards, and achieves\nbetter results than supervised learning MLE baseline models. Furthermore, we\ndiscuss the prospects and limitations of using reinforcement learning for\nmachine translation. Our experiment results suggest a promising reinforcement\nlearning solution to the general task of multimodal sequence to sequence\nlearning.\n",
        "published": "2018",
        "authors": [
            "Xin Qian",
            "Ziyi Zhong",
            "Jieli Zhou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.04958v1",
        "title": "Hallmarks of Human-Machine Collaboration: A framework for assessment in\n  the DARPA Communicating with Computers Program",
        "abstract": "  There is a growing desire to create computer systems that can communicate\neffectively to collaborate with humans on complex, open-ended activities.\nAssessing these systems presents significant challenges. We describe a\nframework for evaluating systems engaged in open-ended complex scenarios where\nevaluators do not have the luxury of comparing performance to a single right\nanswer. This framework has been used to evaluate human-machine creative\ncollaborations across story and music generation, interactive block building,\nand exploration of molecular mechanisms in cancer. These activities are\nfundamentally different from the more constrained tasks performed by most\ncontemporary personal assistants as they are generally open-ended, with no\nsingle correct solution, and often no obvious completion criteria.\n  We identified the Key Properties that must be exhibited by successful\nsystems. From there we identified \"Hallmarks\" of success -- capabilities and\nfeatures that evaluators can observe that would be indicative of progress\ntoward achieving a Key Property. In addition to being a framework for\nassessment, the Key Properties and Hallmarks are intended to serve as goals in\nguiding research direction.\n",
        "published": "2021",
        "authors": [
            "Robyn Kozierok",
            "John Aberdeen",
            "Cheryl Clark",
            "Christopher Garay",
            "Bradley Goodman",
            "Tonia Korves",
            "Lynette Hirschman",
            "Patricia L. McDermott",
            "Matthew W. Peterson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.06737v1",
        "title": "Natural-Language Multi-Agent Simulations of Argumentative Opinion\n  Dynamics",
        "abstract": "  This paper develops a natural-language agent-based model of argumentation\n(ABMA). Its artificial deliberative agents (ADAs) are constructed with the help\nof so-called neural language models recently developed in AI and computational\nlinguistics. ADAs are equipped with a minimalist belief system and may generate\nand submit novel contributions to a conversation. The natural-language ABMA\nallows us to simulate collective deliberation in English, i.e. with arguments,\nreasons, and claims themselves -- rather than with their mathematical\nrepresentations (as in formal models). This paper uses the natural-language\nABMA to test the robustness of formal reason-balancing models of argumentation\n[Maes & Flache 2013, Singer et al. 2019]: First of all, as long as ADAs remain\npassive, confirmation bias and homophily updating trigger polarization, which\nis consistent with results from formal models. However, once ADAs start to\nactively generate new contributions, the evolution of a conservation is\ndominated by properties of the agents *as authors*. This suggests that the\ncreation of new arguments, reasons, and claims critically affects a\nconversation and is of pivotal importance for understanding the dynamics of\ncollective deliberation. The paper closes by pointing out further fruitful\napplications of the model and challenges for future research.\n",
        "published": "2021",
        "authors": [
            "Gregor Betz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.10996v1",
        "title": "CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling",
        "abstract": "  In real-world applications of natural language generation, there are often\nconstraints on the target sentences in addition to fluency and naturalness\nrequirements. Existing language generation techniques are usually based on\nrecurrent neural networks (RNNs). However, it is non-trivial to impose\nconstraints on RNNs while maintaining generation quality, since RNNs generate\nsentences sequentially (or with beam search) from the first word to the last.\nIn this paper, we propose CGMH, a novel approach using Metropolis-Hastings\nsampling for constrained sentence generation. CGMH allows complicated\nconstraints such as the occurrence of multiple keywords in the target\nsentences, which cannot be handled in traditional RNN-based approaches.\nMoreover, CGMH works in the inference stage, and does not require parallel\ncorpora for training. We evaluate our method on a variety of tasks, including\nkeywords-to-sentence generation, unsupervised sentence paraphrasing, and\nunsupervised sentence error correction. CGMH achieves high performance compared\nwith previous supervised methods for sentence generation. Our code is released\nat https://github.com/NingMiao/CGMH\n",
        "published": "2018",
        "authors": [
            "Ning Miao",
            "Hao Zhou",
            "Lili Mou",
            "Rui Yan",
            "Lei Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.04023v4",
        "title": "On the Possibility of Rewarding Structure Learning Agents: Mutual\n  Information on Linguistic Random Sets",
        "abstract": "  We present a first attempt to elucidate a theoretical and empirical approach\nto design the reward provided by a natural language environment to some\nstructure learning agent. To this end, we revisit the Information Theory of\nunsupervised induction of phrase-structure grammars to characterize the\nbehavior of simulated actions modeled as set-valued random variables (random\nsets of linguistic samples) constituting semantic structures. Our results\nshowed empirical evidence of that simulated semantic structures (Open\nInformation Extraction triplets) can be distinguished from randomly constructed\nones by observing the Mutual Information among their constituents. This\nsuggests the possibility of rewarding structure learning agents without using\npretrained structural analyzers (oracle actors/experts).\n",
        "published": "2019",
        "authors": [
            "Ignacio Arroyo-Fern\u00e1ndez",
            "Mauricio Carrasco-Ru\u00edz",
            "J. Anibal Arias-Aguilar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.09265v2",
        "title": "IsarStep: a Benchmark for High-level Mathematical Reasoning",
        "abstract": "  A well-defined benchmark is essential for measuring and accelerating research\nprogress of machine learning models. In this paper, we present a benchmark for\nhigh-level mathematical reasoning and study the reasoning capabilities of\nneural sequence-to-sequence models. We build a non-synthetic dataset from the\nlargest repository of proofs written by human experts in a theorem prover. The\ndataset has a broad coverage of undergraduate and research-level mathematical\nand computer science theorems. In our defined task, a model is required to fill\nin a missing intermediate proposition given surrounding proofs. This task\nprovides a starting point for the long-term goal of having machines generate\nhuman-readable proofs automatically. Our experiments and analysis reveal that\nwhile the task is challenging, neural models can capture non-trivial\nmathematical reasoning. We further design a hierarchical transformer that\noutperforms the transformer baseline.\n",
        "published": "2020",
        "authors": [
            "Wenda Li",
            "Lei Yu",
            "Yuhuai Wu",
            "Lawrence C. Paulson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.10188v7",
        "title": "Modeling Psychotherapy Dialogues with Kernelized Hashcode\n  Representations: A Nonparametric Information-Theoretic Approach",
        "abstract": "  We propose a novel dialogue modeling framework, the first-ever nonparametric\nkernel functions based approach for dialogue modeling, which learns kernelized\nhashcodes as compressed text representations; unlike traditional deep learning\nmodels, it handles well relatively small datasets, while also scaling to large\nones. We also derive a novel lower bound on mutual information, used as a\nmodel-selection criterion favoring representations with better alignment\nbetween the utterances of participants in a collaborative dialogue setting, as\nwell as higher predictability of the generated responses. As demonstrated on\nthree real-life datasets, including prominently psychotherapy sessions, the\nproposed approach significantly outperforms several state-of-art neural network\nbased dialogue systems, both in terms of computational efficiency, reducing\ntraining time from days or weeks to hours, and the response quality, achieving\nan order of magnitude improvement over competitors in frequency of being chosen\nas the best model by human evaluators.\n",
        "published": "2018",
        "authors": [
            "Sahil Garg",
            "Irina Rish",
            "Guillermo Cecchi",
            "Palash Goyal",
            "Sarik Ghazarian",
            "Shuyang Gao",
            "Greg Ver Steeg",
            "Aram Galstyan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.02125v2",
        "title": "Strong and Simple Baselines for Multimodal Utterance Embeddings",
        "abstract": "  Human language is a rich multimodal signal consisting of spoken words, facial\nexpressions, body gestures, and vocal intonations. Learning representations for\nthese spoken utterances is a complex research problem due to the presence of\nmultiple heterogeneous sources of information. Recent advances in multimodal\nlearning have followed the general trend of building more complex models that\nutilize various attention, memory and recurrent components. In this paper, we\npropose two simple but strong baselines to learn embeddings of multimodal\nutterances. The first baseline assumes a conditional factorization of the\nutterance into unimodal factors. Each unimodal factor is modeled using the\nsimple form of a likelihood function obtained via a linear transformation of\nthe embedding. We show that the optimal embedding can be derived in closed form\nby taking a weighted average of the unimodal features. In order to capture\nricher representations, our second baseline extends the first by factorizing\ninto unimodal, bimodal, and trimodal factors, while retaining simplicity and\nefficiency during learning and inference. From a set of experiments across two\ntasks, we show strong performance on both supervised and semi-supervised\nmultimodal prediction, as well as significant (10 times) speedups over neural\nmodels during inference. Overall, we believe that our strong baseline models\noffer new benchmarking options for future research in multimodal learning.\n",
        "published": "2019",
        "authors": [
            "Paul Pu Liang",
            "Yao Chong Lim",
            "Yao-Hung Hubert Tsai",
            "Ruslan Salakhutdinov",
            "Louis-Philippe Morency"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.03881v1",
        "title": "Nearly-Unsupervised Hashcode Representations for Relation Extraction",
        "abstract": "  Recently, kernelized locality sensitive hashcodes have been successfully\nemployed as representations of natural language text, especially showing high\nrelevance to biomedical relation extraction tasks. In this paper, we propose to\noptimize the hashcode representations in a nearly unsupervised manner, in which\nwe only use data points, but not their class labels, for learning. The\noptimized hashcode representations are then fed to a supervised classifier\nfollowing the prior work. This nearly unsupervised approach allows fine-grained\noptimization of each hash function, which is particularly suitable for building\nhashcode representations generalizing from a training set to a test set. We\nempirically evaluate the proposed approach for biomedical relation extraction\ntasks, obtaining significant accuracy improvements w.r.t. state-of-the-art\nsupervised and semi-supervised approaches.\n",
        "published": "2019",
        "authors": [
            "Sahil Garg",
            "Aram Galstyan",
            "Greg Ver Steeg",
            "Guillermo Cecchi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.03869v1",
        "title": "Pre-Training With Scientific Text Improves Educational Question\n  Generation",
        "abstract": "  With the boom of digital educational materials and scalable e-learning\nsystems, the potential for realising AI-assisted personalised learning has\nskyrocketed. In this landscape, the automatic generation of educational\nquestions will play a key role, enabling scalable self-assessment when a global\npopulation is manoeuvring their personalised learning journeys. We develop\nEduQG, a novel educational question generation model built by adapting a large\nlanguage model. Our initial experiments demonstrate that EduQG can produce\nsuperior educational questions by pre-training on scientific text.\n",
        "published": "2022",
        "authors": [
            "Hamze Muse",
            "Sahan Bulathwela",
            "Emine Yilmaz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.04637v2",
        "title": "Transformers as Statisticians: Provable In-Context Learning with\n  In-Context Algorithm Selection",
        "abstract": "  Neural sequence models based on the transformer architecture have\ndemonstrated remarkable \\emph{in-context learning} (ICL) abilities, where they\ncan perform new tasks when prompted with training and test examples, without\nany parameter update to the model. This work first provides a comprehensive\nstatistical theory for transformers to perform ICL. Concretely, we show that\ntransformers can implement a broad class of standard machine learning\nalgorithms in context, such as least squares, ridge regression, Lasso, learning\ngeneralized linear models, and gradient descent on two-layer neural networks,\nwith near-optimal predictive power on various in-context data distributions.\nUsing an efficient implementation of in-context gradient descent as the\nunderlying mechanism, our transformer constructions admit mild size bounds, and\ncan be learned with polynomially many pretraining sequences.\n  Building on these ``base'' ICL algorithms, intriguingly, we show that\ntransformers can implement more complex ICL procedures involving\n\\emph{in-context algorithm selection}, akin to what a statistician can do in\nreal life -- A \\emph{single} transformer can adaptively select different base\nICL algorithms -- or even perform qualitatively different tasks -- on different\ninput sequences, without any explicit prompting of the right algorithm or task.\nWe both establish this in theory by explicit constructions, and also observe\nthis phenomenon experimentally. In theory, we construct two general mechanisms\nfor algorithm selection with concrete examples: pre-ICL testing, and post-ICL\nvalidation. As an example, we use the post-ICL validation mechanism to\nconstruct a transformer that can perform nearly Bayes-optimal ICL on a\nchallenging task -- noisy linear models with mixed noise levels.\nExperimentally, we demonstrate the strong in-context algorithm selection\ncapabilities of standard transformer architectures.\n",
        "published": "2023",
        "authors": [
            "Yu Bai",
            "Fan Chen",
            "Huan Wang",
            "Caiming Xiong",
            "Song Mei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.08566v1",
        "title": "Transformers as Decision Makers: Provable In-Context Reinforcement\n  Learning via Supervised Pretraining",
        "abstract": "  Large transformer models pretrained on offline reinforcement learning\ndatasets have demonstrated remarkable in-context reinforcement learning (ICRL)\ncapabilities, where they can make good decisions when prompted with interaction\ntrajectories from unseen environments. However, when and how transformers can\nbe trained to perform ICRL have not been theoretically well-understood. In\nparticular, it is unclear which reinforcement-learning algorithms transformers\ncan perform in context, and how distribution mismatch in offline training data\naffects the learned algorithms. This paper provides a theoretical framework\nthat analyzes supervised pretraining for ICRL. This includes two recently\nproposed training methods -- algorithm distillation and decision-pretrained\ntransformers. First, assuming model realizability, we prove the\nsupervised-pretrained transformer will imitate the conditional expectation of\nthe expert algorithm given the observed trajectory. The generalization error\nwill scale with model capacity and a distribution divergence factor between the\nexpert and offline algorithms. Second, we show transformers with ReLU attention\ncan efficiently approximate near-optimal online reinforcement learning\nalgorithms like LinUCB and Thompson sampling for stochastic linear bandits, and\nUCB-VI for tabular Markov decision processes. This provides the first\nquantitative analysis of the ICRL capabilities of transformers pretrained from\noffline trajectories.\n",
        "published": "2023",
        "authors": [
            "Licong Lin",
            "Yu Bai",
            "Song Mei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.8419v3",
        "title": "Simple Image Description Generator via a Linear Phrase-Based Approach",
        "abstract": "  Generating a novel textual description of an image is an interesting problem\nthat connects computer vision and natural language processing. In this paper,\nwe present a simple model that is able to generate descriptive sentences given\na sample image. This model has a strong focus on the syntax of the\ndescriptions. We train a purely bilinear model that learns a metric between an\nimage representation (generated from a previously trained Convolutional Neural\nNetwork) and phrases that are used to described them. The system is then able\nto infer phrases from a given image sample. Based on caption syntax statistics,\nwe propose a simple language model that can produce relevant descriptions for a\ngiven test image using the phrases inferred. Our approach, which is\nconsiderably simpler than state-of-the-art models, achieves comparable results\non the recently release Microsoft COCO dataset.\n",
        "published": "2014",
        "authors": [
            "Remi Lebret",
            "Pedro O. Pinheiro",
            "Ronan Collobert"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.09137v2",
        "title": "Where to put the Image in an Image Caption Generator",
        "abstract": "  When a recurrent neural network language model is used for caption\ngeneration, the image information can be fed to the neural network either by\ndirectly incorporating it in the RNN -- conditioning the language model by\n`injecting' image features -- or in a layer following the RNN -- conditioning\nthe language model by `merging' image features. While both options are attested\nin the literature, there is as yet no systematic comparison between the two. In\nthis paper we empirically show that it is not especially detrimental to\nperformance whether one architecture is used or another. The merge architecture\ndoes have practical advantages, as conditioning by merging allows the RNN's\nhidden state vector to shrink in size by up to four times. Our results suggest\nthat the visual and linguistic modalities for caption generation need not be\njointly encoded by the RNN as that yields large, memory-intensive models with\nfew tangible advantages in performance; rather, the multimodal integration\nshould be delayed to a subsequent stage.\n",
        "published": "2017",
        "authors": [
            "Marc Tanti",
            "Albert Gatt",
            "Kenneth P. Camilleri"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.09872v2",
        "title": "Full-Network Embedding in a Multimodal Embedding Pipeline",
        "abstract": "  The current state-of-the-art for image annotation and image retrieval tasks\nis obtained through deep neural networks, which combine an image representation\nand a text representation into a shared embedding space. In this paper we\nevaluate the impact of using the Full-Network embedding in this setting,\nreplacing the original image representation in a competitive multimodal\nembedding generation scheme. Unlike the one-layer image embeddings typically\nused by most approaches, the Full-Network embedding provides a multi-scale\nrepresentation of images, which results in richer characterizations. To measure\nthe influence of the Full-Network embedding, we evaluate its performance on\nthree different datasets, and compare the results with the original multimodal\nembedding generation scheme when using a one-layer image embedding, and with\nthe rest of the state-of-the-art. Results for image annotation and image\nretrieval tasks indicate that the Full-Network embedding is consistently\nsuperior to the one-layer embedding. These results motivate the integration of\nthe Full-Network embedding on any multimodal embedding generation scheme,\nsomething feasible thanks to the flexibility of the approach.\n",
        "published": "2017",
        "authors": [
            "Armand Vilalta",
            "Dario Garcia-Gasulla",
            "Ferran Par\u00e9s",
            "Eduard Ayguad\u00e9",
            "Jesus Labarta",
            "Ulises Cort\u00e9s",
            "Toyotaro Suzumura"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.14264v1",
        "title": "Self-Segregating and Coordinated-Segregating Transformer for Focused\n  Deep Multi-Modular Network for Visual Question Answering",
        "abstract": "  Attention mechanism has gained huge popularity due to its effectiveness in\nachieving high accuracy in different domains. But attention is opportunistic\nand is not justified by the content or usability of the content. Transformer\nlike structure creates all/any possible attention(s). We define segregating\nstrategies that can prioritize the contents for the applications for\nenhancement of performance. We defined two strategies: Self-Segregating\nTransformer (SST) and Coordinated-Segregating Transformer (CST) and used it to\nsolve visual question answering application. Self-segregation strategy for\nattention contributes in better understanding and filtering the information\nthat can be most helpful for answering the question and create diversity of\nvisual-reasoning for attention. This work can easily be used in many other\napplications that involve repetition and multiple frames of features and would\nreduce the commonality of the attentions to a great extent. Visual Question\nAnswering (VQA) requires understanding and coordination of both images and\ntextual interpretations. Experiments demonstrate that segregation strategies\nfor cascaded multi-head transformer attention outperforms many previous works\nand achieved considerable improvement for VQA-v2 dataset benchmark.\n",
        "published": "2020",
        "authors": [
            "Chiranjib Sur"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1708.02043v2",
        "title": "What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption\n  Generator?",
        "abstract": "  In neural image captioning systems, a recurrent neural network (RNN) is\ntypically viewed as the primary `generation' component. This view suggests that\nthe image features should be `injected' into the RNN. This is in fact the\ndominant view in the literature. Alternatively, the RNN can instead be viewed\nas only encoding the previously generated words. This view suggests that the\nRNN should only be used to encode linguistic features and that only the final\nrepresentation should be `merged' with the image features at a later stage.\nThis paper compares these two architectures. We find that, in general, late\nmerging outperforms injection, suggesting that RNNs are better viewed as\nencoders, rather than generators.\n",
        "published": "2017",
        "authors": [
            "Marc Tanti",
            "Albert Gatt",
            "Kenneth P. Camilleri"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.10547v1",
        "title": "Using Syntax to Ground Referring Expressions in Natural Images",
        "abstract": "  We introduce GroundNet, a neural network for referring expression recognition\n-- the task of localizing (or grounding) in an image the object referred to by\na natural language expression. Our approach to this task is the first to rely\non a syntactic analysis of the input referring expression in order to inform\nthe structure of the computation graph. Given a parse tree for an input\nexpression, we explicitly map the syntactic constituents and relationships\npresent in the tree to a composed graph of neural modules that defines our\narchitecture for performing localization. This syntax-based approach aids\nlocalization of \\textit{both} the target object and auxiliary supporting\nobjects mentioned in the expression. As a result, GroundNet is more\ninterpretable than previous methods: we can (1) determine which phrase of the\nreferring expression points to which object in the image and (2) track how the\nlocalization of the target object is determined by the network. We study this\nproperty empirically by introducing a new set of annotations on the GoogleRef\ndataset to evaluate localization of supporting objects. Our experiments show\nthat GroundNet achieves state-of-the-art accuracy in identifying supporting\nobjects, while maintaining comparable performance in the localization of target\nobjects.\n",
        "published": "2018",
        "authors": [
            "Volkan Cirik",
            "Taylor Berg-Kirkpatrick",
            "Louis-Philippe Morency"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.01446v3",
        "title": "Open Sesame! Universal Black Box Jailbreaking of Large Language Models",
        "abstract": "  Large language models (LLMs), designed to provide helpful and safe responses,\noften rely on alignment techniques to align with user intent and social\nguidelines. Unfortunately, this alignment can be exploited by malicious actors\nseeking to manipulate an LLM's outputs for unintended purposes. In this paper\nwe introduce a novel approach that employs a genetic algorithm (GA) to\nmanipulate LLMs when model architecture and parameters are inaccessible. The GA\nattack works by optimizing a universal adversarial prompt that -- when combined\nwith a user's query -- disrupts the attacked model's alignment, resulting in\nunintended and potentially harmful outputs. Our novel approach systematically\nreveals a model's limitations and vulnerabilities by uncovering instances where\nits responses deviate from expected behavior. Through extensive experiments we\ndemonstrate the efficacy of our technique, thus contributing to the ongoing\ndiscussion on responsible AI development by providing a diagnostic tool for\nevaluating and enhancing alignment of LLMs with human intent. To our knowledge\nthis is the first automated universal black box jailbreak attack.\n",
        "published": "2023",
        "authors": [
            "Raz Lapid",
            "Ron Langberg",
            "Moshe Sipper"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1601.01705v4",
        "title": "Learning to Compose Neural Networks for Question Answering",
        "abstract": "  We describe a question answering model that applies to both images and\nstructured knowledge bases. The model uses natural language strings to\nautomatically assemble neural networks from a collection of composable modules.\nParameters for these modules are learned jointly with network-assembly\nparameters via reinforcement learning, with only (world, question, answer)\ntriples as supervision. Our approach, which we term a dynamic neural model\nnetwork, achieves state-of-the-art results on benchmark datasets in both visual\nand structured domains.\n",
        "published": "2016",
        "authors": [
            "Jacob Andreas",
            "Marcus Rohrbach",
            "Trevor Darrell",
            "Dan Klein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1602.04278v1",
        "title": "Signer-independent Fingerspelling Recognition with Deep Neural Network\n  Adaptation",
        "abstract": "  We study the problem of recognition of fingerspelled letter sequences in\nAmerican Sign Language in a signer-independent setting. Fingerspelled sequences\nare both challenging and important to recognize, as they are used for many\ncontent words such as proper nouns and technical terms. Previous work has shown\nthat it is possible to achieve almost 90% accuracies on fingerspelling\nrecognition in a signer-dependent setting. However, the more realistic\nsigner-independent setting presents challenges due to significant variations\namong signers, coupled with the dearth of available training data. We\ninvestigate this problem with approaches inspired by automatic speech\nrecognition. We start with the best-performing approaches from prior work,\nbased on tandem models and segmental conditional random fields (SCRFs), with\nfeatures based on deep neural network (DNN) classifiers of letters and\nphonological features. Using DNN adaptation, we find that it is possible to\nbridge a large part of the gap between signer-dependent and signer-independent\nperformance. Using only about 115 transcribed words for adaptation from the\ntarget signer, we obtain letter accuracies of up to 82.7% with frame-level\nadaptation labels and 69.7% with only word labels.\n",
        "published": "2016",
        "authors": [
            "Taehwan Kim",
            "Weiran Wang",
            "Hao Tang",
            "Karen Livescu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.01417v1",
        "title": "Dynamic Memory Networks for Visual and Textual Question Answering",
        "abstract": "  Neural network architectures with memory and attention mechanisms exhibit\ncertain reasoning capabilities required for question answering. One such\narchitecture, the dynamic memory network (DMN), obtained high accuracy on a\nvariety of language tasks. However, it was not shown whether the architecture\nachieves strong results for question answering when supporting facts are not\nmarked during training or whether it could be applied to other modalities such\nas images. Based on an analysis of the DMN, we propose several improvements to\nits memory and input modules. Together with these changes we introduce a novel\ninput module for images in order to be able to answer visual questions. Our new\nDMN+ model improves the state of the art on both the Visual Question Answering\ndataset and the \\babi-10k text question-answering dataset without supporting\nfact supervision.\n",
        "published": "2016",
        "authors": [
            "Caiming Xiong",
            "Stephen Merity",
            "Richard Socher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1504.06063v5",
        "title": "Multimodal Convolutional Neural Networks for Matching Image and Sentence",
        "abstract": "  In this paper, we propose multimodal convolutional neural networks (m-CNNs)\nfor matching image and sentence. Our m-CNN provides an end-to-end framework\nwith convolutional architectures to exploit image representation, word\ncomposition, and the matching relations between the two modalities. More\nspecifically, it consists of one image CNN encoding the image content, and one\nmatching CNN learning the joint representation of image and sentence. The\nmatching CNN composes words to different semantic fragments and learns the\ninter-modal relations between image and the composed fragments at different\nlevels, thus fully exploit the matching relations between image and sentence.\nExperimental results on benchmark databases of bidirectional image and sentence\nretrieval demonstrate that the proposed m-CNNs can effectively capture the\ninformation necessary for image and sentence matching. Specifically, our\nproposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and\nMicrosoft COCO databases achieve the state-of-the-art performances.\n",
        "published": "2015",
        "authors": [
            "Lin Ma",
            "Zhengdong Lu",
            "Lifeng Shang",
            "Hang Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1604.01485v1",
        "title": "A Focused Dynamic Attention Model for Visual Question Answering",
        "abstract": "  Visual Question and Answering (VQA) problems are attracting increasing\ninterest from multiple research disciplines. Solving VQA problems requires\ntechniques from both computer vision for understanding the visual contents of a\npresented image or video, as well as the ones from natural language processing\nfor understanding semantics of the question and generating the answers.\nRegarding visual content modeling, most of existing VQA methods adopt the\nstrategy of extracting global features from the image or video, which\ninevitably fails in capturing fine-grained information such as spatial\nconfiguration of multiple objects. Extracting features from auto-generated\nregions -- as some region-based image recognition methods do -- cannot\nessentially address this problem and may introduce some overwhelming irrelevant\nfeatures with the question. In this work, we propose a novel Focused Dynamic\nAttention (FDA) model to provide better aligned image content representation\nwith proposed questions. Being aware of the key words in the question, FDA\nemploys off-the-shelf object detector to identify important regions and fuse\nthe information from the regions and global features via an LSTM unit. Such\nquestion-driven representations are then combined with question representation\nand fed into a reasoning unit for generating the answers. Extensive evaluation\non a large-scale benchmark dataset, VQA, clearly demonstrate the superior\nperformance of FDA over well-established baselines.\n",
        "published": "2016",
        "authors": [
            "Ilija Ilievski",
            "Shuicheng Yan",
            "Jiashi Feng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.14262v1",
        "title": "SACT: Self-Aware Multi-Space Feature Composition Transformer for\n  Multinomial Attention for Video Captioning",
        "abstract": "  Video captioning works on the two fundamental concepts, feature detection and\nfeature composition. While modern day transformers are beneficial in composing\nfeatures, they lack the fundamental problems of selecting and understanding of\nthe contents. As the feature length increases, it becomes increasingly\nimportant to include provisions for improved capturing of the pertinent\ncontents. In this work, we have introduced a new concept of Self-Aware\nComposition Transformer (SACT) that is capable of generating Multinomial\nAttention (MultAtt) which is a way of generating distributions of various\ncombinations of frames. Also, multi-head attention transformer works on the\nprinciple of combining all possible contents for attention, which is good for\nnatural language classification, but has limitations for video captioning.\nVideo contents have repetitions and require parsing of important contents for\nbetter content composition. In this work, we have introduced SACT for more\nselective attention and combined them for different attention heads for better\ncapturing of the usable contents for any applications. To address the problem\nof diversification and encourage selective utilization, we propose the\nSelf-Aware Composition Transformer model for dense video captioning and apply\nthe technique on two benchmark datasets like ActivityNet and YouCookII.\n",
        "published": "2020",
        "authors": [
            "Chiranjib Sur"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.07287v1",
        "title": "Picture It In Your Mind: Generating High Level Visual Representations\n  From Textual Descriptions",
        "abstract": "  In this paper we tackle the problem of image search when the query is a short\ntextual description of the image the user is looking for. We choose to\nimplement the actual search process as a similarity search in a visual feature\nspace, by learning to translate a textual query into a visual representation.\nSearching in the visual feature space has the advantage that any update to the\ntranslation model does not require to reprocess the, typically huge, image\ncollection on which the search is performed. We propose Text2Vis, a neural\nnetwork that generates a visual representation, in the visual feature space of\nthe fc6-fc7 layers of ImageNet, from a short descriptive text. Text2Vis\noptimizes two loss functions, using a stochastic loss-selection method. A\nvisual-focused loss is aimed at learning the actual text-to-visual feature\nmapping, while a text-focused loss is aimed at modeling the higher-level\nsemantic concepts expressed in language and countering the overfit on\nnon-relevant visual components of the visual loss. We report preliminary\nresults on the MS-COCO dataset.\n",
        "published": "2016",
        "authors": [
            "Fabio Carrara",
            "Andrea Esuli",
            "Tiziano Fagni",
            "Fabrizio Falchi",
            "Alejandro Moreo Fern\u00e1ndez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.06108v3",
        "title": "Modeling the Formation of Social Conventions from Embodied Real-Time\n  Interactions",
        "abstract": "  What is the role of real-time control and learning in the formation of social\nconventions? To answer this question, we propose a computational model that\nmatches human behavioral data in a social decision-making game that was\nanalyzed both in discrete-time and continuous-time setups. Furthermore, unlike\nprevious approaches, our model takes into account the role of sensorimotor\ncontrol loops in embodied decision-making scenarios. For this purpose, we\nintroduce the Control-based Reinforcement Learning (CRL) model. CRL is grounded\nin the Distributed Adaptive Control (DAC) theory of mind and brain, where\nlow-level sensorimotor control is modulated through perceptual and behavioral\nlearning in a layered structure. CRL follows these principles by implementing a\nfeedback control loop handling the agent's reactive behaviors (pre-wired\nreflexes), along with an adaptive layer that uses reinforcement learning to\nmaximize long-term reward. We test our model in a multi-agent game-theoretic\ntask in which coordination must be achieved to find an optimal solution. We\nshow that CRL is able to reach human-level performance on standard\ngame-theoretic metrics such as efficiency in acquiring rewards and fairness in\nreward distribution.\n",
        "published": "2018",
        "authors": [
            "Ismael T. Freire",
            "Clement Moulin-Frier",
            "Marti Sanchez-Fibla",
            "Xerxes D. Arsiwalla",
            "Paul Verschure"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.14107v1",
        "title": "Informative Scene Decomposition for Crowd Analysis, Comparison and\n  Simulation Guidance",
        "abstract": "  Crowd simulation is a central topic in several fields including graphics. To\nachieve high-fidelity simulations, data has been increasingly relied upon for\nanalysis and simulation guidance. However, the information in real-world data\nis often noisy, mixed and unstructured, making it difficult for effective\nanalysis, therefore has not been fully utilized. With the fast-growing volume\nof crowd data, such a bottleneck needs to be addressed. In this paper, we\npropose a new framework which comprehensively tackles this problem. It centers\nat an unsupervised method for analysis. The method takes as input raw and noisy\ndata with highly mixed multi-dimensional (space, time and dynamics)\ninformation, and automatically structure it by learning the correlations among\nthese dimensions. The dimensions together with their correlations fully\ndescribe the scene semantics which consists of recurring activity patterns in a\nscene, manifested as space flows with temporal and dynamics profiles. The\neffectiveness and robustness of the analysis have been tested on datasets with\ngreat variations in volume, duration, environment and crowd dynamics. Based on\nthe analysis, new methods for data visualization, simulation evaluation and\nsimulation guidance are also proposed. Together, our framework establishes a\nhighly automated pipeline from raw data to crowd analysis, comparison and\nsimulation guidance. Extensive experiments and evaluations have been conducted\nto show the flexibility, versatility and intuitiveness of our framework.\n",
        "published": "2020",
        "authors": [
            "Feixiang He",
            "Yuanhang Xiang",
            "Xi Zhao",
            "He Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.04469v1",
        "title": "Collaborative Multi-agent Learning for MR Knee Articular Cartilage\n  Segmentation",
        "abstract": "  The 3D morphology and quantitative assessment of knee articular cartilages\n(i.e., femoral, tibial, and patellar cartilage) in magnetic resonance (MR)\nimaging is of great importance for knee radiographic osteoarthritis (OA)\ndiagnostic decision making. However, effective and efficient delineation of all\nthe knee articular cartilages in large-sized and high-resolution 3D MR knee\ndata is still an open challenge. In this paper, we propose a novel framework to\nsolve the MR knee cartilage segmentation task. The key contribution is the\nadversarial learning based collaborative multi-agent segmentation network. In\nthe proposed network, we use three parallel segmentation agents to label\ncartilages in their respective region of interest (ROI), and then fuse the\nthree cartilages by a novel ROI-fusion layer. The collaborative learning is\ndriven by an adversarial sub-network. The ROI-fusion layer not only fuses the\nindividual cartilages from multiple agents, but also backpropagates the\ntraining loss from the adversarial sub-network to each agent to enable joint\nlearning of shape and spatial constraints. Extensive evaluations are conducted\non a dataset including hundreds of MR knee volumes with diverse populations,\nand the proposed method shows superior performance.\n",
        "published": "2019",
        "authors": [
            "Chaowei Tan",
            "Zhennan Yan",
            "Shaoting Zhang",
            "Kang Li",
            "Dimitris N. Metaxas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.01096v2",
        "title": "Represented Value Function Approach for Large Scale Multi Agent\n  Reinforcement Learning",
        "abstract": "  In this paper, we consider the problem of large scale multi agent\nreinforcement learning. Firstly, we studied the representation problem of the\npairwise value function to reduce the complexity of the interactions among\nagents. Secondly, we adopt a l2-norm trick to ensure the trivial term of the\napproximated value function is bounded. Thirdly, experimental results on battle\ngame demonstrate the effectiveness of the proposed approach.\n",
        "published": "2020",
        "authors": [
            "Weiya Ren"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.01051v4",
        "title": "MANAS: Multi-Agent Neural Architecture Search",
        "abstract": "  The Neural Architecture Search (NAS) problem is typically formulated as a\ngraph search problem where the goal is to learn the optimal operations over\nedges in order to maximise a graph-level global objective. Due to the large\narchitecture parameter space, efficiency is a key bottleneck preventing NAS\nfrom its practical use. In this paper, we address the issue by framing NAS as a\nmulti-agent problem where agents control a subset of the network and coordinate\nto reach optimal architectures. We provide two distinct lightweight\nimplementations, with reduced memory requirements (1/8th of state-of-the-art),\nand performances above those of much more computationally expensive methods.\nTheoretically, we demonstrate vanishing regrets of the form O(sqrt(T)), with T\nbeing the total number of rounds. Finally, aware that random search is an,\noften ignored, effective baseline we perform additional experiments on 3\nalternative datasets and 2 network configurations, and achieve favourable\nresults in comparison.\n",
        "published": "2019",
        "authors": [
            "Vasco Lopes",
            "Fabio Maria Carlucci",
            "Pedro M Esperan\u00e7a",
            "Marco Singh",
            "Victor Gabillon",
            "Antoine Yang",
            "Hang Xu",
            "Zewei Chen",
            "Jun Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.05734v3",
        "title": "Learning Efficient Multi-Agent Cooperative Visual Exploration",
        "abstract": "  We tackle the problem of cooperative visual exploration where multiple agents\nneed to jointly explore unseen regions as fast as possible based on visual\nsignals. Classical planning-based methods often suffer from expensive\ncomputation overhead at each step and a limited expressiveness of complex\ncooperation strategy. By contrast, reinforcement learning (RL) has recently\nbecome a popular paradigm for tackling this challenge due to its modeling\ncapability of arbitrarily complex strategies and minimal inference overhead. In\nthis paper, we extend the state-of-the-art single-agent visual navigation\nmethod, Active Neural SLAM (ANS), to the multi-agent setting by introducing a\nnovel RL-based planning module, Multi-agent Spatial Planner (MSP).MSP leverages\na transformer-based architecture, Spatial-TeamFormer, which effectively\ncaptures spatial relations and intra-agent interactions via hierarchical\nspatial self-attentions. In addition, we also implement a few multi-agent\nenhancements to process local information from each agent for an aligned\nspatial representation and more precise planning. Finally, we perform policy\ndistillation to extract a meta policy to significantly improve the\ngeneralization capability of final policy. We call this overall solution,\nMulti-Agent Active Neural SLAM (MAANS). MAANS substantially outperforms\nclassical planning-based baselines for the first time in a photo-realistic 3D\nsimulator, Habitat. Code and videos can be found at\nhttps://sites.google.com/view/maans.\n",
        "published": "2021",
        "authors": [
            "Chao Yu",
            "Xinyi Yang",
            "Jiaxuan Gao",
            "Huazhong Yang",
            "Yu Wang",
            "Yi Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.09389v3",
        "title": "Low Precision Decentralized Distributed Training over IID and non-IID\n  Data",
        "abstract": "  Decentralized distributed learning is the key to enabling large-scale machine\nlearning (training) on edge devices utilizing private user-generated local\ndata, without relying on the cloud. However, the practical realization of such\non-device training is limited by the communication and compute bottleneck. In\nthis paper, we propose and show the convergence of low precision decentralized\ntraining that aims to reduce the computational complexity and communication\ncost of decentralized training. Many feedback-based compression techniques have\nbeen proposed in the literature to reduce communication costs. To the best of\nour knowledge, there is no work that applies and shows compute efficient\ntraining techniques such as quantization, pruning, etc., for peer-to-peer\ndecentralized learning setups. Since real-world applications have a significant\nskew in the data distribution, we design \"Range-EvoNorm\" as the normalization\nactivation layer which is better suited for low precision training over non-IID\ndata. Moreover, we show that the proposed low precision training can be used in\nsynergy with other communication compression methods decreasing the\ncommunication cost further. Our experiments indicate that 8-bit decentralized\ntraining has minimal accuracy loss compared to its full precision counterpart\neven with non-IID data. However, when low precision training is accompanied by\ncommunication compression through sparsification we observe a 1-2% drop in\naccuracy. The proposed low precision decentralized training decreases\ncomputational complexity, memory usage, and communication cost by 4x and\ncompute energy by a factor of ~20x, while trading off less than a $1\\%$\naccuracy for both IID and non-IID data. In particular, with higher skew values,\nwe observe an increase in accuracy (by ~ 0.5%) with low precision training,\nindicating the regularization effect of the quantization.\n",
        "published": "2021",
        "authors": [
            "Sai Aparna Aketi",
            "Sangamesh Kodge",
            "Kaushik Roy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.09293v1",
        "title": "PreTR: Spatio-Temporal Non-Autoregressive Trajectory Prediction\n  Transformer",
        "abstract": "  Nowadays, our mobility systems are evolving into the era of intelligent\nvehicles that aim to improve road safety. Due to their vulnerability,\npedestrians are the users who will benefit the most from these developments.\nHowever, predicting their trajectory is one of the most challenging concerns.\nIndeed, accurate prediction requires a good understanding of multi-agent\ninteractions that can be complex. Learning the underlying spatial and temporal\npatterns caused by these interactions is even more of a competitive and open\nproblem that many researchers are tackling. In this paper, we introduce a model\ncalled PRediction Transformer (PReTR) that extracts features from the\nmulti-agent scenes by employing a factorized spatio-temporal attention module.\nIt shows less computational needs than previously studied models with\nempirically better results. Besides, previous works in motion prediction suffer\nfrom the exposure bias problem caused by generating future sequences\nconditioned on model prediction samples rather than ground-truth samples. In\norder to go beyond the proposed solutions, we leverage encoder-decoder\nTransformer networks for parallel decoding a set of learned object queries.\nThis non-autoregressive solution avoids the need for iterative conditioning and\narguably decreases training and testing computational time. We evaluate our\nmodel on the ETH/UCY datasets, a publicly available benchmark for pedestrian\ntrajectory prediction. Finally, we justify our usage of the parallel decoding\ntechnique by showing that the trajectory prediction task can be better solved\nas a non-autoregressive task.\n",
        "published": "2022",
        "authors": [
            "Lina Achaji",
            "Thierno Barry",
            "Thibault Fouqueray",
            "Julien Moreau",
            "Francois Aioun",
            "Francois Charpillet"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.08793v1",
        "title": "Rethinking Trajectory Prediction via \"Team Game\"",
        "abstract": "  To accurately predict trajectories in multi-agent settings, e.g. team games,\nit is important to effectively model the interactions among agents. Whereas a\nnumber of methods have been developed for this purpose, existing methods\nimplicitly model these interactions as part of the deep net architecture.\nHowever, in the real world, interactions often exist at multiple levels, e.g.\nindividuals may form groups, where interactions among groups and those among\nthe individuals in the same group often follow significantly different\npatterns. In this paper, we present a novel formulation for multi-agent\ntrajectory prediction, which explicitly introduces the concept of interactive\ngroup consensus via an interactive hierarchical latent space. This formulation\nallows group-level and individual-level interactions to be captured jointly,\nthus substantially improving the capability of modeling complex dynamics. On\ntwo multi-agent settings, i.e. team sports and pedestrians, the proposed\nframework consistently achieves superior performance compared to existing\nmethods.\n",
        "published": "2022",
        "authors": [
            "Zikai Wei",
            "Xinge Zhu",
            "Bo Dai",
            "Dahua Lin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.03767v1",
        "title": "Proactive Multi-Camera Collaboration For 3D Human Pose Estimation",
        "abstract": "  This paper presents a multi-agent reinforcement learning (MARL) scheme for\nproactive Multi-Camera Collaboration in 3D Human Pose Estimation in dynamic\nhuman crowds. Traditional fixed-viewpoint multi-camera solutions for human\nmotion capture (MoCap) are limited in capture space and susceptible to dynamic\nocclusions. Active camera approaches proactively control camera poses to find\noptimal viewpoints for 3D reconstruction. However, current methods still face\nchallenges with credit assignment and environment dynamics. To address these\nissues, our proposed method introduces a novel Collaborative Triangulation\nContribution Reward (CTCR) that improves convergence and alleviates multi-agent\ncredit assignment issues resulting from using 3D reconstruction accuracy as the\nshared reward. Additionally, we jointly train our model with multiple world\ndynamics learning tasks to better capture environment dynamics and encourage\nanticipatory behaviors for occlusion avoidance. We evaluate our proposed method\nin four photo-realistic UE4 environments to ensure validity and\ngeneralizability. Empirical results show that our method outperforms fixed and\nactive baselines in various scenarios with different numbers of cameras and\nhumans.\n",
        "published": "2023",
        "authors": [
            "Hai Ci",
            "Mickel Liu",
            "Xuehai Pan",
            "Fangwei Zhong",
            "Yizhou Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.00370v1",
        "title": "Graph Switching Dynamical Systems",
        "abstract": "  Dynamical systems with complex behaviours, e.g. immune system cells\ninteracting with a pathogen, are commonly modelled by splitting the behaviour\ninto different regimes, or modes, each with simpler dynamics, and then learning\nthe switching behaviour from one mode to another. Switching Dynamical Systems\n(SDS) are a powerful tool that automatically discovers these modes and\nmode-switching behaviour from time series data. While effective, these methods\nfocus on independent objects, where the modes of one object are independent of\nthe modes of the other objects. In this paper, we focus on the more general\ninteracting object setting for switching dynamical systems, where the\nper-object dynamics also depends on an unknown and dynamically changing subset\nof other objects and their modes. To this end, we propose a novel graph-based\napproach for switching dynamical systems, GRAph Switching dynamical Systems\n(GRASS), in which we use a dynamic graph to characterize interactions between\nobjects and learn both intra-object and inter-object mode-switching behaviour.\nWe introduce two new datasets for this setting, a synthesized ODE-driven\nparticles dataset and a real-world Salsa Couple Dancing dataset. Experiments\nshow that GRASS can consistently outperforms previous state-of-the-art methods.\n",
        "published": "2023",
        "authors": [
            "Yongtuo Liu",
            "Sara Magliacane",
            "Miltiadis Kofinas",
            "Efstratios Gavves"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1004.0085v1",
        "title": "A stochastic model of human visual attention with a dynamic Bayesian\n  network",
        "abstract": "  Recent studies in the field of human vision science suggest that the human\nresponses to the stimuli on a visual display are non-deterministic. People may\nattend to different locations on the same visual input at the same time. Based\non this knowledge, we propose a new stochastic model of visual attention by\nintroducing a dynamic Bayesian network to predict the likelihood of where\nhumans typically focus on a video scene. The proposed model is composed of a\ndynamic Bayesian network with 4 layers. Our model provides a framework that\nsimulates and combines the visual saliency response and the cognitive state of\na person to estimate the most probable attended regions. Sample-based inference\nwith Markov chain Monte-Carlo based particle filter and stream processing with\nmulti-core processors enable us to estimate human visual attention in near real\ntime. Experimental results have demonstrated that our model performs\nsignificantly better in predicting human visual attention compared to the\nprevious deterministic models.\n",
        "published": "2010",
        "authors": [
            "Akisato kimura",
            "Derek Pang",
            "Tatsuto Takeuchi",
            "Kouji Miyazato",
            "Junji Yamato",
            "Kunio Kashino"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1407.0733v2",
        "title": "Cortical spatio-temporal dimensionality reduction for visual grouping",
        "abstract": "  The visual systems of many mammals, including humans, is able to integrate\nthe geometric information of visual stimuli and to perform cognitive tasks\nalready at the first stages of the cortical processing. This is thought to be\nthe result of a combination of mechanisms, which include feature extraction at\nsingle cell level and geometric processing by means of cells connectivity. We\npresent a geometric model of such connectivities in the space of detected\nfeatures associated to spatio-temporal visual stimuli, and show how they can be\nused to obtain low-level object segmentation. The main idea is that of defining\na spectral clustering procedure with anisotropic affinities over datasets\nconsisting of embeddings of the visual stimuli into higher dimensional spaces.\nNeural plausibility of the proposed arguments will be discussed.\n",
        "published": "2014",
        "authors": [
            "Giacomo Cocci",
            "Davide Barbieri",
            "Giovanna Citti",
            "Alessandro Sarti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1411.5731v1",
        "title": "Visual Sentiment Prediction with Deep Convolutional Neural Networks",
        "abstract": "  Images have become one of the most popular types of media through which users\nconvey their emotions within online social networks. Although vast amount of\nresearch is devoted to sentiment analysis of textual data, there has been very\nlimited work that focuses on analyzing sentiment of image data. In this work,\nwe propose a novel visual sentiment prediction framework that performs image\nunderstanding with Deep Convolutional Neural Networks (CNN). Specifically, the\nproposed sentiment prediction framework performs transfer learning from a CNN\nwith millions of parameters, which is pre-trained on large-scale data for\nobject recognition. Experiments conducted on two real-world datasets from\nTwitter and Tumblr demonstrate the effectiveness of the proposed visual\nsentiment analysis framework.\n",
        "published": "2014",
        "authors": [
            "Can Xu",
            "Suleyman Cetintas",
            "Kuang-Chih Lee",
            "Li-Jia Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1602.01729v1",
        "title": "Correntropy Maximization via ADMM - Application to Robust Hyperspectral\n  Unmixing",
        "abstract": "  In hyperspectral images, some spectral bands suffer from low signal-to-noise\nratio due to noisy acquisition and atmospheric effects, thus requiring robust\ntechniques for the unmixing problem. This paper presents a robust supervised\nspectral unmixing approach for hyperspectral images. The robustness is achieved\nby writing the unmixing problem as the maximization of the correntropy\ncriterion subject to the most commonly used constraints. Two unmixing problems\nare derived: the first problem considers the fully-constrained unmixing, with\nboth the non-negativity and sum-to-one constraints, while the second one deals\nwith the non-negativity and the sparsity-promoting of the abundances. The\ncorresponding optimization problems are solved efficiently using an alternating\ndirection method of multipliers (ADMM) approach. Experiments on synthetic and\nreal hyperspectral images validate the performance of the proposed algorithms\nfor different scenarios, demonstrating that the correntropy-based unmixing is\nrobust to outlier bands.\n",
        "published": "2016",
        "authors": [
            "Fei Zhu",
            "Abderrahim Halimi",
            "Paul Honeine",
            "Badong Chen",
            "Nanning Zheng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.04058v7",
        "title": "Neural Style Transfer: A Review",
        "abstract": "  The seminal work of Gatys et al. demonstrated the power of Convolutional\nNeural Networks (CNNs) in creating artistic imagery by separating and\nrecombining image content and style. This process of using CNNs to render a\ncontent image in different styles is referred to as Neural Style Transfer\n(NST). Since then, NST has become a trending topic both in academic literature\nand industrial applications. It is receiving increasing attention and a variety\nof approaches are proposed to either improve or extend the original NST\nalgorithm. In this paper, we aim to provide a comprehensive overview of the\ncurrent progress towards NST. We first propose a taxonomy of current algorithms\nin the field of NST. Then, we present several evaluation methods and compare\ndifferent NST algorithms both qualitatively and quantitatively. The review\nconcludes with a discussion of various applications of NST and open problems\nfor future research. A list of papers discussed in this review, corresponding\ncodes, pre-trained models and more comparison results are publicly available at\nhttps://github.com/ycjing/Neural-Style-Transfer-Papers.\n",
        "published": "2017",
        "authors": [
            "Yongcheng Jing",
            "Yezhou Yang",
            "Zunlei Feng",
            "Jingwen Ye",
            "Yizhou Yu",
            "Mingli Song"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1706.07888v2",
        "title": "Evolving Spatially Aggregated Features from Satellite Imagery for\n  Regional Modeling",
        "abstract": "  Satellite imagery and remote sensing provide explanatory variables at\nrelatively high resolutions for modeling geospatial phenomena, yet regional\nsummaries are often desirable for analysis and actionable insight. In this\npaper, we propose a novel method of inducing spatial aggregations as a\ncomponent of the machine learning process, yielding regional model features\nwhose construction is driven by model prediction performance rather than prior\nassumptions. Our results demonstrate that Genetic Programming is particularly\nwell suited to this type of feature construction because it can automatically\nsynthesize appropriate aggregations, as well as better incorporate them into\npredictive models compared to other regression methods we tested. In our\nexperiments we consider a specific problem instance and real-world dataset\nrelevant to predicting snow properties in high-mountain Asia.\n",
        "published": "2017",
        "authors": [
            "Sam Kriegman",
            "Marcin Szubert",
            "Josh C. Bongard",
            "Christian Skalka"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.13494v1",
        "title": "On the difficulty of learning and predicting the long-term dynamics of\n  bouncing objects",
        "abstract": "  The ability to accurately predict the surrounding environment is a\nfoundational principle of intelligence in biological and artificial agents. In\nrecent years, a variety of approaches have been proposed for learning to\npredict the physical dynamics of objects interacting in a visual scene. Here we\nconduct a systematic empirical evaluation of several state-of-the-art\nunsupervised deep learning models that are considered capable of learning the\nspatio-temporal structure of a popular dataset composed by synthetic videos of\nbouncing objects. We show that most of the models indeed obtain high accuracy\non the standard benchmark of predicting the next frame of a sequence, and one\nof them even achieves state-of-the-art performance. However, all models fall\nshort when probed with the more challenging task of generating multiple\nsuccessive frames. Our results show that the ability to perform short-term\npredictions does not imply that the model has captured the underlying structure\nand dynamics of the visual environment, thereby calling for a careful\nrethinking of the metrics commonly adopted for evaluating temporal models. We\nalso investigate whether the learning outcome could be affected by the use of\ncurriculum-based teaching.\n",
        "published": "2019",
        "authors": [
            "Alberto Cenzato",
            "Alberto Testolin",
            "Marco Zorzi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.07285v2",
        "title": "Identifying individual facial expressions by deconstructing a neural\n  network",
        "abstract": "  This paper focuses on the problem of explaining predictions of psychological\nattributes such as attractiveness, happiness, confidence and intelligence from\nface photographs using deep neural networks. Since psychological attribute\ndatasets typically suffer from small sample sizes, we apply transfer learning\nwith two base models to avoid overfitting. These models were trained on an age\nand gender prediction task, respectively. Using a novel explanation method we\nextract heatmaps that highlight the parts of the image most responsible for the\nprediction. We further observe that the explanation method provides important\ninsights into the nature of features of the base model, which allow one to\nassess the aptitude of the base model for a given transfer learning task.\nFinally, we observe that the multiclass model is more feature rich than its\nbinary counterpart. The experimental evaluation is performed on the 2222 images\nfrom the 10k US faces dataset containing psychological attribute labels as well\nas on a subset of KDEF images.\n",
        "published": "2016",
        "authors": [
            "Farhad Arbabzadah",
            "Gr\u00e9goire Montavon",
            "Klaus-Robert M\u00fcller",
            "Wojciech Samek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.09187v3",
        "title": "Object Boundary Detection and Classification with Image-level Labels",
        "abstract": "  Semantic boundary and edge detection aims at simultaneously detecting object\nedge pixels in images and assigning class labels to them. Systematic training\nof predictors for this task requires the labeling of edges in images which is a\nparticularly tedious task. We propose a novel strategy for solving this task,\nwhen pixel-level annotations are not available, performing it in an almost\nzero-shot manner by relying on conventional whole image neural net classifiers\nthat were trained using large bounding boxes. Our method performs the following\ntwo steps at test time. Firstly it predicts the class labels by applying the\ntrained whole image network to the test images. Secondly, it computes\npixel-wise scores from the obtained predictions by applying backprop gradients\nas well as recent visualization algorithms such as deconvolution and layer-wise\nrelevance propagation. We show that high pixel-wise scores are indicative for\nthe location of semantic boundaries, which suggests that the semantic boundary\nproblem can be approached without using edge labels during the training phase.\n",
        "published": "2016",
        "authors": [
            "Jing Yu Koh",
            "Wojciech Samek",
            "Klaus-Robert M\u00fcller",
            "Alexander Binder"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1612.09506v2",
        "title": "Smart Content Recognition from Images Using a Mixture of Convolutional\n  Neural Networks",
        "abstract": "  With rapid development of the Internet, web contents become huge. Most of the\nwebsites are publicly available, and anyone can access the contents from\nanywhere such as workplace, home and even schools. Nevertheless, not all the\nweb contents are appropriate for all users, especially children. An example of\nthese contents is pornography images which should be restricted to certain age\ngroup. Besides, these images are not safe for work (NSFW) in which employees\nshould not be seen accessing such contents during work. Recently, convolutional\nneural networks have been successfully applied to many computer vision\nproblems. Inspired by these successes, we propose a mixture of convolutional\nneural networks for adult content recognition. Unlike other works, our method\nis formulated on a weighted sum of multiple deep neural network models. The\nweights of each CNN models are expressed as a linear regression problem learned\nusing Ordinary Least Squares (OLS). Experimental results demonstrate that the\nproposed model outperforms both single CNN model and the average sum of CNN\nmodels in adult content recognition.\n",
        "published": "2016",
        "authors": [
            "Tee Connie",
            "Mundher Al-Shabi",
            "Michael Goh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1708.06250v1",
        "title": "Pillar Networks++: Distributed non-parametric deep and wide networks",
        "abstract": "  In recent work, it was shown that combining multi-kernel based support vector\nmachines (SVMs) can lead to near state-of-the-art performance on an action\nrecognition dataset (HMDB-51 dataset). This was 0.4\\% lower than frameworks\nthat used hand-crafted features in addition to the deep convolutional feature\nextractors. In the present work, we show that combining distributed Gaussian\nProcesses with multi-stream deep convolutional neural networks (CNN) alleviate\nthe need to augment a neural network with hand-crafted features. In contrast to\nprior work, we treat each deep neural convolutional network as an expert\nwherein the individual predictions (and their respective uncertainties) are\ncombined into a Product of Experts (PoE) framework.\n",
        "published": "2017",
        "authors": [
            "Biswa Sengupta",
            "Yu Qian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.15297v1",
        "title": "Kernel Modulation: A Parameter-Efficient Method for Training\n  Convolutional Neural Networks",
        "abstract": "  Deep Neural Networks, particularly Convolutional Neural Networks (ConvNets),\nhave achieved incredible success in many vision tasks, but they usually require\nmillions of parameters for good accuracy performance. With increasing\napplications that use ConvNets, updating hundreds of networks for multiple\ntasks on an embedded device can be costly in terms of memory, bandwidth, and\nenergy. Approaches to reduce this cost include model compression and\nparameter-efficient models that adapt a subset of network layers for each new\ntask. This work proposes a novel parameter-efficient kernel modulation (KM)\nmethod that adapts all parameters of a base network instead of a subset of\nlayers. KM uses lightweight task-specialized kernel modulators that require\nonly an additional 1.4% of the base network parameters. With multiple tasks,\nonly the task-specialized KM weights are communicated and stored on the\nend-user device. We applied this method in training ConvNets for Transfer\nLearning and Meta-Learning scenarios. Our results show that KM delivers up to\n9% higher accuracy than other parameter-efficient methods on the Transfer\nLearning benchmark.\n",
        "published": "2022",
        "authors": [
            "Yuhuang Hu",
            "Shih-Chii Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.10033v1",
        "title": "Safety-enhanced UAV Path Planning with Spherical Vector-based Particle\n  Swarm Optimization",
        "abstract": "  This paper presents a new algorithm named spherical vector-based particle\nswarm optimization (SPSO) to deal with the problem of path planning for\nunmanned aerial vehicles (UAVs) in complicated environments subjected to\nmultiple threats. A cost function is first formulated to convert the path\nplanning into an optimization problem that incorporates requirements and\nconstraints for the feasible and safe operation of the UAV. SPSO is then used\nto find the optimal path that minimizes the cost function by efficiently\nsearching the configuration space of the UAV via the correspondence between the\nparticle position and the speed, turn angle and climb/dive angle of the UAV. To\nevaluate the performance of SPSO, eight benchmarking scenarios have been\ngenerated from real digital elevation model maps. The results show that the\nproposed SPSO outperforms not only other particle swarm optimization (PSO)\nvariants including the classic PSO, phase angle-encoded PSO and quantum-behave\nPSO but also other state-of-the-art metaheuristic optimization algorithms\nincluding the genetic algorithm (GA), artificial bee colony (ABC), and\ndifferential evolution (DE) in most scenarios. In addition, experiments have\nbeen conducted to demonstrate the validity of the generated paths for real UAV\noperations. Source code of the algorithm can be found at\nhttps://github.com/duongpm/SPSO.\n",
        "published": "2021",
        "authors": [
            "Manh Duong Phung",
            "Quang Phuc Ha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1304.1018v2",
        "title": "Estimating Phoneme Class Conditional Probabilities from Raw Speech\n  Signal using Convolutional Neural Networks",
        "abstract": "  In hybrid hidden Markov model/artificial neural networks (HMM/ANN) automatic\nspeech recognition (ASR) system, the phoneme class conditional probabilities\nare estimated by first extracting acoustic features from the speech signal\nbased on prior knowledge such as, speech perception or/and speech production\nknowledge, and, then modeling the acoustic features with an ANN. Recent\nadvances in machine learning techniques, more specifically in the field of\nimage processing and text processing, have shown that such divide and conquer\nstrategy (i.e., separating feature extraction and modeling steps) may not be\nnecessary. Motivated from these studies, in the framework of convolutional\nneural networks (CNNs), this paper investigates a novel approach, where the\ninput to the ANN is raw speech signal and the output is phoneme class\nconditional probability estimates. On TIMIT phoneme recognition task, we study\ndifferent ANN architectures to show the benefit of CNNs and compare the\nproposed approach against conventional approach where, spectral-based feature\nMFCC is extracted and modeled by a multilayer perceptron. Our studies show that\nthe proposed approach can yield comparable or better phoneme recognition\nperformance when compared to the conventional approach. It indicates that CNNs\ncan learn features relevant for phoneme classification automatically from the\nraw speech signal.\n",
        "published": "2013",
        "authors": [
            "Dimitri Palaz",
            "Ronan Collobert",
            "Mathew Magimai. -Doss"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1312.2137v1",
        "title": "End-to-end Phoneme Sequence Recognition using Convolutional Neural\n  Networks",
        "abstract": "  Most phoneme recognition state-of-the-art systems rely on a classical neural\nnetwork classifiers, fed with highly tuned features, such as MFCC or PLP\nfeatures. Recent advances in ``deep learning'' approaches questioned such\nsystems, but while some attempts were made with simpler features such as\nspectrograms, state-of-the-art systems still rely on MFCCs. This might be\nviewed as a kind of failure from deep learning approaches, which are often\nclaimed to have the ability to train with raw signals, alleviating the need of\nhand-crafted features. In this paper, we investigate a convolutional neural\nnetwork approach for raw speech signals. While convolutional architectures got\ntremendous success in computer vision or text processing, they seem to have\nbeen let down in the past recent years in the speech processing field. We show\nthat it is possible to learn an end-to-end phoneme sequence classifier system\ndirectly from raw signal, with similar performance on the TIMIT and WSJ\ndatasets than existing systems based on MFCC, questioning the need of complex\nhand-crafted features on large datasets.\n",
        "published": "2013",
        "authors": [
            "Dimitri Palaz",
            "Ronan Collobert",
            "Mathew Magimai. -Doss"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1408.2873v2",
        "title": "First-Pass Large Vocabulary Continuous Speech Recognition using\n  Bi-Directional Recurrent DNNs",
        "abstract": "  We present a method to perform first-pass large vocabulary continuous speech\nrecognition using only a neural network and language model. Deep neural network\nacoustic models are now commonplace in HMM-based speech recognition systems,\nbut building such systems is a complex, domain-specific task. Recent work\ndemonstrated the feasibility of discarding the HMM sequence modeling framework\nby directly predicting transcript text from audio. This paper extends this\napproach in two ways. First, we demonstrate that a straightforward recurrent\nneural network architecture can achieve a high level of accuracy. Second, we\npropose and evaluate a modified prefix-search decoding algorithm. This approach\nto decoding enables first-pass speech recognition with a language model,\ncompletely unaided by the cumbersome infrastructure of HMM-based systems.\nExperiments on the Wall Street Journal corpus demonstrate fairly competitive\nword error rates, and the importance of bi-directional network recurrence.\n",
        "published": "2014",
        "authors": [
            "Awni Y. Hannun",
            "Andrew L. Maas",
            "Daniel Jurafsky",
            "Andrew Y. Ng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1409.8558v1",
        "title": "A Deep Learning Approach to Data-driven Parameterizations for\n  Statistical Parametric Speech Synthesis",
        "abstract": "  Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstral\ncoefficients as the vocal tract parameterization of the speech signal. Mel\nCepstral coefficients were never intended to work in a parametric speech\nsynthesis framework, but as yet, there has been little success in creating a\nbetter parameterization that is more suited to synthesis. In this paper, we use\ndeep learning algorithms to investigate a data-driven parameterization\ntechnique that is designed for the specific requirements of synthesis. We\ncreate an invertible, low-dimensional, noise-robust encoding of the Mel Log\nSpectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA is\nthen unwrapped and used as the initialization for a Multi-Layer Perceptron\n(MLP). The MLP is fine-tuned by training it to reconstruct the input at the\noutput layer. This MLP is then split down the middle to form encoding and\ndecoding networks. These networks produce a parameterization of the Mel Log\nSpectrum that is intended to better fulfill the requirements of synthesis.\nResults are reported for experiments conducted using this resulting\nparameterization with the ClusterGen speech synthesizer.\n",
        "published": "2014",
        "authors": [
            "Prasanna Kumar Muthukumar",
            "Alan W. Black"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1410.8206v4",
        "title": "Addressing the Rare Word Problem in Neural Machine Translation",
        "abstract": "  Neural Machine Translation (NMT) is a new approach to machine translation\nthat has shown promising results that are comparable to traditional approaches.\nA significant weakness in conventional NMT systems is their inability to\ncorrectly translate very rare words: end-to-end NMTs tend to have relatively\nsmall vocabularies with a single unk symbol that represents every possible\nout-of-vocabulary (OOV) word. In this paper, we propose and implement an\neffective technique to address this problem. We train an NMT system on data\nthat is augmented by the output of a word alignment algorithm, allowing the NMT\nsystem to emit, for each OOV word in the target sentence, the position of its\ncorresponding word in the source sentence. This information is later utilized\nin a post-processing step that translates every OOV word using a dictionary.\nOur experiments on the WMT14 English to French translation task show that this\nmethod provides a substantial improvement of up to 2.8 BLEU points over an\nequivalent NMT system that does not use this technique. With 37.5 BLEU points,\nour NMT system is the first to surpass the best result achieved on a WMT14\ncontest task.\n",
        "published": "2014",
        "authors": [
            "Minh-Thang Luong",
            "Ilya Sutskever",
            "Quoc V. Le",
            "Oriol Vinyals",
            "Wojciech Zaremba"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1411.4116v1",
        "title": "Investigating the Role of Prior Disambiguation in Deep-learning\n  Compositional Models of Meaning",
        "abstract": "  This paper aims to explore the effect of prior disambiguation on neural\nnetwork- based compositional models, with the hope that better semantic\nrepresentations for text compounds can be produced. We disambiguate the input\nword vectors before they are fed into a compositional deep net. A series of\nevaluations shows the positive effect of prior disambiguation for such deep\nmodels.\n",
        "published": "2014",
        "authors": [
            "Jianpeng Cheng",
            "Dimitri Kartsaklis",
            "Edward Grefenstette"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.5335v7",
        "title": "Ensemble of Generative and Discriminative Techniques for Sentiment\n  Analysis of Movie Reviews",
        "abstract": "  Sentiment analysis is a common task in natural language processing that aims\nto detect polarity of a text document (typically a consumer review). In the\nsimplest settings, we discriminate only between positive and negative\nsentiment, turning the task into a standard binary classification problem. We\ncompare several ma- chine learning approaches to this problem, and combine them\nto achieve the best possible results. We show how to use for this task the\nstandard generative lan- guage models, which are slightly complementary to the\nstate of the art techniques. We achieve strong results on a well-known dataset\nof IMDB movie reviews. Our results are easily reproducible, as we publish also\nthe code needed to repeat the experiments. This should simplify further advance\nof the state of the art, as other researchers can combine their techniques with\nours with little effort.\n",
        "published": "2014",
        "authors": [
            "Gr\u00e9goire Mesnil",
            "Tomas Mikolov",
            "Marc'Aurelio Ranzato",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.5567v2",
        "title": "Deep Speech: Scaling up end-to-end speech recognition",
        "abstract": "  We present a state-of-the-art speech recognition system developed using\nend-to-end deep learning. Our architecture is significantly simpler than\ntraditional speech systems, which rely on laboriously engineered processing\npipelines; these traditional systems also tend to perform poorly when used in\nnoisy environments. In contrast, our system does not need hand-designed\ncomponents to model background noise, reverberation, or speaker variation, but\ninstead directly learns a function that is robust to such effects. We do not\nneed a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our\napproach is a well-optimized RNN training system that uses multiple GPUs, as\nwell as a set of novel data synthesis techniques that allow us to efficiently\nobtain a large amount of varied data for training. Our system, called Deep\nSpeech, outperforms previously published results on the widely studied\nSwitchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech\nalso handles challenging noisy environments better than widely used,\nstate-of-the-art commercial speech systems.\n",
        "published": "2014",
        "authors": [
            "Awni Hannun",
            "Carl Case",
            "Jared Casper",
            "Bryan Catanzaro",
            "Greg Diamos",
            "Erich Elsen",
            "Ryan Prenger",
            "Sanjeev Satheesh",
            "Shubho Sengupta",
            "Adam Coates",
            "Andrew Y. Ng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.6650v4",
        "title": "Incremental Adaptation Strategies for Neural Network Language Models",
        "abstract": "  It is today acknowledged that neural network language models outperform\nbackoff language models in applications like speech recognition or statistical\nmachine translation. However, training these models on large amounts of data\ncan take several days. We present efficient techniques to adapt a neural\nnetwork language model to new data. Instead of training a completely new model\nor relying on mixture approaches, we propose two new methods: continued\ntraining on resampled data or insertion of adaptation layers. We present\nexperimental results in an CAT environment where the post-edits of professional\ntranslators are used to improve an SMT system. Both methods are very fast and\nachieve significant improvements without overfitting the small adaptation data.\n",
        "published": "2014",
        "authors": [
            "Aram Ter-Sarkisov",
            "Holger Schwenk",
            "Loic Barrault",
            "Fethi Bougares"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.7028v4",
        "title": "Joint RNN-Based Greedy Parsing and Word Composition",
        "abstract": "  This paper introduces a greedy parser based on neural networks, which\nleverages a new compositional sub-tree representation. The greedy parser and\nthe compositional procedure are jointly trained, and tightly depends on\neach-other. The composition procedure outputs a vector representation which\nsummarizes syntactically (parsing tags) and semantically (words) sub-trees.\nComposition and tagging is achieved over continuous (word or tag)\nrepresentations, and recurrent neural networks. We reach F1 performance on par\nwith well-known existing parsers, while having the advantage of speed, thanks\nto the greedy nature of the parser. We provide a fully functional\nimplementation of the method described in this paper.\n",
        "published": "2014",
        "authors": [
            "Jo\u00ebl Legrand",
            "Ronan Collobert"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.7063v5",
        "title": "Diverse Embedding Neural Network Language Models",
        "abstract": "  We propose Diverse Embedding Neural Network (DENN), a novel architecture for\nlanguage models (LMs). A DENNLM projects the input word history vector onto\nmultiple diverse low-dimensional sub-spaces instead of a single\nhigher-dimensional sub-space as in conventional feed-forward neural network\nLMs. We encourage these sub-spaces to be diverse during network training\nthrough an augmented loss function. Our language modeling experiments on the\nPenn Treebank data set show the performance benefit of using a DENNLM.\n",
        "published": "2014",
        "authors": [
            "Kartik Audhkhasi",
            "Abhinav Sethy",
            "Bhuvana Ramabhadran"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.7091v3",
        "title": "Efficient Exact Gradient Update for training Deep Networks with Very\n  Large Sparse Targets",
        "abstract": "  An important class of problems involves training deep neural networks with\nsparse prediction targets of very high dimension D. These occur naturally in\ne.g. neural language models or the learning of word-embeddings, often posed as\npredicting the probability of next words among a vocabulary of size D (e.g. 200\n000). Computing the equally large, but typically non-sparse D-dimensional\noutput vector from a last hidden layer of reasonable dimension d (e.g. 500)\nincurs a prohibitive O(Dd) computational cost for each example, as does\nupdating the D x d output weight matrix and computing the gradient needed for\nbackpropagation to previous layers. While efficient handling of large sparse\nnetwork inputs is trivial, the case of large sparse targets is not, and has\nthus so far been sidestepped with approximate alternatives such as hierarchical\nsoftmax or sampling-based approximations during training. In this work we\ndevelop an original algorithmic approach which, for a family of loss functions\nthat includes squared error and spherical softmax, can compute the exact loss,\ngradient update for the output weights, and gradient for backpropagation, all\nin O(d^2) per example instead of O(Dd), remarkably without ever computing the\nD-dimensional output. The proposed algorithm yields a speedup of D/4d , i.e.\ntwo orders of magnitude for typical sizes, for that critical part of the\ncomputations that often dominates the training time in this kind of network\narchitecture.\n",
        "published": "2014",
        "authors": [
            "Pascal Vincent",
            "Alexandre de Br\u00e9bisson",
            "Xavier Bouthillier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.7110v6",
        "title": "Learning linearly separable features for speech recognition using\n  convolutional neural networks",
        "abstract": "  Automatic speech recognition systems usually rely on spectral-based features,\nsuch as MFCC of PLP. These features are extracted based on prior knowledge such\nas, speech perception or/and speech production. Recently, convolutional neural\nnetworks have been shown to be able to estimate phoneme conditional\nprobabilities in a completely data-driven manner, i.e. using directly temporal\nraw speech signal as input. This system was shown to yield similar or better\nperformance than HMM/ANN based system on phoneme recognition task and on large\nscale continuous speech recognition task, using less parameters. Motivated by\nthese studies, we investigate the use of simple linear classifier in the\nCNN-based framework. Thus, the network learns linearly separable features from\nraw speech. We show that such system yields similar or better performance than\nMLP based system using cepstral-based features as input.\n",
        "published": "2014",
        "authors": [
            "Dimitri Palaz",
            "Mathew Magimai Doss",
            "Ronan Collobert"
        ]
    }
]