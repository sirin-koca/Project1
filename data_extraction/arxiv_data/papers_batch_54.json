[
    {
        "id": "http://arxiv.org/abs/1502.03682v1",
        "title": "Applying deep learning techniques on medical corpora from the World Wide\n  Web: a prototypical system and evaluation",
        "abstract": "  BACKGROUND: The amount of biomedical literature is rapidly growing and it is\nbecoming increasingly difficult to keep manually curated knowledge bases and\nontologies up-to-date. In this study we applied the word2vec deep learning\ntoolkit to medical corpora to test its potential for identifying relationships\nfrom unstructured text. We evaluated the efficiency of word2vec in identifying\nproperties of pharmaceuticals based on mid-sized, unstructured medical text\ncorpora available on the web. Properties included relationships to diseases\n('may treat') or physiological processes ('has physiological effect'). We\ncompared the relationships identified by word2vec with manually curated\ninformation from the National Drug File - Reference Terminology (NDF-RT)\nontology as a gold standard. RESULTS: Our results revealed a maximum accuracy\nof 49.28% which suggests a limited ability of word2vec to capture linguistic\nregularities on the collected medical corpora compared with other published\nresults. We were able to document the influence of different parameter settings\non result accuracy and found and unexpected trade-off between ranking quality\nand accuracy. Pre-processing corpora to reduce syntactic variability proved to\nbe a good strategy for increasing the utility of the trained vector models.\nCONCLUSIONS: Word2vec is a very efficient implementation for computing vector\nrepresentations and for its ability to identify relationships in textual data\nwithout any prior domain knowledge. We found that the ranking and retrieved\nresults generated by word2vec were not of sufficient quality for automatic\npopulation of knowledge bases and ontologies, but could serve as a starting\npoint for further manual curation.\n",
        "published": "2015",
        "authors": [
            "Jose Antonio Mi\u00f1arro-Gim\u00e9nez",
            "Oscar Mar\u00edn-Alonso",
            "Matthias Samwald"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1502.06922v3",
        "title": "Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis\n  and Application to Information Retrieval",
        "abstract": "  This paper develops a model that addresses sentence embedding, a hot topic in\ncurrent natural language processing research, using recurrent neural networks\nwith Long Short-Term Memory (LSTM) cells. Due to its ability to capture long\nterm memory, the LSTM-RNN accumulates increasingly richer information as it\ngoes through the sentence, and when it reaches the last word, the hidden layer\nof the network provides a semantic representation of the whole sentence. In\nthis paper, the LSTM-RNN is trained in a weakly supervised manner on user\nclick-through data logged by a commercial web search engine. Visualization and\nanalysis are performed to understand how the embedding process works. The model\nis found to automatically attenuate the unimportant words and detects the\nsalient keywords in the sentence. Furthermore, these detected keywords are\nfound to automatically activate different cells of the LSTM-RNN, where words\nbelonging to a similar topic activate the same cell. As a semantic\nrepresentation of the sentence, the embedding vector can be used in many\ndifferent applications. These automatic keyword detection and topic allocation\nabilities enabled by the LSTM-RNN allow the network to perform document\nretrieval, a difficult language processing task, where the similarity between\nthe query and documents can be measured by the distance between their\ncorresponding sentence embedding vectors computed by the LSTM-RNN. On a web\nsearch task, the LSTM-RNN embedding is shown to significantly outperform\nseveral existing state of the art methods. We emphasize that the proposed model\ngenerates sentence embedding vectors that are specially useful for web document\nretrieval tasks. A comparison with a well known general sentence embedding\nmethod, the Paragraph Vector, is performed. The results show that the proposed\nmethod in this paper significantly outperforms it for web document retrieval\ntask.\n",
        "published": "2015",
        "authors": [
            "Hamid Palangi",
            "Li Deng",
            "Yelong Shen",
            "Jianfeng Gao",
            "Xiaodong He",
            "Jianshu Chen",
            "Xinying Song",
            "Rabab Ward"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1505.01504v2",
        "title": "A Fixed-Size Encoding Method for Variable-Length Sequences with its\n  Application to Neural Network Language Models",
        "abstract": "  In this paper, we propose the new fixed-size ordinally-forgetting encoding\n(FOFE) method, which can almost uniquely encode any variable-length sequence of\nwords into a fixed-size representation. FOFE can model the word order in a\nsequence using a simple ordinally-forgetting mechanism according to the\npositions of words. In this work, we have applied FOFE to feedforward neural\nnetwork language models (FNN-LMs). Experimental results have shown that without\nusing any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform\nnot only the standard fixed-input FNN-LMs but also the popular RNN-LMs.\n",
        "published": "2015",
        "authors": [
            "Shiliang Zhang",
            "Hui Jiang",
            "Mingbin Xu",
            "Junfeng Hou",
            "Lirong Dai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1505.05667v1",
        "title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional\n  Neural Network",
        "abstract": "  In this work, we address the problem to model all the nodes (words or\nphrases) in a dependency tree with the dense representations. We propose a\nrecursive convolutional neural network (RCNN) architecture to capture syntactic\nand compositional-semantic representations of phrases and words in a dependency\ntree. Different with the original recursive neural network, we introduce the\nconvolution and pooling layers, which can model a variety of compositions by\nthe feature maps and choose the most informative compositions by the pooling\nlayers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list\nof candidate dependency parsing trees. The experiments show that RCNN is very\neffective to improve the state-of-the-art dependency parsing on both English\nand Chinese datasets.\n",
        "published": "2015",
        "authors": [
            "Chenxi Zhu",
            "Xipeng Qiu",
            "Xinchi Chen",
            "Xuanjing Huang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1505.06427v1",
        "title": "Deep Speaker Vectors for Semi Text-independent Speaker Verification",
        "abstract": "  Recent research shows that deep neural networks (DNNs) can be used to extract\ndeep speaker vectors (d-vectors) that preserve speaker characteristics and can\nbe used in speaker verification. This new method has been tested on\ntext-dependent speaker verification tasks, and improvement was reported when\ncombined with the conventional i-vector method.\n  This paper extends the d-vector approach to semi text-independent speaker\nverification tasks, i.e., the text of the speech is in a limited set of short\nphrases. We explore various settings of the DNN structure used for d-vector\nextraction, and present a phone-dependent training which employs the posterior\nfeatures obtained from an ASR system. The experimental results show that it is\npossible to apply d-vectors on semi text-independent speaker recognition, and\nthe phone-dependent training improves system performance.\n",
        "published": "2015",
        "authors": [
            "Lantian Li",
            "Dong Wang",
            "Zhiyong Zhang",
            "Thomas Fang Zheng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1505.08075v1",
        "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory",
        "abstract": "  We propose a technique for learning representations of parser states in\ntransition-based dependency parsers. Our primary innovation is a new control\nstructure for sequence-to-sequence neural networks---the stack LSTM. Like the\nconventional stack data structures used in transition-based parsing, elements\ncan be pushed to or popped from the top of the stack in constant time, but, in\naddition, an LSTM maintains a continuous space embedding of the stack contents.\nThis lets us formulate an efficient parsing model that captures three facets of\na parser's state: (i) unbounded look-ahead into the buffer of incoming words,\n(ii) the complete history of actions taken by the parser, and (iii) the\ncomplete contents of the stack of partially built tree fragments, including\ntheir internal structures. Standard backpropagation techniques are used for\ntraining and yield state-of-the-art parsing performance.\n",
        "published": "2015",
        "authors": [
            "Chris Dyer",
            "Miguel Ballesteros",
            "Wang Ling",
            "Austin Matthews",
            "Noah A. Smith"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.02078v2",
        "title": "Visualizing and Understanding Recurrent Networks",
        "abstract": "  Recurrent Neural Networks (RNNs), and specifically a variant with Long\nShort-Term Memory (LSTM), are enjoying renewed interest as a result of\nsuccessful applications in a wide range of machine learning problems that\ninvolve sequential data. However, while LSTMs provide exceptional results in\npractice, the source of their performance and their limitations remain rather\npoorly understood. Using character-level language models as an interpretable\ntestbed, we aim to bridge this gap by providing an analysis of their\nrepresentations, predictions and error types. In particular, our experiments\nreveal the existence of interpretable cells that keep track of long-range\ndependencies such as line lengths, quotes and brackets. Moreover, our\ncomparative analysis with finite horizon n-gram models traces the source of the\nLSTM improvements to long-range structural dependencies. Finally, we provide\nanalysis of the remaining errors and suggests areas for further study.\n",
        "published": "2015",
        "authors": [
            "Andrej Karpathy",
            "Justin Johnson",
            "Li Fei-Fei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.02327v1",
        "title": "A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for\n  Unsupervised Discovery of Linguistic Units and Generation of High Quality\n  Features",
        "abstract": "  This paper summarizes the work done by the authors for the Zero Resource\nSpeech Challenge organized in the technical program of Interspeech 2015. The\ngoal of the challenge is to discover linguistic units directly from unlabeled\nspeech data. The Multi-layered Acoustic Tokenizer (MAT) proposed in this work\nautomatically discovers multiple sets of acoustic tokens from the given corpus.\nEach acoustic token set is specified by a set of hyperparameters that describe\nthe model configuration. These sets of acoustic tokens carry different\ncharacteristics of the given corpus and the language behind thus can be\nmutually reinforced. The multiple sets of token labels are then used as the\ntargets of a Multi-target DNN (MDNN) trained on low-level acoustic features.\nBottleneck features extracted from the MDNN are used as feedback for the MAT\nand the MDNN itself. We call this iterative system the Multi-layered Acoustic\nTokenizing Deep Neural Network (MAT-DNN) which generates high quality features\nfor track 1 of the challenge and acoustic tokens for track 2 of the challenge.\n",
        "published": "2015",
        "authors": [
            "Cheng-Tao Chung",
            "Cheng-Yu Tsai",
            "Hsiang-Hung Lu",
            "Yuan-ming Liou",
            "Yen-Chen Wu",
            "Yen-Ju Lu",
            "Hung-yi Lee",
            "Lin-shan Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.02516v3",
        "title": "Learning to Transduce with Unbounded Memory",
        "abstract": "  Recently, strong results have been demonstrated by Deep Recurrent Neural\nNetworks on natural language transduction problems. In this paper we explore\nthe representational power of these models using synthetic grammars designed to\nexhibit phenomena similar to those found in real transduction problems such as\nmachine translation. These experiments lead us to propose new memory-based\nrecurrent networks that implement continuously differentiable analogues of\ntraditional data structures such as Stacks, Queues, and DeQues. We show that\nthese architectures exhibit superior generalisation performance to Deep RNNs\nand are often able to learn the underlying generating algorithms in our\ntransduction experiments.\n",
        "published": "2015",
        "authors": [
            "Edward Grefenstette",
            "Karl Moritz Hermann",
            "Mustafa Suleyman",
            "Phil Blunsom"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.04891v2",
        "title": "Author Identification using Multi-headed Recurrent Neural Networks",
        "abstract": "  Recurrent neural networks (RNNs) are very good at modelling the flow of text,\nbut typically need to be trained on a far larger corpus than is available for\nthe PAN 2015 Author Identification task. This paper describes a novel approach\nwhere the output layer of a character-level RNN language model is split into\nseveral independent predictive sub-models, each representing an author, while\nthe recurrent layer is shared by all. This allows the recurrent layer to model\nthe language as a whole without over-fitting, while the outputs select aspects\nof the underlying model that reflect their author's style. The method proves\ncompetitive, ranking first in two of the four languages.\n",
        "published": "2015",
        "authors": [
            "Douglas Bagnall"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.06442v4",
        "title": "A Deep Memory-based Architecture for Sequence-to-Sequence Learning",
        "abstract": "  We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence\nlearning, which performs the task through a series of nonlinear transformations\nfrom the representation of the input sequence (e.g., a Chinese sentence) to the\nfinal output sequence (e.g., translation to English). Inspired by the recently\nproposed Neural Turing Machine (Graves et al., 2014), we store the intermediate\nrepresentations in stacked layers of memories, and use read-write operations on\nthe memories to realize the nonlinear transformations between the\nrepresentations. The types of transformations are designed in advance but the\nparameters are learned from data. Through layer-by-layer transformations,\nDEEPMEMORY can model complicated relations between sequences necessary for\napplications such as machine translation between distant languages. The\narchitecture can be trained with normal back-propagation on sequenceto-sequence\ndata, and the learning can be easily scaled up to a large corpus. DEEPMEMORY is\nbroad enough to subsume the state-of-the-art neural translation model in\n(Bahdanau et al., 2015) as its special case, while significantly improving upon\nthe model with its deeper architecture. Remarkably, DEEPMEMORY, being purely\nneural network-based, can achieve performance comparable to the traditional\nphrase-based machine translation system Moses with a small vocabulary and a\nmodest parameter size.\n",
        "published": "2015",
        "authors": [
            "Fandong Meng",
            "Zhengdong Lu",
            "Zhaopeng Tu",
            "Hang Li",
            "Qun Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.07285v5",
        "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing",
        "abstract": "  Most tasks in natural language processing can be cast into question answering\n(QA) problems over language input. We introduce the dynamic memory network\n(DMN), a neural network architecture which processes input sequences and\nquestions, forms episodic memories, and generates relevant answers. Questions\ntrigger an iterative attention process which allows the model to condition its\nattention on the inputs and the result of previous iterations. These results\nare then reasoned over in a hierarchical recurrent sequence model to generate\nanswers. The DMN can be trained end-to-end and obtains state-of-the-art results\non several types of tasks and datasets: question answering (Facebook's bAbI\ndataset), text classification for sentiment analysis (Stanford Sentiment\nTreebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The\ntraining for these different tasks relies exclusively on trained word vector\nrepresentations and input-question-answer triplets.\n",
        "published": "2015",
        "authors": [
            "Ankit Kumar",
            "Ozan Irsoy",
            "Peter Ondruska",
            "Mohit Iyyer",
            "James Bradbury",
            "Ishaan Gulrajani",
            "Victor Zhong",
            "Romain Paulus",
            "Richard Socher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.08349v1",
        "title": "Improved Deep Speaker Feature Learning for Text-Dependent Speaker\n  Recognition",
        "abstract": "  A deep learning approach has been proposed recently to derive speaker\nidentifies (d-vector) by a deep neural network (DNN). This approach has been\napplied to text-dependent speaker recognition tasks and shows reasonable\nperformance gains when combined with the conventional i-vector approach.\nAlthough promising, the existing d-vector implementation still can not compete\nwith the i-vector baseline. This paper presents two improvements for the deep\nlearning approach: a phonedependent DNN structure to normalize phone variation,\nand a new scoring approach based on dynamic time warping (DTW). Experiments on\na text-dependent speaker recognition task demonstrated that the proposed\nmethods can provide considerable performance improvement over the existing\nd-vector implementation.\n",
        "published": "2015",
        "authors": [
            "Lantian Li",
            "Yiye Lin",
            "Zhiyong Zhang",
            "Dong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1507.01526v3",
        "title": "Grid Long Short-Term Memory",
        "abstract": "  This paper introduces Grid Long Short-Term Memory, a network of LSTM cells\narranged in a multidimensional grid that can be applied to vectors, sequences\nor higher dimensional data such as images. The network differs from existing\ndeep LSTM architectures in that the cells are connected between network layers\nas well as along the spatiotemporal dimensions of the data. The network\nprovides a unified way of using LSTM for both deep and sequential computation.\nWe apply the model to algorithmic tasks such as 15-digit integer addition and\nsequence memorization, where it is able to significantly outperform the\nstandard LSTM. We then give results for two empirical tasks. We find that 2D\nGrid LSTM achieves 1.47 bits per character on the Wikipedia character\nprediction benchmark, which is state-of-the-art among neural approaches. In\naddition, we use the Grid LSTM to define a novel two-dimensional translation\nmodel, the Reencoder, and show that it outperforms a phrase-based reference\nsystem on a Chinese-to-English translation task.\n",
        "published": "2015",
        "authors": [
            "Nal Kalchbrenner",
            "Ivo Danihelka",
            "Alex Graves"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1507.04646v1",
        "title": "A Dependency-Based Neural Network for Relation Classification",
        "abstract": "  Previous research on relation classification has verified the effectiveness\nof using dependency shortest paths or subtrees. In this paper, we further\nexplore how to make full use of the combination of these dependency\ninformation. We first propose a new structure, termed augmented dependency path\n(ADP), which is composed of the shortest dependency path between two entities\nand the subtrees attached to the shortest path. To exploit the semantic\nrepresentation behind the ADP structure, we develop dependency-based neural\nnetworks (DepNN): a recursive neural network designed to model the subtrees,\nand a convolutional neural network to capture the most important features on\nthe shortest path. Experiments on the SemEval-2010 dataset show that our\nproposed method achieves state-of-art results.\n",
        "published": "2015",
        "authors": [
            "Yang Liu",
            "Furu Wei",
            "Sujian Li",
            "Heng Ji",
            "Ming Zhou",
            "Houfeng Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1508.00200v1",
        "title": "PTE: Predictive Text Embedding through Large-scale Heterogeneous Text\n  Networks",
        "abstract": "  Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector,\nhave been attracting increasing attention due to their simplicity, scalability,\nand effectiveness. However, comparing to sophisticated deep learning\narchitectures such as convolutional neural networks, these methods usually\nyield inferior results when applied to particular machine learning tasks. One\npossible reason is that these text embedding methods learn the representation\nof text in a fully unsupervised way, without leveraging the labeled information\navailable for the task. Although the low dimensional representations learned\nare applicable to many different tasks, they are not particularly tuned for any\ntask. In this paper, we fill this gap by proposing a semi-supervised\nrepresentation learning method for text data, which we call the\n\\textit{predictive text embedding} (PTE). Predictive text embedding utilizes\nboth labeled and unlabeled data to learn the embedding of text. The labeled\ninformation and different levels of word co-occurrence information are first\nrepresented as a large-scale heterogeneous text network, which is then embedded\ninto a low dimensional space through a principled and efficient algorithm. This\nlow dimensional embedding not only preserves the semantic closeness of words\nand documents, but also has a strong predictive power for the particular task.\nCompared to recent supervised approaches based on convolutional neural\nnetworks, predictive text embedding is comparable or more effective, much more\nefficient, and has fewer parameters to tune.\n",
        "published": "2015",
        "authors": [
            "Jian Tang",
            "Meng Qu",
            "Qiaozhu Mei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1508.01006v2",
        "title": "Relation Classification via Recurrent Neural Network",
        "abstract": "  Deep learning has gained much success in sentence-level relation\nclassification. For example, convolutional neural networks (CNN) have delivered\ncompetitive performance without much effort on feature engineering as the\nconventional pattern-based methods. Thus a lot of works have been produced\nbased on CNN structures. However, a key issue that has not been well addressed\nby the CNN-based method is the lack of capability to learn temporal features,\nespecially long-distance dependency between nominal pairs. In this paper, we\npropose a simple framework based on recurrent neural networks (RNN) and compare\nit with CNN-based model. To show the limitation of popular used SemEval-2010\nTask 8 dataset, we introduce another dataset refined from MIMLRE(Angeli et al.,\n2014). Experiments on two different datasets strongly indicates that the\nRNN-based model can deliver better performance on relation classification, and\nit is particularly capable of learning long-distance relation patterns. This\nmakes it suitable for real-world applications where complicated expressions are\noften involved.\n",
        "published": "2015",
        "authors": [
            "Dongxu Zhang",
            "Dong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1508.01011v1",
        "title": "Learning from LDA using Deep Neural Networks",
        "abstract": "  Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian\nmodel for topic inference. In spite of its great success, inferring the latent\ntopic distribution with LDA is time-consuming. Motivated by the transfer\nlearning approach proposed by~\\newcite{hinton2015distilling}, we present a\nnovel method that uses LDA to supervise the training of a deep neural network\n(DNN), so that the DNN can approximate the costly LDA inference with less\ncomputation. Our experiments on a document classification task show that a\nsimple DNN can learn the LDA behavior pretty well, while the inference is\nspeeded up tens or hundreds of times.\n",
        "published": "2015",
        "authors": [
            "Dongxu Zhang",
            "Tianyi Luo",
            "Dong Wang",
            "Rong Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1508.03854v1",
        "title": "Online Representation Learning in Recurrent Neural Language Models",
        "abstract": "  We investigate an extension of continuous online learning in recurrent neural\nnetwork language models. The model keeps a separate vector representation of\nthe current unit of text being processed and adaptively adjusts it after each\nprediction. The initial experiments give promising results, indicating that the\nmethod is able to increase language modelling accuracy, while also decreasing\nthe parameters needed to store the model along with the computation required at\neach step.\n",
        "published": "2015",
        "authors": [
            "Marek Rei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1510.02693v1",
        "title": "Feedforward Sequential Memory Neural Networks without Recurrent Feedback",
        "abstract": "  We introduce a new structure for memory neural networks, called feedforward\nsequential memory networks (FSMN), which can learn long-term dependency without\nusing recurrent feedback. The proposed FSMN is a standard feedforward neural\nnetworks equipped with learnable sequential memory blocks in the hidden layers.\nIn this work, we have applied FSMN to several language modeling (LM) tasks.\nExperimental results have shown that the memory blocks in FSMN can learn\neffective representations of long history. Experiments have shown that FSMN\nbased language models can significantly outperform not only feedforward neural\nnetwork (FNN) based LMs but also the popular recurrent neural network (RNN)\nLMs.\n",
        "published": "2015",
        "authors": [
            "ShiLiang Zhang",
            "Hui Jiang",
            "Si Wei",
            "LiRong Dai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1510.03820v4",
        "title": "A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional\n  Neural Networks for Sentence Classification",
        "abstract": "  Convolutional Neural Networks (CNNs) have recently achieved remarkably strong\nperformance on the practically important task of sentence classification (kim\n2014, kalchbrenner 2014, johnson 2014). However, these models require\npractitioners to specify an exact model architecture and set accompanying\nhyperparameters, including the filter region size, regularization parameters,\nand so on. It is currently unknown how sensitive model performance is to\nchanges in these configurations for the task of sentence classification. We\nthus conduct a sensitivity analysis of one-layer CNNs to explore the effect of\narchitecture components on model performance; our aim is to distinguish between\nimportant and comparatively inconsequential design decisions for sentence\nclassification. We focus on one-layer CNNs (to the exclusion of more complex\nmodels) due to their comparative simplicity and strong empirical performance,\nwhich makes it a modern standard baseline method akin to Support Vector Machine\n(SVMs) and logistic regression. We derive practical advice from our extensive\nempirical results for those interested in getting the most out of CNNs for\nsentence classification in real world settings.\n",
        "published": "2015",
        "authors": [
            "Ye Zhang",
            "Byron Wallace"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1510.08985v1",
        "title": "Prediction-Adaptation-Correction Recurrent Neural Networks for\n  Low-Resource Language Speech Recognition",
        "abstract": "  In this paper, we investigate the use of prediction-adaptation-correction\nrecurrent neural networks (PAC-RNNs) for low-resource speech recognition. A\nPAC-RNN is comprised of a pair of neural networks in which a {\\it correction}\nnetwork uses auxiliary information given by a {\\it prediction} network to help\nestimate the state probability. The information from the correction network is\nalso used by the prediction network in a recurrent loop. Our model outperforms\nother state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.\nMoreover, transfer learning from a language that is similar to the target\nlanguage can help improve performance further.\n",
        "published": "2015",
        "authors": [
            "Yu Zhang",
            "Ekapol Chuangsuwanich",
            "James Glass",
            "Dong Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1510.09202v1",
        "title": "Generating Text with Deep Reinforcement Learning",
        "abstract": "  We introduce a novel schema for sequence to sequence learning with a Deep\nQ-Network (DQN), which decodes the output sequence iteratively. The aim here is\nto enable the decoder to first tackle easier portions of the sequences, and\nthen turn to cope with difficult parts. Specifically, in each iteration, an\nencoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the\ninput sequence, automatically create features to represent the internal states\nof and formulate a list of potential actions for the DQN. Take rephrasing a\nnatural sentence as an example. This list can contain ranked potential words.\nNext, the DQN learns to make decision on which action (e.g., word) will be\nselected from the list to modify the current decoded sequence. The newly\nmodified output sequence is subsequently used as the input to the DQN for the\nnext decoding iteration. In each iteration, we also bias the reinforcement\nlearning's attention to explore sequence portions which are previously\ndifficult to be decoded. For evaluation, the proposed strategy was trained to\ndecode ten thousands natural sentences. Our experiments indicate that, when\ncompared to a left-to-right greedy beam search LSTM decoder, the proposed\nmethod performed competitively well when decoding sentences from the training\nset, but significantly outperformed the baseline when decoding unseen\nsentences, in terms of BLEU score obtained.\n",
        "published": "2015",
        "authors": [
            "Hongyu Guo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1601.06581v2",
        "title": "Character-Level Incremental Speech Recognition with Recurrent Neural\n  Networks",
        "abstract": "  In real-time speech recognition applications, the latency is an important\nissue. We have developed a character-level incremental speech recognition (ISR)\nsystem that responds quickly even during the speech, where the hypotheses are\ngradually improved while the speaking proceeds. The algorithm employs a\nspeech-to-character unidirectional recurrent neural network (RNN), which is\nend-to-end trained with connectionist temporal classification (CTC), and an\nRNN-based character-level language model (LM). The output values of the\nCTC-trained RNN are character-level probabilities, which are processed by beam\nsearch decoding. The RNN LM augments the decoding by providing long-term\ndependency information. We propose tree-based online beam search with\nadditional depth-pruning, which enables the system to process infinitely long\ninput speech with low latency. This system not only responds quickly on speech\nbut also can dictate out-of-vocabulary (OOV) words according to pronunciation.\nThe proposed model achieves the word error rate (WER) of 8.90% on the Wall\nStreet Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284\ntraining set.\n",
        "published": "2016",
        "authors": [
            "Kyuyeon Hwang",
            "Wonyong Sung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1602.07393v1",
        "title": "Domain Specific Author Attribution Based on Feedforward Neural Network\n  Language Models",
        "abstract": "  Authorship attribution refers to the task of automatically determining the\nauthor based on a given sample of text. It is a problem with a long history and\nhas a wide range of application. Building author profiles using language models\nis one of the most successful methods to automate this task. New language\nmodeling methods based on neural networks alleviate the curse of dimensionality\nand usually outperform conventional N-gram methods. However, there have not\nbeen much research applying them to authorship attribution. In this paper, we\npresent a novel setup of a Neural Network Language Model (NNLM) and apply it to\na database of text samples from different authors. We investigate how the NNLM\nperforms on a task with moderate author set size and relatively limited\ntraining and test data, and how the topics of the text samples affect the\naccuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement of\nfitness of a trained language model to the test data. Given 5 random test\nsentences, it also increases the author classification accuracy by 3.43% on\naverage, compared with the N-gram methods using SRILM tools. An open source\nimplementation of our methodology is freely available at\nhttps://github.com/zge/authorship-attribution/.\n",
        "published": "2016",
        "authors": [
            "Zhenhao Ge",
            "Yufang Sun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.00223v2",
        "title": "Segmental Recurrent Neural Networks for End-to-end Speech Recognition",
        "abstract": "  We study the segmental recurrent neural network for end-to-end acoustic\nmodelling. This model connects the segmental conditional random field (CRF)\nwith a recurrent neural network (RNN) used for feature extraction. Compared to\nmost previous CRF-based acoustic models, it does not rely on an external system\nto provide features or segmentation boundaries. Instead, this model\nmarginalises out all the possible segmentations, and features are extracted\nfrom the RNN trained together with the segmental CRF. In essence, this model is\nself-contained and can be trained end-to-end. In this paper, we discuss\npractical training and decoding issues as well as the method to speed up the\ntraining in the context of speech recognition. We performed experiments on the\nTIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass\ndecoding --- the best reported result using CRFs, despite the fact that we only\nused a zeroth-order CRF and without using any language model.\n",
        "published": "2016",
        "authors": [
            "Liang Lu",
            "Lingpeng Kong",
            "Chris Dyer",
            "Noah A. Smith",
            "Steve Renals"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.06042v2",
        "title": "Globally Normalized Transition-Based Neural Networks",
        "abstract": "  We introduce a globally normalized transition-based neural network model that\nachieves state-of-the-art part-of-speech tagging, dependency parsing and\nsentence compression results. Our model is a simple feed-forward neural network\nthat operates on a task-specific transition system, yet achieves comparable or\nbetter accuracies than recurrent models. We discuss the importance of global as\nopposed to local normalization: a key insight is that the label bias problem\nimplies that globally normalized models can be strictly more expressive than\nlocally normalized models.\n",
        "published": "2016",
        "authors": [
            "Daniel Andor",
            "Chris Alberti",
            "David Weiss",
            "Aliaksei Severyn",
            "Alessandro Presta",
            "Kuzman Ganchev",
            "Slav Petrov",
            "Michael Collins"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.06111v2",
        "title": "How Transferable are Neural Networks in NLP Applications?",
        "abstract": "  Transfer learning is aimed to make use of valuable knowledge in a source\ndomain to help model performance in a target domain. It is particularly\nimportant to neural networks, which are very likely to be overfitting. In some\nfields like image processing, many studies have shown the effectiveness of\nneural network-based transfer learning. For neural NLP, however, existing\nstudies have only casually applied transfer learning, and conclusions are\ninconsistent. In this paper, we conduct systematic case studies and provide an\nilluminating picture on the transferability of neural networks in NLP.\n",
        "published": "2016",
        "authors": [
            "Lili Mou",
            "Zhao Meng",
            "Rui Yan",
            "Ge Li",
            "Yan Xu",
            "Lu Zhang",
            "Zhi Jin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.07044v1",
        "title": "Recurrent Neural Network Encoder with Attention for Community Question\n  Answering",
        "abstract": "  We apply a general recurrent neural network (RNN) encoder framework to\ncommunity question answering (cQA) tasks. Our approach does not rely on any\nlinguistic processing, and can be applied to different languages or domains.\nFurther improvements are observed when we extend the RNN encoders with a neural\nattention mechanism that encourages reasoning over entire sequences. To deal\nwith practical issues such as data sparsity and imbalanced labels, we apply\nvarious techniques such as transfer learning and multitask learning. Our\nexperiments on the SemEval-2016 cQA task show 10% improvement on a MAP score\ncompared to an information retrieval-based approach, and achieve comparable\nperformance to a strong handcrafted feature-based method.\n",
        "published": "2016",
        "authors": [
            "Wei-Ning Hsu",
            "Yu Zhang",
            "James Glass"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.07646v1",
        "title": "Recursive Neural Language Architecture for Tag Prediction",
        "abstract": "  We consider the problem of learning distributed representations for tags from\ntheir associated content for the task of tag recommendation. Considering\ntagging information is usually very sparse, effective learning from content and\ntag association is very crucial and challenging task. Recently, various neural\nrepresentation learning models such as WSABIE and its variants show promising\nperformance, mainly due to compact feature representations learned in a\nsemantic space. However, their capacity is limited by a linear compositional\napproach for representing tags as sum of equal parts and hurt their\nperformance. In this work, we propose a neural feedback relevance model for\nlearning tag representations with weighted feature representations. Our\nexperiments on two widely used datasets show significant improvement for\nquality of recommendations over various baselines.\n",
        "published": "2016",
        "authors": [
            "Saurabh Kataria"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.08042v2",
        "title": "On the Compression of Recurrent Neural Networks with an Application to\n  LVCSR acoustic modeling for Embedded Speech Recognition",
        "abstract": "  We study the problem of compressing recurrent neural networks (RNNs). In\nparticular, we focus on the compression of RNN acoustic models, which are\nmotivated by the goal of building compact and accurate speech recognition\nsystems which can be run efficiently on mobile devices. In this work, we\npresent a technique for general recurrent model compression that jointly\ncompresses both recurrent and non-recurrent inter-layer weight matrices. We\nfind that the proposed technique allows us to reduce the size of our Long\nShort-Term Memory (LSTM) acoustic model to a third of its original size with\nnegligible loss in accuracy.\n",
        "published": "2016",
        "authors": [
            "Rohit Prabhavalkar",
            "Ouais Alsharif",
            "Antoine Bruguier",
            "Ian McGraw"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.08148v3",
        "title": "Pointing the Unknown Words",
        "abstract": "  The problem of rare and unknown words is an important issue that can\npotentially influence the performance of many NLP systems, including both the\ntraditional count-based and the deep learning models. We propose a novel way to\ndeal with the rare and unseen words for the neural network models using\nattention. Our model uses two softmax layers in order to predict the next word\nin conditional language models: one predicts the location of a word in the\nsource sentence, and the other predicts a word in the shortlist vocabulary. At\neach time-step, the decision of which softmax layer to use choose adaptively\nmade by an MLP which is conditioned on the context.~We motivate our work from a\npsychological evidence that humans naturally have a tendency to point towards\nobjects in the context or the environment when the name of an object is not\nknown.~We observe improvements on two tasks, neural machine translation on the\nEuroparl English to French parallel corpora and text summarization on the\nGigaword dataset using our proposed model.\n",
        "published": "2016",
        "authors": [
            "Caglar Gulcehre",
            "Sungjin Ahn",
            "Ramesh Nallapati",
            "Bowen Zhou",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.09381v1",
        "title": "Clinical Information Extraction via Convolutional Neural Network",
        "abstract": "  We report an implementation of a clinical information extraction tool that\nleverages deep neural network to annotate event spans and their attributes from\nraw clinical notes and pathology reports. Our approach uses context words and\ntheir part-of-speech tags and shape information as features. Then we hire\ntemporal (1D) convolutional neural network to learn hidden feature\nrepresentations. Finally, we use Multilayer Perceptron (MLP) to predict event\nspans. The empirical evaluation demonstrates that our approach significantly\noutperforms baselines.\n",
        "published": "2016",
        "authors": [
            "Peng Li",
            "Heng Huang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.09509v2",
        "title": "Learning Multiscale Features Directly From Waveforms",
        "abstract": "  Deep learning has dramatically improved the performance of speech recognition\nsystems through learning hierarchies of features optimized for the task at\nhand. However, true end-to-end learning, where features are learned directly\nfrom waveforms, has only recently reached the performance of hand-tailored\nrepresentations based on the Fourier transform. In this paper, we detail an\napproach to use convolutional filters to push past the inherent tradeoff of\ntemporal and frequency resolution that exists for spectral representations. At\nincreased computational cost, we show that increasing temporal resolution via\nreduced stride and increasing frequency resolution via additional filters\ndelivers significant performance improvements. Further, we find more efficient\nrepresentations by simultaneously learning at multiple scales, leading to an\noverall decrease in word error rate on a difficult internal speech test set by\n20.7% relative to networks with the same number of parameters trained on\nspectrograms.\n",
        "published": "2016",
        "authors": [
            "Zhenyao Zhu",
            "Jesse H. Engel",
            "Awni Hannun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1605.04655v2",
        "title": "Joint Learning of Sentence Embeddings for Relevance and Entailment",
        "abstract": "  We consider the problem of Recognizing Textual Entailment within an\nInformation Retrieval context, where we must simultaneously determine the\nrelevancy as well as degree of entailment for individual pieces of evidence to\ndetermine a yes/no answer to a binary natural language question.\n  We compare several variants of neural networks for sentence embeddings in a\nsetting of decision-making based on evidence of varying relevance. We propose a\nbasic model to integrate evidence for entailment, show that joint training of\nthe sentence embeddings to model relevance and entailment is feasible even with\nno explicit per-evidence supervision, and show the importance of evaluating\nstrong baselines. We also demonstrate the benefit of carrying over text\ncomprehension model trained on an unrelated task for our small datasets.\n  Our research is motivated primarily by a new open dataset we introduce,\nconsisting of binary questions and news-based evidence snippets. We also apply\nthe proposed relevance-entailment model on a similar task of ranking\nmultiple-choice test answers, evaluating it on a preliminary dataset of school\ntest questions as well as the standard MCTest dataset, where we improve the\nneural model state-of-art.\n",
        "published": "2016",
        "authors": [
            "Petr Baudis",
            "Silvestr Stanko",
            "Jan Sedivy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1605.08535v3",
        "title": "Deep API Learning",
        "abstract": "  Developers often wonder how to implement a certain functionality (e.g., how\nto parse XML files) using APIs. Obtaining an API usage sequence based on an\nAPI-related natural language query is very helpful in this regard. Given a\nquery, existing approaches utilize information retrieval models to search for\nmatching API sequences. These approaches treat queries and APIs as bag-of-words\n(i.e., keyword matching or word-to-word alignment) and lack a deep\nunderstanding of the semantics of the query.\n  We propose DeepAPI, a deep learning based approach to generate API usage\nsequences for a given natural language query. Instead of a bags-of-words\nassumption, it learns the sequence of words in a query and the sequence of\nassociated APIs. DeepAPI adapts a neural language model named RNN\nEncoder-Decoder. It encodes a word sequence (user query) into a fixed-length\ncontext vector, and generates an API sequence based on the context vector. We\nalso augment the RNN Encoder-Decoder by considering the importance of\nindividual APIs. We empirically evaluate our approach with more than 7 million\nannotated code snippets collected from GitHub. The results show that our\napproach generates largely accurate API sequences and outperforms the related\napproaches.\n",
        "published": "2016",
        "authors": [
            "Xiaodong Gu",
            "Hongyu Zhang",
            "Dongmei Zhang",
            "Sunghun Kim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1605.09186v4",
        "title": "Does Multimodality Help Human and Machine for Translation and Image\n  Captioning?",
        "abstract": "  This paper presents the systems developed by LIUM and CVC for the WMT16\nMultimodal Machine Translation challenge. We explored various comparative\nmethods, namely phrase-based systems and attentional recurrent neural networks\nmodels trained using monomodal or multimodal data. We also performed a human\nevaluation in order to estimate the usefulness of multimodal data for human\nmachine translation and image description generation. Our systems obtained the\nbest results for both tasks according to the automatic evaluation metrics BLEU\nand METEOR.\n",
        "published": "2016",
        "authors": [
            "Ozan Caglayan",
            "Walid Aransa",
            "Yaxing Wang",
            "Marc Masana",
            "Mercedes Garc\u00eda-Mart\u00ednez",
            "Fethi Bougares",
            "Lo\u00efc Barrault",
            "Joost van de Weijer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1608.00466v2",
        "title": "Learning Semantically Coherent and Reusable Kernels in Convolution\n  Neural Nets for Sentence Classification",
        "abstract": "  The state-of-the-art CNN models give good performance on sentence\nclassification tasks. The purpose of this work is to empirically study\ndesirable properties such as semantic coherence, attention mechanism and\nreusability of CNNs in these tasks. Semantically coherent kernels are\npreferable as they are a lot more interpretable for explaining the decision of\nthe learned CNN model. We observe that the learned kernels do not have semantic\ncoherence. Motivated by this observation, we propose to learn kernels with\nsemantic coherence using clustering scheme combined with Word2Vec\nrepresentation and domain knowledge such as SentiWordNet. We suggest a\ntechnique to visualize attention mechanism of CNNs for decision explanation\npurpose. Reusable property enables kernels learned on one problem to be used in\nanother problem. This helps in efficient learning as only a few additional\ndomain specific filters may have to be learned. We demonstrate the efficacy of\nour core ideas of learning semantically coherent kernels and leveraging\nreusable kernels for efficient learning on several benchmark datasets.\nExperimental results show the usefulness of our approach by achieving\nperformance close to the state-of-the-art methods but with semantic and\nreusable properties.\n",
        "published": "2016",
        "authors": [
            "Madhusudan Lakshmana",
            "Sundararajan Sellamanickam",
            "Shirish Shevade",
            "Keerthi Selvaraj"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1608.00895v2",
        "title": "RETURNN: The RWTH Extensible Training framework for Universal Recurrent\n  Neural Networks",
        "abstract": "  In this work we release our extensible and easily configurable neural network\ntraining software. It provides a rich set of functional layers with a\nparticular focus on efficient training of recurrent neural network topologies\non multiple GPUs. The source of the software package is public and freely\navailable for academic research purposes and can be used as a framework or as a\nstandalone tool which supports a flexible configuration. The software allows to\ntrain state-of-the-art deep bidirectional long short-term memory (LSTM) models\non both one dimensional data like speech or two dimensional data like\nhandwritten text and was used to develop successful submission systems in\nseveral evaluation campaigns.\n",
        "published": "2016",
        "authors": [
            "Patrick Doetsch",
            "Albert Zeyer",
            "Paul Voigtlaender",
            "Ilya Kulikov",
            "Ralf Schl\u00fcter",
            "Hermann Ney"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1608.02996v1",
        "title": "Towards cross-lingual distributed representations without parallel text\n  trained with adversarial autoencoders",
        "abstract": "  Current approaches to learning vector representations of text that are\ncompatible between different languages usually require some amount of parallel\ntext, aligned at word, sentence or at least document level. We hypothesize\nhowever, that different natural languages share enough semantic structure that\nit should be possible, in principle, to learn compatible vector representations\njust by analyzing the monolingual distribution of words.\n  In order to evaluate this hypothesis, we propose a scheme to map word vectors\ntrained on a source language to vectors semantically compatible with word\nvectors trained on a target language using an adversarial autoencoder.\n  We present preliminary qualitative results and discuss possible future\ndevelopments of this technique, such as applications to cross-lingual sentence\nrepresentations.\n",
        "published": "2016",
        "authors": [
            "Antonio Valerio Miceli Barone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1609.03777v2",
        "title": "Character-Level Language Modeling with Hierarchical Recurrent Neural\n  Networks",
        "abstract": "  Recurrent neural network (RNN) based character-level language models (CLMs)\nare extremely useful for modeling out-of-vocabulary words by nature. However,\ntheir performance is generally much worse than the word-level language models\n(WLMs), since CLMs need to consider longer history of tokens to properly\npredict the next one. We address this problem by proposing hierarchical RNN\narchitectures, which consist of multiple modules with different timescales.\nDespite the multi-timescale structures, the input and output layers operate\nwith the character-level clock, which allows the existing RNN CLM training\napproaches to be directly applicable without any modifications. Our CLM models\nshow better perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word\nBenchmark with only 2% of parameters. Also, we present real-time\ncharacter-level end-to-end speech recognition examples on the Wall Street\nJournal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the\nproposed models results in better recognition accuracies even though the number\nof parameters are reduced to 30%.\n",
        "published": "2016",
        "authors": [
            "Kyuyeon Hwang",
            "Wonyong Sung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1609.08337v1",
        "title": "Multi-task Recurrent Model for True Multilingual Speech Recognition",
        "abstract": "  Research on multilingual speech recognition remains attractive yet\nchallenging. Recent studies focus on learning shared structures under the\nmulti-task paradigm, in particular a feature sharing structure. This approach\nhas been found effective to improve performance on each individual language.\nHowever, this approach is only useful when the deployed system supports just\none language. In a true multilingual scenario where multiple languages are\nallowed, performance will be significantly reduced due to the competition among\nlanguages in the decoding space. This paper presents a multi-task recurrent\nmodel that involves a multilingual speech recognition (ASR) component and a\nlanguage recognition (LR) component, and the ASR component is informed of the\nlanguage information by the LR component, leading to a language-aware\nrecognition. We tested the approach on an English-Chinese bilingual recognition\ntask. The results show that the proposed multi-task recurrent model can improve\nperformance of multilingual recognition systems.\n",
        "published": "2016",
        "authors": [
            "Zhiyuan Tang",
            "Lantian Li",
            "Dong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1609.08789v3",
        "title": "Memory Visualization for Gated Recurrent Neural Networks in Speech\n  Recognition",
        "abstract": "  Recurrent neural networks (RNNs) have shown clear superiority in sequence\nmodeling, particularly the ones with gated units, such as long short-term\nmemory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties\nbehind the remarkable performance remain unclear in many applications, e.g.,\nautomatic speech recognition (ASR). This paper employs visualization techniques\nto study the behavior of LSTM and GRU when performing speech recognition tasks.\nOur experiments show some interesting patterns in the gated memory, and some of\nthem have inspired simple yet effective modifications on the network structure.\nWe report two of such modifications: (1) lazy cell update in LSTM, and (2)\nshortcut connections for residual learning. Both modifications lead to more\ncomprehensible and powerful networks.\n",
        "published": "2016",
        "authors": [
            "Zhiyuan Tang",
            "Ying Shi",
            "Dong Wang",
            "Yang Feng",
            "Shiyue Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1701.04313v1",
        "title": "End-to-End ASR-free Keyword Search from Speech",
        "abstract": "  End-to-end (E2E) systems have achieved competitive results compared to\nconventional hybrid hidden Markov model (HMM)-deep neural network based\nautomatic speech recognition (ASR) systems. Such E2E systems are attractive due\nto the lack of dependence on alignments between input acoustic and output\ngrapheme or HMM state sequence during training. This paper explores the design\nof an ASR-free end-to-end system for text query-based keyword search (KWS) from\nspeech trained with minimal supervision. Our E2E KWS system consists of three\nsub-systems. The first sub-system is a recurrent neural network (RNN)-based\nacoustic auto-encoder trained to reconstruct the audio through a\nfinite-dimensional representation. The second sub-system is a character-level\nRNN language model using embeddings learned from a convolutional neural\nnetwork. Since the acoustic and text query embeddings occupy different\nrepresentation spaces, they are input to a third feed-forward neural network\nthat predicts whether the query occurs in the acoustic utterance or not. This\nE2E ASR-free KWS system performs respectably despite lacking a conventional ASR\nsystem and trains much faster.\n",
        "published": "2017",
        "authors": [
            "Kartik Audhkhasi",
            "Andrew Rosenberg",
            "Abhinav Sethy",
            "Bhuvana Ramabhadran",
            "Brian Kingsbury"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1702.00887v3",
        "title": "Structured Attention Networks",
        "abstract": "  Attention networks have proven to be an effective approach for embedding\ncategorical inference within a deep neural network. However, for many tasks we\nmay want to model richer structural dependencies without abandoning end-to-end\ntraining. In this work, we experiment with incorporating richer structural\ndistributions, encoded using graphical models, within deep networks. We show\nthat these structured attention networks are simple extensions of the basic\nattention procedure, and that they allow for extending attention beyond the\nstandard soft-selection approach, such as attending to partial segmentations or\nto subtrees. We experiment with two different classes of structured attention\nnetworks: a linear-chain conditional random field and a graph-based parsing\nmodel, and describe how these models can be practically implemented as neural\nnetwork layers. Experiments show that this approach is effective for\nincorporating structural biases, and structured attention networks outperform\nbaseline attention models on a variety of synthetic and real tasks: tree\ntransduction, neural machine translation, question answering, and natural\nlanguage inference. We further find that models trained in this way learn\ninteresting unsupervised hidden representations that generalize simple\nattention.\n",
        "published": "2017",
        "authors": [
            "Yoon Kim",
            "Carl Denton",
            "Luong Hoang",
            "Alexander M. Rush"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1702.04770v1",
        "title": "Training Language Models Using Target-Propagation",
        "abstract": "  While Truncated Back-Propagation through Time (BPTT) is the most popular\napproach to training Recurrent Neural Networks (RNNs), it suffers from being\ninherently sequential (making parallelization difficult) and from truncating\ngradient flow between distant time-steps. We investigate whether Target\nPropagation (TPROP) style approaches can address these shortcomings.\nUnfortunately, extensive experiments suggest that TPROP generally underperforms\nBPTT, and we end with an analysis of this phenomenon, and suggestions for\nfuture work.\n",
        "published": "2017",
        "authors": [
            "Sam Wiseman",
            "Sumit Chopra",
            "Marc'Aurelio Ranzato",
            "Arthur Szlam",
            "Ruoyu Sun",
            "Soumith Chintala",
            "Nicolas Vasilache"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1702.07825v2",
        "title": "Deep Voice: Real-time Neural Text-to-Speech",
        "abstract": "  We present Deep Voice, a production-quality text-to-speech system constructed\nentirely from deep neural networks. Deep Voice lays the groundwork for truly\nend-to-end neural speech synthesis. The system comprises five major building\nblocks: a segmentation model for locating phoneme boundaries, a\ngrapheme-to-phoneme conversion model, a phoneme duration prediction model, a\nfundamental frequency prediction model, and an audio synthesis model. For the\nsegmentation model, we propose a novel way of performing phoneme boundary\ndetection with deep neural networks using connectionist temporal classification\n(CTC) loss. For the audio synthesis model, we implement a variant of WaveNet\nthat requires fewer parameters and trains faster than the original. By using a\nneural network for each component, our system is simpler and more flexible than\ntraditional text-to-speech systems, where each component requires laborious\nfeature engineering and extensive domain expertise. Finally, we show that\ninference with our system can be performed faster than real time and describe\noptimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x\nspeedups over existing implementations.\n",
        "published": "2017",
        "authors": [
            "Sercan O. Arik",
            "Mike Chrzanowski",
            "Adam Coates",
            "Gregory Diamos",
            "Andrew Gibiansky",
            "Yongguo Kang",
            "Xian Li",
            "John Miller",
            "Andrew Ng",
            "Jonathan Raiman",
            "Shubho Sengupta",
            "Mohammad Shoeybi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1702.08139v2",
        "title": "Improved Variational Autoencoders for Text Modeling using Dilated\n  Convolutions",
        "abstract": "  Recent work on generative modeling of text has found that variational\nauto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM\nlanguage models (Bowman et al., 2015). This negative result is so far poorly\nunderstood, but has been attributed to the propensity of LSTM decoders to\nignore conditioning information from the encoder. In this paper, we experiment\nwith a new type of decoder for VAE: a dilated CNN. By changing the decoder's\ndilation architecture, we control the effective context from previously\ngenerated words. In experiments, we find that there is a trade off between the\ncontextual capacity of the decoder and the amount of encoding information used.\nWe show that with the right decoder, VAE can outperform LSTM language models.\nWe demonstrate perplexity gains on two datasets, representing the first\npositive experimental result on the use VAE for generative modeling of text.\nFurther, we conduct an in-depth investigation of the use of VAE (with our new\ndecoding architecture) for semi-supervised and unsupervised labeling tasks,\ndemonstrating gains over several strong baselines.\n",
        "published": "2017",
        "authors": [
            "Zichao Yang",
            "Zhiting Hu",
            "Ruslan Salakhutdinov",
            "Taylor Berg-Kirkpatrick"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.00096v2",
        "title": "Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence\n  Labelling",
        "abstract": "  Most existing sequence labelling models rely on a fixed decomposition of a\ntarget sequence into a sequence of basic units. These methods suffer from two\nmajor drawbacks: 1) the set of basic units is fixed, such as the set of words,\ncharacters or phonemes in speech recognition, and 2) the decomposition of\ntarget sequences is fixed. These drawbacks usually result in sub-optimal\nperformance of modeling sequences. In this pa- per, we extend the popular CTC\nloss criterion to alleviate these limitations, and propose a new loss function\ncalled Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically\nlearns the best set of basic units (grams), as well as the most suitable\ndecomposition of tar- get sequences. Unlike CTC, Gram-CTC allows the model to\noutput variable number of characters at each time step, which enables the model\nto capture longer term dependency and improves the computational efficiency. We\ndemonstrate that the proposed Gram-CTC improves CTC in terms of both\nperformance and efficiency on the large vocabulary speech recognition task at\nmultiple scales of data, and that with Gram-CTC we can outperform the\nstate-of-the-art on a standard speech benchmark.\n",
        "published": "2017",
        "authors": [
            "Hairong Liu",
            "Zhenyao Zhu",
            "Xiangang Li",
            "Sanjeev Satheesh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.03939v1",
        "title": "Ask Me Even More: Dynamic Memory Tensor Networks (Extended Model)",
        "abstract": "  We examine Memory Networks for the task of question answering (QA), under\ncommon real world scenario where training examples are scarce and under weakly\nsupervised scenario, that is only extrinsic labels are available for training.\nWe propose extensions for the Dynamic Memory Network (DMN), specifically within\nthe attention mechanism, we call the resulting Neural Architecture as Dynamic\nMemory Tensor Network (DMTN). Ultimately, we see that our proposed extensions\nresults in over 80% improvement in the number of task passed against the\nbaselined standard DMN and 20% more task passed compared to state-of-the-art\nEnd-to-End Memory Network for Facebook's single task weakly trained 1K bAbi\ndataset.\n",
        "published": "2017",
        "authors": [
            "Govardana Sachithanandam Ramachandran",
            "Ajay Sohmshetty"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.10356v2",
        "title": "Simplified End-to-End MMI Training and Voting for ASR",
        "abstract": "  A simplified speech recognition system that uses the maximum mutual\ninformation (MMI) criterion is considered. End-to-end training using gradient\ndescent is suggested, similarly to the training of connectionist temporal\nclassification (CTC). We use an MMI criterion with a simple language model in\nthe training stage, and a standard HMM decoder. Our method compares favorably\nto CTC in terms of performance, robustness, decoding time, disk footprint and\nquality of alignments. The good alignments enable the use of a straightforward\nensemble method, obtained by simply averaging the predictions of several neural\nnetwork models, that were trained separately end-to-end. The ensemble method\nyields a considerable reduction in the word error rate.\n",
        "published": "2017",
        "authors": [
            "Lior Fritz",
            "David Burshtein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.01346v1",
        "title": "Going Wider: Recurrent Neural Network With Parallel Cells",
        "abstract": "  Recurrent Neural Network (RNN) has been widely applied for sequence modeling.\nIn RNN, the hidden states at current step are full connected to those at\nprevious step, thus the influence from less related features at previous step\nmay potentially decrease model's learning ability. We propose a simple\ntechnique called parallel cells (PCs) to enhance the learning ability of\nRecurrent Neural Network (RNN). In each layer, we run multiple small RNN cells\nrather than one single large cell. In this paper, we evaluate PCs on 2 tasks.\nOn language modeling task on PTB (Penn Tree Bank), our model outperforms state\nof art models by decreasing perplexity from 78.6 to 75.3. On Chinese-English\ntranslation task, our model increases BLEU score for 0.39 points than baseline\nmodel.\n",
        "published": "2017",
        "authors": [
            "Danhao Zhu",
            "Si Shen",
            "Xin-Yu Dai",
            "Jiajun Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.03151v3",
        "title": "Phonetic Temporal Neural Model for Language Identification",
        "abstract": "  Deep neural models, particularly the LSTM-RNN model, have shown great\npotential for language identification (LID). However, the use of phonetic\ninformation has been largely overlooked by most existing neural LID methods,\nalthough this information has been used very successfully in conventional\nphonetic LID systems. We present a phonetic temporal neural model for LID,\nwhich is an LSTM-RNN LID system that accepts phonetic features produced by a\nphone-discriminative DNN as the input, rather than raw acoustic features. This\nnew model is similar to traditional phonetic LID methods, but the phonetic\nknowledge here is much richer: it is at the frame level and involves compacted\ninformation of all phones. Our experiments conducted on the Babel database and\nthe AP16-OLR database demonstrate that the temporal phonetic neural approach is\nvery effective, and significantly outperforms existing acoustic neural models.\nIt also outperforms the conventional i-vector approach on short utterances and\nin noisy conditions.\n",
        "published": "2017",
        "authors": [
            "Zhiyuan Tang",
            "Dong Wang",
            "Yixiang Chen",
            "Lantian Li",
            "Andrew Abel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.03152v2",
        "title": "Phone-aware Neural Language Identification",
        "abstract": "  Pure acoustic neural models, particularly the LSTM-RNN model, have shown\ngreat potential in language identification (LID). However, the phonetic\ninformation has been largely overlooked by most of existing neural LID models,\nalthough this information has been used in the conventional phonetic LID\nsystems with a great success. We present a phone-aware neural LID architecture,\nwhich is a deep LSTM-RNN LID system but accepts output from an RNN-based ASR\nsystem. By utilizing the phonetic knowledge, the LID performance can be\nsignificantly improved. Interestingly, even if the test language is not\ninvolved in the ASR training, the phonetic knowledge still presents a large\ncontribution. Our experiments conducted on four languages within the Babel\ncorpus demonstrated that the phone-aware approach is highly effective.\n",
        "published": "2017",
        "authors": [
            "Zhiyuan Tang",
            "Dong Wang",
            "Yixiang Chen",
            "Ying Shi",
            "Lantian Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.03556v2",
        "title": "Relevance-based Word Embedding",
        "abstract": "  Learning a high-dimensional dense representation for vocabulary terms, also\nknown as a word embedding, has recently attracted much attention in natural\nlanguage processing and information retrieval tasks. The embedding vectors are\ntypically learned based on term proximity in a large corpus. This means that\nthe objective in well-known word embedding algorithms, e.g., word2vec, is to\naccurately predict adjacent word(s) for a given word or context. However, this\nobjective is not necessarily equivalent to the goal of many information\nretrieval (IR) tasks. The primary objective in various IR tasks is to capture\nrelevance instead of term proximity, syntactic, or even semantic similarity.\nThis is the motivation for developing unsupervised relevance-based word\nembedding models that learn word representations based on query-document\nrelevance information. In this paper, we propose two learning models with\ndifferent objective functions; one learns a relevance distribution over the\nvocabulary set for each query, and the other classifies each term as belonging\nto the relevant or non-relevant class for each query. To train our models, we\nused over six million unique queries and the top ranked documents retrieved in\nresponse to each query, which are assumed to be relevant to the query. We\nextrinsically evaluate our learned word representation models using two IR\ntasks: query expansion and query classification. Both query expansion\nexperiments on four TREC collections and query classification experiments on\nthe KDD Cup 2005 dataset suggest that the relevance-based word embedding models\nsignificantly outperform state-of-the-art proximity-based embedding models,\nsuch as word2vec and GloVe.\n",
        "published": "2017",
        "authors": [
            "Hamed Zamani",
            "W. Bruce Croft"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.09037v3",
        "title": "Deriving Neural Architectures from Sequence and Graph Kernels",
        "abstract": "  The design of neural architectures for structured objects is typically guided\nby experimental insights rather than a formal process. In this work, we appeal\nto kernels over combinatorial structures, such as sequences and graphs, to\nderive appropriate neural operations. We introduce a class of deep recurrent\nneural operations and formally characterize their associated kernel spaces. Our\nrecurrent modules compare the input to virtual reference objects (cf. filters\nin CNN) via the kernels. Similar to traditional neural operations, these\nreference objects are parameterized and directly optimized in end-to-end\ntraining. We empirically evaluate the proposed class of neural architectures on\nstandard applications such as language modeling and molecular graph regression,\nachieving state-of-the-art results across these applications.\n",
        "published": "2017",
        "authors": [
            "Tao Lei",
            "Wengong Jin",
            "Regina Barzilay",
            "Tommi Jaakkola"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.10209v1",
        "title": "On Multilingual Training of Neural Dependency Parsers",
        "abstract": "  We show that a recently proposed neural dependency parser can be improved by\njoint training on multiple languages from the same family. The parser is\nimplemented as a deep neural network whose only input is orthographic\nrepresentations of words. In order to successfully parse, the network has to\ndiscover how linguistically relevant concepts can be inferred from word\nspellings. We analyze the representations of characters and words that are\nlearned by the network to establish which properties of languages were\naccounted for. In particular we show that the parser has approximately learned\nto associate Latin characters with their Cyrillic counterparts and that it can\ngroup Polish and Russian words that have a similar grammatical function.\nFinally, we evaluate the parser on selected languages from the Universal\nDependencies dataset and show that it is competitive with other recently\nproposed state-of-the art methods, while having a simple structure.\n",
        "published": "2017",
        "authors": [
            "Micha\u0142 Zapotoczny",
            "Pawe\u0142 Rychlikowski",
            "Jan Chorowski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1706.02124v2",
        "title": "Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks",
        "abstract": "  Ladder networks are a notable new concept in the field of semi-supervised\nlearning by showing state-of-the-art results in image recognition tasks while\nbeing compatible with many existing neural architectures. We present the\nrecurrent ladder network, a novel modification of the ladder network, for\nsemi-supervised learning of recurrent neural networks which we evaluate with a\nphoneme recognition task on the TIMIT corpus. Our results show that the model\nis able to consistently outperform the baseline and achieve fully-supervised\nbaseline performance with only 75% of all labels which demonstrates that the\nmodel is capable of using unsupervised data as an effective regulariser.\n",
        "published": "2017",
        "authors": [
            "Marian Tietz",
            "Tayfun Alpay",
            "Johannes Twiefel",
            "Stefan Wermter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1706.04223v3",
        "title": "Adversarially Regularized Autoencoders",
        "abstract": "  Deep latent variable models, trained using variational autoencoders or\ngenerative adversarial networks, are now a key technique for representation\nlearning of continuous structures. However, applying similar methods to\ndiscrete structures, such as text sequences or discretized images, has proven\nto be more challenging. In this work, we propose a flexible method for training\ndeep latent variable models of discrete structures. Our approach is based on\nthe recently-proposed Wasserstein autoencoder (WAE) which formalizes the\nadversarial autoencoder (AAE) as an optimal transport problem. We first extend\nthis framework to model discrete sequences, and then further explore different\nlearned priors targeting a controllable representation. This adversarially\nregularized autoencoder (ARAE) allows us to generate natural textual outputs as\nwell as perform manipulations in the latent space to induce change in the\noutput space. Finally we show that the latent representation can be trained to\nperform unaligned textual style transfer, giving improvements both in\nautomatic/human evaluation compared to existing methods.\n",
        "published": "2017",
        "authors": [
            "Jake Zhao",
            "Yoon Kim",
            "Kelly Zhang",
            "Alexander M. Rush",
            "Yann LeCun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.05227v1",
        "title": "Auxiliary Objectives for Neural Error Detection Models",
        "abstract": "  We investigate the utility of different auxiliary objectives and training\nstrategies within a neural sequence labeling approach to error detection in\nlearner writing. Auxiliary costs provide the model with additional linguistic\ninformation, allowing it to learn general-purpose compositional features that\ncan then be exploited for other objectives. Our experiments show that a joint\nlearning approach trained with parallel labels on in-domain data improves\nperformance over the previous best error detection system. While the resulting\nmodel has the same number of parameters, the additional objectives allow it to\nbe optimised more efficiently and achieve better performance.\n",
        "published": "2017",
        "authors": [
            "Marek Rei",
            "Helen Yannakoudakis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.05233v1",
        "title": "Detecting Off-topic Responses to Visual Prompts",
        "abstract": "  Automated methods for essay scoring have made great progress in recent years,\nachieving accuracies very close to human annotators. However, a known weakness\nof such automated scorers is not taking into account the semantic relevance of\nthe submitted text. While there is existing work on detecting answer relevance\ngiven a textual prompt, very little previous research has been done to\nincorporate visual writing prompts. We propose a neural architecture and\nseveral extensions for detecting off-topic responses to visual prompts and\nevaluate it on a dataset of texts written by language learners.\n",
        "published": "2017",
        "authors": [
            "Marek Rei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.06841v1",
        "title": "An Error-Oriented Approach to Word Embedding Pre-Training",
        "abstract": "  We propose a novel word embedding pre-training approach that exploits writing\nerrors in learners' scripts. We compare our method to previous models that tune\nthe embeddings based on script scores and the discrimination between correct\nand corrupt word contexts in addition to the generic commonly-used embeddings\npre-trained on large corpora. The comparison is achieved by using the\naforementioned models to bootstrap a neural network that learns to predict a\nholistic score for scripts. Furthermore, we investigate augmenting our model\nwith error corrections and monitor the impact on performance. Our results show\nthat our error-oriented approach outperforms other comparable ones which is\nfurther demonstrated when training on more data. Additionally, extending the\nmodel with corrections provides further performance gains when data sparsity is\nan issue.\n",
        "published": "2017",
        "authors": [
            "Youmna Farag",
            "Marek Rei",
            "Ted Briscoe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.08214v2",
        "title": "Dual Rectified Linear Units (DReLUs): A Replacement for Tanh Activation\n  Functions in Quasi-Recurrent Neural Networks",
        "abstract": "  In this paper, we introduce a novel type of Rectified Linear Unit (ReLU),\ncalled a Dual Rectified Linear Unit (DReLU). A DReLU, which comes with an\nunbounded positive and negative image, can be used as a drop-in replacement for\na tanh activation function in the recurrent step of Quasi-Recurrent Neural\nNetworks (QRNNs) (Bradbury et al. (2017)). Similar to ReLUs, DReLUs are less\nprone to the vanishing gradient problem, they are noise robust, and they induce\nsparse activations.\n  We independently reproduce the QRNN experiments of Bradbury et al. (2017) and\ncompare our DReLU-based QRNNs with the original tanh-based QRNNs and Long\nShort-Term Memory networks (LSTMs) on sentiment classification and word-level\nlanguage modeling. Additionally, we evaluate on character-level language\nmodeling, showing that we are able to stack up to eight QRNN layers with\nDReLUs, thus making it possible to improve the current state-of-the-art in\ncharacter-level language modeling over shallow architectures based on LSTMs.\n",
        "published": "2017",
        "authors": [
            "Fr\u00e9deric Godin",
            "Jonas Degrave",
            "Joni Dambre",
            "Wesley De Neve"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.02799v2",
        "title": "Fidelity-Weighted Learning",
        "abstract": "  Training deep neural networks requires many training samples, but in practice\ntraining labels are expensive to obtain and may be of varying quality, as some\nmay be from trusted expert labelers while others might be from heuristics or\nother sources of weak supervision such as crowd-sourcing. This creates a\nfundamental quality versus-quantity trade-off in the learning process. Do we\nlearn from the small amount of high-quality data or the potentially large\namount of weakly-labeled data? We argue that if the learner could somehow know\nand take the label-quality into account when learning the data representation,\nwe could get the best of both worlds. To this end, we propose\n\"fidelity-weighted learning\" (FWL), a semi-supervised student-teacher approach\nfor training deep neural networks using weakly-labeled data. FWL modulates the\nparameter updates to a student network (trained on the task we care about) on a\nper-sample basis according to the posterior confidence of its label-quality\nestimated by a teacher (who has access to the high-quality labels). Both\nstudent and teacher are learned from the data. We evaluate FWL on two tasks in\ninformation retrieval and natural language processing where we outperform\nstate-of-the-art alternative semi-supervised methods, indicating that our\napproach makes better use of strong and weak labels, and leads to better\ntask-dependent data representations.\n",
        "published": "2017",
        "authors": [
            "Mostafa Dehghani",
            "Arash Mehrjou",
            "Stephan Gouws",
            "Jaap Kamps",
            "Bernhard Sch\u00f6lkopf"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.00628v1",
        "title": "A Novel Framework for Recurrent Neural Networks with Enhancing\n  Information Processing and Transmission between Units",
        "abstract": "  This paper proposes a novel framework for recurrent neural networks (RNNs)\ninspired by the human memory models in the field of cognitive neuroscience to\nenhance information processing and transmission between adjacent RNNs' units.\nThe proposed framework for RNNs consists of three stages that is working\nmemory, forget, and long-term store. The first stage includes taking input data\ninto sensory memory and transferring it to working memory for preliminary\ntreatment. And the second stage mainly focuses on proactively forgetting the\nsecondary information rather than the primary in the working memory. And\nfinally, we get the long-term store normally using some kind of RNN's unit. Our\nframework, which is generalized and simple, is evaluated on 6 datasets which\nfall into 3 different tasks, corresponding to text classification, image\nclassification and language modelling. Experiments reveal that our framework\ncan obviously improve the performance of traditional recurrent neural networks.\nAnd exploratory task shows the ability of our framework of correctly forgetting\nthe secondary information.\n",
        "published": "2018",
        "authors": [
            "Xi Chen",
            "Zhihong Deng",
            "Gehui Shen",
            "Ting Huang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.08760v3",
        "title": "Combination of Domain Knowledge and Deep Learning for Sentiment Analysis",
        "abstract": "  The emerging technique of deep learning has been widely applied in many\ndifferent areas. However, when adopted in a certain specific domain, this\ntechnique should be combined with domain knowledge to improve efficiency and\naccuracy. In particular, when analyzing the applications of deep learning in\nsentiment analysis, we found that the current approaches are suffering from the\nfollowing drawbacks: (i) the existing works have not paid much attention to the\nimportance of different types of sentiment terms, which is an important concept\nin this area; and (ii) the loss function currently employed does not well\nreflect the degree of error of sentiment misclassification. To overcome such\nproblem, we propose to combine domain knowledge with deep learning. Our\nproposal includes using sentiment scores, learnt by quadratic programming, to\naugment training data; and introducing the penalty matrix for enhancing the\nloss function of cross entropy. When experimented, we achieved a significant\nimprovement in classification results.\n",
        "published": "2018",
        "authors": [
            "Khuong Vo",
            "Dang Pham",
            "Mao Nguyen",
            "Trung Mai",
            "Tho Quan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.11420v1",
        "title": "Discourse-Wizard: Discovering Deep Discourse Structure in your\n  Conversation with RNNs",
        "abstract": "  Spoken language understanding is one of the key factors in a dialogue system,\nand a context in a conversation plays an important role to understand the\ncurrent utterance. In this work, we demonstrate the importance of context\nwithin the dialogue for neural network models through an online web interface\nlive demo. We developed two different neural models: a model that does not use\ncontext and a context-based model. The no-context model classifies dialogue\nacts at an utterance-level whereas the context-based model takes some preceding\nutterances into account. We make these trained neural models available as a\nlive demo called Discourse-Wizard using a modular server architecture. The live\ndemo provides an easy to use interface for conversational analysis and for\ndiscovering deep discourse structures in a conversation.\n",
        "published": "2018",
        "authors": [
            "Chandrakant Bothe",
            "Sven Magg",
            "Cornelius Weber",
            "Stefan Wermter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.00659v1",
        "title": "Implicit Regularization of Stochastic Gradient Descent in Natural\n  Language Processing: Observations and Implications",
        "abstract": "  Deep neural networks with remarkably strong generalization performances are\nusually over-parameterized. Despite explicit regularization strategies are used\nfor practitioners to avoid over-fitting, the impacts are often small. Some\ntheoretical studies have analyzed the implicit regularization effect of\nstochastic gradient descent (SGD) on simple machine learning models with\ncertain assumptions. However, how it behaves practically in state-of-the-art\nmodels and real-world datasets is still unknown. To bridge this gap, we study\nthe role of SGD implicit regularization in deep learning systems. We show pure\nSGD tends to converge to minimas that have better generalization performances\nin multiple natural language processing (NLP) tasks. This phenomenon coexists\nwith dropout, an explicit regularizer. In addition, neural network's finite\nlearning capability does not impact the intrinsic nature of SGD's implicit\nregularization effect. Specifically, under limited training samples or with\ncertain corrupted labels, the implicit regularization effect remains strong. We\nfurther analyze the stability by varying the weight initialization range. We\ncorroborate these experimental findings with a decision boundary visualization\nusing a 3-layer neural network for interpretation. Altogether, our work enables\na deepened understanding on how implicit regularization affects the deep\nlearning model and sheds light on the future study of the over-parameterized\nmodel's generalization ability.\n",
        "published": "2018",
        "authors": [
            "Deren Lei",
            "Zichen Sun",
            "Yijun Xiao",
            "William Yang Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.00998v1",
        "title": "Analysing Dropout and Compounding Errors in Neural Language Models",
        "abstract": "  This paper carries out an empirical analysis of various dropout techniques\nfor language modelling, such as Bernoulli dropout, Gaussian dropout, Curriculum\nDropout, Variational Dropout and Concrete Dropout. Moreover, we propose an\nextension of variational dropout to concrete dropout and curriculum dropout\nwith varying schedules. We find these extensions to perform well when compared\nto standard dropout approaches, particularly variational curriculum dropout\nwith a linear schedule. Largest performance increases are made when applying\ndropout on the decoder layer. Lastly, we analyze where most of the errors occur\nat test time as a post-analysis step to determine if the well-known problem of\ncompounding errors is apparent and to what end do the proposed methods mitigate\nthis issue for each dataset. We report results on a 2-hidden layer LSTM, GRU\nand Highway network with embedding dropout, dropout on the gated hidden layers\nand the output projection layer for each model. We report our results on\nPenn-TreeBank and WikiText-2 word-level language modelling datasets, where the\nformer reduces the long-tail distribution through preprocessing and one which\npreserves rare words in the training and test set.\n",
        "published": "2018",
        "authors": [
            "James O' Neill",
            "Danushka Bollegala"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.05949v1",
        "title": "Jointly Learning to Label Sentences and Tokens",
        "abstract": "  Learning to construct text representations in end-to-end systems can be\ndifficult, as natural languages are highly compositional and task-specific\nannotated datasets are often limited in size. Methods for directly supervising\nlanguage composition can allow us to guide the models based on existing\nknowledge, regularizing them towards more robust and interpretable\nrepresentations. In this paper, we investigate how objectives at different\ngranularities can be used to learn better language representations and we\npropose an architecture for jointly learning to label sentences and tokens. The\npredictions at each level are combined together using an attention mechanism,\nwith token-level labels also acting as explicit supervision for composing\nsentence-level representations. Our experiments show that by learning to\nperform these tasks jointly on multiple levels, the model achieves substantial\nimprovements for both sentence classification and sequence labeling.\n",
        "published": "2018",
        "authors": [
            "Marek Rei",
            "Anders S\u00f8gaard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.06477v1",
        "title": "Multi-cell LSTM Based Neural Language Model",
        "abstract": "  Language models, being at the heart of many NLP problems, are always of great\ninterest to researchers. Neural language models come with the advantage of\ndistributed representations and long range contexts. With its particular\ndynamics that allow the cycling of information within the network, `Recurrent\nneural network' (RNN) becomes an ideal paradigm for neural language modeling.\nLong Short-Term Memory (LSTM) architecture solves the inadequacies of the\nstandard RNN in modeling long-range contexts. In spite of a plethora of RNN\nvariants, possibility to add multiple memory cells in LSTM nodes was seldom\nexplored. Here we propose a multi-cell node architecture for LSTMs and study\nits applicability for neural language modeling. The proposed multi-cell LSTM\nlanguage models outperform the state-of-the-art results on well-known Penn\nTreebank (PTB) setup.\n",
        "published": "2018",
        "authors": [
            "Thomas Cherian",
            "Akshay Badola",
            "Vineet Padmanabhan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.07453v2",
        "title": "The PyTorch-Kaldi Speech Recognition Toolkit",
        "abstract": "  The availability of open-source software is playing a remarkable role in the\npopularization of speech recognition and deep learning. Kaldi, for instance, is\nnowadays an established framework used to develop state-of-the-art speech\nrecognizers. PyTorch is used to build neural networks with the Python language\nand has recently spawn tremendous interest within the machine learning\ncommunity thanks to its simplicity and flexibility.\n  The PyTorch-Kaldi project aims to bridge the gap between these popular\ntoolkits, trying to inherit the efficiency of Kaldi and the flexibility of\nPyTorch. PyTorch-Kaldi is not only a simple interface between these software,\nbut it embeds several useful features for developing modern speech recognizers.\nFor instance, the code is specifically designed to naturally plug-in\nuser-defined acoustic models. As an alternative, users can exploit several\npre-implemented neural networks that can be customized using intuitive\nconfiguration files. PyTorch-Kaldi supports multiple feature and label streams\nas well as combinations of neural networks, enabling the use of complex neural\narchitectures. The toolkit is publicly-released along with a rich documentation\nand is designed to properly work locally or on HPC clusters.\n  Experiments, that are conducted on several datasets and tasks, show that\nPyTorch-Kaldi can effectively be used to develop modern state-of-the-art speech\nrecognizers.\n",
        "published": "2018",
        "authors": [
            "Mirco Ravanelli",
            "Titouan Parcollet",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.09725v2",
        "title": "Interpretable Convolutional Filters with SincNet",
        "abstract": "  Deep learning is currently playing a crucial role toward higher levels of\nartificial intelligence. This paradigm allows neural networks to learn complex\nand abstract representations, that are progressively obtained by combining\nsimpler ones. Nevertheless, the internal \"black-box\" representations\nautomatically discovered by current neural architectures often suffer from a\nlack of interpretability, making of primary interest the study of explainable\nmachine learning techniques. This paper summarizes our recent efforts to\ndevelop a more interpretable neural model for directly processing speech from\nthe raw waveform. In particular, we propose SincNet, a novel Convolutional\nNeural Network (CNN) that encourages the first layer to discover more\nmeaningful filters by exploiting parametrized sinc functions. In contrast to\nstandard CNNs, which learn all the elements of each filter, only low and high\ncutoff frequencies of band-pass filters are directly learned from data. This\ninductive bias offers a very compact way to derive a customized filter-bank\nfront-end, that only depends on some parameters with a clear physical meaning.\nOur experiments, conducted on both speaker and speech recognition, show that\nthe proposed architecture converges faster, performs better, and is more\ninterpretable than standard CNNs.\n",
        "published": "2018",
        "authors": [
            "Mirco Ravanelli",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.01216v1",
        "title": "Transfer learning from language models to image caption generators:\n  Better models may not transfer better",
        "abstract": "  When designing a neural caption generator, a convolutional neural network can\nbe used to extract image features. Is it possible to also use a neural language\nmodel to extract sentence prefix features? We answer this question by trying\ndifferent ways to transfer the recurrent neural network and embedding layer\nfrom a neural language model to an image caption generator. We find that image\ncaption generators with transferred parameters perform better than those\ntrained from scratch, even when simply pre-training them on the text of the\nsame captions dataset it will later be trained on. We also find that the best\nlanguage models (in terms of perplexity) do not result in the best caption\ngenerators after transfer learning.\n",
        "published": "2019",
        "authors": [
            "Marc Tanti",
            "Albert Gatt",
            "Kenneth P. Camilleri"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.02671v1",
        "title": "Is it Time to Swish? Comparing Deep Learning Activation Functions Across\n  NLP tasks",
        "abstract": "  Activation functions play a crucial role in neural networks because they are\nthe nonlinearities which have been attributed to the success story of deep\nlearning. One of the currently most popular activation functions is ReLU, but\nseveral competitors have recently been proposed or 'discovered', including\nLReLU functions and swish. While most works compare newly proposed activation\nfunctions on few tasks (usually from image classification) and against few\ncompetitors (usually ReLU), we perform the first large-scale comparison of 21\nactivation functions across eight different NLP tasks. We find that a largely\nunknown activation function performs most stably across all tasks, the\nso-called penalized tanh function. We also show that it can successfully\nreplace the sigmoid and tanh gates in LSTM cells, leading to a 2 percentage\npoint (pp) improvement over the standard choices on a challenging NLP task.\n",
        "published": "2019",
        "authors": [
            "Steffen Eger",
            "Paul Youssef",
            "Iryna Gurevych"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.06050v2",
        "title": "Combination of Domain Knowledge and Deep Learning for Sentiment Analysis\n  of Short and Informal Messages on Social Media",
        "abstract": "  Sentiment analysis has been emerging recently as one of the major natural\nlanguage processing (NLP) tasks in many applications. Especially, as social\nmedia channels (e.g. social networks or forums) have become significant sources\nfor brands to observe user opinions about their products, this task is thus\nincreasingly crucial. However, when applied with real data obtained from social\nmedia, we notice that there is a high volume of short and informal messages\nposted by users on those channels. This kind of data makes the existing works\nsuffer from many difficulties to handle, especially ones using deep learning\napproaches. In this paper, we propose an approach to handle this problem. This\nwork is extended from our previous work, in which we proposed to combine the\ntypical deep learning technique of Convolutional Neural Networks with domain\nknowledge. The combination is used for acquiring additional training data\naugmentation and a more reasonable loss function. In this work, we further\nimprove our architecture by various substantial enhancements, including\nnegation-based data augmentation, transfer learning for word embeddings, the\ncombination of word-level embeddings and character-level embeddings, and using\nmultitask learning technique for attaching domain knowledge rules in the\nlearning process. Those enhancements, specifically aiming to handle short and\ninformal messages, help us to enjoy significant improvement in performance once\nexperimenting on real datasets.\n",
        "published": "2019",
        "authors": [
            "Khuong Vo",
            "Tri Nguyen",
            "Dang Pham",
            "Mao Nguyen",
            "Minh Truong",
            "Trung Mai",
            "Tho Quan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.00998v1",
        "title": "SummAE: Zero-Shot Abstractive Text Summarization using Length-Agnostic\n  Auto-Encoders",
        "abstract": "  We propose an end-to-end neural model for zero-shot abstractive text\nsummarization of paragraphs, and introduce a benchmark task, ROCSumm, based on\nROCStories, a subset for which we collected human summaries. In this task,\nfive-sentence stories (paragraphs) are summarized with one sentence, using\nhuman summaries only for evaluation. We show results for extractive and human\nbaselines to demonstrate a large abstractive gap in performance. Our model,\nSummAE, consists of a denoising auto-encoder that embeds sentences and\nparagraphs in a common space, from which either can be decoded. Summaries for\nparagraphs are generated by decoding a sentence from the paragraph\nrepresentations. We find that traditional sequence-to-sequence auto-encoders\nfail to produce good summaries and describe how specific architectural choices\nand pre-training techniques can significantly improve performance,\noutperforming extractive baselines. The data, training, evaluation code, and\nbest model weights are open-sourced.\n",
        "published": "2019",
        "authors": [
            "Peter J. Liu",
            "Yu-An Chung",
            "Jie Ren"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.00502v1",
        "title": "Deep Learning Approach for Intelligent Named Entity Recognition of Cyber\n  Security",
        "abstract": "  In recent years, the amount of Cyber Security data generated in the form of\nunstructured texts, for example, social media resources, blogs, articles, and\nso on has exceptionally increased. Named Entity Recognition (NER) is an initial\nstep towards converting this unstructured data into structured data which can\nbe used by a lot of applications. The existing methods on NER for Cyber\nSecurity data are based on rules and linguistic characteristics. A Deep\nLearning (DL) based approach embedded with Conditional Random Fields (CRFs) is\nproposed in this paper. Several DL architectures are evaluated to find the most\noptimal architecture. The combination of Bidirectional Gated Recurrent Unit\n(Bi-GRU), Convolutional Neural Network (CNN), and CRF performed better compared\nto various other DL frameworks on a publicly available benchmark dataset. This\nmay be due to the reason that the bidirectional structures preserve the\nfeatures related to the future and previous words in a sequence.\n",
        "published": "2020",
        "authors": [
            "Simran K",
            "Sriram S",
            "Vinayakumar R",
            "Soman KP"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.02555v3",
        "title": "Joint Embedding of Words and Category Labels for Hierarchical\n  Multi-label Text Classification",
        "abstract": "  Text classification has become increasingly challenging due to the continuous\nrefinement of classification label granularity and the expansion of\nclassification label scale. To address that, some research has been applied\nonto strategies that exploit the hierarchical structure in problems with a\nlarge number of categories. At present, hierarchical text classification (HTC)\nhas received extensive attention and has broad application prospects. Making\nfull use of the relationship between parent category and child category in text\nclassification task can greatly improve the performance of classification. In\nthis paper, We propose a joint embedding of text and parent category based on\nhierarchical fine-tuning ordered neurons LSTM (HFT-ONLSTM) for HTC. Our method\nmakes full use of the connection between the upper-level and lower-level\nlabels. Experiments show that our model outperforms the state-of-the-art\nhierarchical model at a lower computation cost.\n",
        "published": "2020",
        "authors": [
            "Jingpeng Zhao",
            "Yinglong Ma"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.04902v1",
        "title": "An In-depth Walkthrough on Evolution of Neural Machine Translation",
        "abstract": "  Neural Machine Translation (NMT) methodologies have burgeoned from using\nsimple feed-forward architectures to the state of the art; viz. BERT model. The\nuse cases of NMT models have been broadened from just language translations to\nconversational agents (chatbots), abstractive text summarization, image\ncaptioning, etc. which have proved to be a gem in their respective\napplications. This paper aims to study the major trends in Neural Machine\nTranslation, the state of the art models in the domain and a high level\ncomparison between them.\n",
        "published": "2020",
        "authors": [
            "Rohan Jagtap",
            "Dr. Sudhir N. Dhage"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.05328v1",
        "title": "DeepSentiPers: Novel Deep Learning Models Trained Over Proposed\n  Augmented Persian Sentiment Corpus",
        "abstract": "  This paper focuses on how to extract opinions over each Persian\nsentence-level text. Deep learning models provided a new way to boost the\nquality of the output. However, these architectures need to feed on big\nannotated data as well as an accurate design. To best of our knowledge, we do\nnot merely suffer from lack of well-annotated Persian sentiment corpus, but\nalso a novel model to classify the Persian opinions in terms of both multiple\nand binary classification. So in this work, first we propose two novel deep\nlearning architectures comprises of bidirectional LSTM and CNN. They are a part\nof a deep hierarchy designed precisely and also able to classify sentences in\nboth cases. Second, we suggested three data augmentation techniques for the\nlow-resources Persian sentiment corpus. Our comprehensive experiments on three\nbaselines and two different neural word embedding methods show that our data\naugmentation methods and intended models successfully address the aims of the\nresearch.\n",
        "published": "2020",
        "authors": [
            "Javad PourMostafa Roshan Sharami",
            "Parsa Abbasi Sarabestani",
            "Seyed Abolghasem Mirroshandel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.08114v1",
        "title": "Show Us the Way: Learning to Manage Dialog from Demonstrations",
        "abstract": "  We present our submission to the End-to-End Multi-Domain Dialog Challenge\nTrack of the Eighth Dialog System Technology Challenge. Our proposed dialog\nsystem adopts a pipeline architecture, with distinct components for Natural\nLanguage Understanding, Dialog State Tracking, Dialog Management and Natural\nLanguage Generation. At the core of our system is a reinforcement learning\nalgorithm which uses Deep Q-learning from Demonstrations to learn a dialog\npolicy with the help of expert examples. We find that demonstrations are\nessential to training an accurate dialog policy where both state and action\nspaces are large. Evaluation of our Dialog Management component shows that our\napproach is effective - beating supervised and reinforcement learning\nbaselines.\n",
        "published": "2020",
        "authors": [
            "Gabriel Gordon-Hall",
            "Philip John Gorinski",
            "Gerasimos Lampouras",
            "Ignacio Iacobacci"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.08900v1",
        "title": "The Cost of Training NLP Models: A Concise Overview",
        "abstract": "  We review the cost of training large-scale language models, and the drivers\nof these costs. The intended audience includes engineers and scientists\nbudgeting their model-training experiments, as well as non-practitioners trying\nto make sense of the economics of modern-day Natural Language Processing (NLP).\n",
        "published": "2020",
        "authors": [
            "Or Sharir",
            "Barak Peleg",
            "Yoav Shoham"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.11054v2",
        "title": "Learning Dialog Policies from Weak Demonstrations",
        "abstract": "  Deep reinforcement learning is a promising approach to training a dialog\nmanager, but current methods struggle with the large state and action spaces of\nmulti-domain dialog systems. Building upon Deep Q-learning from Demonstrations\n(DQfD), an algorithm that scores highly in difficult Atari games, we leverage\ndialog data to guide the agent to successfully respond to a user's requests. We\nmake progressively fewer assumptions about the data needed, using labeled,\nreduced-labeled, and even unlabeled data to train expert demonstrators. We\nintroduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to\novercome the domain gap between the datasets and the environment. Experiments\nin a challenging multi-domain dialog system framework validate our approaches,\nand get high success rates even when trained on out-of-domain data.\n",
        "published": "2020",
        "authors": [
            "Gabriel Gordon-Hall",
            "Philip John Gorinski",
            "Shay B. Cohen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.01042v2",
        "title": "Detecting Interrogative Utterances with Recurrent Neural Networks",
        "abstract": "  In this paper, we explore different neural network architectures that can\npredict if a speaker of a given utterance is asking a question or making a\nstatement. We com- pare the outcomes of regularization methods that are\npopularly used to train deep neural networks and study how different context\nfunctions can affect the classification performance. We also compare the\nefficacy of gated activation functions that are favorably used in recurrent\nneural networks and study how to combine multimodal inputs. We evaluate our\nmodels on two multimodal datasets: MSR-Skype and CALLHOME.\n",
        "published": "2015",
        "authors": [
            "Junyoung Chung",
            "Jacob Devlin",
            "Hany Hassan Awadalla"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.02506v1",
        "title": "Towards Structured Deep Neural Network for Automatic Speech Recognition",
        "abstract": "  In this paper we propose the Structured Deep Neural Network (structured DNN)\nas a structured and deep learning framework. This approach can learn to find\nthe best structured object (such as a label sequence) given a structured input\n(such as a vector sequence) by globally considering the mapping relationships\nbetween the structures rather than item by item.\n  When automatic speech recognition is viewed as a special case of such a\nstructured learning problem, where we have the acoustic vector sequence as the\ninput and the phoneme label sequence as the output, it becomes possible to\ncomprehensively learn utterance by utterance as a whole, rather than frame by\nframe.\n  Structured Support Vector Machine (structured SVM) was proposed to perform\nASR with structured learning previously, but limited by the linear nature of\nSVM. Here we propose structured DNN to use nonlinear transformations in\nmulti-layers as a structured and deep learning approach. This approach was\nshown to beat structured SVM in preliminary experiments on TIMIT.\n",
        "published": "2015",
        "authors": [
            "Yi-Hsiu Liao",
            "Hung-yi Lee",
            "Lin-shan Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.04868v4",
        "title": "A Neural Transducer",
        "abstract": "  Sequence-to-sequence models have achieved impressive results on various\ntasks. However, they are unsuitable for tasks that require incremental\npredictions to be made as more data arrives or tasks that have long input\nsequences and output sequences. This is because they generate an output\nsequence conditioned on an entire input sequence. In this paper, we present a\nNeural Transducer that can make incremental predictions as more input arrives,\nwithout redoing the entire computation. Unlike sequence-to-sequence models, the\nNeural Transducer computes the next-step distribution conditioned on the\npartially observed input sequence and the partially generated sequence. At each\ntime step, the transducer can decide to emit zero to many output symbols. The\ndata can be processed using an encoder and presented as input to the\ntransducer. The discrete decision to emit a symbol at every time step makes it\ndifficult to learn with conventional backpropagation. It is however possible to\ntrain the transducer by using a dynamic programming algorithm to generate\ntarget discrete decisions. Our experiments show that the Neural Transducer\nworks well in settings where it is required to produce output predictions as\ndata come in. We also find that the Neural Transducer performs well for long\nsequences even when attention mechanisms are not used.\n",
        "published": "2015",
        "authors": [
            "Navdeep Jaitly",
            "David Sussillo",
            "Quoc V. Le",
            "Oriol Vinyals",
            "Ilya Sutskever",
            "Samy Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.06420v2",
        "title": "Skip-Thought Memory Networks",
        "abstract": "  Question Answering (QA) is fundamental to natural language processing in that\nmost nlp problems can be phrased as QA (Kumar et al., 2015). Current weakly\nsupervised memory network models that have been proposed so far struggle at\nanswering questions that involve relations among multiple entities (such as\nfacebook's bAbi qa5-three-arg-relations in (Weston et al., 2015)). To address\nthis problem of learning multi-argument multi-hop semantic relations for the\npurpose of QA, we propose a method that combines the jointly learned long-term\nread-write memory and attentive inference components of end-to-end memory\nnetworks (MemN2N) (Sukhbaatar et al., 2015) with distributed sentence vector\nrepresentations encoded by a Skip-Thought model (Kiros et al., 2015). This\nchoice to append Skip-Thought Vectors to the existing MemN2N framework is\nmotivated by the fact that Skip-Thought Vectors have been shown to accurately\nmodel multi-argument semantic relations (Kiros et al., 2015).\n",
        "published": "2015",
        "authors": [
            "Ethan Caballero"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.08308v5",
        "title": "Named Entity Recognition with Bidirectional LSTM-CNNs",
        "abstract": "  Named entity recognition is a challenging task that has traditionally\nrequired large amounts of knowledge in the form of feature engineering and\nlexicons to achieve high performance. In this paper, we present a novel neural\nnetwork architecture that automatically detects word- and character-level\nfeatures using a hybrid bidirectional LSTM and CNN architecture, eliminating\nthe need for most feature engineering. We also propose a novel method of\nencoding partial lexicon matches in neural networks and compare it to existing\napproaches. Extensive evaluation shows that, given only tokenized text and\npublicly available word embeddings, our system is competitive on the CoNLL-2003\ndataset and surpasses the previously reported state of the art performance on\nthe OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed\nfrom publicly-available sources, we establish new state of the art performance\nwith an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing\nsystems that employ heavy feature engineering, proprietary lexicons, and rich\nentity linking information.\n",
        "published": "2015",
        "authors": [
            "Jason P. C. Chiu",
            "Eric Nichols"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1512.01712v1",
        "title": "Generating News Headlines with Recurrent Neural Networks",
        "abstract": "  We describe an application of an encoder-decoder recurrent neural network\nwith LSTM units and attention to generating headlines from the text of news\narticles. We find that the model is quite effective at concisely paraphrasing\nnews articles. Furthermore, we study how the neural network decides which input\nwords to pay attention to, and specifically we identify the function of the\ndifferent neurons in a simplified attention mechanism. Interestingly, our\nsimplified attention mechanism performs better that the more complex attention\nmechanism on a held out set of articles.\n",
        "published": "2015",
        "authors": [
            "Konstantin Lopyrev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1512.03549v1",
        "title": "Words are not Equal: Graded Weighting Model for building Composite\n  Document Vectors",
        "abstract": "  Despite the success of distributional semantics, composing phrases from word\nvectors remains an important challenge. Several methods have been tried for\nbenchmark tasks such as sentiment classification, including word vector\naveraging, matrix-vector approaches based on parsing, and on-the-fly learning\nof paragraph vectors. Most models usually omit stop words from the composition.\nInstead of such an yes-no decision, we consider several graded schemes where\nwords are weighted according to their discriminatory relevance with respect to\nits use in the document (e.g., idf). Some of these methods (particularly\ntf-idf) are seen to result in a significant improvement in performance over\nprior state of the art. Further, combining such approaches into an ensemble\nbased on alternate classifiers such as the RNN model, results in an 1.6%\nperformance improvement on the standard IMDB movie review dataset, and a 7.01%\nimprovement on Amazon product reviews. Since these are language free models and\ncan be obtained in an unsupervised manner, they are of interest also for\nunder-resourced languages such as Hindi as well and many more languages. We\ndemonstrate the language free aspects by showing a gain of 12% for two review\ndatasets over earlier results, and also release a new larger dataset for future\ntesting (Singh,2015).\n",
        "published": "2015",
        "authors": [
            "Pranjal Singh",
            "Amitabha Mukerjee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1512.04280v4",
        "title": "Small-footprint Deep Neural Networks with Highway Connections for Speech\n  Recognition",
        "abstract": "  For speech recognition, deep neural networks (DNNs) have significantly\nimproved the recognition accuracy in most of benchmark datasets and application\ndomains. However, compared to the conventional Gaussian mixture models,\nDNN-based acoustic models usually have much larger number of model parameters,\nmaking it challenging for their applications in resource constrained platforms,\ne.g., mobile devices. In this paper, we study the application of the recently\nproposed highway network to train small-footprint DNNs, which are {\\it thinner}\nand {\\it deeper}, and have significantly smaller number of model parameters\ncompared to conventional DNNs. We investigated this approach on the AMI meeting\nspeech transcription corpus which has around 70 hours of audio data. The\nhighway neural networks constantly outperformed their plain DNN counterparts,\nand the number of model parameters can be reduced significantly without\nsacrificing the recognition accuracy.\n",
        "published": "2015",
        "authors": [
            "Liang Lu",
            "Steve Renals"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1512.06612v2",
        "title": "Backward and Forward Language Modeling for Constrained Sentence\n  Generation",
        "abstract": "  Recent language models, especially those based on recurrent neural networks\n(RNNs), make it possible to generate natural language from a learned\nprobability. Language generation has wide applications including machine\ntranslation, summarization, question answering, conversation systems, etc.\nExisting methods typically learn a joint probability of words conditioned on\nadditional information, which is (either statically or dynamically) fed to\nRNN's hidden layer. In many applications, we are likely to impose hard\nconstraints on the generated texts, i.e., a particular word must appear in the\nsentence. Unfortunately, existing approaches could not solve this problem. In\nthis paper, we propose a novel backward and forward language model. Provided a\nspecific word, we use RNNs to generate previous words and future words, either\nsimultaneously or asynchronously, resulting in two model variants. In this way,\nthe given word could appear at any position in the sentence. Experimental\nresults show that the generated texts are comparable to sequential LMs in\nquality.\n",
        "published": "2015",
        "authors": [
            "Lili Mou",
            "Rui Yan",
            "Ge Li",
            "Lu Zhang",
            "Zhi Jin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1512.08903v1",
        "title": "Online Keyword Spotting with a Character-Level Recurrent Neural Network",
        "abstract": "  In this paper, we propose a context-aware keyword spotting model employing a\ncharacter-level recurrent neural network (RNN) for spoken term detection in\ncontinuous speech. The RNN is end-to-end trained with connectionist temporal\nclassification (CTC) to generate the probabilities of character and\nword-boundary labels. There is no need for the phonetic transcription, senone\nmodeling, or system dictionary in training and testing. Also, keywords can\neasily be added and modified by editing the text based keyword list without\nretraining the RNN. Moreover, the unidirectional RNN processes an infinitely\nlong input audio streams without pre-segmentation and keywords are detected\nwith low-latency before the utterance is finished. Experimental results show\nthat the proposed keyword spotter significantly outperforms the deep neural\nnetwork (DNN) and hidden Markov model (HMM) based keyword-filler model even\nwith less computations.\n",
        "published": "2015",
        "authors": [
            "Kyuyeon Hwang",
            "Minjae Lee",
            "Wonyong Sung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.00575v1",
        "title": "Grasping the Finer Point: A Supervised Similarity Network for Metaphor\n  Detection",
        "abstract": "  The ubiquity of metaphor in our everyday communication makes it an important\nproblem for natural language understanding. Yet, the majority of metaphor\nprocessing systems to date rely on hand-engineered features and there is still\nno consensus in the field as to which features are optimal for this task. In\nthis paper, we present the first deep learning architecture designed to capture\nmetaphorical composition. Our results demonstrate that it outperforms the\nexisting approaches in the metaphor identification task.\n",
        "published": "2017",
        "authors": [
            "Marek Rei",
            "Luana Bulat",
            "Douwe Kiela",
            "Ekaterina Shutova"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.06671v1",
        "title": "Think Globally, Embed Locally --- Locally Linear Meta-embedding of Words",
        "abstract": "  Distributed word embeddings have shown superior performances in numerous\nNatural Language Processing (NLP) tasks. However, their performances vary\nsignificantly across different tasks, implying that the word embeddings learnt\nby those methods capture complementary aspects of lexical semantics. Therefore,\nwe believe that it is important to combine the existing word embeddings to\nproduce more accurate and complete \\emph{meta-embeddings} of words. For this\npurpose, we propose an unsupervised locally linear meta-embedding learning\nmethod that takes pre-trained word embeddings as the input, and produces more\naccurate meta embeddings. Unlike previously proposed meta-embedding learning\nmethods that learn a global projection over all words in a vocabulary, our\nproposed method is sensitive to the differences in local neighbourhoods of the\nindividual source word embeddings. Moreover, we show that vector concatenation,\na previously proposed highly competitive baseline approach for integrating word\nembeddings, can be derived as a special case of the proposed method.\nExperimental results on semantic similarity, word analogy, relation\nclassification, and short-text classification tasks show that our\nmeta-embeddings to significantly outperform prior methods in several benchmark\ndatasets, establishing a new state of the art for meta-embeddings.\n",
        "published": "2017",
        "authors": [
            "Danushka Bollegala",
            "Kohei Hayashi",
            "Ken-ichi Kawarabayashi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.09749v1",
        "title": "KeyVec: Key-semantics Preserving Document Representations",
        "abstract": "  Previous studies have demonstrated the empirical success of word embeddings\nin various applications. In this paper, we investigate the problem of learning\ndistributed representations for text documents which many machine learning\nalgorithms take as input for a number of NLP tasks.\n  We propose a neural network model, KeyVec, which learns document\nrepresentations with the goal of preserving key semantics of the input text. It\nenables the learned low-dimensional vectors to retain the topics and important\ninformation from the documents that will flow to downstream tasks. Our\nempirical evaluations show the superior quality of KeyVec representations in\ntwo different document understanding tasks.\n",
        "published": "2017",
        "authors": [
            "Bin Bi",
            "Hao Ma"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.00820v1",
        "title": "Understanding Memory Modules on Learning Simple Algorithms",
        "abstract": "  Recent work has shown that memory modules are crucial for the generalization\nability of neural networks on learning simple algorithms. However, we still\nhave little understanding of the working mechanism of memory modules. To\nalleviate this problem, we apply a two-step analysis pipeline consisting of\nfirst inferring hypothesis about what strategy the model has learned according\nto visualization and then verify it by a novel proposed qualitative analysis\nmethod based on dimension reduction. Using this method, we have analyzed two\npopular memory-augmented neural networks, neural Turing machine and\nstack-augmented neural network on two simple algorithm tasks including\nreversing a random sequence and evaluation of arithmetic expressions. Results\nhave shown that on the former task both models can learn to generalize and on\nthe latter task only the stack-augmented model can do so. We show that\ndifferent strategies are learned by the models, in which specific categories of\ninput are monitored and different policies are made based on that to change the\nmemory.\n",
        "published": "2019",
        "authors": [
            "Kexin Wang",
            "Yu Zhou",
            "Shaonan Wang",
            "Jiajun Zhang",
            "Chengqing Zong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.03885v1",
        "title": "An Intrinsic Nearest Neighbor Analysis of Neural Machine Translation\n  Architectures",
        "abstract": "  Earlier approaches indirectly studied the information captured by the hidden\nstates of recurrent and non-recurrent neural machine translation models by\nfeeding them into different classifiers. In this paper, we look at the encoder\nhidden states of both transformer and recurrent machine translation models from\nthe nearest neighbors perspective. We investigate to what extent the nearest\nneighbors share information with the underlying word embeddings as well as\nrelated WordNet entries. Additionally, we study the underlying syntactic\nstructure of the nearest neighbors to shed light on the role of syntactic\nsimilarities in bringing the neighbors together. We compare transformer and\nrecurrent models in a more intrinsic way in terms of capturing lexical\nsemantics and syntactic structures, in contrast to extrinsic approaches used by\nprevious works. In agreement with the extrinsic evaluations in the earlier\nworks, our experimental results show that transformers are superior in\ncapturing lexical semantics, but not necessarily better in capturing the\nunderlying syntax. Additionally, we show that the backward recurrent layer in a\nrecurrent model learns more about the semantics of words, whereas the forward\nrecurrent layer encodes more context.\n",
        "published": "2019",
        "authors": [
            "Hamidreza Ghader",
            "Christof Monz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.07899v1",
        "title": "Evaluating Defensive Distillation For Defending Text Processing Neural\n  Networks Against Adversarial Examples",
        "abstract": "  Adversarial examples are artificially modified input samples which lead to\nmisclassifications, while not being detectable by humans. These adversarial\nexamples are a challenge for many tasks such as image and text classification,\nespecially as research shows that many adversarial examples are transferable\nbetween different classifiers. In this work, we evaluate the performance of a\npopular defensive strategy for adversarial examples called defensive\ndistillation, which can be successful in hardening neural networks against\nadversarial examples in the image domain. However, instead of applying\ndefensive distillation to networks for image classification, we examine, for\nthe first time, its performance on text classification tasks and also evaluate\nits effect on the transferability of adversarial text examples. Our results\nindicate that defensive distillation only has a minimal impact on text\nclassifying neural networks and does neither help with increasing their\nrobustness against adversarial examples nor prevent the transferability of\nadversarial examples between neural networks.\n",
        "published": "2019",
        "authors": [
            "Marcus Soll",
            "Tobias Hinz",
            "Sven Magg",
            "Stefan Wermter"
        ]
    }
]