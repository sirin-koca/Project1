[
    {
        "id": "http://arxiv.org/abs/1808.05488v2",
        "title": "CBinfer: Exploiting Frame-to-Frame Locality for Faster Convolutional\n  Network Inference on Video Streams",
        "abstract": "  The last few years have brought advances in computer vision at an amazing\npace, grounded on new findings in deep neural network construction and training\nas well as the availability of large labeled datasets. Applying these networks\nto images demands a high computational effort and pushes the use of\nstate-of-the-art networks on real-time video data out of reach of embedded\nplatforms. Many recent works focus on reducing network complexity for real-time\ninference on embedded computing platforms. We adopt an orthogonal viewpoint and\npropose a novel algorithm exploiting the spatio-temporal sparsity of pixel\nchanges. This optimized inference procedure resulted in an average speed-up of\n9.1x over cuDNN on the Tegra X2 platform at a negligible accuracy loss of <0.1%\nand no retraining of the network for a semantic segmentation application.\nSimilarly, an average speed-up of 7.0x has been achieved for a pose detection\nDNN and a reduction of 5x of the number of arithmetic operations to be\nperformed for object detection on static camera video surveillance data. These\nthroughput gains combined with a lower power consumption result in an energy\nefficiency of 511 GOp/s/W compared to 70 GOp/s/W for the baseline.\n",
        "published": "2018",
        "authors": [
            "Lukas Cavigelli",
            "Luca Benini"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.11058v1",
        "title": "Investigating Emotion-Color Association in Deep Neural Networks",
        "abstract": "  It has been found that representations learned by Deep Neural Networks (DNNs)\ncorrelate very well to neural responses measured in primates' brains and\npsychological representations exhibited by human similarity judgment. On\nanother hand, past studies have shown that particular colors can be associated\nwith specific emotion arousal in humans. Do deep neural networks also learn\nthis behavior? In this study, we investigate if DNNs can learn implicit\nassociations in stimuli, particularly, an emotion-color association between\nimage stimuli. Our study was conducted in two parts. First, we collected human\nresponses on a forced-choice decision task in which subjects were asked to\nselect a color for a specified emotion-inducing image. Next, we modeled this\ndecision task on neural networks using the similarity between deep\nrepresentation (extracted using DNNs trained on object classification tasks) of\nthe images and images of colors used in the task. We found that our model\nshowed a fuzzy linear relationship between the two decision probabilities. This\nresults in two interesting findings, 1. The representations learned by deep\nneural networks can indeed show an emotion-color association 2. The\nemotion-color association is not just random but involves some cognitive\nphenomena. Finally, we also show that this method can help us in the emotion\nclassification task, specifically when there are very few examples to train the\nmodel. This analysis can be relevant to psychologists studying emotion-color\nassociations and artificial intelligence researchers modeling emotional\nintelligence in machines or studying representations learned by deep neural\nnetworks.\n",
        "published": "2020",
        "authors": [
            "Shivi Gupta",
            "Shashi Kant Gupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.04700v1",
        "title": "Loss Function Discovery for Object Detection via Convergence-Simulation\n  Driven Search",
        "abstract": "  Designing proper loss functions for vision tasks has been a long-standing\nresearch direction to advance the capability of existing models. For object\ndetection, the well-established classification and regression loss functions\nhave been carefully designed by considering diverse learning challenges.\nInspired by the recent progress in network architecture search, it is\ninteresting to explore the possibility of discovering new loss function\nformulations via directly searching the primitive operation combinations. So\nthat the learned losses not only fit for diverse object detection challenges to\nalleviate huge human efforts, but also have better alignment with evaluation\nmetric and good mathematical convergence property. Beyond the previous\nauto-loss works on face recognition and image classification, our work makes\nthe first attempt to discover new loss functions for the challenging object\ndetection from primitive operation levels. We propose an effective\nconvergence-simulation driven evolutionary search algorithm, called\nCSE-Autoloss, for speeding up the search progress by regularizing the\nmathematical rationality of loss candidates via convergence property\nverification and model optimization simulation. CSE-Autoloss involves the\nsearch space that cover a wide range of the possible variants of existing\nlosses and discovers best-searched loss function combination within a short\ntime (around 1.5 wall-clock days). We conduct extensive evaluations of loss\nfunction search on popular detectors and validate the good generalization\ncapability of searched losses across diverse architectures and datasets. Our\nexperiments show that the best-discovered loss function combinations outperform\ndefault combinations by 1.1% and 0.8% in terms of mAP for two-stage and\none-stage detectors on COCO respectively. Our searched losses are available at\nhttps://github.com/PerdonLiu/CSE-Autoloss.\n",
        "published": "2021",
        "authors": [
            "Peidong Liu",
            "Gengwei Zhang",
            "Bochao Wang",
            "Hang Xu",
            "Xiaodan Liang",
            "Yong Jiang",
            "Zhenguo Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.13621v1",
        "title": "Spike time displacement based error backpropagation in convolutional\n  spiking neural networks",
        "abstract": "  We recently proposed the STiDi-BP algorithm, which avoids backward recursive\ngradient computation, for training multi-layer spiking neural networks (SNNs)\nwith single-spike-based temporal coding. The algorithm employs a linear\napproximation to compute the derivative of the spike latency with respect to\nthe membrane potential and it uses spiking neurons with piecewise linear\npostsynaptic potential to reduce the computational cost and the complexity of\nneural processing. In this paper, we extend the STiDi-BP algorithm to employ it\nin deeper and convolutional architectures. The evaluation results on the image\nclassification task based on two popular benchmarks, MNIST and Fashion-MNIST\ndatasets with the accuracies of respectively 99.2% and 92.8%, confirm that this\nalgorithm has been applicable in deep SNNs. Another issue we consider is the\nreduction of memory storage and computational cost. To do so, we consider a\nconvolutional SNN (CSNN) with two sets of weights: real-valued weights that are\nupdated in the backward pass and their signs, binary weights, that are employed\nin the feedforward process. We evaluate the binary CSNN on two datasets of\nMNIST and Fashion-MNIST and obtain acceptable performance with a negligible\naccuracy drop with respect to real-valued weights (about $0.6%$ and $0.8%$\ndrops, respectively).\n",
        "published": "2021",
        "authors": [
            "Maryam Mirsadeghi",
            "Majid Shalchian",
            "Saeed Reza Kheradpisheh",
            "Timoth\u00e9e Masquelier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.14339v1",
        "title": "Heterogeneous Visible-Thermal and Visible-Infrared Face Recognition\n  using Unit-Class Loss and Cross-Modality Discriminator",
        "abstract": "  Visible-to-thermal face image matching is a challenging variate of\ncross-modality recognition. The challenge lies in the large modality gap and\nlow correlation between visible and thermal modalities. Existing approaches\nemploy image preprocessing, feature extraction, or common subspace projection,\nwhich are independent problems in themselves. In this paper, we propose an\nend-to-end framework for cross-modal face recognition. The proposed algorithm\naims to learn identity-discriminative features from unprocessed facial images\nand identify cross-modal image pairs. A novel Unit-Class Loss is proposed for\npreserving identity information while discarding modality information. In\naddition, a Cross-Modality Discriminator block is proposed for integrating\nimage-pair classification capability into the network. The proposed network can\nbe used to extract modality-independent vector representations or a\nmatching-pair classification for test images. Our cross-modality face\nrecognition experiments on five independent databases demonstrate that the\nproposed method achieves marked improvement over existing state-of-the-art\nmethods.\n",
        "published": "2021",
        "authors": [
            "Usman Cheema",
            "Mobeen Ahmad",
            "Dongil Han",
            "Seungbin Moon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.03740v4",
        "title": "Dilated convolution with learnable spacings",
        "abstract": "  Recent works indicate that convolutional neural networks (CNN) need large\nreceptive fields (RF) to compete with visual transformers and their attention\nmechanism. In CNNs, RFs can simply be enlarged by increasing the convolution\nkernel sizes. Yet the number of trainable parameters, which scales\nquadratically with the kernel's size in the 2D case, rapidly becomes\nprohibitive, and the training is notoriously difficult. This paper presents a\nnew method to increase the RF size without increasing the number of parameters.\nThe dilated convolution (DC) has already been proposed for the same purpose. DC\ncan be seen as a convolution with a kernel that contains only a few non-zero\nelements placed on a regular grid. Here we present a new version of the DC in\nwhich the spacings between the non-zero elements, or equivalently their\npositions, are no longer fixed but learnable via backpropagation thanks to an\ninterpolation technique. We call this method \"Dilated Convolution with\nLearnable Spacings\" (DCLS) and generalize it to the n-dimensional convolution\ncase. However, our main focus here will be on the 2D case. We first tried our\napproach on ResNet50: we drop-in replaced the standard convolutions with DCLS\nones, which increased the accuracy of ImageNet1k classification at\niso-parameters, but at the expense of the throughput. Next, we used the recent\nConvNeXt state-of-the-art convolutional architecture and drop-in replaced the\ndepthwise convolutions with DCLS ones. This not only increased the accuracy of\nImageNet1k classification but also of typical downstream and robustness tasks,\nagain at iso-parameters but this time with negligible cost on throughput, as\nConvNeXt uses separable convolutions. Conversely, classic DC led to poor\nperformance with both ResNet50 and ConvNeXt. The code of the method is\navailable at:\nhttps://github.com/K-H-Ismail/Dilated-Convolution-with-Learnable-Spacings-PyTorch.\n",
        "published": "2021",
        "authors": [
            "Ismail Khalfaoui-Hassani",
            "Thomas Pellegrini",
            "Timoth\u00e9e Masquelier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.07173v1",
        "title": "On the use of Cortical Magnification and Saccades as Biological Proxies\n  for Data Augmentation",
        "abstract": "  Self-supervised learning is a powerful way to learn useful representations\nfrom natural data. It has also been suggested as one possible means of building\nvisual representation in humans, but the specific objective and algorithm are\nunknown. Currently, most self-supervised methods encourage the system to learn\nan invariant representation of different transformations of the same image in\ncontrast to those of other images. However, such transformations are generally\nnon-biologically plausible, and often consist of contrived perceptual schemes\nsuch as random cropping and color jittering. In this paper, we attempt to\nreverse-engineer these augmentations to be more biologically or perceptually\nplausible while still conferring the same benefits for encouraging robust\nrepresentation. Critically, we find that random cropping can be substituted by\ncortical magnification, and saccade-like sampling of the image could also\nassist the representation learning. The feasibility of these transformations\nsuggests a potential way that biological visual systems could implement\nself-supervision. Further, they break the widely accepted spatially-uniform\nprocessing assumption used in many computer vision algorithms, suggesting a\nrole for spatially-adaptive computation in humans and machines alike. Our code\nand demo can be found here.\n",
        "published": "2021",
        "authors": [
            "Binxu Wang",
            "David Mayo",
            "Arturo Deza",
            "Andrei Barbu",
            "Colin Conwell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.13243v1",
        "title": "Evolutionary Generation of Visual Motion Illusions",
        "abstract": "  Why do we sometimes perceive static images as if they were moving? Visual\nmotion illusions enjoy a sustained popularity, yet there is no definitive\nanswer to the question of why they work. We present a generative model, the\nEvolutionary Illusion GENerator (EIGen), that creates new visual motion\nillusions. The structure of EIGen supports the hypothesis that illusory motion\nmight be the result of perceiving the brain's own predictions rather than\nperceiving raw visual input from the eyes. The scientific motivation of this\npaper is to demonstrate that the perception of illusory motion could be a side\neffect of the predictive abilities of the brain. The philosophical motivation\nof this paper is to call attention to the untapped potential of \"motivated\nfailures\", ways for artificial systems to fail as biological systems fail, as a\nworthy outlet for Artificial Intelligence and Artificial Life research.\n",
        "published": "2021",
        "authors": [
            "Lana Sinapayen",
            "Eiji Watanabe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.12091v1",
        "title": "FxP-QNet: A Post-Training Quantizer for the Design of Mixed\n  Low-Precision DNNs with Dynamic Fixed-Point Representation",
        "abstract": "  Deep neural networks (DNNs) have demonstrated their effectiveness in a wide\nrange of computer vision tasks, with the state-of-the-art results obtained\nthrough complex and deep structures that require intensive computation and\nmemory. Now-a-days, efficient model inference is crucial for consumer\napplications on resource-constrained platforms. As a result, there is much\ninterest in the research and development of dedicated deep learning (DL)\nhardware to improve the throughput and energy efficiency of DNNs. Low-precision\nrepresentation of DNN data-structures through quantization would bring great\nbenefits to specialized DL hardware. However, the rigorous quantization leads\nto a severe accuracy drop. As such, quantization opens a large hyper-parameter\nspace at bit-precision levels, the exploration of which is a major challenge.\nIn this paper, we propose a novel framework referred to as the Fixed-Point\nQuantizer of deep neural Networks (FxP-QNet) that flexibly designs a mixed\nlow-precision DNN for integer-arithmetic-only deployment. Specifically, the\nFxP-QNet gradually adapts the quantization level for each data-structure of\neach layer based on the trade-off between the network accuracy and the\nlow-precision requirements. Additionally, it employs post-training\nself-distillation and network prediction error statistics to optimize the\nquantization of floating-point values into fixed-point numbers. Examining\nFxP-QNet on state-of-the-art architectures and the benchmark ImageNet dataset,\nwe empirically demonstrate the effectiveness of FxP-QNet in achieving the\naccuracy-compression trade-off without the need for training. The results show\nthat FxP-QNet-quantized AlexNet, VGG-16, and ResNet-18 reduce the overall\nmemory requirements of their full-precision counterparts by 7.16x, 10.36x, and\n6.44x with less than 0.95%, 0.95%, and 1.99% accuracy drop, respectively.\n",
        "published": "2022",
        "authors": [
            "Ahmad Shawahna",
            "Sadiq M. Sait",
            "Aiman El-Maleh",
            "Irfan Ahmad"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.05625v2",
        "title": "Exploring the Intersection between Neural Architecture Search and\n  Continual Learning",
        "abstract": "  Despite the significant advances achieved in Artificial Neural Networks\n(ANNs), their design process remains notoriously tedious, depending primarily\non intuition, experience and trial-and-error. This human-dependent process is\noften time-consuming and prone to errors. Furthermore, the models are generally\nbound to their training contexts, with no considerations to their surrounding\nenvironments. Continual adaptiveness and automation of neural networks is of\nparamount importance to several domains where model accessibility is limited\nafter deployment (e.g IoT devices, self-driving vehicles, etc.). Additionally,\neven accessible models require frequent maintenance post-deployment to overcome\nissues such as Concept/Data Drift, which can be cumbersome and restrictive. By\nleveraging and combining approaches from Neural Architecture Search (NAS) and\nContinual Learning (CL), more robust and adaptive agents can be developed. This\nstudy conducts the first extensive review on the intersection between NAS and\nCL, formalizing the prospective Continually-Adaptive Neural Networks (CANNs)\nparadigm and outlining research directions for lifelong autonomous ANNs.\n",
        "published": "2022",
        "authors": [
            "Mohamed Shahawy",
            "Elhadj Benkhelifa",
            "David White"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.02595v1",
        "title": "A neuromorphic approach to image processing and machine vision",
        "abstract": "  Neuromorphic engineering is essentially the development of artificial\nsystems, such as electronic analog circuits that employ information\nrepresentations found in biological nervous systems. Despite being faster and\nmore accurate than the human brain, computers lag behind in recognition\ncapability. However, it is envisioned that the advancement in neuromorphics,\npertaining to the fields of computer vision and image processing will provide a\nconsiderable improvement in the way computers can interpret and analyze\ninformation. In this paper, we explore the implementation of visual tasks such\nas image segmentation, visual attention and object recognition. Moreover, the\nconcept of anisotropic diffusion has been examined followed by a novel approach\nemploying memristors to execute image segmentation. Additionally, we have\ndiscussed the role of neuromorphic vision sensors in artificial visual systems\nand the protocol involved in order to enable asynchronous transmission of\nsignals. Moreover, two widely accepted algorithms that are used to emulate the\nprocess of object recognition and visual attention have also been discussed.\nThroughout the span of this paper, we have emphasized on the employment of\nnon-volatile memory devices such as memristors to realize artificial visual\nsystems. Finally, we discuss about hardware accelerators and wish to represent\na case in point for arguing that progress in computer vision may benefit\ndirectly from progress in non-volatile memory technology.\n",
        "published": "2022",
        "authors": [
            "Arvind Subramaniam"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.04374v1",
        "title": "Energy-Aware JPEG Image Compression: A Multi-Objective Approach",
        "abstract": "  Customer satisfaction is crucially affected by energy consumption in mobile\ndevices. One of the most energy-consuming parts of an application is images.\nWhile different images with different quality consume different amounts of\nenergy, there are no straightforward methods to calculate the energy\nconsumption of an operation in a typical image. This paper, first, investigates\nthat there is a correlation between energy consumption and image quality as\nwell as image file size. Therefore, these two can be considered as a proxy for\nenergy consumption. Then, we propose a multi-objective strategy to enhance\nimage quality and reduce image file size based on the quantisation tables in\nJPEG image compression. To this end, we have used two general multi-objective\nmetaheuristic approaches: scalarisation and Pareto-based. Scalarisation methods\nfind a single optimal solution based on combining different objectives, while\nPareto-based techniques aim to achieve a set of solutions. In this paper, we\nembed our strategy into five scalarisation algorithms, including energy-aware\nmulti-objective genetic algorithm (EnMOGA), energy-aware multi-objective\nparticle swarm optimisation (EnMOPSO), energy-aware multi-objective\ndifferential evolution (EnMODE), energy-aware multi-objective evolutionary\nstrategy (EnMOES), and energy-aware multi-objective pattern search (EnMOPS).\nAlso, two Pareto-based methods, including a non-dominated sorting genetic\nalgorithm (NSGA-II) and a reference-point-based NSGA-II (NSGA-III) are used for\nthe embedding scheme, and two Pareto-based algorithms, EnNSGAII and EnNSGAIII,\nare presented. Experimental studies show that the performance of the baseline\nalgorithm is improved by embedding the proposed strategy into metaheuristic\nalgorithms.\n",
        "published": "2022",
        "authors": [
            "Seyed Jalaleddin Mousavirad",
            "Lu\u00eds A. Alexandre"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.06756v2",
        "title": "Decoding Visual Neural Representations by Multimodal Learning of\n  Brain-Visual-Linguistic Features",
        "abstract": "  Decoding human visual neural representations is a challenging task with great\nscientific significance in revealing vision-processing mechanisms and\ndeveloping brain-like intelligent machines. Most existing methods are difficult\nto generalize to novel categories that have no corresponding neural data for\ntraining. The two main reasons are 1) the under-exploitation of the multimodal\nsemantic knowledge underlying the neural data and 2) the small number of paired\n(stimuli-responses) training data. To overcome these limitations, this paper\npresents a generic neural decoding method called BraVL that uses multimodal\nlearning of brain-visual-linguistic features. We focus on modeling the\nrelationships between brain, visual and linguistic features via multimodal deep\ngenerative models. Specifically, we leverage the mixture-of-product-of-experts\nformulation to infer a latent code that enables a coherent joint generation of\nall three modalities. To learn a more consistent joint representation and\nimprove the data efficiency in the case of limited brain activity data, we\nexploit both intra- and inter-modality mutual information maximization\nregularization terms. In particular, our BraVL model can be trained under\nvarious semi-supervised scenarios to incorporate the visual and textual\nfeatures obtained from the extra categories. Finally, we construct three\ntrimodal matching datasets, and the extensive experiments lead to some\ninteresting conclusions and cognitive insights: 1) decoding novel visual\ncategories from human brain activity is practically possible with good\naccuracy; 2) decoding models using the combination of visual and linguistic\nfeatures perform much better than those using either of them alone; 3) visual\nperception may be accompanied by linguistic influences to represent the\nsemantics of visual stimuli. Code and data: https://github.com/ChangdeDu/BraVL.\n",
        "published": "2022",
        "authors": [
            "Changde Du",
            "Kaicheng Fu",
            "Jinpeng Li",
            "Huiguang He"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.01957v1",
        "title": "Sub-network Multi-objective Evolutionary Algorithm for Filter Pruning",
        "abstract": "  Filter pruning is a common method to achieve model compression and\nacceleration in deep neural networks (DNNs).Some research regarded filter\npruning as a combinatorial optimization problem and thus used evolutionary\nalgorithms (EA) to prune filters of DNNs. However, it is difficult to find a\nsatisfactory compromise solution in a reasonable time due to the complexity of\nsolution space searching. To solve this problem, we first formulate a\nmulti-objective optimization problem based on a sub-network of the full model\nand propose a Sub-network Multiobjective Evolutionary Algorithm (SMOEA) for\nfilter pruning. By progressively pruning the convolutional layers in groups,\nSMOEA can obtain a lightweight pruned result with better\nperformance.Experiments on VGG-14 model for CIFAR-10 verify the effectiveness\nof the proposed SMOEA. Specifically, the accuracy of the pruned model with\n16.56% parameters decreases by 0.28% only, which is better than the widely used\npopular filter pruning criteria.\n",
        "published": "2022",
        "authors": [
            "Xuhua Li",
            "Weize Sun",
            "Lei Huang",
            "Shaowu Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.09648v1",
        "title": "HARDVS: Revisiting Human Activity Recognition with Dynamic Vision\n  Sensors",
        "abstract": "  The main streams of human activity recognition (HAR) algorithms are developed\nbased on RGB cameras which are suffered from illumination, fast motion,\nprivacy-preserving, and large energy consumption. Meanwhile, the biologically\ninspired event cameras attracted great interest due to their unique features,\nsuch as high dynamic range, dense temporal but sparse spatial resolution, low\nlatency, low power, etc. As it is a newly arising sensor, even there is no\nrealistic large-scale dataset for HAR. Considering its great practical value,\nin this paper, we propose a large-scale benchmark dataset to bridge this gap,\ntermed HARDVS, which contains 300 categories and more than 100K event\nsequences. We evaluate and report the performance of multiple popular HAR\nalgorithms, which provide extensive baselines for future works to compare. More\nimportantly, we propose a novel spatial-temporal feature learning and fusion\nframework, termed ESTF, for event stream based human activity recognition. It\nfirst projects the event streams into spatial and temporal embeddings using\nStemNet, then, encodes and fuses the dual-view representations using\nTransformer networks. Finally, the dual features are concatenated and fed into\na classification head for activity prediction. Extensive experiments on\nmultiple datasets fully validated the effectiveness of our model. Both the\ndataset and source code will be released on\n\\url{https://github.com/Event-AHU/HARDVS}.\n",
        "published": "2022",
        "authors": [
            "Xiao Wang",
            "Zongzhen Wu",
            "Bo Jiang",
            "Zhimin Bao",
            "Lin Zhu",
            "Guoqi Li",
            "Yaowei Wang",
            "Yonghong Tian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.11010v2",
        "title": "Revisiting Color-Event based Tracking: A Unified Network, Dataset, and\n  Metric",
        "abstract": "  Combining the Color and Event cameras (also called Dynamic Vision Sensors,\nDVS) for robust object tracking is a newly emerging research topic in recent\nyears. Existing color-event tracking framework usually contains multiple\nscattered modules which may lead to low efficiency and high computational\ncomplexity, including feature extraction, fusion, matching, interactive\nlearning, etc. In this paper, we propose a single-stage backbone network for\nColor-Event Unified Tracking (CEUTrack), which achieves the above functions\nsimultaneously. Given the event points and RGB frames, we first transform the\npoints into voxels and crop the template and search regions for both\nmodalities, respectively. Then, these regions are projected into tokens and\nparallelly fed into the unified Transformer backbone network. The output\nfeatures will be fed into a tracking head for target object localization. Our\nproposed CEUTrack is simple, effective, and efficient, which achieves over 75\nFPS and new SOTA performance. To better validate the effectiveness of our model\nand address the data deficiency of this task, we also propose a generic and\nlarge-scale benchmark dataset for color-event tracking, termed COESOT, which\ncontains 90 categories and 1354 video sequences. Additionally, a new evaluation\nmetric named BOC is proposed in our evaluation toolkit to evaluate the\nprominence with respect to the baseline methods. We hope the newly proposed\nmethod, dataset, and evaluation metric provide a better platform for\ncolor-event-based tracking. The dataset, toolkit, and source code will be\nreleased on: \\url{https://github.com/Event-AHU/COESOT}.\n",
        "published": "2022",
        "authors": [
            "Chuanming Tang",
            "Xiao Wang",
            "Ju Huang",
            "Bo Jiang",
            "Lin Zhu",
            "Jianlin Zhang",
            "Yaowei Wang",
            "Yonghong Tian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.12478v1",
        "title": "Image Classification with Small Datasets: Overview and Benchmark",
        "abstract": "  Image classification with small datasets has been an active research area in\nthe recent past. However, as research in this scope is still in its infancy,\ntwo key ingredients are missing for ensuring reliable and truthful progress: a\nsystematic and extensive overview of the state of the art, and a common\nbenchmark to allow for objective comparisons between published methods. This\narticle addresses both issues. First, we systematically organize and connect\npast studies to consolidate a community that is currently fragmented and\nscattered. Second, we propose a common benchmark that allows for an objective\ncomparison of approaches. It consists of five datasets spanning various domains\n(e.g., natural images, medical imagery, satellite data) and data types (RGB,\ngrayscale, multispectral). We use this benchmark to re-evaluate the standard\ncross-entropy baseline and ten existing methods published between 2017 and 2021\nat renowned venues. Surprisingly, we find that thorough hyper-parameter tuning\non held-out validation data results in a highly competitive baseline and\nhighlights a stunted growth of performance over the years. Indeed, only a\nsingle specialized method dating back to 2019 clearly wins our benchmark and\noutperforms the baseline classifier.\n",
        "published": "2022",
        "authors": [
            "L. Brigato",
            "B. Barz",
            "L. Iocchi",
            "J. Denzler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.10685v1",
        "title": "Bridging the Gap between ANNs and SNNs by Calibrating Offset Spikes",
        "abstract": "  Spiking Neural Networks (SNNs) have attracted great attention due to their\ndistinctive characteristics of low power consumption and temporal information\nprocessing. ANN-SNN conversion, as the most commonly used training method for\napplying SNNs, can ensure that converted SNNs achieve comparable performance to\nANNs on large-scale datasets. However, the performance degrades severely under\nlow quantities of time-steps, which hampers the practical applications of SNNs\nto neuromorphic chips. In this paper, instead of evaluating different\nconversion errors and then eliminating these errors, we define an offset spike\nto measure the degree of deviation between actual and desired SNN firing rates.\nWe perform a detailed analysis of offset spike and note that the firing of one\nadditional (or one less) spike is the main cause of conversion errors. Based on\nthis, we propose an optimization strategy based on shifting the initial\nmembrane potential and we theoretically prove the corresponding optimal\nshifting distance for calibrating the spike. In addition, we also note that our\nmethod has a unique iterative property that enables further reduction of\nconversion errors. The experimental results show that our proposed method\nachieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet\ndatasets. For example, we reach a top-1 accuracy of 67.12% on ImageNet when\nusing 6 time-steps. To the best of our knowledge, this is the first time an\nANN-SNN conversion has been shown to simultaneously achieve high accuracy and\nultralow latency on complex datasets. Code is available at\nhttps://github.com/hzc1208/ANN2SNN_COS.\n",
        "published": "2023",
        "authors": [
            "Zecheng Hao",
            "Jianhao Ding",
            "Tong Bu",
            "Tiejun Huang",
            "Zhaofei Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.04873v1",
        "title": "MOREA: a GPU-accelerated Evolutionary Algorithm for Multi-Objective\n  Deformable Registration of 3D Medical Images",
        "abstract": "  Finding a realistic deformation that transforms one image into another, in\ncase large deformations are required, is considered a key challenge in medical\nimage analysis. Having a proper image registration approach to achieve this\ncould unleash a number of applications requiring information to be transferred\nbetween images. Clinical adoption is currently hampered by many existing\nmethods requiring extensive configuration effort before each use, or not being\nable to (realistically) capture large deformations. A recent multi-objective\napproach that uses the Multi-Objective Real-Valued Gene-pool Optimal Mixing\nEvolutionary Algorithm (MO-RV-GOMEA) and a dual-dynamic mesh transformation\nmodel has shown promise, exposing the trade-offs inherent to image registration\nproblems and modeling large deformations in 2D. This work builds on this\npromise and introduces MOREA: the first evolutionary algorithm-based\nmulti-objective approach to deformable registration of 3D images capable of\ntackling large deformations. MOREA includes a 3D biomechanical mesh model for\nphysical plausibility and is fully GPU-accelerated. We compare MOREA to two\nstate-of-the-art approaches on abdominal CT scans of 4 cervical cancer\npatients, with the latter two approaches configured for the best results per\npatient. Without requiring per-patient configuration, MOREA significantly\noutperforms these approaches on 3 of the 4 patients that represent the most\ndifficult cases.\n",
        "published": "2023",
        "authors": [
            "Georgios Andreadis",
            "Peter A. N. Bosman",
            "Tanja Alderliesten"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.11127v1",
        "title": "MT-SNN: Enhance Spiking Neural Network with Multiple Thresholds",
        "abstract": "  Spiking neural networks (SNNs), as a biology-inspired method mimicking the\nspiking nature of brain neurons, is a promising energy-efficient alternative to\nthe traditional artificial neural networks (ANNs). The energy saving of SNNs is\nmainly from multiplication free property brought by binarized intermediate\nactivations. In this paper, we proposed a Multiple Threshold (MT) approach to\nalleviate the precision loss brought by the binarized activations, such that\nSNNs can reach higher accuracy at fewer steps. We evaluate the approach on\nCIFAR10, CIFAR100 and DVS-CIFAR10, and demonstrate that MT can promote SNNs\nextensively, especially at early steps. For example, With MT,\nParametric-Leaky-Integrate-Fire(PLIF) based VGG net can even outperform the ANN\ncounterpart with 1 step.\n",
        "published": "2023",
        "authors": [
            "Xiaoting Wang",
            "Yanxiang Zhang",
            "Yongzhe Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.10987v3",
        "title": "SpikingNeRF: Making Bio-inspired Neural Networks See through the Real\n  World",
        "abstract": "  Spiking neural networks (SNNs) have been thriving on numerous tasks to\nleverage their promising energy efficiency and exploit their potentialities as\nbiologically plausible intelligence. Meanwhile, the Neural Radiance Fields\n(NeRF) render high-quality 3D scenes with massive energy consumption, but few\nworks delve into the energy-saving solution with a bio-inspired approach. In\nthis paper, we propose SpikingNeRF, which aligns the radiance ray with the\ntemporal dimension of SNN, to naturally accommodate the SNN to the\nreconstruction of Radiance Fields. Thus, the computation turns into a\nspike-based, multiplication-free manner, reducing the energy consumption. In\nSpikingNeRF, each sampled point on the ray is matched onto a particular time\nstep, and represented in a hybrid manner where the voxel grids are maintained\nas well. Based on the voxel grids, sampled points are determined whether to be\nmasked for better training and inference. However, this operation also incurs\nirregular temporal length. We propose the temporal padding strategy to tackle\nthe masked samples to maintain regular temporal length, i.e., regular tensors,\nand the temporal condensing strategy to form a denser data structure for\nhardware-friendly computation. Extensive experiments on various datasets\ndemonstrate that our method reduces the 70.79% energy consumption on average\nand obtains comparable synthesis quality with the ANN baseline.\n",
        "published": "2023",
        "authors": [
            "Xingting Yao",
            "Qinghao Hu",
            "Tielong Liu",
            "Zitao Mo",
            "Zeyu Zhu",
            "Zhengyang Zhuge",
            "Jian Cheng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.14839v1",
        "title": "ESVAE: An Efficient Spiking Variational Autoencoder with\n  Reparameterizable Poisson Spiking Sampling",
        "abstract": "  In recent years, studies on image generation models of spiking neural\nnetworks (SNNs) have gained the attention of many researchers. Variational\nautoencoders (VAEs), as one of the most popular image generation models, have\nattracted a lot of work exploring their SNN implementation. Due to the\nconstrained binary representation in SNNs, existing SNN VAE methods implicitly\nconstruct the latent space by an elaborated autoregressive network and use the\nnetwork outputs as the sampling variables. However, this unspecified implicit\nrepresentation of the latent space will increase the difficulty of generating\nhigh-quality images and introduces additional network parameters. In this\npaper, we propose an efficient spiking variational autoencoder (ESVAE) that\nconstructs an interpretable latent space distribution and design a\nreparameterizable spiking sampling method. Specifically, we construct the prior\nand posterior of the latent space as a Poisson distribution using the firing\nrate of the spiking neurons. Subsequently, we propose a reparameterizable\nPoisson spiking sampling method, which is free from the additional network.\nComprehensive experiments have been conducted, and the experimental results\nshow that the proposed ESVAE outperforms previous SNN VAE methods in\nreconstructed & generated images quality. In addition, experiments demonstrate\nthat ESVAE's encoder is able to retain the original image information more\nefficiently, and the decoder is more robust. The source code is available at\nhttps://github.com/QgZhan/ESVAE.\n",
        "published": "2023",
        "authors": [
            "Qiugang Zhan",
            "Xiurui Xie",
            "Guisong Liu",
            "Malu Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1509.00728v1",
        "title": "On Transitive Consistency for Linear Invertible Transformations between\n  Euclidean Coordinate Systems",
        "abstract": "  Transitive consistency is an intrinsic property for collections of linear\ninvertible transformations between Euclidean coordinate frames. In practice,\nwhen the transformations are estimated from data, this property is lacking.\nThis work addresses the problem of synchronizing transformations that are not\ntransitively consistent. Once the transformations have been synchronized, they\nsatisfy the transitive consistency condition - a transformation from frame $A$\nto frame $C$ is equal to the composite transformation of first transforming A\nto B and then transforming B to C. The coordinate frames correspond to nodes in\na graph and the transformations correspond to edges in the same graph. Two\ndirect or centralized synchronization methods are presented for different graph\ntopologies; the first one for quasi-strongly connected graphs, and the second\none for connected graphs. As an extension of the second method, an iterative\nGauss-Newton method is presented, which is later adapted to the case of affine\nand Euclidean transformations. Two distributed synchronization methods are also\npresented for orthogonal matrices, which can be seen as distributed versions of\nthe two direct or centralized methods; they are similar in nature to standard\nconsensus protocols used for distributed averaging. When the transformations\nare orthogonal matrices, a bound on the optimality gap can be computed.\nSimulations show that the gap is almost right, even for noise large in\nmagnitude. This work also contributes on a theoretical level by providing\nlinear algebraic relationships for transitively consistent transformations. One\nof the benefits of the proposed methods is their simplicity - basic linear\nalgebraic methods are used, e.g., the Singular Value Decomposition (SVD). For a\nwide range of parameter settings, the methods are numerically validated.\n",
        "published": "2015",
        "authors": [
            "Johan Thunberg",
            "Florian Bernard",
            "Jorge Goncalves"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.09714v1",
        "title": "Language Bootstrapping: Learning Word Meanings From Perception-Action\n  Association",
        "abstract": "  We address the problem of bootstrapping language acquisition for an\nartificial system similarly to what is observed in experiments with human\ninfants. Our method works by associating meanings to words in manipulation\ntasks, as a robot interacts with objects and listens to verbal descriptions of\nthe interactions. The model is based on an affordance network, i.e., a mapping\nbetween robot actions, robot perceptions, and the perceived effects of these\nactions upon objects. We extend the affordance model to incorporate spoken\nwords, which allows us to ground the verbal symbols to the execution of actions\nand the perception of the environment. The model takes verbal descriptions of a\ntask as the input and uses temporal co-occurrence to create links between\nspeech utterances and the involved objects, actions, and effects. We show that\nthe robot is able form useful word-to-meaning associations, even without\nconsidering grammatical structure in the learning process and in the presence\nof recognition errors. These word-to-meaning associations are embedded in the\nrobot's own understanding of its actions. Thus, they can be directly used to\ninstruct the robot to perform tasks and also allow to incorporate context in\nthe speech recognition task. We believe that the encouraging results with our\napproach may afford robots with a capacity to acquire language descriptors in\ntheir operation's environment as well as to shed some light as to how this\nchallenging process develops with human infants.\n",
        "published": "2017",
        "authors": [
            "Giampiero Salvi",
            "Luis Montesano",
            "Alexandre Bernardino",
            "Jos\u00e9 Santos-Victor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/0809.0490v2",
        "title": "Principal Graphs and Manifolds",
        "abstract": "  In many physical, statistical, biological and other investigations it is\ndesirable to approximate a system of points by objects of lower dimension\nand/or complexity. For this purpose, Karl Pearson invented principal component\nanalysis in 1901 and found 'lines and planes of closest fit to system of\npoints'. The famous k-means algorithm solves the approximation problem too, but\nby finite sets instead of lines and planes. This chapter gives a brief\npractical introduction into the methods of construction of general principal\nobjects, i.e. objects embedded in the 'middle' of the multidimensional data\nset. As a basis, the unifying framework of mean squared distance approximation\nof finite datasets is selected. Principal graphs and manifolds are constructed\nas generalisations of principal components and k-means principal points. For\nthis purpose, the family of expectation/maximisation algorithms with nearest\ngeneralisations is presented. Construction of principal graphs with controlled\ncomplexity is based on the graph grammar approach.\n",
        "published": "2008",
        "authors": [
            "A. N. Gorban",
            "A. Y. Zinovyev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1210.5474v1",
        "title": "Disentangling Factors of Variation via Generative Entangling",
        "abstract": "  Here we propose a novel model family with the objective of learning to\ndisentangle the factors of variation in data. Our approach is based on the\nspike-and-slab restricted Boltzmann machine which we generalize to include\nhigher-order interactions among multiple latent variables. Seen from a\ngenerative perspective, the multiplicative interactions emulates the entangling\nof factors of variation. Inference in the model can be seen as disentangling\nthese generative factors. Unlike previous attempts at disentangling latent\nfactors, the proposed model is trained using no supervised information\nregarding the latent factors. We apply our model to the task of facial\nexpression classification.\n",
        "published": "2012",
        "authors": [
            "Guillaume Desjardins",
            "Aaron Courville",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1210.6511v1",
        "title": "Neural Networks for Complex Data",
        "abstract": "  Artificial neural networks are simple and efficient machine learning tools.\nDefined originally in the traditional setting of simple vector data, neural\nnetwork models have evolved to address more and more difficulties of complex\nreal world problems, ranging from time evolving data to sophisticated data\nstructures such as graphs and functions. This paper summarizes advances on\nthose themes from the last decade, with a focus on results obtained by members\nof the SAMM team of Universit\\'e Paris 1\n",
        "published": "2012",
        "authors": [
            "Marie Cottrell",
            "Madalina Olteanu",
            "Fabrice Rossi",
            "Joseph Rynkiewicz",
            "Nathalie Villa-Vialaneix"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1211.3711v1",
        "title": "Sequence Transduction with Recurrent Neural Networks",
        "abstract": "  Many machine learning tasks can be expressed as the transformation---or\n\\emph{transduction}---of input sequences into output sequences: speech\nrecognition, machine translation, protein secondary structure prediction and\ntext-to-speech to name but a few. One of the key challenges in sequence\ntransduction is learning to represent both the input and output sequences in a\nway that is invariant to sequential distortions such as shrinking, stretching\nand translating. Recurrent neural networks (RNNs) are a powerful sequence\nlearning architecture that has proven capable of learning such representations.\nHowever RNNs traditionally require a pre-defined alignment between the input\nand output sequences to perform transduction. This is a severe limitation since\n\\emph{finding} the alignment is the most difficult aspect of many sequence\ntransduction problems. Indeed, even determining the length of the output\nsequence is often challenging. This paper introduces an end-to-end,\nprobabilistic sequence transduction system, based entirely on RNNs, that is in\nprinciple able to transform any input sequence into any finite, discrete output\nsequence. Experimental results for phoneme recognition are provided on the\nTIMIT speech corpus.\n",
        "published": "2012",
        "authors": [
            "Alex Graves"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1212.1524v2",
        "title": "Layer-wise learning of deep generative models",
        "abstract": "  When using deep, multi-layered architectures to build generative models of\ndata, it is difficult to train all layers at once. We propose a layer-wise\ntraining procedure admitting a performance guarantee compared to the global\noptimum. It is based on an optimistic proxy of future performance, the best\nlatent marginal. We interpret auto-encoders in this setting as generative\nmodels, by showing that they train a lower bound of this criterion. We test the\nnew learning procedure against a state of the art method (stacked RBMs), and\nfind it to improve performance. Both theory and experiments highlight the\nimportance, when training deep architectures, of using an inference model (from\ndata to hidden variables) richer than the generative model (from hidden\nvariables to data).\n",
        "published": "2012",
        "authors": [
            "Ludovic Arnold",
            "Yann Ollivier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1212.5921v1",
        "title": "Distributed optimization of deeply nested systems",
        "abstract": "  In science and engineering, intelligent processing of complex signals such as\nimages, sound or language is often performed by a parameterized hierarchy of\nnonlinear processing layers, sometimes biologically inspired. Hierarchical\nsystems (or, more generally, nested systems) offer a way to generate complex\nmappings using simple stages. Each layer performs a different operation and\nachieves an ever more sophisticated representation of the input, as, for\nexample, in an deep artificial neural network, an object recognition cascade in\ncomputer vision or a speech front-end processing. Joint estimation of the\nparameters of all the layers and selection of an optimal architecture is widely\nconsidered to be a difficult numerical nonconvex optimization problem,\ndifficult to parallelize for execution in a distributed computation\nenvironment, and requiring significant human expert effort, which leads to\nsuboptimal systems in practice. We describe a general mathematical strategy to\nlearn the parameters and, to some extent, the architecture of nested systems,\ncalled the method of auxiliary coordinates (MAC). This replaces the original\nproblem involving a deeply nested function with a constrained problem involving\na different function in an augmented space without nesting. The constrained\nproblem may be solved with penalty-based methods using alternating optimization\nover the parameters and the auxiliary coordinates. MAC has provable\nconvergence, is easy to implement reusing existing algorithms for single\nlayers, can be parallelized trivially and massively, applies even when\nparameter derivatives are not available or not desirable, and is competitive\nwith state-of-the-art nonlinear optimizers even in the serial computation\nsetting, often providing reasonable models within a few iterations.\n",
        "published": "2012",
        "authors": [
            "Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n",
            "Weiran Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1302.3931v7",
        "title": "Understanding Boltzmann Machine and Deep Learning via A Confident\n  Information First Principle",
        "abstract": "  Typical dimensionality reduction methods focus on directly reducing the\nnumber of random variables while retaining maximal variations in the data. In\nthis paper, we consider the dimensionality reduction in parameter spaces of\nbinary multivariate distributions. We propose a general\nConfident-Information-First (CIF) principle to maximally preserve parameters\nwith confident estimates and rule out unreliable or noisy parameters. Formally,\nthe confidence of a parameter can be assessed by its Fisher information, which\nestablishes a connection with the inverse variance of any unbiased estimate for\nthe parameter via the Cram\\'{e}r-Rao bound. We then revisit Boltzmann machines\n(BM) and theoretically show that both single-layer BM without hidden units\n(SBM) and restricted BM (RBM) can be solidly derived using the CIF principle.\nThis can not only help us uncover and formalize the essential parts of the\ntarget density that SBM and RBM capture, but also suggest that the deep neural\nnetwork consisting of several layers of RBM can be seen as the layer-wise\napplication of CIF. Guided by the theoretical analysis, we develop a\nsample-specific CIF-based contrastive divergence (CD-CIF) algorithm for SBM and\na CIF-based iterative projection procedure (IP) for RBM. Both CD-CIF and IP are\nstudied in a series of density estimation experiments.\n",
        "published": "2013",
        "authors": [
            "Xiaozhao Zhao",
            "Yuexian Hou",
            "Qian Yu",
            "Dawei Song",
            "Wenjie Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1302.4141v1",
        "title": "Canonical dual solutions to nonconvex radial basis neural network\n  optimization problem",
        "abstract": "  Radial Basis Functions Neural Networks (RBFNNs) are tools widely used in\nregression problems. One of their principal drawbacks is that the formulation\ncorresponding to the training with the supervision of both the centers and the\nweights is a highly non-convex optimization problem, which leads to some\nfundamentally difficulties for traditional optimization theory and methods.\n  This paper presents a generalized canonical duality theory for solving this\nchallenging problem. We demonstrate that by sequential canonical dual\ntransformations, the nonconvex optimization problem of the RBFNN can be\nreformulated as a canonical dual problem (without duality gap). Both global\noptimal solution and local extrema can be classified. Several applications to\none of the most used Radial Basis Functions, the Gaussian function, are\nillustrated. Our results show that even for one-dimensional case, the global\nminimizer of the nonconvex problem may not be the best solution to the RBFNNs,\nand the canonical dual theory is a promising tool for solving general neural\nnetworks training problems.\n",
        "published": "2013",
        "authors": [
            "Vittorio Latorre",
            "David Yang Gao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1305.3794v2",
        "title": "Evolution of Covariance Functions for Gaussian Process Regression using\n  Genetic Programming",
        "abstract": "  In this contribution we describe an approach to evolve composite covariance\nfunctions for Gaussian processes using genetic programming. A critical aspect\nof Gaussian processes and similar kernel-based models such as SVM is, that the\ncovariance function should be adapted to the modeled data. Frequently, the\nsquared exponential covariance function is used as a default. However, this can\nlead to a misspecified model, which does not fit the data well. In the proposed\napproach we use a grammar for the composition of covariance functions and\ngenetic programming to search over the space of sentences that can be derived\nfrom the grammar. We tested the proposed approach on synthetic data from\ntwo-dimensional test functions, and on the Mauna Loa CO2 time series. The\nresults show, that our approach is feasible, finding covariance functions that\nperform much better than a default covariance function. For the CO2 data set a\ncomposite covariance function is found, that matches the performance of a\nhand-tuned covariance function.\n",
        "published": "2013",
        "authors": [
            "Gabriel Kronberger",
            "Michael Kommenda"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1306.0543v2",
        "title": "Predicting Parameters in Deep Learning",
        "abstract": "  We demonstrate that there is significant redundancy in the parameterization\nof several deep learning models. Given only a few weight values for each\nfeature it is possible to accurately predict the remaining values. Moreover, we\nshow that not only can the parameter values be predicted, but many of them need\nnot be learned at all. We train several different architectures by learning\nonly a small number of weights and predicting the rest. In the best case we are\nable to predict more than 95% of the weights of a network without any drop in\naccuracy.\n",
        "published": "2013",
        "authors": [
            "Misha Denil",
            "Babak Shakibi",
            "Laurent Dinh",
            "Marc'Aurelio Ranzato",
            "Nando de Freitas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1306.2801v4",
        "title": "Understanding Dropout: Training Multi-Layer Perceptrons with Auxiliary\n  Independent Stochastic Neurons",
        "abstract": "  In this paper, a simple, general method of adding auxiliary stochastic\nneurons to a multi-layer perceptron is proposed. It is shown that the proposed\nmethod is a generalization of recently successful methods of dropout (Hinton et\nal., 2012), explicit noise injection (Vincent et al., 2010; Bishop, 1995) and\nsemantic hashing (Salakhutdinov & Hinton, 2009). Under the proposed framework,\nan extension of dropout which allows using separate dropping probabilities for\ndifferent hidden neurons, or layers, is found to be available. The use of\ndifferent dropping probabilities for hidden layers separately is empirically\ninvestigated.\n",
        "published": "2013",
        "authors": [
            "Kyunghyun Cho"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1311.0701v7",
        "title": "On Fast Dropout and its Applicability to Recurrent Networks",
        "abstract": "  Recurrent Neural Networks (RNNs) are rich models for the processing of\nsequential data. Recent work on advancing the state of the art has been focused\non the optimization or modelling of RNNs, mostly motivated by adressing the\nproblems of the vanishing and exploding gradients. The control of overfitting\nhas seen considerably less attention. This paper contributes to that by\nanalyzing fast dropout, a recent regularization method for generalized linear\nmodels and neural networks from a back-propagation inspired perspective. We\nshow that fast dropout implements a quadratic form of an adaptive,\nper-parameter regularizer, which rewards large weights in the light of\nunderfitting, penalizes them for overconfident predictions and vanishes at\nminima of an unregularized training loss. The derivatives of that regularizer\nare exclusively based on the training error signal. One consequence of this is\nthe absense of a global weight attractor, which is particularly appealing for\nRNNs, since the dynamics are not biased towards a certain regime. We positively\ntest the hypothesis that this improves the performance of RNNs on four musical\ndata sets.\n",
        "published": "2013",
        "authors": [
            "Justin Bayer",
            "Christian Osendorfer",
            "Daniela Korhammer",
            "Nutan Chen",
            "Sebastian Urban",
            "Patrick van der Smagt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1311.1780v7",
        "title": "Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks",
        "abstract": "  In this paper we propose and investigate a novel nonlinear unit, called $L_p$\nunit, for deep neural networks. The proposed $L_p$ unit receives signals from\nseveral projections of a subset of units in the layer below and computes a\nnormalized $L_p$ norm. We notice two interesting interpretations of the $L_p$\nunit. First, the proposed unit can be understood as a generalization of a\nnumber of conventional pooling operators such as average, root-mean-square and\nmax pooling widely used in, for instance, convolutional neural networks (CNN),\nHMAX models and neocognitrons. Furthermore, the $L_p$ unit is, to a certain\ndegree, similar to the recently proposed maxout unit (Goodfellow et al., 2013)\nwhich achieved the state-of-the-art object recognition results on a number of\nbenchmark datasets. Secondly, we provide a geometrical interpretation of the\nactivation function based on which we argue that the $L_p$ unit is more\nefficient at representing complex, nonlinear separating boundaries. Each $L_p$\nunit defines a superelliptic boundary, with its exact shape defined by the\norder $p$. We claim that this makes it possible to model arbitrarily shaped,\ncurved boundaries more efficiently by combining a few $L_p$ units of different\norders. This insight justifies the need for learning different orders for each\nunit in the model. We empirically evaluate the proposed $L_p$ units on a number\nof datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$\nunits achieve the state-of-the-art results on a number of benchmark datasets.\nFurthermore, we evaluate the proposed $L_p$ unit on the recently proposed deep\nrecurrent neural networks (RNN).\n",
        "published": "2013",
        "authors": [
            "Caglar Gulcehre",
            "Kyunghyun Cho",
            "Razvan Pascanu",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1312.5394v1",
        "title": "Missing Value Imputation With Unsupervised Backpropagation",
        "abstract": "  Many data mining and data analysis techniques operate on dense matrices or\ncomplete tables of data. Real-world data sets, however, often contain unknown\nvalues. Even many classification algorithms that are designed to operate with\nmissing values still exhibit deteriorated accuracy. One approach to handling\nmissing values is to fill in (impute) the missing values. In this paper, we\npresent a technique for unsupervised learning called Unsupervised\nBackpropagation (UBP), which trains a multi-layer perceptron to fit to the\nmanifold sampled by a set of observed point-vectors. We evaluate UBP with the\ntask of imputing missing values in datasets, and show that UBP is able to\npredict missing values with significantly lower sum-squared error than other\ncollaborative filtering and imputation techniques. We also demonstrate with 24\ndatasets and 9 supervised learning algorithms that classification accuracy is\nusually higher when randomly-withheld values are imputed using UBP, rather than\nwith other methods.\n",
        "published": "2013",
        "authors": [
            "Michael S. Gashler",
            "Michael R. Smith",
            "Richard Morris",
            "Tony Martinez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1312.5847v3",
        "title": "Deep learning for neuroimaging: a validation study",
        "abstract": "  Deep learning methods have recently made notable advances in the tasks of\nclassification and representation learning. These tasks are important for brain\nimaging and neuroscience discovery, making the methods attractive for porting\nto a neuroimager's toolbox. Success of these methods is, in part, explained by\nthe flexibility of deep learning models. However, this flexibility makes the\nprocess of porting to new areas a difficult parameter optimization problem. In\nthis work we demonstrate our results (and feasible parameter ranges) in\napplication of deep learning methods to structural and functional brain imaging\ndata. We also describe a novel constraint-based approach to visualizing high\ndimensional data. We use it to analyze the effect of parameter choices on data\ntransformations. Our results show that deep learning methods are able to learn\nphysiologically important representations and detect latent relations in\nneuroimaging data.\n",
        "published": "2013",
        "authors": [
            "Sergey M. Plis",
            "Devon R. Hjelm",
            "Ruslan Salakhutdinov",
            "Vince D. Calhoun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1312.6002v3",
        "title": "Stochastic Gradient Estimate Variance in Contrastive Divergence and\n  Persistent Contrastive Divergence",
        "abstract": "  Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) are\npopular methods for training the weights of Restricted Boltzmann Machines.\nHowever, both methods use an approximate method for sampling from the model\ndistribution. As a side effect, these approximations yield significantly\ndifferent biases and variances for stochastic gradient estimates of individual\ndata points. It is well known that CD yields a biased gradient estimate. In\nthis paper we however show empirically that CD has a lower stochastic gradient\nestimate variance than exact sampling, while the mean of subsequent PCD\nestimates has a higher variance than exact sampling. The results give one\nexplanation to the finding that CD can be used with smaller minibatches or\nhigher learning rates than PCD.\n",
        "published": "2013",
        "authors": [
            "Mathias Berglund",
            "Tapani Raiko"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1312.6026v5",
        "title": "How to Construct Deep Recurrent Neural Networks",
        "abstract": "  In this paper, we explore different ways to extend a recurrent neural network\n(RNN) to a \\textit{deep} RNN. We start by arguing that the concept of depth in\nan RNN is not as clear as it is in feedforward neural networks. By carefully\nanalyzing and understanding the architecture of an RNN, however, we find three\npoints of an RNN which may be made deeper; (1) input-to-hidden function, (2)\nhidden-to-hidden transition and (3) hidden-to-output function. Based on this\nobservation, we propose two novel architectures of a deep RNN which are\northogonal to an earlier attempt of stacking multiple recurrent layers to build\na deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an\nalternative interpretation of these deep RNNs using a novel framework based on\nneural operators. The proposed deep RNNs are empirically evaluated on the tasks\nof polyphonic music prediction and language modeling. The experimental result\nsupports our claim that the proposed deep RNNs benefit from the depth and\noutperform the conventional, shallow RNNs.\n",
        "published": "2013",
        "authors": [
            "Razvan Pascanu",
            "Caglar Gulcehre",
            "Kyunghyun Cho",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1312.6115v5",
        "title": "Neuronal Synchrony in Complex-Valued Deep Networks",
        "abstract": "  Deep learning has recently led to great successes in tasks such as image\nrecognition (e.g Krizhevsky et al., 2012). However, deep networks are still\noutmatched by the power and versatility of the brain, perhaps in part due to\nthe richer neuronal computations available to cortical circuits. The challenge\nis to identify which neuronal mechanisms are relevant, and to find suitable\nabstractions to model them. Here, we show how aspects of spike timing, long\nhypothesized to play a crucial role in cortical information processing, could\nbe incorporated into deep networks to build richer, versatile representations.\n  We introduce a neural network formulation based on complex-valued neuronal\nunits that is not only biologically meaningful but also amenable to a variety\nof deep learning frameworks. Here, units are attributed both a firing rate and\na phase, the latter indicating properties of spike timing. We show how this\nformulation qualitatively captures several aspects thought to be related to\nneuronal synchrony, including gating of information processing and dynamic\nbinding of distributed object representations. Focusing on the latter, we\ndemonstrate the potential of the approach in several simple experiments. Thus,\nneuronal synchrony could be a flexible mechanism that fulfills multiple\nfunctional roles in deep networks.\n",
        "published": "2013",
        "authors": [
            "David P. Reichert",
            "Thomas Serre"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1312.6116v2",
        "title": "Improving Deep Neural Networks with Probabilistic Maxout Units",
        "abstract": "  We present a probabilistic variant of the recently introduced maxout unit.\nThe success of deep neural networks utilizing maxout can partly be attributed\nto favorable performance under dropout, when compared to rectified linear\nunits. It however also depends on the fact that each maxout unit performs a\npooling operation over a group of linear transformations and is thus partially\ninvariant to changes in its input. Starting from this observation we ask the\nquestion: Can the desirable properties of maxout units be preserved while\nimproving their invariance properties ? We argue that our probabilistic maxout\n(probout) units successfully achieve this balance. We quantitatively verify\nthis claim and report classification performance matching or exceeding the\ncurrent state of the art on three challenging image classification benchmarks\n(CIFAR-10, CIFAR-100 and SVHN).\n",
        "published": "2013",
        "authors": [
            "Jost Tobias Springenberg",
            "Martin Riedmiller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1312.6197v2",
        "title": "An empirical analysis of dropout in piecewise linear networks",
        "abstract": "  The recently introduced dropout training criterion for neural networks has\nbeen the subject of much attention due to its simplicity and remarkable\neffectiveness as a regularizer, as well as its interpretation as a training\nprocedure for an exponentially large ensemble of networks that share\nparameters. In this work we empirically investigate several questions related\nto the efficacy of dropout, specifically as it concerns networks employing the\npopular rectified linear activation function. We investigate the quality of the\ntest time weight-scaling inference procedure by evaluating the geometric\naverage exactly in small models, as well as compare the performance of the\ngeometric mean to the arithmetic mean more commonly employed by ensemble\ntechniques. We explore the effect of tied weights on the ensemble\ninterpretation by training ensembles of masked networks without tied weights.\nFinally, we investigate an alternative criterion based on a biased estimator of\nthe maximum likelihood ensemble gradient.\n",
        "published": "2013",
        "authors": [
            "David Warde-Farley",
            "Ian J. Goodfellow",
            "Aaron Courville",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1312.6211v3",
        "title": "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based\n  Neural Networks",
        "abstract": "  Catastrophic forgetting is a problem faced by many machine learning models\nand algorithms. When trained on one task, then trained on a second task, many\nmachine learning models \"forget\" how to perform the first task. This is widely\nbelieved to be a serious problem for neural networks. Here, we investigate the\nextent to which the catastrophic forgetting problem occurs for modern neural\nnetworks, comparing both established and recent gradient-based training\nalgorithms and activation functions. We also examine the effect of the\nrelationship between the first task and the second task on catastrophic\nforgetting. We find that it is always best to train using the dropout\nalgorithm--the dropout algorithm is consistently best at adapting to the new\ntask, remembering the old task, and has the best tradeoff curve between these\ntwo extremes. We find that different tasks and relationships between tasks\nresult in very different rankings of activation function performance. This\nsuggests the choice of activation function should always be cross-validated.\n",
        "published": "2013",
        "authors": [
            "Ian J. Goodfellow",
            "Mehdi Mirza",
            "Da Xiao",
            "Aaron Courville",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1401.5900v1",
        "title": "Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image\n  Statistics",
        "abstract": "  We present a theoretical analysis of Gaussian-binary restricted Boltzmann\nmachines (GRBMs) from the perspective of density models. The key aspect of this\nanalysis is to show that GRBMs can be formulated as a constrained mixture of\nGaussians, which gives a much better insight into the model's capabilities and\nlimitations. We show that GRBMs are capable of learning meaningful features\nboth in a two-dimensional blind source separation task and in modeling natural\nimages. Further, we show that reported difficulties in training GRBMs are due\nto the failure of the training algorithm rather than the model itself. Based on\nour analysis we are able to propose several training recipes, which allowed\nsuccessful and fast training in our experiments. Finally, we discuss the\nrelationship of GRBMs to several modifications that have been proposed to\nimprove the model.\n",
        "published": "2014",
        "authors": [
            "Nan Wang",
            "Jan Melchior",
            "Laurenz Wiskott"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1402.1869v2",
        "title": "On the Number of Linear Regions of Deep Neural Networks",
        "abstract": "  We study the complexity of functions computable by deep feedforward neural\nnetworks with piecewise linear activations in terms of the symmetries and the\nnumber of linear regions that they have. Deep networks are able to sequentially\nmap portions of each layer's input-space to the same output. In this way, deep\nmodels compute functions that react equally to complicated patterns of\ndifferent inputs. The compositional structure of these functions enables them\nto re-use pieces of computation exponentially often in terms of the network's\ndepth. This paper investigates the complexity of such compositional maps and\ncontributes new theoretical results regarding the advantage of depth for neural\nnetworks with piecewise linear activation functions. In particular, our\nanalysis is not specific to a single family of models, and as an example, we\nemploy it for rectifier and maxout networks. We improve complexity bounds from\npre-existing work and investigate the behavior of units in higher layers.\n",
        "published": "2014",
        "authors": [
            "Guido Mont\u00fafar",
            "Razvan Pascanu",
            "Kyunghyun Cho",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1402.3346v3",
        "title": "Geometry and Expressive Power of Conditional Restricted Boltzmann\n  Machines",
        "abstract": "  Conditional restricted Boltzmann machines are undirected stochastic neural\nnetworks with a layer of input and output units connected bipartitely to a\nlayer of hidden units. These networks define models of conditional probability\ndistributions on the states of the output units given the states of the input\nunits, parametrized by interaction weights and biases. We address the\nrepresentational power of these models, proving results their ability to\nrepresent conditional Markov random fields and conditional distributions with\nrestricted supports, the minimal size of universal approximators, the maximal\nmodel approximation errors, and on the dimension of the set of representable\nconditional distributions. We contribute new tools for investigating\nconditional probability models, which allow us to improve the results that can\nbe derived from existing work on restricted Boltzmann machine probability\nmodels.\n",
        "published": "2014",
        "authors": [
            "Guido Montufar",
            "Nihat Ay",
            "Keyan Ghazi-Zahedi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1405.1380v4",
        "title": "Is Joint Training Better for Deep Auto-Encoders?",
        "abstract": "  Traditionally, when generative models of data are developed via deep\narchitectures, greedy layer-wise pre-training is employed. In a well-trained\nmodel, the lower layer of the architecture models the data distribution\nconditional upon the hidden variables, while the higher layers model the hidden\ndistribution prior. But due to the greedy scheme of the layerwise training\ntechnique, the parameters of lower layers are fixed when training higher\nlayers. This makes it extremely challenging for the model to learn the hidden\ndistribution prior, which in turn leads to a suboptimal model for the data\ndistribution. We therefore investigate joint training of deep autoencoders,\nwhere the architecture is viewed as one stack of two or more single-layer\nautoencoders. A single global reconstruction objective is jointly optimized,\nsuch that the objective for the single autoencoders at each layer acts as a\nlocal, layer-level regularizer. We empirically evaluate the performance of this\njoint training scheme and observe that it not only learns a better data model,\nbut also learns better higher layer representations, which highlights its\npotential for unsupervised feature learning. In addition, we find that the\nusage of regularizations in the joint training scheme is crucial in achieving\ngood performance. In the supervised setting, joint training also shows superior\nperformance when training deeper models. The joint training framework can thus\nprovide a platform for investigating more efficient usage of different types of\nregularizers, especially in light of the growing volumes of available unlabeled\ndata.\n",
        "published": "2014",
        "authors": [
            "Yingbo Zhou",
            "Devansh Arpit",
            "Ifeoma Nwogu",
            "Venu Govindaraju"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1405.1436v1",
        "title": "Training Restricted Boltzmann Machine by Perturbation",
        "abstract": "  A new approach to maximum likelihood learning of discrete graphical models\nand RBM in particular is introduced. Our method, Perturb and Descend (PD) is\ninspired by two ideas (I) perturb and MAP method for sampling (II) learning by\nContrastive Divergence minimization. In contrast to perturb and MAP, PD\nleverages training data to learn the models that do not allow efficient MAP\nestimation. During the learning, to produce a sample from the current model, we\nstart from a training data and descend in the energy landscape of the\n\"perturbed model\", for a fixed number of steps, or until a local optima is\nreached. For RBM, this involves linear calculations and thresholding which can\nbe very fast. Furthermore we show that the amount of perturbation is closely\nrelated to the temperature parameter and it can regularize the model by\nproducing robust features resulting in sparse hidden layer activation.\n",
        "published": "2014",
        "authors": [
            "Siamak Ravanbakhsh",
            "Russell Greiner",
            "Brendan Frey"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1407.0611v1",
        "title": "How Many Dissimilarity/Kernel Self Organizing Map Variants Do We Need?",
        "abstract": "  In numerous applicative contexts, data are too rich and too complex to be\nrepresented by numerical vectors. A general approach to extend machine learning\nand data mining techniques to such data is to really on a dissimilarity or on a\nkernel that measures how different or similar two objects are. This approach\nhas been used to define several variants of the Self Organizing Map (SOM). This\npaper reviews those variants in using a common set of notations in order to\noutline differences and similarities between them. It discusses the advantages\nand drawbacks of the variants, as well as the actual relevance of the\ndissimilarity/kernel SOM for practical applications.\n",
        "published": "2014",
        "authors": [
            "Fabrice Rossi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1408.0848v8",
        "title": "Multilayer bootstrap networks",
        "abstract": "  Multilayer bootstrap network builds a gradually narrowed multilayer nonlinear\nnetwork from bottom up for unsupervised nonlinear dimensionality reduction.\nEach layer of the network is a nonparametric density estimator. It consists of\na group of k-centroids clusterings. Each clustering randomly selects data\npoints with randomly selected features as its centroids, and learns a one-hot\nencoder by one-nearest-neighbor optimization. Geometrically, the nonparametric\ndensity estimator at each layer projects the input data space to a\nuniformly-distributed discrete feature space, where the similarity of two data\npoints in the discrete feature space is measured by the number of the nearest\ncentroids they share in common. The multilayer network gradually reduces the\nnonlinear variations of data from bottom up by building a vast number of\nhierarchical trees implicitly on the original data space. Theoretically, the\nestimation error caused by the nonparametric density estimator is proportional\nto the correlation between the clusterings, both of which are reduced by the\nrandomization steps.\n",
        "published": "2014",
        "authors": [
            "Xiao-Lei Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1409.2574v4",
        "title": "Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures",
        "abstract": "  Model-based methods and deep neural networks have both been tremendously\nsuccessful paradigms in machine learning. In model-based methods, problem\ndomain knowledge can be built into the constraints of the model, typically at\nthe expense of difficulties during inference. In contrast, deterministic deep\nneural networks are constructed in such a way that inference is\nstraightforward, but their architectures are generic and it is unclear how to\nincorporate knowledge. This work aims to obtain the advantages of both\napproaches. To do so, we start with a model-based approach and an associated\ninference algorithm, and \\emph{unfold} the inference iterations as layers in a\ndeep network. Rather than optimizing the original model, we \\emph{untie} the\nmodel parameters across layers, in order to create a more powerful network. The\nresulting architecture can be trained discriminatively to perform accurate\ninference within a fixed network size. We show how this framework allows us to\ninterpret conventional networks as mean-field inference in Markov random\nfields, and to obtain new architectures by instead using belief propagation as\nthe inference algorithm. We then show its application to a non-negative matrix\nfactorization model that incorporates the problem-domain knowledge that sound\nsources are additive. Deep unfolding of this model yields a new kind of\nnon-negative deep neural network, that can be trained using a multiplicative\nbackpropagation-style update algorithm. We present speech enhancement\nexperiments showing that our approach is competitive with conventional neural\nnetworks despite using far fewer parameters.\n",
        "published": "2014",
        "authors": [
            "John R. Hershey",
            "Jonathan Le Roux",
            "Felix Weninger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1409.7495v2",
        "title": "Unsupervised Domain Adaptation by Backpropagation",
        "abstract": "  Top-performing deep architectures are trained on massive amounts of labeled\ndata. In the absence of labeled data for a certain task, domain adaptation\noften provides an attractive option given that labeled data of similar nature\nbut from a different domain (e.g. synthetic images) are available. Here, we\npropose a new approach to domain adaptation in deep architectures that can be\ntrained on large amount of labeled data from the source domain and large amount\nof unlabeled data from the target domain (no labeled target-domain data is\nnecessary).\n  As the training progresses, the approach promotes the emergence of \"deep\"\nfeatures that are (i) discriminative for the main learning task on the source\ndomain and (ii) invariant with respect to the shift between the domains. We\nshow that this adaptation behaviour can be achieved in almost any feed-forward\nmodel by augmenting it with few standard layers and a simple new gradient\nreversal layer. The resulting augmented architecture can be trained using\nstandard backpropagation.\n  Overall, the approach can be implemented with little effort using any of the\ndeep-learning packages. The method performs very well in a series of image\nclassification experiments, achieving adaptation effect in the presence of big\ndomain shifts and outperforming previous state-of-the-art on Office datasets.\n",
        "published": "2014",
        "authors": [
            "Yaroslav Ganin",
            "Victor Lempitsky"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1410.0630v1",
        "title": "Deep Directed Generative Autoencoders",
        "abstract": "  For discrete data, the likelihood $P(x)$ can be rewritten exactly and\nparametrized into $P(X = x) = P(X = x | H = f(x)) P(H = f(x))$ if $P(X | H)$\nhas enough capacity to put no probability mass on any $x'$ for which $f(x')\\neq\nf(x)$, where $f(\\cdot)$ is a deterministic discrete function. The log of the\nfirst factor gives rise to the log-likelihood reconstruction error of an\nautoencoder with $f(\\cdot)$ as the encoder and $P(X|H)$ as the (probabilistic)\ndecoder. The log of the second term can be seen as a regularizer on the encoded\nactivations $h=f(x)$, e.g., as in sparse autoencoders. Both encoder and decoder\ncan be represented by a deep neural network and trained to maximize the average\nof the optimal log-likelihood $\\log p(x)$. The objective is to learn an encoder\n$f(\\cdot)$ that maps $X$ to $f(X)$ that has a much simpler distribution than\n$X$ itself, estimated by $P(H)$. This \"flattens the manifold\" or concentrates\nprobability mass in a smaller number of (relevant) dimensions over which the\ndistribution factorizes. Generating samples from the model is straightforward\nusing ancestral sampling. One challenge is that regular back-propagation cannot\nbe used to obtain the gradient on the parameters of the encoder, but we find\nthat using the straight-through estimator works well here. We also find that\nalthough optimizing a single level of such architecture may be difficult, much\nbetter results can be obtained by pre-training and stacking them, gradually\ntransforming the data distribution into one that is more easily captured by a\nsimple parametric model.\n",
        "published": "2014",
        "authors": [
            "Sherjil Ozair",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1410.3831v1",
        "title": "An exact mapping between the Variational Renormalization Group and Deep\n  Learning",
        "abstract": "  Deep learning is a broad set of techniques that uses multiple layers of\nrepresentation to automatically learn relevant features directly from\nstructured data. Recently, such techniques have yielded record-breaking results\non a diverse set of difficult machine learning tasks in computer vision, speech\nrecognition, and natural language processing. Despite the enormous success of\ndeep learning, relatively little is understood theoretically about why these\ntechniques are so successful at feature learning and compression. Here, we show\nthat deep learning is intimately related to one of the most important and\nsuccessful techniques in theoretical physics, the renormalization group (RG).\nRG is an iterative coarse-graining scheme that allows for the extraction of\nrelevant features (i.e. operators) as a physical system is examined at\ndifferent length scales. We construct an exact mapping from the variational\nrenormalization group, first introduced by Kadanoff, and deep learning\narchitectures based on Restricted Boltzmann Machines (RBMs). We illustrate\nthese ideas using the nearest-neighbor Ising Model in one and two-dimensions.\nOur results suggests that deep learning algorithms may be employing a\ngeneralized RG-like scheme to learn relevant features from data.\n",
        "published": "2014",
        "authors": [
            "Pankaj Mehta",
            "David J. Schwab"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1410.4599v2",
        "title": "Non-parametric Bayesian Learning with Deep Learning Structure and Its\n  Applications in Wireless Networks",
        "abstract": "  In this paper, we present an infinite hierarchical non-parametric Bayesian\nmodel to extract the hidden factors over observed data, where the number of\nhidden factors for each layer is unknown and can be potentially infinite.\nMoreover, the number of layers can also be infinite. We construct the model\nstructure that allows continuous values for the hidden factors and weights,\nwhich makes the model suitable for various applications. We use the\nMetropolis-Hastings method to infer the model structure. Then the performance\nof the algorithm is evaluated by the experiments. Simulation results show that\nthe model fits the underlying structure of simulated data.\n",
        "published": "2014",
        "authors": [
            "Erte Pan",
            "Zhu Han"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1410.7455v8",
        "title": "Parallel training of DNNs with Natural Gradient and Parameter Averaging",
        "abstract": "  We describe the neural-network training framework used in the Kaldi speech\nrecognition toolkit, which is geared towards training DNNs with large amounts\nof training data using multiple GPU-equipped or multi-core machines. In order\nto be as hardware-agnostic as possible, we needed a way to use multiple\nmachines without generating excessive network traffic. Our method is to average\nthe neural network parameters periodically (typically every minute or two), and\nredistribute the averaged parameters to the machines for further training. Each\nmachine sees different data. By itself, this method does not work very well.\nHowever, we have another method, an approximate and efficient implementation of\nNatural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow\nour periodic-averaging method to work well, as well as substantially improving\nthe convergence of SGD on a single machine.\n",
        "published": "2014",
        "authors": [
            "Daniel Povey",
            "Xiaohui Zhang",
            "Sanjeev Khudanpur"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1410.7550v1",
        "title": "Learning deep dynamical models from image pixels",
        "abstract": "  Modeling dynamical systems is important in many disciplines, e.g., control,\nrobotics, or neurotechnology. Commonly the state of these systems is not\ndirectly observed, but only available through noisy and potentially\nhigh-dimensional observations. In these cases, system identification, i.e.,\nfinding the measurement mapping and the transition mapping (system dynamics) in\nlatent space can be challenging. For linear system dynamics and measurement\nmappings efficient solutions for system identification are available. However,\nin practical applications, the linearity assumptions does not hold, requiring\nnon-linear system identification techniques. If additionally the observations\nare high-dimensional (e.g., images), non-linear system identification is\ninherently hard. To address the problem of non-linear system identification\nfrom high-dimensional observations, we combine recent advances in deep learning\nand system identification. In particular, we jointly learn a low-dimensional\nembedding of the observation by means of deep auto-encoders and a predictive\ntransition model in this low-dimensional space. We demonstrate that our model\nenables learning good predictive models of dynamical systems from pixel\ninformation only.\n",
        "published": "2014",
        "authors": [
            "Niklas Wahlstr\u00f6m",
            "Thomas B. Sch\u00f6n",
            "Marc Peter Deisenroth"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1411.7783v2",
        "title": "From neural PCA to deep unsupervised learning",
        "abstract": "  A network supporting deep unsupervised learning is presented. The network is\nan autoencoder with lateral shortcut connections from the encoder to decoder at\neach level of the hierarchy. The lateral shortcut connections allow the higher\nlevels of the hierarchy to focus on abstract invariant features. While standard\nautoencoders are analogous to latent variable models with a single layer of\nstochastic variables, the proposed network is analogous to hierarchical latent\nvariables models. Learning combines denoising autoencoder and denoising sources\nseparation frameworks. Each layer of the network contributes to the cost\nfunction a term which measures the distance of the representations produced by\nthe encoder and the decoder. Since training signals originate from all levels\nof the network, all layers can learn efficiently even in deep networks. The\nspeedup offered by cost terms from higher levels of the hierarchy and the\nability to learn invariant features are demonstrated in experiments.\n",
        "published": "2014",
        "authors": [
            "Harri Valpola"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.1602v1",
        "title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent\n  NN: First Results",
        "abstract": "  We replace the Hidden Markov Model (HMM) which is traditionally used in in\ncontinuous speech recognition with a bi-directional recurrent neural network\nencoder coupled to a recurrent neural network decoder that directly emits a\nstream of phonemes. The alignment between the input and output sequences is\nestablished using an attention mechanism: the decoder emits each symbol based\non a context created with a subset of input symbols elected by the attention\nmechanism. We report initial results demonstrating that this new approach\nachieves phoneme error rates that are comparable to the state-of-the-art\nHMM-based decoders, on the TIMIT dataset.\n",
        "published": "2014",
        "authors": [
            "Jan Chorowski",
            "Dzmitry Bahdanau",
            "Kyunghyun Cho",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.2693v4",
        "title": "Provable Methods for Training Neural Networks with Sparse Connectivity",
        "abstract": "  We provide novel guaranteed approaches for training feedforward neural\nnetworks with sparse connectivity. We leverage on the techniques developed\npreviously for learning linear networks and show that they can also be\neffectively adopted to learn non-linear networks. We operate on the moments\ninvolving label and the score function of the input, and show that their\nfactorization provably yields the weight matrix of the first layer of a deep\nnetwork under mild conditions. In practice, the output of our method can be\nemployed as effective initializers for gradient descent.\n",
        "published": "2014",
        "authors": [
            "Hanie Sedghi",
            "Anima Anandkumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.4446v2",
        "title": "Domain-Adversarial Neural Networks",
        "abstract": "  We introduce a new representation learning algorithm suited to the context of\ndomain adaptation, in which data at training and test time come from similar\nbut different distributions. Our algorithm is directly inspired by theory on\ndomain adaptation suggesting that, for effective domain transfer to be\nachieved, predictions must be made based on a data representation that cannot\ndiscriminate between the training (source) and test (target) domains. We\npropose a training objective that implements this idea in the context of a\nneural network, whose hidden layer is trained to be predictive of the\nclassification task, but uninformative as to the domain of the input. Our\nexperiments on a sentiment analysis classification benchmark, where the target\ndomain data available at training time is unlabeled, show that our neural\nnetwork for domain adaption algorithm has better performance than either a\nstandard neural network or an SVM, even if trained on input features extracted\nwith the state-of-the-art marginalized stacked denoising autoencoders of Chen\net al. (2012).\n",
        "published": "2014",
        "authors": [
            "Hana Ajakan",
            "Pascal Germain",
            "Hugo Larochelle",
            "Fran\u00e7ois Laviolette",
            "Mario Marchand"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.4864v1",
        "title": "Learning with Pseudo-Ensembles",
        "abstract": "  We formalize the notion of a pseudo-ensemble, a (possibly infinite)\ncollection of child models spawned from a parent model by perturbing it\naccording to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep\nneural network trains a pseudo-ensemble of child subnetworks generated by\nrandomly masking nodes in the parent network. We present a novel regularizer\nbased on making the behavior of a pseudo-ensemble robust with respect to the\nnoise process generating it. In the fully-supervised setting, our regularizer\nmatches the performance of dropout. But, unlike dropout, our regularizer\nnaturally extends to the semi-supervised setting, where it produces\nstate-of-the-art results. We provide a case study in which we transform the\nRecursive Neural Tensor Network of (Socher et. al, 2013) into a\npseudo-ensemble, which significantly improves its performance on a real-world\nsentiment analysis benchmark.\n",
        "published": "2014",
        "authors": [
            "Philip Bachman",
            "Ouais Alsharif",
            "Doina Precup"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.6544v6",
        "title": "Qualitatively characterizing neural network optimization problems",
        "abstract": "  Training neural networks involves solving large-scale non-convex optimization\nproblems. This task has long been believed to be extremely difficult, with fear\nof local minima and other obstacles motivating a variety of schemes to improve\noptimization, such as unsupervised pretraining. However, modern neural networks\nare able to achieve negligible training error on complex tasks, using only\ndirect training with stochastic gradient descent. We introduce a simple\nanalysis technique to look for evidence that such networks are overcoming local\noptima. We find that, in fact, on a straight path from initialization to\nsolution, a variety of state of the art neural networks never encounter any\nsignificant obstacles.\n",
        "published": "2014",
        "authors": [
            "Ian J. Goodfellow",
            "Oriol Vinyals",
            "Andrew M. Saxe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.6558v3",
        "title": "Random Walk Initialization for Training Very Deep Feedforward Networks",
        "abstract": "  Training very deep networks is an important open problem in machine learning.\nOne of many difficulties is that the norm of the back-propagated error gradient\ncan grow or decay exponentially. Here we show that training very deep\nfeed-forward networks (FFNs) is not as difficult as previously thought. Unlike\nwhen back-propagation is applied to a recurrent network, application to an FFN\namounts to multiplying the error gradient by a different random matrix at each\nlayer. We show that the successive application of correctly scaled random\nmatrices to an initial vector results in a random walk of the log of the norm\nof the resulting vectors, and we compute the scaling that makes this walk\nunbiased. The variance of the random walk grows only linearly with network\ndepth and is inversely proportional to the size of each layer. Practically,\nthis implies a gradient whose log-norm scales with the square root of the\nnetwork depth and shows that the vanishing gradient problem can be mitigated by\nincreasing the width of the layers. Mathematical analyses and experimental\nresults using stochastic gradient descent to optimize tasks related to the\nMNIST and TIMIT datasets are provided to support these claims. Equations for\nthe optimal matrix scaling are provided for the linear and ReLU cases.\n",
        "published": "2014",
        "authors": [
            "David Sussillo",
            "L. F. Abbott"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.6581v6",
        "title": "Variational Recurrent Auto-Encoders",
        "abstract": "  In this paper we propose a model that combines the strengths of RNNs and\nSGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used\nfor efficient, large scale unsupervised learning on time series data, mapping\nthe time series data to a latent vector representation. The model is\ngenerative, such that data can be generated from samples of the latent space.\nAn important contribution of this work is that the model can make use of\nunlabeled data in order to facilitate supervised training of RNNs by\ninitialising the weights and network state.\n",
        "published": "2014",
        "authors": [
            "Otto Fabius",
            "Joost R. van Amersfoort"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.6621v3",
        "title": "Why does Deep Learning work? - A perspective from Group Theory",
        "abstract": "  Why does Deep Learning work? What representations does it capture? How do\nhigher-order representations emerge? We study these questions from the\nperspective of group theory, thereby opening a new approach towards a theory of\nDeep learning.\n  One factor behind the recent resurgence of the subject is a key algorithmic\nstep called pre-training: first search for a good generative model for the\ninput samples, and repeat the process one layer at a time. We show deeper\nimplications of this simple principle, by establishing a connection with the\ninterplay of orbits and stabilizers of group actions. Although the neural\nnetworks themselves may not form groups, we show the existence of {\\em shadow}\ngroups whose elements serve as close approximations.\n  Over the shadow groups, the pre-training step, originally introduced as a\nmechanism to better initialize a network, becomes equivalent to a search for\nfeatures with minimal orbits. Intuitively, these features are in a way the {\\em\nsimplest}. Which explains why a deep learning network learns simple features\nfirst. Next, we show how the same principle, when repeated in the deeper\nlayers, can capture higher order representations, and why representation\ncomplexity increases as the layers get deeper.\n",
        "published": "2014",
        "authors": [
            "Arnab Paul",
            "Suresh Venkatasubramanian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.6630v2",
        "title": "Neural Network Regularization via Robust Weight Factorization",
        "abstract": "  Regularization is essential when training large neural networks. As deep\nneural networks can be mathematically interpreted as universal function\napproximators, they are effective at memorizing sampling noise in the training\ndata. This results in poor generalization to unseen data. Therefore, it is no\nsurprise that a new regularization technique, Dropout, was partially\nresponsible for the now-ubiquitous winning entry to ImageNet 2012 by the\nUniversity of Toronto. Currently, Dropout (and related methods such as\nDropConnect) are the most effective means of regularizing large neural\nnetworks. These amount to efficiently visiting a large number of related models\nat training time, while aggregating them to a single predictor at test time.\nThe proposed FaMe model aims to apply a similar strategy, yet learns a\nfactorization of each weight matrix such that the factors are robust to noise.\n",
        "published": "2014",
        "authors": [
            "Jan Rudy",
            "Weiguang Ding",
            "Daniel Jiwoong Im",
            "Graham W. Taylor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.7003v3",
        "title": "A Bayesian encourages dropout",
        "abstract": "  Dropout is one of the key techniques to prevent the learning from\noverfitting. It is explained that dropout works as a kind of modified L2\nregularization. Here, we shed light on the dropout from Bayesian standpoint.\nBayesian interpretation enables us to optimize the dropout rate, which is\nbeneficial for learning of weight parameters and prediction after learning. The\nexperiment result also encourages the optimization of the dropout.\n",
        "published": "2014",
        "authors": [
            "Shin-ichi Maeda"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.7149v4",
        "title": "Deep Fried Convnets",
        "abstract": "  The fully connected layers of a deep convolutional neural network typically\ncontain over 90% of the network parameters, and consume the majority of the\nmemory required to store the network parameters. Reducing the number of\nparameters while preserving essentially the same predictive performance is\ncritically important for operating deep neural networks in memory constrained\nenvironments such as GPUs or embedded devices.\n  In this paper we show how kernel methods, in particular a single Fastfood\nlayer, can be used to replace all fully connected layers in a deep\nconvolutional neural network. This novel Fastfood layer is also end-to-end\ntrainable in conjunction with convolutional layers, allowing us to combine them\ninto a new architecture, named deep fried convolutional networks, which\nsubstantially reduces the memory footprint of convolutional networks trained on\nMNIST and ImageNet with no drop in predictive performance.\n",
        "published": "2014",
        "authors": [
            "Zichao Yang",
            "Marcin Moczulski",
            "Misha Denil",
            "Nando de Freitas",
            "Alex Smola",
            "Le Song",
            "Ziyu Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.7419v5",
        "title": "ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient",
        "abstract": "  Stochastic gradient algorithms have been the main focus of large-scale\nlearning problems and they led to important successes in machine learning. The\nconvergence of SGD depends on the careful choice of learning rate and the\namount of the noise in stochastic estimates of the gradients. In this paper, we\npropose a new adaptive learning rate algorithm, which utilizes curvature\ninformation for automatically tuning the learning rates. The information about\nthe element-wise curvature of the loss function is estimated from the local\nstatistics of the stochastic first order gradients. We further propose a new\nvariance reduction technique to speed up the convergence. In our preliminary\nexperiments with deep neural networks, we obtained better performance compared\nto the popular stochastic gradient algorithms.\n",
        "published": "2014",
        "authors": [
            "Caglar Gulcehre",
            "Marcin Moczulski",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.7489v3",
        "title": "A Unified Perspective on Multi-Domain and Multi-Task Learning",
        "abstract": "  In this paper, we provide a new neural-network based perspective on\nmulti-task learning (MTL) and multi-domain learning (MDL). By introducing the\nconcept of a semantic descriptor, this framework unifies MDL and MTL as well as\nencompassing various classic and recent MTL/MDL algorithms by interpreting them\nas different ways of constructing semantic descriptors. Our interpretation\nprovides an alternative pipeline for zero-shot learning (ZSL), where a model\nfor a novel class can be constructed without training data. Moreover, it leads\nto a new and practically relevant problem setting of zero-shot domain\nadaptation (ZSDA), which is the analogous to ZSL but for novel domains: A model\nfor an unseen domain can be generated by its semantic descriptor. Experiments\nacross this range of problems demonstrate that our framework outperforms a\nvariety of alternatives.\n",
        "published": "2014",
        "authors": [
            "Yongxin Yang",
            "Timothy M. Hospedales"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1501.07227v5",
        "title": "A Neural Network Anomaly Detector Using the Random Cluster Model",
        "abstract": "  The random cluster model is used to define an upper bound on a distance\nmeasure as a function of the number of data points to be classified and the\nexpected value of the number of classes to form in a hybrid K-means and\nregression classification methodology, with the intent of detecting anomalies.\nConditions are given for the identification of classes which contain anomalies\nand individual anomalies within identified classes. A neural network model\ndescribes the decision region-separating surface for offline storage and recall\nin any new anomaly detection.\n",
        "published": "2015",
        "authors": [
            "Robert A. Murphy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1502.02072v1",
        "title": "Massively Multitask Networks for Drug Discovery",
        "abstract": "  Massively multitask neural architectures provide a learning framework for\ndrug discovery that synthesizes information from many distinct biological\nsources. To train these architectures at scale, we gather large amounts of data\nfrom public sources to create a dataset of nearly 40 million measurements\nacross more than 200 biological targets. We investigate several aspects of the\nmultitask framework by performing a series of empirical studies and obtain some\ninteresting results: (1) massively multitask networks obtain predictive\naccuracies significantly better than single-task methods, (2) the predictive\npower of multitask networks improves as additional tasks and data are added,\n(3) the total amount of data and the total number of tasks both contribute\nsignificantly to multitask improvement, and (4) multitask networks afford\nlimited transferability to tasks not in the training set. Our results\nunderscore the need for greater data sharing and further algorithmic innovation\nto accelerate the drug discovery process.\n",
        "published": "2015",
        "authors": [
            "Bharath Ramsundar",
            "Steven Kearnes",
            "Patrick Riley",
            "Dale Webster",
            "David Konerding",
            "Vijay Pande"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1502.02367v4",
        "title": "Gated Feedback Recurrent Neural Networks",
        "abstract": "  In this work, we propose a novel recurrent neural network (RNN) architecture.\nThe proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of\nstacking multiple recurrent layers by allowing and controlling signals flowing\nfrom upper recurrent layers to lower layers using a global gating unit for each\npair of layers. The recurrent signals exchanged between layers are gated\nadaptively based on the previous hidden states and the current input. We\nevaluated the proposed GF-RNN with different types of recurrent units, such as\ntanh, long short-term memory and gated recurrent units, on the tasks of\ncharacter-level language modeling and Python program evaluation. Our empirical\nevaluation of different RNN units, revealed that in both tasks, the GF-RNN\noutperforms the conventional approaches to build deep stacked RNNs. We suggest\nthat the improvement arises because the GF-RNN can adaptively assign different\nlayers to different timescales and layer-to-layer interactions (including the\ntop-down ones which are not usually present in a stacked RNN) by learning to\ngate these interactions.\n",
        "published": "2015",
        "authors": [
            "Junyoung Chung",
            "Caglar Gulcehre",
            "Kyunghyun Cho",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1502.02551v1",
        "title": "Deep Learning with Limited Numerical Precision",
        "abstract": "  Training of large-scale deep neural networks is often constrained by the\navailable computational resources. We study the effect of limited precision\ndata representation and computation on neural network training. Within the\ncontext of low-precision fixed-point computations, we observe the rounding\nscheme to play a crucial role in determining the network's behavior during\ntraining. Our results show that deep networks can be trained using only 16-bit\nwide fixed-point number representation when using stochastic rounding, and\nincur little to no degradation in the classification accuracy. We also\ndemonstrate an energy-efficient hardware accelerator that implements\nlow-precision fixed-point arithmetic with stochastic rounding.\n",
        "published": "2015",
        "authors": [
            "Suyog Gupta",
            "Ankur Agrawal",
            "Kailash Gopalakrishnan",
            "Pritish Narayanan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1502.03509v2",
        "title": "MADE: Masked Autoencoder for Distribution Estimation",
        "abstract": "  There has been a lot of recent interest in designing neural network models to\nestimate a distribution from a set of examples. We introduce a simple\nmodification for autoencoder neural networks that yields powerful generative\nmodels. Our method masks the autoencoder's parameters to respect autoregressive\nconstraints: each input is reconstructed only from previous inputs in a given\nordering. Constrained this way, the autoencoder outputs can be interpreted as a\nset of conditional probabilities, and their product, the full joint\nprobability. We can also train a single network that can decompose the joint\nprobability in multiple different orderings. Our simple framework can be\napplied to multiple architectures, including deep ones. Vectorized\nimplementations, such as on GPUs, are simple and fast. Experiments demonstrate\nthat this approach is competitive with state-of-the-art tractable distribution\nestimators. At test time, the method is significantly faster and scales better\nthan other autoregressive estimators.\n",
        "published": "2015",
        "authors": [
            "Mathieu Germain",
            "Karol Gregor",
            "Iain Murray",
            "Hugo Larochelle"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1502.04434v3",
        "title": "Invariant backpropagation: how to train a transformation-invariant\n  neural network",
        "abstract": "  In many classification problems a classifier should be robust to small\nvariations in the input vector. This is a desired property not only for\nparticular transformations, such as translation and rotation in image\nclassification problems, but also for all others for which the change is small\nenough to retain the object perceptually indistinguishable. We propose two\nextensions of the backpropagation algorithm that train a neural network to be\nrobust to variations in the feature vector. While the first of them enforces\nrobustness of the loss function to all variations, the second method trains the\npredictions to be robust to a particular variation which changes the loss\nfunction the most. The second methods demonstrates better results, but is\nslightly slower. We analytically compare the proposed algorithm with two the\nmost similar approaches (Tangent BP and Adversarial Training), and propose\ntheir fast versions. In the experimental part we perform comparison of all\nalgorithms in terms of classification accuracy and robustness to noise on MNIST\nand CIFAR-10 datasets. Additionally we analyze how the performance of the\nproposed algorithm depends on the dataset size and data augmentation.\n",
        "published": "2015",
        "authors": [
            "Sergey Demyanov",
            "James Bailey",
            "Ramamohanarao Kotagiri",
            "Christopher Leckie"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1505.02462v3",
        "title": "Soft-Deep Boltzmann Machines",
        "abstract": "  We present a layered Boltzmann machine (BM) that can better exploit the\nadvantages of a distributed representation. It is widely believed that deep BMs\n(DBMs) have far greater representational power than its shallow counterpart,\nrestricted Boltzmann machines (RBMs). However, this expectation on the\nsupremacy of DBMs over RBMs has not ever been validated in a theoretical\nfashion. In this paper, we provide both theoretical and empirical evidences\nthat the representational power of DBMs can be actually rather limited in\ntaking advantages of distributed representations. We propose an approximate\nmeasure for the representational power of a BM regarding to the efficiency of a\ndistributed representation. With this measure, we show a surprising fact that\nDBMs can make inefficient use of distributed representations. Based on these\nobservations, we propose an alternative BM architecture, which we dub soft-deep\nBMs (sDBMs). We show that sDBMs can more efficiently exploit the distributed\nrepresentations in terms of the measure. Experiments demonstrate that sDBMs\noutperform several state-of-the-art models, including DBMs, in generative tasks\non binarized MNIST and Caltech-101 silhouettes.\n",
        "published": "2015",
        "authors": [
            "Taichi Kiwaki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1505.07818v4",
        "title": "Domain-Adversarial Training of Neural Networks",
        "abstract": "  We introduce a new representation learning approach for domain adaptation, in\nwhich data at training and test time come from similar but different\ndistributions. Our approach is directly inspired by the theory on domain\nadaptation suggesting that, for effective domain transfer to be achieved,\npredictions must be made based on features that cannot discriminate between the\ntraining (source) and test (target) domains. The approach implements this idea\nin the context of neural network architectures that are trained on labeled data\nfrom the source domain and unlabeled data from the target domain (no labeled\ntarget-domain data is necessary). As the training progresses, the approach\npromotes the emergence of features that are (i) discriminative for the main\nlearning task on the source domain and (ii) indiscriminate with respect to the\nshift between the domains. We show that this adaptation behaviour can be\nachieved in almost any feed-forward model by augmenting it with few standard\nlayers and a new gradient reversal layer. The resulting augmented architecture\ncan be trained using standard backpropagation and stochastic gradient descent,\nand can thus be implemented with little effort using any of the deep learning\npackages. We demonstrate the success of our approach for two distinct\nclassification problems (document sentiment analysis and image classification),\nwhere state-of-the-art domain adaptation performance on standard benchmarks is\nachieved. We also validate the approach for descriptor learning task in the\ncontext of person re-identification application.\n",
        "published": "2015",
        "authors": [
            "Yaroslav Ganin",
            "Evgeniya Ustinova",
            "Hana Ajakan",
            "Pascal Germain",
            "Hugo Larochelle",
            "Fran\u00e7ois Laviolette",
            "Mario Marchand",
            "Victor Lempitsky"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.00327v1",
        "title": "Imaging Time-Series to Improve Classification and Imputation",
        "abstract": "  Inspired by recent successes of deep learning in computer vision, we propose\na novel framework for encoding time series as different types of images,\nnamely, Gramian Angular Summation/Difference Fields (GASF/GADF) and Markov\nTransition Fields (MTF). This enables the use of techniques from computer\nvision for time series classification and imputation. We used Tiled\nConvolutional Neural Networks (tiled CNNs) on 20 standard datasets to learn\nhigh-level features from the individual and compound GASF-GADF-MTF images. Our\napproaches achieve highly competitive results when compared to nine of the\ncurrent best time series classification approaches. Inspired by the bijection\nproperty of GASF on 0/1 rescaled data, we train Denoised Auto-encoders (DA) on\nthe GASF images of four standard and one synthesized compound dataset. The\nimputation MSE on test data is reduced by 12.18%-48.02% when compared to using\nthe raw data. An analysis of the features and weights learned via tiled CNNs\nand DAs explains why the approaches work.\n",
        "published": "2015",
        "authors": [
            "Zhiguang Wang",
            "Tim Oates"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.00619v1",
        "title": "Blocks and Fuel: Frameworks for deep learning",
        "abstract": "  We introduce two Python frameworks to train neural networks on large\ndatasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler\nwith CUDA-support. It facilitates the training of complex neural network models\nby providing parametrized Theano operations, attaching metadata to Theano's\nsymbolic computational graph, and providing an extensive set of utilities to\nassist training the networks, e.g. training algorithms, logging, monitoring,\nvisualization, and serialization. Fuel provides a standard format for machine\nlearning datasets. It allows the user to easily iterate over large datasets,\nperforming many types of pre-processing on the fly.\n",
        "published": "2015",
        "authors": [
            "Bart van Merri\u00ebnboer",
            "Dzmitry Bahdanau",
            "Vincent Dumoulin",
            "Dmitriy Serdyuk",
            "David Warde-Farley",
            "Jan Chorowski",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.02256v1",
        "title": "Knowledge Transfer Pre-training",
        "abstract": "  Pre-training is crucial for learning deep neural networks. Most of existing\npre-training methods train simple models (e.g., restricted Boltzmann machines)\nand then stack them layer by layer to form the deep structure. This layer-wise\npre-training has found strong theoretical foundation and broad empirical\nsupport. However, it is not easy to employ such method to pre-train models\nwithout a clear multi-layer structure,e.g., recurrent neural networks (RNNs).\nThis paper presents a new pre-training approach based on knowledge transfer\nlearning. In contrast to the layer-wise approach which trains model components\nincrementally, the new approach trains the entire model as a whole but with an\neasier objective function. This is achieved by utilizing soft targets produced\nby a prior trained model (teacher model). Compared to the conventional\nlayer-wise methods, this new method does not care about the model structure, so\ncan be used to pre-train very complex models. Experiments on a speech\nrecognition task demonstrated that with this approach, complex RNNs can be well\ntrained with a weaker deep neural network (DNN) model. Furthermore, the new\nmethod can be combined with conventional layer-wise pre-training to deliver\nadditional gains.\n",
        "published": "2015",
        "authors": [
            "Zhiyuan Tang",
            "Dong Wang",
            "Yiqiao Pan",
            "Zhiyong Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.02351v8",
        "title": "Stacked What-Where Auto-encoders",
        "abstract": "  We present a novel architecture, the \"stacked what-where auto-encoders\"\n(SWWAE), which integrates discriminative and generative pathways and provides a\nunified approach to supervised, semi-supervised and unsupervised learning\nwithout relying on sampling during training. An instantiation of SWWAE uses a\nconvolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and\nemploys a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the\nreconstruction. The objective function includes reconstruction terms that\ninduce the hidden states in the Deconvnet to be similar to those of the\nConvnet. Each pooling layer produces two sets of variables: the \"what\" which\nare fed to the next layer, and its complementary variable \"where\" that are fed\nto the corresponding layer in the generative decoder.\n",
        "published": "2015",
        "authors": [
            "Junbo Zhao",
            "Michael Mathieu",
            "Ross Goroshin",
            "Yann LeCun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.02690v3",
        "title": "Adaptive Normalized Risk-Averting Training For Deep Neural Networks",
        "abstract": "  This paper proposes a set of new error criteria and learning approaches,\nAdaptive Normalized Risk-Averting Training (ANRAT), to attack the non-convex\noptimization problem in training deep neural networks (DNNs). Theoretically, we\ndemonstrate its effectiveness on global and local convexity lower-bounded by\nthe standard $L_p$-norm error. By analyzing the gradient on the convexity index\n$\\lambda$, we explain the reason why to learn $\\lambda$ adaptively using\ngradient descent works. In practice, we show how this method improves training\nof deep neural networks to solve visual recognition tasks on the MNIST and\nCIFAR-10 datasets. Without using pretraining or other tricks, we obtain results\ncomparable or superior to those reported in recent literature on the same tasks\nusing standard ConvNets + MSE/cross entropy. Performance on deep/shallow\nmultilayer perceptrons and Denoised Auto-encoders is also explored. ANRAT can\nbe combined with other quasi-Newton training methods, innovative network\nvariants, regularization techniques and other specific tricks in DNNs. Other\nthan unsupervised pretraining, it provides a new perspective to address the\nnon-convex optimization problem in DNNs.\n",
        "published": "2015",
        "authors": [
            "Zhiguang Wang",
            "Tim Oates",
            "James Lo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.02914v2",
        "title": "Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer\n  Free Energy",
        "abstract": "  Restricted Boltzmann machines are undirected neural networks which have been\nshown to be effective in many applications, including serving as\ninitializations for training deep multi-layer neural networks. One of the main\nreasons for their success is the existence of efficient and practical\nstochastic algorithms, such as contrastive divergence, for unsupervised\ntraining. We propose an alternative deterministic iterative procedure based on\nan improved mean field method from statistical physics known as the\nThouless-Anderson-Palmer approach. We demonstrate that our algorithm provides\nperformance equal to, and sometimes superior to, persistent contrastive\ndivergence, while also providing a clear and easy to evaluate objective\nfunction. We believe that this strategy can be easily generalized to other\nmodels as well as to more accurate higher-order approximations, paving the way\nfor systematic improvements in training Boltzmann machines with hidden units.\n",
        "published": "2015",
        "authors": [
            "Marylou Gabri\u00e9",
            "Eric W. Tramel",
            "Florent Krzakala"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.03134v2",
        "title": "Pointer Networks",
        "abstract": "  We introduce a new neural architecture to learn the conditional probability\nof an output sequence with elements that are discrete tokens corresponding to\npositions in an input sequence. Such problems cannot be trivially addressed by\nexistent approaches such as sequence-to-sequence and Neural Turing Machines,\nbecause the number of target classes in each step of the output depends on the\nlength of the input, which is variable. Problems such as sorting variable sized\nsequences, and various combinatorial optimization problems belong to this\nclass. Our model solves the problem of variable size output dictionaries using\na recently proposed mechanism of neural attention. It differs from the previous\nattention attempts in that, instead of using attention to blend hidden units of\nan encoder to a context vector at each decoder step, it uses attention as a\npointer to select a member of the input sequence as the output. We call this\narchitecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn\napproximate solutions to three challenging geometric problems -- finding planar\nconvex hulls, computing Delaunay triangulations, and the planar Travelling\nSalesman Problem -- using training examples alone. Ptr-Nets not only improve\nover sequence-to-sequence with input attention, but also allow us to generalize\nto variable size output dictionaries. We show that the learnt models generalize\nbeyond the maximum lengths they were trained on. We hope our results on these\ntasks will encourage a broader exploration of neural learning for discrete\nproblems.\n",
        "published": "2015",
        "authors": [
            "Oriol Vinyals",
            "Meire Fortunato",
            "Navdeep Jaitly"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.06472v2",
        "title": "A Theory of Local Learning, the Learning Channel, and the Optimality of\n  Backpropagation",
        "abstract": "  In a physical neural system, where storage and processing are intimately\nintertwined, the rules for adjusting the synaptic weights can only depend on\nvariables that are available locally, such as the activity of the pre- and\npost-synaptic neurons, resulting in local learning rules. A systematic\nframework for studying the space of local learning rules is obtained by first\nspecifying the nature of the local variables, and then the functional form that\nties them together into each learning rule. Such a framework enables also the\nsystematic discovery of new learning rules and exploration of relationships\nbetween learning rules and group symmetries. We study polynomial local learning\nrules stratified by their degree and analyze their behavior and capabilities in\nboth linear and non-linear units and networks. Stacking local learning rules in\ndeep feedforward networks leads to deep local learning. While deep local\nlearning can learn interesting representations, it cannot learn complex\ninput-output functions, even when targets are available for the top layer.\nLearning complex input-output functions requires local deep learning where\ntarget information is communicated to the deep layers through a backward\nlearning channel. The nature of the communicated information about the targets\nand the structure of the learning channel partition the space of learning\nalgorithms. We estimate the learning channel capacity associated with several\nalgorithms and show that backpropagation outperforms them by simultaneously\nmaximizing the information rate and minimizing the computational cost, even in\nrecurrent networks. The theory clarifies the concept of Hebbian learning,\nestablishes the power and limitations of local learning rules, introduces the\nlearning channel which enables a formal analysis of the optimality of\nbackpropagation, and explains the sparsity of the space of learning rules\ndiscovered so far.\n",
        "published": "2015",
        "authors": [
            "Pierre Baldi",
            "Peter Sadowski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.08473v3",
        "title": "Beating the Perils of Non-Convexity: Guaranteed Training of Neural\n  Networks using Tensor Methods",
        "abstract": "  Training neural networks is a challenging non-convex optimization problem,\nand backpropagation or gradient descent can get stuck in spurious local optima.\nWe propose a novel algorithm based on tensor decomposition for guaranteed\ntraining of two-layer neural networks. We provide risk bounds for our proposed\nmethod, with a polynomial sample complexity in the relevant parameters, such as\ninput dimension and number of neurons. While learning arbitrary target\nfunctions is NP-hard, we provide transparent conditions on the function and the\ninput for learnability. Our training method is based on tensor decomposition,\nwhich provably converges to the global optimum, under a set of mild\nnon-degeneracy conditions. It consists of simple embarrassingly parallel linear\nand multi-linear operations, and is competitive with standard stochastic\ngradient descent (SGD), in terms of computational complexity. Thus, we propose\na computationally efficient method with guaranteed risk bounds for training\nneural networks with one hidden layer.\n",
        "published": "2015",
        "authors": [
            "Majid Janzamin",
            "Hanie Sedghi",
            "Anima Anandkumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1507.00210v1",
        "title": "Natural Neural Networks",
        "abstract": "  We introduce Natural Neural Networks, a novel family of algorithms that speed\nup convergence by adapting their internal representation during training to\nimprove conditioning of the Fisher matrix. In particular, we show a specific\nexample that employs a simple and efficient reparametrization of the neural\nnetwork weights by implicitly whitening the representation obtained at each\nlayer, while preserving the feed-forward computation of the network. Such\nnetworks can be trained efficiently via the proposed Projected Natural Gradient\nDescent algorithm (PRONG), which amortizes the cost of these reparametrizations\nover many parameter updates and is closely related to the Mirror Descent online\nlearning algorithm. We highlight the benefits of our method on both\nunsupervised and supervised learning tasks, and showcase its scalability by\ntraining on the large-scale ImageNet Challenge dataset.\n",
        "published": "2015",
        "authors": [
            "Guillaume Desjardins",
            "Karen Simonyan",
            "Razvan Pascanu",
            "Koray Kavukcuoglu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1507.02672v2",
        "title": "Semi-Supervised Learning with Ladder Networks",
        "abstract": "  We combine supervised learning with unsupervised learning in deep neural\nnetworks. The proposed model is trained to simultaneously minimize the sum of\nsupervised and unsupervised cost functions by backpropagation, avoiding the\nneed for layer-wise pre-training. Our work builds on the Ladder network\nproposed by Valpola (2015), which we extend by combining the model with\nsupervision. We show that the resulting model reaches state-of-the-art\nperformance in semi-supervised MNIST and CIFAR-10 classification, in addition\nto permutation-invariant MNIST classification with all labels.\n",
        "published": "2015",
        "authors": [
            "Antti Rasmus",
            "Harri Valpola",
            "Mikko Honkala",
            "Mathias Berglund",
            "Tapani Raiko"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1507.07680v2",
        "title": "Training recurrent networks online without backtracking",
        "abstract": "  We introduce the \"NoBackTrack\" algorithm to train the parameters of dynamical\nsystems such as recurrent neural networks. This algorithm works in an online,\nmemoryless setting, thus requiring no backpropagation through time, and is\nscalable, avoiding the large computational and memory cost of maintaining the\nfull gradient of the current state with respect to the parameters.\n  The algorithm essentially maintains, at each time, a single search direction\nin parameter space. The evolution of this search direction is partly stochastic\nand is constructed in such a way to provide, at every time, an unbiased random\nestimate of the gradient of the loss function with respect to the parameters.\nBecause the gradient estimate is unbiased, on average over time the parameter\nis updated as it should.\n  The resulting gradient estimate can then be fed to a lightweight Kalman-like\nfilter to yield an improved algorithm. For recurrent neural networks, the\nresulting algorithms scale linearly with the number of parameters.\n  Small-scale experiments confirm the suitability of the approach, showing that\nthe stochastic approximation of the gradient introduced in the algorithm is not\ndetrimental to learning. In particular, the Kalman-like version of NoBackTrack\nis superior to backpropagation through time (BPTT) when the time span of\ndependencies in the data is longer than the truncation span for BPTT.\n",
        "published": "2015",
        "authors": [
            "Yann Ollivier",
            "Corentin Tallec",
            "Guillaume Charpiat"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1508.04306v1",
        "title": "Deep clustering: Discriminative embeddings for segmentation and\n  separation",
        "abstract": "  We address the problem of acoustic source separation in a deep learning\nframework we call \"deep clustering.\" Rather than directly estimating signals or\nmasking functions, we train a deep network to produce spectrogram embeddings\nthat are discriminative for partition labels given in training data. Previous\ndeep network approaches provide great advantages in terms of learning power and\nspeed, but previously it has been unclear how to use them to separate signals\nin a class-independent way. In contrast, spectral clustering approaches are\nflexible with respect to the classes and number of items to be segmented, but\nit has been unclear how to leverage the learning power and speed of deep\nnetworks. To obtain the best of both worlds, we use an objective function that\nto train embeddings that yield a low-rank approximation to an ideal pairwise\naffinity matrix, in a class-independent way. This avoids the high cost of\nspectral factorization and instead produces compact clusters that are amenable\nto simple clustering methods. The segmentations are therefore implicitly\nencoded in the embeddings, and can be \"decoded\" by clustering. Preliminary\nexperiments show that the proposed method can separate speech: when trained on\nspectrogram features containing mixtures of two speakers, and tested on\nmixtures of a held-out set of speakers, it can infer masking functions that\nimprove signal quality by around 6dB. We show that the model can generalize to\nthree-speaker mixtures despite training only on two-speaker mixtures. The\nframework can be used without class labels, and therefore has the potential to\nbe trained on a diverse set of sound types, and to generalize to novel sources.\nWe hope that future work will lead to segmentation of arbitrary sounds, with\nextensions to microphone array methods as well as image segmentation and other\ndomains.\n",
        "published": "2015",
        "authors": [
            "John R. Hershey",
            "Zhuo Chen",
            "Jonathan Le Roux",
            "Shinji Watanabe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1508.04422v3",
        "title": "Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural\n  Networks",
        "abstract": "  Several popular graph embedding techniques for representation learning and\ndimensionality reduction rely on performing computationally expensive\neigendecompositions to derive a nonlinear transformation of the input data\nspace. The resulting eigenvectors encode the embedding coordinates for the\ntraining samples only, and so the embedding of novel data samples requires\nfurther costly computation. In this paper, we present a method for the\nout-of-sample extension of graph embeddings using deep neural networks (DNN) to\nparametrically approximate these nonlinear maps. Compared with traditional\nnonparametric out-of-sample extension methods, we demonstrate that the DNNs can\ngeneralize with equal or better fidelity and require orders of magnitude less\ncomputation at test time. Moreover, we find that unsupervised pretraining of\nthe DNNs improves optimization for larger network sizes, thus removing\nsensitivity to model selection.\n",
        "published": "2015",
        "authors": [
            "Aren Jansen",
            "Gregory Sell",
            "Vince Lyzinski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1508.06095v1",
        "title": "OCReP: An Optimally Conditioned Regularization for Pseudoinversion Based\n  Neural Training",
        "abstract": "  In this paper we consider the training of single hidden layer neural networks\nby pseudoinversion, which, in spite of its popularity, is sometimes affected by\nnumerical instability issues. Regularization is known to be effective in such\ncases, so that we introduce, in the framework of Tikhonov regularization, a\nmatricial reformulation of the problem which allows us to use the condition\nnumber as a diagnostic tool for identification of instability. By imposing\nwell-conditioning requirements on the relevant matrices, our theoretical\nanalysis allows the identification of an optimal value for the regularization\nparameter from the standpoint of stability. We compare with the value derived\nby cross-validation for overfitting control and optimisation of the\ngeneralization performance. We test our method for both regression and\nclassification tasks. The proposed method is quite effective in terms of\npredictivity, often with some improvement on performance with respect to the\nreference cases considered. This approach, due to analytical determination of\nthe regularization parameter, dramatically reduces the computational load\nrequired by many other techniques.\n",
        "published": "2015",
        "authors": [
            "Rossella Cancelliere",
            "Mario Gai",
            "Patrick Gallinari",
            "Luca Rubini"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1508.07096v1",
        "title": "Partitioning Large Scale Deep Belief Networks Using Dropout",
        "abstract": "  Deep learning methods have shown great promise in many practical\napplications, ranging from speech recognition, visual object recognition, to\ntext processing. However, most of the current deep learning methods suffer from\nscalability problems for large-scale applications, forcing researchers or users\nto focus on small-scale problems with fewer parameters.\n  In this paper, we consider a well-known machine learning model, deep belief\nnetworks (DBNs) that have yielded impressive classification performance on a\nlarge number of benchmark machine learning tasks. To scale up DBN, we propose\nan approach that can use the computing clusters in a distributed environment to\ntrain large models, while the dense matrix computations within a single machine\nare sped up using graphics processors (GPU). When training a DBN, each machine\nrandomly drops out a portion of neurons in each hidden layer, for each training\ncase, making the remaining neurons only learn to detect features that are\ngenerally helpful for producing the correct answer. Within our approach, we\nhave developed four methods to combine outcomes from each machine to form a\nunified model. Our preliminary experiment on the mnst handwritten digit\ndatabase demonstrates that our approach outperforms the state of the art test\nerror rate.\n",
        "published": "2015",
        "authors": [
            "Yanping Huang",
            "Sai Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1509.03475v2",
        "title": "Hessian-free Optimization for Learning Deep Multidimensional Recurrent\n  Neural Networks",
        "abstract": "  Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable\nperformance in the area of speech and handwriting recognition. The performance\nof an MDRNN is improved by further increasing its depth, and the difficulty of\nlearning the deeper network is overcome by using Hessian-free (HF)\noptimization. Given that connectionist temporal classification (CTC) is\nutilized as an objective of learning an MDRNN for sequence labeling, the\nnon-convexity of CTC poses a problem when applying HF to the network. As a\nsolution, a convex approximation of CTC is formulated and its relationship with\nthe EM algorithm and the Fisher information matrix is discussed. An MDRNN up to\na depth of 15 layers is successfully trained using HF, resulting in an improved\nperformance for sequence labeling.\n",
        "published": "2015",
        "authors": [
            "Minhyung Cho",
            "Chandra Shekhar Dhir",
            "Jaehyung Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1509.04210v3",
        "title": "Model Accuracy and Runtime Tradeoff in Distributed Deep Learning:A\n  Systematic Study",
        "abstract": "  This paper presents Rudra, a parameter server based distributed computing\nframework tuned for training large-scale deep neural networks. Using variants\nof the asynchronous stochastic gradient descent algorithm we study the impact\nof synchronization protocol, stale gradient updates, minibatch size, learning\nrates, and number of learners on runtime performance and model accuracy. We\nintroduce a new learning rate modulation strategy to counter the effect of\nstale gradients and propose a new synchronization protocol that can effectively\nbound the staleness in gradients, improve runtime performance and achieve good\nmodel accuracy. Our empirical investigation reveals a principled approach for\ndistributed training of neural networks: the mini-batch size per learner should\nbe reduced as more learners are added to the system to preserve the model\naccuracy. We validate this approach using commonly-used image classification\nbenchmarks: CIFAR10 and ImageNet.\n",
        "published": "2015",
        "authors": [
            "Suyog Gupta",
            "Wei Zhang",
            "Fei Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1509.05009v3",
        "title": "On the Expressive Power of Deep Learning: A Tensor Analysis",
        "abstract": "  It has long been conjectured that hypotheses spaces suitable for data that is\ncompositional in nature, such as text or images, may be more efficiently\nrepresented with deep hierarchical networks than with shallow ones. Despite the\nvast empirical evidence supporting this belief, theoretical justifications to\ndate are limited. In particular, they do not account for the locality, sharing\nand pooling constructs of convolutional networks, the most successful deep\nlearning architecture to date. In this work we derive a deep network\narchitecture based on arithmetic circuits that inherently employs locality,\nsharing and pooling. An equivalence between the networks and hierarchical\ntensor factorizations is established. We show that a shallow network\ncorresponds to CP (rank-1) decomposition, whereas a deep network corresponds to\nHierarchical Tucker decomposition. Using tools from measure theory and matrix\nalgebra, we prove that besides a negligible set, all functions that can be\nimplemented by a deep network of polynomial size, require exponential size in\norder to be realized (or even approximated) by a shallow network. Since\nlog-space computation transforms our networks into SimNets, the result applies\ndirectly to a deep learning architecture demonstrating promising empirical\nperformance. The construction and theory developed in this paper shed new light\non various practices and ideas employed by the deep learning community.\n",
        "published": "2015",
        "authors": [
            "Nadav Cohen",
            "Or Sharir",
            "Amnon Shashua"
        ]
    }
]