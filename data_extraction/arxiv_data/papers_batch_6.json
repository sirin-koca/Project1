[
    {
        "id": "http://arxiv.org/abs/1807.09936v1",
        "title": "Multi-Agent Generative Adversarial Imitation Learning",
        "abstract": "  Imitation learning algorithms can be used to learn a policy from expert\ndemonstrations without access to a reward signal. However, most existing\napproaches are not applicable in multi-agent settings due to the existence of\nmultiple (Nash) equilibria and non-stationary environments. We propose a new\nframework for multi-agent imitation learning for general Markov games, where we\nbuild upon a generalized notion of inverse reinforcement learning. We further\nintroduce a practical multi-agent actor-critic algorithm with good empirical\nperformance. Our method can be used to imitate complex behaviors in\nhigh-dimensional environments with multiple cooperative or competing agents.\n",
        "published": "2018-07-26",
        "authors": [
            "Jiaming Song",
            "Hongyu Ren",
            "Dorsa Sadigh",
            "Stefano Ermon"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.07029v1",
        "title": "Modelling the Dynamic Joint Policy of Teammates with Attention\n  Multi-agent DDPG",
        "abstract": "  Modelling and exploiting teammates' policies in cooperative multi-agent\nsystems have long been an interest and also a big challenge for the\nreinforcement learning (RL) community. The interest lies in the fact that if\nthe agent knows the teammates' policies, it can adjust its own policy\naccordingly to arrive at proper cooperations; while the challenge is that the\nagents' policies are changing continuously due to they are learning\nconcurrently, which imposes difficulty to model the dynamic policies of\nteammates accurately. In this paper, we present \\emph{ATTention Multi-Agent\nDeep Deterministic Policy Gradient} (ATT-MADDPG) to address this challenge.\nATT-MADDPG extends DDPG, a single-agent actor-critic RL method, with two\nspecial designs. First, in order to model the teammates' policies, the agent\nshould get access to the observations and actions of teammates. ATT-MADDPG\nadopts a centralized critic to collect such information. Second, to model the\nteammates' policies using the collected information in an effective way,\nATT-MADDPG enhances the centralized critic with an attention mechanism. This\nattention mechanism introduces a special structure to explicitly model the\ndynamic joint policy of teammates, making sure that the collected information\ncan be processed efficiently. We evaluate ATT-MADDPG on both benchmark tasks\nand the real-world packet routing tasks. Experimental results show that it not\nonly outperforms the state-of-the-art RL-based methods and rule-based methods\nby a large margin, but also achieves better performance in terms of scalability\nand robustness.\n",
        "published": "2018-11-13",
        "authors": [
            "Hangyu Mao",
            "Zhengchao Zhang",
            "Zhen Xiao",
            "Zhibo Gong"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.03887v3",
        "title": "Improving Coordination in Small-Scale Multi-Agent Deep Reinforcement\n  Learning through Memory-driven Communication",
        "abstract": "  Deep reinforcement learning algorithms have recently been used to train\nmultiple interacting agents in a centralised manner whilst keeping their\nexecution decentralised. When the agents can only acquire partial observations\nand are faced with tasks requiring coordination and synchronisation skills,\ninter-agent communication plays an essential role. In this work, we propose a\nframework for multi-agent training using deep deterministic policy gradients\nthat enables concurrent, end-to-end learning of an explicit communication\nprotocol through a memory device. During training, the agents learn to perform\nread and write operations enabling them to infer a shared representation of the\nworld. We empirically demonstrate that concurrent learning of the communication\ndevice and individual policies can improve inter-agent coordination and\nperformance in small-scale systems. Our experimental results show that the\nproposed method achieves superior performance in scenarios with up to six\nagents. We illustrate how different communication patterns can emerge on six\ndifferent tasks of increasing complexity. Furthermore, we study the effects of\ncorrupting the communication channel, provide a visualisation of the\ntime-varying memory content as the underlying task is being solved and validate\nthe building blocks of the proposed memory device through ablation studies.\n",
        "published": "2019-01-12",
        "authors": [
            "Emanuele Pesce",
            "Giovanni Montana"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.01465v2",
        "title": "Reducing Overestimation Bias in Multi-Agent Domains Using Double\n  Centralized Critics",
        "abstract": "  Many real world tasks require multiple agents to work together. Multi-agent\nreinforcement learning (RL) methods have been proposed in recent years to solve\nthese tasks, but current methods often fail to efficiently learn policies. We\nthus investigate the presence of a common weakness in single-agent RL, namely\nvalue function overestimation bias, in the multi-agent setting. Based on our\nfindings, we propose an approach that reduces this bias by using double\ncentralized critics. We evaluate it on six mixed cooperative-competitive tasks,\nshowing a significant advantage over current methods. Finally, we investigate\nthe application of multi-agent methods to high-dimensional robotic tasks and\nshow that our approach can be used to learn decentralized policies in this\ndomain.\n",
        "published": "2019-10-03",
        "authors": [
            "Johannes Ackermann",
            "Volker Gabler",
            "Takayuki Osa",
            "Masashi Sugiyama"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.14472v1",
        "title": "Learning Fairness in Multi-Agent Systems",
        "abstract": "  Fairness is essential for human society, contributing to stability and\nproductivity. Similarly, fairness is also the key for many multi-agent systems.\nTaking fairness into multi-agent learning could help multi-agent systems become\nboth efficient and stable. However, learning efficiency and fairness\nsimultaneously is a complex, multi-objective, joint-policy optimization. To\ntackle these difficulties, we propose FEN, a novel hierarchical reinforcement\nlearning model. We first decompose fairness for each agent and propose\nfair-efficient reward that each agent learns its own policy to optimize. To\navoid multi-objective conflict, we design a hierarchy consisting of a\ncontroller and several sub-policies, where the controller maximizes the\nfair-efficient reward by switching among the sub-policies that provides diverse\nbehaviors to interact with the environment. FEN can be trained in a fully\ndecentralized way, making it easy to be deployed in real-world applications.\nEmpirically, we show that FEN easily learns both fairness and efficiency and\nsignificantly outperforms baselines in a variety of multi-agent scenarios.\n",
        "published": "2019-10-31",
        "authors": [
            "Jiechuan Jiang",
            "Zongqing Lu"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.11145v2",
        "title": "F2A2: Flexible Fully-decentralized Approximate Actor-critic for\n  Cooperative Multi-agent Reinforcement Learning",
        "abstract": "  Traditional centralized multi-agent reinforcement learning (MARL) algorithms\nare sometimes unpractical in complicated applications, due to non-interactivity\nbetween agents, curse of dimensionality and computation complexity. Hence,\nseveral decentralized MARL algorithms are motivated. However, existing\ndecentralized methods only handle the fully cooperative setting where massive\ninformation needs to be transmitted in training. The block coordinate gradient\ndescent scheme they used for successive independent actor and critic steps can\nsimplify the calculation, but it causes serious bias. In this paper, we propose\na flexible fully decentralized actor-critic MARL framework, which can combine\nmost of actor-critic methods, and handle large-scale general cooperative\nmulti-agent setting. A primal-dual hybrid gradient descent type algorithm\nframework is designed to learn individual agents separately for\ndecentralization. From the perspective of each agent, policy improvement and\nvalue evaluation are jointly optimized, which can stabilize multi-agent policy\nlearning. Furthermore, our framework can achieve scalability and stability for\nlarge-scale environment and reduce information transmission, by the parameter\nsharing mechanism and a novel modeling-other-agents methods based on\ntheory-of-mind and online supervised learning. Sufficient experiments in\ncooperative Multi-agent Particle Environment and StarCraft II show that our\ndecentralized MARL instantiation algorithms perform competitively against\nconventional centralized and decentralized methods.\n",
        "published": "2020-04-17",
        "authors": [
            "Wenhao Li",
            "Bo Jin",
            "Xiangfeng Wang",
            "Junchi Yan",
            "Hongyuan Zha"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.01022v4",
        "title": "Health-Informed Policy Gradients for Multi-Agent Reinforcement Learning",
        "abstract": "  This paper proposes a definition of system health in the context of multiple\nagents optimizing a joint reward function. We use this definition as a credit\nassignment term in a policy gradient algorithm to distinguish the contributions\nof individual agents to the global reward. The health-informed credit\nassignment is then extended to a multi-agent variant of the proximal policy\noptimization algorithm and demonstrated on particle and multiwalker robot\nenvironments that have characteristics such as system health, risk-taking,\nsemi-expendable agents, continuous action spaces, and partial observability. We\nshow significant improvement in learning performance compared to policy\ngradient methods that do not perform multi-agent credit assignment.\n",
        "published": "2019-08-02",
        "authors": [
            "Ross E. Allen",
            "Jayesh K. Gupta",
            "Jaime Pena",
            "Yutai Zhou",
            "Javona White Bear",
            "Mykel J. Kochenderfer"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.04573v2",
        "title": "Competitive Multi-Agent Deep Reinforcement Learning with Counterfactual\n  Thinking",
        "abstract": "  Counterfactual thinking describes a psychological phenomenon that people\nre-infer the possible results with different solutions about things that have\nalready happened. It helps people to gain more experience from mistakes and\nthus to perform better in similar future tasks. This paper investigates the\ncounterfactual thinking for agents to find optimal decision-making strategies\nin multi-agent reinforcement learning environments. In particular, we propose a\nmulti-agent deep reinforcement learning model with a structure which mimics the\nhuman-psychological counterfactual thinking process to improve the competitive\nabilities for agents. To this end, our model generates several possible actions\n(intent actions) with a parallel policy structure and estimates the rewards and\nregrets for these intent actions based on its current understanding of the\nenvironment. Our model incorporates a scenario-based framework to link the\nestimated regrets with its inner policies. During the iterations, our model\nupdates the parallel policies and the corresponding scenario-based regrets for\nagents simultaneously. To verify the effectiveness of our proposed model, we\nconduct extensive experiments on two different environments with real-world\napplications. Experimental results show that counterfactual thinking can\nactually benefit the agents to obtain more accumulative rewards from the\nenvironments with fair information by comparing to their opponents while\nkeeping high performing efficiency.\n",
        "published": "2019-08-13",
        "authors": [
            "Yue Wang",
            "Yao Wan",
            "Chenwei Zhang",
            "Lixin Cui",
            "Lu Bai",
            "Philip S. Yu"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.07066v1",
        "title": "Prosocial or Selfish? Agents with different behaviors for Contract\n  Negotiation using Reinforcement Learning",
        "abstract": "  We present an effective technique for training deep learning agents capable\nof negotiating on a set of clauses in a contract agreement using a simple\ncommunication protocol. We use Multi Agent Reinforcement Learning to train both\nagents simultaneously as they negotiate with each other in the training\nenvironment. We also model selfish and prosocial behavior to varying degrees in\nthese agents. Empirical evidence is provided showing consistency in agent\nbehaviors. We further train a meta agent with a mixture of behaviors by\nlearning an ensemble of different models using reinforcement learning. Finally,\nto ascertain the deployability of the negotiating agents, we conducted\nexperiments pitting the trained agents against human players. Results\ndemonstrate that the agents are able to hold their own against human players,\noften emerging as winners in the negotiation. Our experiments demonstrate that\nthe meta agent is able to reasonably emulate human behavior.\n",
        "published": "2018-09-19",
        "authors": [
            "Vishal Sunder",
            "Lovekesh Vig",
            "Arnab Chatterjee",
            "Gautam Shroff"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.11044v1",
        "title": "Relational Forward Models for Multi-Agent Learning",
        "abstract": "  The behavioral dynamics of multi-agent systems have a rich and orderly\nstructure, which can be leveraged to understand these systems, and to improve\nhow artificial agents learn to operate in them. Here we introduce Relational\nForward Models (RFM) for multi-agent learning, networks that can learn to make\naccurate predictions of agents' future behavior in multi-agent environments.\nBecause these models operate on the discrete entities and relations present in\nthe environment, they produce interpretable intermediate representations which\noffer insights into what drives agents' behavior, and what events mediate the\nintensity and valence of social interactions. Furthermore, we show that\nembedding RFM modules inside agents results in faster learning systems compared\nto non-augmented baselines. As more and more of the autonomous systems we\ndevelop and interact with become multi-agent in nature, developing richer\nanalysis tools for characterizing how and why agents make decisions is\nincreasingly necessary. Moreover, developing artificial agents that quickly and\nsafely learn to coordinate with one another, and with humans in shared\nenvironments, is crucial.\n",
        "published": "2018-09-28",
        "authors": [
            "Andrea Tacchetti",
            "H. Francis Song",
            "Pedro A. M. Mediano",
            "Vinicius Zambaldi",
            "Neil C. Rabinowitz",
            "Thore Graepel",
            "Matthew Botvinick",
            "Peter W. Battaglia"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.02783v8",
        "title": "Finite-Sample Analysis For Decentralized Batch Multi-Agent Reinforcement\n  Learning With Networked Agents",
        "abstract": "  Despite the increasing interest in multi-agent reinforcement learning (MARL)\nin multiple communities, understanding its theoretical foundation has long been\nrecognized as a challenging problem. In this work, we address this problem by\nproviding a finite-sample analysis for decentralized batch MARL with networked\nagents. Specifically, we consider two decentralized MARL settings, where teams\nof agents are connected by time-varying communication networks, and either\ncollaborate or compete in a zero-sum game setting, without any central\ncontroller. These settings cover many conventional MARL settings in the\nliterature. For both settings, we develop batch MARL algorithms that can be\nimplemented in a decentralized fashion, and quantify the finite-sample errors\nof the estimated action-value functions. Our error analysis captures how the\nfunction class, the number of samples within each iteration, and the number of\niterations determine the statistical accuracy of the proposed algorithms. Our\nresults, compared to the finite-sample bounds for single-agent RL, involve\nadditional error terms caused by decentralized computation, which is inherent\nin our decentralized MARL setting. This work appears to be the first\nfinite-sample analysis for batch MARL, a step towards rigorous theoretical\nunderstanding of general MARL algorithms in the finite-sample regime.\n",
        "published": "2018-12-06",
        "authors": [
            "Kaiqing Zhang",
            "Zhuoran Yang",
            "Han Liu",
            "Tong Zhang",
            "Tamer Ba\u015far"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.09755v1",
        "title": "Learning when to Communicate at Scale in Multiagent Cooperative and\n  Competitive Tasks",
        "abstract": "  Learning when to communicate and doing that effectively is essential in\nmulti-agent tasks. Recent works show that continuous communication allows\nefficient training with back-propagation in multi-agent scenarios, but have\nbeen restricted to fully-cooperative tasks. In this paper, we present\nIndividualized Controlled Continuous Communication Model (IC3Net) which has\nbetter training efficiency than simple continuous communication model, and can\nbe applied to semi-cooperative and competitive settings along with the\ncooperative settings. IC3Net controls continuous communication with a gating\nmechanism and uses individualized rewards foreach agent to gain better\nperformance and scalability while fixing credit assignment issues. Using\nvariety of tasks including StarCraft BroodWars explore and combat scenarios, we\nshow that our network yields improved performance and convergence rates than\nthe baselines as the scale increases. Our results convey that IC3Net agents\nlearn when to communicate based on the scenario and profitability.\n",
        "published": "2018-12-23",
        "authors": [
            "Amanpreet Singh",
            "Tushar Jain",
            "Sainbayar Sukhbaatar"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.11794v2",
        "title": "Deep Reinforcement Learning for Multi-Agent Systems: A Review of\n  Challenges, Solutions and Applications",
        "abstract": "  Reinforcement learning (RL) algorithms have been around for decades and\nemployed to solve various sequential decision-making problems. These algorithms\nhowever have faced great challenges when dealing with high-dimensional\nenvironments. The recent development of deep learning has enabled RL methods to\ndrive optimal policies for sophisticated and capable agents, which can perform\nefficiently in these challenging environments. This paper addresses an\nimportant aspect of deep RL related to situations that require multiple agents\nto communicate and cooperate to solve complex tasks. A survey of different\napproaches to problems related to multi-agent deep RL (MADRL) is presented,\nincluding non-stationarity, partial observability, continuous state and action\nspaces, multi-agent training schemes, multi-agent transfer learning. The merits\nand demerits of the reviewed methods will be analyzed and discussed, with their\ncorresponding applications explored. It is envisaged that this review provides\ninsights about various MADRL methods and can lead to future development of more\nrobust and highly useful multi-agent learning methods for solving real-world\nproblems.\n",
        "published": "2018-12-31",
        "authors": [
            "Thanh Thi Nguyen",
            "Ngoc Duy Nguyen",
            "Saeid Nahavandi"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.01537v2",
        "title": "MGpi: A Computational Model of Multiagent Group Perception and\n  Interaction",
        "abstract": "  Toward enabling next-generation robots capable of socially intelligent\ninteraction with humans, we present a $\\mathbf{computational\\; model}$ of\ninteractions in a social environment of multiple agents and multiple groups.\nThe Multiagent Group Perception and Interaction (MGpi) network is a deep neural\nnetwork that predicts the appropriate social action to execute in a group\nconversation (e.g., speak, listen, respond, leave), taking into account\nneighbors' observable features (e.g., location of people, gaze orientation,\ndistraction, etc.). A central component of MGpi is the Kinesic-Proxemic-Message\n(KPM) gate, that performs social signal gating to extract important information\nfrom a group conversation. In particular, KPM gate filters incoming social cues\nfrom nearby agents by observing their body gestures (kinesics) and spatial\nbehavior (proxemics). The MGpi network and its KPM gate are learned via\nimitation learning, using demonstrations from our designed $\\mathbf{social\\;\ninteraction\\; simulator}$. Further, we demonstrate the efficacy of the KPM gate\nas a social attention mechanism, achieving state-of-the-art performance on the\ntask of $\\mathbf{group\\; identification}$ without using explicit group\nannotations, layout assumptions, or manually chosen parameters.\n",
        "published": "2019-03-04",
        "authors": [
            "Navyata Sanghvi",
            "Ryo Yonetani",
            "Kris Kitani"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.04959v1",
        "title": "Deep Multi-Agent Reinforcement Learning with Discrete-Continuous Hybrid\n  Action Spaces",
        "abstract": "  Deep Reinforcement Learning (DRL) has been applied to address a variety of\ncooperative multi-agent problems with either discrete action spaces or\ncontinuous action spaces. However, to the best of our knowledge, no previous\nwork has ever succeeded in applying DRL to multi-agent problems with\ndiscrete-continuous hybrid (or parameterized) action spaces which is very\ncommon in practice. Our work fills this gap by proposing two novel algorithms:\nDeep Multi-Agent Parameterized Q-Networks (Deep MAPQN) and Deep Multi-Agent\nHierarchical Hybrid Q-Networks (Deep MAHHQN). We follow the centralized\ntraining but decentralized execution paradigm: different levels of\ncommunication between different agents are used to facilitate the training\nprocess, while each agent executes its policy independently based on local\nobservations during execution. Our empirical results on several challenging\ntasks (simulated RoboCup Soccer and game Ghost Story) show that both Deep MAPQN\nand Deep MAHHQN are effective and significantly outperform existing independent\ndeep parameterized Q-learning method.\n",
        "published": "2019-03-12",
        "authors": [
            "Haotian Fu",
            "Hongyao Tang",
            "Jianye Hao",
            "Zihan Lei",
            "Yingfeng Chen",
            "Changjie Fan"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.00949v1",
        "title": "Multi-Agent Deep Reinforcement Learning with Adaptive Policies",
        "abstract": "  We propose a novel approach to address one aspect of the non-stationarity\nproblem in multi-agent reinforcement learning (RL), where the other agents may\nalter their policies due to environment changes during execution. This violates\nthe Markov assumption that governs most single-agent RL methods and is one of\nthe key challenges in multi-agent RL. To tackle this, we propose to train\nmultiple policies for each agent and postpone the selection of the best policy\nat execution time. Specifically, we model the environment non-stationarity with\na finite set of scenarios and train policies fitting each scenario. In addition\nto multiple policies, each agent also learns a policy predictor to determine\nwhich policy is the best with its local information. By doing so, each agent is\nable to adapt its policy when the environment changes and consequentially the\nother agents alter their policies during execution. We empirically evaluated\nour method on a variety of common benchmark problems proposed for multi-agent\ndeep RL in the literature. Our experimental results show that the agents\ntrained by our algorithm have better adaptiveness in changing environments and\noutperform the state-of-the-art methods in all the tested environments.\n",
        "published": "2019-11-28",
        "authors": [
            "Yixiang Wang",
            "Feng Wu"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.07527v1",
        "title": "Model-based Multi-Agent Reinforcement Learning with Cooperative\n  Prioritized Sweeping",
        "abstract": "  We present a new model-based reinforcement learning algorithm, Cooperative\nPrioritized Sweeping, for efficient learning in multi-agent Markov decision\nprocesses. The algorithm allows for sample-efficient learning on large problems\nby exploiting a factorization to approximate the value function. Our approach\nonly requires knowledge about the structure of the problem in the form of a\ndynamic decision network. Using this information, our method learns a model of\nthe environment and performs temporal difference updates which affect multiple\njoint states and actions at once. Batch updates are additionally performed\nwhich efficiently back-propagate knowledge throughout the factored Q-function.\nOur method outperforms the state-of-the-art algorithm sparse cooperative\nQ-learning algorithm, both on the well-known SysAdmin benchmark and randomized\nenvironments.\n",
        "published": "2020-01-15",
        "authors": [
            "Eugenio Bargiacchi",
            "Timothy Verstraeten",
            "Diederik M. Roijers",
            "Ann Now\u00e9"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.12004v2",
        "title": "Neural MMO v1.3: A Massively Multiagent Game Environment for Training\n  and Evaluating Neural Networks",
        "abstract": "  Progress in multiagent intelligence research is fundamentally limited by the\nnumber and quality of environments available for study. In recent years,\nsimulated games have become a dominant research platform within reinforcement\nlearning, in part due to their accessibility and interpretability. Previous\nworks have targeted and demonstrated success on arcade, first person shooter\n(FPS), real-time strategy (RTS), and massive online battle arena (MOBA) games.\nOur work considers massively multiplayer online role-playing games (MMORPGs or\nMMOs), which capture several complexities of real-world learning that are not\nwell modeled by any other game genre. We present Neural MMO, a massively\nmultiagent game environment inspired by MMOs and discuss our progress on two\nmore general challenges in multiagent systems engineering for AI research:\ndistributed infrastructure and game IO. We further demonstrate that standard\npolicy gradient methods and simple baseline models can learn interesting\nemergent exploration and specialization behaviors in this setting.\n",
        "published": "2020-01-31",
        "authors": [
            "Joseph Suarez",
            "Yilun Du",
            "Igor Mordatch",
            "Phillip Isola"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.08353v2",
        "title": "A Deep Multi-Agent Reinforcement Learning Approach to Autonomous\n  Separation Assurance",
        "abstract": "  A novel deep multi-agent reinforcement learning framework is proposed to\nidentify and resolve conflicts among a variable number of aircraft in a\nhigh-density, stochastic, and dynamic sector. Currently the sector capacity is\nconstrained by human air traffic controller's cognitive limitation. We\ninvestigate the feasibility of a new concept (autonomous separation assurance)\nand a new approach to push the sector capacity above human cognitive\nlimitation. We propose the concept of using distributed vehicle autonomy to\nensure separation, instead of a centralized sector air traffic controller. Our\nproposed framework utilizes Proximal Policy Optimization (PPO) that we modify\nto incorporate an attention network. This allows the agents to have access to\nvariable aircraft information in the sector in a scalable, efficient approach\nto achieve high traffic throughput under uncertainty. Agents are trained using\na centralized learning, decentralized execution scheme where one neural network\nis learned and shared by all agents. The proposed framework is validated on\nthree challenging case studies in the BlueSky air traffic control environment.\nNumerical results show the proposed framework significantly reduces offline\ntraining time, increases performance, and results in a more efficient policy.\n",
        "published": "2020-03-17",
        "authors": [
            "Marc Brittain",
            "Xuxi Yang",
            "Peng Wei"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.10903v2",
        "title": "Distributional Reinforcement Learning with Ensembles",
        "abstract": "  It is well known that ensemble methods often provide enhanced performance in\nreinforcement learning. In this paper, we explore this concept further by using\ngroup-aided training within the distributional reinforcement learning paradigm.\nSpecifically, we propose an extension to categorical reinforcement learning,\nwhere distributional learning targets are implicitly based on the total\ninformation gathered by an ensemble. We empirically show that this may lead to\nmuch more robust initial learning, a stronger individual performance level, and\ngood efficiency on a per-sample basis.\n",
        "published": "2020-03-24",
        "authors": [
            "Bj\u00f6rn Lindenberg",
            "Jonas Nordqvist",
            "Karl-Olof Lindahl"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.00587v5",
        "title": "Towards Understanding Cooperative Multi-Agent Q-Learning with Value\n  Factorization",
        "abstract": "  Value factorization is a popular and promising approach to scaling up\nmulti-agent reinforcement learning in cooperative settings, which balances the\nlearning scalability and the representational capacity of value functions.\nHowever, the theoretical understanding of such methods is limited. In this\npaper, we formalize a multi-agent fitted Q-iteration framework for analyzing\nfactorized multi-agent Q-learning. Based on this framework, we investigate\nlinear value factorization and reveal that multi-agent Q-learning with this\nsimple decomposition implicitly realizes a powerful counterfactual credit\nassignment, but may not converge in some settings. Through further analysis, we\nfind that on-policy training or richer joint value function classes can improve\nits local or global convergence properties, respectively. Finally, to support\nour theoretical implications in practical realization, we conduct an empirical\nanalysis of state-of-the-art deep multi-agent Q-learning algorithms on didactic\nexamples and a broad set of StarCraft II unit micromanagement tasks.\n",
        "published": "2020-05-31",
        "authors": [
            "Jianhao Wang",
            "Zhizhou Ren",
            "Beining Han",
            "Jianing Ye",
            "Chongjie Zhang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.04037v1",
        "title": "Reinforcement Learning for Multi-Product Multi-Node Inventory Management\n  in Supply Chains",
        "abstract": "  This paper describes the application of reinforcement learning (RL) to\nmulti-product inventory management in supply chains. The problem description\nand solution are both adapted from a real-world business solution. The novelty\nof this problem with respect to supply chain literature is (i) we consider\nconcurrent inventory management of a large number (50 to 1000) of products with\nshared capacity, (ii) we consider a multi-node supply chain consisting of a\nwarehouse which supplies three stores, (iii) the warehouse, stores, and\ntransportation from warehouse to stores have finite capacities, (iv) warehouse\nand store replenishment happen at different time scales and with realistic time\nlags, and (v) demand for products at the stores is stochastic. We describe a\nnovel formulation in a multi-agent (hierarchical) reinforcement learning\nframework that can be used for parallelised decision-making, and use the\nadvantage actor critic (A2C) algorithm with quantised action spaces to solve\nthe problem. Experiments show that the proposed approach is able to handle a\nmulti-objective reward comprised of maximising product sales and minimising\nwastage of perishable products.\n",
        "published": "2020-06-07",
        "authors": [
            "Nazneen N Sultana",
            "Hardik Meisheri",
            "Vinita Baniwal",
            "Somjit Nath",
            "Balaraman Ravindran",
            "Harshad Khadilkar"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.04222v3",
        "title": "Randomized Entity-wise Factorization for Multi-Agent Reinforcement\n  Learning",
        "abstract": "  Multi-agent settings in the real world often involve tasks with varying types\nand quantities of agents and non-agent entities; however, common patterns of\nbehavior often emerge among these agents/entities. Our method aims to leverage\nthese commonalities by asking the question: ``What is the expected utility of\neach agent when only considering a randomly selected sub-group of its observed\nentities?'' By posing this counterfactual question, we can recognize\nstate-action trajectories within sub-groups of entities that we may have\nencountered in another task and use what we learned in that task to inform our\nprediction in the current one. We then reconstruct a prediction of the full\nreturns as a combination of factors considering these disjoint groups of\nentities and train this ``randomly factorized\" value function as an auxiliary\nobjective for value-based multi-agent reinforcement learning. By doing so, our\nmodel can recognize and leverage similarities across tasks to improve learning\nefficiency in a multi-task setting. Our approach, Randomized Entity-wise\nFactorization for Imagined Learning (REFIL), outperforms all strong baselines\nby a significant margin in challenging multi-task StarCraft micromanagement\nsettings.\n",
        "published": "2020-06-07",
        "authors": [
            "Shariq Iqbal",
            "Christian A. Schroeder de Witt",
            "Bei Peng",
            "Wendelin B\u00f6hmer",
            "Shimon Whiteson",
            "Fei Sha"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.05842v2",
        "title": "The Emergence of Individuality",
        "abstract": "  Individuality is essential in human society, which induces the division of\nlabor and thus improves the efficiency and productivity. Similarly, it should\nalso be the key to multi-agent cooperation. Inspired by that individuality is\nof being an individual separate from others, we propose a simple yet efficient\nmethod for the emergence of individuality (EOI) in multi-agent reinforcement\nlearning (MARL). EOI learns a probabilistic classifier that predicts a\nprobability distribution over agents given their observation and gives each\nagent an intrinsic reward of being correctly predicted by the classifier. The\nintrinsic reward encourages the agents to visit their own familiar\nobservations, and learning the classifier by such observations makes the\nintrinsic reward signals stronger and the agents more identifiable. To further\nenhance the intrinsic reward and promote the emergence of individuality, two\nregularizers are proposed to increase the discriminability of the classifier.\nWe implement EOI on top of popular MARL algorithms. Empirically, we show that\nEOI significantly outperforms existing methods in a variety of multi-agent\ncooperative scenarios.\n",
        "published": "2020-06-10",
        "authors": [
            "Jiechuan Jiang",
            "Zongqing Lu"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.07869v4",
        "title": "Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in\n  Cooperative Tasks",
        "abstract": "  Multi-agent deep reinforcement learning (MARL) suffers from a lack of\ncommonly-used evaluation tasks and criteria, making comparisons between\napproaches difficult. In this work, we provide a systematic evaluation and\ncomparison of three different classes of MARL algorithms (independent learning,\ncentralised multi-agent policy gradient, value decomposition) in a diverse\nrange of cooperative multi-agent learning tasks. Our experiments serve as a\nreference for the expected performance of algorithms across different learning\ntasks, and we provide insights regarding the effectiveness of different\nlearning approaches. We open-source EPyMARL, which extends the PyMARL codebase\nto include additional algorithms and allow for flexible configuration of\nalgorithm implementation details such as parameter sharing. Finally, we\nopen-source two environments for multi-agent research which focus on\ncoordination under sparse rewards.\n",
        "published": "2020-06-14",
        "authors": [
            "Georgios Papoudakis",
            "Filippos Christianos",
            "Lukas Sch\u00e4fer",
            "Stefano V. Albrecht"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.01062v3",
        "title": "QPLEX: Duplex Dueling Multi-Agent Q-Learning",
        "abstract": "  We explore value-based multi-agent reinforcement learning (MARL) in the\npopular paradigm of centralized training with decentralized execution (CTDE).\nCTDE has an important concept, Individual-Global-Max (IGM) principle, which\nrequires the consistency between joint and local action selections to support\nefficient local decision-making. However, in order to achieve scalability,\nexisting MARL methods either limit representation expressiveness of their value\nfunction classes or relax the IGM consistency, which may suffer from\ninstability risk or may not perform well in complex domains. This paper\npresents a novel MARL approach, called duPLEX dueling multi-agent Q-learning\n(QPLEX), which takes a duplex dueling network architecture to factorize the\njoint value function. This duplex dueling structure encodes the IGM principle\ninto the neural network architecture and thus enables efficient value function\nlearning. Theoretical analysis shows that QPLEX achieves a complete IGM\nfunction class. Empirical experiments on StarCraft II micromanagement tasks\ndemonstrate that QPLEX significantly outperforms state-of-the-art baselines in\nboth online and offline data collection settings, and also reveal that QPLEX\nachieves high sample efficiency and can benefit from offline datasets without\nadditional online exploration.\n",
        "published": "2020-08-03",
        "authors": [
            "Jianhao Wang",
            "Zhizhou Ren",
            "Terry Liu",
            "Yang Yu",
            "Chongjie Zhang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.05214v2",
        "title": "REMAX: Relational Representation for Multi-Agent Exploration",
        "abstract": "  Training a multi-agent reinforcement learning (MARL) model with a sparse\nreward is generally difficult because numerous combinations of interactions\namong agents induce a certain outcome (i.e., success or failure). Earlier\nstudies have tried to resolve this issue by employing an intrinsic reward to\ninduce interactions that are helpful for learning an effective policy. However,\nthis approach requires extensive prior knowledge for designing an intrinsic\nreward. To train the MARL model effectively without designing the intrinsic\nreward, we propose a learning-based exploration strategy to generate the\ninitial states of a game. The proposed method adopts a variational graph\nautoencoder to represent a game state such that (1) the state can be compactly\nencoded to a latent representation by considering relationships among agents,\nand (2) the latent representation can be used as an effective input for a\ncoupled surrogate model to predict an exploration score. The proposed method\nthen finds new latent representations that maximize the exploration scores and\ndecodes these representations to generate initial states from which the MARL\nmodel starts training in the game and thus experiences novel and rewardable\nstates. We demonstrate that our method improves the training and performance of\nthe MARL model more than the existing exploration methods.\n",
        "published": "2020-08-12",
        "authors": [
            "Heechang Ryu",
            "Hayong Shin",
            "Jinkyoo Park"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.00581v3",
        "title": "Emergent Social Learning via Multi-agent Reinforcement Learning",
        "abstract": "  Social learning is a key component of human and animal intelligence. By\ntaking cues from the behavior of experts in their environment, social learners\ncan acquire sophisticated behavior and rapidly adapt to new circumstances. This\npaper investigates whether independent reinforcement learning (RL) agents in a\nmulti-agent environment can learn to use social learning to improve their\nperformance. We find that in most circumstances, vanilla model-free RL agents\ndo not use social learning. We analyze the reasons for this deficiency, and\nshow that by imposing constraints on the training environment and introducing a\nmodel-based auxiliary loss we are able to obtain generalized social learning\npolicies which enable agents to: i) discover complex skills that are not\nlearned from single-agent training, and ii) adapt online to novel environments\nby taking cues from experts present in the new environment. In contrast, agents\ntrained with model-free RL or imitation learning generalize poorly and do not\nsucceed in the transfer tasks. By mixing multi-agent and solo training, we can\nobtain agents that use social learning to gain skills that they can deploy when\nalone, even out-performing agents trained alone from the start.\n",
        "published": "2020-10-01",
        "authors": [
            "Kamal Ndousse",
            "Douglas Eck",
            "Sergey Levine",
            "Natasha Jaques"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.08531v1",
        "title": "Multi-Agent Collaboration via Reward Attribution Decomposition",
        "abstract": "  Recent advances in multi-agent reinforcement learning (MARL) have achieved\nsuper-human performance in games like Quake 3 and Dota 2. Unfortunately, these\ntechniques require orders-of-magnitude more training rounds than humans and\ndon't generalize to new agent configurations even on the same game. In this\nwork, we propose Collaborative Q-learning (CollaQ) that achieves\nstate-of-the-art performance in the StarCraft multi-agent challenge and\nsupports ad hoc team play. We first formulate multi-agent collaboration as a\njoint optimization on reward assignment and show that each agent has an\napproximately optimal policy that decomposes into two parts: one part that only\nrelies on the agent's own state, and the other part that is related to states\nof nearby agents. Following this novel finding, CollaQ decomposes the\nQ-function of each agent into a self term and an interactive term, with a\nMulti-Agent Reward Attribution (MARA) loss that regularizes the training.\nCollaQ is evaluated on various StarCraft maps and shows that it outperforms\nexisting state-of-the-art techniques (i.e., QMIX, QTRAN, and VDN) by improving\nthe win rate by 40% with the same number of samples. In the more challenging ad\nhoc team play setting (i.e., reweight/add/remove units without re-training or\nfinetuning), CollaQ outperforms previous SoTA by over 30%.\n",
        "published": "2020-10-16",
        "authors": [
            "Tianjun Zhang",
            "Huazhe Xu",
            "Xiaolong Wang",
            "Yi Wu",
            "Kurt Keutzer",
            "Joseph E. Gonzalez",
            "Yuandong Tian"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1704.02882v2",
        "title": "Dynamic Safe Interruptibility for Decentralized Multi-Agent\n  Reinforcement Learning",
        "abstract": "  In reinforcement learning, agents learn by performing actions and observing\ntheir outcomes. Sometimes, it is desirable for a human operator to\n\\textit{interrupt} an agent in order to prevent dangerous situations from\nhappening. Yet, as part of their learning process, agents may link these\ninterruptions, that impact their reward, to specific states and deliberately\navoid them. The situation is particularly challenging in a multi-agent context\nbecause agents might not only learn from their own past interruptions, but also\nfrom those of other agents. Orseau and Armstrong defined \\emph{safe\ninterruptibility} for one learner, but their work does not naturally extend to\nmulti-agent systems. This paper introduces \\textit{dynamic safe\ninterruptibility}, an alternative definition more suited to decentralized\nlearning problems, and studies this notion in two learning frameworks:\n\\textit{joint action learners} and \\textit{independent learners}. We give\nrealistic sufficient conditions on the learning algorithm to enable dynamic\nsafe interruptibility in the case of joint action learners, yet show that these\nconditions are not sufficient for independent learners. We show however that if\nagents can detect interruptions, it is possible to prune the observations to\nensure dynamic safe interruptibility even for independent learners.\n",
        "published": "2017-04-10",
        "authors": [
            "El Mahdi El Mhamdi",
            "Rachid Guerraoui",
            "Hadrien Hendrikx",
            "Alexandre Maurer"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.02698v1",
        "title": "Hierarchical Modular Reinforcement Learning Method and Knowledge\n  Acquisition of State-Action Rule for Multi-target Problem",
        "abstract": "  Hierarchical Modular Reinforcement Learning (HMRL), consists of 2 layered\nlearning where Profit Sharing works to plan a prey position in the higher layer\nand Q-learning method trains the state-actions to the target in the lower\nlayer. In this paper, we expanded HMRL to multi-target problem to take the\ndistance between targets to the consideration. The function, called `AT field',\ncan estimate the interests for an agent according to the distance between 2\nagents and the advantage/disadvantage of the other agent. Moreover, the\nknowledge related to state-action rules is extracted by C4.5. The action under\nthe situation is decided by using the acquired knowledge. To verify the\neffectiveness of proposed method, some experimental results are reported.\n",
        "published": "2018-04-08",
        "authors": [
            "Takumi Ichimura",
            "Daisuke Igaue"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.08776v1",
        "title": "Scalable Centralized Deep Multi-Agent Reinforcement Learning via Policy\n  Gradients",
        "abstract": "  In this paper, we explore using deep reinforcement learning for problems with\nmultiple agents. Most existing methods for deep multi-agent reinforcement\nlearning consider only a small number of agents. When the number of agents\nincreases, the dimensionality of the input and control spaces increase as well,\nand these methods do not scale well. To address this, we propose casting the\nmulti-agent reinforcement learning problem as a distributed optimization\nproblem. Our algorithm assumes that for multi-agent settings, policies of\nindividual agents in a given population live close to each other in parameter\nspace and can be approximated by a single policy. With this simple assumption,\nwe show our algorithm to be extremely effective for reinforcement learning in\nmulti-agent settings. We demonstrate its effectiveness against existing\ncomparable approaches on co-operative and competitive tasks.\n",
        "published": "2018-05-22",
        "authors": [
            "Arbaaz Khan",
            "Clark Zhang",
            "Daniel D. Lee",
            "Vijay Kumar",
            "Alejandro Ribeiro"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.00147v3",
        "title": "M$^3$RL: Mind-aware Multi-agent Management Reinforcement Learning",
        "abstract": "  Most of the prior work on multi-agent reinforcement learning (MARL) achieves\noptimal collaboration by directly controlling the agents to maximize a common\nreward. In this paper, we aim to address this from a different angle. In\nparticular, we consider scenarios where there are self-interested agents (i.e.,\nworker agents) which have their own minds (preferences, intentions, skills,\netc.) and can not be dictated to perform tasks they do not wish to do. For\nachieving optimal coordination among these agents, we train a super agent\n(i.e., the manager) to manage them by first inferring their minds based on both\ncurrent and past observations and then initiating contracts to assign suitable\ntasks to workers and promise to reward them with corresponding bonuses so that\nthey will agree to work together. The objective of the manager is maximizing\nthe overall productivity as well as minimizing payments made to the workers for\nad-hoc worker teaming. To train the manager, we propose Mind-aware Multi-agent\nManagement Reinforcement Learning (M^3RL), which consists of agent modeling and\npolicy learning. We have evaluated our approach in two environments, Resource\nCollection and Crafting, to simulate multi-agent management problems with\nvarious task settings and multiple designs for the worker agents. The\nexperimental results have validated the effectiveness of our approach in\nmodeling worker agents' minds online, and in achieving optimal ad-hoc teaming\nwith good generalization and fast adaptation.\n",
        "published": "2018-09-29",
        "authors": [
            "Tianmin Shu",
            "Yuandong Tian"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.00510v1",
        "title": "Interactive Agent Modeling by Learning to Probe",
        "abstract": "  The ability of modeling the other agents, such as understanding their\nintentions and skills, is essential to an agent's interactions with other\nagents. Conventional agent modeling relies on passive observation from\ndemonstrations. In this work, we propose an interactive agent modeling scheme\nenabled by encouraging an agent to learn to probe. In particular, the probing\nagent (i.e. a learner) learns to interact with the environment and with a\ntarget agent (i.e., a demonstrator) to maximize the change in the observed\nbehaviors of that agent. Through probing, rich behaviors can be observed and\nare used for enhancing the agent modeling to learn a more accurate mind model\nof the target agent. Our framework consists of two learning processes: i)\nimitation learning for an approximated agent model and ii) pure\ncuriosity-driven reinforcement learning for an efficient probing policy to\ndiscover new behaviors that otherwise can not be observed. We have validated\nour approach in four different tasks. The experimental results suggest that the\nagent model learned by our approach i) generalizes better in novel scenarios\nthan the ones learned by passive observation, random probing, and other\ncuriosity-driven approaches do, and ii) can be used for enhancing performance\nin multiple applications including distilling optimal planning to a policy net,\ncollaboration, and competition. A video demo is available at\nhttps://www.dropbox.com/s/8mz6rd3349tso67/Probing_Demo.mov?dl=0\n",
        "published": "2018-10-01",
        "authors": [
            "Tianmin Shu",
            "Caiming Xiong",
            "Ying Nian Wu",
            "Song-Chun Zhu"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.02912v2",
        "title": "Actor-Attention-Critic for Multi-Agent Reinforcement Learning",
        "abstract": "  Reinforcement learning in multi-agent scenarios is important for real-world\napplications but presents challenges beyond those seen in single-agent\nsettings. We present an actor-critic algorithm that trains decentralized\npolicies in multi-agent settings, using centrally computed critics that share\nan attention mechanism which selects relevant information for each agent at\nevery timestep. This attention mechanism enables more effective and scalable\nlearning in complex multi-agent environments, when compared to recent\napproaches. Our approach is applicable not only to cooperative settings with\nshared rewards, but also individualized reward settings, including adversarial\nsettings, as well as settings that do not provide global states, and it makes\nno assumptions about the action spaces of the agents. As such, it is flexible\nenough to be applied to most multi-agent learning problems.\n",
        "published": "2018-10-05",
        "authors": [
            "Shariq Iqbal",
            "Fei Sha"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.03679v2",
        "title": "Multi-agent Deep Reinforcement Learning for Zero Energy Communities",
        "abstract": "  Advances in renewable energy generation and introduction of the government\ntargets to improve energy efficiency gave rise to a concept of a Zero Energy\nBuilding (ZEB). A ZEB is a building whose net energy usage over a year is zero,\ni.e., its energy use is not larger than its overall renewables generation. A\ncollection of ZEBs forms a Zero Energy Community (ZEC). This paper addresses\nthe problem of energy sharing in such a community. This is different from\npreviously addressed energy sharing between buildings as our focus is on the\nimprovement of community energy status, while traditionally research focused on\nreducing losses due to transmission and storage, or achieving economic gains.\nWe model this problem in a multi-agent environment and propose a Deep\nReinforcement Learning (DRL) based solution. Each building is represented by an\nintelligent agent that learns over time the appropriate behaviour to share\nenergy. We have evaluated the proposed solution in a multi-agent simulation\nbuilt using osBrain. Results indicate that with time agents learn to\ncollaborate and learn a policy comparable to the optimal policy, which in turn\nimproves the ZEC's energy status. Buildings with no renewables preferred to\nrequest energy from their neighbours rather than from the supply grid.\n",
        "published": "2018-10-08",
        "authors": [
            "Amit Prasad",
            "Ivana Dusparic"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.07254v1",
        "title": "The Concept of Criticality in Reinforcement Learning",
        "abstract": "  Reinforcement learning methods carry a well known bias-variance trade-off in\nn-step algorithms for optimal control. Unfortunately, this has rarely been\naddressed in current research. This trade-off principle holds independent of\nthe choice of the algorithm, such as n-step SARSA, n-step Expected SARSA or\nn-step Tree backup. A small n results in a large bias, while a large n leads to\nlarge variance. The literature offers no straightforward recipe for the best\nchoice of this value. While currently all n-step algorithms use a fixed value\nof n over the state space we extend the framework of n-step updates by allowing\neach state to have its specific n.\n  We propose a solution to this problem within the context of human aided\nreinforcement learning. Our approach is based on the observation that a human\ncan learn more efficiently if she receives input regarding the criticality of a\ngiven state and thus the amount of attention she needs to invest into the\nlearning in that state. This observation is related to the idea that each state\nof the MDP has a certain measure of criticality which indicates how much the\nchoice of the action in that state influences the return. In our algorithm the\nRL agent utilizes the criticality measure, a function provided by a human\ntrainer, in order to locally choose the best stepnumber n for the update of the\nQ function.\n",
        "published": "2018-10-16",
        "authors": [
            "Yitzhak Spielberg",
            "Amos Azaria"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.08647v4",
        "title": "Social Influence as Intrinsic Motivation for Multi-Agent Deep\n  Reinforcement Learning",
        "abstract": "  We propose a unified mechanism for achieving coordination and communication\nin Multi-Agent Reinforcement Learning (MARL), through rewarding agents for\nhaving causal influence over other agents' actions. Causal influence is\nassessed using counterfactual reasoning. At each timestep, an agent simulates\nalternate actions that it could have taken, and computes their effect on the\nbehavior of other agents. Actions that lead to bigger changes in other agents'\nbehavior are considered influential and are rewarded. We show that this is\nequivalent to rewarding agents for having high mutual information between their\nactions. Empirical results demonstrate that influence leads to enhanced\ncoordination and communication in challenging social dilemma environments,\ndramatically increasing the learning curves of the deep RL agents, and leading\nto more meaningful learned communication protocols. The influence rewards for\nall agents can be computed in a decentralized way by enabling agents to learn a\nmodel of other agents using deep neural networks. In contrast, key previous\nworks on emergent communication in the MARL setting were unable to learn\ndiverse policies in a decentralized manner and had to resort to centralized\ntraining. Consequently, the influence reward opens up a window of new\nopportunities for research in this area.\n",
        "published": "2018-10-19",
        "authors": [
            "Natasha Jaques",
            "Angeliki Lazaridou",
            "Edward Hughes",
            "Caglar Gulcehre",
            "Pedro A. Ortega",
            "DJ Strouse",
            "Joel Z. Leibo",
            "Nando de Freitas"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.09202v5",
        "title": "Graph Convolutional Reinforcement Learning",
        "abstract": "  Learning to cooperate is crucially important in multi-agent environments. The\nkey is to understand the mutual interplay between agents. However, multi-agent\nenvironments are highly dynamic, where agents keep moving and their neighbors\nchange quickly. This makes it hard to learn abstract representations of mutual\ninterplay between agents. To tackle these difficulties, we propose graph\nconvolutional reinforcement learning, where graph convolution adapts to the\ndynamics of the underlying graph of the multi-agent environment, and relation\nkernels capture the interplay between agents by their relation representations.\nLatent features produced by convolutional layers from gradually increased\nreceptive fields are exploited to learn cooperation, and cooperation is further\nimproved by temporal relation regularization for consistency. Empirically, we\nshow that our method substantially outperforms existing methods in a variety of\ncooperative scenarios.\n",
        "published": "2018-10-22",
        "authors": [
            "Jiechuan Jiang",
            "Chen Dun",
            "Tiejun Huang",
            "Zongqing Lu"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.11187v2",
        "title": "TarMAC: Targeted Multi-Agent Communication",
        "abstract": "  We propose a targeted communication architecture for multi-agent\nreinforcement learning, where agents learn both what messages to send and whom\nto address them to while performing cooperative tasks in partially-observable\nenvironments. This targeting behavior is learnt solely from downstream\ntask-specific reward without any communication supervision. We additionally\naugment this with a multi-round communication approach where agents coordinate\nvia multiple rounds of communication before taking actions in the environment.\nWe evaluate our approach on a diverse set of cooperative multi-agent tasks, of\nvarying difficulties, with varying number of agents, in a variety of\nenvironments ranging from 2D grid layouts of shapes and simulated traffic\njunctions to 3D indoor environments, and demonstrate the benefits of targeted\nand multi-round communication. Moreover, we show that the targeted\ncommunication strategies learned by agents are interpretable and intuitive.\nFinally, we show that our architecture can be easily extended to mixed and\ncompetitive environments, leading to improved performance and sample complexity\nover recent state-of-the-art approaches.\n",
        "published": "2018-10-26",
        "authors": [
            "Abhishek Das",
            "Th\u00e9ophile Gervet",
            "Joshua Romoff",
            "Dhruv Batra",
            "Devi Parikh",
            "Michael Rabbat",
            "Joelle Pineau"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.01303v1",
        "title": "Autonomous Air Traffic Controller: A Deep Multi-Agent Reinforcement\n  Learning Approach",
        "abstract": "  Air traffic control is a real-time safety-critical decision making process in\nhighly dynamic and stochastic environments. In today's aviation practice, a\nhuman air traffic controller monitors and directs many aircraft flying through\nits designated airspace sector. With the fast growing air traffic complexity in\ntraditional (commercial airliners) and low-altitude (drones and eVTOL aircraft)\nairspace, an autonomous air traffic control system is needed to accommodate\nhigh density air traffic and ensure safe separation between aircraft. We\npropose a deep multi-agent reinforcement learning framework that is able to\nidentify and resolve conflicts between aircraft in a high-density, stochastic,\nand dynamic en-route sector with multiple intersections and merging points. The\nproposed framework utilizes an actor-critic model, A2C that incorporates the\nloss function from Proximal Policy Optimization (PPO) to help stabilize the\nlearning process. In addition we use a centralized learning, decentralized\nexecution scheme where one neural network is learned and shared by all agents\nin the environment. We show that our framework is both scalable and efficient\nfor large number of incoming aircraft to achieve extremely high traffic\nthroughput with safety guarantee. We evaluate our model via extensive\nsimulations in the BlueSky environment. Results show that our framework is able\nto resolve 99.97% and 100% of all conflicts both at intersections and merging\npoints, respectively, in extreme high-density air traffic scenarios.\n",
        "published": "2019-05-02",
        "authors": [
            "Marc Brittain",
            "Peng Wei"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.05408v1",
        "title": "QTRAN: Learning to Factorize with Transformation for Cooperative\n  Multi-Agent Reinforcement Learning",
        "abstract": "  We explore value-based solutions for multi-agent reinforcement learning\n(MARL) tasks in the centralized training with decentralized execution (CTDE)\nregime popularized recently. However, VDN and QMIX are representative examples\nthat use the idea of factorization of the joint action-value function into\nindividual ones for decentralized execution. VDN and QMIX address only a\nfraction of factorizable MARL tasks due to their structural constraint in\nfactorization such as additivity and monotonicity. In this paper, we propose a\nnew factorization method for MARL, QTRAN, which is free from such structural\nconstraints and takes on a new approach to transforming the original joint\naction-value function into an easily factorizable one, with the same optimal\nactions. QTRAN guarantees more general factorization than VDN or QMIX, thus\ncovering a much wider class of MARL tasks than does previous methods. Our\nexperiments for the tasks of multi-domain Gaussian-squeeze and modified\npredator-prey demonstrate QTRAN's superior performance with especially larger\nmargins in games whose payoffs penalize non-cooperative behavior more\naggressively.\n",
        "published": "2019-05-14",
        "authors": [
            "Kyunghwan Son",
            "Daewoo Kim",
            "Wan Ju Kang",
            "David Earl Hostallero",
            "Yung Yi"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.12127v3",
        "title": "Coordinated Exploration via Intrinsic Rewards for Multi-Agent\n  Reinforcement Learning",
        "abstract": "  Solving tasks with sparse rewards is one of the most important challenges in\nreinforcement learning. In the single-agent setting, this challenge is\naddressed by introducing intrinsic rewards that motivate agents to explore\nunseen regions of their state spaces; however, applying these techniques\nnaively to the multi-agent setting results in agents exploring independently,\nwithout any coordination among themselves. Exploration in cooperative\nmulti-agent settings can be accelerated and improved if agents coordinate their\nexploration. In this paper we introduce a framework for designing intrinsic\nrewards which consider what other agents have explored such that the agents can\ncoordinate. Then, we develop an approach for learning how to dynamically select\nbetween several exploration modalities to maximize extrinsic rewards.\nConcretely, we formulate the approach as a hierarchical policy where a\nhigh-level controller selects among sets of policies trained on diverse\nintrinsic rewards and the low-level controllers learn the action policies of\nall agents under these specific rewards. We demonstrate the effectiveness of\nthe proposed approach in cooperative domains with sparse rewards where\nstate-of-the-art methods fail and challenging multi-stage tasks that\nnecessitate changing modes of coordination.\n",
        "published": "2019-05-28",
        "authors": [
            "Shariq Iqbal",
            "Fei Sha"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.04737v1",
        "title": "Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning",
        "abstract": "  Recent developments in deep reinforcement learning are concerned with\ncreating decision-making agents which can perform well in various complex\ndomains. A particular approach which has received increasing attention is\nmulti-agent reinforcement learning, in which multiple agents learn concurrently\nto coordinate their actions. In such multi-agent environments, additional\nlearning problems arise due to the continually changing decision-making\npolicies of agents. This paper surveys recent works that address the\nnon-stationarity problem in multi-agent deep reinforcement learning. The\nsurveyed methods range from modifications in the training procedure, such as\ncentralized training, to learning representations of the opponent's policy,\nmeta-learning, communication, and decentralized learning. The survey concludes\nwith a list of open problems and possible lines of future research.\n",
        "published": "2019-06-11",
        "authors": [
            "Georgios Papoudakis",
            "Filippos Christianos",
            "Arrasy Rahman",
            "Stefano V. Albrecht"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.07315v3",
        "title": "Evolutionary Reinforcement Learning for Sample-Efficient Multiagent\n  Coordination",
        "abstract": "  Many cooperative multiagent reinforcement learning environments provide\nagents with a sparse team-based reward, as well as a dense agent-specific\nreward that incentivizes learning basic skills. Training policies solely on the\nteam-based reward is often difficult due to its sparsity. Furthermore, relying\nsolely on the agent-specific reward is sub-optimal because it usually does not\ncapture the team coordination objective. A common approach is to use reward\nshaping to construct a proxy reward by combining the individual rewards.\nHowever, this requires manual tuning for each environment. We introduce\nMultiagent Evolutionary Reinforcement Learning (MERL), a split-level training\nplatform that handles the two objectives separately through two optimization\nprocesses. An evolutionary algorithm maximizes the sparse team-based objective\nthrough neuroevolution on a population of teams. Concurrently, a gradient-based\noptimizer trains policies to only maximize the dense agent-specific rewards.\nThe gradient-based policies are periodically added to the evolutionary\npopulation as a way of information transfer between the two optimization\nprocesses. This enables the evolutionary algorithm to use skills learned via\nthe agent-specific rewards toward optimizing the global objective. Results\ndemonstrate that MERL significantly outperforms state-of-the-art methods, such\nas MADDPG, on a number of difficult coordination benchmarks.\n",
        "published": "2019-06-18",
        "authors": [
            "Shauharda Khadka",
            "Somdeb Majumdar",
            "Santiago Miret",
            "Stephen McAleer",
            "Kagan Tumer"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.07528v2",
        "title": "Emergent Tool Use From Multi-Agent Autocurricula",
        "abstract": "  Through multi-agent competition, the simple objective of hide-and-seek, and\nstandard reinforcement learning algorithms at scale, we find that agents create\na self-supervised autocurriculum inducing multiple distinct rounds of emergent\nstrategy, many of which require sophisticated tool use and coordination. We\nfind clear evidence of six emergent phases in agent strategy in our\nenvironment, each of which creates a new pressure for the opposing team to\nadapt; for instance, agents learn to build multi-object shelters using moveable\nboxes which in turn leads to agents discovering that they can overcome\nobstacles using ramps. We further provide evidence that multi-agent competition\nmay scale better with increasing environment complexity and leads to behavior\nthat centers around far more human-relevant skills than other self-supervised\nreinforcement learning methods such as intrinsic motivation. Finally, we\npropose transfer and fine-tuning as a way to quantitatively evaluate targeted\ncapabilities, and we compare hide-and-seek agents to both intrinsic motivation\nand random initialization baselines in a suite of domain-specific intelligence\ntests.\n",
        "published": "2019-09-17",
        "authors": [
            "Bowen Baker",
            "Ingmar Kanitscheider",
            "Todor Markov",
            "Yi Wu",
            "Glenn Powell",
            "Bob McGrew",
            "Igor Mordatch"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.07543v3",
        "title": "Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement\n  Learning",
        "abstract": "  Continuous control tasks in reinforcement learning are important because they\nprovide an important framework for learning in high-dimensional state spaces\nwith deceptive rewards, where the agent can easily become trapped into\nsuboptimal solutions. One way to avoid local optima is to use a population of\nagents to ensure coverage of the policy space, yet learning a population with\nthe \"best\" coverage is still an open problem. In this work, we present a novel\napproach to population-based RL in continuous control that leverages properties\nof normalizing flows to perform attractive and repulsive operations between\ncurrent members of the population and previously observed policies. Empirical\nresults on the MuJoCo suite demonstrate a high performance gain for our\nalgorithm compared to prior work, including Soft-Actor Critic (SAC).\n",
        "published": "2019-09-17",
        "authors": [
            "Thang Doan",
            "Bogdan Mazoure",
            "Moloud Abdar",
            "Audrey Durand",
            "Joelle Pineau",
            "R Devon Hjelm"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.12557v2",
        "title": "Multi-Agent Actor-Critic with Hierarchical Graph Attention Network",
        "abstract": "  Most previous studies on multi-agent reinforcement learning focus on deriving\ndecentralized and cooperative policies to maximize a common reward and rarely\nconsider the transferability of trained policies to new tasks. This prevents\nsuch policies from being applied to more complex multi-agent tasks. To resolve\nthese limitations, we propose a model that conducts both representation\nlearning for multiple agents using hierarchical graph attention network and\npolicy learning using multi-agent actor-critic. The hierarchical graph\nattention network is specially designed to model the hierarchical relationships\namong multiple agents that either cooperate or compete with each other to\nderive more advanced strategic policies. Two attention networks, the\ninter-agent and inter-group attention layers, are used to effectively model\nindividual and group level interactions, respectively. The two attention\nnetworks have been proven to facilitate the transfer of learned policies to new\ntasks with different agent compositions and allow one to interpret the learned\nstrategies. Empirically, we demonstrate that the proposed model outperforms\nexisting methods in several mixed cooperative and competitive tasks.\n",
        "published": "2019-09-27",
        "authors": [
            "Heechang Ryu",
            "Hayong Shin",
            "Jinkyoo Park"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.04870v1",
        "title": "Network Classifiers With Output Smoothing",
        "abstract": "  This work introduces two strategies for training network classifiers with\nheterogeneous agents. One strategy promotes global smoothing over the graph and\na second strategy promotes local smoothing over neighbourhoods. It is assumed\nthat the feature sizes can vary from one agent to another, with some agents\nobserving insufficient attributes to be able to make reliable decisions on\ntheir own. As a result, cooperation with neighbours is necessary. However, due\nto the fact that the feature dimensions are different across the agents, their\nclassifier dimensions will also be different. This means that cooperation\ncannot rely on combining the classifier parameters. We instead propose\nsmoothing the outputs of the classifiers, which are the predicted labels. By\ndoing so, the dynamics that describes the evolution of the network classifier\nbecomes more challenging than usual because the classifier parameters end up\nappearing as part of the regularization term as well. We illustrate performance\nby means of computer simulations.\n",
        "published": "2019-10-30",
        "authors": [
            "Elsa Rizk",
            "Roula Nassif",
            "Ali H. Sayed"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.10120v2",
        "title": "Multi-Agent Thompson Sampling for Bandit Applications with Sparse\n  Neighbourhood Structures",
        "abstract": "  Multi-agent coordination is prevalent in many real-world applications.\nHowever, such coordination is challenging due to its combinatorial nature. An\nimportant observation in this regard is that agents in the real world often\nonly directly affect a limited set of neighbouring agents. Leveraging such\nloose couplings among agents is key to making coordination in multi-agent\nsystems feasible. In this work, we focus on learning to coordinate.\nSpecifically, we consider the multi-agent multi-armed bandit framework, in\nwhich fully cooperative loosely-coupled agents must learn to coordinate their\ndecisions to optimize a common objective. We propose multi-agent Thompson\nsampling (MATS), a new Bayesian exploration-exploitation algorithm that\nleverages loose couplings. We provide a regret bound that is sublinear in time\nand low-order polynomial in the highest number of actions of a single agent for\nsparse coordination graphs. Additionally, we empirically show that MATS\noutperforms the state-of-the-art algorithm, MAUCE, on two synthetic benchmarks,\nand a novel benchmark with Poisson distributions. An example of a\nloosely-coupled multi-agent system is a wind farm. Coordination within the wind\nfarm is necessary to maximize power production. As upstream wind turbines only\naffect nearby downstream turbines, we can use MATS to efficiently learn the\noptimal control mechanism for the farm. To demonstrate the benefits of our\nmethod toward applications we apply MATS to a realistic wind farm control task.\nIn this task, wind turbines must coordinate their alignments with respect to\nthe incoming wind vector in order to optimize power production. Our results\nshow that MATS improves significantly upon state-of-the-art coordination\nmethods in terms of performance, demonstrating the value of using MATS in\npractical applications with sparse neighbourhood structures.\n",
        "published": "2019-11-22",
        "authors": [
            "Timothy Verstraeten",
            "Eugenio Bargiacchi",
            "Pieter JK Libin",
            "Jan Helsen",
            "Diederik M Roijers",
            "Ann Now\u00e9"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.10635v2",
        "title": "Multi-Agent Reinforcement Learning: A Selective Overview of Theories and\n  Algorithms",
        "abstract": "  Recent years have witnessed significant advances in reinforcement learning\n(RL), which has registered great success in solving various sequential\ndecision-making problems in machine learning. Most of the successful RL\napplications, e.g., the games of Go and Poker, robotics, and autonomous\ndriving, involve the participation of more than one single agent, which\nnaturally fall into the realm of multi-agent RL (MARL), a domain with a\nrelatively long history, and has recently re-emerged due to advances in\nsingle-agent RL techniques. Though empirically successful, theoretical\nfoundations for MARL are relatively lacking in the literature. In this chapter,\nwe provide a selective overview of MARL, with focus on algorithms backed by\ntheoretical analysis. More specifically, we review the theoretical results of\nMARL algorithms mainly within two representative frameworks, Markov/stochastic\ngames and extensive-form games, in accordance with the types of tasks they\naddress, i.e., fully cooperative, fully competitive, and a mix of the two. We\nalso introduce several significant but challenging applications of these\nalgorithms. Orthogonal to the existing reviews on MARL, we highlight several\nnew angles and taxonomies of MARL theory, including learning in extensive-form\ngames, decentralized MARL with networked agents, MARL in the mean-field regime,\n(non-)convergence of policy-based methods for learning in games, etc. Some of\nthe new angles extrapolate from our own research endeavors and interests. Our\noverall goal with this chapter is, beyond providing an assessment of the\ncurrent state of the field on the mark, to identify fruitful future research\ndirections on theoretical studies of MARL. We expect this chapter to serve as\ncontinuing stimulus for researchers interested in working on this exciting\nwhile challenging topic.\n",
        "published": "2019-11-24",
        "authors": [
            "Kaiqing Zhang",
            "Zhuoran Yang",
            "Tamer Ba\u015far"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.05233v4",
        "title": "Learning Multi-Agent Coordination through Connectivity-driven\n  Communication",
        "abstract": "  In artificial multi-agent systems, the ability to learn collaborative\npolicies is predicated upon the agents' communication skills: they must be able\nto encode the information received from the environment and learn how to share\nit with other agents as required by the task at hand. We present a deep\nreinforcement learning approach, Connectivity Driven Communication (CDC), that\nfacilitates the emergence of multi-agent collaborative behaviour only through\nexperience. The agents are modelled as nodes of a weighted graph whose\nstate-dependent edges encode pair-wise messages that can be exchanged. We\nintroduce a graph-dependent attention mechanisms that controls how the agents'\nincoming messages are weighted. This mechanism takes into full account the\ncurrent state of the system as represented by the graph, and builds upon a\ndiffusion process that captures how the information flows on the graph. The\ngraph topology is not assumed to be known a priori, but depends dynamically on\nthe agents' observations, and is learnt concurrently with the attention\nmechanism and policy in an end-to-end fashion. Our empirical results show that\nCDC is able to learn effective collaborative policies and can over-perform\ncompeting learning algorithms on cooperative navigation tasks.\n",
        "published": "2020-02-12",
        "authors": [
            "Emanuele Pesce",
            "Giovanni Montana"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.06306v1",
        "title": "Jelly Bean World: A Testbed for Never-Ending Learning",
        "abstract": "  Machine learning has shown growing success in recent years. However, current\nmachine learning systems are highly specialized, trained for particular\nproblems or domains, and typically on a single narrow dataset. Human learning,\non the other hand, is highly general and adaptable. Never-ending learning is a\nmachine learning paradigm that aims to bridge this gap, with the goal of\nencouraging researchers to design machine learning systems that can learn to\nperform a wider variety of inter-related tasks in more complex environments. To\ndate, there is no environment or testbed to facilitate the development and\nevaluation of never-ending learning systems. To this end, we propose the Jelly\nBean World testbed. The Jelly Bean World allows experimentation over\ntwo-dimensional grid worlds which are filled with items and in which agents can\nnavigate. This testbed provides environments that are sufficiently complex and\nwhere more generally intelligent algorithms ought to perform better than\ncurrent state-of-the-art reinforcement learning approaches. It does so by\nproducing non-stationary environments and facilitating experimentation with\nmulti-task, multi-agent, multi-modal, and curriculum learning settings. We hope\nthat this new freely-available software will prompt new research and interest\nin the development and evaluation of never-ending learning systems and more\nbroadly, general intelligence systems.\n",
        "published": "2020-02-15",
        "authors": [
            "Emmanouil Antonios Platanios",
            "Abulhair Saparov",
            "Tom Mitchell"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.13625v8",
        "title": "Revisiting Parameter Sharing in Multi-Agent Deep Reinforcement Learning",
        "abstract": "  Parameter sharing, where each agent independently learns a policy with fully\nshared parameters between all policies, is a popular baseline method for\nmulti-agent deep reinforcement learning. Unfortunately, since all agents share\nthe same policy network, they cannot learn different policies or tasks. This\nissue has been circumvented experimentally by adding an agent-specific\nindicator signal to observations, which we term \"agent indication\". Agent\nindication is limited, however, in that without modification it does not allow\nparameter sharing to be applied to environments where the action spaces and/or\nobservation spaces are heterogeneous. This work formalizes the notion of agent\nindication and proves that it enables convergence to optimal policies for the\nfirst time. Next, we formally introduce methods to extend parameter sharing to\nlearning in heterogeneous observation and action spaces, and prove that these\nmethods allow for convergence to optimal policies. Finally, we experimentally\nconfirm that the methods we introduce function empirically, and conduct a wide\narray of experiments studying the empirical efficacy of many different agent\nindication schemes for image based observation spaces.\n",
        "published": "2020-05-27",
        "authors": [
            "J. K. Terry",
            "Nathaniel Grammel",
            "Sanghyun Son",
            "Benjamin Black",
            "Aakriti Agrawal"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.06797v4",
        "title": "Competing AI: How does competition feedback affect machine learning?",
        "abstract": "  This papers studies how competition affects machine learning (ML) predictors.\nAs ML becomes more ubiquitous, it is often deployed by companies to compete\nover customers. For example, digital platforms like Yelp use ML to predict user\npreference and make recommendations. A service that is more often queried by\nusers, perhaps because it more accurately anticipates user preferences, is also\nmore likely to obtain additional user data (e.g. in the form of a Yelp review).\nThus, competing predictors cause feedback loops whereby a predictor's\nperformance impacts what training data it receives and biases its predictions\nover time. We introduce a flexible model of competing ML predictors that\nenables both rapid experimentation and theoretical tractability. We show with\nempirical and mathematical analysis that competition causes predictors to\nspecialize for specific sub-populations at the cost of worse performance over\nthe general population. We further analyze the impact of predictor\nspecialization on the overall prediction quality experienced by users. We show\nthat having too few or too many competing predictors in a market can hurt the\noverall prediction quality. Our theory is complemented by experiments on\nseveral real datasets using popular learning algorithms, such as neural\nnetworks and nearest neighbor methods.\n",
        "published": "2020-09-15",
        "authors": [
            "Antonio Ginart",
            "Eva Zhang",
            "Yongchan Kwon",
            "James Zou"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.02178v2",
        "title": "Steady-State Planning in Expected Reward Multichain MDPs",
        "abstract": "  The planning domain has experienced increased interest in the formal\nsynthesis of decision-making policies. This formal synthesis typically entails\nfinding a policy which satisfies formal specifications in the form of some\nwell-defined logic. While many such logics have been proposed with varying\ndegrees of expressiveness and complexity in their capacity to capture desirable\nagent behavior, their value is limited when deriving decision-making policies\nwhich satisfy certain types of asymptotic behavior in general system models. In\nparticular, we are interested in specifying constraints on the steady-state\nbehavior of an agent, which captures the proportion of time an agent spends in\neach state as it interacts for an indefinite period of time with its\nenvironment. This is sometimes called the average or expected behavior of the\nagent and the associated planning problem is faced with significant challenges\nunless strong restrictions are imposed on the underlying model in terms of the\nconnectivity of its graph structure. In this paper, we explore this\nsteady-state planning problem that consists of deriving a decision-making\npolicy for an agent such that constraints on its steady-state behavior are\nsatisfied. A linear programming solution for the general case of multichain\nMarkov Decision Processes (MDPs) is proposed and we prove that optimal\nsolutions to the proposed programs yield stationary policies with rigorous\nguarantees of behavior.\n",
        "published": "2020-12-03",
        "authors": [
            "George K. Atia",
            "Andre Beckus",
            "Ismail Alkhouri",
            "Alvaro Velasquez"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.10362v3",
        "title": "Factored Policy Gradients: Leveraging Structure for Efficient Learning\n  in MOMDPs",
        "abstract": "  Policy gradient methods can solve complex tasks but often fail when the\ndimensionality of the action-space or objective multiplicity grow very large.\nThis occurs, in part, because the variance on score-based gradient estimators\nscales quadratically. In this paper, we address this problem through a factor\nbaseline which exploits independence structure encoded in a novel action-target\ninfluence network. Factored policy gradients (FPGs), which follow, provide a\ncommon framework for analysing key state-of-the-art algorithms, are shown to\ngeneralise traditional policy gradients, and yield a principled way of\nincorporating prior knowledge of a problem domain's generative processes. We\nprovide an analysis of the proposed estimator and identify the conditions under\nwhich variance is reduced. The algorithmic aspects of FPGs are discussed,\nincluding optimal policy factorisation, as characterised by minimum biclique\ncoverings, and the implications for the bias-variance trade-off of incorrectly\nspecifying the network. Finally, we demonstrate the performance advantages of\nour algorithm on large-scale bandit and traffic intersection problems,\nproviding a novel contribution to the latter in the form of a spatial\napproximation.\n",
        "published": "2021-02-20",
        "authors": [
            "Thomas Spooner",
            "Nelson Vadori",
            "Sumitra Ganesh"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.14375v1",
        "title": "Final Adaptation Reinforcement Learning for N-Player Games",
        "abstract": "  This paper covers n-tuple-based reinforcement learning (RL) algorithms for\ngames. We present new algorithms for TD-, SARSA- and Q-learning which work\nseamlessly on various games with arbitrary number of players. This is achieved\nby taking a player-centered view where each player propagates his/her rewards\nback to previous rounds. We add a new element called Final Adaptation RL (FARL)\nto all these algorithms. Our main contribution is that FARL is a vitally\nimportant ingredient to achieve success with the player-centered view in\nvarious games. We report results on seven board games with 1, 2 and 3 players,\nincluding Othello, ConnectFour and Hex. In most cases it is found that FARL is\nimportant to learn a near-perfect playing strategy. All algorithms are\navailable in the GBG framework on GitHub.\n",
        "published": "2021-11-29",
        "authors": [
            "Wolfgang Konen",
            "Samineh Bagheri"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.02481v1",
        "title": "AutoDIME: Automatic Design of Interesting Multi-Agent Environments",
        "abstract": "  Designing a distribution of environments in which RL agents can learn\ninteresting and useful skills is a challenging and poorly understood task, for\nmulti-agent environments the difficulties are only exacerbated. One approach is\nto train a second RL agent, called a teacher, who samples environments that are\nconducive for the learning of student agents. However, most previous proposals\nfor teacher rewards do not generalize straightforwardly to the multi-agent\nsetting. We examine a set of intrinsic teacher rewards derived from prediction\nproblems that can be applied in multi-agent settings and evaluate them in\nMujoco tasks such as multi-agent Hide and Seek as well as a diagnostic\nsingle-agent maze task. Of the intrinsic rewards considered we found value\ndisagreement to be most consistent across tasks, leading to faster and more\nreliable emergence of advanced skills in Hide and Seek and the maze task.\nAnother candidate intrinsic reward considered, value prediction error, also\nworked well in Hide and Seek but was susceptible to noisy-TV style distractions\nin stochastic environments. Policy disagreement performed well in the maze task\nbut did not speed up learning in Hide and Seek. Our results suggest that\nintrinsic teacher rewards, and in particular value disagreement, are a\npromising approach for automating both single and multi-agent environment\ndesign.\n",
        "published": "2022-03-04",
        "authors": [
            "Ingmar Kanitscheider",
            "Harri Edwards"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.16846v1",
        "title": "Universal Feature Selection Tool (UniFeat): An Open-Source Tool for\n  Dimensionality Reduction",
        "abstract": "  The Universal Feature Selection Tool (UniFeat) is an open-source tool\ndeveloped entirely in Java for performing feature selection processes in\nvarious research areas. It provides a set of well-known and advanced feature\nselection methods within its significant auxiliary tools. This allows users to\ncompare the performance of feature selection methods. Moreover, due to the\nopen-source nature of UniFeat, researchers can use and modify it in their\nresearch, which facilitates the rapid development of new feature selection\nalgorithms.\n",
        "published": "2022-11-30",
        "authors": [
            "Sina Tabakhi",
            "Parham Moradi"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.06279v1",
        "title": "Decentralized Stochastic Multi-Player Multi-Armed Walking Bandits",
        "abstract": "  Multi-player multi-armed bandit is an increasingly relevant decision-making\nproblem, motivated by applications to cognitive radio systems. Most research\nfor this problem focuses exclusively on the settings that players have\n\\textit{full access} to all arms and receive no reward when pulling the same\narm. Hence all players solve the same bandit problem with the goal of\nmaximizing their cumulative reward. However, these settings neglect several\nimportant factors in many real-world applications, where players have\n\\textit{limited access} to \\textit{a dynamic local subset of arms} (i.e., an\narm could sometimes be ``walking'' and not accessible to the player). To this\nend, this paper proposes a \\textit{multi-player multi-armed walking bandits}\nmodel, aiming to address aforementioned modeling issues. The goal now is to\nmaximize the reward, however, players can only pull arms from the local subset\nand only collect a full reward if no other players pull the same arm. We adopt\nUpper Confidence Bound (UCB) to deal with the exploration-exploitation tradeoff\nand employ distributed optimization techniques to properly handle collisions.\nBy carefully integrating these two techniques, we propose a decentralized\nalgorithm with near-optimal guarantee on the regret, and can be easily\nimplemented to obtain competitive empirical performance.\n",
        "published": "2022-12-12",
        "authors": [
            "Guojun Xiong",
            "Jian Li"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.09223v2",
        "title": "Doubly Adversarial Federated Bandits",
        "abstract": "  We study a new non-stochastic federated multi-armed bandit problem with\nmultiple agents collaborating via a communication network. The losses of the\narms are assigned by an oblivious adversary that specifies the loss of each arm\nnot only for each time step but also for each agent, which we call ``doubly\nadversarial\". In this setting, different agents may choose the same arm in the\nsame time step but observe different feedback. The goal of each agent is to\nfind a globally best arm in hindsight that has the lowest cumulative loss\naveraged over all agents, which necessities the communication among agents. We\nprovide regret lower bounds for any federated bandit algorithm under different\nsettings, when agents have access to full-information feedback, or the bandit\nfeedback. For the bandit feedback setting, we propose a near-optimal federated\nbandit algorithm called FEDEXP3. Our algorithm gives a positive answer to an\nopen question proposed in Cesa-Bianchi et al. (2016): FEDEXP3 can guarantee a\nsub-linear regret without exchanging sequences of selected arm identities or\nloss sequences among agents. We also provide numerical evaluations of our\nalgorithm to validate our theoretical results and demonstrate its effectiveness\non synthetic and real-world datasets\n",
        "published": "2023-01-22",
        "authors": [
            "Jialin Yi",
            "Milan Vojnovi\u0107"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.02571v1",
        "title": "Offline Learning in Markov Games with General Function Approximation",
        "abstract": "  We study offline multi-agent reinforcement learning (RL) in Markov games,\nwhere the goal is to learn an approximate equilibrium -- such as Nash\nequilibrium and (Coarse) Correlated Equilibrium -- from an offline dataset\npre-collected from the game. Existing works consider relatively restricted\ntabular or linear models and handle each equilibria separately. In this work,\nwe provide the first framework for sample-efficient offline learning in Markov\ngames under general function approximation, handling all 3 equilibria in a\nunified manner. By using Bellman-consistent pessimism, we obtain interval\nestimation for policies' returns, and use both the upper and the lower bounds\nto obtain a relaxation on the gap of a candidate policy, which becomes our\noptimization objective. Our results generalize prior works and provide several\nadditional insights. Importantly, we require a data coverage condition that\nimproves over the recently proposed \"unilateral concentrability\". Our condition\nallows selective coverage of deviation policies that optimally trade-off\nbetween their greediness (as approximate best responses) and coverage, and we\nshow scenarios where this leads to significantly better guarantees. As a new\nconnection, we also show how our algorithmic framework can subsume seemingly\ndifferent solution concepts designed for the special case of two-player\nzero-sum games.\n",
        "published": "2023-02-06",
        "authors": [
            "Yuheng Zhang",
            "Yu Bai",
            "Nan Jiang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.11793v2",
        "title": "Revisiting the Gumbel-Softmax in MADDPG",
        "abstract": "  MADDPG is an algorithm in multi-agent reinforcement learning (MARL) that\nextends the popular single-agent method, DDPG, to multi-agent scenarios.\nImportantly, DDPG is an algorithm designed for continuous action spaces, where\nthe gradient of the state-action value function exists. For this algorithm to\nwork in discrete action spaces, discrete gradient estimation must be performed.\nFor MADDPG, the Gumbel-Softmax (GS) estimator is used -- a reparameterisation\nwhich relaxes a discrete distribution into a similar continuous one. This\nmethod, however, is statistically biased, and a recent MARL benchmarking paper\nsuggests that this bias makes MADDPG perform poorly in grid-world situations,\nwhere the action space is discrete. Fortunately, many alternatives to the GS\nexist, boasting a wide range of properties. This paper explores several of\nthese alternatives and integrates them into MADDPG for discrete grid-world\nscenarios. The corresponding impact on various performance metrics is then\nmeasured and analysed. It is found that one of the proposed estimators performs\nsignificantly better than the original GS in several tasks, achieving up to 55%\nhigher returns, along with faster convergence.\n",
        "published": "2023-02-23",
        "authors": [
            "Callum Rhys Tilbury",
            "Filippos Christianos",
            "Stefano V. Albrecht"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.17052v2",
        "title": "Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning",
        "abstract": "  Many applications, e.g., in shared mobility, require coordinating a large\nnumber of agents. Mean-field reinforcement learning addresses the resulting\nscalability challenge by optimizing the policy of a representative agent\ninteracting with the infinite population of identical agents instead of\nconsidering individual pairwise interactions. In this paper, we address an\nimportant generalization where there exist global constraints on the\ndistribution of agents (e.g., requiring capacity constraints or minimum\ncoverage requirements to be met). We propose Safe-M$^3$-UCRL, the first\nmodel-based mean-field reinforcement learning algorithm that attains safe\npolicies even in the case of unknown transitions. As a key ingredient, it uses\nepistemic uncertainty in the transition model within a log-barrier approach to\nensure pessimistic constraints satisfaction with high probability. Beyond the\nsynthetic swarm motion benchmark, we showcase Safe-M$^3$-UCRL on the vehicle\nrepositioning problem faced by many shared mobility operators and evaluate its\nperformance through simulations built on vehicle trajectory data from a service\nprovider in Shenzhen. Our algorithm effectively meets the demand in critical\nareas while ensuring service accessibility in regions with low demand.\n",
        "published": "2023-06-29",
        "authors": [
            "Matej Jusup",
            "Barna P\u00e1sztor",
            "Tadeusz Janik",
            "Kenan Zhang",
            "Francesco Corman",
            "Andreas Krause",
            "Ilija Bogunovic"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/cs/0204043v1",
        "title": "Learning from Scarce Experience",
        "abstract": "  Searching the space of policies directly for the optimal policy has been one\npopular method for solving partially observable reinforcement learning\nproblems. Typically, with each change of the target policy, its value is\nestimated from the results of following that very policy. This requires a large\nnumber of interactions with the environment as different polices are\nconsidered. We present a family of algorithms based on likelihood ratio\nestimation that use data gathered when executing one policy (or collection of\npolicies) to estimate the value of a different policy. The algorithms combine\nestimation and optimization stages. The former utilizes experience to build a\nnon-parametric representation of an optimized function. The latter performs\noptimization on this estimate. We show positive empirical results and provide\nthe sample complexity bound.\n",
        "published": "2002-04-20",
        "authors": [
            "Leonid Peshkin",
            "Christian R. Shelton"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1207.4931v1",
        "title": "Motion Planning Of an Autonomous Mobile Robot Using Artificial Neural\n  Network",
        "abstract": "  The paper presents the electronic design and motion planning of a robot based\non decision making regarding its straight motion and precise turn using\nArtificial Neural Network (ANN). The ANN helps in learning of robot so that it\nperforms motion autonomously. The weights calculated are implemented in\nmicrocontroller. The performance has been tested to be excellent.\n",
        "published": "2012-07-20",
        "authors": [
            "G. N. Tripathi",
            "V. Rihani"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1407.3269v1",
        "title": "Multiple chaotic central pattern generators with learning for legged\n  locomotion and malfunction compensation",
        "abstract": "  An originally chaotic system can be controlled into various periodic\ndynamics. When it is implemented into a legged robot's locomotion control as a\ncentral pattern generator (CPG), sophisticated gait patterns arise so that the\nrobot can perform various walking behaviors. However, such a single chaotic CPG\ncontroller has difficulties dealing with leg malfunction. Specifically, in the\nscenarios presented here, its movement permanently deviates from the desired\ntrajectory. To address this problem, we extend the single chaotic CPG to\nmultiple CPGs with learning. The learning mechanism is based on a simulated\nannealing algorithm. In a normal situation, the CPGs synchronize and their\ndynamics are identical. With leg malfunction or disability, the CPGs lose\nsynchronization leading to independent dynamics. In this case, the learning\nmechanism is applied to automatically adjust the remaining legs' oscillation\nfrequencies so that the robot adapts its locomotion to deal with the\nmalfunction. As a consequence, the trajectory produced by the multiple chaotic\nCPGs resembles the original trajectory far better than the one produced by only\na single CPG. The performance of the system is evaluated first in a physical\nsimulation of a quadruped as well as a hexapod robot and finally in a real\nsix-legged walking machine called AMOSII. The experimental results presented\nhere reveal that using multiple CPGs with learning is an effective approach for\nadaptive locomotion generation where, for instance, different body parts have\nto perform independent movements for malfunction compensation.\n",
        "published": "2014-07-11",
        "authors": [
            "Guanjiao Ren",
            "Weihai Chen",
            "Sakyasingha Dasgupta",
            "Christoph Kolodziejski",
            "Florentin W\u00f6rg\u00f6tter",
            "Poramate Manoonpong"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.07326v3",
        "title": "One-Shot Imitation Learning",
        "abstract": "  Imitation learning has been commonly applied to solve different tasks in\nisolation. This usually requires either careful feature engineering, or a\nsignificant number of samples. This is far from what we desire: ideally, robots\nshould be able to learn from very few demonstrations of any given task, and\ninstantly generalize to new situations of the same task, without requiring\ntask-specific engineering. In this paper, we propose a meta-learning framework\nfor achieving such capability, which we call one-shot imitation learning.\n  Specifically, we consider the setting where there is a very large set of\ntasks, and each task has many instantiations. For example, a task could be to\nstack all blocks on a table into a single tower, another task could be to place\nall blocks on a table into two-block towers, etc. In each case, different\ninstances of the task would consist of different sets of blocks with different\ninitial states. At training time, our algorithm is presented with pairs of\ndemonstrations for a subset of all tasks. A neural net is trained that takes as\ninput one demonstration and the current state (which initially is the initial\nstate of the other demonstration of the pair), and outputs an action with the\ngoal that the resulting sequence of states and actions matches as closely as\npossible with the second demonstration. At test time, a demonstration of a\nsingle instance of a new task is presented, and the neural net is expected to\nperform well on new instances of this new task. The use of soft attention\nallows the model to generalize to conditions and tasks unseen in the training\ndata. We anticipate that by training this model on a much greater variety of\ntasks and settings, we will obtain a general system that can turn any\ndemonstrations into robust policies that can accomplish an overwhelming variety\nof tasks.\n  Videos available at https://bit.ly/nips2017-oneshot .\n",
        "published": "2017-03-21",
        "authors": [
            "Yan Duan",
            "Marcin Andrychowicz",
            "Bradly C. Stadie",
            "Jonathan Ho",
            "Jonas Schneider",
            "Ilya Sutskever",
            "Pieter Abbeel",
            "Wojciech Zaremba"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.01495v3",
        "title": "Hindsight Experience Replay",
        "abstract": "  Dealing with sparse rewards is one of the biggest challenges in Reinforcement\nLearning (RL). We present a novel technique called Hindsight Experience Replay\nwhich allows sample-efficient learning from rewards which are sparse and binary\nand therefore avoid the need for complicated reward engineering. It can be\ncombined with an arbitrary off-policy RL algorithm and may be seen as a form of\nimplicit curriculum.\n  We demonstrate our approach on the task of manipulating objects with a\nrobotic arm. In particular, we run experiments on three different tasks:\npushing, sliding, and pick-and-place, in each case using only binary rewards\nindicating whether or not the task is completed. Our ablation studies show that\nHindsight Experience Replay is a crucial ingredient which makes training\npossible in these challenging environments. We show that our policies trained\non a physics simulation can be deployed on a physical robot and successfully\ncomplete the task.\n",
        "published": "2017-07-05",
        "authors": [
            "Marcin Andrychowicz",
            "Filip Wolski",
            "Alex Ray",
            "Jonas Schneider",
            "Rachel Fong",
            "Peter Welinder",
            "Bob McGrew",
            "Josh Tobin",
            "Pieter Abbeel",
            "Wojciech Zaremba"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.05300v3",
        "title": "Reverse Curriculum Generation for Reinforcement Learning",
        "abstract": "  Many relevant tasks require an agent to reach a certain state, or to\nmanipulate objects into a desired configuration. For example, we might want a\nrobot to align and assemble a gear onto an axle or insert and turn a key in a\nlock. These goal-oriented tasks present a considerable challenge for\nreinforcement learning, since their natural reward function is sparse and\nprohibitive amounts of exploration are required to reach the goal and receive\nsome learning signal. Past approaches tackle these problems by exploiting\nexpert demonstrations or by manually designing a task-specific reward shaping\nfunction to guide the learning agent. Instead, we propose a method to learn\nthese tasks without requiring any prior knowledge other than obtaining a single\nstate in which the task is achieved. The robot is trained in reverse, gradually\nlearning to reach the goal from a set of start states increasingly far from the\ngoal. Our method automatically generates a curriculum of start states that\nadapts to the agent's performance, leading to efficient training on\ngoal-oriented tasks. We demonstrate our approach on difficult simulated\nnavigation and fine-grained manipulation problems, not solvable by\nstate-of-the-art reinforcement learning methods.\n",
        "published": "2017-07-17",
        "authors": [
            "Carlos Florensa",
            "David Held",
            "Markus Wulfmeier",
            "Michael Zhang",
            "Pieter Abbeel"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.06006v3",
        "title": "Hindsight policy gradients",
        "abstract": "  A reinforcement learning agent that needs to pursue different goals across\nepisodes requires a goal-conditional policy. In addition to their potential to\ngeneralize desirable behavior to unseen goals, such policies may also enable\nhigher-level planning based on subgoals. In sparse-reward environments, the\ncapacity to exploit information about the degree to which an arbitrary goal has\nbeen achieved while another goal was intended appears crucial to enable sample\nefficient learning. However, reinforcement learning agents have only recently\nbeen endowed with such capacity for hindsight. In this paper, we demonstrate\nhow hindsight can be introduced to policy gradient methods, generalizing this\nidea to a broad class of successful algorithms. Our experiments on a diverse\nselection of sparse-reward environments show that hindsight leads to a\nremarkable increase in sample efficiency.\n",
        "published": "2017-11-16",
        "authors": [
            "Paulo Rauber",
            "Avinash Ummadisingu",
            "Filipe Mutz",
            "Juergen Schmidhuber"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.00943v1",
        "title": "Self-supervised Learning of Image Embedding for Continuous Control",
        "abstract": "  Operating directly from raw high dimensional sensory inputs like images is\nstill a challenge for robotic control. Recently, Reinforcement Learning methods\nhave been proposed to solve specific tasks end-to-end, from pixels to torques.\nHowever, these approaches assume the access to a specified reward which may\nrequire specialized instrumentation of the environment. Furthermore, the\nobtained policy and representations tend to be task specific and may not\ntransfer well. In this work we investigate completely self-supervised learning\nof a general image embedding and control primitives, based on finding the\nshortest time to reach any state. We also introduce a new structure for the\nstate-action value function that builds a connection between model-free and\nmodel-based methods, and improves the performance of the learning algorithm. We\nexperimentally demonstrate these findings in three simulated robotic tasks.\n",
        "published": "2019-01-03",
        "authors": [
            "Carlos Florensa",
            "Jonas Degrave",
            "Nicolas Heess",
            "Jost Tobias Springenberg",
            "Martin Riedmiller"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.10089v2",
        "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations",
        "abstract": "  Exploration in environments with sparse rewards has been a persistent problem\nin reinforcement learning (RL). Many tasks are natural to specify with a sparse\nreward, and manually shaping a reward function can result in suboptimal\nperformance. However, finding a non-zero reward is exponentially more difficult\nwith increasing task horizon or action dimensionality. This puts many\nreal-world tasks out of practical reach of RL methods. In this work, we use\ndemonstrations to overcome the exploration problem and successfully learn to\nperform long-horizon, multi-step robotics tasks with continuous control such as\nstacking blocks with a robot arm. Our method, which builds on top of Deep\nDeterministic Policy Gradients and Hindsight Experience Replay, provides an\norder of magnitude of speedup over RL on simulated robotics tasks. It is simple\nto implement and makes only the additional assumption that we can collect a\nsmall set of demonstrations. Furthermore, our method is able to solve tasks not\nsolvable by either RL or behavior cloning alone, and often ends up\noutperforming the demonstrator policy.\n",
        "published": "2017-09-28",
        "authors": [
            "Ashvin Nair",
            "Bob McGrew",
            "Marcin Andrychowicz",
            "Wojciech Zaremba",
            "Pieter Abbeel"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.06511v4",
        "title": "Reinforcement Learning with Chromatic Networks for Compact Architecture\n  Search",
        "abstract": "  We present a neural architecture search algorithm to construct compact\nreinforcement learning (RL) policies, by combining ENAS and ES in a highly\nscalable and intuitive way. By defining the combinatorial search space of NAS\nto be the set of different edge-partitionings (colorings) into same-weight\nclasses, we represent compact architectures via efficient learned\nedge-partitionings. For several RL tasks, we manage to learn colorings\ntranslating to effective policies parameterized by as few as $17$ weight\nparameters, providing >90% compression over vanilla policies and 6x compression\nover state-of-the-art compact policies based on Toeplitz matrices, while still\nmaintaining good reward. We believe that our work is one of the first attempts\nto propose a rigorous approach to training structured neural network\narchitectures for RL problems that are of interest especially in mobile\nrobotics with limited storage and computational resources.\n",
        "published": "2019-07-10",
        "authors": [
            "Xingyou Song",
            "Krzysztof Choromanski",
            "Jack Parker-Holder",
            "Yunhao Tang",
            "Wenbo Gao",
            "Aldo Pacchiano",
            "Tamas Sarlos",
            "Deepali Jain",
            "Yuxiang Yang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.07029v3",
        "title": "Adaptive Prior Selection for Repertoire-based Online Adaptation in\n  Robotics",
        "abstract": "  Repertoire-based learning is a data-efficient adaptation approach based on a\ntwo-step process in which (1) a large and diverse set of policies is learned in\nsimulation, and (2) a planning or learning algorithm chooses the most\nappropriate policies according to the current situation (e.g., a damaged robot,\na new object, etc.). In this paper, we relax the assumption of previous works\nthat a single repertoire is enough for adaptation. Instead, we generate\nrepertoires for many different situations (e.g., with a missing leg, on\ndifferent floors, etc.) and let our algorithm selects the most useful prior.\nOur main contribution is an algorithm, APROL (Adaptive Prior selection for\nRepertoire-based Online Learning) to plan the next action by incorporating\nthese priors when the robot has no information about the current situation. We\nevaluate APROL on two simulated tasks: (1) pushing unknown objects of various\nshapes and sizes with a robotic arm and (2) a goal reaching task with a damaged\nhexapod robot. We compare with \"Reset-free Trial and Error\" (RTE) and various\nsingle repertoire-based baselines. The results show that APROL solves both the\ntasks in less interaction time than the baselines. Additionally, we demonstrate\nAPROL on a real, damaged hexapod that quickly learns to pick compensatory\npolicies to reach a goal by avoiding obstacles in the path.\n",
        "published": "2019-07-16",
        "authors": [
            "Rituraj Kaushik",
            "Pierre Desreumaux",
            "Jean-Baptiste Mouret"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1712.00948v5",
        "title": "Learning Multi-Level Hierarchies with Hindsight",
        "abstract": "  Hierarchical agents have the potential to solve sequential decision making\ntasks with greater sample efficiency than their non-hierarchical counterparts\nbecause hierarchical agents can break down tasks into sets of subtasks that\nonly require short sequences of decisions. In order to realize this potential\nof faster learning, hierarchical agents need to be able to learn their multiple\nlevels of policies in parallel so these simpler subproblems can be solved\nsimultaneously. Yet, learning multiple levels of policies in parallel is hard\nbecause it is inherently unstable: changes in a policy at one level of the\nhierarchy may cause changes in the transition and reward functions at higher\nlevels in the hierarchy, making it difficult to jointly learn multiple levels\nof policies. In this paper, we introduce a new Hierarchical Reinforcement\nLearning (HRL) framework, Hierarchical Actor-Critic (HAC), that can overcome\nthe instability issues that arise when agents try to jointly learn multiple\nlevels of policies. The main idea behind HAC is to train each level of the\nhierarchy independently of the lower levels by training each level as if the\nlower level policies are already optimal. We demonstrate experimentally in both\ngrid world and simulated robotics domains that our approach can significantly\naccelerate learning relative to other non-hierarchical and hierarchical\nmethods. Indeed, our framework is the first to successfully learn 3-level\nhierarchies in parallel in tasks with continuous state and action spaces.\n",
        "published": "2017-12-04",
        "authors": [
            "Andrew Levy",
            "George Konidaris",
            "Robert Platt",
            "Kate Saenko"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1712.05249v1",
        "title": "Proximodistal Exploration in Motor Learning as an Emergent Property of\n  Optimization",
        "abstract": "  To harness the complexity of their high-dimensional bodies during\nsensorimotor development, infants are guided by patterns of freezing and\nfreeing of degrees of freedom. For instance, when learning to reach, infants\nfree the degrees of freedom in their arm proximodistally, i.e. from joints that\nare closer to the body to those that are more distant. Here, we formulate and\nstudy computationally the hypothesis that such patterns can emerge\nspontaneously as the result of a family of stochastic optimization processes\n(evolution strategies with covariance-matrix adaptation), without an innate\nencoding of a maturational schedule. In particular, we present simulated\nexperiments with an arm where a computational learner progressively acquires\nreaching skills through adaptive exploration, and we show that a proximodistal\norganization appears spontaneously, which we denote PDFF (ProximoDistal\nFreezing and Freeing of degrees of freedom). We also compare this emergent\norganization between different arm morphologies -- from human-like to quite\nunnatural ones -- to study the effect of different kinematic structures on the\nemergence of PDFF. Keywords: human motor learning; proximo-distal exploration;\nstochastic optimization; modelling; evolution strategies; cross-entropy\nmethods; policy search; morphology.}\n",
        "published": "2017-12-14",
        "authors": [
            "Freek Stulp",
            "Pierre-Yves Oudeyer"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.10283v3",
        "title": "Adding Neural Network Controllers to Behavior Trees without Destroying\n  Performance Guarantees",
        "abstract": "  In this paper, we show how Behavior Trees that have performance guarantees,\nin terms of safety and goal convergence, can be extended with components that\nwere designed using machine learning, without destroying those performance\nguarantees.\n  Machine learning approaches such as reinforcement learning or learning from\ndemonstration can be very appealing to AI designers that want efficient and\nrealistic behaviors in their agents. However, those algorithms seldom provide\nguarantees for solving the given task in all different situations while keeping\nthe agent safe. Instead, such guarantees are often easier to find for manually\ndesigned model-based approaches. In this paper we exploit the modularity of\nbehavior trees to extend a given design with an efficient, but possibly\nunreliable, machine learning component in a way that preserves the guarantees.\nThe approach is illustrated with an inverted pendulum example.\n",
        "published": "2018-09-26",
        "authors": [
            "Christopher Iliffe Sprague",
            "Petter \u00d6gren"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.01431v1",
        "title": "Embodied Synaptic Plasticity with Online Reinforcement learning",
        "abstract": "  The endeavor to understand the brain involves multiple collaborating research\nfields. Classically, synaptic plasticity rules derived by theoretical\nneuroscientists are evaluated in isolation on pattern classification tasks.\nThis contrasts with the biological brain which purpose is to control a body in\nclosed-loop. This paper contributes to bringing the fields of computational\nneuroscience and robotics closer together by integrating open-source software\ncomponents from these two fields. The resulting framework allows to evaluate\nthe validity of biologically-plausibe plasticity models in closed-loop robotics\nenvironments. We demonstrate this framework to evaluate Synaptic Plasticity\nwith Online REinforcement learning (SPORE), a reward-learning rule based on\nsynaptic sampling, on two visuomotor tasks: reaching and lane following. We\nshow that SPORE is capable of learning to perform policies within the course of\nsimulated hours for both tasks. Provisional parameter explorations indicate\nthat the learning rate and the temperature driving the stochastic processes\nthat govern synaptic learning dynamics need to be regulated for performance\nimprovements to be retained. We conclude by discussing the recent deep\nreinforcement learning techniques which would be beneficial to increase the\nfunctionality of SPORE on visuomotor tasks.\n",
        "published": "2020-03-03",
        "authors": [
            "Jacques Kaiser",
            "Michael Hoff",
            "Andreas Konle",
            "J. Camilo Vasquez Tieck",
            "David Kappel",
            "Daniel Reichard",
            "Anand Subramoney",
            "Robert Legenstein",
            "Arne Roennau",
            "Wolfgang Maass",
            "Rudiger Dillmann"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.04589v1",
        "title": "Model-Based Quality-Diversity Search for Efficient Robot Learning",
        "abstract": "  Despite recent progress in robot learning, it still remains a challenge to\nprogram a robot to deal with open-ended object manipulation tasks. One approach\nthat was recently used to autonomously generate a repertoire of diverse skills\nis a novelty based Quality-Diversity~(QD) algorithm. However, as most\nevolutionary algorithms, QD suffers from sample-inefficiency and, thus, it is\nchallenging to apply it in real-world scenarios. This paper tackles this\nproblem by integrating a neural network that predicts the behavior of the\nperturbed parameters into a novelty based QD algorithm. In the proposed\nModel-based Quality-Diversity search (M-QD), the network is trained\nconcurrently to the repertoire and is used to avoid executing unpromising\nactions in the novelty search process. Furthermore, it is used to adapt the\nskills of the final repertoire in order to generalize the skills to different\nscenarios. Our experiments show that enhancing a QD algorithm with such a\nforward model improves the sample-efficiency and performance of the\nevolutionary process and the skill adaptation.\n",
        "published": "2020-08-11",
        "authors": [
            "Leon Keller",
            "Daniel Tanneberg",
            "Svenja Stark",
            "Jan Peters"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.01258v3",
        "title": "Accelerated Quality-Diversity through Massive Parallelism",
        "abstract": "  Quality-Diversity (QD) optimization algorithms are a well-known approach to\ngenerate large collections of diverse and high-quality solutions. However,\nderived from evolutionary computation, QD algorithms are population-based\nmethods which are known to be data-inefficient and requires large amounts of\ncomputational resources. This makes QD algorithms slow when used in\napplications where solution evaluations are computationally costly. A common\napproach to speed up QD algorithms is to evaluate solutions in parallel, for\ninstance by using physical simulators in robotics. Yet, this approach is\nlimited to several dozen of parallel evaluations as most physics simulators can\nonly be parallelized more with a greater number of CPUs. With recent advances\nin simulators that run on accelerators, thousands of evaluations can now be\nperformed in parallel on single GPU/TPU. In this paper, we present QDax, an\naccelerated implementation of MAP-Elites which leverages massive parallelism on\naccelerators to make QD algorithms more accessible. We show that QD algorithms\nare ideal candidates to take advantage of progress in hardware acceleration. We\ndemonstrate that QD algorithms can scale with massive parallelism to be run at\ninteractive timescales without any significant effect on the performance.\nResults across standard optimization functions and four neuroevolution\nbenchmark environments shows that experiment runtimes are reduced by two\nfactors of magnitudes, turning days of computation into minutes. More\nsurprising, we observe that reducing the number of generations by two orders of\nmagnitude, and thus having significantly shorter lineage does not impact the\nperformance of QD algorithms. These results show that QD can now benefit from\nhardware acceleration, which contributed significantly to the bloom of deep\nlearning.\n",
        "published": "2022-02-02",
        "authors": [
            "Bryan Lim",
            "Maxime Allard",
            "Luca Grillotti",
            "Antoine Cully"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.05508v4",
        "title": "Unsupervised Learning and Exploration of Reachable Outcome Space",
        "abstract": "  Performing Reinforcement Learning in sparse rewards settings, with very\nlittle prior knowledge, is a challenging problem since there is no signal to\nproperly guide the learning process. In such situations, a good search strategy\nis fundamental. At the same time, not having to adapt the algorithm to every\nsingle problem is very desirable. Here we introduce TAXONS, a Task Agnostic\neXploration of Outcome spaces through Novelty and Surprise algorithm. Based on\na population-based divergent-search approach, it learns a set of diverse\npolicies directly from high-dimensional observations, without any task-specific\ninformation. TAXONS builds a repertoire of policies while training an\nautoencoder on the high-dimensional observation of the final state of the\nsystem to build a low-dimensional outcome space. The learned outcome space,\ncombined with the reconstruction error, is used to drive the search for new\npolicies. Results show that TAXONS can find a diverse set of controllers,\ncovering a good part of the ground-truth outcome space, while having no\ninformation about such space.\n",
        "published": "2019-09-12",
        "authors": [
            "Giuseppe Paolo",
            "Alban Laflaqui\u00e8re",
            "Alexandre Coninx",
            "Stephane Doncieux"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.05151v1",
        "title": "Autonomous learning and chaining of motor primitives using the Free\n  Energy Principle",
        "abstract": "  In this article, we apply the Free-Energy Principle to the question of motor\nprimitives learning. An echo-state network is used to generate motor\ntrajectories. We combine this network with a perception module and a controller\nthat can influence its dynamics. This new compound network permits the\nautonomous learning of a repertoire of motor trajectories. To evaluate the\nrepertoires built with our method, we exploit them in a handwriting task where\nprimitives are chained to produce long-range sequences.\n",
        "published": "2020-05-11",
        "authors": [
            "Louis Annabi",
            "Alexandre Pitti",
            "Mathias Quoy"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.06223v1",
        "title": "DREAM Architecture: a Developmental Approach to Open-Ended Learning in\n  Robotics",
        "abstract": "  Robots are still limited to controlled conditions, that the robot designer\nknows with enough details to endow the robot with the appropriate models or\nbehaviors. Learning algorithms add some flexibility with the ability to\ndiscover the appropriate behavior given either some demonstrations or a reward\nto guide its exploration with a reinforcement learning algorithm. Reinforcement\nlearning algorithms rely on the definition of state and action spaces that\ndefine reachable behaviors. Their adaptation capability critically depends on\nthe representations of these spaces: small and discrete spaces result in fast\nlearning while large and continuous spaces are challenging and either require a\nlong training period or prevent the robot from converging to an appropriate\nbehavior. Beside the operational cycle of policy execution and the learning\ncycle, which works at a slower time scale to acquire new policies, we introduce\nthe redescription cycle, a third cycle working at an even slower time scale to\ngenerate or adapt the required representations to the robot, its environment\nand the task. We introduce the challenges raised by this cycle and we present\nDREAM (Deferred Restructuring of Experience in Autonomous Machines), a\ndevelopmental cognitive architecture to bootstrap this redescription process\nstage by stage, build new state representations with appropriate motivations,\nand transfer the acquired knowledge across domains or tasks or even across\nrobots. We describe results obtained so far with this approach and end up with\na discussion of the questions it raises in Neuroscience.\n",
        "published": "2020-05-13",
        "authors": [
            "Stephane Doncieux",
            "Nicolas Bredeche",
            "L\u00e9ni Le Goff",
            "Beno\u00eet Girard",
            "Alexandre Coninx",
            "Olivier Sigaud",
            "Mehdi Khamassi",
            "Natalia D\u00edaz-Rodr\u00edguez",
            "David Filliat",
            "Timothy Hospedales",
            "A. Eiben",
            "Richard Duro"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.06224v1",
        "title": "Novelty Search makes Evolvability Inevitable",
        "abstract": "  Evolvability is an important feature that impacts the ability of evolutionary\nprocesses to find interesting novel solutions and to deal with changing\nconditions of the problem to solve. The estimation of evolvability is not\nstraightforward and is generally too expensive to be directly used as selective\npressure in the evolutionary process. Indirectly promoting evolvability as a\nside effect of other easier and faster to compute selection pressures would\nthus be advantageous. In an unbounded behavior space, it has already been shown\nthat evolvable individuals naturally appear and tend to be selected as they are\nmore likely to invade empty behavior niches. Evolvability is thus a natural\nbyproduct of the search in this context. However, practical agents and\nenvironments often impose limits on the reach-able behavior space. How do these\nboundaries impact evolvability? In this context, can evolvability still be\npromoted without explicitly rewarding it? We show that Novelty Search\nimplicitly creates a pressure for high evolvability even in bounded behavior\nspaces, and explore the reasons for such a behavior. More precisely we show\nthat, throughout the search, the dynamic evaluation of novelty rewards\nindividuals which are very mobile in the behavior space, which in turn promotes\nevolvability.\n",
        "published": "2020-05-13",
        "authors": [
            "Stephane Doncieux",
            "Giuseppe Paolo",
            "Alban Laflaqui\u00e8re",
            "Alexandre Coninx"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.04700v1",
        "title": "Emergence of Different Modes of Tool Use in a Reaching and Dragging Task",
        "abstract": "  Tool use is an important milestone in the evolution of intelligence. In this\npaper, we investigate different modes of tool use that emerge in a reaching and\ndragging task. In this task, a jointed arm with a gripper must grab a tool (T,\nI, or L-shaped) and drag an object down to the target location (the bottom of\nthe arena). The simulated environment had real physics such as gravity and\nfriction. We trained a deep-reinforcement learning based controller (with raw\nvisual and proprioceptive input) with minimal reward shaping information to\ntackle this task. We observed the emergence of a wide range of unexpected\nbehaviors, not directly encoded in the motor primitives or reward functions.\nExamples include hitting the object to the target location, correcting error of\ninitial contact, throwing the tool toward the object, as well as normal\nexpected behavior such as wide sweep. Also, we further analyzed these behaviors\nbased on the type of tool and the initial position of the target object. Our\nresults show a rich repertoire of behaviors, beyond the basic built-in\nmechanisms of the deep reinforcement learning method we used.\n",
        "published": "2020-12-08",
        "authors": [
            "Khuong Nguyen",
            "Yoonsuck Choe"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.03140v2",
        "title": "Sparse Reward Exploration via Novelty Search and Emitters",
        "abstract": "  Reward-based optimization algorithms require both exploration, to find\nrewards, and exploitation, to maximize performance. The need for efficient\nexploration is even more significant in sparse reward settings, in which\nperformance feedback is given sparingly, thus rendering it unsuitable for\nguiding the search process. In this work, we introduce the SparsE Reward\nExploration via Novelty and Emitters (SERENE) algorithm, capable of efficiently\nexploring a search space, as well as optimizing rewards found in potentially\ndisparate areas. Contrary to existing emitters-based approaches, SERENE\nseparates the search space exploration and reward exploitation into two\nalternating processes. The first process performs exploration through Novelty\nSearch, a divergent search algorithm. The second one exploits discovered reward\nareas through emitters, i.e. local instances of population-based optimization\nalgorithms. A meta-scheduler allocates a global computational budget by\nalternating between the two processes, ensuring the discovery and efficient\nexploitation of disjoint reward areas. SERENE returns both a collection of\ndiverse solutions covering the search space and a collection of high-performing\nsolutions for each distinct reward area. We evaluate SERENE on various sparse\nreward environments and show it compares favorably to existing baselines.\n",
        "published": "2021-02-05",
        "authors": [
            "Giuseppe Paolo",
            "Alexandre Coninx",
            "Stephane Doncieux",
            "Alban Laflaqui\u00e8re"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.05648v3",
        "title": "Unsupervised Behaviour Discovery with Quality-Diversity Optimisation",
        "abstract": "  Quality-Diversity algorithms refer to a class of evolutionary algorithms\ndesigned to find a collection of diverse and high-performing solutions to a\ngiven problem. In robotics, such algorithms can be used for generating a\ncollection of controllers covering most of the possible behaviours of a robot.\nTo do so, these algorithms associate a behavioural descriptor to each of these\nbehaviours. Each behavioural descriptor is used for estimating the novelty of\none behaviour compared to the others. In most existing algorithms, the\nbehavioural descriptor needs to be hand-coded, thus requiring prior knowledge\nabout the task to solve. In this paper, we introduce: Autonomous Robots\nRealising their Abilities, an algorithm that uses a dimensionality reduction\ntechnique to automatically learn behavioural descriptors based on raw sensory\ndata. The performance of this algorithm is assessed on three robotic tasks in\nsimulation. The experimental results show that it performs similarly to\ntraditional hand-coded approaches without the requirement to provide any\nhand-coded behavioural descriptor. In the collection of diverse and\nhigh-performing solutions, it also manages to find behaviours that are novel\nwith respect to more features than its hand-coded baselines. Finally, we\nintroduce a variant of the algorithm which is robust to the dimensionality of\nthe behavioural descriptor space.\n",
        "published": "2021-06-10",
        "authors": [
            "Luca Grillotti",
            "Antoine Cully"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.08314v2",
        "title": "Causal Navigation by Continuous-time Neural Networks",
        "abstract": "  Imitation learning enables high-fidelity, vision-based learning of policies\nwithin rich, photorealistic environments. However, such techniques often rely\non traditional discrete-time neural models and face difficulties in\ngeneralizing to domain shifts by failing to account for the causal\nrelationships between the agent and the environment. In this paper, we propose\na theoretical and experimental framework for learning causal representations\nusing continuous-time neural networks, specifically over their discrete-time\ncounterparts. We evaluate our method in the context of visual-control learning\nof drones over a series of complex tasks, ranging from short- and long-term\nnavigation, to chasing static and dynamic objects through photorealistic\nenvironments. Our results demonstrate that causal continuous-time deep models\ncan perform robust navigation tasks, where advanced recurrent models fail.\nThese models learn complex causal control representations directly from raw\nvisual inputs and scale to solve a variety of tasks using imitation learning.\n",
        "published": "2021-06-15",
        "authors": [
            "Charles Vorbach",
            "Ramin Hasani",
            "Alexander Amini",
            "Mathias Lechner",
            "Daniela Rus"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.08522v1",
        "title": "Dynamics-Aware Quality-Diversity for Efficient Learning of Skill\n  Repertoires",
        "abstract": "  Quality-Diversity (QD) algorithms are powerful exploration algorithms that\nallow robots to discover large repertoires of diverse and high-performing\nskills. However, QD algorithms are sample inefficient and require millions of\nevaluations. In this paper, we propose Dynamics-Aware Quality-Diversity\n(DA-QD), a framework to improve the sample efficiency of QD algorithms through\nthe use of dynamics models. We also show how DA-QD can then be used for\ncontinual acquisition of new skill repertoires. To do so, we incrementally\ntrain a deep dynamics model from experience obtained when performing skill\ndiscovery using QD. We can then perform QD exploration in imagination with an\nimagined skill repertoire. We evaluate our approach on three robotic\nexperiments. First, our experiments show DA-QD is 20 times more sample\nefficient than existing QD approaches for skill discovery. Second, we\ndemonstrate learning an entirely new skill repertoire in imagination to perform\nzero-shot learning. Finally, we show how DA-QD is useful and effective for\nsolving a long horizon navigation task and for damage adaptation in the real\nworld. Videos and source code are available at:\nhttps://sites.google.com/view/da-qd.\n",
        "published": "2021-09-16",
        "authors": [
            "Bryan Lim",
            "Luca Grillotti",
            "Lorenzo Bernasconi",
            "Antoine Cully"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.05437v2",
        "title": "Autonomous Racing using a Hybrid Imitation-Reinforcement Learning\n  Architecture",
        "abstract": "  In this work, we present a rigorous end-to-end control strategy for\nautonomous vehicles aimed at minimizing lap times in a time attack racing\nevent. We also introduce AutoRACE Simulator developed as a part of this\nresearch project, which was employed to simulate accurate vehicular and\nenvironmental dynamics along with realistic audio-visual effects. We adopted a\nhybrid imitation-reinforcement learning architecture and crafted a novel reward\nfunction to train a deep neural network policy to drive (using imitation\nlearning) and race (using reinforcement learning) a car autonomously in less\nthan 20 hours. Deployment results were reported as a direct comparison of 10\nautonomous laps against 100 manual laps by 10 different human players. The\nautonomous agent not only exhibited superior performance by gaining 0.96\nseconds over the best manual lap, but it also dominated the human players by\n1.46 seconds with regard to the mean lap time. This dominance could be\njustified in terms of better trajectory optimization and lower reaction time of\nthe autonomous agent.\n",
        "published": "2021-10-11",
        "authors": [
            "Chinmay Vilas Samak",
            "Tanmay Vilas Samak",
            "Sivanathan Kandhasamy"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.01919v2",
        "title": "Discovering and Exploiting Sparse Rewards in a Learned Behavior Space",
        "abstract": "  Learning optimal policies in sparse rewards settings is difficult as the\nlearning agent has little to no feedback on the quality of its actions. In\nthese situations, a good strategy is to focus on exploration, hopefully leading\nto the discovery of a reward signal to improve on. A learning algorithm capable\nof dealing with this kind of settings has to be able to (1) explore possible\nagent behaviors and (2) exploit any possible discovered reward. Efficient\nexploration algorithms have been proposed that require to define a behavior\nspace, that associates to an agent its resulting behavior in a space that is\nknown to be worth exploring. The need to define this space is a limitation of\nthese algorithms. In this work, we introduce STAX, an algorithm designed to\nlearn a behavior space on-the-fly and to explore it while efficiently\noptimizing any reward discovered. It does so by separating the exploration and\nlearning of the behavior space from the exploitation of the reward through an\nalternating two-steps process. In the first step, STAX builds a repertoire of\ndiverse policies while learning a low-dimensional representation of the\nhigh-dimensional observations generated during the policies evaluation. In the\nexploitation step, emitters are used to optimize the performance of the\ndiscovered rewarding solutions. Experiments conducted on three different sparse\nreward environments show that STAX performs comparably to existing baselines\nwhile requiring much less prior information about the task as it autonomously\nbuilds the behavior space.\n",
        "published": "2021-11-02",
        "authors": [
            "Giuseppe Paolo",
            "Miranda Coninx",
            "Alban Laflaqui\u00e8re",
            "Stephane Doncieux"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.09746v1",
        "title": "Reinforcement Learning Textbook",
        "abstract": "  This textbook covers principles behind main modern deep reinforcement\nlearning algorithms that achieved breakthrough results in many domains from\ngame AI to robotics. All required theory is explained with proofs using unified\nnotation and emphasize on the differences between different types of algorithms\nand the reasons why they are constructed the way they are.\n",
        "published": "2022-01-19",
        "authors": [
            "Sergey Ivanov"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.13248v1",
        "title": "SafeAPT: Safe Simulation-to-Real Robot Learning using Diverse Policies\n  Learned in Simulation",
        "abstract": "  The framework of Simulation-to-real learning, i.e, learning policies in\nsimulation and transferring those policies to the real world is one of the most\npromising approaches towards data-efficient learning in robotics. However, due\nto the inevitable reality gap between the simulation and the real world, a\npolicy learned in the simulation may not always generate a safe behaviour on\nthe real robot. As a result, during adaptation of the policy in the real world,\nthe robot may damage itself or cause harm to its surroundings. In this work, we\nintroduce a novel learning algorithm called SafeAPT that leverages a diverse\nrepertoire of policies evolved in the simulation and transfers the most\npromising safe policy to the real robot through episodic interaction. To\nachieve this, SafeAPT iteratively learns a probabilistic reward model as well\nas a safety model using real-world observations combined with simulated\nexperiences as priors. Then, it performs Bayesian optimization on the\nrepertoire with the reward model while maintaining the specified safety\nconstraint using the safety model. SafeAPT allows a robot to adapt to a wide\nrange of goals safely with the same repertoire of policies evolved in the\nsimulation. We compare SafeAPT with several baselines, both in simulated and\nreal robotic experiments and show that SafeAPT finds high-performance policies\nwithin a few minutes in the real world while minimizing safety violations\nduring the interactions.\n",
        "published": "2022-01-27",
        "authors": [
            "Rituraj Kaushik",
            "Karol Arndt",
            "Ville Kyrki"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.03655v1",
        "title": "Learning to Walk Autonomously via Reset-Free Quality-Diversity",
        "abstract": "  Quality-Diversity (QD) algorithms can discover large and complex behavioural\nrepertoires consisting of both diverse and high-performing skills. However, the\ngeneration of behavioural repertoires has mainly been limited to simulation\nenvironments instead of real-world learning. This is because existing QD\nalgorithms need large numbers of evaluations as well as episodic resets, which\nrequire manual human supervision and interventions. This paper proposes\nReset-Free Quality-Diversity optimization (RF-QD) as a step towards autonomous\nlearning for robotics in open-ended environments. We build on Dynamics-Aware\nQuality-Diversity (DA-QD) and introduce a behaviour selection policy that\nleverages the diversity of the imagined repertoire and environmental\ninformation to intelligently select of behaviours that can act as automatic\nresets. We demonstrate this through a task of learning to walk within defined\ntraining zones with obstacles. Our experiments show that we can learn full\nrepertoires of legged locomotion controllers autonomously without manual resets\nwith high sample efficiency in spite of harsh safety constraints. Finally,\nusing an ablation of different target objectives, we show that it is important\nfor RF-QD to have diverse types solutions available for the behaviour selection\npolicy over solutions optimised with a specific objective. Videos and code\navailable at https://sites.google.com/view/rf-qd.\n",
        "published": "2022-04-07",
        "authors": [
            "Bryan Lim",
            "Alexander Reichenbach",
            "Antoine Cully"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.05726v1",
        "title": "Hierarchical Quality-Diversity for Online Damage Recovery",
        "abstract": "  Adaptation capabilities, like damage recovery, are crucial for the deployment\nof robots in complex environments. Several works have demonstrated that using\nrepertoires of pre-trained skills can enable robots to adapt to unforeseen\nmechanical damages in a few minutes. These adaptation capabilities are directly\nlinked to the behavioural diversity in the repertoire. The more alternatives\nthe robot has to execute a skill, the better are the chances that it can adapt\nto a new situation. However, solving complex tasks, like maze navigation,\nusually requires multiple different skills. Finding a large behavioural\ndiversity for these multiple skills often leads to an intractable exponential\ngrowth of the number of required solutions. In this paper, we introduce the\nHierarchical Trial and Error algorithm, which uses a hierarchical behavioural\nrepertoire to learn diverse skills and leverages them to make the robot more\nadaptive to different situations. We show that the hierarchical decomposition\nof skills enables the robot to learn more complex behaviours while keeping the\nlearning of the repertoire tractable. The experiments with a hexapod robot show\nthat our method solves maze navigation tasks with 20% less actions in the most\nchallenging scenarios than the best baseline while having 57% less complete\nfailures.\n",
        "published": "2022-04-12",
        "authors": [
            "Maxime Allard",
            "Sim\u00f3n C. Smith",
            "Konstantinos Chatzilygeroudis",
            "Antoine Cully"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.09828v1",
        "title": "Relevance-guided Unsupervised Discovery of Abilities with\n  Quality-Diversity Algorithms",
        "abstract": "  Quality-Diversity algorithms provide efficient mechanisms to generate large\ncollections of diverse and high-performing solutions, which have shown to be\ninstrumental for solving downstream tasks. However, most of those algorithms\nrely on a behavioural descriptor to characterise the diversity that is\nhand-coded, hence requiring prior knowledge about the considered tasks. In this\nwork, we introduce Relevance-guided Unsupervised Discovery of Abilities; a\nQuality-Diversity algorithm that autonomously finds a behavioural\ncharacterisation tailored to the task at hand. In particular, our method\nintroduces a custom diversity metric that leads to higher densities of\nsolutions near the areas of interest in the learnt behavioural descriptor\nspace. We evaluate our approach on a simulated robotic environment, where the\nrobot has to autonomously discover its abilities based on its full sensory\ndata. We evaluated the algorithms on three tasks: navigation to random targets,\nmoving forward with a high velocity, and performing half-rolls. The\nexperimental results show that our method manages to discover collections of\nsolutions that are not only diverse, but also well-adapted to the considered\ndownstream task.\n",
        "published": "2022-04-21",
        "authors": [
            "Luca Grillotti",
            "Antoine Cully"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.01080v1",
        "title": "Event-Driven Tactile Learning with Location Spiking Neurons",
        "abstract": "  The sense of touch is essential for a variety of daily tasks. New advances in\nevent-based tactile sensors and Spiking Neural Networks (SNNs) spur the\nresearch in event-driven tactile learning. However, SNN-enabled event-driven\ntactile learning is still in its infancy due to the limited representative\nabilities of existing spiking neurons and high spatio-temporal complexity in\nthe data. In this paper, to improve the representative capabilities of existing\nspiking neurons, we propose a novel neuron model called \"location spiking\nneuron\", which enables us to extract features of event-based data in a novel\nway. Moreover, based on the classical Time Spike Response Model (TSRM), we\ndevelop a specific location spiking neuron model - Location Spike Response\nModel (LSRM) that serves as a new building block of SNNs. Furthermore, we\npropose a hybrid model which combines an SNN with TSRM neurons and an SNN with\nLSRM neurons to capture the complex spatio-temporal dependencies in the data.\nExtensive experiments demonstrate the significant improvements of our models\nover other works on event-driven tactile learning and show the superior energy\nefficiency of our models and location spiking neurons, which may unlock their\npotential on neuromorphic hardware.\n",
        "published": "2022-07-23",
        "authors": [
            "Peng Kang",
            "Srutarshi Banerjee",
            "Henry Chopp",
            "Aggelos Katsaggelos",
            "Oliver Cossairt"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.09007v1",
        "title": "Comparative Study of Q-Learning and NeuroEvolution of Augmenting\n  Topologies for Self Driving Agents",
        "abstract": "  Autonomous driving vehicles have been of keen interest ever since automation\nof various tasks started. Humans are prone to exhaustion and have a slow\nresponse time on the road, and on top of that driving is already quite a\ndangerous task with around 1.35 million road traffic incident deaths each year.\nIt is expected that autonomous driving can reduce the number of driving\naccidents around the world which is why this problem has been of keen interest\nfor researchers. Currently, self-driving vehicles use different algorithms for\nvarious sub-problems in making the vehicle autonomous. We will focus\nreinforcement learning algorithms, more specifically Q-learning algorithms and\nNeuroEvolution of Augment Topologies (NEAT), a combination of evolutionary\nalgorithms and artificial neural networks, to train a model agent to learn how\nto drive on a given path. This paper will focus on drawing a comparison between\nthe two aforementioned algorithms.\n",
        "published": "2022-09-19",
        "authors": [
            "Arhum Ishtiaq",
            "Maheen Anees",
            "Sara Mahmood",
            "Neha Jafry"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    }
]