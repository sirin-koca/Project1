[
    {
        "id": "http://arxiv.org/abs/2306.09686v1",
        "title": "Collapsed Inference for Bayesian Deep Learning",
        "abstract": "  Bayesian neural networks (BNNs) provide a formalism to quantify and calibrate\nuncertainty in deep learning. Current inference approaches for BNNs often\nresort to few-sample estimation for scalability, which can harm predictive\nperformance, while its alternatives tend to be computationally prohibitively\nexpensive. We tackle this challenge by revealing a previously unseen connection\nbetween inference on BNNs and volume computation problems. With this\nobservation, we introduce a novel collapsed inference scheme that performs\nBayesian model averaging using collapsed samples. It improves over a\nMonte-Carlo sample by limiting sampling to a subset of the network weights\nwhile pairing it with some closed-form conditional distribution over the rest.\nA collapsed sample represents uncountably many models drawn from the\napproximate posterior and thus yields higher sample efficiency. Further, we\nshow that the marginalization of a collapsed sample can be solved analytically\nand efficiently despite the non-linearity of neural networks by leveraging\nexisting volume computation solvers. Our proposed use of collapsed samples\nachieves a balance between scalability and accuracy. On various regression and\nclassification tasks, our collapsed Bayesian deep learning approach\ndemonstrates significant improvements over existing methods and sets a new\nstate of the art in terms of uncertainty estimation as well as predictive\nperformance.\n",
        "published": "2023",
        "authors": [
            "Zhe Zeng",
            "Guy Van den Broeck"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.11040v1",
        "title": "Application of Deep Learning for Predictive Maintenance of Oilfield\n  Equipment",
        "abstract": "  This thesis explored applications of the new emerging techniques of\nartificial intelligence and deep learning (neural networks in particular) for\npredictive maintenance, diagnostics and prognostics. Many neural architectures\nsuch as fully-connected, convolutional and recurrent neural networks were\ndeveloped and tested on public datasets such as NASA C-MAPSS, Case Western\nReserve University Bearings and FEMTO Bearings datasets to diagnose equipment\nhealth state and/or predict the remaining useful life (RUL) before breakdown.\nMany data processing and feature extraction procedures were used in combination\nwith deep learning techniques such as dimensionality reduction (Principal\nComponent Analysis) and signal processing (Fourier and Wavelet analyses) in\norder to create more meaningful and robust features to use as an input for\nneural networks architectures. This thesis also explored the potential use of\nthese techniques in predictive maintenance within oil rigs for monitoring\noilfield critical equipment in order to reduce unpredicted downtime and\nmaintenance costs.\n",
        "published": "2023",
        "authors": [
            "Abdeldjalil Latrach"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.14789v1",
        "title": "Segmentation of Industrial Burner Flames: A Comparative Study from\n  Traditional Image Processing to Machine and Deep Learning",
        "abstract": "  In many industrial processes, such as power generation, chemical production,\nand waste management, accurately monitoring industrial burner flame\ncharacteristics is crucial for safe and efficient operation. A key step\ninvolves separating the flames from the background through binary segmentation.\nDecades of machine vision research have produced a wide range of possible\nsolutions, from traditional image processing to traditional machine learning\nand modern deep learning methods. In this work, we present a comparative study\nof multiple segmentation approaches, namely Global Thresholding, Region\nGrowing, Support Vector Machines, Random Forest, Multilayer Perceptron, U-Net,\nand DeepLabV3+, that are evaluated on a public benchmark dataset of industrial\nburner flames. We provide helpful insights and guidance for researchers and\npractitioners aiming to select an appropriate approach for the binary\nsegmentation of industrial burner flames and beyond. For the highest accuracy,\ndeep learning is the leading approach, while for fast and simple solutions,\ntraditional image processing techniques remain a viable option.\n",
        "published": "2023",
        "authors": [
            "Steven Landgraf",
            "Markus Hillemann",
            "Moritz Aberle",
            "Valentin Jung",
            "Markus Ulrich"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.12526v2",
        "title": "Neural Network Pruning by Gradient Descent",
        "abstract": "  The rapid increase in the parameters of deep learning models has led to\nsignificant costs, challenging computational efficiency and model\ninterpretability. In this paper, we introduce a novel and straightforward\nneural network pruning framework that incorporates the Gumbel-Softmax\ntechnique. This framework enables the simultaneous optimization of a network's\nweights and topology in an end-to-end process using stochastic gradient\ndescent. Empirical results demonstrate its exceptional compression capability,\nmaintaining high accuracy on the MNIST dataset with only 0.15\\% of the original\nnetwork parameters. Moreover, our framework enhances neural network\ninterpretability, not only by allowing easy extraction of feature importance\ndirectly from the pruned network but also by enabling visualization of feature\nsymmetry and the pathways of information propagation from features to outcomes.\nAlthough the pruning strategy is learned through deep learning, it is\nsurprisingly intuitive and understandable, focusing on selecting key\nrepresentative features and exploiting data patterns to achieve extreme sparse\npruning. We believe our method opens a promising new avenue for deep learning\npruning and the creation of interpretable machine learning systems.\n",
        "published": "2023",
        "authors": [
            "Zhang Zhang",
            "Ruyi Tao",
            "Jiang Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.13333v1",
        "title": "Responsible Deep Learning for Software as a Medical Device",
        "abstract": "  Tools, models and statistical methods for signal processing and medical image\nanalysis and training deep learning models to create research prototypes for\neventual clinical applications are of special interest to the biomedical\nimaging community. But material and optical properties of biological tissues\nare complex and not easily captured by imaging devices. Added complexity can be\nintroduced by datasets with underrepresentation of medical images from races\nand ethnicities for deep learning, and limited knowledge about the regulatory\nframework needed for commercialization and safety of emerging Artificial\nIntelligence (AI) and Machine Learning (ML) technologies for medical image\nanalysis. This extended version of the workshop paper presented at the special\nsession of the 2022 IEEE 19th International Symposium on Biomedical Imaging,\ndescribes strategy and opportunities by University of California professors\nengaged in machine learning (section I) and clinical research (section II), the\nOffice of Science and Engineering Laboratories (OSEL) section III, and\nofficials at the US FDA in Center for Devices & Radiological Health (CDRH)\nsection IV. Performance evaluations of AI/ML models of skin (RGB), tissue\nbiopsy (digital pathology), and lungs and kidneys (Magnetic Resonance, X-ray,\nComputed Tomography) medical images for regulatory evaluations and real-world\ndeployment are discussed.\n",
        "published": "2023",
        "authors": [
            "Pratik Shah",
            "Jenna Lester",
            "Jana G Deflino",
            "Vinay Pai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.09479v1",
        "title": "Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep\n  Learning",
        "abstract": "  The risk of hardware Trojans being inserted at various stages of chip\nproduction has increased in a zero-trust fabless era. To counter this, various\nmachine learning solutions have been developed for the detection of hardware\nTrojans. While most of the focus has been on either a statistical or deep\nlearning approach, the limited number of Trojan-infected benchmarks affects the\ndetection accuracy and restricts the possibility of detecting zero-day Trojans.\nTo close the gap, we first employ generative adversarial networks to amplify\nour data in two alternative representation modalities, a graph and a tabular,\nensuring that the dataset is distributed in a representative manner. Further,\nwe propose a multimodal deep learning approach to detect hardware Trojans and\nevaluate the results from both early fusion and late fusion strategies. We also\nestimate the uncertainty quantification metrics of each prediction for\nrisk-aware decision-making. The outcomes not only confirms the efficacy of our\nproposed hardware Trojan detection method but also opens a new door for future\nstudies employing multimodality and uncertainty quantification to address other\nhardware security challenges.\n",
        "published": "2024",
        "authors": [
            "Rahul Vishwakarma",
            "Amin Rezaei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1704.01407v3",
        "title": "Embodied Artificial Intelligence through Distributed Adaptive Control:\n  An Integrated Framework",
        "abstract": "  In this paper, we argue that the future of Artificial Intelligence research\nresides in two keywords: integration and embodiment. We support this claim by\nanalyzing the recent advances of the field. Regarding integration, we note that\nthe most impactful recent contributions have been made possible through the\nintegration of recent Machine Learning methods (based in particular on Deep\nLearning and Recurrent Neural Networks) with more traditional ones (e.g.\nMonte-Carlo tree search, goal babbling exploration or addressable memory\nsystems). Regarding embodiment, we note that the traditional benchmark tasks\n(e.g. visual classification or board games) are becoming obsolete as\nstate-of-the-art learning algorithms approach or even surpass human performance\nin most of them, having recently encouraged the development of first-person 3D\ngame platforms embedding realistic physics. Building upon this analysis, we\nfirst propose an embodied cognitive architecture integrating heterogenous\nsub-fields of Artificial Intelligence into a unified framework. We demonstrate\nthe utility of our approach by showing how major contributions of the field can\nbe expressed within the proposed framework. We then claim that benchmarking\nenvironments need to reproduce ecologically-valid conditions for bootstrapping\nthe acquisition of increasingly complex cognitive skills through the concept of\na cognitive arms race between embodied agents.\n",
        "published": "2017",
        "authors": [
            "Cl\u00e9ment Moulin-Frier",
            "Jordi-Ysard Puigb\u00f2",
            "Xerxes D. Arsiwalla",
            "Mart\u00ec Sanchez-Fibla",
            "Paul F. M. J. Verschure"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.12770v1",
        "title": "Malware Detection and Prevention using Artificial Intelligence\n  Techniques",
        "abstract": "  With the rapid technological advancement, security has become a major issue\ndue to the increase in malware activity that poses a serious threat to the\nsecurity and safety of both computer systems and stakeholders. To maintain\nstakeholders, particularly, end users security, protecting the data from\nfraudulent efforts is one of the most pressing concerns. A set of malicious\nprogramming code, scripts, active content, or intrusive software that is\ndesigned to destroy intended computer systems and programs or mobile and web\napplications is referred to as malware. According to a study, naive users are\nunable to distinguish between malicious and benign applications. Thus, computer\nsystems and mobile applications should be designed to detect malicious\nactivities towards protecting the stakeholders. A number of algorithms are\navailable to detect malware activities by utilizing novel concepts including\nArtificial Intelligence, Machine Learning, and Deep Learning. In this study, we\nemphasize Artificial Intelligence (AI) based techniques for detecting and\npreventing malware activity. We present a detailed review of current malware\ndetection technologies, their shortcomings, and ways to improve efficiency. Our\nstudy shows that adopting futuristic approaches for the development of malware\ndetection applications shall provide significant advantages. The comprehension\nof this synthesis shall help researchers for further research on malware\ndetection and prevention using AI.\n",
        "published": "2022",
        "authors": [
            "Md Jobair Hossain Faruk",
            "Hossain Shahriar",
            "Maria Valero",
            "Farhat Lamia Barsha",
            "Shahriar Sobhan",
            "Md Abdullah Khan",
            "Michael Whitman",
            "Alfredo Cuzzocreak",
            "Dan Lo",
            "Akond Rahman",
            "Fan Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.06410v2",
        "title": "Beneficial and Harmful Explanatory Machine Learning",
        "abstract": "  Given the recent successes of Deep Learning in AI there has been increased\ninterest in the role and need for explanations in machine learned theories. A\ndistinct notion in this context is that of Michie's definition of Ultra-Strong\nMachine Learning (USML). USML is demonstrated by a measurable increase in human\nperformance of a task following provision to the human of a symbolic machine\nlearned theory for task performance. A recent paper demonstrates the beneficial\neffect of a machine learned logic theory for a classification task, yet no\nexisting work to our knowledge has examined the potential harmfulness of\nmachine's involvement for human comprehension during learning. This paper\ninvestigates the explanatory effects of a machine learned theory in the context\nof simple two person games and proposes a framework for identifying the\nharmfulness of machine explanations based on the Cognitive Science literature.\nThe approach involves a cognitive window consisting of two quantifiable bounds\nand it is supported by empirical evidence collected from human trials. Our\nquantitative and qualitative results indicate that human learning aided by a\nsymbolic machine learned theory which satisfies a cognitive window has achieved\nsignificantly higher performance than human self learning. Results also\ndemonstrate that human learning aided by a symbolic machine learned theory that\nfails to satisfy this window leads to significantly worse performance than\nunaided human learning.\n",
        "published": "2020",
        "authors": [
            "Lun Ai",
            "Stephen H. Muggleton",
            "C\u00e9line Hocquette",
            "Mark Gromowski",
            "Ute Schmid"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.01681v1",
        "title": "VulDeePecker: A Deep Learning-Based System for Vulnerability Detection",
        "abstract": "  The automatic detection of software vulnerabilities is an important research\nproblem. However, existing solutions to this problem rely on human experts to\ndefine features and often miss many vulnerabilities (i.e., incurring high false\nnegative rate). In this paper, we initiate the study of using deep\nlearning-based vulnerability detection to relieve human experts from the\ntedious and subjective task of manually defining features. Since deep learning\nis motivated to deal with problems that are very different from the problem of\nvulnerability detection, we need some guiding principles for applying deep\nlearning to vulnerability detection. In particular, we need to find\nrepresentations of software programs that are suitable for deep learning. For\nthis purpose, we propose using code gadgets to represent programs and then\ntransform them into vectors, where a code gadget is a number of (not\nnecessarily consecutive) lines of code that are semantically related to each\nother. This leads to the design and implementation of a deep learning-based\nvulnerability detection system, called Vulnerability Deep Pecker\n(VulDeePecker). In order to evaluate VulDeePecker, we present the first\nvulnerability dataset for deep learning approaches. Experimental results show\nthat VulDeePecker can achieve much fewer false negatives (with reasonable false\npositives) than other approaches. We further apply VulDeePecker to 3 software\nproducts (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which\nare not reported in the National Vulnerability Database but were \"silently\"\npatched by the vendors when releasing later versions of these products; in\ncontrast, these vulnerabilities are almost entirely missed by the other\nvulnerability detection systems we experimented with.\n",
        "published": "2018",
        "authors": [
            "Zhen Li",
            "Deqing Zou",
            "Shouhuai Xu",
            "Xinyu Ou",
            "Hai Jin",
            "Sujuan Wang",
            "Zhijun Deng",
            "Yuyi Zhong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.05457v1",
        "title": "Solutions to problems with deep learning",
        "abstract": "  Despite the several successes of deep learning systems, there are concerns\nabout their limitations, discussed most recently by Gary Marcus. This paper\ndiscusses Marcus's concerns and some others, together with solutions to several\nof these problems provided by the \"P theory of intelligence\" and its\nrealisation in the \"SP computer model\". The main advantages of the SP system\nare: relatively small requirements for data and the ability to learn from a\nsingle experience; the ability to model both hierarchical and non-hierarchical\nstructures; strengths in several kinds of reasoning, including `commonsense'\nreasoning; transparency in the representation of knowledge, and the provision\nof an audit trail for all processing; the likelihood that the SP system could\nnot be fooled into bizarre or eccentric recognition of stimuli, as deep\nlearning systems can be; the SP system provides a robust solution to the\nproblem of `catastrophic forgetting' in deep learning systems; the SP system\nprovides a theoretically-coherent solution to the problems of correcting over-\nand under-generalisations in learning, and learning correct structures despite\nerrors in data; unlike most research on deep learning, the SP programme of\nresearch draws extensively on research on human learning, perception, and\ncognition; and the SP programme of research has an overarching theory,\nsupported by evidence, something that is largely missing from research on deep\nlearning. In general, the SP system provides a much firmer foundation than deep\nlearning for the development of artificial general intelligence.\n",
        "published": "2018",
        "authors": [
            "J Gerard Wolff"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.10541v1",
        "title": "Online Learning for Orchestration of Inference in Multi-User\n  End-Edge-Cloud Networks",
        "abstract": "  Deep-learning-based intelligent services have become prevalent in\ncyber-physical applications including smart cities and health-care. Deploying\ndeep-learning-based intelligence near the end-user enhances privacy protection,\nresponsiveness, and reliability. Resource-constrained end-devices must be\ncarefully managed in order to meet the latency and energy requirements of\ncomputationally-intensive deep learning services. Collaborative end-edge-cloud\ncomputing for deep learning provides a range of performance and efficiency that\ncan address application requirements through computation offloading. The\ndecision to offload computation is a communication-computation co-optimization\nproblem that varies with both system parameters (e.g., network condition) and\nworkload characteristics (e.g., inputs). On the other hand, deep learning model\noptimization provides another source of tradeoff between latency and model\naccuracy. An end-to-end decision-making solution that considers such\ncomputation-communication problem is required to synergistically find the\noptimal offloading policy and model for deep learning services. To this end, we\npropose a reinforcement-learning-based computation offloading solution that\nlearns optimal offloading policy considering deep learning model selection\ntechniques to minimize response time while providing sufficient accuracy. We\ndemonstrate the effectiveness of our solution for edge devices in an\nend-edge-cloud system and evaluate with a real-setup implementation using\nmultiple AWS and ARM core configurations. Our solution provides 35% speedup in\nthe average response time compared to the state-of-the-art with less than 0.9%\naccuracy reduction, demonstrating the promise of our online learning framework\nfor orchestrating DL inference in end-edge-cloud systems.\n",
        "published": "2022",
        "authors": [
            "Sina Shahhosseini",
            "Dongjoo Seo",
            "Anil Kanduri",
            "Tianyi Hu",
            "Sung-soo Lim",
            "Bryan Donyanavard",
            "Amir M. Rahmani",
            "Nikil Dutt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.01961v1",
        "title": "Insights into Fairness through Trust: Multi-scale Trust Quantification\n  for Financial Deep Learning",
        "abstract": "  The success of deep learning in recent years have led to a significant\nincrease in interest and prevalence for its adoption to tackle financial\nservices tasks. One particular question that often arises as a barrier to\nadopting deep learning for financial services is whether the developed\nfinancial deep learning models are fair in their predictions, particularly in\nlight of strong governance and regulatory compliance requirements in the\nfinancial services industry. A fundamental aspect of fairness that has not been\nexplored in financial deep learning is the concept of trust, whose variations\nmay point to an egocentric view of fairness and thus provide insights into the\nfairness of models. In this study we explore the feasibility and utility of a\nmulti-scale trust quantification strategy to gain insights into the fairness of\na financial deep learning model, particularly under different scenarios at\ndifferent scales. More specifically, we conduct multi-scale trust\nquantification on a deep neural network for the purpose of credit card default\nprediction to study: 1) the overall trustworthiness of the model 2) the trust\nlevel under all possible prediction-truth relationships, 3) the trust level\nacross the spectrum of possible predictions, 4) the trust level across\ndifferent demographic groups (e.g., age, gender, and education), and 5)\ndistribution of overall trust for an individual prediction scenario. The\ninsights for this proof-of-concept study demonstrate that such a multi-scale\ntrust quantification strategy may be helpful for data scientists and regulators\nin financial services as part of the verification and certification of\nfinancial deep learning solutions to gain insights into fairness and trust of\nthese solutions.\n",
        "published": "2020",
        "authors": [
            "Alexander Wong",
            "Andrew Hryniowski",
            "Xiao Yu Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.00009v2",
        "title": "Deep-Learning Discovers Macroscopic Governing Equations for Viscous\n  Gravity Currents from Microscopic Simulation Data",
        "abstract": "  Although deep-learning has been successfully applied in a variety of science\nand engineering problems owing to its strong high-dimensional nonlinear mapping\ncapability, it is of limited use in scientific knowledge discovery. In this\nwork, we propose a deep-learning based framework to discover the macroscopic\ngoverning equation of viscous gravity current based on high-resolution\nmicroscopic simulation data without the need for prior knowledge of underlying\nterms. For two typical scenarios with different viscosity ratios, the\ndeep-learning based equations exactly capture the same dominated terms as the\ntheoretically derived equations for describing long-term asymptotic behaviors,\nwhich validates the proposed framework. Unknown macroscopic equations are then\nobtained for describing short-term behaviors, and additional deep-learned\ncompensation terms are eventually discovered. Comparison of posterior tests\nshows that the deep-learning based PDEs actually perform better than the\ntheoretically derived PDEs in predicting evolving viscous gravity currents for\nboth long-term and short-term regimes. Moreover, the proposed framework is\nproven to be very robust against non-biased data noise for training, which is\nup to 20%. Consequently, the presented deep-learning framework shows\nconsiderable potential for discovering unrevealed intrinsic laws in scientific\nsemantic space from raw experimental or simulation results in data space.\n",
        "published": "2021",
        "authors": [
            "Junsheng Zeng",
            "Hao Xu",
            "Yuntian Chen",
            "Dongxiao Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.13233v1",
        "title": "Genetic Programming-Based Evolutionary Deep Learning for Data-Efficient\n  Image Classification",
        "abstract": "  Data-efficient image classification is a challenging task that aims to solve\nimage classification using small training data. Neural network-based deep\nlearning methods are effective for image classification, but they typically\nrequire large-scale training data and have major limitations such as requiring\nexpertise to design network architectures and having poor interpretability.\nEvolutionary deep learning is a recent hot topic that combines evolutionary\ncomputation with deep learning. However, most evolutionary deep learning\nmethods focus on evolving architectures of neural networks, which still suffer\nfrom limitations such as poor interpretability. To address this, this paper\nproposes a new genetic programming-based evolutionary deep learning approach to\ndata-efficient image classification. The new approach can automatically evolve\nvariable-length models using many important operators from both image and\nclassification domains. It can learn different types of image features from\ncolour or gray-scale images, and construct effective and diverse ensembles for\nimage classification. A flexible multi-layer representation enables the new\napproach to automatically construct shallow or deep models/trees for different\ntasks and perform effective transformations on the input data via multiple\ninternal nodes. The new approach is applied to solve five image classification\ntasks with different training set sizes. The results show that it achieves\nbetter performance in most cases than deep learning methods for data-efficient\nimage classification. A deep analysis shows that the new approach has good\nconvergence and evolves models with high interpretability, different\nlengths/sizes/shapes, and good transferability.\n",
        "published": "2022",
        "authors": [
            "Ying Bi",
            "Bing Xue",
            "Mengjie Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.00017v1",
        "title": "Knowledge-augmented Deep Learning and Its Applications: A Survey",
        "abstract": "  Deep learning models, though having achieved great success in many different\nfields over the past years, are usually data hungry, fail to perform well on\nunseen samples, and lack of interpretability. Various prior knowledge often\nexists in the target domain and their use can alleviate the deficiencies with\ndeep learning. To better mimic the behavior of human brains, different advanced\nmethods have been proposed to identify domain knowledge and integrate it into\ndeep models for data-efficient, generalizable, and interpretable deep learning,\nwhich we refer to as knowledge-augmented deep learning (KADL). In this survey,\nwe define the concept of KADL, and introduce its three major tasks, i.e.,\nknowledge identification, knowledge representation, and knowledge integration.\nDifferent from existing surveys that are focused on a specific type of\nknowledge, we provide a broad and complete taxonomy of domain knowledge and its\nrepresentations. Based on our taxonomy, we provide a systematic review of\nexisting techniques, different from existing works that survey integration\napproaches agnostic to taxonomy of knowledge. This survey subsumes existing\nworks and offers a bird's-eye view of research in the general area of\nknowledge-augmented deep learning. The thorough and critical reviews of\nnumerous papers help not only understand current progresses but also identify\nfuture directions for the research on knowledge-augmented deep learning.\n",
        "published": "2022",
        "authors": [
            "Zijun Cui",
            "Tian Gao",
            "Kartik Talamadupula",
            "Qiang Ji"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.13039v1",
        "title": "Optimizing Deep Learning Models For Raspberry Pi",
        "abstract": "  Deep learning models have become increasingly popular for a wide range of\napplications, including computer vision, natural language processing, and\nspeech recognition. However, these models typically require large amounts of\ncomputational resources, making them challenging to run on low-power devices\nsuch as the Raspberry Pi. One approach to addressing this challenge is to use\npruning techniques to reduce the size of the deep learning models. Pruning\ninvolves removing unimportant weights and connections from the model, resulting\nin a smaller and more efficient model. Pruning can be done during training or\nafter the model has been trained. Another approach is to optimize the deep\nlearning models specifically for the Raspberry Pi architecture. This can\ninclude optimizing the model's architecture and parameters to take advantage of\nthe Raspberry Pi's hardware capabilities, such as its CPU and GPU.\nAdditionally, the model can be optimized for energy efficiency by minimizing\nthe amount of computation required. Pruning and optimizing deep learning models\nfor the Raspberry Pi can help overcome the computational and energy constraints\nof low-power devices, making it possible to run deep learning models on a wider\nrange of devices. In the following sections, we will explore these approaches\nin more detail and discuss their effectiveness for optimizing deep learning\nmodels for the Raspberry Pi.\n",
        "published": "2023",
        "authors": [
            "Salem Ameen",
            "Kangaranmulle Siriwardana",
            "Theo Theodoridis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.03601v1",
        "title": "Artificial Intelligence in Governance, Risk and Compliance: Results of a\n  study on potentials for the application of artificial intelligence (AI) in\n  governance, risk and compliance (GRC)",
        "abstract": "  The digital transformation leads to fundamental change in organizational\nstructures. To be able to apply new technologies not only selectively,\nprocesses in companies must be revised and functional units must be viewed\nholistically, especially with regard to interfaces. Target-oriented management\ndecisions are made, among other things, on the basis of risk management and\ncompliance in combination with the internal control system as governance\nfunctions. The effectiveness and efficiency of these functions is decisive to\nfollow guidelines and regulatory requirements as well as for the evaluation of\nalternative options for acting with regard to activities of companies. GRC\n(Governance, Risk and Compliance) means an integrated governance-approach, in\nwhich the mentioned governance functions are interlinked and not separated from\neach other. Methods of artificial intelligence represents an important\ntechnology of digital transformation. This technology, which offers a broad\nrange of methods such as machine learning, artificial neural networks, natural\nlanguage processing or deep learning, offers a lot of possible applications in\nmany business areas from purchasing to production or customer service.\nArtificial intelligence is also being used in GRC, for example for processing\nand analysis of unstructured data sets. This study contains the results of a\nsurvey conducted in 2021 to identify and analyze the potential applications of\nartificial intelligence in GRC.\n",
        "published": "2022",
        "authors": [
            "Eva Ponick",
            "Gabriele Wieczorek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.01396v1",
        "title": "Artificial Intelligence and its Role in Near Future",
        "abstract": "  AI technology has a long history which is actively and constantly changing\nand growing. It focuses on intelligent agents, which contain devices that\nperceive the environment and based on which takes actions in order to maximize\ngoal success chances. In this paper, we will explain the modern AI basics and\nvarious representative applications of AI. In the context of the modern\ndigitalized world, AI is the property of machines, computer programs, and\nsystems to perform the intellectual and creative functions of a person,\nindependently find ways to solve problems, be able to draw conclusions and make\ndecisions. Most artificial intelligence systems have the ability to learn,\nwhich allows people to improve their performance over time. The recent research\non AI tools, including machine learning, deep learning and predictive analysis\nintended toward increasing the planning, learning, reasoning, thinking and\naction taking ability. Based on which, the proposed research intends towards\nexploring on how the human intelligence differs from the artificial\nintelligence. Moreover, we critically analyze what AI of today is capable of\ndoing, why it still cannot reach human intelligence and what are the open\nchallenges existing in front of AI to reach and outperform human level of\nintelligence. Furthermore, it will explore the future predictions for\nartificial intelligence and based on which potential solution will be\nrecommended to solve it within next decades.\n",
        "published": "2018",
        "authors": [
            "Jahanzaib Shabbir",
            "Tarique Anwer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.00880v1",
        "title": "Introducing Fuzzy Layers for Deep Learning",
        "abstract": "  Many state-of-the-art technologies developed in recent years have been\ninfluenced by machine learning to some extent. Most popular at the time of this\nwriting are artificial intelligence methodologies that fall under the umbrella\nof deep learning. Deep learning has been shown across many applications to be\nextremely powerful and capable of handling problems that possess great\ncomplexity and difficulty. In this work, we introduce a new layer to deep\nlearning: the fuzzy layer. Traditionally, the network architecture of neural\nnetworks is composed of an input layer, some combination of hidden layers, and\nan output layer. We propose the introduction of fuzzy layers into the deep\nlearning architecture to exploit the powerful aggregation properties expressed\nthrough fuzzy methodologies, such as the Choquet and Sugueno fuzzy integrals.\nTo date, fuzzy approaches taken to deep learning have been through the\napplication of various fusion strategies at the decision level to aggregate\noutputs from state-of-the-art pre-trained models, e.g., AlexNet, VGG16,\nGoogLeNet, Inception-v3, ResNet-18, etc. While these strategies have been shown\nto improve accuracy performance for image classification tasks, none have\nexplored the use of fuzzified intermediate, or hidden, layers. Herein, we\npresent a new deep learning strategy that incorporates fuzzy strategies into\nthe deep learning architecture focused on the application of semantic\nsegmentation using per-pixel classification. Experiments are conducted on a\nbenchmark data set as well as a data set collected via an unmanned aerial\nsystem at a U.S. Army test site for the task of automatic road segmentation,\nand preliminary results are promising.\n",
        "published": "2020",
        "authors": [
            "Stanton R. Price",
            "Steven R. Price",
            "Derek T. Anderson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.13146v1",
        "title": "Deep Learning Techniques for Geospatial Data Analysis",
        "abstract": "  Consumer electronic devices such as mobile handsets, goods tagged with RFID\nlabels, location and position sensors are continuously generating a vast amount\nof location enriched data called geospatial data. Conventionally such\ngeospatial data is used for military applications. In recent times, many useful\ncivilian applications have been designed and deployed around such geospatial\ndata. For example, a recommendation system to suggest restaurants or places of\nattraction to a tourist visiting a particular locality. At the same time, civic\nbodies are harnessing geospatial data generated through remote sensing devices\nto provide better services to citizens such as traffic monitoring, pothole\nidentification, and weather reporting. Typically such applications are\nleveraged upon non-hierarchical machine learning techniques such as Naive-Bayes\nClassifiers, Support Vector Machines, and decision trees. Recent advances in\nthe field of deep-learning showed that Neural Network-based techniques\noutperform conventional techniques and provide effective solutions for many\ngeospatial data analysis tasks such as object recognition, image\nclassification, and scene understanding. The chapter presents a survey on the\ncurrent state of the applications of deep learning techniques for analyzing\ngeospatial data.\n  The chapter is organized as below: (i) A brief overview of deep learning\nalgorithms. (ii)Geospatial Analysis: a Data Science Perspective (iii)\nDeep-learning techniques for Remote Sensing data analytics tasks (iv)\nDeep-learning techniques for GPS data analytics(iv) Deep-learning techniques\nfor RFID data analytics.\n",
        "published": "2020",
        "authors": [
            "Arvind W. Kiwelekar",
            "Geetanjali S. Mahamunkar",
            "Laxman D. Netak",
            "Valmik B Nikam"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.13931v1",
        "title": "Comparative Study of Predicting Stock Index Using Deep Learning Models",
        "abstract": "  Time series forecasting has seen many methods attempted over the past few\ndecades, including traditional technical analysis, algorithmic statistical\nmodels, and more recent machine learning and artificial intelligence\napproaches. Recently, neural networks have been incorporated into the\nforecasting scenario, such as the LSTM and conventional RNN approaches, which\nutilize short-term and long-term dependencies. This study evaluates traditional\nforecasting methods, such as ARIMA, SARIMA, and SARIMAX, and newer neural\nnetwork approaches, such as DF-RNN, DSSM, and Deep AR, built using RNNs. The\nstandard NIFTY-50 dataset from Kaggle is used to assess these models using\nmetrics such as MSE, RMSE, MAPE, POCID, and Theil's U. Results show that Deep\nAR outperformed all other conventional deep learning and traditional\napproaches, with the lowest MAPE of 0.01 and RMSE of 189. Additionally, the\nperformance of Deep AR and GRU did not degrade when the amount of training data\nwas reduced, suggesting that these models may not require a large amount of\ndata to achieve consistent and reliable performance. The study demonstrates\nthat incorporating deep learning approaches in a forecasting scenario\nsignificantly outperforms conventional approaches and can handle complex\ndatasets, with potential applications in various domains, such as weather\npredictions and other time series applications in a real-world scenario.\n",
        "published": "2023",
        "authors": [
            "Harshal Patel",
            "Bharath Kumar Bolla",
            "Sabeesh E",
            "Dinesh Reddy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.06517v2",
        "title": "Wikipedia for Smart Machines and Double Deep Machine Learning",
        "abstract": "  Very important breakthroughs in data centric deep learning algorithms led to\nimpressive performance in transactional point applications of Artificial\nIntelligence (AI) such as Face Recognition, or EKG classification. With all due\nappreciation, however, knowledge blind data only machine learning algorithms\nhave severe limitations for non-transactional AI applications, such as medical\ndiagnosis beyond the EKG results. Such applications require deeper and broader\nknowledge in their problem solving capabilities, e.g. integrating anatomy and\nphysiology knowledge with EKG results and other patient findings. Following a\nreview and illustrations of such limitations for several real life AI\napplications, we point at ways to overcome them. The proposed Wikipedia for\nSmart Machines initiative aims at building repositories of software structures\nthat represent humanity science & technology knowledge in various parts of\nlife; knowledge that we all learn in schools, universities and during our\nprofessional life. Target readers for these repositories are smart machines;\nnot human. AI software developers will have these Reusable Knowledge structures\nreadily available, hence, the proposed name ReKopedia. Big Data is by now a\nmature technology, it is time to focus on Big Knowledge. Some will be derived\nfrom data, some will be obtained from mankind gigantic repository of knowledge.\nWikipedia for smart machines along with the new Double Deep Learning approach\noffer a paradigm for integrating datacentric deep learning algorithms with\nalgorithms that leverage deep knowledge, e.g. evidential reasoning and\ncausality reasoning. For illustration, a project is described to produce\nReKopedia knowledge modules for medical diagnosis of about 1,000 disorders.\nData is important, but knowledge deep, basic, and commonsense is equally\nimportant.\n",
        "published": "2017",
        "authors": [
            "Moshe BenBassat"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.11917v1",
        "title": "Adversarial Examples and the Deeper Riddle of Induction: The Need for a\n  Theory of Artifacts in Deep Learning",
        "abstract": "  Deep learning is currently the most widespread and successful technology in\nartificial intelligence. It promises to push the frontier of scientific\ndiscovery beyond current limits. However, skeptics have worried that deep\nneural networks are black boxes, and have called into question whether these\nadvances can really be deemed scientific progress if humans cannot understand\nthem. Relatedly, these systems also possess bewildering new vulnerabilities:\nmost notably a susceptibility to \"adversarial examples\". In this paper, I argue\nthat adversarial examples will become a flashpoint of debate in philosophy and\ndiverse sciences. Specifically, new findings concerning adversarial examples\nhave challenged the consensus view that the networks' verdicts on these cases\nare caused by overfitting idiosyncratic noise in the training set, and may\ninstead be the result of detecting predictively useful \"intrinsic features of\nthe data geometry\" that humans cannot perceive (Ilyas et al., 2019). These\nresults should cause us to re-examine responses to one of the deepest puzzles\nat the intersection of philosophy and science: Nelson Goodman's \"new riddle\" of\ninduction. Specifically, they raise the possibility that progress in a number\nof sciences will depend upon the detection and manipulation of useful features\nthat humans find inscrutable. Before we can evaluate this possibility, however,\nwe must decide which (if any) of these inscrutable features are real but\navailable only to \"alien\" perception and cognition, and which are distinctive\nartifacts of deep learning-for artifacts like lens flares or Gibbs phenomena\ncan be similarly useful for prediction, but are usually seen as obstacles to\nscientific theorizing. Thus, machine learning researchers urgently need to\ndevelop a theory of artifacts for deep neural networks, and I conclude by\nsketching some initial directions for this area of research.\n",
        "published": "2020",
        "authors": [
            "Cameron Buckner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.15581v1",
        "title": "The De-democratization of AI: Deep Learning and the Compute Divide in\n  Artificial Intelligence Research",
        "abstract": "  Increasingly, modern Artificial Intelligence (AI) research has become more\ncomputationally intensive. However, a growing concern is that due to unequal\naccess to computing power, only certain firms and elite universities have\nadvantages in modern AI research. Using a novel dataset of 171394 papers from\n57 prestigious computer science conferences, we document that firms, in\nparticular, large technology firms and elite universities have increased\nparticipation in major AI conferences since deep learning's unanticipated rise\nin 2012. The effect is concentrated among elite universities, which are ranked\n1-50 in the QS World University Rankings. Further, we find two strategies\nthrough which firms increased their presence in AI research: first, they have\nincreased firm-only publications; and second, firms are collaborating primarily\nwith elite universities. Consequently, this increased presence of firms and\nelite universities in AI research has crowded out mid-tier (QS ranked 201-300)\nand lower-tier (QS ranked 301-500) universities. To provide causal evidence\nthat deep learning's unanticipated rise resulted in this divergence, we\nleverage the generalized synthetic control method, a data-driven counterfactual\nestimator. Using machine learning based text analysis methods, we provide\nadditional evidence that the divergence between these two groups - large firms\nand non-elite universities - is driven by access to computing power or compute,\nwhich we term as the \"compute divide\". This compute divide between large firms\nand non-elite universities increases concerns around bias and fairness within\nAI technology, and presents an obstacle towards \"democratizing\" AI. These\nresults suggest that a lack of access to specialized equipment such as compute\ncan de-democratize knowledge production.\n",
        "published": "2020",
        "authors": [
            "Nur Ahmed",
            "Muntasir Wahed"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.12446v1",
        "title": "Counterfactual State Explanations for Reinforcement Learning Agents via\n  Generative Deep Learning",
        "abstract": "  Counterfactual explanations, which deal with \"why not?\" scenarios, can\nprovide insightful explanations to an AI agent's behavior. In this work, we\nfocus on generating counterfactual explanations for deep reinforcement learning\n(RL) agents which operate in visual input environments like Atari. We introduce\ncounterfactual state explanations, a novel example-based approach to\ncounterfactual explanations based on generative deep learning. Specifically, a\ncounterfactual state illustrates what minimal change is needed to an Atari game\nimage such that the agent chooses a different action. We also evaluate the\neffectiveness of counterfactual states on human participants who are not\nmachine learning experts. Our first user study investigates if humans can\ndiscern if the counterfactual state explanations are produced by the actual\ngame or produced by a generative deep learning approach. Our second user study\ninvestigates if counterfactual state explanations can help non-expert\nparticipants identify a flawed agent; we compare against a baseline approach\nbased on a nearest neighbor explanation which uses images from the actual game.\nOur results indicate that counterfactual state explanations have sufficient\nfidelity to the actual game images to enable non-experts to more effectively\nidentify a flawed RL agent compared to the nearest neighbor baseline and to\nhaving no explanation at all.\n",
        "published": "2021",
        "authors": [
            "Matthew L. Olson",
            "Roli Khanna",
            "Lawrence Neal",
            "Fuxin Li",
            "Weng-Keen Wong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.11209v1",
        "title": "Interpretable Knowledge Tracing: Simple and Efficient Student Modeling\n  with Causal Relations",
        "abstract": "  Intelligent Tutoring Systems have become critically important in future\nlearning environments. Knowledge Tracing (KT) is a crucial part of that system.\nIt is about inferring the skill mastery of students and predicting their\nperformance to adjust the curriculum accordingly. Deep Learning-based KT models\nhave shown significant predictive performance compared with traditional models.\nHowever, it is difficult to extract psychologically meaningful explanations\nfrom the tens of thousands of parameters in neural networks, that would relate\nto cognitive theory. There are several ways to achieve high accuracy in student\nperformance prediction but diagnostic and prognostic reasoning is more critical\nin learning sciences. Since KT problem has few observable features (problem ID\nand student's correctness at each practice), we extract meaningful latent\nfeatures from students' response data by using machine learning and data mining\ntechniques. In this work, we present Interpretable Knowledge Tracing (IKT), a\nsimple model that relies on three meaningful latent features: individual skill\nmastery, ability profile (learning transfer across skills), and problem\ndifficulty. IKT's prediction of future student performance is made using a\nTree-Augmented Naive Bayes Classifier (TAN), therefore its predictions are\neasier to explain than deep learning-based student models. IKT also shows\nbetter student performance prediction than deep learning-based student models\nwithout requiring a huge amount of parameters. We conduct ablation studies on\neach feature to examine their contribution to student performance prediction.\nThus, IKT has great potential for providing adaptive and personalized\ninstructions with causal reasoning in real-world educational systems.\n",
        "published": "2021",
        "authors": [
            "Sein Minn",
            "Jill-Jenn Vie",
            "Koh Takeuchi",
            "Hisashi Kashima",
            "Feida Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.15944v2",
        "title": "Hide and Seek: on the Stealthiness of Attacks against Deep Learning\n  Systems",
        "abstract": "  With the growing popularity of artificial intelligence and machine learning,\na wide spectrum of attacks against deep learning models have been proposed in\nthe literature. Both the evasion attacks and the poisoning attacks attempt to\nutilize adversarially altered samples to fool the victim model to misclassify\nthe adversarial sample. While such attacks claim to be or are expected to be\nstealthy, i.e., imperceptible to human eyes, such claims are rarely evaluated.\nIn this paper, we present the first large-scale study on the stealthiness of\nadversarial samples used in the attacks against deep learning. We have\nimplemented 20 representative adversarial ML attacks on six popular\nbenchmarking datasets. We evaluate the stealthiness of the attack samples using\ntwo complementary approaches: (1) a numerical study that adopts 24 metrics for\nimage similarity or quality assessment; and (2) a user study of 3 sets of\nquestionnaires that has collected 20,000+ annotations from 1,000+ responses.\nOur results show that the majority of the existing attacks introduce\nnonnegligible perturbations that are not stealthy to human eyes. We further\nanalyze the factors that contribute to attack stealthiness. We further examine\nthe correlation between the numerical analysis and the user studies, and\ndemonstrate that some image quality metrics may provide useful guidance in\nattack designs, while there is still a significant gap between assessed image\nquality and visual stealthiness of attacks.\n",
        "published": "2022",
        "authors": [
            "Zeyan Liu",
            "Fengjun Li",
            "Jingqiang Lin",
            "Zhu Li",
            "Bo Luo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.05079v1",
        "title": "Horizontal Federated Learning and Secure Distributed Training for\n  Recommendation System with Intel SGX",
        "abstract": "  With the advent of big data era and the development of artificial\nintelligence and other technologies, data security and privacy protection have\nbecome more important. Recommendation systems have many applications in our\nsociety, but the model construction of recommendation systems is often\ninseparable from users' data. Especially for deep learning-based recommendation\nsystems, due to the complexity of the model and the characteristics of deep\nlearning itself, its training process not only requires long training time and\nabundant computational resources but also needs to use a large amount of user\ndata, which poses a considerable challenge in terms of data security and\nprivacy protection. How to train a distributed recommendation system while\nensuring data security has become an urgent problem to be solved. In this\npaper, we implement two schemes, Horizontal Federated Learning and Secure\nDistributed Training, based on Intel SGX(Software Guard Extensions), an\nimplementation of a trusted execution environment, and TensorFlow framework, to\nachieve secure, distributed recommendation system-based learning schemes in\ndifferent scenarios. We experiment on the classical Deep Learning\nRecommendation Model (DLRM), which is a neural network-based machine learning\nmodel designed for personalization and recommendation, and the results show\nthat our implementation introduces approximately no loss in model performance.\nThe training speed is within acceptable limits.\n",
        "published": "2022",
        "authors": [
            "Siyuan Hui",
            "Yuqiu Zhang",
            "Albert Hu",
            "Edmund Song"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.06350v1",
        "title": "Towards Deep Learning Models for Psychological State Prediction using\n  Smartphone Data: Challenges and Opportunities",
        "abstract": "  There is an increasing interest in exploiting mobile sensing technologies and\nmachine learning techniques for mental health monitoring and intervention.\nResearchers have effectively used contextual information, such as mobility,\ncommunication and mobile phone usage patterns for quantifying individuals' mood\nand wellbeing. In this paper, we investigate the effectiveness of neural\nnetwork models for predicting users' level of stress by using the location\ninformation collected by smartphones. We characterize the mobility patterns of\nindividuals using the GPS metrics presented in the literature and employ these\nmetrics as input to the network. We evaluate our approach on the open-source\nStudentLife dataset. Moreover, we discuss the challenges and trade-offs\ninvolved in building machine learning models for digital mental health and\nhighlight potential future work in this direction.\n",
        "published": "2017",
        "authors": [
            "Gatis Mikelsons",
            "Matthew Smith",
            "Abhinav Mehrotra",
            "Mirco Musolesi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.07592v2",
        "title": "What Can Machine Learning Teach Us about Communications?",
        "abstract": "  Rapid improvements in machine learning over the past decade are beginning to\nhave far-reaching effects. For communications, engineers with limited domain\nexpertise can now use off-the-shelf learning packages to design\nhigh-performance systems based on simulations. Prior to the current revolution\nin machine learning, the majority of communication engineers were quite aware\nthat system parameters (such as filter coefficients) could be learned using\nstochastic gradient descent. It was not at all clear, however, that more\ncomplicated parts of the system architecture could be learned as well. In this\npaper, we discuss the application of machine-learning techniques to two\ncommunications problems and focus on what can be learned from the resulting\nsystems. We were pleasantly surprised that the observed gains in one example\nhave a simple explanation that only became clear in hindsight. In essence, deep\nlearning discovered a simple and effective strategy that had not been\nconsidered earlier.\n",
        "published": "2019",
        "authors": [
            "Mengke Lian",
            "Christian H\u00e4ger",
            "Henry D. Pfister"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.10600v2",
        "title": "Myriad: a real-world testbed to bridge trajectory optimization and deep\n  learning",
        "abstract": "  We present Myriad, a testbed written in JAX for learning and planning in\nreal-world continuous environments. The primary contributions of Myriad are\nthreefold. First, Myriad provides machine learning practitioners access to\ntrajectory optimization techniques for application within a typical automatic\ndifferentiation workflow. Second, Myriad presents many real-world optimal\ncontrol problems, ranging from biology to medicine to engineering, for use by\nthe machine learning community. Formulated in continuous space and time, these\nenvironments retain some of the complexity of real-world systems often\nabstracted away by standard benchmarks. As such, Myriad strives to serve as a\nstepping stone towards application of modern machine learning techniques for\nimpactful real-world tasks. Finally, we use the Myriad repository to showcase a\nnovel approach for learning and control tasks. Trained in a fully end-to-end\nfashion, our model leverages an implicit planning module over neural ordinary\ndifferential equations, enabling simultaneous learning and planning with\ncomplex environment dynamics.\n",
        "published": "2022",
        "authors": [
            "Nikolaus H. R. Howe",
            "Simon Dufort-Labb\u00e9",
            "Nitarshan Rajkumar",
            "Pierre-Luc Bacon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.06257v2",
        "title": "ViWi: A Deep Learning Dataset Framework for Vision-Aided Wireless\n  Communications",
        "abstract": "  The growing role that artificial intelligence and specifically machine\nlearning is playing in shaping the future of wireless communications has opened\nup many new and intriguing research directions. This paper motivates the\nresearch in the novel direction of \\textit{vision-aided wireless\ncommunications}, which aims at leveraging visual sensory information in\ntackling wireless communication problems. Like any new research direction\ndriven by machine learning, obtaining a development dataset poses the first and\nmost important challenge to vision-aided wireless communications. This paper\naddresses this issue by introducing the Vision-Wireless (ViWi) dataset\nframework. It is developed to be a parametric, systematic, and scalable data\ngeneration framework. It utilizes advanced 3D-modeling and ray-tracing\nsoftwares to generate high-fidelity synthetic wireless and vision data samples\nfor the same scenes. The result is a framework that does not only offer a way\nto generate training and testing datasets but helps provide a common ground on\nwhich the quality of different machine learning-powered solutions could be\nassessed.\n",
        "published": "2019",
        "authors": [
            "Muhammad Alrabeiah",
            "Andrew Hredzak",
            "Zhenhao Liu",
            "Ahmed Alkhateeb"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.15538v1",
        "title": "Machine learning based disease diagnosis: A comprehensive review",
        "abstract": "  Globally, there is a substantial unmet need to diagnose various diseases\neffectively. The complexity of the different disease mechanisms and underlying\nsymptoms of the patient population presents massive challenges to developing\nthe early diagnosis tool and effective treatment. Machine Learning (ML), an\narea of Artificial Intelligence (AI), enables researchers, physicians, and\npatients to solve some of these issues. Based on relevant research, this review\nexplains how Machine Learning (ML) and Deep Learning (DL) are being used to\nhelp in the early identification of numerous diseases. To begin, a bibliometric\nstudy of the publication is given using data from the Scopus and Web of Science\n(WOS) databases. The bibliometric study of 1216 publications was undertaken to\ndetermine the most prolific authors, nations, organizations, and most cited\narticles. The review then summarizes the most recent trends and approaches in\nMachine Learning-based Disease Diagnosis (MLBDD), considering the following\nfactors: algorithm, disease types, data type, application, and evaluation\nmetrics. Finally, the paper highlights key results and provides insight into\nfuture trends and opportunities in the MLBDD area.\n",
        "published": "2021",
        "authors": [
            "Md Manjurul Ahsan",
            "Zahed Siddique"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.02412v1",
        "title": "Android Malware Detection using Machine learning: A Review",
        "abstract": "  Malware for Android is becoming increasingly dangerous to the safety of\nmobile devices and the data they hold. Although machine learning(ML) techniques\nhave been shown to be effective at detecting malware for Android, a\ncomprehensive analysis of the methods used is required. We review the current\nstate of Android malware detection us ing machine learning in this paper. We\nbegin by providing an overview of Android malware and the security issues it\ncauses. Then, we look at the various supervised, unsupervised, and deep\nlearning machine learning approaches that have been utilized for Android\nmalware detection. Addi tionally, we present a comparison of the performance of\nvarious Android malware detection methods and talk about the performance\nevaluation metrics that are utilized to evaluate their efficacy. Finally, we\ndraw atten tion to the drawbacks and difficulties of the methods that are\ncurrently in use and suggest possible future directions for research in this\narea. In addition to providing insights into the current state of Android\nmalware detection using machine learning, our review provides a comprehensive\noverview of the subject.\n",
        "published": "2023",
        "authors": [
            "Md Naseef-Ur-Rahman Chowdhury",
            "Ahshanul Haque",
            "Hamdy Soliman",
            "Mohammad Sahinur Hossen",
            "Tanjim Fatima",
            "Imtiaz Ahmed"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.10387v1",
        "title": "A Bridge between Dynamical Systems and Machine Learning: Engineered\n  Ordinary Differential Equations as Classification Algorithm (EODECA)",
        "abstract": "  In a world increasingly reliant on machine learning, the interpretability of\nthese models remains a substantial challenge, with many equating their\nfunctionality to an enigmatic black box. This study seeks to bridge machine\nlearning and dynamical systems. Recognizing the deep parallels between dense\nneural networks and dynamical systems, particularly in the light of\nnon-linearities and successive transformations, this manuscript introduces the\nEngineered Ordinary Differential Equations as Classification Algorithms\n(EODECAs). Uniquely designed as neural networks underpinned by continuous\nordinary differential equations, EODECAs aim to capitalize on the\nwell-established toolkit of dynamical systems. Unlike traditional deep learning\nmodels, which often suffer from opacity, EODECAs promise both high\nclassification performance and intrinsic interpretability. They are naturally\ninvertible, granting them an edge in understanding and transparency over their\ncounterparts. By bridging these domains, we hope to usher in a new era of\nmachine learning models where genuine comprehension of data processes\ncomplements predictive prowess.\n",
        "published": "2023",
        "authors": [
            "Raffaele Marino",
            "Lorenzo Giambagli",
            "Lorenzo Chicchi",
            "Lorenzo Buffoni",
            "Duccio Fanelli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.16810v1",
        "title": "Review of Machine Learning Approaches for Diagnostics and Prognostics of\n  Industrial Systems Using Industrial Open Source Data",
        "abstract": "  In the field of Prognostics and Health Management (PHM), recent years have\nwitnessed a significant surge in the application of machine learning (ML).\nDespite this growth, the field grapples with a lack of unified guidelines and\nsystematic approaches for effectively implementing these ML techniques and\ncomprehensive analysis regarding industrial open-source data across varied\nscenarios. To address these gaps, this paper provides a comprehensive review of\nmachine learning approaches for diagnostics and prognostics of industrial\nsystems using open-source datasets from PHM Data Challenge Competitions held\nbetween 2018 and 2023 by PHM Society and IEEE Reliability Society and\nsummarizes a unified ML framework. This review systematically categorizes and\nscrutinizes the problems, challenges, methodologies, and advancements\ndemonstrated in these competitions, highlighting the evolving role of both\nconventional machine learning and deep learning in tackling complex industrial\ntasks related to detection, diagnosis, assessment, and prognosis. Moreover,\nthis paper delves into the common challenges in PHM data challenge competitions\nby emphasizing both data-related and model-related issues and summarizes the\nsolutions that have been employed to address these challenges. Finally, we\nidentify key themes and potential directions for future research, providing\nopportunities and prospects for ML further development in PHM.\n",
        "published": "2023",
        "authors": [
            "Hanqi Su",
            "Jay Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.03985v2",
        "title": "Applications of Deep Learning and Reinforcement Learning to Biological\n  Data",
        "abstract": "  Rapid advances of hardware-based technologies during the past decades have\nopened up new possibilities for Life scientists to gather multimodal data in\nvarious application domains (e.g., Omics, Bioimaging, Medical Imaging, and\n[Brain/Body]-Machine Interfaces), thus generating novel opportunities for\ndevelopment of dedicated data intensive machine learning techniques. Overall,\nrecent research in Deep learning (DL), Reinforcement learning (RL), and their\ncombination (Deep RL) promise to revolutionize Artificial Intelligence. The\ngrowth in computational power accompanied by faster and increased data storage\nand declining computing costs have already allowed scientists in various fields\nto apply these techniques on datasets that were previously intractable for\ntheir size and complexity. This review article provides a comprehensive survey\non the application of DL, RL, and Deep RL techniques in mining Biological data.\nIn addition, we compare performances of DL techniques when applied to different\ndatasets across various application domains. Finally, we outline open issues in\nthis challenging research area and discuss future development perspectives.\n",
        "published": "2017",
        "authors": [
            "Mufti Mahmud",
            "M. Shamim Kaiser",
            "Amir Hussain",
            "Stefano Vassanelli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.04854v3",
        "title": "Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam",
        "abstract": "  Uncertainty computation in deep learning is essential to design robust and\nreliable systems. Variational inference (VI) is a promising approach for such\ncomputation, but requires more effort to implement and execute compared to\nmaximum-likelihood methods. In this paper, we propose new natural-gradient\nalgorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms\ncan be implemented within the Adam optimizer by perturbing the network weights\nduring gradient evaluations, and uncertainty estimates can be cheaply obtained\nby using the vector that adapts the learning rate. This requires lower memory,\ncomputation, and implementation effort than existing VI methods, while\nobtaining uncertainty estimates of comparable quality. Our empirical results\nconfirm this and further suggest that the weight-perturbation in our algorithm\ncould be useful for exploration in reinforcement learning and stochastic\noptimization.\n",
        "published": "2018",
        "authors": [
            "Mohammad Emtiyaz Khan",
            "Didrik Nielsen",
            "Voot Tangkaratt",
            "Wu Lin",
            "Yarin Gal",
            "Akash Srivastava"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.08894v1",
        "title": "Deep Reinforcement Learning: An Overview",
        "abstract": "  In recent years, a specific machine learning method called deep learning has\ngained huge attraction, as it has obtained astonishing results in broad\napplications such as pattern recognition, speech recognition, computer vision,\nand natural language processing. Recent research has also been shown that deep\nlearning techniques can be combined with reinforcement learning methods to\nlearn useful representations for the problems with high dimensional raw data\ninput. This chapter reviews the recent advances in deep reinforcement learning\nwith a focus on the most used deep architectures such as autoencoders,\nconvolutional neural networks and recurrent neural networks which have\nsuccessfully been come together with the reinforcement learning framework.\n",
        "published": "2018",
        "authors": [
            "Seyed Sajad Mousavi",
            "Michael Schukat",
            "Enda Howley"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.03046v1",
        "title": "Deep Learning for Singing Processing: Achievements, Challenges and\n  Impact on Singers and Listeners",
        "abstract": "  This paper summarizes some recent advances on a set of tasks related to the\nprocessing of singing using state-of-the-art deep learning techniques. We\ndiscuss their achievements in terms of accuracy and sound quality, and the\ncurrent challenges, such as availability of data and computing resources. We\nalso discuss the impact that these advances do and will have on listeners and\nsingers when they are integrated in commercial applications.\n",
        "published": "2018",
        "authors": [
            "Emilia G\u00f3mez",
            "Merlijn Blaauw",
            "Jordi Bonada",
            "Pritish Chandna",
            "Helena Cuesta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.04906v3",
        "title": "Performing Highly Accurate Predictions Through Convolutional Networks\n  for Actual Telecommunication Challenges",
        "abstract": "  We investigated how the application of deep learning, specifically the use of\nconvolutional networks trained with GPUs, can help to build better predictive\nmodels in telecommunication business environments, and fill this gap. In\nparticular, we focus on the non-trivial problem of predicting customer churn in\ntelecommunication operators. Our model, called WiseNet, consists of a\nconvolutional network and a novel encoding method that transforms customer\nactivity data and Call Detail Records (CDRs) into images. Experimental\nevaluation with several machine learning classifiers supports the ability of\nWiseNet for learning features when using structured input data. For this type\nof telecommunication business problems, we found that WiseNet outperforms\nmachine learning models with hand-crafted features, and does not require the\nlabor-intensive step of feature engineering. Furthermore, the same model has\nbeen applied without retraining to a different market, achieving consistent\nresults. This confirms the generalization property of WiseNet and the ability\nto extract useful representations.\n",
        "published": "2015",
        "authors": [
            "Jaime Zaratiegui",
            "Ana Montoro",
            "Federico Castanedo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.01779v2",
        "title": "Deep learning from crowds",
        "abstract": "  Over the last few years, deep learning has revolutionized the field of\nmachine learning by dramatically improving the state-of-the-art in various\ndomains. However, as the size of supervised artificial neural networks grows,\ntypically so does the need for larger labeled datasets. Recently, crowdsourcing\nhas established itself as an efficient and cost-effective solution for labeling\nlarge sets of data in a scalable manner, but it often requires aggregating\nlabels from multiple noisy contributors with different levels of expertise. In\nthis paper, we address the problem of learning deep neural networks from\ncrowds. We begin by describing an EM algorithm for jointly learning the\nparameters of the network and the reliabilities of the annotators. Then, a\nnovel general-purpose crowd layer is proposed, which allows us to train deep\nneural networks end-to-end, directly from the noisy labels of multiple\nannotators, using only backpropagation. We empirically show that the proposed\napproach is able to internally capture the reliability and biases of different\nannotators and achieve new state-of-the-art results for various crowdsourced\ndatasets across different settings, namely classification, regression and\nsequence labeling.\n",
        "published": "2017",
        "authors": [
            "Filipe Rodrigues",
            "Francisco Pereira"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.03628v2",
        "title": "Recent Trends in Deep Learning Based Personality Detection",
        "abstract": "  Recently, the automatic prediction of personality traits has received a lot\nof attention. Specifically, personality trait prediction from multimodal data\nhas emerged as a hot topic within the field of affective computing. In this\npaper, we review significant machine learning models which have been employed\nfor personality detection, with an emphasis on deep learning-based methods.\nThis review paper provides an overview of the most popular approaches to\nautomated personality detection, various computational datasets, its industrial\napplications, and state-of-the-art machine learning models for personality\ndetection with specific focus on multimodal approaches. Personality detection\nis a very broad and diverse topic: this survey only focuses on computational\napproaches and leaves out psychological studies on personality detection.\n",
        "published": "2019",
        "authors": [
            "Yash Mehta",
            "Navonil Majumder",
            "Alexander Gelbukh",
            "Erik Cambria"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.06426v2",
        "title": "Deep generative models for musical audio synthesis",
        "abstract": "  Sound modelling is the process of developing algorithms that generate sound\nunder parametric control. There are a few distinct approaches that have been\ndeveloped historically including modelling the physics of sound production and\npropagation, assembling signal generating and processing elements to capture\nacoustic features, and manipulating collections of recorded audio samples.\nWhile each of these approaches has been able to achieve high-quality synthesis\nand interaction for specific applications, they are all labour-intensive and\neach comes with its own challenges for designing arbitrary control strategies.\nRecent generative deep learning systems for audio synthesis are able to learn\nmodels that can traverse arbitrary spaces of sound defined by the data they\ntrain on. Furthermore, machine learning systems are providing new techniques\nfor designing control and navigation strategies for these models. This paper is\na review of developments in deep learning that are changing the practice of\nsound modelling.\n",
        "published": "2020",
        "authors": [
            "M. Huzaifah",
            "L. Wyse"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.01097v1",
        "title": "PrototypeML: A Neural Network Integrated Design and Development\n  Environment",
        "abstract": "  Neural network architectures are most often conceptually designed and\ndescribed in visual terms, but are implemented by writing error-prone code.\nPrototypeML is a machine learning development environment that bridges the\ndichotomy between the design and development processes: it provides a highly\nintuitive visual neural network design interface that supports (yet abstracts)\nthe full capabilities of the PyTorch deep learning framework, reduces model\ndesign and development time, makes debugging easier, and automates many\nframework and code writing idiosyncrasies. In this paper, we detail the deep\nlearning development deficiencies that drove the implementation of PrototypeML,\nand propose a hybrid approach to resolve these issues without limiting network\nexpressiveness or reducing code quality. We demonstrate the real-world benefits\nof a visual approach to neural network design for research, industry and\nteaching. Available at https://PrototypeML.com\n",
        "published": "2020",
        "authors": [
            "Daniel Reiss Harris"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.03427v2",
        "title": "Deep Learning for Two-Sided Matching",
        "abstract": "  We initiate the study of deep learning for the automated design of two-sided\nmatching mechanisms. What is of most interest is to use machine learning to\nunderstand the possibility of new tradeoffs between strategy-proofness and\nstability. These properties cannot be achieved simultaneously, but the\nefficient frontier is not understood. We introduce novel differentiable\nsurrogates for quantifying ordinal strategy-proofness and stability and use\nthem to train differentiable matching mechanisms that map discrete preferences\nto valid randomized matchings. We demonstrate that the efficient frontier\ncharacterized by these learned mechanisms is substantially better than that\nachievable through a convex combination of baselines of deferred acceptance\n(stable and strategy-proof for only one side of the market), top trading cycles\n(strategy-proof for one side, but not stable), and randomized serial\ndictatorship (strategy-proof for both sides, but not stable). This gives a new\ntarget for economic theory and opens up new possibilities for machine learning\npipelines in matching market design.\n",
        "published": "2021",
        "authors": [
            "Sai Srivatsa Ravindranath",
            "Zhe Feng",
            "Shira Li",
            "Jonathan Ma",
            "Scott D. Kominers",
            "David C. Parkes"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.05408v1",
        "title": "The Concept of the Deep Learning-Based System \"Artificial Dispatcher\" to\n  Power System Control and Dispatch",
        "abstract": "  Year by year control of normal and emergency conditions of up-to-date power\nsystems becomes an increasingly complicated problem. With the increasing\ncomplexity the existing control system of power system conditions which\nincludes operative actions of the dispatcher and work of special automatic\ndevices proves to be insufficiently effective more and more frequently, which\nraises risks of dangerous and emergency conditions in power systems. The paper\nis aimed at compensating for the shortcomings of man (a cognitive barrier,\nexposure to stresses and so on) and automatic devices by combining their strong\npoints, i.e. the dispatcher's intelligence and the speed of automatic devices\nby virtue of development of the intelligent system \"Artificial dispatcher\" on\nthe basis of deep machine learning technology. For realization of the system\n\"Artificial dispatcher\" in addition to deep learning it is planned to attract\nthe game theory approaches to formalize work of the up-to-date power system as\na game problem. The \"gain\" for \"Artificial dispatcher\" will consist in bringing\nin a power system in the normal steady-state or post-emergency conditions by\nmeans of the required control actions.\n",
        "published": "2018",
        "authors": [
            "Nikita Tomin",
            "Victor Kurbatsky",
            "Michael Negnevitsky"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.06444v1",
        "title": "Synthetic Patient Generation: A Deep Learning Approach Using Variational\n  Autoencoders",
        "abstract": "  Artificial Intelligence in healthcare is a new and exciting frontier and the\npossibilities are endless. With deep learning approaches beating human\nperformances in many areas, the logical next step is to attempt their\napplication in the health space. For these and other Machine Learning\napproaches to produce good results and have their potential realized, the need\nfor, and importance of, large amounts of accurate data is second to none. This\nis a challenge faced by many industries and more so in the healthcare space. We\npresent an approach of using Variational Autoencoders (VAE's) as an approach to\ngenerating more data for training deeper networks, as well as uncovering\nunderlying patterns in diagnoses and the patients suffering from them. By\ntraining a VAE, on available data, it was able to learn the latent distribution\nof the patient features given the diagnosis. It is then possible, after\ntraining, to sample from the learnt latent distribution to generate new\naccurate patient records given the patient diagnosis.\n",
        "published": "2018",
        "authors": [
            "Ally Salim Jr"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.07220v2",
        "title": "Approximating Poker Probabilities with Deep Learning",
        "abstract": "  Many poker systems, whether created with heuristics or machine learning, rely\non the probability of winning as a key input. However calculating the precise\nprobability using combinatorics is an intractable problem, so instead we\napproximate it. Monte Carlo simulation is an effective technique that can be\nused to approximate the probability that a player will win and/or tie a hand.\nHowever, without the use of a memory-intensive lookup table or a supercomputer,\nit becomes infeasible to run millions of times when training an agent with\nself-play. To combat the space-time tradeoff, we use deep learning to\napproximate the probabilities obtained from the Monte Carlo simulation with\nhigh accuracy. The learned model proves to be a lightweight alternative to\nMonte Carlo simulation, which ultimately allows us to use the probabilities as\ninputs during self-play efficiently. The source code and optimized neural\nnetwork can be found at\nhttps://github.com/brandinho/Poker-Probability-Approximation\n",
        "published": "2018",
        "authors": [
            "Brandon Da Silva"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.09591v2",
        "title": "Applying Deep Learning To Airbnb Search",
        "abstract": "  The application to search ranking is one of the biggest machine learning\nsuccess stories at Airbnb. Much of the initial gains were driven by a gradient\nboosted decision tree model. The gains, however, plateaued over time. This\npaper discusses the work done in applying neural networks in an attempt to\nbreak out of that plateau. We present our perspective not with the intention of\npushing the frontier of new modeling techniques. Instead, ours is a story of\nthe elements we found useful in applying neural networks to a real life\nproduct. Deep learning was steep learning for us. To other teams embarking on\nsimilar journeys, we hope an account of our struggles and triumphs will provide\nsome useful pointers. Bon voyage!\n",
        "published": "2018",
        "authors": [
            "Malay Haldar",
            "Mustafa Abdool",
            "Prashant Ramanathan",
            "Tao Xu",
            "Shulin Yang",
            "Huizhong Duan",
            "Qing Zhang",
            "Nick Barrow-Williams",
            "Bradley C. Turnbull",
            "Brendan M. Collins",
            "Thomas Legrand"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.12969v1",
        "title": "Counterfactual States for Atari Agents via Generative Deep Learning",
        "abstract": "  Although deep reinforcement learning agents have produced impressive results\nin many domains, their decision making is difficult to explain to humans. To\naddress this problem, past work has mainly focused on explaining why an action\nwas chosen in a given state. A different type of explanation that is useful is\na counterfactual, which deals with \"what if?\" scenarios. In this work, we\nintroduce the concept of a counterfactual state to help humans gain a better\nunderstanding of what would need to change (minimally) in an Atari game image\nfor the agent to choose a different action. We introduce a novel method to\ncreate counterfactual states from a generative deep learning architecture. In\naddition, we evaluate the effectiveness of counterfactual states on human\nparticipants who are not machine learning experts. Our user study results\nsuggest that our generated counterfactual states are useful in helping\nnon-expert participants gain a better understanding of an agent's decision\nmaking process.\n",
        "published": "2019",
        "authors": [
            "Matthew L. Olson",
            "Lawrence Neal",
            "Fuxin Li",
            "Weng-Keen Wong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.10840v2",
        "title": "Analyzing the Performance of Graph Neural Networks with Pipe Parallelism",
        "abstract": "  Many interesting datasets ubiquitous in machine learning and deep learning\ncan be described via graphs. As the scale and complexity of graph-structured\ndatasets increase, such as in expansive social networks, protein folding,\nchemical interaction networks, and material phase transitions, improving the\nefficiency of the machine learning techniques applied to these is crucial. In\nthis study, we focus on Graph Neural Networks (GNN) that have found great\nsuccess in tasks such as node or edge classification and link prediction.\nHowever, standard GNN models have scaling limits due to necessary recursive\ncalculations performed through dense graph relationships that lead to memory\nand runtime bottlenecks. While new approaches for processing larger networks\nare needed to advance graph techniques, and several have been proposed, we\nstudy how GNNs could be parallelized using existing tools and frameworks that\nare known to be successful in the deep learning community. In particular, we\ninvestigate applying pipeline parallelism to GNN models with GPipe, introduced\nby Google in 2018.\n",
        "published": "2020",
        "authors": [
            "Matthew T. Dearing",
            "Xiaoyan Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.15931v1",
        "title": "Informed Machine Learning for Improved Similarity Assessment in\n  Process-Oriented Case-Based Reasoning",
        "abstract": "  Currently, Deep Learning (DL) components within a Case-Based Reasoning (CBR)\napplication often lack the comprehensive integration of available domain\nknowledge. The trend within machine learning towards so-called Informed machine\nlearning can help to overcome this limitation. In this paper, we therefore\ninvestigate the potential of integrating domain knowledge into Graph Neural\nNetworks (GNNs) that are used for similarity assessment between semantic graphs\nwithin process-oriented CBR applications. We integrate knowledge in two ways:\nFirst, a special data representation and processing method is used that encodes\nstructural knowledge about the semantic annotations of each graph node and\nedge. Second, the message-passing component of the GNNs is constrained by\nknowledge on legal node mappings. The evaluation examines the quality and\ntraining time of the extended GNNs, compared to the stock models. The results\nshow that both extensions are capable of providing better quality, shorter\ntraining times, or in some configurations both advantages at once.\n",
        "published": "2021",
        "authors": [
            "Maximilian Hoffmann",
            "Ralph Bergmann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.04296v2",
        "title": "TensorIR: An Abstraction for Automatic Tensorized Program Optimization",
        "abstract": "  Deploying deep learning models on various devices has become an important\ntopic. The wave of hardware specialization brings a diverse set of acceleration\nprimitives for multi-dimensional tensor computations. These new acceleration\nprimitives, along with the emerging machine learning models, bring tremendous\nengineering challenges. In this paper, we present TensorIR, a compiler\nabstraction for optimizing programs with these tensor computation primitives.\nTensorIR generalizes the loop nest representation used in existing machine\nlearning compilers to bring tensor computation as the first-class citizen.\nFinally, we build an end-to-end framework on top of our abstraction to\nautomatically optimize deep learning models for given tensor computation\nprimitives. Experimental results show that TensorIR compilation automatically\nuses the tensor computation primitives for given hardware backends and delivers\nperformance that is competitive to state-of-art hand-optimized systems across\nplatforms.\n",
        "published": "2022",
        "authors": [
            "Siyuan Feng",
            "Bohan Hou",
            "Hongyi Jin",
            "Wuwei Lin",
            "Junru Shao",
            "Ruihang Lai",
            "Zihao Ye",
            "Lianmin Zheng",
            "Cody Hao Yu",
            "Yong Yu",
            "Tianqi Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.08385v1",
        "title": "CardiacGen: A Hierarchical Deep Generative Model for Cardiac Signals",
        "abstract": "  We present CardiacGen, a Deep Learning framework for generating synthetic but\nphysiologically plausible cardiac signals like ECG. Based on the physiology of\ncardiovascular system function, we propose a modular hierarchical generative\nmodel and impose explicit regularizing constraints for training each module\nusing multi-objective loss functions. The model comprises 2 modules, an HRV\nmodule focused on producing realistic Heart-Rate-Variability characteristics\nand a Morphology module focused on generating realistic signal morphologies for\ndifferent modalities. We empirically show that in addition to having realistic\nphysiological features, the synthetic data from CardiacGen can be used for data\naugmentation to improve the performance of Deep Learning based classifiers.\nCardiacGen code is available at\nhttps://github.com/SENSE-Lab-OSU/cardiac_gen_model.\n",
        "published": "2022",
        "authors": [
            "Tushar Agarwal",
            "Emre Ertin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.08727v2",
        "title": "Neural Architecture Search: Insights from 1000 Papers",
        "abstract": "  In the past decade, advances in deep learning have resulted in breakthroughs\nin a variety of areas, including computer vision, natural language\nunderstanding, speech recognition, and reinforcement learning. Specialized,\nhigh-performing neural architectures are crucial to the success of deep\nlearning in these areas. Neural architecture search (NAS), the process of\nautomating the design of neural architectures for a given task, is an\ninevitable next step in automating machine learning and has already outpaced\nthe best human-designed architectures on many tasks. In the past few years,\nresearch in NAS has been progressing rapidly, with over 1000 papers released\nsince 2020 (Deng and Lindauer, 2021). In this survey, we provide an organized\nand comprehensive guide to neural architecture search. We give a taxonomy of\nsearch spaces, algorithms, and speedup techniques, and we discuss resources\nsuch as benchmarks, best practices, other surveys, and open-source libraries.\n",
        "published": "2023",
        "authors": [
            "Colin White",
            "Mahmoud Safari",
            "Rhea Sukthanker",
            "Binxin Ru",
            "Thomas Elsken",
            "Arber Zela",
            "Debadeepta Dey",
            "Frank Hutter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.12707v1",
        "title": "Comparison of Probabilistic Deep Learning Methods for Autism Detection",
        "abstract": "  Autism Spectrum Disorder (ASD) is one neuro developmental disorder that is\nnow widespread in the world. ASD persists throughout the life of an individual,\nimpacting the way they behave and communicate, resulting to notable deficits\nconsisting of social life retardation, repeated behavioural traits and a\nrestriction in their interests. Early detection of the disorder helps in the\nonset treatment and helps one to lead a normal life. There are clinical\napproaches used in detection of autism, relying on behavioural data and in\nworst cases, neuroimaging. Quantitative methods involving machine learning have\nbeen studied and developed to overcome issues with clinical approaches. These\nquantitative methods rely on machine learning, with some complex methods based\non deep learning developed to accelerate detection and diagnosis of ASD. These\nliterature is aimed at exploring most state-of-the-art probabilistic methods in\nuse today, characterizing them with the type of dataset they're most applied\non, their accuracy according to their novel research and how well they are\nsuited in ASD classification. The findings will purposely serve as a benchmark\nin selection of the model to use when performing ASD detection.\n",
        "published": "2023",
        "authors": [
            "Godfrin Ismail",
            "Kenneth Chesoli",
            "Golda Moni",
            "Kinyua Gikunda"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.07470v1",
        "title": "Few-shot Weakly-supervised Cybersecurity Anomaly Detection",
        "abstract": "  With increased reliance on Internet based technologies, cyberattacks\ncompromising users' sensitive data are becoming more prevalent. The scale and\nfrequency of these attacks are escalating rapidly, affecting systems and\ndevices connected to the Internet. The traditional defense mechanisms may not\nbe sufficiently equipped to handle the complex and ever-changing new threats.\nThe significant breakthroughs in the machine learning methods including deep\nlearning, had attracted interests from the cybersecurity research community for\nfurther enhancements in the existing anomaly detection methods. Unfortunately,\ncollecting labelled anomaly data for all new evolving and sophisticated attacks\nis not practical. Training and tuning the machine learning model for anomaly\ndetection using only a handful of labelled data samples is a pragmatic\napproach. Therefore, few-shot weakly supervised anomaly detection is an\nencouraging research direction. In this paper, we propose an enhancement to an\nexisting few-shot weakly-supervised deep learning anomaly detection framework.\nThis framework incorporates data augmentation, representation learning and\nordinal regression. We then evaluated and showed the performance of our\nimplemented framework on three benchmark datasets: NSL-KDD, CIC-IDS2018, and\nTON_IoT.\n",
        "published": "2023",
        "authors": [
            "Rahul Kale",
            "Vrizlynn L. L. Thing"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.17109v1",
        "title": "Synthetic Demographic Data Generation for Card Fraud Detection Using\n  GANs",
        "abstract": "  Using machine learning models to generate synthetic data has become common in\nmany fields. Technology to generate synthetic transactions that can be used to\ndetect fraud is also growing fast. Generally, this synthetic data contains only\ninformation about the transaction, such as the time, place, and amount of\nmoney. It does not usually contain the individual user's characteristics (age\nand gender are occasionally included). Using relatively complex synthetic\ndemographic data may improve the complexity of transaction data features, thus\nimproving the fraud detection performance. Benefiting from developments of\nmachine learning, some deep learning models have potential to perform better\nthan other well-established synthetic data generation methods, such as\nmicrosimulation. In this study, we built a deep-learning Generative Adversarial\nNetwork (GAN), called DGGAN, which will be used for demographic data\ngeneration. Our model generates samples during model training, which we found\nimportant to overcame class imbalance issues. This study can help improve the\ncognition of synthetic data and further explore the application of synthetic\ndata generation in card fraud detection.\n",
        "published": "2023",
        "authors": [
            "Shuo Wang",
            "Terrence Tricco",
            "Xianta Jiang",
            "Charles Robertson",
            "John Hawkin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.03760v1",
        "title": "Investigating Deep Neural Network Architecture and Feature Extraction\n  Designs for Sensor-based Human Activity Recognition",
        "abstract": "  The extensive ubiquitous availability of sensors in smart devices and the\nInternet of Things (IoT) has opened up the possibilities for implementing\nsensor-based activity recognition. As opposed to traditional sensor time-series\nprocessing and hand-engineered feature extraction, in light of deep learning's\nproven effectiveness across various domains, numerous deep methods have been\nexplored to tackle the challenges in activity recognition, outperforming the\ntraditional signal processing and traditional machine learning approaches. In\nthis work, by performing extensive experimental studies on two human activity\nrecognition datasets, we investigate the performance of common deep learning\nand machine learning approaches as well as different training mechanisms (such\nas contrastive learning), and various feature representations extracted from\nthe sensor time-series data and measure their effectiveness for the human\nactivity recognition task.\n",
        "published": "2023",
        "authors": [
            "Danial Ahangarani",
            "Mohammad Shirazi",
            "Navid Ashraf"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.09250v1",
        "title": "It's an Alignment, Not a Trade-off: Revisiting Bias and Variance in Deep\n  Models",
        "abstract": "  Classical wisdom in machine learning holds that the generalization error can\nbe decomposed into bias and variance, and these two terms exhibit a\n\\emph{trade-off}. However, in this paper, we show that for an ensemble of deep\nlearning based classification models, bias and variance are \\emph{aligned} at a\nsample level, where squared bias is approximately \\emph{equal} to variance for\ncorrectly classified sample points. We present empirical evidence confirming\nthis phenomenon in a variety of deep learning models and datasets. Moreover, we\nstudy this phenomenon from two theoretical perspectives: calibration and neural\ncollapse. We first show theoretically that under the assumption that the models\nare well calibrated, we can observe the bias-variance alignment. Second,\nstarting from the picture provided by the neural collapse theory, we show an\napproximate correlation between bias and variance.\n",
        "published": "2023",
        "authors": [
            "Lin Chen",
            "Michal Lukasik",
            "Wittawat Jitkrittum",
            "Chong You",
            "Sanjiv Kumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.03532v1",
        "title": "The Fairness Stitch: Unveiling the Potential of Model Stitching in\n  Neural Network De-Biasing",
        "abstract": "  The pursuit of fairness in machine learning models has emerged as a critical\nresearch challenge in different applications ranging from bank loan approval to\nface detection. Despite the widespread adoption of artificial intelligence\nalgorithms across various domains, concerns persist regarding the presence of\nbiases and discrimination within these models. To address this pressing issue,\nthis study introduces a novel method called \"The Fairness Stitch (TFS)\" to\nenhance fairness in deep learning models. This method combines model stitching\nand training jointly, while incorporating fairness constraints. In this\nresearch, we assess the effectiveness of our proposed method by conducting a\ncomprehensive evaluation of two well-known datasets, CelebA and UTKFace. We\nsystematically compare the performance of our approach with the existing\nbaseline method. Our findings reveal a notable improvement in achieving a\nbalanced trade-off between fairness and performance, highlighting the promising\npotential of our method to address bias-related challenges and foster equitable\noutcomes in machine learning models. This paper poses a challenge to the\nconventional wisdom of the effectiveness of the last layer in deep learning\nmodels for de-biasing.\n",
        "published": "2023",
        "authors": [
            "Modar Sulaiman",
            "Kallol Roy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.06192v1",
        "title": "Greedy PIG: Adaptive Integrated Gradients",
        "abstract": "  Deep learning has become the standard approach for most machine learning\ntasks. While its impact is undeniable, interpreting the predictions of deep\nlearning models from a human perspective remains a challenge. In contrast to\nmodel training, model interpretability is harder to quantify and pose as an\nexplicit optimization problem. Inspired by the AUC softmax information curve\n(AUC SIC) metric for evaluating feature attribution methods, we propose a\nunified discrete optimization framework for feature attribution and feature\nselection based on subset selection. This leads to a natural adaptive\ngeneralization of the path integrated gradients (PIG) method for feature\nattribution, which we call Greedy PIG. We demonstrate the success of Greedy PIG\non a wide variety of tasks, including image feature attribution, graph\ncompression/explanation, and post-hoc feature selection on tabular data. Our\nresults show that introducing adaptivity is a powerful and versatile method for\nmaking attribution methods more powerful.\n",
        "published": "2023",
        "authors": [
            "Kyriakos Axiotis",
            "Sami Abu-al-haija",
            "Lin Chen",
            "Matthew Fahrbach",
            "Gang Fu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.12814v1",
        "title": "HydraScreen: A Generalizable Structure-Based Deep Learning Approach to\n  Drug Discovery",
        "abstract": "  We propose HydraScreen, a deep-learning approach that aims to provide a\nframework for more robust machine-learning-accelerated drug discovery.\nHydraScreen utilizes a state-of-the-art 3D convolutional neural network,\ndesigned for the effective representation of molecular structures and\ninteractions in protein-ligand binding. We design an end-to-end pipeline for\nhigh-throughput screening and lead optimization, targeting applications in\nstructure-based drug design. We assess our approach using established public\nbenchmarks based on the CASF 2016 core set, achieving top-tier results in\naffinity and pose prediction (Pearson's r = 0.86, RMSE = 1.15, Top-1 = 0.95).\nFurthermore, we utilize a novel interaction profiling approach to identify\npotential biases in the model and dataset to boost interpretability and support\nthe unbiased nature of our method. Finally, we showcase HydraScreen's capacity\nto generalize across unseen proteins and ligands, offering directions for\nfuture development of robust machine learning scoring functions. HydraScreen\n(accessible at https://hydrascreen.ro5.ai) provides a user-friendly GUI and a\npublic API, facilitating easy assessment of individual protein-ligand\ncomplexes.\n",
        "published": "2023",
        "authors": [
            "Alvaro Prat",
            "Hisham Abdel Aty",
            "Gintautas Kamuntavi\u010dius",
            "Tanya Paquet",
            "Povilas Norvai\u0161as",
            "Piero Gasparotto",
            "Roy Tal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.12022v1",
        "title": "LightGCNet: A Lightweight Geometric Constructive Neural Network for\n  Data-Driven Soft sensors",
        "abstract": "  Data-driven soft sensors provide a potentially cost-effective and more\naccurate modeling approach to measure difficult-to-measure indices in\nindustrial processes compared to mechanistic approaches. Artificial\nintelligence (AI) techniques, such as deep learning, have become a popular soft\nsensors modeling approach in the area of machine learning and big data.\nHowever, soft sensors models based deep learning potentially lead to complex\nmodel structures and excessive training time. In addition, industrial processes\noften rely on distributed control systems (DCS) characterized by resource\nconstraints. Herein, guided by spatial geometric, a lightweight geometric\nconstructive neural network, namely LightGCNet, is proposed, which utilizes\ncompact angle constraint to assign the hidden parameters from dynamic\nintervals. At the same time, a node pool strategy and spatial geometric\nrelationships are used to visualize and optimize the process of assigning\nhidden parameters, enhancing interpretability. In addition, the universal\napproximation property of LightGCNet is proved by spatial geometric analysis.\nTwo versions algorithmic implementations of LightGCNet are presented in this\narticle. Simulation results concerning both benchmark datasets and the ore\ngrinding process indicate remarkable merits of LightGCNet in terms of small\nnetwork size, fast learning speed, and sound generalization.\n",
        "published": "2023",
        "authors": [
            "Jing Nan",
            "Yan Qin",
            "Wei Dai",
            "Chau Yuen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.02323v1",
        "title": "Deep Super Learner: A Deep Ensemble for Classification Problems",
        "abstract": "  Deep learning has become very popular for tasks such as predictive modeling\nand pattern recognition in handling big data. Deep learning is a powerful\nmachine learning method that extracts lower level features and feeds them\nforward for the next layer to identify higher level features that improve\nperformance. However, deep neural networks have drawbacks, which include many\nhyper-parameters and infinite architectures, opaqueness into results, and\nrelatively slower convergence on smaller datasets. While traditional machine\nlearning algorithms can address these drawbacks, they are not typically capable\nof the performance levels achieved by deep neural networks. To improve\nperformance, ensemble methods are used to combine multiple base learners. Super\nlearning is an ensemble that finds the optimal combination of diverse learning\nalgorithms. This paper proposes deep super learning as an approach which\nachieves log loss and accuracy results competitive to deep neural networks\nwhile employing traditional machine learning algorithms in a hierarchical\nstructure. The deep super learner is flexible, adaptable, and easy to train\nwith good performance across different tasks using identical hyper-parameter\nvalues. Using traditional machine learning requires fewer hyper-parameters,\nallows transparency into results, and has relatively fast convergence on\nsmaller datasets. Experimental results show that the deep super learner has\nsuperior performance compared to the individual base learners, single-layer\nensembles, and in some cases deep neural networks. Performance of the deep\nsuper learner may further be improved with task-specific tuning.\n",
        "published": "2018",
        "authors": [
            "Steven Young",
            "Tamer Abdou",
            "Ayse Bener"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.00347v2",
        "title": "Anderson Acceleration For Bioinformatics-Based Machine Learning",
        "abstract": "  Anderson acceleration (AA) is a well-known method for accelerating the\nconvergence of iterative algorithms, with applications in various fields\nincluding deep learning and optimization. Despite its popularity in these\nareas, the effectiveness of AA in classical machine learning classifiers has\nnot been thoroughly studied. Tabular data, in particular, presents a unique\nchallenge for deep learning models, and classical machine learning models are\nknown to perform better in these scenarios. However, the convergence analysis\nof these models has received limited attention. To address this gap in\nresearch, we implement a support vector machine (SVM) classifier variant that\nincorporates AA to speed up convergence. We evaluate the performance of our SVM\nwith and without Anderson acceleration on several datasets from the biology\ndomain and demonstrate that the use of AA significantly improves convergence\nand reduces the training loss as the number of iterations increases. Our\nfindings provide a promising perspective on the potential of Anderson\nacceleration in the training of simple machine learning classifiers and\nunderscore the importance of further research in this area. By showing the\neffectiveness of AA in this setting, we aim to inspire more studies that\nexplore the applications of AA in classical machine learning.\n",
        "published": "2023",
        "authors": [
            "Sarwan Ali",
            "Prakash Chourasia",
            "Murray Patterson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.04587v2",
        "title": "Assessing the Scalability of Biologically-Motivated Deep Learning\n  Algorithms and Architectures",
        "abstract": "  The backpropagation of error algorithm (BP) is impossible to implement in a\nreal brain. The recent success of deep networks in machine learning and AI,\nhowever, has inspired proposals for understanding how the brain might learn\nacross multiple layers, and hence how it might approximate BP. As of yet, none\nof these proposals have been rigorously evaluated on tasks where BP-guided deep\nlearning has proved critical, or in architectures more structured than simple\nfully-connected networks. Here we present results on scaling up biologically\nmotivated models of deep learning on datasets which need deep networks with\nappropriate architectures to achieve good performance. We present results on\nthe MNIST, CIFAR-10, and ImageNet datasets and explore variants of\ntarget-propagation (TP) and feedback alignment (FA) algorithms, and explore\nperformance in both fully- and locally-connected architectures. We also\nintroduce weight-transport-free variants of difference target propagation (DTP)\nmodified to remove backpropagation from the penultimate layer. Many of these\nalgorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP\nand FA variants perform significantly worse than BP, especially for networks\ncomposed of locally connected units, opening questions about whether new\narchitectures and algorithms are required to scale these approaches. Our\nresults and implementation details help establish baselines for biologically\nmotivated deep learning schemes going forward.\n",
        "published": "2018",
        "authors": [
            "Sergey Bartunov",
            "Adam Santoro",
            "Blake A. Richards",
            "Luke Marris",
            "Geoffrey E. Hinton",
            "Timothy Lillicrap"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.06756v3",
        "title": "SySeVR: A Framework for Using Deep Learning to Detect Software\n  Vulnerabilities",
        "abstract": "  The detection of software vulnerabilities (or vulnerabilities for short) is\nan important problem that has yet to be tackled, as manifested by the many\nvulnerabilities reported on a daily basis. This calls for machine learning\nmethods for vulnerability detection. Deep learning is attractive for this\npurpose because it alleviates the requirement to manually define features.\nDespite the tremendous success of deep learning in other application domains,\nits applicability to vulnerability detection is not systematically understood.\nIn order to fill this void, we propose the first systematic framework for using\ndeep learning to detect vulnerabilities in C/C++ programs with source code. The\nframework, dubbed Syntax-based, Semantics-based, and Vector Representations\n(SySeVR), focuses on obtaining program representations that can accommodate\nsyntax and semantic information pertinent to vulnerabilities. Our experiments\nwith 4 software products demonstrate the usefulness of the framework: we detect\n15 vulnerabilities that are not reported in the National Vulnerability\nDatabase. Among these 15 vulnerabilities, 7 are unknown and have been reported\nto the vendors, and the other 8 have been \"silently\" patched by the vendors\nwhen releasing newer versions of the pertinent software products.\n",
        "published": "2018",
        "authors": [
            "Zhen Li",
            "Deqing Zou",
            "Shouhuai Xu",
            "Hai Jin",
            "Yawei Zhu",
            "Zhaoxuan Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.02884v1",
        "title": "Deep Learning for Human Affect Recognition: Insights and New\n  Developments",
        "abstract": "  Automatic human affect recognition is a key step towards more natural\nhuman-computer interaction. Recent trends include recognition in the wild using\na fusion of audiovisual and physiological sensors, a challenging setting for\nconventional machine learning algorithms. Since 2010, novel deep learning\nalgorithms have been applied increasingly in this field. In this paper, we\nreview the literature on human affect recognition between 2010 and 2017, with a\nspecial focus on approaches using deep neural networks. By classifying a total\nof 950 studies according to their usage of shallow or deep architectures, we\nare able to show a trend towards deep learning. Reviewing a subset of 233\nstudies that employ deep neural networks, we comprehensively quantify their\napplications in this field. We find that deep learning is used for learning of\n(i) spatial feature representations, (ii) temporal feature representations, and\n(iii) joint feature representations for multimodal sensor data. Exemplary\nstate-of-the-art architectures illustrate the progress. Our findings show the\nrole deep architectures will play in human affect recognition, and can serve as\na reference point for researchers working on related applications.\n",
        "published": "2019",
        "authors": [
            "Philipp V. Rouast",
            "Marc T. P. Adam",
            "Raymond Chiong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.00682v1",
        "title": "Auditing and Debugging Deep Learning Models via Decision Boundaries:\n  Individual-level and Group-level Analysis",
        "abstract": "  Deep learning models have been criticized for their lack of easy\ninterpretation, which undermines confidence in their use for important\napplications. Nevertheless, they are consistently utilized in many\napplications, consequential to humans' lives, mostly because of their better\nperformance. Therefore, there is a great need for computational methods that\ncan explain, audit, and debug such models. Here, we use flip points to\naccomplish these goals for deep learning models with continuous output scores\n(e.g., computed by softmax), used in social applications. A flip point is any\npoint that lies on the boundary between two output classes: e.g. for a model\nwith a binary yes/no output, a flip point is any input that generates equal\nscores for \"yes\" and \"no\". The flip point closest to a given input is of\nparticular importance because it reveals the least changes in the input that\nwould change a model's classification, and we show that it is the solution to a\nwell-posed optimization problem. Flip points also enable us to systematically\nstudy the decision boundaries of a deep learning classifier. The resulting\ninsight into the decision boundaries of a deep model can clearly explain the\nmodel's output on the individual-level, via an explanation report that is\nunderstandable by non-experts. We also develop a procedure to understand and\naudit model behavior towards groups of people. Flip points can also be used to\nalter the decision boundaries in order to improve undesirable behaviors. We\ndemonstrate our methods by investigating several models trained on standard\ndatasets used in social applications of machine learning. We also identify the\nfeatures that are most responsible for particular classifications and\nmisclassifications.\n",
        "published": "2020",
        "authors": [
            "Roozbeh Yousefzadeh",
            "Dianne P. O'Leary"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.01390v1",
        "title": "Memory and attention in deep learning",
        "abstract": "  Intelligence necessitates memory. Without memory, humans fail to perform\nvarious nontrivial tasks such as reading novels, playing games or solving\nmaths. As the ultimate goal of machine learning is to derive intelligent\nsystems that learn and act automatically just like human, memory construction\nfor machine is inevitable. Artificial neural networks model neurons and\nsynapses in the brain by interconnecting computational units via weights, which\nis a typical class of machine learning algorithms that resembles memory\nstructure. Their descendants with more complicated modeling techniques (a.k.a\ndeep learning) have been successfully applied to many practical problems and\ndemonstrated the importance of memory in the learning process of machinery\nsystems. Recent progresses on modeling memory in deep learning have revolved\naround external memory constructions, which are highly inspired by\ncomputational Turing models and biological neuronal systems. Attention\nmechanisms are derived to support acquisition and retention operations on the\nexternal memory. Despite the lack of theoretical foundations, these approaches\nhave shown promises to help machinery systems reach a higher level of\nintelligence. The aim of this thesis is to advance the understanding on memory\nand attention in deep learning. Its contributions include: (i) presenting a\ncollection of taxonomies for memory, (ii) constructing new memory-augmented\nneural networks (MANNs) that support multiple control and memory units, (iii)\nintroducing variability via memory in sequential generative models, (iv)\nsearching for optimal writing operations to maximise the memorisation capacity\nin slot-based memory networks, and (v) simulating the Universal Turing Machine\nvia Neural Stored-program Memory-a new kind of external memory for neural\nnetworks.\n",
        "published": "2021",
        "authors": [
            "Hung Le"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.06540v1",
        "title": "Deep Learning for Political Science",
        "abstract": "  Political science, and social science in general, have traditionally been\nusing computational methods to study areas such as voting behavior, policy\nmaking, international conflict, and international development. More recently,\nincreasingly available quantities of data are being combined with improved\nalgorithms and affordable computational resources to predict, learn, and\ndiscover new insights from data that is large in volume and variety. New\ndevelopments in the areas of machine learning, deep learning, natural language\nprocessing (NLP), and, more generally, artificial intelligence (AI) are opening\nup new opportunities for testing theories and evaluating the impact of\ninterventions and programs in a more dynamic and effective way. Applications\nusing large volumes of structured and unstructured data are becoming common in\ngovernment and industry, and increasingly also in social science research. This\nchapter offers an introduction to such methods drawing examples from political\nscience. Focusing on the areas where the strengths of the methods coincide with\nchallenges in these fields, the chapter first presents an introduction to AI\nand its core technology - machine learning, with its rapidly developing\nsubfield of deep learning. The discussion of deep neural networks is\nillustrated with the NLP tasks that are relevant to political science. The\nlatest advances in deep learning methods for NLP are also reviewed, together\nwith their potential for improving information extraction and pattern\nrecognition from political science texts.\n",
        "published": "2020",
        "authors": [
            "Kakia Chatsiou",
            "Slava Jankin Mikhaylov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.02572v1",
        "title": "SINGA-Easy: An Easy-to-Use Framework for MultiModal Analysis",
        "abstract": "  Deep learning has achieved great success in a wide spectrum of multimedia\napplications such as image classification, natural language processing and\nmultimodal data analysis. Recent years have seen the development of many deep\nlearning frameworks that provide a high-level programming interface for users\nto design models, conduct training and deploy inference. However, it remains\nchallenging to build an efficient end-to-end multimedia application with most\nexisting frameworks. Specifically, in terms of usability, it is demanding for\nnon-experts to implement deep learning models, obtain the right settings for\nthe entire machine learning pipeline, manage models and datasets, and exploit\nexternal data sources all together. Further, in terms of adaptability, elastic\ncomputation solutions are much needed as the actual serving workload fluctuates\nconstantly, and scaling the hardware resources to handle the fluctuating\nworkload is typically infeasible. To address these challenges, we introduce\nSINGA-Easy, a new deep learning framework that provides distributed\nhyper-parameter tuning at the training stage, dynamic computational cost\ncontrol at the inference stage, and intuitive user interactions with multimedia\ncontents facilitated by model explanation. Our experiments on the training and\ndeployment of multi-modality data analysis applications show that the framework\nis both usable and adaptable to dynamic inference loads. We implement\nSINGA-Easy on top of Apache SINGA and demonstrate our system with the entire\nmachine learning life cycle.\n",
        "published": "2021",
        "authors": [
            "Naili Xing",
            "Sai Ho Yeung",
            "Chenghao Cai",
            "Teck Khim Ng",
            "Wei Wang",
            "Kaiyuan Yang",
            "Nan Yang",
            "Meihui Zhang",
            "Gang Chen",
            "Beng Chin Ooi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.12304v2",
        "title": "Uncertainty Injection: A Deep Learning Method for Robust Optimization",
        "abstract": "  This paper proposes a paradigm of uncertainty injection for training deep\nlearning model to solve robust optimization problems. The majority of existing\nstudies on deep learning focus on the model learning capability, while assuming\nthe quality and accuracy of the inputs data can be guaranteed. However, in\nrealistic applications of deep learning for solving optimization problems, the\naccuracy of inputs, which are the problem parameters in this case, plays a\nlarge role. This is because, in many situations, it is often costly or sometime\nimpossible to obtain the problem parameters accurately, and correspondingly, it\nis highly desirable to develop learning algorithms that can account for the\nuncertainties in the input and produce solutions that are robust against these\nuncertainties. This paper presents a novel uncertainty injection scheme for\ntraining machine learning models that are capable of implicitly accounting for\nthe uncertainties and producing statistically robust solutions. We further\nidentify the wireless communications as an application field where\nuncertainties are prevalent in problem parameters such as the channel\ncoefficients. We show the effectiveness of the proposed training scheme in two\napplications: the robust power loading for multiuser\nmultiple-input-multiple-output (MIMO) downlink transmissions; and the robust\npower control for device-to-device (D2D) networks.\n",
        "published": "2023",
        "authors": [
            "Wei Cui",
            "Wei Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.13816v2",
        "title": "Homological Convolutional Neural Networks",
        "abstract": "  Deep learning methods have demonstrated outstanding performances on\nclassification and regression tasks on homogeneous data types (e.g., image,\naudio, and text data). However, tabular data still pose a challenge, with\nclassic machine learning approaches being often computationally cheaper and\nequally effective than increasingly complex deep learning architectures. The\nchallenge arises from the fact that, in tabular data, the correlation among\nfeatures is weaker than the one from spatial or semantic relationships in\nimages or natural language, and the dependency structures need to be modeled\nwithout any prior information. In this work, we propose a novel deep learning\narchitecture that exploits the data structural organization through\ntopologically constrained network representations to gain relational\ninformation from sparse tabular inputs. The resulting model leverages the power\nof convolution and is centered on a limited number of concepts from network\ntopology to guarantee: (i) a data-centric and deterministic building pipeline;\n(ii) a high level of interpretability over the inference process; and (iii) an\nadequate room for scalability. We test our model on 18 benchmark datasets\nagainst 5 classic machine learning and 3 deep learning models, demonstrating\nthat our approach reaches state-of-the-art performances on these challenging\ndatasets. The code to reproduce all our experiments is provided at\nhttps://github.com/FinancialComputingUCL/HomologicalCNN.\n",
        "published": "2023",
        "authors": [
            "Antonio Briola",
            "Yuanrong Wang",
            "Silvia Bartolucci",
            "Tomaso Aste"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.03502v2",
        "title": "Deep Learning for Sensor-based Activity Recognition: A Survey",
        "abstract": "  Sensor-based activity recognition seeks the profound high-level knowledge\nabout human activities from multitudes of low-level sensor readings.\nConventional pattern recognition approaches have made tremendous progress in\nthe past years. However, those methods often heavily rely on heuristic\nhand-crafted feature extraction, which could hinder their generalization\nperformance. Additionally, existing methods are undermined for unsupervised and\nincremental learning tasks. Recently, the recent advancement of deep learning\nmakes it possible to perform automatic high-level feature extraction thus\nachieves promising performance in many areas. Since then, deep learning based\nmethods have been widely adopted for the sensor-based activity recognition\ntasks. This paper surveys the recent advance of deep learning based\nsensor-based activity recognition. We summarize existing literature from three\naspects: sensor modality, deep model, and application. We also present detailed\ninsights on existing work and propose grand challenges for future research.\n",
        "published": "2017",
        "authors": [
            "Jindong Wang",
            "Yiqiang Chen",
            "Shuji Hao",
            "Xiaohui Peng",
            "Lisha Hu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.10437v1",
        "title": "Deep Learning Works in Practice. But Does it Work in Theory?",
        "abstract": "  Deep learning relies on a very specific kind of neural networks: those\nsuperposing several neural layers. In the last few years, deep learning\nachieved major breakthroughs in many tasks such as image analysis, speech\nrecognition, natural language processing, and so on. Yet, there is no\ntheoretical explanation of this success. In particular, it is not clear why the\ndeeper the network, the better it actually performs.\n  We argue that the explanation is intimately connected to a key feature of the\ndata collected from our surrounding universe to feed the machine learning\nalgorithms: large non-parallelizable logical depth. Roughly speaking, we\nconjecture that the shortest computational descriptions of the universe are\nalgorithms with inherently large computation times, even when a large number of\ncomputers are available for parallelization. Interestingly, this conjecture,\ncombined with the folklore conjecture in theoretical computer science that $ P\n\\neq NC$, explains the success of deep learning.\n",
        "published": "2018",
        "authors": [
            "L\u00ea Nguy\u00ean Hoang",
            "Rachid Guerraoui"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1712.06113v3",
        "title": "SchNet - a deep learning architecture for molecules and materials",
        "abstract": "  Deep learning has led to a paradigm shift in artificial intelligence,\nincluding web, text and image search, speech recognition, as well as\nbioinformatics, with growing impact in chemical physics. Machine learning in\ngeneral and deep learning in particular is ideally suited for representing\nquantum-mechanical interactions, enabling to model nonlinear potential-energy\nsurfaces or enhancing the exploration of chemical compound space. Here we\npresent the deep learning architecture SchNet that is specifically designed to\nmodel atomistic systems by making use of continuous-filter convolutional\nlayers. We demonstrate the capabilities of SchNet by accurately predicting a\nrange of properties across chemical space for \\emph{molecules and materials}\nwhere our model learns chemically plausible embeddings of atom types across the\nperiodic table. Finally, we employ SchNet to predict potential-energy surfaces\nand energy-conserving force fields for molecular dynamics simulations of small\nmolecules and perform an exemplary study of the quantum-mechanical properties\nof C$_{20}$-fullerene that would have been infeasible with regular ab initio\nmolecular dynamics.\n",
        "published": "2017",
        "authors": [
            "Kristof T. Sch\u00fctt",
            "Huziel E. Sauceda",
            "Pieter-Jan Kindermans",
            "Alexandre Tkatchenko",
            "Klaus-Robert M\u00fcller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.03216v1",
        "title": "Modeling the Chlorophyll-a from Sea Surface Reflectance in West Africa\n  by Deep Learning Methods: A Comparison of Multiple Algorithms",
        "abstract": "  Deep learning provide successful applications in many fields. Recently,\nmachines learning are involved for oceans remote sensing applications. In this\nstudy, we use and compare about eight (8) deep learning estimators for\nretrieval of a mainly pigment of phytoplankton. Depending on the water case and\nthe multiple instruments simultaneouslyobserving the earth on a variety of\nplatforms, several algorithm are used to estimate the chlolophyll-a from marine\nreflectance. By using a long-term multi-sensor time-series of satellite\nocean-colour data, as MODIS, SeaWifs, VIIRS, MERIS, etc, we make a unique deep\nnetwork model able to establish a relationship between sea surface reflectance\nand chlorophyll-a from any measurement satellite sensor over West Africa. These\ndata fusion take into account the bias between case water and instruments. We\nconstruct several chlorophyll-a concentration prediction deep learning based\nmodels, compare them and therefore use the best for our study. Results obtained\nfor accuracy training and test are quite good. The mean absolute error are very\nlow and vary between 0,07 to 0,13 mg/m3.\n",
        "published": "2019",
        "authors": [
            "Daouda Diouf",
            "Djibril Seck"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.01200v4",
        "title": "Natural Language Processing Advancements By Deep Learning: A Survey",
        "abstract": "  Natural Language Processing (NLP) helps empower intelligent machines by\nenhancing a better understanding of the human language for linguistic-based\nhuman-computer communication. Recent developments in computational power and\nthe advent of large amounts of linguistic data have heightened the need and\ndemand for automating semantic analysis using data-driven approaches. The\nutilization of data-driven strategies is pervasive now due to the significant\nimprovements demonstrated through the usage of deep learning methods in areas\nsuch as Computer Vision, Automatic Speech Recognition, and in particular, NLP.\nThis survey categorizes and addresses the different aspects and applications of\nNLP that have benefited from deep learning. It covers core NLP tasks and\napplications and describes how deep learning methods and models advance these\nareas. We further analyze and compare different approaches and state-of-the-art\nmodels.\n",
        "published": "2020",
        "authors": [
            "Amirsina Torfi",
            "Rouzbeh A. Shirvani",
            "Yaser Keneshloo",
            "Nader Tavaf",
            "Edward A. Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.07935v1",
        "title": "Multi-Agent Motion Planning using Deep Learning for Space Applications",
        "abstract": "  State-of-the-art motion planners cannot scale to a large number of systems.\nMotion planning for multiple agents is an NP (non-deterministic\npolynomial-time) hard problem, so the computation time increases exponentially\nwith each addition of agents. This computational demand is a major stumbling\nblock to the motion planner's application to future NASA missions involving the\nswarm of space vehicles. We applied a deep neural network to transform\ncomputationally demanding mathematical motion planning problems into deep\nlearning-based numerical problems. We showed optimal motion trajectories can be\naccurately replicated using deep learning-based numerical models in several 2D\nand 3D systems with multiple agents. The deep learning-based numerical model\ndemonstrates superior computational efficiency with plans generated 1000 times\nfaster than the mathematical model counterpart.\n",
        "published": "2020",
        "authors": [
            "Kyongsik Yun",
            "Changrak Choi",
            "Ryan Alimo",
            "Anthony Davis",
            "Linda Forster",
            "Amir Rahmani",
            "Muhammad Adil",
            "Ramtin Madani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.07065v1",
        "title": "Visual analogy: Deep learning versus compositional models",
        "abstract": "  Is analogical reasoning a task that must be learned to solve from scratch by\napplying deep learning models to massive numbers of reasoning problems? Or are\nanalogies solved by computing similarities between structured representations\nof analogs? We address this question by comparing human performance on visual\nanalogies created using images of familiar three-dimensional objects (cars and\ntheir subregions) with the performance of alternative computational models.\nHuman reasoners achieved above-chance accuracy for all problem types, but made\nmore errors in several conditions (e.g., when relevant subregions were\noccluded). We compared human performance to that of two recent deep learning\nmodels (Siamese Network and Relation Network) directly trained to solve these\nanalogy problems, as well as to that of a compositional model that assesses\nrelational similarity between part-based representations. The compositional\nmodel based on part representations, but not the deep learning models,\ngenerated qualitative performance similar to that of human reasoners.\n",
        "published": "2021",
        "authors": [
            "Nicholas Ichien",
            "Qing Liu",
            "Shuhao Fu",
            "Keith J. Holyoak",
            "Alan Yuille",
            "Hongjing Lu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1704.06857v1",
        "title": "A Review on Deep Learning Techniques Applied to Semantic Segmentation",
        "abstract": "  Image semantic segmentation is more and more being of interest for computer\nvision and machine learning researchers. Many applications on the rise need\naccurate and efficient segmentation mechanisms: autonomous driving, indoor\nnavigation, and even virtual or augmented reality systems to name a few. This\ndemand coincides with the rise of deep learning approaches in almost every\nfield or application target related to computer vision, including semantic\nsegmentation or scene understanding. This paper provides a review on deep\nlearning methods for semantic segmentation applied to various application\nareas. Firstly, we describe the terminology of this field as well as mandatory\nbackground concepts. Next, the main datasets and challenges are exposed to help\nresearchers decide which are the ones that best suit their needs and their\ntargets. Then, existing methods are reviewed, highlighting their contributions\nand their significance in the field. Finally, quantitative results are given\nfor the described methods and the datasets in which they were evaluated,\nfollowing up with a discussion of the results. At last, we point out a set of\npromising future works and draw our own conclusions about the state of the art\nof semantic segmentation using deep learning techniques.\n",
        "published": "2017",
        "authors": [
            "Alberto Garcia-Garcia",
            "Sergio Orts-Escolano",
            "Sergiu Oprea",
            "Victor Villena-Martinez",
            "Jose Garcia-Rodriguez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.05385v3",
        "title": "On the Decision Boundary of Deep Neural Networks",
        "abstract": "  While deep learning models and techniques have achieved great empirical\nsuccess, our understanding of the source of success in many aspects remains\nvery limited. In an attempt to bridge the gap, we investigate the decision\nboundary of a production deep learning architecture with weak assumptions on\nboth the training data and the model. We demonstrate, both theoretically and\nempirically, that the last weight layer of a neural network converges to a\nlinear SVM trained on the output of the last hidden layer, for both the binary\ncase and the multi-class case with the commonly used cross-entropy loss.\nFurthermore, we show empirically that training a neural network as a whole,\ninstead of only fine-tuning the last weight layer, may result in better bias\nconstant for the last weight layer, which is important for generalization. In\naddition to facilitating the understanding of deep learning, our result can be\nhelpful for solving a broad range of practical problems of deep learning, such\nas catastrophic forgetting and adversarial attacking. The experiment codes are\navailable at https://github.com/lykaust15/NN_decision_boundary\n",
        "published": "2018",
        "authors": [
            "Yu Li",
            "Lizhong Ding",
            "Xin Gao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.11187v1",
        "title": "Optimal Approach for Image Recognition using Deep Convolutional\n  Architecture",
        "abstract": "  In the recent time deep learning has achieved huge popularity due to its\nperformance in various machine learning algorithms. Deep learning as\nhierarchical or structured learning attempts to model high level abstractions\nin data by using a group of processing layers. The foundation of deep learning\narchitectures is inspired by the understanding of information processing and\nneural responses in human brain. The architectures are created by stacking\nmultiple linear or non-linear operations. The article mainly focuses on the\nstate-of-art deep learning models and various real world applications specific\ntraining methods. Selecting optimal architecture for specific problem is a\nchallenging task, at a closing stage of the article we proposed optimal\napproach to deep convolutional architecture for the application of image\nrecognition.\n",
        "published": "2019",
        "authors": [
            "Parth Shah",
            "Vishvajit Bakrola",
            "Supriya Pati"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.10074v1",
        "title": "Cost-Based Goal Recognition Meets Deep Learning",
        "abstract": "  The ability to observe the effects of actions performed by others and to\ninfer their intent, most likely goals, or course of action, is known as a plan\nor intention recognition cognitive capability and has long been one of the\nfundamental research challenges in AI. Deep learning has recently been making\nsignificant inroads on various pattern recognition problems, except for\nintention recognition. While extensively explored since the seventies, the\nproblem remains unsolved for most interesting cases in various areas, ranging\nfrom natural language understanding to human behavior understanding based on\nvideo feeds. This paper compares symbolic inverse planning, one of the most\ninvestigated approaches to goal recognition, to deep learning using CNN and\nLTSM neural network architectures, on five synthetic benchmarks often used in\nthe literature. The results show that the deep learning approach achieves\nbetter goal-prediction accuracy and timeliness than the symbolic cost-based\nplan recognizer in these domains. Although preliminary, these results point to\ninteresting future research avenues.\n",
        "published": "2019",
        "authors": [
            "Mariane Maynard",
            "Thibault Duhamel",
            "Froduald Kabanza"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.04452v1",
        "title": "Comparison between ARIMA and Deep Learning Models for Temperature\n  Forecasting",
        "abstract": "  Weather forecasting benefits us in various ways from farmers in cultivation\nand harvesting their crops to airlines to schedule their flights. Weather\nforecasting is a challenging task due to the chaotic nature of the atmosphere.\nTherefore lot of research attention has drawn to obtain the benefits and to\novercome the challenges of weather forecasting. This paper compares ARIMA (Auto\nRegressive Integrated Moving Average) model and deep learning models to\nforecast temperature. The deep learning model consists of one dimensional\nconvolutional layers to extract spatial features and LSTM layers to extract\ntemporal features. Both of these models are applied to hourly temperature data\nset from Szeged, Hungry. According to the experimental results deep learning\nmodel was able to perform better than the traditional ARIMA methodology.\n",
        "published": "2020",
        "authors": [
            "Eranga De Saa",
            "Lochandaka Ranathunga"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.08341v1",
        "title": "Robust Deep Learning with Active Noise Cancellation for Spatial\n  Computing",
        "abstract": "  This paper proposes CANC, a Co-teaching Active Noise Cancellation method,\napplied in spatial computing to address deep learning trained with extreme\nnoisy labels. Deep learning algorithms have been successful in spatial\ncomputing for land or building footprint recognition. However a lot of noise\nexists in ground truth labels due to how labels are collected in spatial\ncomputing and satellite imagery. Existing methods to deal with extreme label\nnoise conduct clean sample selection and do not utilize the remaining samples.\nSuch techniques can be wasteful due to the cost of data retrieval. Our proposed\nCANC algorithm not only conserves high-cost training samples but also provides\nactive label correction to better improve robust deep learning with extreme\nnoisy labels. We demonstrate the effectiveness of CANC for building footprint\nrecognition for spatial computing.\n",
        "published": "2020",
        "authors": [
            "Li Chen",
            "David Yang",
            "Purvi Goel",
            "Ilknur Kabul"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.11347v1",
        "title": "Time Series Data Imputation: A Survey on Deep Learning Approaches",
        "abstract": "  Time series are all around in real-world applications. However, unexpected\naccidents for example broken sensors or missing of the signals will cause\nmissing values in time series, making the data hard to be utilized. It then\ndoes harm to the downstream applications such as traditional classification or\nregression, sequential data integration and forecasting tasks, thus raising the\ndemand for data imputation. Currently, time series data imputation is a\nwell-studied problem with different categories of methods. However, these works\nrarely take the temporal relations among the observations and treat the time\nseries as normal structured data, losing the information from the time data. In\nrecent, deep learning models have raised great attention. Time series methods\nbased on deep learning have made progress with the usage of models like RNN,\nsince it captures time information from data. In this paper, we mainly focus on\ntime series imputation technique with deep learning methods, which recently\nmade progress in this field. We will review and discuss their model\narchitectures, their pros and cons as well as their effects to show the\ndevelopment of the time series imputation methods.\n",
        "published": "2020",
        "authors": [
            "Chenguang Fang",
            "Chen Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.00106v1",
        "title": "Towards Auditability for Fairness in Deep Learning",
        "abstract": "  Group fairness metrics can detect when a deep learning model behaves\ndifferently for advantaged and disadvantaged groups, but even models that score\nwell on these metrics can make blatantly unfair predictions. We present smooth\nprediction sensitivity, an efficiently computed measure of individual fairness\nfor deep learning models that is inspired by ideas from interpretability in\ndeep learning. smooth prediction sensitivity allows individual predictions to\nbe audited for fairness. We present preliminary experimental results suggesting\nthat smooth prediction sensitivity can help distinguish between fair and unfair\npredictions, and that it may be helpful in detecting blatantly unfair\npredictions from \"group-fair\" models.\n",
        "published": "2020",
        "authors": [
            "Ivoline C. Ngong",
            "Krystal Maughan",
            "Joseph P. Near"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.05685v2",
        "title": "Generative Deep Learning Techniques for Password Generation",
        "abstract": "  Password guessing approaches via deep learning have recently been\ninvestigated with significant breakthroughs in their ability to generate novel,\nrealistic password candidates. In the present work we study a broad collection\nof deep learning and probabilistic based models in the light of password\nguessing: attention-based deep neural networks, autoencoding mechanisms and\ngenerative adversarial networks. We provide novel generative deep-learning\nmodels in terms of variational autoencoders exhibiting state-of-art sampling\nperformance, yielding additional latent-space features such as interpolations\nand targeted sampling. Lastly, we perform a thorough empirical analysis in a\nunified controlled framework over well-known datasets (RockYou, LinkedIn,\nYouku, Zomato, Pwnd). Our results not only identify the most promising schemes\ndriven by deep neural networks, but also illustrate the strengths of each\napproach in terms of generation variability and sample uniqueness.\n",
        "published": "2020",
        "authors": [
            "David Biesner",
            "Kostadin Cvejoski",
            "Bogdan Georgiev",
            "Rafet Sifa",
            "Erik Krupicka"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.03645v1",
        "title": "Online Evolutionary Batch Size Orchestration for Scheduling Deep\n  Learning Workloads in GPU Clusters",
        "abstract": "  Efficient GPU resource scheduling is essential to maximize resource\nutilization and save training costs for the increasing amount of deep learning\nworkloads in shared GPU clusters. Existing GPU schedulers largely rely on\nstatic policies to leverage the performance characteristics of deep learning\njobs. However, they can hardly reach optimal efficiency due to the lack of\nelasticity. To address the problem, we propose ONES, an ONline Evolutionary\nScheduler for elastic batch size orchestration. ONES automatically manages the\nelasticity of each job based on the training batch size, so as to maximize GPU\nutilization and improve scheduling efficiency. It determines the batch size for\neach job through an online evolutionary search that can continuously optimize\nthe scheduling decisions. We evaluate the effectiveness of ONES with 64 GPUs on\nTACC's Longhorn supercomputers. The results show that ONES can outperform the\nprior deep learning schedulers with a significantly shorter average job\ncompletion time.\n",
        "published": "2021",
        "authors": [
            "Zhengda Bian",
            "Shenggui Li",
            "Wei Wang",
            "Yang You"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.08876v2",
        "title": "Deep Learning-based Spacecraft Relative Navigation Methods: A Survey",
        "abstract": "  Autonomous spacecraft relative navigation technology has been planned for and\napplied to many famous space missions. The development of on-board electronics\nsystems has enabled the use of vision-based and LiDAR-based methods to achieve\nbetter performances. Meanwhile, deep learning has reached great success in\ndifferent areas, especially in computer vision, which has also attracted the\nattention of space researchers. However, spacecraft navigation differs from\nground tasks due to high reliability requirements but lack of large datasets.\nThis survey aims to systematically investigate the current deep learning-based\nautonomous spacecraft relative navigation methods, focusing on concrete orbital\napplications such as spacecraft rendezvous and landing on small bodies or the\nMoon. The fundamental characteristics, primary motivations, and contributions\nof deep learning-based relative navigation algorithms are first summarised from\nthree perspectives of spacecraft rendezvous, asteroid exploration, and terrain\nnavigation. Furthermore, popular visual tracking benchmarks and their\nrespective properties are compared and summarised. Finally, potential\napplications are discussed, along with expected impediments.\n",
        "published": "2021",
        "authors": [
            "Jianing Song",
            "Duarte Rondao",
            "Nabil Aouf"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.06601v1",
        "title": "Vibration-Based Condition Monitoring By Ensemble Deep Learning",
        "abstract": "  Vibration-based techniques are among the most common condition monitoring\napproaches. With the advancement of computers, these approaches have also been\nimproved such that recently, these approaches in conjunction with deep learning\nmethods attract attention among researchers. This is mostly due to the nature\nof the deep learning method that could facilitate the monitoring procedure by\nintegrating the feature extraction, feature selection, and classification steps\ninto one automated step. However, this can be achieved at the expense of\nchallenges in designing the architecture of a deep learner, tuning its\nhyper-parameters. Moreover, it sometimes gives low generalization capability.\nAs a remedy to these problems, this study proposes a framework based on\nensemble deep learning methodology. The framework was initiated by creating a\npool of Convolutional neural networks (CNN). To create diversity to the CNNs,\nthey are fed by frequency responses which are passed through different\nfunctions. As the next step, proper CNNs are selected based on an information\ncriterion to be used for fusion. The fusion is then carried out by improved\nDempster-Shafer theory. The proposed framework is applied to real test data\ncollected from Equiax Polycrystalline Nickel alloy first-stage turbine blades\nwith complex geometry.\n",
        "published": "2021",
        "authors": [
            "Vahid Yaghoubi",
            "Liangliang Cheng",
            "Wim Van Paepegem",
            "Mathias Keremans"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.08557v2",
        "title": "DPNAS: Neural Architecture Search for Deep Learning with Differential\n  Privacy",
        "abstract": "  Training deep neural networks (DNNs) for meaningful differential privacy (DP)\nguarantees severely degrades model utility. In this paper, we demonstrate that\nthe architecture of DNNs has a significant impact on model utility in the\ncontext of private deep learning, whereas its effect is largely unexplored in\nprevious studies. In light of this missing, we propose the very first framework\nthat employs neural architecture search to automatic model design for private\ndeep learning, dubbed as DPNAS. To integrate private learning with architecture\nsearch, we delicately design a novel search space and propose a DP-aware method\nfor training candidate models. We empirically certify the effectiveness of the\nproposed framework. The searched model DPNASNet achieves state-of-the-art\nprivacy/utility trade-offs, e.g., for the privacy budget of $(\\epsilon,\n\\delta)=(3, 1\\times10^{-5})$, our model obtains test accuracy of $98.57\\%$ on\nMNIST, $88.09\\%$ on FashionMNIST, and $68.33\\%$ on CIFAR-10. Furthermore, by\nstudying the generated architectures, we provide several intriguing findings of\ndesigning private-learning-friendly DNNs, which can shed new light on model\ndesign for deep learning with differential privacy.\n",
        "published": "2021",
        "authors": [
            "Anda Cheng",
            "Jiaxing Wang",
            "Xi Sheryl Zhang",
            "Qiang Chen",
            "Peisong Wang",
            "Jian Cheng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.10233v2",
        "title": "Forecasting Market Prices using DL with Data Augmentation and\n  Meta-learning: ARIMA still wins!",
        "abstract": "  Deep-learning techniques have been successfully used for time-series\nforecasting and have often shown superior performance on many standard\nbenchmark datasets as compared to traditional techniques. Here we present a\ncomprehensive and comparative study of performance of deep-learning techniques\nfor forecasting prices in financial markets. We benchmark state-of-the-art\ndeep-learning baselines, such as NBeats, etc., on data from currency as well as\nstock markets. We also generate synthetic data using a fuzzy-logic based model\nof demand driven by technical rules such as moving averages, which are often\nused by traders. We benchmark the baseline techniques on this synthetic data as\nwell as use it for data augmentation. We also apply gradient-based\nmeta-learning to account for non-stationarity of financial time-series. Our\nextensive experiments notwithstanding, the surprising result is that the\nstandard ARIMA models outperforms deep-learning even using data augmentation or\nmeta-learning. We conclude by speculating as to why this might be the case.\n",
        "published": "2021",
        "authors": [
            "Vedant Shah",
            "Gautam Shroff"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.11424v1",
        "title": "Analysis of memory consumption by neural networks based on\n  hyperparameters",
        "abstract": "  Deep learning models are trained and deployed in multiple domains. Increasing\nusage of deep learning models alarms the usage of memory consumed while\ncomputation by deep learning models. Existing approaches for reducing memory\nconsumption like model compression, hardware changes are specific. We propose a\ngeneric analysis of memory consumption while training deep learning models in\ncomparison with hyperparameters used for training. Hyperparameters which\nincludes the learning rate, batchsize, number of hidden layers and depth of\nlayers decide the model performance, accuracy of the model. We assume the\noptimizers and type of hidden layers as a known values. The change in\nhyperparamaters and the number of hidden layers are the variables considered in\nthis proposed approach. For better understanding of the computation cost, this\nproposed analysis studies the change in memory consumption with respect to\nhyperparameters as main focus. This results in general analysis of memory\nconsumption changes during training when set of hyperparameters are altered.\n",
        "published": "2021",
        "authors": [
            "Mahendran N"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.12123v1",
        "title": "MICS : Multi-steps, Inverse Consistency and Symmetric deep learning\n  registration network",
        "abstract": "  Deformable registration consists of finding the best dense correspondence\nbetween two different images. Many algorithms have been published, but the\nclinical application was made difficult by the high calculation time needed to\nsolve the optimisation problem. Deep learning overtook this limitation by\ntaking advantage of GPU calculation and the learning process. However, many\ndeep learning methods do not take into account desirable properties respected\nby classical algorithms.\n  In this paper, we present MICS, a novel deep learning algorithm for medical\nimaging registration. As registration is an ill-posed problem, we focused our\nalgorithm on the respect of different properties: inverse consistency, symmetry\nand orientation conservation. We also combined our algorithm with a multi-step\nstrategy to refine and improve the deformation grid. While many approaches\napplied registration to brain MRI, we explored a more challenging body\nlocalisation: abdominal CT. Finally, we evaluated our method on a dataset used\nduring the Learn2Reg challenge, allowing a fair comparison with published\nmethods.\n",
        "published": "2021",
        "authors": [
            "Th\u00e9o Estienne",
            "Maria Vakalopoulou",
            "Enzo Battistella",
            "Theophraste Henry",
            "Marvin Lerousseau",
            "Amaury Leroy",
            "Nikos Paragios",
            "Eric Deutsch"
        ]
    }
]