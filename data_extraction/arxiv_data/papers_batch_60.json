[
    {
        "id": "http://arxiv.org/abs/1910.05245v1",
        "title": "Decoupling Hierarchical Recurrent Neural Networks With Locally\n  Computable Losses",
        "abstract": "  Learning long-term dependencies is a key long-standing challenge of recurrent\nneural networks (RNNs). Hierarchical recurrent neural networks (HRNNs) have\nbeen considered a promising approach as long-term dependencies are resolved\nthrough shortcuts up and down the hierarchy. Yet, the memory requirements of\nTruncated Backpropagation Through Time (TBPTT) still prevent training them on\nvery long sequences. In this paper, we empirically show that in (deep) HRNNs,\npropagating gradients back from higher to lower levels can be replaced by\nlocally computable losses, without harming the learning capability of the\nnetwork, over a wide range of tasks. This decoupling by local losses reduces\nthe memory requirements of training by a factor exponential in the depth of the\nhierarchy in comparison to standard TBPTT.\n",
        "published": "2019",
        "authors": [
            "Asier Mujika",
            "Felix Weissenberger",
            "Angelika Steger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.05493v2",
        "title": "Deep Transfer Learning for Source Code Modeling",
        "abstract": "  In recent years, deep learning models have shown great potential in source\ncode modeling and analysis. Generally, deep learning-based approaches are\nproblem-specific and data-hungry. A challenging issue of these approaches is\nthat they require training from starch for a different related problem. In this\nwork, we propose a transfer learning-based approach that significantly improves\nthe performance of deep learning-based source code models. In contrast to\ntraditional learning paradigms, transfer learning can transfer the knowledge\nlearned in solving one problem into another related problem. First, we present\ntwo recurrent neural network-based models RNN and GRU for the purpose of\ntransfer learning in the domain of source code modeling. Next, via transfer\nlearning, these pre-trained (RNN and GRU) models are used as feature\nextractors. Then, these extracted features are combined into attention learner\nfor different downstream tasks. The attention learner leverages from the\nlearned knowledge of pre-trained models and fine-tunes them for a specific\ndownstream task. We evaluate the performance of the proposed approach with\nextensive experiments with the source code suggestion task. The results\nindicate that the proposed approach outperforms the state-of-the-art models in\nterms of accuracy, precision, recall, and F-measure without training the models\nfrom scratch.\n",
        "published": "2019",
        "authors": [
            "Yasir Hussain",
            "Zhiqiu Huang",
            "Yu Zhou",
            "Senzhang Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.05929v1",
        "title": "Emergent properties of the local geometry of neural loss landscapes",
        "abstract": "  The local geometry of high dimensional neural network loss landscapes can\nboth challenge our cherished theoretical intuitions as well as dramatically\nimpact the practical success of neural network training. Indeed recent works\nhave observed 4 striking local properties of neural loss landscapes on\nclassification tasks: (1) the landscape exhibits exactly $C$ directions of high\npositive curvature, where $C$ is the number of classes; (2) gradient directions\nare largely confined to this extremely low dimensional subspace of positive\nHessian curvature, leaving the vast majority of directions in weight space\nunexplored; (3) gradient descent transiently explores intermediate regions of\nhigher positive curvature before eventually finding flatter minima; (4)\ntraining can be successful even when confined to low dimensional {\\it random}\naffine hyperplanes, as long as these hyperplanes intersect a Goldilocks zone of\nhigher than average curvature. We develop a simple theoretical model of\ngradients and Hessians, justified by numerical experiments on architectures and\ndatasets used in practice, that {\\it simultaneously} accounts for all $4$ of\nthese surprising and seemingly unrelated properties. Our unified model provides\nconceptual insights into the emergence of these properties and makes\nconnections with diverse topics in neural networks, random matrix theory, and\nspin glasses, including the neural tangent kernel, BBP phase transitions, and\nDerrida's random energy model.\n",
        "published": "2019",
        "authors": [
            "Stanislav Fort",
            "Surya Ganguli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.07012v1",
        "title": "Transfer Learning for Algorithm Recommendation",
        "abstract": "  Meta-Learning is a subarea of Machine Learning that aims to take advantage of\nprior knowledge to learn faster and with fewer data [1]. There are different\nscenarios where meta-learning can be applied, and one of the most common is\nalgorithm recommendation, where previous experience on applying machine\nlearning algorithms for several datasets can be used to learn which algorithm,\nfrom a set of options, would be more suitable for a new dataset [2]. Perhaps\nthe most popular form of meta-learning is transfer learning, which consists of\ntransferring knowledge acquired by a machine learning algorithm in a previous\nlearning task to increase its performance faster in another and similar task\n[3]. Transfer Learning has been widely applied in a variety of complex tasks\nsuch as image classification, machine translation and, speech recognition,\nachieving remarkable results [4,5,6,7,8]. Although transfer learning is very\nused in traditional or base-learning, it is still unknown if it is useful in a\nmeta-learning setup. For that purpose, in this paper, we investigate the\neffects of transferring knowledge in the meta-level instead of base-level.\nThus, we train a neural network on meta-datasets related to algorithm\nrecommendation, and then using transfer learning, we reuse the knowledge\nlearned by the neural network in other similar datasets from the same domain,\nto verify how transferable is the acquired meta-knowledge.\n",
        "published": "2019",
        "authors": [
            "Gean Trindade Pereira",
            "Mois\u00e9s dos Santos",
            "Edesio Alcoba\u00e7a",
            "Rafael Mantovani",
            "Andr\u00e9 Carvalho"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.08149v1",
        "title": "Multi Label Restricted Boltzmann Machine for Non-Intrusive Load\n  Monitoring",
        "abstract": "  Increasing population indicates that energy demands need to be managed in the\nresidential sector. Prior studies have reflected that the customers tend to\nreduce a significant amount of energy consumption if they are provided with\nappliance-level feedback. This observation has increased the relevance of load\nmonitoring in today's tech-savvy world. Most of the previously proposed\nsolutions claim to perform load monitoring without intrusion, but they are not\ncompletely non-intrusive. These methods require historical appliance-level data\nfor training the model for each of the devices. This data is gathered by\nputting a sensor on each of the appliances present in the home which causes\nintrusion in the building. Some recent studies have proposed that if we frame\nNon-Intrusive Load Monitoring (NILM) as a multi-label classification problem,\nthe need for appliance-level data can be avoided. In this paper, we propose\nMulti-label Restricted Boltzmann Machine(ML-RBM) for NILM and report an\nexperimental evaluation of proposed and state-of-the-art techniques.\n",
        "published": "2019",
        "authors": [
            "Sagar Verma",
            "Shikha Singh",
            "Angshul Majumdar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.08413v1",
        "title": "Efficient Computation of Probabilistic Dominance in Robust\n  Multi-Objective Optimization",
        "abstract": "  Real-world problems typically require the simultaneous optimization of\nseveral, often conflicting objectives. Many of these multi-objective\noptimization problems are characterized by wide ranges of uncertainties in\ntheir decision variables or objective functions, which further increases the\ncomplexity of optimization. To cope with such uncertainties, robust\noptimization is widely studied aiming to distinguish candidate solutions with\nuncertain objectives specified by confidence intervals, probability\ndistributions or sampled data. However, existing techniques mostly either fail\nto consider the actual distributions or assume uncertainty as instances of\nuniform or Gaussian distributions. This paper introduces an empirical approach\nthat enables an efficient comparison of candidate solutions with uncertain\nobjectives that can follow arbitrary distributions. Given two candidate\nsolutions under comparison, this operator calculates the probability that one\nsolution dominates the other in terms of each uncertain objective. It can\nsubstitute for the standard comparison operator of existing optimization\ntechniques such as evolutionary algorithms to enable discovering robust\nsolutions to problems with multiple uncertain objectives. This paper also\nproposes to incorporate various uncertainties in well-known multi-objective\nproblems to provide a benchmark for evaluating uncertainty-aware optimization\ntechniques. The proposed comparison operator and benchmark suite are integrated\ninto an existing optimization tool that features a selection of multi-objective\noptimization problems and algorithms. Experiments show that in comparison with\nexisting techniques, the proposed approach achieves higher optimization quality\nat lower overheads.\n",
        "published": "2019",
        "authors": [
            "Faramarz Khosravi",
            "Alexander Ra\u00df",
            "J\u00fcrgen Teich"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.08475v3",
        "title": "On Warm-Starting Neural Network Training",
        "abstract": "  In many real-world deployments of machine learning systems, data arrive\npiecemeal. These learning scenarios may be passive, where data arrive\nincrementally due to structural properties of the problem (e.g., daily\nfinancial data) or active, where samples are selected according to a measure of\ntheir quality (e.g., experimental design). In both of these cases, we are\nbuilding a sequence of models that incorporate an increasing amount of data. We\nwould like each of these models in the sequence to be performant and take\nadvantage of all the data that are available to that point. Conventional\nintuition suggests that when solving a sequence of related optimization\nproblems of this form, it should be possible to initialize using the solution\nof the previous iterate -- to \"warm start\" the optimization rather than\ninitialize from scratch -- and see reductions in wall-clock time. However, in\npractice this warm-starting seems to yield poorer generalization performance\nthan models that have fresh random initializations, even though the final\ntraining losses are similar. While it appears that some hyperparameter settings\nallow a practitioner to close this generalization gap, they seem to only do so\nin regimes that damage the wall-clock gains of the warm start. Nevertheless, it\nis highly desirable to be able to warm-start neural network training, as it\nwould dramatically reduce the resource usage associated with the construction\nof performant deep learning systems. In this work, we take a closer look at\nthis empirical phenomenon and try to understand when and how it occurs. We also\nprovide a surprisingly simple trick that overcomes this pathology in several\nimportant situations, and present experiments that elucidate some of its\nproperties.\n",
        "published": "2019",
        "authors": [
            "Jordan T. Ash",
            "Ryan P. Adams"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.08581v1",
        "title": "Towards Quantifying Intrinsic Generalization of Deep ReLU Networks",
        "abstract": "  Understanding the underlying mechanisms that enable the empirical successes\nof deep neural networks is essential for further improving their performance\nand explaining such networks. Towards this goal, a specific question is how to\nexplain the \"surprising\" behavior of the same over-parametrized deep neural\nnetworks that can generalize well on real datasets and at the same time\n\"memorize\" training samples when the labels are randomized. In this paper, we\ndemonstrate that deep ReLU networks generalize from training samples to new\npoints via piece-wise linear interpolation. We provide a quantified analysis on\nthe generalization ability of a deep ReLU network: Given a fixed point\n$\\mathbf{x}$ and a fixed direction in the input space $\\mathcal{S}$, there is\nalways a segment such that any point on the segment will be classified the same\nas the fixed point $\\mathbf{x}$. We call this segment the $generalization \\\ninterval$. We show that the generalization intervals of a ReLU network behave\nsimilarly along pairwise directions between samples of the same label in both\nreal and random cases on the MNIST and CIFAR-10 datasets. This result suggests\nthat the same interpolation mechanism is used in both cases. Additionally, for\ndatasets using real labels, such networks provide a good approximation of the\nunderlying manifold in the data, where the changes are much smaller along\ntangent directions than along normal directions. On the other hand, however,\nfor datasets with random labels, generalization intervals along mid-lines of\ntriangles with the same label are much smaller than those on the datasets with\nreal labels, suggesting different behaviors along other directions. Our\nsystematic experiments demonstrate for the first time that such deep neural\nnetworks generalize through the same interpolation and explain the differences\nbetween their performance on datasets with real and random labels.\n",
        "published": "2019",
        "authors": [
            "Shaeke Salman",
            "Canlin Zhang",
            "Xiuwen Liu",
            "Washington Mio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.09594v1",
        "title": "Federated Neuromorphic Learning of Spiking Neural Networks for Low-Power\n  Edge Intelligence",
        "abstract": "  Spiking Neural Networks (SNNs) offer a promising alternative to conventional\nArtificial Neural Networks (ANNs) for the implementation of on-device low-power\nonline learning and inference. On-device training is, however, constrained by\nthe limited amount of data available at each device. In this paper, we propose\nto mitigate this problem via cooperative training through Federated Learning\n(FL). To this end, we introduce an online FL-based learning rule for networked\non-device SNNs, which we refer to as FL-SNN. FL-SNN leverages local feedback\nsignals within each SNN, in lieu of backpropagation, and global feedback\nthrough communication via a base station. The scheme demonstrates significant\nadvantages over separate training and features a flexible trade-off between\ncommunication load and accuracy via the selective exchange of synaptic weights.\n",
        "published": "2019",
        "authors": [
            "Nicolas Skatchkovsky",
            "Hyeryung Jang",
            "Osvaldo Simeone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.09745v1",
        "title": "Vanishing Nodes: Another Phenomenon That Makes Training Deep Neural\n  Networks Difficult",
        "abstract": "  It is well known that the problem of vanishing/exploding gradients is a\nchallenge when training deep networks. In this paper, we describe another\nphenomenon, called vanishing nodes, that also increases the difficulty of\ntraining deep neural networks. As the depth of a neural network increases, the\nnetwork's hidden nodes have more highly correlated behavior. This results in\ngreat similarities between these nodes. The redundancy of hidden nodes thus\nincreases as the network becomes deeper. We call this problem vanishing nodes,\nand we propose the metric vanishing node indicator (VNI) for quantitatively\nmeasuring the degree of vanishing nodes. The VNI can be characterized by the\nnetwork parameters, which is shown analytically to be proportional to the depth\nof the network and inversely proportional to the network width. The theoretical\nresults show that the effective number of nodes vanishes to one when the VNI\nincreases to one (its maximal value), and that vanishing/exploding gradients\nand vanishing nodes are two different challenges that increase the difficulty\nof training deep neural networks. The numerical results from the experiments\nsuggest that the degree of vanishing nodes will become more evident during\nback-propagation training, and that when the VNI is equal to 1, the network\ncannot learn simple tasks (e.g. the XOR problem) even when the gradients are\nneither vanishing nor exploding. We refer to this kind of gradients as the\nwalking dead gradients, which cannot help the network converge when having a\nrelatively large enough scale. Finally, the experiments show that the\nlikelihood of failed training increases as the depth of the network increases.\nThe training will become much more difficult due to the lack of network\nrepresentation capability.\n",
        "published": "2019",
        "authors": [
            "Wen-Yu Chang",
            "Tsung-Nan Lin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.09763v1",
        "title": "Stochastic Feedforward Neural Networks: Universal Approximation",
        "abstract": "  In this chapter we take a look at the universal approximation question for\nstochastic feedforward neural networks. In contrast to deterministic networks,\nwhich represent mappings from a set of inputs to a set of outputs, stochastic\nnetworks represent mappings from a set of inputs to a set of probability\ndistributions over the set of outputs. In particular, even if the sets of\ninputs and outputs are finite, the class of stochastic mappings in question is\nnot finite. Moreover, while for a deterministic function the values of all\noutput variables can be computed independently of each other given the values\nof the inputs, in the stochastic setting the values of the output variables may\nneed to be correlated, which requires that their values are computed jointly. A\nprominent class of stochastic feedforward networks which has played a key role\nin the resurgence of deep learning are deep belief networks. The\nrepresentational power of these networks has been studied mainly in the\ngenerative setting, as models of probability distributions without an input, or\nin the discriminative setting for the special case of deterministic mappings.\nWe study the representational power of deep sigmoid belief networks in terms of\ncompositions of linear transformations of probability distributions, Markov\nkernels, that can be expressed by the layers of the network. We investigate\ndifferent types of shallow and deep architectures, and the minimal number of\nlayers and units per layer that are sufficient and necessary in order for the\nnetwork to be able to approximate any given stochastic mapping from the set of\ninputs to the set of outputs arbitrarily well.\n",
        "published": "2019",
        "authors": [
            "Thomas Merkh",
            "Guido Mont\u00fafar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.10806v1",
        "title": "GenSample: A Genetic Algorithm for Oversampling in Imbalanced Datasets",
        "abstract": "  Imbalanced datasets are ubiquitous. Classification performance on imbalanced\ndatasets is generally poor for the minority class as the classifier cannot\nlearn decision boundaries well. However, in sensitive applications like fraud\ndetection, medical diagnosis, and spam identification, it is extremely\nimportant to classify the minority instances correctly. In this paper, we\npresent a novel technique based on genetic algorithms, GenSample, for\noversampling the minority class in imbalanced datasets. GenSample decides the\nrate of oversampling a minority example by taking into account the difficulty\nin learning that example, along with the performance improvement achieved by\noversampling it. This technique terminates the oversampling process when the\nperformance of the classifier begins to deteriorate. Consequently, it produces\nsynthetic data only as long as a performance boost is obtained. The algorithm\nwas tested on 9 real-world imbalanced datasets of varying sizes and imbalance\nratios. It achieved the highest F-Score on 8 out of 9 datasets, confirming its\nability to better handle imbalanced data compared to other existing\nmethodologies.\n",
        "published": "2019",
        "authors": [
            "Vishwa Karia",
            "Wenhao Zhang",
            "Arash Naeim",
            "Ramin Ramezani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.11858v3",
        "title": "BANANAS: Bayesian Optimization with Neural Architectures for Neural\n  Architecture Search",
        "abstract": "  Over the past half-decade, many methods have been considered for neural\narchitecture search (NAS). Bayesian optimization (BO), which has long had\nsuccess in hyperparameter optimization, has recently emerged as a very\npromising strategy for NAS when it is coupled with a neural predictor. Recent\nwork has proposed different instantiations of this framework, for example,\nusing Bayesian neural networks or graph convolutional networks as the\npredictive model within BO. However, the analyses in these papers often focus\non the full-fledged NAS algorithm, so it is difficult to tell which individual\ncomponents of the framework lead to the best performance.\n  In this work, we give a thorough analysis of the \"BO + neural predictor\"\nframework by identifying five main components: the architecture encoding,\nneural predictor, uncertainty calibration method, acquisition function, and\nacquisition optimization strategy. We test several different methods for each\ncomponent and also develop a novel path-based encoding scheme for neural\narchitectures, which we show theoretically and empirically scales better than\nother encodings. Using all of our analyses, we develop a final algorithm called\nBANANAS, which achieves state-of-the-art performance on NAS search spaces. We\nadhere to the NAS research checklist (Lindauer and Hutter 2019) to facilitate\nbest practices, and our code is available at\nhttps://github.com/naszilla/naszilla.\n",
        "published": "2019",
        "authors": [
            "Colin White",
            "Willie Neiswanger",
            "Yash Savani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.12824v3",
        "title": "Neural Architecture Evolution in Deep Reinforcement Learning for\n  Continuous Control",
        "abstract": "  Current Deep Reinforcement Learning algorithms still heavily rely on\nhandcrafted neural network architectures. We propose a novel approach to\nautomatically find strong topologies for continuous control tasks while only\nadding a minor overhead in terms of interactions in the environment. To achieve\nthis, we combine Neuroevolution techniques with off-policy training and propose\na novel architecture mutation operator. Experiments on five continuous control\nbenchmarks show that the proposed Actor-Critic Neuroevolution algorithm often\noutperforms the strong Actor-Critic baseline and is capable of automatically\nfinding topologies in a sample-efficient manner which would otherwise have to\nbe found by expensive architecture search.\n",
        "published": "2019",
        "authors": [
            "J\u00f6rg K. H. Franke",
            "Gregor K\u00f6hler",
            "Noor Awad",
            "Frank Hutter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.12948v2",
        "title": "GENDIS: GENetic DIscovery of Shapelets",
        "abstract": "  In the time series classification domain, shapelets are small time series\nthat are discriminative for a certain class. It has been shown that classifiers\nare able to achieve state-of-the-art results on a plethora of datasets by\ntaking as input distances from the input time series to different\ndiscriminative shapelets. Additionally, these shapelets can easily be\nvisualized and thus possess an interpretable characteristic, making them very\nappealing in critical domains, such as the health care domain, where\nlongitudinal data is ubiquitous. In this study, a new paradigm for shapelet\ndiscovery is proposed, which is based upon evolutionary computation. The\nadvantages of the proposed approach are that (i) it is gradient-free, which\ncould allow to escape from local optima more easily and to find suited\ncandidates more easily and supports non-differentiable objectives, (ii) no\nbrute-force search is required, which drastically reduces the computational\ncomplexity by several orders of magnitude, (iii) the total amount of shapelets\nand length of each of these shapelets are evolved jointly with the shapelets\nthemselves, alleviating the need to specify this beforehand, (iv) entire sets\nare evaluated at once as opposed to single shapelets, which results in smaller\nfinal sets with less similar shapelets that result in similar predictive\nperformances, and (v) discovered shapelets do not need to be a subsequence of\nthe input time series. We present the results of experiments which validate the\nenumerated advantages.\n",
        "published": "2019",
        "authors": [
            "Gilles Vandewiele",
            "Femke Ongenae",
            "Filip De Turck"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.01395v3",
        "title": "Neural Architecture Generator Optimization",
        "abstract": "  Neural Architecture Search (NAS) was first proposed to achieve\nstate-of-the-art performance through the discovery of new architecture\npatterns, without human intervention. An over-reliance on expert knowledge in\nthe search space design has however led to increased performance (local optima)\nwithout significant architectural breakthroughs, thus preventing truly novel\nsolutions from being reached. In this work we 1) are the first to investigate\ncasting NAS as a problem of finding the optimal network generator and 2) we\npropose a new, hierarchical and graph-based search space capable of\nrepresenting an extremely large variety of network types, yet only requiring\nfew continuous hyper-parameters. This greatly reduces the dimensionality of the\nproblem, enabling the effective use of Bayesian Optimisation as a search\nstrategy. At the same time, we expand the range of valid architectures,\nmotivating a multi-objective learning approach. We demonstrate the\neffectiveness of this strategy on six benchmark datasets and show that our\nsearch space generates extremely lightweight yet highly competitive models.\n",
        "published": "2020",
        "authors": [
            "Binxin Ru",
            "Pedro Esperanca",
            "Fabio Carlucci"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.01899v3",
        "title": "A Generic Graph-based Neural Architecture Encoding Scheme for\n  Predictor-based NAS",
        "abstract": "  This work proposes a novel Graph-based neural ArchiTecture Encoding Scheme,\na.k.a. GATES, to improve the predictor-based neural architecture search.\nSpecifically, different from existing graph-based schemes, GATES models the\noperations as the transformation of the propagating information, which mimics\nthe actual data processing of neural architecture. GATES is a more reasonable\nmodeling of the neural architectures, and can encode architectures from both\nthe \"operation on node\" and \"operation on edge\" cell search spaces\nconsistently. Experimental results on various search spaces confirm GATES's\neffectiveness in improving the performance predictor. Furthermore, equipped\nwith the improved performance predictor, the sample efficiency of the\npredictor-based neural architecture search (NAS) flow is boosted. Codes are\navailable at https://github.com/walkerning/aw_nas.\n",
        "published": "2020",
        "authors": [
            "Xuefei Ning",
            "Yin Zheng",
            "Tianchen Zhao",
            "Yu Wang",
            "Huazhong Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.02094v1",
        "title": "Locality Sensitive Hashing-based Sequence Alignment Using Deep\n  Bidirectional LSTM Models",
        "abstract": "  Bidirectional Long Short-Term Memory (LSTM) is a special kind of Recurrent\nNeural Network (RNN) architecture which is designed to model sequences and\ntheir long-range dependencies more precisely than RNNs. This paper proposes to\nuse deep bidirectional LSTM for sequence modeling as an approach to perform\nlocality-sensitive hashing (LSH)-based sequence alignment. In particular, we\nuse the deep bidirectional LSTM to learn features of LSH. The obtained LSH is\nthen can be utilized to perform sequence alignment. We demonstrate the\nfeasibility of the modeling sequences using the proposed LSTM-based model by\naligning the short read queries over the reference genome. We use the human\nreference genome as our training dataset, in addition to a set of short reads\ngenerated using Illumina sequencing technology. The ultimate goal is to align\nquery sequences into a reference genome. We first decompose the reference\ngenome into multiple sequences. These sequences are then fed into the\nbidirectional LSTM model and then mapped into fixed-length vectors. These\nvectors are what we call the trained LSH, which can then be used for sequence\nalignment. The case study shows that using the introduced LSTM-based model, we\nachieve higher accuracy with the number of epochs.\n",
        "published": "2020",
        "authors": [
            "Neda Tavakoli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.02881v12",
        "title": "Estimate of the Neural Network Dimension using Algebraic Topology and\n  Lie Theory",
        "abstract": "  In this paper we present an approach to determine the smallest possible\nnumber of neurons in a layer of a neural network in such a way that the\ntopology of the input space can be learned sufficiently well. We introduce a\ngeneral procedure based on persistent homology to investigate topological\ninvariants of the manifold on which we suspect the data set. We specify the\nrequired dimensions precisely, assuming that there is a smooth manifold on or\nnear which the data are located. Furthermore, we require that this space is\nconnected and has a commutative group structure in the mathematical sense.\nThese assumptions allow us to derive a decomposition of the underlying space\nwhose topology is well known. We use the representatives of the $k$-dimensional\nhomology groups from the persistence landscape to determine an integer\ndimension for this decomposition. This number is the dimension of the embedding\nthat is capable of capturing the topology of the data manifold. We derive the\ntheory and validate it experimentally on toy data sets.\n",
        "published": "2020",
        "authors": [
            "Luciano Melodia",
            "Richard Lenz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.04030v1",
        "title": "Neural Networks Model for Travel Time Prediction Based on ODTravel Time\n  Matrix",
        "abstract": "  Public transportation system commuters are often interested in getting\naccurate travel time information to plan their daily activities. However, this\ninformation is often difficult to predict accurately due to the irregularities\nof road traffic, caused by factors such as weather conditions, road accidents,\nand traffic jams. In this study, two neural network models namely\nmulti-layer(MLP) perceptron and long short-term model(LSTM) are developed for\npredicting link travel time of a busy route with input generated using\nOrigin-Destination travel time matrix derived from a historical GPS dataset.\nThe experiment result showed that both models can make near-accurate\npredictions however, LSTM is more susceptible to noise as time step increases.\n",
        "published": "2020",
        "authors": [
            "Ayobami E. Adewale",
            "Amnir Hadachi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.04077v1",
        "title": "Continual Learning with Gated Incremental Memories for sequential data\n  processing",
        "abstract": "  The ability to learn in dynamic, nonstationary environments without\nforgetting previous knowledge, also known as Continual Learning (CL), is a key\nenabler for scalable and trustworthy deployments of adaptive solutions. While\nthe importance of continual learning is largely acknowledged in machine vision\nand reinforcement learning problems, this is mostly under-documented for\nsequence processing tasks. This work proposes a Recurrent Neural Network (RNN)\nmodel for CL that is able to deal with concept drift in input distribution\nwithout forgetting previously acquired knowledge. We also implement and test a\npopular CL approach, Elastic Weight Consolidation (EWC), on top of two\ndifferent types of RNNs. Finally, we compare the performances of our enhanced\narchitecture against EWC and RNNs on a set of standard CL benchmarks, adapted\nto the sequential data processing scenario. Results show the superior\nperformance of our architecture and highlight the need for special solutions\ndesigned to address CL in RNNs.\n",
        "published": "2020",
        "authors": [
            "Andrea Cossu",
            "Antonio Carta",
            "Davide Bacciu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.06243v3",
        "title": "Physics-Incorporated Convolutional Recurrent Neural Networks for Source\n  Identification and Forecasting of Dynamical Systems",
        "abstract": "  Spatio-temporal dynamics of physical processes are generally modeled using\npartial differential equations (PDEs). Though the core dynamics follows some\nprinciples of physics, real-world physical processes are often driven by\nunknown external sources. In such cases, developing a purely analytical model\nbecomes very difficult and data-driven modeling can be of assistance. In this\npaper, we present a hybrid framework combining physics-based numerical models\nwith deep learning for source identification and forecasting of spatio-temporal\ndynamical systems with unobservable time-varying external sources. We formulate\nour model PhICNet as a convolutional recurrent neural network (RNN) which is\nend-to-end trainable for spatio-temporal evolution prediction of dynamical\nsystems and learns the source behavior as an internal state of the RNN.\nExperimental results show that the proposed model can forecast the dynamics for\na relatively long time and identify the sources as well.\n",
        "published": "2020",
        "authors": [
            "Priyabrata Saha",
            "Saurabh Dash",
            "Saibal Mukhopadhyay"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.06674v2",
        "title": "Systematically designing better instance counting models on cell images\n  with Neural Arithmetic Logic Units",
        "abstract": "  The big problem for neural network models which are trained to count\ninstances is that whenever test range goes high training range generalization\nerror increases i.e. they are not good generalizers outside training range.\nConsider the case of automating cell counting process where more dense images\nwith higher cell counts are commonly encountered as compared to images used in\ntraining data. By making better predictions for higher ranges of cell count we\nare aiming to create better generalization systems for cell counting. With\narchitecture proposal of neural arithmetic logic units (NALU) for arithmetic\noperations, task of counting has become feasible for higher numeric ranges\nwhich were not included in training data with better accuracy. As a part of our\nstudy we used these units and different other activation functions for learning\ncell counting task with two different architectures namely Fully Convolutional\nRegression Network and U-Net. These numerically biased units are added in the\nform of residual concatenated layers to original architectures and a\ncomparative experimental study is done with these newly proposed changes. This\ncomparative study is described in terms of optimizing regression loss problem\nfrom these models trained with extensive data augmentation techniques. We were\nable to achieve better results in our experiments of cell counting tasks with\nintroduction of these numerically biased units to already existing\narchitectures in the form of residual layer concatenation connections. Our\nresults confirm that above stated numerically biased units does help models to\nlearn numeric quantities for better generalization results.\n",
        "published": "2020",
        "authors": [
            "Ashish Rana",
            "Taranveer Singh",
            "Harpreet Singh",
            "Neeraj Kumar",
            "Prashant Singh Rana"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.07085v2",
        "title": "Neural Status Registers",
        "abstract": "  Standard Neural Networks can learn mathematical operations, but they do not\nextrapolate. Extrapolation means that the model can apply to larger numbers,\nwell beyond those observed during training. Recent architectures tackle\narithmetic operations and can extrapolate; however, the equally important\nproblem of quantitative reasoning remains unaddressed. In this work, we propose\na novel architectural element, the Neural Status Register (NSR), for\nquantitative reasoning over numbers. Our NSR relaxes the discrete bit logic of\nphysical status registers to continuous numbers and allows end-to-end learning\nwith gradient descent. Experiments show that the NSR achieves solutions that\nextrapolate to numbers many orders of magnitude larger than those in the\ntraining set. We successfully train the NSR on number comparisons, piecewise\ndiscontinuous functions, counting in sequences, recurrently finding minimums,\nfinding shortest paths in graphs, and comparing digits in images.\n",
        "published": "2020",
        "authors": [
            "Lukas Faber",
            "Roger Wattenhofer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.08861v1",
        "title": "Role-Wise Data Augmentation for Knowledge Distillation",
        "abstract": "  Knowledge Distillation (KD) is a common method for transferring the\n``knowledge'' learned by one machine learning model (the \\textit{teacher}) into\nanother model (the \\textit{student}), where typically, the teacher has a\ngreater capacity (e.g., more parameters or higher bit-widths). To our\nknowledge, existing methods overlook the fact that although the student absorbs\nextra knowledge from the teacher, both models share the same input data -- and\nthis data is the only medium by which the teacher's knowledge can be\ndemonstrated. Due to the difference in model capacities, the student may not\nbenefit fully from the same data points on which the teacher is trained. On the\nother hand, a human teacher may demonstrate a piece of knowledge with\nindividualized examples adapted to a particular student, for instance, in terms\nof her cultural background and interests. Inspired by this behavior, we design\ndata augmentation agents with distinct roles to facilitate knowledge\ndistillation. Our data augmentation agents generate distinct training data for\nthe teacher and student, respectively. We find empirically that specially\ntailored data points enable the teacher's knowledge to be demonstrated more\neffectively to the student. We compare our approach with existing KD methods on\ntraining popular neural architectures and demonstrate that role-wise data\naugmentation improves the effectiveness of KD over strong prior approaches. The\ncode for reproducing our results can be found at\nhttps://github.com/bigaidream-projects/role-kd\n",
        "published": "2020",
        "authors": [
            "Jie Fu",
            "Xue Geng",
            "Zhijian Duan",
            "Bohan Zhuang",
            "Xingdi Yuan",
            "Adam Trischler",
            "Jie Lin",
            "Chris Pal",
            "Hao Dong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.08925v1",
        "title": "Tree Echo State Autoencoders with Grammars",
        "abstract": "  Tree data occurs in many forms, such as computer programs, chemical\nmolecules, or natural language. Unfortunately, the non-vectorial and discrete\nnature of trees makes it challenging to construct functions with tree-formed\noutput, complicating tasks such as optimization or time series prediction.\nAutoencoders address this challenge by mapping trees to a vectorial latent\nspace, where tasks are easier to solve, and then mapping the solution back to a\ntree structure. However, existing autoencoding approaches for tree data fail to\ntake the specific grammatical structure of tree domains into account and rely\non deep learning, thus requiring large training datasets and long training\ntimes. In this paper, we propose tree echo state autoencoders (TES-AE), which\nare guided by a tree grammar and can be trained within seconds by virtue of\nreservoir computing. In our evaluation on three datasets, we demonstrate that\nour proposed approach is not only much faster than a state-of-the-art deep\nlearning autoencoding approach (D-VAE) but also has less autoencoding error if\nlittle data and time is given.\n",
        "published": "2020",
        "authors": [
            "Benjamin Paassen",
            "Irena Koprinska",
            "Kalina Yacef"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.09043v1",
        "title": "Learning as Reinforcement: Applying Principles of Neuroscience for More\n  General Reinforcement Learning Agents",
        "abstract": "  A significant challenge in developing AI that can generalize well is\ndesigning agents that learn about their world without being told what to learn,\nand apply that learning to challenges with sparse rewards. Moreover, most\ntraditional reinforcement learning approaches explicitly separate learning and\ndecision making in a way that does not correspond to biological learning. We\nimplement an architecture founded in principles of experimental neuroscience,\nby combining computationally efficient abstractions of biological algorithms.\nOur approach is inspired by research on spike-timing dependent plasticity, the\ntransition between short and long term memory, and the role of various\nneurotransmitters in rewarding curiosity. The Neurons-in-a-Box architecture can\nlearn in a wholly generalizable manner, and demonstrates an efficient way to\nbuild and apply representations without explicitly optimizing over a set of\ncriteria or actions. We find it performs well in many environments including\nOpenAI Gym's Mountain Car, which has no reward besides touching a hard-to-reach\nflag on a hill, Inverted Pendulum, where it learns simple strategies to improve\nthe time it holds a pendulum up, a video stream, where it spontaneously learns\nto distinguish an open and closed hand, as well as other environments like\nGoogle Chrome's Dinosaur Game.\n",
        "published": "2020",
        "authors": [
            "Eric Zelikman",
            "William Yin",
            "Kenneth Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.09416v1",
        "title": "VOWEL: A Local Online Learning Rule for Recurrent Networks of\n  Probabilistic Spiking Winner-Take-All Circuits",
        "abstract": "  Networks of spiking neurons and Winner-Take-All spiking circuits (WTA-SNNs)\ncan detect information encoded in spatio-temporal multi-valued events. These\nare described by the timing of events of interest, e.g., clicks, as well as by\ncategorical numerical values assigned to each event, e.g., like or dislike.\nOther use cases include object recognition from data collected by neuromorphic\ncameras, which produce, for each pixel, signed bits at the times of\nsufficiently large brightness variations. Existing schemes for training\nWTA-SNNs are limited to rate-encoding solutions, and are hence able to detect\nonly spatial patterns. Developing more general training algorithms for\narbitrary WTA-SNNs inherits the challenges of training (binary) Spiking Neural\nNetworks (SNNs). These amount, most notably, to the non-differentiability of\nthreshold functions, to the recurrent behavior of spiking neural models, and to\nthe difficulty of implementing backpropagation in neuromorphic hardware. In\nthis paper, we develop a variational online local training rule for WTA-SNNs,\nreferred to as VOWEL, that leverages only local pre- and post-synaptic\ninformation for visible circuits, and an additional common reward signal for\nhidden circuits. The method is based on probabilistic generalized linear neural\nmodels, control variates, and variational regularization. Experimental results\non real-world neuromorphic datasets with multi-valued events demonstrate the\nadvantages of WTA-SNNs over conventional binary SNNs trained with\nstate-of-the-art methods, especially in the presence of limited computing\nresources.\n",
        "published": "2020",
        "authors": [
            "Hyeryung Jang",
            "Nicolas Skatchkovsky",
            "Osvaldo Simeone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.11055v2",
        "title": "On Bayesian Search for the Feasible Space Under Computationally\n  Expensive Constraints",
        "abstract": "  We are often interested in identifying the feasible subset of a decision\nspace under multiple constraints to permit effective design exploration. If\ndetermining feasibility required computationally expensive simulations, the\ncost of exploration would be prohibitive. Bayesian search is data-efficient for\nsuch problems: starting from a small dataset, the central concept is to use\nBayesian models of constraints with an acquisition function to locate promising\nsolutions that may improve predictions of feasibility when the dataset is\naugmented. At the end of this sequential active learning approach with a\nlimited number of expensive evaluations, the models can accurately predict the\nfeasibility of any solution obviating the need for full simulations. In this\npaper, we propose a novel acquisition function that combines the probability\nthat a solution lies at the boundary between feasible and infeasible spaces\n(representing exploitation) and the entropy in predictions (representing\nexploration). Experiments confirmed the efficacy of the proposed function.\n",
        "published": "2020",
        "authors": [
            "Alma Rahat",
            "Michael Wood"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.11685v3",
        "title": "On averaging the best samples in evolutionary computation",
        "abstract": "  Choosing the right selection rate is a long standing issue in evolutionary\ncomputation. In the continuous unconstrained case, we prove mathematically that\na single parent $\\mu=1$ leads to a sub-optimal simple regret in the case of the\nsphere function. We provide a theoretically-based selection rate $\\mu/\\lambda$\nthat leads to better progress rates. With our choice of selection rate, we get\na provable regret of order $O(\\lambda^{-1})$ which has to be compared with\n$O(\\lambda^{-2/d})$ in the case where $\\mu=1$. We complete our study with\nexperiments to confirm our theoretical claims.\n",
        "published": "2020",
        "authors": [
            "Laurent Meunier",
            "Yann Chevaleyre",
            "Jeremy Rapin",
            "Cl\u00e9ment W. Royer",
            "Olivier Teytaud"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.11687v1",
        "title": "Variance Reduction for Better Sampling in Continuous Domains",
        "abstract": "  Design of experiments, random search, initialization of population-based\nmethods, or sampling inside an epoch of an evolutionary algorithm use a sample\ndrawn according to some probability distribution for approximating the location\nof an optimum. Recent papers have shown that the optimal search distribution,\nused for the sampling, might be more peaked around the center of the\ndistribution than the prior distribution modelling our uncertainty about the\nlocation of the optimum. We confirm this statement, provide explicit values for\nthis reshaping of the search distribution depending on the population size\n$\\lambda$ and the dimension $d$, and validate our results experimentally.\n",
        "published": "2020",
        "authors": [
            "Laurent Meunier",
            "Carola Doerr",
            "Jeremy Rapin",
            "Olivier Teytaud"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.11898v1",
        "title": "Adversarial Machine Learning in Network Intrusion Detection Systems",
        "abstract": "  Adversarial examples are inputs to a machine learning system intentionally\ncrafted by an attacker to fool the model into producing an incorrect output.\nThese examples have achieved a great deal of success in several domains such as\nimage recognition, speech recognition and spam detection. In this paper, we\nstudy the nature of the adversarial problem in Network Intrusion Detection\nSystems (NIDS). We focus on the attack perspective, which includes techniques\nto generate adversarial examples capable of evading a variety of machine\nlearning models. More specifically, we explore the use of evolutionary\ncomputation (particle swarm optimization and genetic algorithm) and deep\nlearning (generative adversarial networks) as tools for adversarial example\ngeneration. To assess the performance of these algorithms in evading a NIDS, we\napply them to two publicly available data sets, namely the NSL-KDD and\nUNSW-NB15, and we contrast them to a baseline perturbation method: Monte Carlo\nsimulation. The results show that our adversarial example generation techniques\ncause high misclassification rates in eleven different machine learning models,\nalong with a voting classifier. Our work highlights the vulnerability of\nmachine learning based NIDS in the face of adversarial perturbation.\n",
        "published": "2020",
        "authors": [
            "Elie Alhajjar",
            "Paul Maxwell",
            "Nathaniel D. Bastian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.11946v1",
        "title": "Computation on Sparse Neural Networks: an Inspiration for Future\n  Hardware",
        "abstract": "  Neural network models are widely used in solving many challenging problems,\nsuch as computer vision, personalized recommendation, and natural language\nprocessing. Those models are very computationally intensive and reach the\nhardware limit of the existing server and IoT devices. Thus, finding better\nmodel architectures with much less amount of computation while maximally\npreserving the accuracy is a popular research topic. Among various mechanisms\nthat aim to reduce the computation complexity, identifying the zero values in\nthe model weights and in the activations to avoid computing them is a promising\ndirection.\n  In this paper, we summarize the current status of the research on the\ncomputation of sparse neural networks, from the perspective of the sparse\nalgorithms, the software frameworks, and the hardware accelerations. We observe\nthat the search for the sparse structure can be a general methodology for\nhigh-quality model explorations, in addition to a strategy for high-efficiency\nmodel execution. We discuss the model accuracy influenced by the number of\nweight parameters and the structure of the model. The corresponding models are\ncalled to be located in the weight dominated and structure dominated regions,\nrespectively. We show that for practically complicated problems, it is more\nbeneficial to search large and sparse models in the weight dominated region. In\norder to achieve the goal, new approaches are required to search for proper\nsparse structures, and new sparse training hardware needs to be developed to\nfacilitate fast iterations of sparse models.\n",
        "published": "2020",
        "authors": [
            "Fei Sun",
            "Minghai Qin",
            "Tianyun Zhang",
            "Liu Liu",
            "Yen-Kuang Chen",
            "Yuan Xie"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.12814v2",
        "title": "Why should we add early exits to neural networks?",
        "abstract": "  Deep neural networks are generally designed as a stack of differentiable\nlayers, in which a prediction is obtained only after running the full stack.\nRecently, some contributions have proposed techniques to endow the networks\nwith early exits, allowing to obtain predictions at intermediate points of the\nstack. These multi-output networks have a number of advantages, including: (i)\nsignificant reductions of the inference time, (ii) reduced tendency to\noverfitting and vanishing gradients, and (iii) capability of being distributed\nover multi-tier computation platforms. In addition, they connect to the wider\nthemes of biological plausibility and layered cognitive reasoning. In this\npaper, we provide a comprehensive introduction to this family of neural\nnetworks, by describing in a unified fashion the way these architectures can be\ndesigned, trained, and actually deployed in time-constrained scenarios. We also\ndescribe in-depth their application scenarios in 5G and Fog computing\nenvironments, as long as some of the open research questions connected to them.\n",
        "published": "2020",
        "authors": [
            "Simone Scardapane",
            "Michele Scarpiniti",
            "Enzo Baccarelli",
            "Aurelio Uncini"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.13181v1",
        "title": "EM-GAN: Fast Stress Analysis for Multi-Segment Interconnect Using\n  Generative Adversarial Networks",
        "abstract": "  In this paper, we propose a fast transient hydrostatic stress analysis for\nelectromigration (EM) failure assessment for multi-segment interconnects using\ngenerative adversarial networks (GANs). Our work leverages the image synthesis\nfeature of GAN-based generative deep neural networks. The stress evaluation of\nmulti-segment interconnects, modeled by partial differential equations, can be\nviewed as time-varying 2D-images-to-image problem where the input is the\nmulti-segment interconnects topology with current densities and the output is\nthe EM stress distribution in those wire segments at the given aging time.\nBased on this observation, we train conditional GAN model using the images of\nmany self-generated multi-segment wires and wire current densities and aging\ntime (as conditions) against the COMSOL simulation results. Different\nhyperparameters of GAN were studied and compared. The proposed algorithm,\ncalled {\\it EM-GAN}, can quickly give accurate stress distribution of a general\nmulti-segment wire tree for a given aging time, which is important for\nfull-chip fast EM failure assessment. Our experimental results show that the\nEM-GAN shows 6.6\\% averaged error compared to COMSOL simulation results with\norders of magnitude speedup. It also delivers 8.3X speedup over\nstate-of-the-art analytic based EM analysis solver.\n",
        "published": "2020",
        "authors": [
            "Wentian Jin",
            "Sheriff Sadiqbatcha",
            "Jinwei Zhang",
            "Sheldon X. -D. Tan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.04508v2",
        "title": "Distillation as a Defense to Adversarial Perturbations against Deep\n  Neural Networks",
        "abstract": "  Deep learning algorithms have been shown to perform extremely well on many\nclassical machine learning problems. However, recent studies have shown that\ndeep learning, like other machine learning techniques, is vulnerable to\nadversarial samples: inputs crafted to force a deep neural network (DNN) to\nprovide adversary-selected outputs. Such attacks can seriously undermine the\nsecurity of the system supported by the DNN, sometimes with devastating\nconsequences. For example, autonomous vehicles can be crashed, illicit or\nillegal content can bypass content filters, or biometric authentication systems\ncan be manipulated to allow improper access. In this work, we introduce a\ndefensive mechanism called defensive distillation to reduce the effectiveness\nof adversarial samples on DNNs. We analytically investigate the\ngeneralizability and robustness properties granted by the use of defensive\ndistillation when training DNNs. We also empirically study the effectiveness of\nour defense mechanisms on two DNNs placed in adversarial settings. The study\nshows that defensive distillation can reduce effectiveness of sample creation\nfrom 95% to less than 0.5% on a studied DNN. Such dramatic gains can be\nexplained by the fact that distillation leads gradients used in adversarial\nsample creation to be reduced by a factor of 10^30. We also find that\ndistillation increases the average minimum number of features that need to be\nmodified to create adversarial samples by about 800% on one of the DNNs we\ntested.\n",
        "published": "2015",
        "authors": [
            "Nicolas Papernot",
            "Patrick McDaniel",
            "Xi Wu",
            "Somesh Jha",
            "Ananthram Swami"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.05042v3",
        "title": "An Exploration of Softmax Alternatives Belonging to the Spherical Loss\n  Family",
        "abstract": "  In a multi-class classification problem, it is standard to model the output\nof a neural network as a categorical distribution conditioned on the inputs.\nThe output must therefore be positive and sum to one, which is traditionally\nenforced by a softmax. This probabilistic mapping allows to use the maximum\nlikelihood principle, which leads to the well-known log-softmax loss. However\nthe choice of the softmax function seems somehow arbitrary as there are many\nother possible normalizing functions. It is thus unclear why the log-softmax\nloss would perform better than other loss alternatives. In particular Vincent\net al. (2015) recently introduced a class of loss functions, called the\nspherical family, for which there exists an efficient algorithm to compute the\nupdates of the output weights irrespective of the output size. In this paper,\nwe explore several loss functions from this family as possible alternatives to\nthe traditional log-softmax. In particular, we focus our investigation on\nspherical bounds of the log-softmax loss and on two spherical log-likelihood\nlosses, namely the log-Spherical Softmax suggested by Vincent et al. (2015) and\nthe log-Taylor Softmax that we introduce. Although these alternatives do not\nyield as good results as the log-softmax loss on two language modeling tasks,\nthey surprisingly outperform it in our experiments on MNIST and CIFAR-10,\nsuggesting that they might be relevant in a broad range of applications.\n",
        "published": "2015",
        "authors": [
            "Alexandre de Br\u00e9bisson",
            "Pascal Vincent"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.05432v3",
        "title": "Understanding Adversarial Training: Increasing Local Stability of Neural\n  Nets through Robust Optimization",
        "abstract": "  We propose a general framework for increasing local stability of Artificial\nNeural Nets (ANNs) using Robust Optimization (RO). We achieve this through an\nalternating minimization-maximization procedure, in which the loss of the\nnetwork is minimized over perturbed examples that are generated at each\nparameter update. We show that adversarial training of ANNs is in fact\nrobustification of the network optimization, and that our proposed framework\ngeneralizes previous approaches for increasing local stability of ANNs.\nExperimental results reveal that our approach increases the robustness of the\nnetwork to existing adversarial examples, while making it harder to generate\nnew ones. Furthermore, our algorithm improves the accuracy of the network also\non the original test data.\n",
        "published": "2015",
        "authors": [
            "Uri Shaham",
            "Yutaro Yamada",
            "Sahand Negahban"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.06464v4",
        "title": "Unitary Evolution Recurrent Neural Networks",
        "abstract": "  Recurrent neural networks (RNNs) are notoriously difficult to train. When the\neigenvalues of the hidden to hidden weight matrix deviate from absolute value\n1, optimization becomes difficult due to the well studied issue of vanishing\nand exploding gradients, especially when trying to learn long-term\ndependencies. To circumvent this problem, we propose a new architecture that\nlearns a unitary weight matrix, with eigenvalues of absolute value exactly 1.\nThe challenge we address is that of parametrizing unitary matrices in a way\nthat does not require expensive computations (such as eigendecomposition) after\neach weight update. We construct an expressive unitary weight matrix by\ncomposing several structured matrices that act as building blocks with\nparameters to be learned. Optimization with this parameterization becomes\nfeasible only when considering hidden states in the complex domain. We\ndemonstrate the potential of this architecture by achieving state of the art\nresults in several hard tasks involving very long-term dependencies.\n",
        "published": "2015",
        "authors": [
            "Martin Arjovsky",
            "Amar Shah",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.06499v4",
        "title": "The Variational Gaussian Process",
        "abstract": "  Variational inference is a powerful tool for approximate inference, and it\nhas been recently applied for representation learning with deep generative\nmodels. We develop the variational Gaussian process (VGP), a Bayesian\nnonparametric variational family, which adapts its shape to match complex\nposterior distributions. The VGP generates approximate posterior samples by\ngenerating latent inputs and warping them through random non-linear mappings;\nthe distribution over random mappings is learned during inference, enabling the\ntransformed outputs to adapt to varying complexity. We prove a universal\napproximation theorem for the VGP, demonstrating its representative power for\nlearning any model. For inference we present a variational objective inspired\nby auto-encoders and perform black box inference over a wide class of models.\nThe VGP achieves new state-of-the-art results for unsupervised learning,\ninferring models such as the deep latent Gaussian model and the recently\nproposed DRAW.\n",
        "published": "2015",
        "authors": [
            "Dustin Tran",
            "Rajesh Ranganath",
            "David M. Blei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.07528v1",
        "title": "The Limitations of Deep Learning in Adversarial Settings",
        "abstract": "  Deep learning takes advantage of large datasets and computationally efficient\ntraining algorithms to outperform other approaches at various machine learning\ntasks. However, imperfections in the training phase of deep neural networks\nmake them vulnerable to adversarial samples: inputs crafted by adversaries with\nthe intent of causing deep neural networks to misclassify. In this work, we\nformalize the space of adversaries against deep neural networks (DNNs) and\nintroduce a novel class of algorithms to craft adversarial samples based on a\nprecise understanding of the mapping between inputs and outputs of DNNs. In an\napplication to computer vision, we show that our algorithms can reliably\nproduce samples correctly classified by human subjects but misclassified in\nspecific targets by a DNN with a 97% adversarial success rate while only\nmodifying on average 4.02% of the input features per sample. We then evaluate\nthe vulnerability of different sample classes to adversarial perturbations by\ndefining a hardness measure. Finally, we describe preliminary work outlining\ndefenses against adversarial samples by defining a predictive measure of\ndistance between a benign input and a target classification.\n",
        "published": "2015",
        "authors": [
            "Nicolas Papernot",
            "Patrick McDaniel",
            "Somesh Jha",
            "Matt Fredrikson",
            "Z. Berkay Celik",
            "Ananthram Swami"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1512.03025v1",
        "title": "Partial Reinitialisation for Optimisers",
        "abstract": "  Heuristic optimisers which search for an optimal configuration of variables\nrelative to an objective function often get stuck in local optima where the\nalgorithm is unable to find further improvement. The standard approach to\ncircumvent this problem involves periodically restarting the algorithm from\nrandom initial configurations when no further improvement can be found. We\npropose a method of partial reinitialization, whereby, in an attempt to find a\nbetter solution, only sub-sets of variables are re-initialised rather than the\nwhole configuration. Much of the information gained from previous runs is hence\nretained. This leads to significant improvements in the quality of the solution\nfound in a given time for a variety of optimisation problems in machine\nlearning.\n",
        "published": "2015",
        "authors": [
            "Ilia Zintchenko",
            "Matthew Hastings",
            "Nathan Wiebe",
            "Ethan Brown",
            "Matthias Troyer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1512.03965v4",
        "title": "The Power of Depth for Feedforward Neural Networks",
        "abstract": "  We show that there is a simple (approximately radial) function on $\\reals^d$,\nexpressible by a small 3-layer feedforward neural networks, which cannot be\napproximated by any 2-layer network, to more than a certain constant accuracy,\nunless its width is exponential in the dimension. The result holds for\nvirtually all known activation functions, including rectified linear units,\nsigmoids and thresholds, and formally demonstrates that depth -- even if\nincreased by 1 -- can be exponentially more valuable than width for standard\nfeedforward neural networks. Moreover, compared to related results in the\ncontext of Boolean functions, our result requires fewer assumptions, and the\nproof techniques and construction are very different.\n",
        "published": "2015",
        "authors": [
            "Ronen Eldan",
            "Ohad Shamir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1512.08571v1",
        "title": "Structured Pruning of Deep Convolutional Neural Networks",
        "abstract": "  Real time application of deep learning algorithms is often hindered by high\ncomputational complexity and frequent memory accesses. Network pruning is a\npromising technique to solve this problem. However, pruning usually results in\nirregular network connections that not only demand extra representation efforts\nbut also do not fit well on parallel computation. We introduce structured\nsparsity at various scales for convolutional neural networks, which are channel\nwise, kernel wise and intra kernel strided sparsity. This structured sparsity\nis very advantageous for direct computational resource savings on embedded\ncomputers, parallel computing environments and hardware based systems. To\ndecide the importance of network connections and paths, the proposed method\nuses a particle filtering approach. The importance weight of each particle is\nassigned by computing the misclassification rate with corresponding\nconnectivity pattern. The pruned network is re-trained to compensate for the\nlosses due to pruning. While implementing convolutions as matrix products, we\nparticularly show that intra kernel strided sparsity with a simple constraint\ncan significantly reduce the size of kernel and feature map matrices. The\npruned network is finally fixed point optimized with reduced word length\nprecision. This results in significant reduction in the total storage size\nproviding advantages for on-chip memory based implementations of deep neural\nnetworks.\n",
        "published": "2015",
        "authors": [
            "Sajid Anwar",
            "Kyuyeon Hwang",
            "Wonyong Sung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1512.08806v3",
        "title": "Common Variable Learning and Invariant Representation Learning using\n  Siamese Neural Networks",
        "abstract": "  We consider the statistical problem of learning common source of variability\nin data which are synchronously captured by multiple sensors, and demonstrate\nthat Siamese neural networks can be naturally applied to this problem. This\napproach is useful in particular in exploratory, data-driven applications,\nwhere neither a model nor label information is available. In recent years, many\nresearchers have successfully applied Siamese neural networks to obtain an\nembedding of data which corresponds to a \"semantic similarity\". We present an\ninterpretation of this \"semantic similarity\" as learning of equivalence\nclasses. We discuss properties of the embedding obtained by Siamese networks\nand provide empirical results that demonstrate the ability of Siamese networks\nto learn common variability.\n",
        "published": "2015",
        "authors": [
            "Uri Shaham",
            "Roy Lederman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.01427v1",
        "title": "Stochastic Gradient Descent: Going As Fast As Possible But Not Faster",
        "abstract": "  When applied to training deep neural networks, stochastic gradient descent\n(SGD) often incurs steady progression phases, interrupted by catastrophic\nepisodes in which loss and gradient norm explode. A possible mitigation of such\nevents is to slow down the learning process. This paper presents a novel\napproach to control the SGD learning rate, that uses two statistical tests. The\nfirst one, aimed at fast learning, compares the momentum of the normalized\ngradient vectors to that of random unit vectors and accordingly gracefully\nincreases or decreases the learning rate. The second one is a change point\ndetection test, aimed at the detection of catastrophic learning episodes; upon\nits triggering the learning rate is instantly halved. Both abilities of\nspeeding up and slowing down the learning rate allows the proposed approach,\ncalled SALeRA, to learn as fast as possible but not faster. Experiments on\nstandard benchmarks show that SALeRA performs well in practice, and compares\nfavorably to the state of the art.\n",
        "published": "2017",
        "authors": [
            "Alice Schoenauer-Sebag",
            "Marc Schoenauer",
            "Mich\u00e8le Sebag"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.02797v1",
        "title": "On the exact relationship between the denoising function and the data\n  distribution",
        "abstract": "  We prove an exact relationship between the optimal denoising function and the\ndata distribution in the case of additive Gaussian noise, showing that\ndenoising implicitly models the structure of data allowing it to be exploited\nin the unsupervised learning of representations. This result generalizes a\nknown relationship [2], which is valid only in the limit of small corruption\nnoise.\n",
        "published": "2017",
        "authors": [
            "Heikki Arponen",
            "Matti Herranen",
            "Harri Valpola"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.03082v8",
        "title": "A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and\n  Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data",
        "abstract": "  Gated Recurrent Unit (GRU) is a recently-developed variation of the long\nshort-term memory (LSTM) unit, both of which are types of recurrent neural\nnetwork (RNN). Through empirical evidence, both models have been proven to be\neffective in a wide variety of machine learning tasks such as natural language\nprocessing (Wen et al., 2015), speech recognition (Chorowski et al., 2015), and\ntext classification (Yang et al., 2016). Conventionally, like most neural\nnetworks, both of the aforementioned RNN variants employ the Softmax function\nas its final output layer for its prediction, and the cross-entropy function\nfor computing its loss. In this paper, we present an amendment to this norm by\nintroducing linear support vector machine (SVM) as the replacement for Softmax\nin the final output layer of a GRU model. Furthermore, the cross-entropy\nfunction shall be replaced with a margin-based function. While there have been\nsimilar studies (Alalshekmubarak & Smith, 2013; Tang, 2013), this proposal is\nprimarily intended for binary classification on intrusion detection using the\n2013 network traffic data from the honeypot systems of Kyoto University.\nResults show that the GRU-SVM model performs relatively higher than the\nconventional GRU-Softmax model. The proposed model reached a training accuracy\nof ~81.54% and a testing accuracy of ~84.15%, while the latter was able to\nreach a training accuracy of ~63.07% and a testing accuracy of ~70.75%. In\naddition, the juxtaposition of these two final output layers indicate that the\nSVM would outperform Softmax in prediction time - a theoretical implication\nwhich was supported by the actual training and testing time in the study.\n",
        "published": "2017",
        "authors": [
            "Abien Fred Agarap"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.04054v3",
        "title": "Shifting Mean Activation Towards Zero with Bipolar Activation Functions",
        "abstract": "  We propose a simple extension to the ReLU-family of activation functions that\nallows them to shift the mean activation across a layer towards zero. Combined\nwith proper weight initialization, this alleviates the need for normalization\nlayers. We explore the training of deep vanilla recurrent neural networks\n(RNNs) with up to 144 layers, and show that bipolar activation functions help\nlearning in this setting. On the Penn Treebank and Text8 language modeling\ntasks we obtain competitive results, improving on the best reported results for\nnon-gated networks. In experiments with convolutional neural networks without\nbatch normalization, we find that bipolar activations produce a faster drop in\ntraining error, and results in a lower test error on the CIFAR-10\nclassification task.\n",
        "published": "2017",
        "authors": [
            "Lars Eidnes",
            "Arild N\u00f8kland"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.05804v1",
        "title": "Minimal Effort Back Propagation for Convolutional Neural Networks",
        "abstract": "  As traditional neural network consumes a significant amount of computing\nresources during back propagation, \\citet{Sun2017mePropSB} propose a simple yet\neffective technique to alleviate this problem. In this technique, only a small\nsubset of the full gradients are computed to update the model parameters. In\nthis paper we extend this technique into the Convolutional Neural Network(CNN)\nto reduce calculation in back propagation, and the surprising results verify\nits validity in CNN: only 5\\% of the gradients are passed back but the model\nstill achieves the same effect as the traditional CNN, or even better. We also\nshow that the top-$k$ selection of gradients leads to a sparse calculation in\nback propagation, which may bring significant computational benefits for high\ncomputational complexity of convolution operation in CNN.\n",
        "published": "2017",
        "authors": [
            "Bingzhen Wei",
            "Xu Sun",
            "Xuancheng Ren",
            "Jingjing Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.09161v1",
        "title": "EDEN: Evolutionary Deep Networks for Efficient Machine Learning",
        "abstract": "  Deep neural networks continue to show improved performance with increasing\ndepth, an encouraging trend that implies an explosion in the possible\npermutations of network architectures and hyperparameters for which there is\nlittle intuitive guidance. To address this increasing complexity, we propose\nEvolutionary DEep Networks (EDEN), a computationally efficient\nneuro-evolutionary algorithm which interfaces to any deep neural network\nplatform, such as TensorFlow. We show that EDEN evolves simple yet successful\narchitectures built from embedding, 1D and 2D convolutional, max pooling and\nfully connected layers along with their hyperparameters. Evaluation of EDEN\nacross seven image and sentiment classification datasets shows that it reliably\nfinds good networks -- and in three cases achieves state-of-the-art results --\neven on a single GPU, in just 6-24 hours. Our study provides a first attempt at\napplying neuro-evolution to the creation of 1D convolutional networks for\nsentiment analysis including the optimisation of the embedding layer.\n",
        "published": "2017",
        "authors": [
            "Emmanuel Dufourq",
            "Bruce A. Bassett"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.02649v1",
        "title": "A Unified Framework of Online Learning Algorithms for Training Recurrent\n  Neural Networks",
        "abstract": "  We present a framework for compactly summarizing many recent results in\nefficient and/or biologically plausible online training of recurrent neural\nnetworks (RNN). The framework organizes algorithms according to several\ncriteria: (a) past vs. future facing, (b) tensor structure, (c) stochastic vs.\ndeterministic, and (d) closed form vs. numerical. These axes reveal latent\nconceptual connections among several recent advances in online learning.\nFurthermore, we provide novel mathematical intuitions for their degree of\nsuccess. Testing various algorithms on two synthetic tasks shows that\nperformances cluster according to our criteria. Although a similar clustering\nis also observed for gradient alignment, alignment with exact methods does not\nalone explain ultimate performance, especially for stochastic algorithms. This\nsuggests the need for better comparison metrics.\n",
        "published": "2019",
        "authors": [
            "Owen Marschall",
            "Kyunghyun Cho",
            "Cristina Savin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.03122v1",
        "title": "Takens-inspired neuromorphic processor: a downsizing tool for random\n  recurrent neural networks via feature extraction",
        "abstract": "  We describe a new technique which minimizes the amount of neurons in the\nhidden layer of a random recurrent neural network (rRNN) for time series\nprediction. Merging Takens-based attractor reconstruction methods with machine\nlearning, we identify a mechanism for feature extraction that can be leveraged\nto lower the network size. We obtain criteria specific to the particular\nprediction task and derive the scaling law of the prediction error. The\nconsequences of our theory are demonstrated by designing a Takens-inspired\nhybrid processor, which extends a rRNN with a priori designed delay external\nmemory. Our hybrid architecture is therefore designed including both, real and\nvirtual nodes. Via this symbiosis, we show performance of the hybrid processor\nby stabilizing an arrhythmic neural model. Thanks to our obtained design rules,\nwe can reduce the stabilizing neural network's size by a factor of 15 with\nrespect to a standard system.\n",
        "published": "2019",
        "authors": [
            "Bicky A. Marquez",
            "Jose Suarez-Vargas",
            "Bhavin J. Shastri"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.04840v2",
        "title": "Sparse Networks from Scratch: Faster Training without Losing Performance",
        "abstract": "  We demonstrate the possibility of what we call sparse learning: accelerated\ntraining of deep neural networks that maintain sparse weights throughout\ntraining while achieving dense performance levels. We accomplish this by\ndeveloping sparse momentum, an algorithm which uses exponentially smoothed\ngradients (momentum) to identify layers and weights which reduce the error\nefficiently. Sparse momentum redistributes pruned weights across layers\naccording to the mean momentum magnitude of each layer. Within a layer, sparse\nmomentum grows weights according to the momentum magnitude of zero-valued\nweights. We demonstrate state-of-the-art sparse performance on MNIST, CIFAR-10,\nand ImageNet, decreasing the mean error by a relative 8%, 15%, and 6% compared\nto other sparse algorithms. Furthermore, we show that sparse momentum reliably\nreproduces dense performance levels while providing up to 5.61x faster\ntraining. In our analysis, ablations show that the benefits of momentum\nredistribution and growth increase with the depth and size of the network.\nAdditionally, we find that sparse momentum is insensitive to the choice of its\nhyperparameters suggesting that sparse momentum is robust and easy to use.\n",
        "published": "2019",
        "authors": [
            "Tim Dettmers",
            "Luke Zettlemoyer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.04913v1",
        "title": "Prediction of Compression Index of Fine-Grained Soils Using a Gene\n  Expression Programming Model",
        "abstract": "  In construction projects, estimation of the settlement of fine-grained soils\nis of critical importance, and yet is a challenging task. The coefficient of\nconsolidation for the compression index (Cc) is a key parameter in modeling the\nsettlement of fine-grained soil layers. However, the estimation of this\nparameter is costly, time-consuming, and requires skilled technicians. To\novercome these drawbacks, we aimed to predict Cc through other soil parameters,\ni.e., the liquid limit (LL), plastic limit (PL), and initial void ratio (e0).\nUsing these parameters is more convenient and requires substantially less time\nand cost compared to the conventional tests to estimate Cc. This study presents\na novel prediction model for the Cc of fine-grained soils using gene expression\nprogramming (GEP). A database consisting of 108 different data points was used\nto develop the model. A closed-form equation solution was derived to estimate\nCc based on LL, PL, and e0. The performance of the developed GEP-based model\nwas evaluated through the coefficient of determination (R2), the root mean\nsquared error (RMSE), and the mean average error (MAE). The proposed model\nperformed better in terms of R2, RMSE, and MAE compared to the other models.\n",
        "published": "2019",
        "authors": [
            "Danial Mohammadzadeh",
            "Seyed-Farzan Kazemi",
            "Amir Mosavi",
            "Ehsan Nasseralshariati",
            "Joseph H. M. Tah"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.06341v1",
        "title": "Controlling Model Complexity in Probabilistic Model-Based Dynamic\n  Optimization of Neural Network Structures",
        "abstract": "  A method of simultaneously optimizing both the structure of neural networks\nand the connection weights in a single training loop can reduce the enormous\ncomputational cost of neural architecture search. We focus on the probabilistic\nmodel-based dynamic neural network structure optimization that considers the\nprobability distribution of structure parameters and simultaneously optimizes\nboth the distribution parameters and connection weights based on gradient\nmethods. Since the existing algorithm searches for the structures that only\nminimize the training loss, this method might find overly complicated\nstructures. In this paper, we propose the introduction of a penalty term to\ncontrol the model complexity of obtained structures. We formulate a penalty\nterm using the number of weights or units and derive its analytical natural\ngradient. The proposed method minimizes the objective function injected the\npenalty term based on the stochastic gradient descent. We apply the proposed\nmethod in the unit selection of a fully-connected neural network and the\nconnection selection of a convolutional neural network. The experimental\nresults show that the proposed method can control model complexity while\nmaintaining performance.\n",
        "published": "2019",
        "authors": [
            "Shota Saito",
            "Shinichi Shirakawa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.07066v4",
        "title": "Selection Heuristics on Semantic Genetic Programming for Classification\n  Problems",
        "abstract": "  Individual's semantics have been used for guiding the learning process of\nGenetic Programming solving supervised learning problems. The semantics has\nbeen used to proposed novel genetic operators as well as different ways of\nperforming parent selection. The latter is the focus of this contribution by\nproposing three heuristics for parent selection that replace the fitness\nfunction on the selection mechanism entirely. These heuristics complement\nprevious work by being inspired in the characteristics of the addition, Naive\nBayes, and Nearest Centroid functions and applying them only when the function\nis used to create an offspring. These heuristics use different similarity\nmeasures among the parents to decide which of them is more appropriate given a\nfunction. The similarity functions considered are the cosine similarity,\nPearson's correlation, and agreement. We analyze these heuristics' performance\nagainst random selection, state-of-the-art selection schemes, and 18\nclassifiers, including auto-machine-learning techniques, on 30 classification\nproblems with a variable number of samples, variables, and classes. The result\nindicated that the combination of parent selection based on agreement and\nrandom selection to replace an individual in the population produces\nstatistically better results than the classical selection and state-of-the-art\nschemes, and it is competitive with state-of-the-art classifiers. Finally, the\ncode is released as open-source software.\n",
        "published": "2019",
        "authors": [
            "Claudia N. S\u00e1nchez",
            "Mario Graff"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.07746v1",
        "title": "Deep Invertible Networks for EEG-based brain-signal decoding",
        "abstract": "  In this manuscript, we investigate deep invertible networks for EEG-based\nbrain signal decoding and find them to generate realistic EEG signals as well\nas classify novel signals above chance. Further ideas for their regularization\ntowards better decoding accuracies are discussed.\n",
        "published": "2019",
        "authors": [
            "Robin Tibor Schirrmeister",
            "Tonio Ball"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.08040v1",
        "title": "Convolutional Reservoir Computing for World Models",
        "abstract": "  Recently, reinforcement learning models have achieved great success,\ncompleting complex tasks such as mastering Go and other games with higher\nscores than human players. Many of these models collect considerable data on\nthe tasks and improve accuracy by extracting visual and time-series features\nusing convolutional neural networks (CNNs) and recurrent neural networks,\nrespectively. However, these networks have very high computational costs\nbecause they need to be trained by repeatedly using a large volume of past\nplaying data. In this study, we propose a novel practical approach called\nreinforcement learning with convolutional reservoir computing (RCRC) model. The\nRCRC model has several desirable features: 1. it can extract visual and\ntime-series features very fast because it uses random fixed-weight CNN and the\nreservoir computing model; 2. it does not require the training data to be\nstored because it extracts features without training and decides action with\nevolution strategy. Furthermore, the model achieves state of the art score in\nthe popular reinforcement learning task. Incredibly, we find the random\nweight-fixed simple networks like only one dense layer network can also reach\nhigh score in the RL task.\n",
        "published": "2019",
        "authors": [
            "Hanten Chang",
            "Katsuya Futagami"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.08220v2",
        "title": "Modified swarm-based metaheuristics enhance Gradient Descent\n  initialization performance: Application for EEG spatial filtering",
        "abstract": "  Gradient Descent (GD) approximators often fail in the solution space with\nmultiple scales of convexities, i.e., in subspace learning and neural network\nscenarios. To handle that, one solution is to run GD multiple times from\ndifferent randomized initial states and select the best solution over all\nexperiments. However, this idea is proved impractical in plenty of cases. Even\nSwarm-based optimizers like Particle Swarm Optimization (PSO) or Imperialistic\nCompetitive Algorithm (ICA), as commonly used GD initializers, have failed to\nfind optimal solutions in some applications. In this paper, Swarm-based\noptimizers like ICA and PSO are modified by a new optimization framework to\nimprove GD optimization performance. This improvement is for applications with\nhigh number of convex localities in multiple scales. Performance of the\nproposed method is analyzed in a nonlinear subspace filtering objective\nfunction over EEG data. The proposed metaheuristic outperforms commonly used\nbaseline optimizers as GD initializers in both the EEG classification accuracy\nand EEG loss function fitness. The optimizers have been also compared to each\nother in some of CEC 2014 benchmark functions, where again our method\noutperforms other algorithms.\n",
        "published": "2019",
        "authors": [
            "Mojtaba Moattari",
            "Mohammad Hassan Moradi",
            "Reza Boostani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.08325v1",
        "title": "Scalable Topological Data Analysis and Visualization for Evaluating\n  Data-Driven Models in Scientific Applications",
        "abstract": "  With the rapid adoption of machine learning techniques for large-scale\napplications in science and engineering comes the convergence of two grand\nchallenges in visualization. First, the utilization of black box models (e.g.,\ndeep neural networks) calls for advanced techniques in exploring and\ninterpreting model behaviors. Second, the rapid growth in computing has\nproduced enormous datasets that require techniques that can handle millions or\nmore samples. Although some solutions to these interpretability challenges have\nbeen proposed, they typically do not scale beyond thousands of samples, nor do\nthey provide the high-level intuition scientists are looking for. Here, we\npresent the first scalable solution to explore and analyze high-dimensional\nfunctions often encountered in the scientific data analysis pipeline. By\ncombining a new streaming neighborhood graph construction, the corresponding\ntopology computation, and a novel data aggregation scheme, namely topology\naware datacubes, we enable interactive exploration of both the topological and\nthe geometric aspect of high-dimensional data. Following two use cases from\nhigh-energy-density (HED) physics and computational biology, we demonstrate how\nthese capabilities have led to crucial new insights in both applications.\n",
        "published": "2019",
        "authors": [
            "Shusen Liu",
            "Di Wang",
            "Dan Maljovec",
            "Rushil Anirudh",
            "Jayaraman J. Thiagarajan",
            "Sam Ade Jacobs",
            "Brian C. Van Essen",
            "David Hysom",
            "Jae-Seung Yeom",
            "Jim Gaffney",
            "Luc Peterson",
            "Peter B. Robinson",
            "Harsh Bhatia",
            "Valerio Pascucci",
            "Brian K. Spears",
            "Peer-Timo Bremer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.08544v1",
        "title": "Post-synaptic potential regularization has potential",
        "abstract": "  Improving generalization is one of the main challenges for training deep\nneural networks on classification tasks. In particular, a number of techniques\nhave been proposed, aiming to boost the performance on unseen data: from\nstandard data augmentation techniques to the $\\ell_2$ regularization, dropout,\nbatch normalization, entropy-driven SGD and many more.\\\\ In this work we\npropose an elegant, simple and principled approach: post-synaptic potential\nregularization (PSP). We tested this regularization on a number of different\nstate-of-the-art scenarios. Empirical results show that PSP achieves a\nclassification error comparable to more sophisticated learning strategies in\nthe MNIST scenario, while improves the generalization compared to $\\ell_2$\nregularization in deep architectures trained on CIFAR-10.\n",
        "published": "2019",
        "authors": [
            "Enzo Tartaglione",
            "Daniele Perlo",
            "Marco Grangetto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.08610v2",
        "title": "Lookahead Optimizer: k steps forward, 1 step back",
        "abstract": "  The vast majority of successful deep neural networks are trained using\nvariants of stochastic gradient descent (SGD) algorithms. Recent attempts to\nimprove SGD can be broadly categorized into two approaches: (1) adaptive\nlearning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes,\nsuch as heavy-ball and Nesterov momentum. In this paper, we propose a new\noptimization algorithm, Lookahead, that is orthogonal to these previous\napproaches and iteratively updates two sets of weights. Intuitively, the\nalgorithm chooses a search direction by looking ahead at the sequence of fast\nweights generated by another optimizer. We show that Lookahead improves the\nlearning stability and lowers the variance of its inner optimizer with\nnegligible computation and memory cost. We empirically demonstrate Lookahead\ncan significantly improve the performance of SGD and Adam, even with their\ndefault hyperparameter settings on ImageNet, CIFAR-10/100, neural machine\ntranslation, and Penn Treebank.\n",
        "published": "2019",
        "authors": [
            "Michael R. Zhang",
            "James Lucas",
            "Geoffrey Hinton",
            "Jimmy Ba"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.08651v2",
        "title": "Hyperparameter Optimisation with Early Termination of Poor Performers",
        "abstract": "  It is typical for a machine learning system to have numerous hyperparameters\nthat affect its learning rate and prediction quality. Finding a good\ncombination of the hyperparameters is, however, a challenging job. This is\nmainly because evaluation of each combination is extremely expensive\ncomputationally; indeed, training a machine learning system on real data with\njust a single combination of hyperparameters usually takes hours or even days.\nIn this paper, we address this challenge by trying to predict the performance\nof the machine learning system with a given combination of hyperparameters\nwithout completing the expensive learning process. Instead, we terminate the\ntraining process at an early stage, collect the model performance data and use\nit to predict which of the combinations of hyperparameters is most promising.\nOur preliminary experiments show that such a prediction improves the\nperformance of the commonly used random search approach.\n",
        "published": "2019",
        "authors": [
            "Dobromir Marinov",
            "Daniel Karapetyan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.08931v2",
        "title": "Adaptive Weight Decay for Deep Neural Networks",
        "abstract": "  Regularization in the optimization of deep neural networks is often critical\nto avoid undesirable over-fitting leading to better generalization of model.\nOne of the most popular regularization algorithms is to impose L-2 penalty on\nthe model parameters resulting in the decay of parameters, called weight-decay,\nand the decay rate is generally constant to all the model parameters in the\ncourse of optimization. In contrast to the previous approach based on the\nconstant rate of weight-decay, we propose to consider the residual that\nmeasures dissimilarity between the current state of model and observations in\nthe determination of the weight-decay for each parameter in an adaptive way,\ncalled adaptive weight-decay (AdaDecay) where the gradient norms are normalized\nwithin each layer and the degree of regularization for each parameter is\ndetermined in proportional to the magnitude of its gradient using the sigmoid\nfunction. We empirically demonstrate the effectiveness of AdaDecay in\ncomparison to the state-of-the-art optimization algorithms using popular\nbenchmark datasets: MNIST, Fashion-MNIST, and CIFAR-10 with conventional neural\nnetwork models ranging from shallow to deep. The quantitative evaluation of our\nproposed algorithm indicates that AdaDecay improves generalization leading to\nbetter accuracy across all the datasets and models.\n",
        "published": "2019",
        "authors": [
            "Kensuke Nakamura",
            "Byung-Woo Hong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.09109v1",
        "title": "Efficient Novelty-Driven Neural Architecture Search",
        "abstract": "  One-Shot Neural architecture search (NAS) attracts broad attention recently\ndue to its capacity to reduce the computational hours through weight sharing.\nHowever, extensive experiments on several recent works show that there is no\npositive correlation between the validation accuracy with inherited weights\nfrom the supernet and the test accuracy after re-training for One-Shot NAS.\nDifferent from devising a controller to find the best performing architecture\nwith inherited weights, this paper focuses on how to sample architectures to\ntrain the supernet to make it more predictive. A single-path supernet is\nadopted, where only a small part of weights are optimized in each step, to\nreduce the memory demand greatly. Furthermore, we abandon devising complicated\nreward based architecture sampling controller, and sample architectures to\ntrain supernet based on novelty search. An efficient novelty search method for\nNAS is devised in this paper, and extensive experiments demonstrate the\neffectiveness and efficiency of our novelty search based architecture sampling\nmethod. The best architecture obtained by our algorithm with the same search\nspace achieves the state-of-the-art test error rate of 2.51\\% on CIFAR-10 with\nonly 7.5 hours search time in a single GPU, and a validation perplexity of\n60.02 and a test perplexity of 57.36 on PTB. We also transfer these search cell\nstructures to larger datasets ImageNet and WikiText-2, respectively.\n",
        "published": "2019",
        "authors": [
            "Miao Zhang",
            "Huiqi Li",
            "Shirui Pan",
            "Taoping Liu",
            "Steven Su"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.09720v2",
        "title": "Metalearned Neural Memory",
        "abstract": "  We augment recurrent neural networks with an external memory mechanism that\nbuilds upon recent progress in metalearning. We conceptualize this memory as a\nrapidly adaptable function that we parameterize as a deep neural network.\nReading from the neural memory function amounts to pushing an input (the key\nvector) through the function to produce an output (the value vector). Writing\nto memory means changing the function; specifically, updating the parameters of\nthe neural network to encode desired information. We leverage training and\nalgorithmic techniques from metalearning to update the neural memory function\nin one shot. The proposed memory-augmented model achieves strong performance on\na variety of learning problems, from supervised question answering to\nreinforcement learning.\n",
        "published": "2019",
        "authors": [
            "Tsendsuren Munkhdalai",
            "Alessandro Sordoni",
            "Tong Wang",
            "Adam Trischler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.10599v4",
        "title": "A Fine-Grained Spectral Perspective on Neural Networks",
        "abstract": "  Are neural networks biased toward simple functions? Does depth always help\nlearn more complex features? Is training the last layer of a network as good as\ntraining all layers? How to set the range for learning rate tuning? These\nquestions seem unrelated at face value, but in this work we give all of them a\ncommon treatment from the spectral perspective. We will study the spectra of\nthe *Conjugate Kernel, CK,* (also called the *Neural Network-Gaussian Process\nKernel*), and the *Neural Tangent Kernel, NTK*. Roughly, the CK and the NTK\ntell us respectively \"what a network looks like at initialization\" and \"what a\nnetwork looks like during and after training.\" Their spectra then encode\nvaluable information about the initial distribution and the training and\ngeneralization properties of neural networks. By analyzing the eigenvalues, we\nlend novel insights into the questions put forth at the beginning, and we\nverify these insights by extensive experiments of neural networks. We derive\nfast algorithms for computing the spectra of CK and NTK when the data is\nuniformly distributed over the boolean cube, and show this spectra is the same\nin high dimensions when data is drawn from isotropic Gaussian or uniformly over\nthe sphere. Code replicating our results is available at\ngithub.com/thegregyang/NNspectra.\n",
        "published": "2019",
        "authors": [
            "Greg Yang",
            "Hadi Salman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.11114v2",
        "title": "Graph Neural Lasso for Dynamic Network Regression",
        "abstract": "  The regression of multiple inter-connected sequence data is a problem in\nvarious disciplines. Formally, we name the regression problem of multiple\ninter-connected data entities as the \"dynamic network regression\" in this\npaper. Within the problem of stock forecasting or traffic speed prediction, we\nneed to consider both the trends of the entities and the relationships among\nthe entities. A majority of existing approaches can't capture that information\ntogether. Some of the approaches are proposed to deal with the sequence data,\nlike LSTM. The others use the prior knowledge in a network to get a fixed graph\nstructure and do prediction on some unknown entities, like GCN. To overcome the\nlimitations in those methods, we propose a novel graph neural network, namely\nGraph Neural Lasso (GNL), to deal with the dynamic network problem. GNL extends\nthe GDU (gated diffusive unit) as the base neuron to capture the information\nbehind the sequence. Rather than using a fixed graph structure, GNL can learn\nthe dynamic graph structure automatically. By adding the attention mechanism in\nGNL, we can learn the dynamic relations among entities within each network\nsnapshot. Combining these two parts, GNL is able to model the dynamic network\nproblem well. Experimental results provided on two networked sequence datasets,\ni.e., Nasdaq-100 and METR-LA, show that GNL can address the network regression\nproblem very well and is also very competitive among the existing approaches.\n",
        "published": "2019",
        "authors": [
            "Yixin Chen",
            "Lin Meng",
            "Jiawei Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.11639v1",
        "title": "Training capsules as a routing-weighted product of expert neurons",
        "abstract": "  Capsules are the multidimensional analogue to scalar neurons in neural\nnetworks, and because they are multidimensional, much more complex routing\nschemes can be used to pass information forward through the network than what\ncan be used in traditional neural networks. This work treats capsules as\ncollections of neurons in a fully connected neural network, where sub-networks\nconnecting capsules are weighted according to the routing coefficients\ndetermined by routing by agreement. An energy function is designed to reflect\nthis model, and it follows that capsule networks with dynamic routing can be\nformulated as a product of expert neurons. By alternating between dynamic\nrouting, which acts to both find subnetworks within the overall network as well\nas to mix the model distribution, and updating the parameters by the gradient\nof the contrastive divergence, a bottom-up, unsupervised learning algorithm is\nconstructed for capsule networks with dynamic routing. The model and its\ntraining algorithm are qualitatively tested in the generative sense, and is\nable to produce realistic looking images from standard vision datasets.\n",
        "published": "2019",
        "authors": [
            "Michael Hauser"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.11643v1",
        "title": "Training products of expert capsules with mixing by dynamic routing",
        "abstract": "  This study develops an unsupervised learning algorithm for products of expert\ncapsules with dynamic routing. Analogous to binary-valued neurons in Restricted\nBoltzmann Machines, the magnitude of a squashed capsule firing takes values\nbetween zero and one, representing the probability of the capsule being on.\nThis analogy motivates the design of an energy function for capsule networks.\nIn order to have an efficient sampling procedure where hidden layer nodes are\nnot connected, the energy function is made consistent with dynamic routing in\nthe sense of the probability of a capsule firing, and inference on the capsule\nnetwork is computed with the dynamic routing between capsules procedure. In\norder to optimize the log-likelihood of the visible layer capsules, the\ngradient is found in terms of this energy function. The developed unsupervised\nlearning algorithm is used to train a capsule network on standard vision\ndatasets, and is able to generate realistic looking images from its learned\ndistribution.\n",
        "published": "2019",
        "authors": [
            "Michael Hauser"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.12179v1",
        "title": "A hybrid neural network model based on improved PSO and SA for\n  bankruptcy prediction",
        "abstract": "  Predicting firm's failure is one of the most interesting subjects for\ninvestors and decision makers. In this paper, a bankruptcy prediction model is\nproposed based on Artificial Neural networks (ANN). Taking into consideration\nthat the choice of variables to discriminate between bankrupt and non-bankrupt\nfirms influences significantly the model's accuracy and considering the problem\nof local minima, we propose a hybrid ANN based on variables selection\ntechniques. Moreover, we evolve the convergence of Particle Swarm Optimization\n(PSO) by proposing a training algorithm based on an improved PSO and Simulated\nAnnealing. A comparative performance study is reported, and the proposed hybrid\nmodel shows a high performance and convergence in the context of missing data.\n",
        "published": "2019",
        "authors": [
            "Fatima Zahra Azayite",
            "Said Achchab"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.00187v1",
        "title": "Graph Neural Networks for Small Graph and Giant Network Representation\n  Learning: An Overview",
        "abstract": "  Graph neural networks denote a group of neural network models introduced for\nthe representation learning tasks on graph data specifically. Graph neural\nnetworks have been demonstrated to be effective for capturing network structure\ninformation, and the learned representations can achieve the state-of-the-art\nperformance on node and graph classification tasks. Besides the different\napplication scenarios, the architectures of graph neural network models also\ndepend on the studied graph types a lot. Graph data studied in research can be\ngenerally categorized into two main types, i.e., small graphs vs. giant\nnetworks, which differ from each other a lot in the size, instance number and\nlabel annotation. Several different types of graph neural network models have\nbeen introduced for learning the representations from such different types of\ngraphs already. In this paper, for these two different types of graph data, we\nwill introduce the graph neural networks introduced in recent years. To be more\nspecific, the graph neural networks introduced in this paper include IsoNN,\nSDBN, LF&ER, GCN, GAT, DifNN, GNL, GraphSage and seGEN. Among these graph\nneural network models, IsoNN, SDBN and LF&ER are initially proposed for small\ngraphs and the remaining ones are initially proposed for giant networks\ninstead. The readers are also suggested to refer to these papers for detailed\ninformation when reading this tutorial paper.\n",
        "published": "2019",
        "authors": [
            "Jiawei Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.00637v2",
        "title": "Conditional Finite Mixtures of Poisson Distributions for\n  Context-Dependent Neural Correlations",
        "abstract": "  Parallel recordings of neural spike counts have revealed the existence of\ncontext-dependent noise correlations in neural populations. Theories of\npopulation coding have also shown that such correlations can impact the\ninformation encoded by neural populations about external stimuli. Although\nstudies have shown that these correlations often have a low-dimensional\nstructure, it has proven difficult to capture this structure in a model that is\ncompatible with theories of rate coding in correlated populations. To address\nthis difficulty we develop a novel model based on conditional finite mixtures\nof independent Poisson distributions. The model can be conditioned on context\nvariables (e.g. stimuli or task variables), and the number of mixture\ncomponents in the model can be cross-validated to estimate the dimensionality\nof the target correlations. We derive an expectation-maximization algorithm to\nefficiently fit the model to realistic amounts of data from large neural\npopulations. We then demonstrate that the model successfully captures\nstimulus-dependent correlations in the responses of macaque V1 neurons to\noriented gratings. Our model incorporates arbitrary nonlinear\ncontext-dependence, and can thus be applied to improve predictions of neural\nactivity based on deep neural networks.\n",
        "published": "2019",
        "authors": [
            "Sacha Sokoloski",
            "Ruben Coen-Cagli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.01052v2",
        "title": "Weight Friction: A Simple Method to Overcome Catastrophic Forgetting and\n  Enable Continual Learning",
        "abstract": "  In recent years, deep neural networks have found success in replicating\nhuman-level cognitive skills, yet they suffer from several major obstacles. One\nsignificant limitation is the inability to learn new tasks without forgetting\npreviously learned tasks, a shortcoming known as catastrophic forgetting. In\nthis research, we propose a simple method to overcome catastrophic forgetting\nand enable continual learning in neural networks. We draw inspiration from\nprinciples in neurology and physics to develop the concept of weight friction.\nWeight friction operates by a modification to the update rule in the gradient\ndescent optimization method. It converges at a rate comparable to that of the\nstochastic gradient descent algorithm and can operate over multiple task\ndomains. It performs comparably to current methods while offering improvements\nin computation and memory efficiency.\n",
        "published": "2019",
        "authors": [
            "Gabrielle K. Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.02386v1",
        "title": "Cheetah: Mixed Low-Precision Hardware & Software Co-Design Framework for\n  DNNs on the Edge",
        "abstract": "  Low-precision DNNs have been extensively explored in order to reduce the size\nof DNN models for edge devices. Recently, the posit numerical format has shown\npromise for DNN data representation and compute with ultra-low precision in\n[5..8]-bits. However, previous studies were limited to studying posit for DNN\ninference only. In this paper, we propose the Cheetah framework, which supports\nboth DNN training and inference using posits, as well as other commonly used\nformats. Additionally, the framework is amenable for different quantization\napproaches and supports mixed-precision floating point and fixed-point\nnumerical formats. Cheetah is evaluated on three datasets: MNIST, Fashion\nMNIST, and CIFAR-10. Results indicate that 16-bit posits outperform 16-bit\nfloating point in DNN training. Furthermore, performing inference with\n[5..8]-bit posits improves the trade-off between performance and\nenergy-delay-product over both [5..8]-bit float and fixed-point.\n",
        "published": "2019",
        "authors": [
            "Hamed F. Langroudi",
            "Zachariah Carmichael",
            "David Pastuch",
            "Dhireesha Kudithipudi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.02419v3",
        "title": "Gradient Descent Finds Global Minima for Generalizable Deep Neural\n  Networks of Practical Sizes",
        "abstract": "  In this paper, we theoretically prove that gradient descent can find a global\nminimum of non-convex optimization of all layers for nonlinear deep neural\nnetworks of sizes commonly encountered in practice. The theory developed in\nthis paper only requires the practical degrees of over-parameterization unlike\nprevious theories. Our theory only requires the number of trainable parameters\nto increase linearly as the number of training samples increases. This allows\nthe size of the deep neural networks to be consistent with practice and to be\nseveral orders of magnitude smaller than that required by the previous\ntheories. Moreover, we prove that the linear increase of the size of the\nnetwork is the optimal rate and that it cannot be improved, except by a\nlogarithmic factor. Furthermore, deep neural networks with the trainability\nguarantee are shown to generalize well to unseen test samples with a natural\ndataset but not a random dataset.\n",
        "published": "2019",
        "authors": [
            "Kenji Kawaguchi",
            "Jiaoyang Huang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.02830v1",
        "title": "Self-Organizing Maps with Variable Input Length for Motif Discovery and\n  Word Segmentation",
        "abstract": "  Time Series Motif Discovery (TSMD) is defined as searching for patterns that\nare previously unknown and appear with a given frequency in time series.\nAnother problem strongly related with TSMD is Word Segmentation. This problem\nhas received much attention from the community that studies early language\nacquisition in babies and toddlers. The development of biologically plausible\nmodels for word segmentation could greatly advance this field. Therefore, in\nthis article, we propose the Variable Input Length Map (VILMAP) for Motif\nDiscovery and Word Segmentation. The model is based on the Self-Organizing Maps\nand can identify Motifs with different lengths in time series. In our\nexperiments, we show that VILMAP presents good results in finding Motifs in a\nstandard Motif discovery dataset and can avoid catastrophic forgetting when\ntrained with datasets with increasing values of input size. We also show that\nVILMAP achieves results similar or superior to other methods in the literature\ndeveloped for the task of word segmentation.\n",
        "published": "2019",
        "authors": [
            "Raphael C. Brito",
            "Hansenclever F. Bassani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.02831v1",
        "title": "Visualizing the PHATE of Neural Networks",
        "abstract": "  Understanding why and how certain neural networks outperform others is key to\nguiding future development of network architectures and optimization methods.\nTo this end, we introduce a novel visualization algorithm that reveals the\ninternal geometry of such networks: Multislice PHATE (M-PHATE), the first\nmethod designed explicitly to visualize how a neural network's hidden\nrepresentations of data evolve throughout the course of training. We\ndemonstrate that our visualization provides intuitive, detailed summaries of\nthe learning dynamics beyond simple global measures (i.e., validation loss and\naccuracy), without the need to access validation data. Furthermore, M-PHATE\nbetter captures both the dynamics and community structure of the hidden units\nas compared to visualization based on standard dimensionality reduction methods\n(e.g., ISOMAP, t-SNE). We demonstrate M-PHATE with two vignettes: continual\nlearning and generalization. In the former, the M-PHATE visualizations display\nthe mechanism of \"catastrophic forgetting\" which is a major challenge for\nlearning in task-switching contexts. In the latter, our visualizations reveal\nhow increased heterogeneity among hidden units correlates with improved\ngeneralization performance. An implementation of M-PHATE, along with scripts to\nreproduce the figures in this paper, is available at\nhttps://github.com/scottgigante/M-PHATE.\n",
        "published": "2019",
        "authors": [
            "Scott Gigante",
            "Adam S. Charles",
            "Smita Krishnaswamy",
            "Gal Mishne"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.03891v1",
        "title": "Data-Driven Randomized Learning of Feedforward Neural Networks",
        "abstract": "  Randomized methods of neural network learning suffer from a problem with the\ngeneration of random parameters as they are difficult to set optimally to\nobtain a good projection space. The standard method draws the parameters from a\nfixed interval which is independent of the data scope and activation function\ntype. This does not lead to good results in the approximation of the strongly\nnonlinear functions. In this work, a method which adjusts the random\nparameters, representing the slopes and positions of the sigmoids, to the\ntarget function features is proposed. The method randomly selects the input\nspace regions, places the sigmoids in these regions and then adjusts the\nsigmoid slopes to the local fluctuations of the target function. This brings\nvery good results in the approximation of the complex target functions when\ncompared to the standard fixed interval method and other methods recently\nproposed in the literature.\n",
        "published": "2019",
        "authors": [
            "Grzegorz Dudek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.04909v1",
        "title": "Constrained Multi-Objective Optimization for Automated Machine Learning",
        "abstract": "  Automated machine learning has gained a lot of attention recently. Building\nand selecting the right machine learning models is often a multi-objective\noptimization problem. General purpose machine learning software that\nsimultaneously supports multiple objectives and constraints is scant, though\nthe potential benefits are great. In this work, we present a framework called\nAutotune that effectively handles multiple objectives and constraints that\narise in machine learning problems. Autotune is built on a suite of\nderivative-free optimization methods, and utilizes multi-level parallelism in a\ndistributed computing environment for automatically training, scoring, and\nselecting good models. Incorporation of multiple objectives and constraints in\nthe model exploration and selection process provides the flexibility needed to\nsatisfy trade-offs necessary in practical machine learning applications.\nExperimental results from standard multi-objective optimization benchmark\nproblems show that Autotune is very efficient in capturing Pareto fronts. These\nbenchmark results also show how adding constraints can guide the search to more\npromising regions of the solution space, ultimately producing more desirable\nPareto fronts. Results from two real-world case studies demonstrate the\neffectiveness of the constrained multi-objective optimization capability\noffered by Autotune.\n",
        "published": "2019",
        "authors": [
            "Steven Gardner",
            "Oleg Golovidov",
            "Joshua Griffin",
            "Patrick Koch",
            "Wayne Thompson",
            "Brett Wujek",
            "Yan Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.05164v3",
        "title": "Unconstrained Monotonic Neural Networks",
        "abstract": "  Monotonic neural networks have recently been proposed as a way to define\ninvertible transformations. These transformations can be combined into powerful\nautoregressive flows that have been shown to be universal approximators of\ncontinuous probability distributions. Architectures that ensure monotonicity\ntypically enforce constraints on weights and activation functions, which\nenables invertibility but leads to a cap on the expressiveness of the resulting\ntransformations. In this work, we propose the Unconstrained Monotonic Neural\nNetwork (UMNN) architecture based on the insight that a function is monotonic\nas long as its derivative is strictly positive. In particular, this latter\ncondition can be enforced with a free-form neural network whose only constraint\nis the positiveness of its output. We evaluate our new invertible building\nblock within a new autoregressive flow (UMNN-MAF) and demonstrate its\neffectiveness on density estimation experiments. We also illustrate the ability\nof UMNNs to improve variational inference.\n",
        "published": "2019",
        "authors": [
            "Antoine Wehenkel",
            "Gilles Louppe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.05542v1",
        "title": "Improving Randomized Learning of Feedforward Neural Networks by\n  Appropriate Generation of Random Parameters",
        "abstract": "  In this work, a method of random parameters generation for randomized\nlearning of a single-hidden-layer feedforward neural network is proposed. The\nmethod firstly, randomly selects the slope angles of the hidden neurons\nactivation functions from an interval adjusted to the target function, then\nrandomly rotates the activation functions, and finally distributes them across\nthe input space. For complex target functions the proposed method gives better\nresults than the approach commonly used in practice, where the random\nparameters are selected from the fixed interval. This is because it introduces\nthe steepest fragments of the activation functions into the input hypercube,\navoiding their saturation fragments.\n",
        "published": "2019",
        "authors": [
            "Grzegorz Dudek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.05759v2",
        "title": "Similarity-based Android Malware Detection Using Hamming Distance of\n  Static Binary Features",
        "abstract": "  In this paper, we develop four malware detection methods using Hamming\ndistance to find similarity between samples which are first nearest neighbors\n(FNN), all nearest neighbors (ANN), weighted all nearest neighbors (WANN), and\nk-medoid based nearest neighbors (KMNN). In our proposed methods, we can\ntrigger the alarm if we detect an Android app is malicious. Hence, our\nsolutions help us to avoid the spread of detected malware on a broader scale.\nWe provide a detailed description of the proposed detection methods and related\nalgorithms. We include an extensive analysis to asses the suitability of our\nproposed similarity-based detection methods. In this way, we perform our\nexperiments on three datasets, including benign and malware Android apps like\nDrebin, Contagio, and Genome. Thus, to corroborate the actual effectiveness of\nour classifier, we carry out performance comparisons with some state-of-the-art\nclassification and malware detection algorithms, namely Mixed and Separated\nsolutions, the program dissimilarity measure based on entropy (PDME) and the\nFalDroid algorithms. We test our experiments in a different type of features:\nAPI, intent, and permission features on these three datasets. The results\nconfirm that accuracy rates of proposed algorithms are more than 90% and in\nsome cases (i.e., considering API features) are more than 99%, and are\ncomparable with existing state-of-the-art solutions.\n",
        "published": "2019",
        "authors": [
            "Rahim Taheri",
            "Meysam Ghahramani",
            "Reza Javidan",
            "Mohammad Shojafar",
            "Zahra Pooranian",
            "Mauro Conti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.05864v2",
        "title": "Generating Random Parameters in Feedforward Neural Networks with Random\n  Hidden Nodes: Drawbacks of the Standard Method and How to Improve It",
        "abstract": "  The standard method of generating random weights and biases in feedforward\nneural networks with random hidden nodes, selects them both from the uniform\ndistribution over the same fixed interval. In this work, we show the drawbacks\nof this approach and propose a new method of generating random parameters. This\nmethod ensures the most nonlinear fragments of sigmoids, which are most useful\nin modeling target function nonlinearity, are kept in the input hypercube. In\naddition, we show how to generate activation functions with uniformly\ndistributed slope angles.\n",
        "published": "2019",
        "authors": [
            "Grzegorz Dudek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.05978v3",
        "title": "The Partial Response Network: a neural network nomogram",
        "abstract": "  Among interpretable machine learning methods, the class of Generalised\nAdditive Neural Networks (GANNs) is referred to as Self-Explaining Neural\nNetworks (SENN) because of the linear dependence on explicit functions of the\ninputs. In binary classification this shows the precise weight that each input\ncontributes towards the logit. The nomogram is a graphical representation of\nthese weights. We show that functions of individual and pairs of variables can\nbe derived from a functional Analysis of Variance (ANOVA) representation,\nenabling an efficient feature selection to be carried by application of the\nlogistic Lasso. This process infers the structure of GANNs which otherwise\nneeds to be predefined. As this method is particularly suited for tabular data,\nit starts by fitting a generic flexible model, in this case a Multi-layer\nPerceptron (MLP) to which the ANOVA decomposition is applied. This has the\nfurther advantage that the resulting GANN can be replicated as a SENN, enabling\nfurther refinement of the univariate and bivariate component functions to take\nplace. The component functions are partial responses hence the SENN is a\npartial response network. The Partial Response Network (PRN) is equally as\ntransparent as a traditional logistic regression model, but capable of\nnon-linear classification with comparable or superior performance to the\noriginal MLP. In other words, the PRN is a fully interpretable representation\nof the MLP, at the level of univariate and bivariate effects. The performance\nof the PRN is shown to be competitive for benchmark data, against\nstate-of-the-art machine learning methods including GBM, SVM and Random\nForests. It is also compared with spline-based Sparse Additive Models (SAM)\nshowing that a semi-parametric representation of the GAM as a neural network\ncan be as effective as the SAM though less constrained by the need to set\nspline nodes.\n",
        "published": "2019",
        "authors": [
            "Paulo J. G. Lisboa",
            "Sandra Ortega-Martorell",
            "Sadie Cashman",
            "Ivan Olier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.07063v1",
        "title": "Analysis of Memory Capacity for Deep Echo State Networks",
        "abstract": "  In this paper, the echo state network (ESN) memory capacity, which represents\nthe amount of input data an ESN can store, is analyzed for a new type of deep\nESNs. In particular, two deep ESN architectures are studied. First, a parallel\ndeep ESN is proposed in which multiple reservoirs are connected in parallel\nallowing them to average outputs of multiple ESNs, thus decreasing the\nprediction error. Then, a series architecture ESN is proposed in which ESN\nreservoirs are placed in cascade that the output of each ESN is the input of\nthe next ESN in the series. This series ESN architecture can capture more\nfeatures between the input sequence and the output sequence thus improving the\noverall prediction accuracy. Fundamental analysis shows that the memory\ncapacity of parallel ESNs is equivalent to that of a traditional shallow ESN,\nwhile the memory capacity of series ESNs is smaller than that of a traditional\nshallow ESN.In terms of normalized root mean square error, simulation results\nshow that the parallel deep ESN achieves 38.5% reduction compared to the\ntraditional shallow ESN while the series deep ESN achieves 16.8% reduction.\n",
        "published": "2019",
        "authors": [
            "Xuanlin Liu",
            "Mingzhe Chen",
            "Changchuan Yin",
            "Walid Saad"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.08005v1",
        "title": "Consistent Feature Construction with Constrained Genetic Programming for\n  Experimental Physics",
        "abstract": "  A good feature representation is a determinant factor to achieve high\nperformance for many machine learning algorithms in terms of classification.\nThis is especially true for techniques that do not build complex internal\nrepresentations of data (e.g. decision trees, in contrast to deep neural\nnetworks). To transform the feature space, feature construction techniques\nbuild new high-level features from the original ones. Among these techniques,\nGenetic Programming is a good candidate to provide interpretable features\nrequired for data analysis in high energy physics. Classically, original\nfeatures or higher-level features based on physics first principles are used as\ninputs for training. However, physicists would benefit from an automatic and\ninterpretable feature construction for the classification of particle collision\nevents.\n  Our main contribution consists in combining different aspects of Genetic\nProgramming and applying them to feature construction for experimental physics.\nIn particular, to be applicable to physics, dimensional consistency is enforced\nusing grammars.\n  Results of experiments on three physics datasets show that the constructed\nfeatures can bring a significant gain to the classification accuracy. To the\nbest of our knowledge, it is the first time a method is proposed for\ninterpretable feature construction with units of measurement, and that experts\nin high-energy physics validate the overall approach as well as the\ninterpretability of the built features.\n",
        "published": "2019",
        "authors": [
            "No\u00eblie Cherrier",
            "Jean-Philippe Poli",
            "Maxime Defurne",
            "Franck Sabati\u00e9"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.08006v1",
        "title": "Evolutionary Computation, Optimization and Learning Algorithms for Data\n  Science",
        "abstract": "  A large number of engineering, science and computational problems have yet to\nbe solved in a computationally efficient way. One of the emerging challenges is\nhow evolving technologies grow towards autonomy and intelligent decision\nmaking. This leads to collection of large amounts of data from various sensing\nand measurement technologies, e.g., cameras, smart phones, health sensors,\nsmart electricity meters, and environment sensors. Hence, it is imperative to\ndevelop efficient algorithms for generation, analysis, classification, and\nillustration of data. Meanwhile, data is structured purposefully through\ndifferent representations, such as large-scale networks and graphs. We focus on\ndata science as a crucial area, specifically focusing on a curse of\ndimensionality (CoD) which is due to the large amount of\ngenerated/sensed/collected data. This motivates researchers to think about\noptimization and to apply nature-inspired algorithms, such as evolutionary\nalgorithms (EAs) to solve optimization problems. Although these algorithms look\nun-deterministic, they are robust enough to reach an optimal solution.\nResearchers do not adopt evolutionary algorithms unless they face a problem\nwhich is suffering from placement in local optimal solution, rather than global\noptimal solution. In this chapter, we first develop a clear and formal\ndefinition of the CoD problem, next we focus on feature extraction techniques\nand categories, then we provide a general overview of meta-heuristic\nalgorithms, its terminology, and desirable properties of evolutionary\nalgorithms.\n",
        "published": "2019",
        "authors": [
            "Farid Ghareh Mohammadi",
            "M. Hadi Amini",
            "Hamid R. Arabnia"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.08021v1",
        "title": "Reservoir-size dependent learning in analogue neural networks",
        "abstract": "  The implementation of artificial neural networks in hardware substrates is a\nmajor interdisciplinary enterprise. Well suited candidates for physical\nimplementations must combine nonlinear neurons with dedicated and efficient\nhardware solutions for both connectivity and training. Reservoir computing\naddresses the problems related with the network connectivity and training in an\nelegant and efficient way. However, important questions regarding impact of\nreservoir size and learning routines on the convergence-speed during learning\nremain unaddressed. Here, we study in detail the learning process of a recently\ndemonstrated photonic neural network based on a reservoir. We use a greedy\nalgorithm to train our neural network for the task of chaotic signals\nprediction and analyze the learning-error landscape. Our results unveil\nfundamental properties of the system's optimization hyperspace. Particularly,\nwe determine the convergence speed of learning as a function of reservoir size\nand find exceptional, close to linear scaling. This linear dependence, together\nwith our parallel diffractive coupling, represent optimal scaling conditions\nfor our photonic neural network scheme.\n",
        "published": "2019",
        "authors": [
            "Xavier Porte",
            "Louis Andreoli",
            "Maxime Jacquot",
            "Laurent Larger",
            "Daniel Brunner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.08118v3",
        "title": "Neural Plasticity Networks",
        "abstract": "  Neural plasticity is an important functionality of human brain, in which\nnumber of neurons and synapses can shrink or expand in response to stimuli\nthroughout the span of life. We model this dynamic learning process as an\n$L_0$-norm regularized binary optimization problem, in which each unit of a\nneural network (e.g., weight, neuron or channel, etc.) is attached with a\nstochastic binary gate, whose parameters determine the level of activity of a\nunit in the network. At the beginning, only a small portion of binary gates\n(therefore the corresponding neurons) are activated, while the remaining\nneurons are in a hibernation mode. As the learning proceeds, some neurons might\nbe activated or deactivated if doing so can be justified by the cost-benefit\ntradeoff measured by the $L_0$-norm regularized objective. As the training gets\nmature, the probability of transition between activation and deactivation will\ndiminish until a final hardening stage. We demonstrate that all of these\nlearning dynamics can be modulated by a single parameter $k$ seamlessly. Our\nneural plasticity network (NPN) can prune or expand a network depending on the\ninitial capacity of network provided by the user; it also unifies dropout (when\n$k=0$), traditional training of DNNs (when $k=\\infty$) and interpolates between\nthese two. To the best of our knowledge, this is the first learning framework\nthat unifies network sparsification and network expansion in an end-to-end\ntraining pipeline. Extensive experiments on synthetic dataset and multiple\nimage classification benchmarks demonstrate the superior performance of NPN. We\nshow that both network sparsification and network expansion can yield compact\nmodels of similar architectures, while retaining competitive accuracies of the\noriginal networks.\n",
        "published": "2019",
        "authors": [
            "Yang Li",
            "Shihao Ji"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.08380v1",
        "title": "Analysis of Wide and Deep Echo State Networks for Multiscale\n  Spatiotemporal Time Series Forecasting",
        "abstract": "  Echo state networks are computationally lightweight reservoir models inspired\nby the random projections observed in cortical circuitry. As interest in\nreservoir computing has grown, networks have become deeper and more intricate.\nWhile these networks are increasingly applied to nontrivial forecasting tasks,\nthere is a need for comprehensive performance analysis of deep reservoirs. In\nthis work, we study the influence of partitioning neurons given a budget and\nthe effect of parallel reservoir pathways across different datasets exhibiting\nmulti-scale and nonlinear dynamics.\n",
        "published": "2019",
        "authors": [
            "Zachariah Carmichael",
            "Humza Syed",
            "Dhireesha Kudithipudi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.08563v1",
        "title": "Applications of Nature-Inspired Algorithms for Dimension Reduction:\n  Enabling Efficient Data Analytics",
        "abstract": "  In [1], we have explored the theoretical aspects of feature selection and\nevolutionary algorithms. In this chapter, we focus on optimization algorithms\nfor enhancing data analytic process, i.e., we propose to explore applications\nof nature-inspired algorithms in data science. Feature selection optimization\nis a hybrid approach leveraging feature selection techniques and evolutionary\nalgorithms process to optimize the selected features. Prior works solve this\nproblem iteratively to converge to an optimal feature subset. Feature selection\noptimization is a non-specific domain approach. Data scientists mainly attempt\nto find an advanced way to analyze data n with high computational efficiency\nand low time complexity, leading to efficient data analytics. Thus, by\nincreasing generated/measured/sensed data from various sources, analysis,\nmanipulation and illustration of data grow exponentially. Due to the large\nscale data sets, Curse of dimensionality (CoD) is one of the NP-hard problems\nin data science. Hence, several efforts have been focused on leveraging\nevolutionary algorithms (EAs) to address the complex issues in large scale data\nanalytics problems. Dimension reduction, together with EAs, lends itself to\nsolve CoD and solve complex problems, in terms of time complexity, efficiently.\nIn this chapter, we first provide a brief overview of previous studies that\nfocused on solving CoD using feature extraction optimization process. We then\ndiscuss practical examples of research studies are successfully tackled some\napplication domains, such as image processing, sentiment analysis, network\ntraffics / anomalies analysis, credit score analysis and other benchmark\nfunctions/data sets analysis.\n",
        "published": "2019",
        "authors": [
            "Farid Ghareh Mohammadi",
            "M. Hadi Amini",
            "Hamid R. Arabnia"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.08783v5",
        "title": "Learning Fitness Functions for Machine Programming",
        "abstract": "  The problem of automatic software generation is known as Machine Programming.\nIn this work, we propose a framework based on genetic algorithms to solve this\nproblem. Although genetic algorithms have been used successfully for many\nproblems, one criticism is that hand-crafting its fitness function, the test\nthat aims to effectively guide its evolution, can be notably challenging. Our\nframework presents a novel approach to learn the fitness function using neural\nnetworks to predict values of ideal fitness functions. We also augment the\nevolutionary process with a minimally intrusive search heuristic. This\nheuristic improves the framework's ability to discover correct programs from\nones that are approximately correct and does so with negligible computational\noverhead. We compare our approach with several state-of-the-art program\nsynthesis methods and demonstrate that it finds more correct programs with\nfewer candidate program generations.\n",
        "published": "2019",
        "authors": [
            "Shantanu Mandal",
            "Todd A. Anderson",
            "Javier S. Turek",
            "Justin Gottschlich",
            "Shengtian Zhou",
            "Abdullah Muzahid"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.09942v1",
        "title": "On the Bounds of Function Approximations",
        "abstract": "  Within machine learning, the subfield of Neural Architecture Search (NAS) has\nrecently garnered research attention due to its ability to improve upon\nhuman-designed models. However, the computational requirements for finding an\nexact solution to this problem are often intractable, and the design of the\nsearch space still requires manual intervention. In this paper we attempt to\nestablish a formalized framework from which we can better understand the\ncomputational bounds of NAS in relation to its search space. For this, we first\nreformulate the function approximation problem in terms of sequences of\nfunctions, and we call it the Function Approximation (FA) problem; then we show\nthat it is computationally infeasible to devise a procedure that solves FA for\nall functions to zero error, regardless of the search space. We show also that\nsuch error will be minimal if a specific class of functions is present in the\nsearch space. Subsequently, we show that machine learning as a mathematical\nproblem is a solution strategy for FA, albeit not an effective one, and further\ndescribe a stronger version of this approach: the Approximate Architectural\nSearch Problem (a-ASP), which is the mathematical equivalent of NAS. We\nleverage the framework from this paper and results from the literature to\ndescribe the conditions under which a-ASP can potentially solve FA as well as\nan exhaustive search, but in polynomial time.\n",
        "published": "2019",
        "authors": [
            "Adrian de Wynter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.10714v1",
        "title": "Automated Architecture Design for Deep Neural Networks",
        "abstract": "  Machine learning has made tremendous progress in recent years and received\nlarge amounts of public attention. Though we are still far from designing a\nfull artificially intelligent agent, machine learning has brought us many\napplications in which computers solve human learning tasks remarkably well.\nMuch of this progress comes from a recent trend within machine learning, called\ndeep learning. Deep learning models are responsible for many state-of-the-art\napplications of machine learning. Despite their success, deep learning models\nare hard to train, very difficult to understand, and often times so complex\nthat training is only possible on very large GPU clusters. Lots of work has\nbeen done on enabling neural networks to learn efficiently. However, the design\nand architecture of such neural networks is often done manually through trial\nand error and expert knowledge. This thesis inspects different approaches,\nexisting and novel, to automate the design of deep feedforward neural networks\nin an attempt to create less complex models with good performance that take\naway the burden of deciding on an architecture and make it more efficient to\ndesign and train such deep networks.\n",
        "published": "2019",
        "authors": [
            "Steven Abreu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1301.3533v2",
        "title": "Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint",
        "abstract": "  Deep Belief Networks (DBN) have been successfully applied on popular machine\nlearning tasks. Specifically, when applied on hand-written digit recognition,\nDBNs have achieved approximate accuracy rates of 98.8%. In an effort to\noptimize the data representation achieved by the DBN and maximize their\ndescriptive power, recent advances have focused on inducing sparse constraints\nat each layer of the DBN. In this paper we present a theoretical approach for\nsparse constraints in the DBN using the mixed norm for both non-overlapping and\noverlapping groups. We explore how these constraints affect the classification\naccuracy for digit recognition in three different datasets (MNIST, USPS, RIMES)\nand provide initial estimations of their usefulness by altering different\nparameters such as the group size and overlap percentage.\n",
        "published": "2013",
        "authors": [
            "Xanadu Halkias",
            "Sebastien Paris",
            "Herve Glotin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1301.3545v2",
        "title": "Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines",
        "abstract": "  This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for\ntraining Boltzmann Machines. Similar in spirit to the Hessian-Free method of\nMartens [8], our algorithm belongs to the family of truncated Newton methods\nand exploits an efficient matrix-vector product to avoid explicitely storing\nthe natural gradient metric $L$. This metric is shown to be the expected second\nderivative of the log-partition function (under the model distribution), or\nequivalently, the variance of the vector of partial derivatives of the energy\nfunction. We evaluate our method on the task of joint-training a 3-layer Deep\nBoltzmann Machine and show that MFNG does indeed have faster per-epoch\nconvergence compared to Stochastic Maximum Likelihood with centering, though\nwall-clock performance is currently not competitive.\n",
        "published": "2013",
        "authors": [
            "Guillaume Desjardins",
            "Razvan Pascanu",
            "Aaron Courville",
            "Yoshua Bengio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1301.3557v1",
        "title": "Stochastic Pooling for Regularization of Deep Convolutional Neural\n  Networks",
        "abstract": "  We introduce a simple and effective method for regularizing large\nconvolutional neural networks. We replace the conventional deterministic\npooling operations with a stochastic procedure, randomly picking the activation\nwithin each pooling region according to a multinomial distribution, given by\nthe activities within the pooling region. The approach is hyper-parameter free\nand can be combined with other regularization approaches, such as dropout and\ndata augmentation. We achieve state-of-the-art performance on four image\ndatasets, relative to other approaches that do not utilize data augmentation.\n",
        "published": "2013",
        "authors": [
            "Matthew D. Zeiler",
            "Rob Fergus"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1301.3641v3",
        "title": "Training Neural Networks with Stochastic Hessian-Free Optimization",
        "abstract": "  Hessian-free (HF) optimization has been successfully used for training deep\nautoencoders and recurrent networks. HF uses the conjugate gradient algorithm\nto construct update directions through curvature-vector products that can be\ncomputed on the same order of time as gradients. In this paper we exploit this\nproperty and study stochastic HF with gradient and curvature mini-batches\nindependent of the dataset size. We modify Martens' HF for these settings and\nintegrate dropout, a method for preventing co-adaptation of feature detectors,\nto guard against overfitting. Stochastic Hessian-free optimization gives an\nintermediary between SGD and HF that achieves competitive performance on both\nclassification and deep autoencoder experiments.\n",
        "published": "2013",
        "authors": [
            "Ryan Kiros"
        ]
    }
]