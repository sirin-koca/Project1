[
    {
        "id": "http://arxiv.org/abs/1710.09574v1",
        "title": "Biologically Inspired Feedforward Supervised Learning for Deep\n  Self-Organizing Map Networks",
        "abstract": "  In this study, we propose a novel deep neural network and its supervised\nlearning method that uses a feedforward supervisory signal. The method is\ninspired by the human visual system and performs human-like association-based\nlearning without any backward error propagation. The feedforward supervisory\nsignal that produces the correct result is preceded by the target signal and\nassociates its confirmed label with the classification result of the target\nsignal. It effectively uses a large amount of information from the feedforward\nsignal, and forms a continuous and rich learning representation. The method is\nvalidated using visual recognition tasks on the MNIST handwritten dataset.\n",
        "published": "2017",
        "authors": [
            "Takashi Shinozaki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.09668v2",
        "title": "PDE-Net: Learning PDEs from Data",
        "abstract": "  In this paper, we present an initial attempt to learn evolution PDEs from\ndata. Inspired by the latest development of neural network designs in deep\nlearning, we propose a new feed-forward deep network, called PDE-Net, to\nfulfill two objectives at the same time: to accurately predict dynamics of\ncomplex systems and to uncover the underlying hidden PDE models. The basic idea\nof the proposed PDE-Net is to learn differential operators by learning\nconvolution kernels (filters), and apply neural networks or other machine\nlearning methods to approximate the unknown nonlinear responses. Comparing with\nexisting approaches, which either assume the form of the nonlinear response is\nknown or fix certain finite difference approximations of differential\noperators, our approach has the most flexibility by learning both differential\noperators and the nonlinear responses. A special feature of the proposed\nPDE-Net is that all filters are properly constrained, which enables us to\neasily identify the governing PDE models while still maintaining the expressive\nand predictive power of the network. These constrains are carefully designed by\nfully exploiting the relation between the orders of differential operators and\nthe orders of sum rules of filters (an important concept originated from\nwavelet theory). We also discuss relations of the PDE-Net with some existing\nnetworks in computer vision such as Network-In-Network (NIN) and Residual\nNeural Network (ResNet). Numerical experiments show that the PDE-Net has the\npotential to uncover the hidden PDE of the observed dynamics, and predict the\ndynamical behavior for a relatively long time, even in a noisy environment.\n",
        "published": "2017",
        "authors": [
            "Zichao Long",
            "Yiping Lu",
            "Xianzhong Ma",
            "Bin Dong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.09825v2",
        "title": "On the role of synaptic stochasticity in training low-precision neural\n  networks",
        "abstract": "  Stochasticity and limited precision of synaptic weights in neural network\nmodels are key aspects of both biological and hardware modeling of learning\nprocesses. Here we show that a neural network model with stochastic binary\nweights naturally gives prominence to exponentially rare dense regions of\nsolutions with a number of desirable properties such as robustness and good\ngeneralization performance, while typical solutions are isolated and hard to\nfind. Binary solutions of the standard perceptron problem are obtained from a\nsimple gradient descent procedure on a set of real values parametrizing a\nprobability distribution over the binary synapses. Both analytical and\nnumerical results are presented. An algorithmic extension aimed at training\ndiscrete deep neural networks is also investigated.\n",
        "published": "2017",
        "authors": [
            "Carlo Baldassi",
            "Federica Gerace",
            "Hilbert J. Kappen",
            "Carlo Lucibello",
            "Luca Saglietti",
            "Enzo Tartaglione",
            "Riccardo Zecchina"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.10196v3",
        "title": "Progressive Growing of GANs for Improved Quality, Stability, and\n  Variation",
        "abstract": "  We describe a new training methodology for generative adversarial networks.\nThe key idea is to grow both the generator and discriminator progressively:\nstarting from a low resolution, we add new layers that model increasingly fine\ndetails as training progresses. This both speeds the training up and greatly\nstabilizes it, allowing us to produce images of unprecedented quality, e.g.,\nCelebA images at 1024^2. We also propose a simple way to increase the variation\nin generated images, and achieve a record inception score of 8.80 in\nunsupervised CIFAR10. Additionally, we describe several implementation details\nthat are important for discouraging unhealthy competition between the generator\nand discriminator. Finally, we suggest a new metric for evaluating GAN results,\nboth in terms of image quality and variation. As an additional contribution, we\nconstruct a higher-quality version of the CelebA dataset.\n",
        "published": "2017",
        "authors": [
            "Tero Karras",
            "Timo Aila",
            "Samuli Laine",
            "Jaakko Lehtinen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.10779v1",
        "title": "Generative Adversarial Source Separation",
        "abstract": "  Generative source separation methods such as non-negative matrix\nfactorization (NMF) or auto-encoders, rely on the assumption of an output\nprobability density. Generative Adversarial Networks (GANs) can learn data\ndistributions without needing a parametric assumption on the output density. We\nshow on a speech source separation experiment that, a multi-layer perceptron\ntrained with a Wasserstein-GAN formulation outperforms NMF, auto-encoders\ntrained with maximum likelihood, and variational auto-encoders in terms of\nsource to distortion ratio.\n",
        "published": "2017",
        "authors": [
            "Cem Subakan",
            "Paris Smaragdis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.10944v3",
        "title": "A Supervised STDP-based Training Algorithm for Living Neural Networks",
        "abstract": "  Neural networks have shown great potential in many applications like speech\nrecognition, drug discovery, image classification, and object detection. Neural\nnetwork models are inspired by biological neural networks, but they are\noptimized to perform machine learning tasks on digital computers. The proposed\nwork explores the possibilities of using living neural networks in vitro as\nbasic computational elements for machine learning applications. A new\nsupervised STDP-based learning algorithm is proposed in this work, which\nconsiders neuron engineering constrains. A 74.7% accuracy is achieved on the\nMNIST benchmark for handwritten digit recognition.\n",
        "published": "2017",
        "authors": [
            "Yuan Zeng",
            "Kevin Devincentis",
            "Yao Xiao",
            "Zubayer Ibne Ferdous",
            "Xiaochen Guo",
            "Zhiyuan Yan",
            "Yevgeny Berdichevsky"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.00222v3",
        "title": "Meta-Learning Update Rules for Unsupervised Representation Learning",
        "abstract": "  A major goal of unsupervised learning is to discover data representations\nthat are useful for subsequent tasks, without access to supervised labels\nduring training. Typically, this involves minimizing a surrogate objective,\nsuch as the negative log likelihood of a generative model, with the hope that\nrepresentations useful for subsequent tasks will arise as a side effect. In\nthis work, we propose instead to directly target later desired tasks by\nmeta-learning an unsupervised learning rule which leads to representations\nuseful for those tasks. Specifically, we target semi-supervised classification\nperformance, and we meta-learn an algorithm -- an unsupervised weight update\nrule -- that produces representations useful for this task. Additionally, we\nconstrain our unsupervised update rule to a be a biologically-motivated,\nneuron-local function, which enables it to generalize to different neural\nnetwork architectures, datasets, and data modalities. We show that the\nmeta-learned update rule produces useful features and sometimes outperforms\nexisting unsupervised learning techniques. We further show that the\nmeta-learned unsupervised update rule generalizes to train networks with\ndifferent widths, depths, and nonlinearities. It also generalizes to train on\ndata with randomly permuted input dimensions and even generalizes from image\ndatasets to a text task.\n",
        "published": "2018",
        "authors": [
            "Luke Metz",
            "Niru Maheswaranathan",
            "Brian Cheung",
            "Jascha Sohl-Dickstein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.00815v1",
        "title": "Convolutional Neural Networks Regularized by Correlated Noise",
        "abstract": "  Neurons in the visual cortex are correlated in their variability. The\npresence of correlation impacts cortical processing because noise cannot be\naveraged out over many neurons. In an effort to understand the functional\npurpose of correlated variability, we implement and evaluate correlated noise\nmodels in deep convolutional neural networks. Inspired by the cortex,\ncorrelation is defined as a function of the distance between neurons and their\nselectivity. We show how to sample from high-dimensional correlated\ndistributions while keeping the procedure differentiable, so that\nback-propagation can proceed as usual. The impact of correlated variability is\nevaluated on the classification of occluded and non-occluded images with and\nwithout the presence of other regularization techniques, such as dropout. More\nwork is needed to understand the effects of correlations in various conditions,\nhowever in 10/12 of the cases we studied, the best performance on occluded\nimages was obtained from a model with correlated noise.\n",
        "published": "2018",
        "authors": [
            "Shamak Dutta",
            "Bryan Tripp",
            "Graham Taylor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.02464v3",
        "title": "Differentiable plasticity: training plastic neural networks with\n  backpropagation",
        "abstract": "  How can we build agents that keep learning from experience, quickly and\nefficiently, after their initial training? Here we take inspiration from the\nmain mechanism of learning in biological brains: synaptic plasticity, carefully\ntuned by evolution to produce efficient lifelong learning. We show that\nplasticity, just like connection weights, can be optimized by gradient descent\nin large (millions of parameters) recurrent networks with Hebbian plastic\nconnections. First, recurrent plastic networks with more than two million\nparameters can be trained to memorize and reconstruct sets of novel,\nhigh-dimensional 1000+ pixels natural images not seen during training.\nCrucially, traditional non-plastic recurrent networks fail to solve this task.\nFurthermore, trained plastic networks can also solve generic meta-learning\ntasks such as the Omniglot task, with competitive results and little parameter\noverhead. Finally, in reinforcement learning settings, plastic networks\noutperform a non-plastic equivalent in a maze exploration task. We conclude\nthat differentiable plasticity may provide a powerful novel approach to the\nlearning-to-learn problem.\n",
        "published": "2018",
        "authors": [
            "Thomas Miconi",
            "Jeff Clune",
            "Kenneth O. Stanley"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.02476v2",
        "title": "Associative Compression Networks for Representation Learning",
        "abstract": "  This paper introduces Associative Compression Networks (ACNs), a new\nframework for variational autoencoding with neural networks. The system differs\nfrom existing variational autoencoders (VAEs) in that the prior distribution\nused to model each code is conditioned on a similar code from the dataset. In\ncompression terms this equates to sequentially transmitting the dataset using\nan ordering determined by proximity in latent space. Since the prior need only\naccount for local, rather than global variations in the latent space, the\ncoding cost is greatly reduced, leading to rich, informative codes. Crucially,\nthe codes remain informative when powerful, autoregressive decoders are used,\nwhich we argue is fundamentally difficult with normal VAEs. Experimental\nresults on MNIST, CIFAR-10, ImageNet and CelebA show that ACNs discover\nhigh-level latent features such as object class, writing style, pose and facial\nexpression, which can be used to cluster and classify the data, as well as to\ngenerate diverse and convincing samples. We conclude that ACNs are a promising\nnew direction for representation learning: one that steps away from IID\nmodelling, and towards learning a structured description of the dataset as a\nwhole.\n",
        "published": "2018",
        "authors": [
            "Alex Graves",
            "Jacob Menick",
            "Aaron van den Oord"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.02491v1",
        "title": "Continuously Constructive Deep Neural Networks",
        "abstract": "  Traditionally, deep learning algorithms update the network weights whereas\nthe network architecture is chosen manually, using a process of trial and\nerror. In this work, we propose two novel approaches that automatically update\nthe network structure while also learning its weights. The novelty of our\napproach lies in our parameterization where the depth, or additional\ncomplexity, is encapsulated continuously in the parameter space through control\nparameters that add additional complexity. We propose two methods: In tunnel\nnetworks, this selection is done at the level of a hidden unit, and in budding\nperceptrons, this is done at the level of a network layer; updating this\ncontrol parameter introduces either another hidden unit or another hidden\nlayer. We show the effectiveness of our methods on the synthetic two-spirals\ndata and on two real data sets of MNIST and MIRFLICKR, where we see that our\nproposed methods, with the same set of hyperparameters, can correctly adjust\nthe network complexity to the task complexity.\n",
        "published": "2018",
        "authors": [
            "Ozan \u0130rsoy",
            "Ethem Alpayd\u0131n"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.04849v3",
        "title": "The unreasonable effectiveness of the forget gate",
        "abstract": "  Given the success of the gated recurrent unit, a natural question is whether\nall the gates of the long short-term memory (LSTM) network are necessary.\nPrevious research has shown that the forget gate is one of the most important\ngates in the LSTM. Here we show that a forget-gate-only version of the LSTM\nwith chrono-initialized biases, not only provides computational savings but\noutperforms the standard LSTM on multiple benchmark datasets and competes with\nsome of the best contemporary models. Our proposed network, the JANET, achieves\naccuracies of 99% and 92.5% on the MNIST and pMNIST datasets, outperforming the\nstandard LSTM which yields accuracies of 98.5% and 91%.\n",
        "published": "2018",
        "authors": [
            "Jos van der Westhuizen",
            "Joan Lasenby"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.05267v1",
        "title": "Low-Precision Floating-Point Schemes for Neural Network Training",
        "abstract": "  The use of low-precision fixed-point arithmetic along with stochastic\nrounding has been proposed as a promising alternative to the commonly used\n32-bit floating point arithmetic to enhance training neural networks training\nin terms of performance and energy efficiency. In the first part of this paper,\nthe behaviour of the 12-bit fixed-point arithmetic when training a\nconvolutional neural network with the CIFAR-10 dataset is analysed, showing\nthat such arithmetic is not the most appropriate for the training phase. After\nthat, the paper presents and evaluates, under the same conditions, alternative\nlow-precision arithmetics, starting with the 12-bit floating-point arithmetic.\nThese two representations are then leveraged using local scaling in order to\nincrease accuracy and get closer to the baseline 32-bit floating-point\narithmetic. Finally, the paper introduces a simplified model in which both the\noutputs and the gradients of the neural networks are constrained to\npower-of-two values, just using 7 bits for their representation. The evaluation\ndemonstrates a minimal loss in accuracy for the proposed Power-of-Two neural\nnetwork, avoiding the use of multiplications and divisions and thereby,\nsignificantly reducing the training time as well as the energy consumption and\nmemory requirements during the training and inference phases.\n",
        "published": "2018",
        "authors": [
            "Marc Ortiz",
            "Adri\u00e1n Cristal",
            "Eduard Ayguad\u00e9",
            "Marc Casas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.06739v4",
        "title": "Are ResNets Provably Better than Linear Predictors?",
        "abstract": "  A residual network (or ResNet) is a standard deep neural net architecture,\nwith state-of-the-art performance across numerous applications. The main\npremise of ResNets is that they allow the training of each layer to focus on\nfitting just the residual of the previous layer's output and the target output.\nThus, we should expect that the trained network is no worse than what we can\nobtain if we remove the residual layers and train a shallower network instead.\nHowever, due to the non-convexity of the optimization problem, it is not at all\nclear that ResNets indeed achieve this behavior, rather than getting stuck at\nsome arbitrarily poor local minimum. In this paper, we rigorously prove that\narbitrarily deep, nonlinear residual units indeed exhibit this behavior, in the\nsense that the optimization landscape contains no local minima with value above\nwhat can be obtained with a linear predictor (namely a 1-layer network).\nNotably, we show this under minimal or no assumptions on the precise network\narchitecture, data distribution, or loss function used. We also provide a\nquantitative analysis of approximate stationary points for this problem.\nFinally, we show that with a certain tweak to the architecture, training the\nnetwork with standard stochastic gradient descent achieves an objective value\nclose or better than any linear predictor.\n",
        "published": "2018",
        "authors": [
            "Ohad Shamir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.07144v1",
        "title": "Human Activity Recognition using Recurrent Neural Networks",
        "abstract": "  Human activity recognition using smart home sensors is one of the bases of\nubiquitous computing in smart environments and a topic undergoing intense\nresearch in the field of ambient assisted living. The increasingly large amount\nof data sets calls for machine learning methods. In this paper, we introduce a\ndeep learning model that learns to classify human activities without using any\nprior knowledge. For this purpose, a Long Short Term Memory (LSTM) Recurrent\nNeural Network was applied to three real world smart home datasets. The results\nof these experiments show that the proposed approach outperforms the existing\nones in terms of accuracy and performance.\n",
        "published": "2018",
        "authors": [
            "Deepika Singh",
            "Erinc Merdivan",
            "Ismini Psychoula",
            "Johannes Kropf",
            "Sten Hanke",
            "Matthieu Geist",
            "Andreas Holzinger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.07209v4",
        "title": "NAIS-Net: Stable Deep Networks from Non-Autonomous Differential\n  Equations",
        "abstract": "  This paper introduces Non-Autonomous Input-Output Stable Network(NAIS-Net), a\nvery deep architecture where each stacked processing block is derived from a\ntime-invariant non-autonomous dynamical system. Non-autonomy is implemented by\nskip connections from the block input to each of the unrolled processing stages\nand allows stability to be enforced so that blocks can be unrolled adaptively\nto a pattern-dependent processing depth. NAIS-Net induces non-trivial,\nLipschitz input-output maps, even for an infinite unroll length. We prove that\nthe network is globally asymptotically stable so that for every initial\ncondition there is exactly one input-dependent equilibrium assuming $tanh$\nunits, and incrementally stable for ReL units. An efficient implementation that\nenforces the stability under derived conditions for both fully-connected and\nconvolutional layers is also presented. Experimental results show how NAIS-Net\nexhibits stability in practice, yielding a significant reduction in\ngeneralization gap compared to ResNets.\n",
        "published": "2018",
        "authors": [
            "Marco Ciccone",
            "Marco Gallieri",
            "Jonathan Masci",
            "Christian Osendorfer",
            "Faustino Gomez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.07824v2",
        "title": "Autotune: A Derivative-free Optimization Framework for Hyperparameter\n  Tuning",
        "abstract": "  Machine learning applications often require hyperparameter tuning. The\nhyperparameters usually drive both the efficiency of the model training process\nand the resulting model quality. For hyperparameter tuning, machine learning\nalgorithms are complex black-boxes. This creates a class of challenging\noptimization problems, whose objective functions tend to be nonsmooth,\ndiscontinuous, unpredictably varying in computational expense, and include\ncontinuous, categorical, and/or integer variables. Further, function\nevaluations can fail for a variety of reasons including numerical difficulties\nor hardware failures. Additionally, not all hyperparameter value combinations\nare compatible, which creates so called hidden constraints. Robust and\nefficient optimization algorithms are needed for hyperparameter tuning. In this\npaper we present an automated parallel derivative-free optimization framework\ncalled \\textbf{Autotune}, which combines a number of specialized sampling and\nsearch methods that are very effective in tuning machine learning models\ndespite these challenges. Autotune provides significantly improved models over\nusing default hyperparameter settings with minimal user interaction on\nreal-world applications. Given the inherent expense of training numerous\ncandidate models, we demonstrate the effectiveness of Autotune's search methods\nand the efficient distributed and parallel paradigms for training and tuning\nmodels, and also discuss the resource trade-offs associated with the ability to\nboth distribute the training process and parallelize the tuning process.\n",
        "published": "2018",
        "authors": [
            "Patrick Koch",
            "Oleg Golovidov",
            "Steven Gardner",
            "Brett Wujek",
            "Joshua Griffin",
            "Yan Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.08838v1",
        "title": "Measuring the Intrinsic Dimension of Objective Landscapes",
        "abstract": "  Many recently trained neural networks employ large numbers of parameters to\nachieve good performance. One may intuitively use the number of parameters\nrequired as a rough gauge of the difficulty of a problem. But how accurate are\nsuch notions? How many parameters are really needed? In this paper we attempt\nto answer this question by training networks not in their native parameter\nspace, but instead in a smaller, randomly oriented subspace. We slowly increase\nthe dimension of this subspace, note at which dimension solutions first appear,\nand define this to be the intrinsic dimension of the objective landscape. The\napproach is simple to implement, computationally tractable, and produces\nseveral suggestive conclusions. Many problems have smaller intrinsic dimensions\nthan one might suspect, and the intrinsic dimension for a given dataset varies\nlittle across a family of models with vastly different sizes. This latter\nresult has the profound implication that once a parameter space is large enough\nto solve a problem, extra parameters serve directly to increase the\ndimensionality of the solution manifold. Intrinsic dimension allows some\nquantitative comparison of problem difficulty across supervised, reinforcement,\nand other types of learning where we conclude, for example, that solving the\ninverted pendulum problem is 100 times easier than classifying digits from\nMNIST, and playing Atari Pong from pixels is about as hard as classifying\nCIFAR-10. In addition to providing new cartography of the objective landscapes\nwandered by parameterized models, the method is a simple technique for\nconstructively obtaining an upper bound on the minimum description length of a\nsolution. A byproduct of this construction is a simple approach for compressing\nnetworks, in some cases by more than 100 times.\n",
        "published": "2018",
        "authors": [
            "Chunyuan Li",
            "Heerad Farkhoor",
            "Rosanne Liu",
            "Jason Yosinski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.11188v1",
        "title": "Can recurrent neural networks warp time?",
        "abstract": "  Successful recurrent models such as long short-term memories (LSTMs) and\ngated recurrent units (GRUs) use ad hoc gating mechanisms. Empirically these\nmodels have been found to improve the learning of medium to long term temporal\ndependencies and to help with vanishing gradient issues. We prove that\nlearnable gates in a recurrent model formally provide quasi- invariance to\ngeneral time transformations in the input data. We recover part of the LSTM\narchitecture from a simple axiomatic approach. This result leads to a new way\nof initializing gate biases in LSTMs and GRUs. Ex- perimentally, this new\nchrono initialization is shown to greatly improve learning of long term\ndependencies, with minimal implementation effort.\n",
        "published": "2018",
        "authors": [
            "Corentin Tallec",
            "Yann Ollivier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.11237v2",
        "title": "Deep learning improved by biological activation functions",
        "abstract": "  `Biologically inspired' activation functions, such as the logistic sigmoid,\nhave been instrumental in the historical advancement of machine learning.\nHowever in the field of deep learning, they have been largely displaced by\nrectified linear units (ReLU) or similar functions, such as its exponential\nlinear unit (ELU) variant, to mitigate the effects of vanishing gradients\nassociated with error back-propagation. The logistic sigmoid however does not\nrepresent the true input-output relation in neuronal cells under physiological\nconditions. Here, bionodal root unit (BRU) activation functions are introduced,\nexhibiting input-output non-linearities that are substantially more\nbiologically plausible since their functional form is based on known\nbiophysical properties of neuronal cells.\n  In order to evaluate the learning performance of BRU activations, deep\nnetworks are constructed with identical architectures except differing in their\ntransfer functions (ReLU, ELU, and BRU). Multilayer perceptrons, stacked\nauto-encoders, and convolutional networks are used to test supervised and\nunsupervised learning based on the MNIST and CIFAR-10/100 datasets. Comparisons\nof learning performance, quantified using loss and error measurements,\ndemonstrate that bionodal networks both train faster than their ReLU and ELU\ncounterparts and result in the best generalised models even in the absence of\nformal regularisation. These results therefore suggest that revisiting the\ndetailed properties of biological neurones and their circuitry might prove\ninvaluable in the field of deep learning for the future.\n",
        "published": "2018",
        "authors": [
            "Gardave S Bhumbra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.11285v2",
        "title": "Adversarially Robust Generalization Requires More Data",
        "abstract": "  Machine learning models are often susceptible to adversarial perturbations of\ntheir inputs. Even small perturbations can cause state-of-the-art classifiers\nwith high \"standard\" accuracy to produce an incorrect prediction with high\nconfidence. To better understand this phenomenon, we study adversarially robust\nlearning from the viewpoint of generalization. We show that already in a simple\nnatural data model, the sample complexity of robust learning can be\nsignificantly larger than that of \"standard\" learning. This gap is information\ntheoretic and holds irrespective of the training algorithm or the model family.\nWe complement our theoretical results with experiments on popular image\nclassification datasets and show that a similar gap exists here as well. We\npostulate that the difficulty of training robust classifiers stems, at least\npartially, from this inherently larger sample complexity.\n",
        "published": "2018",
        "authors": [
            "Ludwig Schmidt",
            "Shibani Santurkar",
            "Dimitris Tsipras",
            "Kunal Talwar",
            "Aleksander M\u0105dry"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.00987v1",
        "title": "Automatic Inference of Cross-modal Connection Topologies for X-CNNs",
        "abstract": "  This paper introduces a way to learn cross-modal convolutional neural network\n(X-CNN) architectures from a base convolutional network (CNN) and the training\ndata to reduce the design cost and enable applying cross-modal networks in\nsparse data environments. Two approaches for building X-CNNs are presented. The\nbase approach learns the topology in a data-driven manner, by using\nmeasurements performed on the base CNN and supplied data. The iterative\napproach performs further optimisation of the topology through a combined\nlearning procedure, simultaneously learning the topology and training the\nnetwork. The approaches were evaluated agains examples of hand-designed X-CNNs\nand their base variants, showing superior performance and, in some cases,\ngaining an additional 9% of accuracy. From further considerations, we conclude\nthat the presented methodology takes less time than any manual approach would,\nwhilst also significantly reducing the design complexity. The application of\nthe methods is fully automated and implemented in Xsertion library.\n",
        "published": "2018",
        "authors": [
            "Laurynas Karazija",
            "Petar Veli\u010dkovi\u0107",
            "Pietro Li\u00f2"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.01516v3",
        "title": "How deep should be the depth of convolutional neural networks: a\n  backyard dog case study",
        "abstract": "  The work concerns the problem of reducing a pre-trained deep neuronal network\nto a smaller network, with just few layers, whilst retaining the network's\nfunctionality on a given task\n  The proposed approach is motivated by the observation that the aim to deliver\nthe highest accuracy possible in the broadest range of operational conditions,\nwhich many deep neural networks models strive to achieve, may not necessarily\nbe always needed, desired, or even achievable due to the lack of data or\ntechnical constraints. In relation to the face recognition problem, we\nformulated an example of such a usecase, the `backyard dog' problem. The\n`backyard dog', implemented by a lean network, should correctly identify\nmembers from a limited group of individuals, a `family', and should distinguish\nbetween them. At the same time, the network must produce an alarm to an image\nof an individual who is not in a member of the family. To produce such a\nnetwork, we propose a shallowing algorithm. The algorithm takes an existing\ndeep learning model on its input and outputs a shallowed version of it. The\nalgorithm is non-iterative and is based on the Advanced Supervised Principal\nComponent Analysis. Performance of the algorithm is assessed in exhaustive\nnumerical experiments. In the above usecase, the `backyard dog' problem, the\nmethod is capable of drastically reducing the depth of deep learning neural\nnetworks, albeit at the cost of mild performance deterioration.\n  We developed a simple non-iterative method for shallowing down pre-trained\ndeep networks. The method is generic in the sense that it applies to a broad\nclass of feed-forward networks, and is based on the Advanced Supervise\nPrincipal Component Analysis. The method enables generation of families of\nsmaller-size shallower specialized networks tuned for specific operational\nconditions and tasks from a single larger and more universal legacy network.\n",
        "published": "2018",
        "authors": [
            "A. N. Gorban",
            "E. M. Mirkes",
            "I. Y. Tyukin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.03908v1",
        "title": "Towards a universal neural network encoder for time series",
        "abstract": "  We study the use of a time series encoder to learn representations that are\nuseful on data set types with which it has not been trained on. The encoder is\nformed of a convolutional neural network whose temporal output is summarized by\na convolutional attention mechanism. This way, we obtain a compact,\nfixed-length representation from longer, variable-length time series. We\nevaluate the performance of the proposed approach on a well-known time series\nclassification benchmark, considering full adaptation, partial adaptation, and\nno adaptation of the encoder to the new data type. Results show that such\nstrategies are competitive with the state-of-the-art, often outperforming\nconceptually-matching approaches. Besides accuracy scores, the facility of\nadaptation and the efficiency of pre-trained encoders make them an appealing\noption for the processing of scarcely- or non-labeled time series.\n",
        "published": "2018",
        "authors": [
            "Joan Serr\u00e0",
            "Santiago Pascual",
            "Alexandros Karatzoglou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.03963v4",
        "title": "Monotone Learning with Rectified Wire Networks",
        "abstract": "  We introduce a new neural network model, together with a tractable and\nmonotone online learning algorithm. Our model describes feed-forward networks\nfor classification, with one output node for each class. The only nonlinear\noperation is rectification using a ReLU function with a bias. However, there is\na rectifier on every edge rather than at the nodes of the network. There are\nalso weights, but these are positive, static, and associated with the nodes.\nOur \"rectified wire\" networks are able to represent arbitrary Boolean\nfunctions. Only the bias parameters, on the edges of the network, are learned.\nAnother departure in our approach, from standard neural networks, is that the\nloss function is replaced by a constraint. This constraint is simply that the\nvalue of the output node associated with the correct class should be zero. Our\nmodel has the property that the exact norm-minimizing parameter update,\nrequired to correctly classify a training item, is the solution to a quadratic\nprogram that can be computed with a few passes through the network. We\ndemonstrate a training algorithm using this update, called sequential\ndeactivation (SDA), on MNIST and some synthetic datasets. Upon adopting a\nnatural choice for the nodal weights, SDA has no hyperparameters other than\nthose describing the network structure. Our experiments explore behavior with\nrespect to network size and depth in a family of sparse expander networks.\n",
        "published": "2018",
        "authors": [
            "Veit Elser",
            "Dan Schmidt",
            "Jonathan Yedidia"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.07500v2",
        "title": "GADAM: Genetic-Evolutionary ADAM for Deep Neural Network Optimization",
        "abstract": "  Deep neural network learning can be formulated as a non-convex optimization\nproblem. Existing optimization algorithms, e.g., Adam, can learn the models\nfast, but may get stuck in local optima easily. In this paper, we introduce a\nnovel optimization algorithm, namely GADAM (Genetic-Evolutionary Adam). GADAM\nlearns deep neural network models based on a number of unit models generations\nby generations: it trains the unit models with Adam, and evolves them to the\nnew generations with genetic algorithm. We will show that GADAM can effectively\njump out of the local optima in the learning process to obtain better\nsolutions, and prove that GADAM can also achieve a very fast convergence.\nExtensive experiments have been done on various benchmark datasets, and the\nlearning results will demonstrate the effectiveness and efficiency of the GADAM\nalgorithm.\n",
        "published": "2018",
        "authors": [
            "Jiawei Zhang",
            "Fisher B. Gouza"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.07502v1",
        "title": "On Deep Ensemble Learning from a Function Approximation Perspective",
        "abstract": "  In this paper, we propose to provide a general ensemble learning framework\nbased on deep learning models. Given a group of unit models, the proposed deep\nensemble learning framework will effectively combine their learning results via\na multilayered ensemble model. In the case when the unit model mathematical\nmappings are bounded, sigmoidal and discriminatory, we demonstrate that the\ndeep ensemble learning framework can achieve a universal approximation of any\nfunctions from the input space to the output space. Meanwhile, to achieve such\na performance, the deep ensemble learning framework also impose a strict\nconstraint on the number of involved unit models. According to the theoretic\nproof provided in this paper, given the input feature space of dimension d, the\nrequired unit model number will be 2d, if the ensemble model involves one\nsingle layer. Furthermore, as the ensemble component goes deeper, the number of\nrequired unit model is proved to be lowered down exponentially.\n",
        "published": "2018",
        "authors": [
            "Jiawei Zhang",
            "Limeng Cui",
            "Fisher B. Gouza"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.07507v1",
        "title": "Reconciled Polynomial Machine: A Unified Representation of Shallow and\n  Deep Learning Models",
        "abstract": "  In this paper, we aim at introducing a new machine learning model, namely\nreconciled polynomial machine, which can provide a unified representation of\nexisting shallow and deep machine learning models. Reconciled polynomial\nmachine predicts the output by computing the inner product of the feature\nkernel function and variable reconciling function. Analysis of several concrete\nmodels, including Linear Models, FM, MVM, Perceptron, MLP and Deep Neural\nNetworks, will be provided in this paper, which can all be reduced to the\nreconciled polynomial machine representations. Detailed analysis of the\nlearning error by these models will also be illustrated in this paper based on\ntheir reduced representations from the function approximation perspective.\n",
        "published": "2018",
        "authors": [
            "Jiawei Zhang",
            "Limeng Cui",
            "Fisher B. Gouza"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.07508v1",
        "title": "GEN Model: An Alternative Approach to Deep Neural Network Models",
        "abstract": "  In this paper, we introduce an alternative approach, namely GEN (Genetic\nEvolution Network) Model, to the deep learning models. Instead of building one\nsingle deep model, GEN adopts a genetic-evolutionary learning strategy to build\na group of unit models generations by generations. Significantly different from\nthe wellknown representation learning models with extremely deep structures,\nthe unit models covered in GEN are of a much shallower architecture. In the\ntraining process, from each generation, a subset of unit models will be\nselected based on their performance to evolve and generate the child models in\nthe next generation. GEN has significant advantages compared with existing deep\nrepresentation learning models in terms of both learning effectiveness,\nefficiency and interpretability of the learning process and learned results.\nExtensive experiments have been done on diverse benchmark datasets, and the\nexperimental results have demonstrated the outstanding performance of GEN\ncompared with the state-of-the-art baseline methods in both effectiveness of\nefficiency.\n",
        "published": "2018",
        "authors": [
            "Jiawei Zhang",
            "Limeng Cui",
            "Fisher B. Gouza"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.07569v2",
        "title": "Reliable counting of weakly labeled concepts by a single spiking neuron\n  model",
        "abstract": "  Making an informed, correct and quick decision can be life-saving. It's\ncrucial for animals during an escape behaviour or for autonomous cars during\ndriving. The decision can be complex and may involve an assessment of the\namount of threats present and the nature of each threat. Thus, we should expect\nearly sensory processing to supply classification information fast and\naccurately, even before relying the information to higher brain areas or more\ncomplex system components downstream. Today, advanced convolutional artificial\nneural networks can successfully solve visual detection and classification\ntasks and are commonly used to build complex decision making systems. However,\nin order to perform well on these tasks they require increasingly complex,\n\"very deep\" model structure, which is costly in inference run-time, energy\nconsumption and number of training samples, only trainable on cloud-computing\nclusters. A single spiking neuron has been shown to be able to solve\nrecognition tasks for homogeneous Poisson input statistics, a commonly used\nmodel for spiking activity in the neocortex. When modeled as leaky integrate\nand fire with gradient decent learning algorithm it was shown to posses a\nvariety of complex computational capabilities. Here we improve its\nimplementation. We also account for more natural stimulus generated inputs that\ndeviate from this homogeneous Poisson spiking. The improved gradient-based\nlocal learning rule allows for significantly better and stable generalization.\nWe also show that with its improved capabilities it can count weakly labeled\nconcepts by applying our model to a problem of multiple instance learning (MIL)\nwith counting where labels are only available for collections of concepts. In\nthis counting MNIST task the neuron exploits the improved implementation and\noutperforms conventional ConvNet architecture under similar condtions.\n",
        "published": "2018",
        "authors": [
            "Hannes Rapp",
            "Martin Paul Nawrot",
            "Merav Stern"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.07816v5",
        "title": "Towards Understanding Limitations of Pixel Discretization Against\n  Adversarial Attacks",
        "abstract": "  Wide adoption of artificial neural networks in various domains has led to an\nincreasing interest in defending adversarial attacks against them.\nPreprocessing defense methods such as pixel discretization are particularly\nattractive in practice due to their simplicity, low computational overhead, and\napplicability to various systems. It is observed that such methods work well on\nsimple datasets like MNIST, but break on more complicated ones like ImageNet\nunder recently proposed strong white-box attacks. To understand the conditions\nfor success and potentials for improvement, we study the pixel discretization\ndefense method, including more sophisticated variants that take into account\nthe properties of the dataset being discretized. Our results again show poor\nresistance against the strong attacks. We analyze our results in a theoretical\nframework and offer strong evidence that pixel discretization is unlikely to\nwork on all but the simplest of the datasets. Furthermore, our arguments\npresent insights why some other preprocessing defenses may be insecure.\n",
        "published": "2018",
        "authors": [
            "Jiefeng Chen",
            "Xi Wu",
            "Vaibhav Rastogi",
            "Yingyu Liang",
            "Somesh Jha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.07917v2",
        "title": "Evolution-Guided Policy Gradient in Reinforcement Learning",
        "abstract": "  Deep Reinforcement Learning (DRL) algorithms have been successfully applied\nto a range of challenging control tasks. However, these methods typically\nsuffer from three core difficulties: temporal credit assignment with sparse\nrewards, lack of effective exploration, and brittle convergence properties that\nare extremely sensitive to hyperparameters. Collectively, these challenges\nseverely limit the applicability of these approaches to real-world problems.\nEvolutionary Algorithms (EAs), a class of black box optimization techniques\ninspired by natural evolution, are well suited to address each of these three\nchallenges. However, EAs typically suffer from high sample complexity and\nstruggle to solve problems that require optimization of a large number of\nparameters. In this paper, we introduce Evolutionary Reinforcement Learning\n(ERL), a hybrid algorithm that leverages the population of an EA to provide\ndiversified data to train an RL agent, and reinserts the RL agent into the EA\npopulation periodically to inject gradient information into the EA. ERL\ninherits EA's ability of temporal credit assignment with a fitness metric,\neffective exploration with a diverse set of policies, and stability of a\npopulation-based approach and complements it with off-policy DRL's ability to\nleverage gradients for higher sample efficiency and faster learning.\nExperiments in a range of challenging continuous control benchmarks demonstrate\nthat ERL significantly outperforms prior DRL and EA methods.\n",
        "published": "2018",
        "authors": [
            "Shauharda Khadka",
            "Kagan Tumer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.08289v3",
        "title": "Measuring and regularizing networks in function space",
        "abstract": "  To optimize a neural network one often thinks of optimizing its parameters,\nbut it is ultimately a matter of optimizing the function that maps inputs to\noutputs. Since a change in the parameters might serve as a poor proxy for the\nchange in the function, it is of some concern that primacy is given to\nparameters but that the correspondence has not been tested. Here, we show that\nit is simple and computationally feasible to calculate distances between\nfunctions in a $L^2$ Hilbert space. We examine how typical networks behave in\nthis space, and compare how parameter $\\ell^2$ distances compare to function\n$L^2$ distances between various points of an optimization trajectory. We find\nthat the two distances are nontrivially related. In particular, the\n$L^2/\\ell^2$ ratio decreases throughout optimization, reaching a steady value\naround when test error plateaus. We then investigate how the $L^2$ distance\ncould be applied directly to optimization. We first propose that in multitask\nlearning, one can avoid catastrophic forgetting by directly limiting how much\nthe input/output function changes between tasks. Secondly, we propose a new\nlearning rule that constrains the distance a network can travel through\n$L^2$-space in any one update. This allows new examples to be learned in a way\nthat minimally interferes with what has previously been learned. These\napplications demonstrate how one can measure and regularize function distances\ndirectly, without relying on parameters or local approximations like loss\ncurvature.\n",
        "published": "2018",
        "authors": [
            "Ari S. Benjamin",
            "David Rolnick",
            "Konrad Kording"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.08574v4",
        "title": "Breaking the Activation Function Bottleneck through Adaptive\n  Parameterization",
        "abstract": "  Standard neural network architectures are non-linear only by virtue of a\nsimple element-wise activation function, making them both brittle and\nexcessively large. In this paper, we consider methods for making the\nfeed-forward layer more flexible while preserving its basic structure. We\ndevelop simple drop-in replacements that learn to adapt their parameterization\nconditional on the input, thereby increasing statistical efficiency\nsignificantly. We present an adaptive LSTM that advances the state of the art\nfor the Penn Treebank and WikiText-2 word-modeling tasks while using fewer\nparameters and converging in less than half as many iterations.\n",
        "published": "2018",
        "authors": [
            "Sebastian Flennerhag",
            "Hujun Yin",
            "John Keane",
            "Mark Elliot"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.08786v2",
        "title": "Mean Field Theory of Activation Functions in Deep Neural Networks",
        "abstract": "  We present a Statistical Mechanics (SM) model of deep neural networks,\nconnecting the energy-based and the feed forward networks (FFN) approach. We\ninfer that FFN can be understood as performing three basic steps: encoding,\nrepresentation validation and propagation. From the meanfield solution of the\nmodel, we obtain a set of natural activations -- such as Sigmoid, $\\tanh$ and\nReLu -- together with the state-of-the-art, Swish; this represents the expected\ninformation propagating through the network and tends to ReLu in the limit of\nzero noise.We study the spectrum of the Hessian on an associated classification\ntask, showing that Swish allows for more consistent performances over a wider\nrange of network architectures.\n",
        "published": "2018",
        "authors": [
            "Mirco Milletar\u00ed",
            "Thiparat Chotibut",
            "Paolo E. Trevisanutto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.09874v2",
        "title": "Learning Nonlinear Brain Dynamics: van der Pol Meets LSTM",
        "abstract": "  Many real-world data sets, especially in biology, are produced by complex\nnonlinear dynamical systems. In this paper, we focus on brain calcium imaging\n(CaI) of different organisms (zebrafish and rat), aiming to build a model of\njoint activation dynamics in large neuronal populations, including the whole\nbrain of zebrafish. We propose a new approach for capturing dynamics of\ntemporal SVD components that uses the coupled (multivariate) van der Pol (VDP)\noscillator, a nonlinear ordinary differential equation (ODE) model describing\nneural activity, with a new parameter estimation technique that combines\nvariable projection optimization and stochastic search. We show that the\napproach successfully handles nonlinearities and hidden state variables in the\ncoupled VDP. The approach is accurate, achieving 0.82 to 0.94 correlation\nbetween the actual and model-generated components, and interpretable, as VDP's\ncoupling matrix reveals anatomically meaningful positive (excitatory) and\nnegative (inhibitory) interactions across different brain subsystems\ncorresponding to spatial SVD components. Moreover, VDP is comparable to (or\nsometimes better than) recurrent neural networks (LSTM) for (short-term)\nprediction of future brain activity; VDP needs less parameters to train, which\nwas a plus on our small training data. Finally, the overall best predictive\nmethod, greatly outperforming both VDP and LSTM in short- and long-term\npredictive settings on both datasets, was the new hybrid VDP-LSTM approach that\nused VDP to simulate large domain-specific dataset for LSTM pretraining; note\nthat simple LSTM data-augmentation via noisy versions of training data was much\nless effective.\n",
        "published": "2018",
        "authors": [
            "German Abrevaya",
            "Irina Rish",
            "Aleksandr Y. Aravkin",
            "Guillermo Cecchi",
            "James Kozloski",
            "Pablo Polosecki",
            "Peng Zheng",
            "Silvina Ponce Dawson",
            "Juliana Rhee",
            "David Cox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.10692v2",
        "title": "Compact and Computationally Efficient Representation of Deep Neural\n  Networks",
        "abstract": "  At the core of any inference procedure in deep neural networks are dot\nproduct operations, which are the component that require the highest\ncomputational resources. A common approach to reduce the cost of inference is\nto reduce its memory complexity by lowering the entropy of the weight matrices\nof the neural network, e.g., by pruning and quantizing their elements. However,\nthe quantized weight matrices are then usually represented either by a dense or\nsparse matrix storage format, whose associated dot product complexity is not\nbounded by the entropy of the matrix. This means that the associated inference\ncomplexity ultimately depends on the implicit statistical assumptions that\nthese matrix representations make about the weight distribution, which can be\nin many cases suboptimal. In this paper we address this issue and present new\nefficient representations for matrices with low entropy statistics. These new\nmatrix formats have the novel property that their memory and algorithmic\ncomplexity are implicitly bounded by the entropy of the matrix, consequently\nimplying that they are guaranteed to become more efficient as the entropy of\nthe matrix is being reduced. In our experiments we show that performing the dot\nproduct under these new matrix formats can indeed be more energy and time\nefficient under practically relevant assumptions. For instance, we are able to\nattain up to x42 compression ratios, x5 speed ups and x90 energy savings when\nwe convert in a lossless manner the weight matrices of state-of-the-art\nnetworks such as AlexNet, VGG-16, ResNet152 and DenseNet into the new matrix\nformats and benchmark their respective dot product operation.\n",
        "published": "2018",
        "authors": [
            "Simon Wiedemann",
            "Klaus-Robert M\u00fcller",
            "Wojciech Samek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.11604v5",
        "title": "How Does Batch Normalization Help Optimization?",
        "abstract": "  Batch Normalization (BatchNorm) is a widely adopted technique that enables\nfaster and more stable training of deep neural networks (DNNs). Despite its\npervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly\nunderstood. The popular belief is that this effectiveness stems from\ncontrolling the change of the layers' input distributions during training to\nreduce the so-called \"internal covariate shift\". In this work, we demonstrate\nthat such distributional stability of layer inputs has little to do with the\nsuccess of BatchNorm. Instead, we uncover a more fundamental impact of\nBatchNorm on the training process: it makes the optimization landscape\nsignificantly smoother. This smoothness induces a more predictive and stable\nbehavior of the gradients, allowing for faster training.\n",
        "published": "2018",
        "authors": [
            "Shibani Santurkar",
            "Dimitris Tsipras",
            "Andrew Ilyas",
            "Aleksander Madry"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.11703v3",
        "title": "Biologically Motivated Algorithms for Propagating Local Target\n  Representations",
        "abstract": "  Finding biologically plausible alternatives to back-propagation of errors is\na fundamentally important challenge in artificial neural network research. In\nthis paper, we propose a learning algorithm called error-driven Local\nRepresentation Alignment (LRA-E), which has strong connections to predictive\ncoding, a theory that offers a mechanistic way of describing neurocomputational\nmachinery. In addition, we propose an improved variant of Difference Target\nPropagation, another procedure that comes from the same family of algorithms as\nLRA-E. We compare our procedures to several other biologically-motivated\nalgorithms, including two feedback alignment algorithms and Equilibrium\nPropagation. In two benchmarks, we find that both of our proposed algorithms\nyield stable performance and strong generalization compared to other competing\nback-propagation alternatives when training deeper, highly nonlinear networks,\nwith LRA-E performing the best overall.\n",
        "published": "2018",
        "authors": [
            "Alexander G. Ororbia",
            "Ankur Mali"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.11987v3",
        "title": "l0-norm Based Centers Selection for Training Fault Tolerant RBF Networks\n  and Selecting Centers",
        "abstract": "  The aim of this paper is to train an RBF neural network and select centers\nunder concurrent faults. It is well known that fault tolerance is a very\nattractive property for neural networks. And center selection is an important\nprocedure during the training process of an RBF neural network. In this paper,\nwe devise two novel algorithms to address these two issues simultaneously. Both\nof them are based on the ADMM framework. In the first method, the minimax\nconcave penalty (MCP) function is introduced to select centers. In the second\nmethod, an l0-norm term is directly used, and the hard threshold (HT) is\nutilized to address the l0-norm term. Under several mild conditions, we can\nprove that both methods can globally converge to a unique limit point.\nSimulation results show that, under concurrent fault, the proposed algorithms\nare superior to many existing methods.\n",
        "published": "2018",
        "authors": [
            "Hao Wang",
            "Chi-Sing Leung",
            "Hing Cheung So",
            "Ruibin Feng",
            "Zifa Han"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.01280v3",
        "title": "Geared Rotationally Identical and Invariant Convolutional Neural Network\n  Systems",
        "abstract": "  Theorems and techniques to form different types of transformationally\ninvariant processing and to produce the same output quantitatively based on\neither transformationally invariant operators or symmetric operations have\nrecently been introduced by the authors. In this study, we further propose to\ncompose a geared rotationally identical CNN system (GRI-CNN) with a small step\nangle by connecting networks of participated processes at the first flatten\nlayer. Using an ordinary CNN structure as a base, requirements for constructing\na GRI-CNN include the use of either symmetric input vector or kernels with an\nangle increment that can form a complete cycle as a \"gearwheel\". Four basic\nGRI-CNN structures were studied. Each of them can produce quantitatively\nidentical output results when a rotation angle of the input vector is evenly\ndivisible by the step angle of the gear. Our study showed when an input vector\nrotated with an angle does not match to a step angle, the GRI-CNN can also\nproduce a highly consistent result. With a design of using an ultra-fine\ngear-tooth step angle (e.g., 1 degree or 0.1 degree), all four GRI-CNN systems\ncan be constructed virtually isotropically.\n",
        "published": "2018",
        "authors": [
            "ShihChung B. Lo",
            "Ph. D.",
            "Matthew T. Freedman",
            "M. D.",
            "Seong K. Mun",
            "Ph. D.",
            "Heang-Ping Chan",
            "Ph. D"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.02016v3",
        "title": "MCRM: Mother Compact Recurrent Memory",
        "abstract": "  LSTMs and GRUs are the most common recurrent neural network architectures\nused to solve temporal sequence problems. The two architectures have differing\ndata flows dealing with a common component called the cell state (also referred\nto as the memory). We attempt to enhance the memory by presenting a\nmodification that we call the Mother Compact Recurrent Memory (MCRM). MCRMs are\na type of a nested LSTM-GRU architecture where the cell state is the GRU hidden\nstate. The concatenation of the forget gate and input gate interactions from\nthe LSTM are considered an input to the GRU cell. Because MCRMs has this type\nof nesting, MCRMs have a compact memory pattern consisting of neurons that acts\nexplicitly in both long-term and short-term fashions. For some specific tasks,\nempirical results show that MCRMs outperform previously used architectures.\n",
        "published": "2018",
        "authors": [
            "Abduallah A. Mohamed",
            "Christian Claudel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.02234v3",
        "title": "Deep Stacked Stochastic Configuration Networks for Lifelong Learning of\n  Non-Stationary Data Streams",
        "abstract": "  The concept of SCN offers a fast framework with universal approximation\nguarantee for lifelong learning of non-stationary data streams. Its adaptive\nscope selection property enables for proper random generation of hidden unit\nparameters advancing conventional randomized approaches constrained with a\nfixed scope of random parameters. This paper proposes deep stacked stochastic\nconfiguration network (DSSCN) for continual learning of non-stationary data\nstreams which contributes two major aspects: 1) DSSCN features a\nself-constructing methodology of deep stacked network structure where hidden\nunit and hidden layer are extracted automatically from continuously generated\ndata streams; 2) the concept of SCN is developed to randomly assign inverse\ncovariance matrix of multivariate Gaussian function in the hidden node addition\nstep bypassing its computationally prohibitive tuning phase. Numerical\nevaluation and comparison with prominent data stream algorithms under two\nprocedures: periodic hold-out and prequential test-then-train processes\ndemonstrate the advantage of proposed methodology.\n",
        "published": "2018",
        "authors": [
            "Mahardhika Pratama",
            "Dianhui Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.02822v1",
        "title": "Backprop Evolution",
        "abstract": "  The back-propagation algorithm is the cornerstone of deep learning. Despite\nits importance, few variations of the algorithm have been attempted. This work\npresents an approach to discover new variations of the back-propagation\nequation. We use a domain specific lan- guage to describe update equations as a\nlist of primitive functions. An evolution-based method is used to discover new\npropagation rules that maximize the generalization per- formance after a few\nepochs of training. We find several update equations that can train faster with\nshort training times than standard back-propagation, and perform similar as\nstandard back-propagation at convergence.\n",
        "published": "2018",
        "authors": [
            "Maximilian Alber",
            "Irwan Bello",
            "Barret Zoph",
            "Pieter-Jan Kindermans",
            "Prajit Ramachandran",
            "Quoc Le"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.03958v4",
        "title": "Neural System Identification with Spike-triggered Non-negative Matrix\n  Factorization",
        "abstract": "  Neuronal circuits formed in the brain are complex with intricate connection\npatterns. Such complexity is also observed in the retina as a relatively simple\nneuronal circuit. A retinal ganglion cell receives excitatory inputs from\nneurons in previous layers as driving forces to fire spikes. Analytical methods\nare required that can decipher these components in a systematic manner.\nRecently a method termed spike-triggered non-negative matrix factorization\n(STNMF) has been proposed for this purpose. In this study, we extend the scope\nof the STNMF method. By using the retinal ganglion cell as a model system, we\nshow that STNMF can detect various computational properties of upstream bipolar\ncells, including spatial receptive field, temporal filter, and transfer\nnonlinearity. In addition, we recover synaptic connection strengths from the\nweight matrix of STNMF. Furthermore, we show that STNMF can separate spikes of\na ganglion cell into a few subsets of spikes where each subset is contributed\nby one presynaptic bipolar cell. Taken together, these results corroborate that\nSTNMF is a useful method for deciphering the structure of neuronal circuits.\n",
        "published": "2018",
        "authors": [
            "Shanshan Jia",
            "Zhaofei Yu",
            "Arno Onken",
            "Yonghong Tian",
            "Tiejun Huang",
            "Jian K. Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.04752v2",
        "title": "A Survey on Methods and Theories of Quantized Neural Networks",
        "abstract": "  Deep neural networks are the state-of-the-art methods for many real-world\ntasks, such as computer vision, natural language processing and speech\nrecognition. For all its popularity, deep neural networks are also criticized\nfor consuming a lot of memory and draining battery life of devices during\ntraining and inference. This makes it hard to deploy these models on mobile or\nembedded devices which have tight resource constraints. Quantization is\nrecognized as one of the most effective approaches to satisfy the extreme\nmemory requirements that deep neural network models demand. Instead of adopting\n32-bit floating point format to represent weights, quantized representations\nstore weights using more compact formats such as integers or even binary\nnumbers. Despite a possible degradation in predictive performance, quantization\nprovides a potential solution to greatly reduce the model size and the energy\nconsumption. In this survey, we give a thorough review of different aspects of\nquantized neural networks. Current challenges and trends of quantized neural\nnetworks are also discussed.\n",
        "published": "2018",
        "authors": [
            "Yunhui Guo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.05377v3",
        "title": "Neural Architecture Search: A Survey",
        "abstract": "  Deep Learning has enabled remarkable progress over the last years on a\nvariety of tasks, such as image recognition, speech recognition, and machine\ntranslation. One crucial aspect for this progress are novel neural\narchitectures. Currently employed architectures have mostly been developed\nmanually by human experts, which is a time-consuming and error-prone process.\nBecause of this, there is growing interest in automated neural architecture\nsearch methods. We provide an overview of existing work in this field of\nresearch and categorize them according to three dimensions: search space,\nsearch strategy, and performance estimation strategy.\n",
        "published": "2018",
        "authors": [
            "Thomas Elsken",
            "Jan Hendrik Metzen",
            "Frank Hutter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.08798v1",
        "title": "Beyond expectation: Deep joint mean and quantile regression for\n  spatio-temporal problems",
        "abstract": "  Spatio-temporal problems are ubiquitous and of vital importance in many\nresearch fields. Despite the potential already demonstrated by deep learning\nmethods in modeling spatio-temporal data, typical approaches tend to focus\nsolely on conditional expectations of the output variables being modeled. In\nthis paper, we propose a multi-output multi-quantile deep learning approach for\njointly modeling several conditional quantiles together with the conditional\nexpectation as a way to provide a more complete \"picture\" of the predictive\ndensity in spatio-temporal problems. Using two large-scale datasets from the\ntransportation domain, we empirically demonstrate that, by approaching the\nquantile regression problem from a multi-task learning perspective, it is\npossible to solve the embarrassing quantile crossings problem, while\nsimultaneously significantly outperforming state-of-the-art quantile regression\nmethods. Moreover, we show that jointly modeling the mean and several\nconditional quantiles not only provides a rich description about the predictive\ndensity that can capture heteroscedastic properties at a neglectable\ncomputational overhead, but also leads to improved predictions of the\nconditional expectation due to the extra information and a regularization\neffect induced by the added quantiles.\n",
        "published": "2018",
        "authors": [
            "Filipe Rodrigues",
            "Francisco C. Pereira"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.01222v3",
        "title": "CEM-RL: Combining evolutionary and gradient-based methods for policy\n  search",
        "abstract": "  Deep neuroevolution and deep reinforcement learning (deep RL) algorithms are\ntwo popular approaches to policy search. The former is widely applicable and\nrather stable, but suffers from low sample efficiency. By contrast, the latter\nis more sample efficient, but the most sample efficient variants are also\nrather unstable and highly sensitive to hyper-parameter setting. So far, these\nfamilies of methods have mostly been compared as competing tools. However, an\nemerging approach consists in combining them so as to get the best of both\nworlds. Two previously existing combinations use either an ad hoc evolutionary\nalgorithm or a goal exploration process together with the Deep Deterministic\nPolicy Gradient (DDPG) algorithm, a sample efficient off-policy deep RL\nalgorithm. In this paper, we propose a different combination scheme using the\nsimple cross-entropy method (CEM) and Twin Delayed Deep Deterministic policy\ngradient (td3), another off-policy deep RL algorithm which improves over ddpg.\nWe evaluate the resulting method, cem-rl, on a set of benchmarks classically\nused in deep RL. We show that cem-rl benefits from several advantages over its\ncompetitors and offers a satisfactory trade-off between performance and sample\nefficiency.\n",
        "published": "2018",
        "authors": [
            "Alo\u00efs Pourchot",
            "Olivier Sigaud"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.01322v3",
        "title": "Learning with Random Learning Rates",
        "abstract": "  Hyperparameter tuning is a bothersome step in the training of deep learning\nmodels. One of the most sensitive hyperparameters is the learning rate of the\ngradient descent. We present the 'All Learning Rates At Once' (Alrao)\noptimization method for neural networks: each unit or feature in the network\ngets its own learning rate sampled from a random distribution spanning several\norders of magnitude. This comes at practically no computational cost. Perhaps\nsurprisingly, stochastic gradient descent (SGD) with Alrao performs close to\nSGD with an optimally tuned learning rate, for various architectures and\nproblems. Alrao could save time when testing deep learning models: a range of\nmodels could be quickly assessed with Alrao, and the most promising models\ncould then be trained more extensively. This text comes with a PyTorch\nimplementation of the method, which can be plugged on an existing PyTorch\nmodel: https://github.com/leonardblier/alrao .\n",
        "published": "2018",
        "authors": [
            "L\u00e9onard Blier",
            "Pierre Wolinski",
            "Yann Ollivier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.02225v1",
        "title": "Memristor-based Deep Convolution Neural Network: A Case Study",
        "abstract": "  In this paper, we firstly introduce a method to efficiently implement\nlarge-scale high-dimensional convolution with realistic memristor-based circuit\ncomponents. An experiment verified simulator is adapted for accurate prediction\nof analog crossbar behavior. An improved conversion algorithm is developed to\nconvert convolution kernels to memristor-based circuits, which minimizes the\nerror with consideration of the data and kernel patterns in CNNs. With circuit\nsimulation for all convolution layers in ResNet-20, we found that 8-bit ADC/DAC\nis necessary to preserve software level classification accuracy.\n",
        "published": "2018",
        "authors": [
            "Fan Zhang",
            "Miao Hu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.02281v3",
        "title": "A Convergence Analysis of Gradient Descent for Deep Linear Neural\n  Networks",
        "abstract": "  We analyze speed of convergence to global optimum for gradient descent\ntraining a deep linear neural network (parameterized as $x \\mapsto W_N W_{N-1}\n\\cdots W_1 x$) by minimizing the $\\ell_2$ loss over whitened data. Convergence\nat a linear rate is guaranteed when the following hold: (i) dimensions of\nhidden layers are at least the minimum of the input and output dimensions; (ii)\nweight matrices at initialization are approximately balanced; and (iii) the\ninitial loss is smaller than the loss of any rank-deficient solution. The\nassumptions on initialization (conditions (ii) and (iii)) are necessary, in the\nsense that violating any one of them may lead to convergence failure. Moreover,\nin the important case of output dimension 1, i.e. scalar regression, they are\nmet, and thus convergence to global optimum holds, with constant probability\nunder a random initialization scheme. Our results significantly extend previous\nanalyses, e.g., of deep linear residual networks (Bartlett et al., 2018).\n",
        "published": "2018",
        "authors": [
            "Sanjeev Arora",
            "Nadav Cohen",
            "Noah Golowich",
            "Wei Hu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.03198v1",
        "title": "Reinforcement Evolutionary Learning Method for self-learning",
        "abstract": "  In statistical modelling the biggest threat is concept drift which makes the\nmodel gradually showing deteriorating performance over time. There are state of\nthe art methodologies to detect the impact of concept drift, however general\nstrategy considered to overcome the issue in performance is to rebuild or\nre-calibrate the model periodically as the variable patterns for the model\nchanges significantly due to market change or consumer behavior change etc.\nQuantitative research is the most widely spread application of data science in\nMarketing or financial domain where applicability of state of the art\nreinforcement learning for auto-learning is less explored paradigm.\nReinforcement learning is heavily dependent on having a simulated environment\nwhich is majorly available for gaming or online systems, to learn from the live\nfeedback. However, there are some research happened on the area of online\nadvertisement, pricing etc where due to the nature of the online learning\nenvironment scope of reinforcement learning is explored. Our proposed solution\nis a reinforcement learning based, true self-learning algorithm which can adapt\nto the data change or concept drift and auto learn and self-calibrate for the\nnew patterns of the data solving the problem of concept drift.\n  Keywords - Reinforcement learning, Genetic Algorithm, Q-learning,\nClassification modelling, CMA-ES, NES, Multi objective optimization, Concept\ndrift, Population stability index, Incremental learning, F1-measure, Predictive\nModelling, Self-learning, MCTS, AlphaGo, AlphaZero\n",
        "published": "2018",
        "authors": [
            "Kumarjit Pathak",
            "Jitin Kapila"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.04122v1",
        "title": "Deep Convolutional Neural Networks for Noise Detection in ECGs",
        "abstract": "  Mobile electrocardiogram (ECG) recording technologies represent a promising\ntool to fight the ongoing epidemic of cardiovascular diseases, which are\nresponsible for more deaths globally than any other cause. While the ability to\nmonitor one's heart activity at any time in any place is a crucial advantage of\nsuch technologies, it is also the cause of a drawback: signal noise due to\nenvironmental factors can render the ECGs illegible. In this work, we develop\nconvolutional neural networks (CNNs) to automatically label ECGs for noise,\ntraining them on a novel noise-annotated dataset. By reducing distraction from\nnoisy intervals of signals, such networks have the potential to increase the\naccuracy of models for the detection of atrial fibrillation, long QT syndrome,\nand other cardiovascular conditions. Comparing several architectures, we find\nthat a 16-layer CNN adapted from the VGG16 network which generates one\nprediction per second on a 10-second input performs exceptionally well on this\ntask, with an AUC of 0.977.\n",
        "published": "2018",
        "authors": [
            "Jennifer N. John",
            "Conner Galloway",
            "Alexander Valys"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.05526v1",
        "title": "Automatic Configuration of Deep Neural Networks with EGO",
        "abstract": "  Designing the architecture for an artificial neural network is a cumbersome\ntask because of the numerous parameters to configure, including activation\nfunctions, layer types, and hyper-parameters. With the large number of\nparameters for most networks nowadays, it is intractable to find a good\nconfiguration for a given task by hand. In this paper an Efficient Global\nOptimization (EGO) algorithm is adapted to automatically optimize and configure\nconvolutional neural network architectures. A configurable neural network\narchitecture based solely on convolutional layers is proposed for the\noptimization. Without using any knowledge on the target problem and not using\nany data augmentation techniques, it is shown that on several image\nclassification tasks this approach is able to find competitive network\narchitectures in terms of prediction accuracy, compared to the best\nhand-crafted ones in literature. In addition, a very small training budget (200\nevaluations and 10 epochs in training) is spent on each optimized architectures\nin contrast to the usual long training time of hand-crafted networks. Moreover,\ninstead of the standard sequential evaluation in EGO, several candidate\narchitectures are proposed and evaluated in parallel, which saves the execution\noverheads significantly and leads to an efficient automation for deep neural\nnetwork design.\n",
        "published": "2018",
        "authors": [
            "Bas van Stein",
            "Hao Wang",
            "Thomas B\u00e4ck"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.05597v3",
        "title": "Learning Grid Cells as Vector Representation of Self-Position Coupled\n  with Matrix Representation of Self-Motion",
        "abstract": "  This paper proposes a representational model for grid cells. In this model,\nthe 2D self-position of the agent is represented by a high-dimensional vector,\nand the 2D self-motion or displacement of the agent is represented by a matrix\nthat transforms the vector. Each component of the vector is a unit or a cell.\nThe model consists of the following three sub-models. (1) Vector-matrix\nmultiplication. The movement from the current position to the next position is\nmodeled by matrix-vector multiplication, i.e., the vector of the next position\nis obtained by multiplying the matrix of the motion to the vector of the\ncurrent position. (2) Magnified local isometry. The angle between two nearby\nvectors equals the Euclidean distance between the two corresponding positions\nmultiplied by a magnifying factor. (3) Global adjacency kernel. The inner\nproduct between two vectors measures the adjacency between the two\ncorresponding positions, which is defined by a kernel function of the Euclidean\ndistance between the two positions. Our representational model has explicit\nalgebra and geometry. It can learn hexagon patterns of grid cells, and it is\ncapable of error correction, path integral and path planning.\n",
        "published": "2018",
        "authors": [
            "Ruiqi Gao",
            "Jianwen Xie",
            "Song-Chun Zhu",
            "Ying Nian Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.06773v1",
        "title": "Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural\n  Networks",
        "abstract": "  We propose a population-based Evolutionary Stochastic Gradient Descent (ESGD)\nframework for optimizing deep neural networks. ESGD combines SGD and\ngradient-free evolutionary algorithms as complementary algorithms in one\nframework in which the optimization alternates between the SGD step and\nevolution step to improve the average fitness of the population. With a\nback-off strategy in the SGD step and an elitist strategy in the evolution\nstep, it guarantees that the best fitness in the population will never degrade.\nIn addition, individuals in the population optimized with various SGD-based\noptimizers using distinct hyper-parameters in the SGD step are considered as\ncompeting species in a coevolution setting such that the complementarity of the\noptimizers is also taken into account. The effectiveness of ESGD is\ndemonstrated across multiple applications including speech recognition, image\nrecognition and language modeling, using networks with a variety of deep\narchitectures.\n",
        "published": "2018",
        "authors": [
            "Xiaodong Cui",
            "Wei Zhang",
            "Zolt\u00e1n T\u00fcske",
            "Michael Picheny"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.06807v1",
        "title": "Morph: Flexible Acceleration for 3D CNN-based Video Understanding",
        "abstract": "  The past several years have seen both an explosion in the use of\nConvolutional Neural Networks (CNNs) and the design of accelerators to make CNN\ninference practical. In the architecture community, the lion share of effort\nhas targeted CNN inference for image recognition. The closely related problem\nof video recognition has received far less attention as an accelerator target.\nThis is surprising, as video recognition is more computationally intensive than\nimage recognition, and video traffic is predicted to be the majority of\ninternet traffic in the coming years.\n  This paper fills the gap between algorithmic and hardware advances for video\nrecognition by providing a design space exploration and flexible architecture\nfor accelerating 3D Convolutional Neural Networks (3D CNNs) - the core kernel\nin modern video understanding. When compared to (2D) CNNs used for image\nrecognition, efficiently accelerating 3D CNNs poses a significant engineering\nchallenge due to their large (and variable over time) memory footprint and\nhigher dimensionality.\n  To address these challenges, we design a novel accelerator, called Morph,\nthat can adaptively support different spatial and temporal tiling strategies\ndepending on the needs of each layer of each target 3D CNN. We codesign a\nsoftware infrastructure alongside the Morph hardware to find good-fit\nparameters to control the hardware. Evaluated on state-of-the-art 3D CNNs,\nMorph achieves up to 3.4x (2.5x average) reduction in energy consumption and\nimproves performance/watt by up to 5.1x (4x average) compared to a baseline 3D\nCNN accelerator, with an area overhead of 5%. Morph further achieves a 15.9x\naverage energy reduction on 3D CNNs when compared to Eyeriss.\n",
        "published": "2018",
        "authors": [
            "Kartik Hegde",
            "Rohit Agrawal",
            "Yulun Yao",
            "Christopher W. Fletcher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.07291v1",
        "title": "Deep Neural Maps",
        "abstract": "  We introduce a new unsupervised representation learning and visualization\nusing deep convolutional networks and self organizing maps called Deep Neural\nMaps (DNM). DNM jointly learns an embedding of the input data and a mapping\nfrom the embedding space to a two-dimensional lattice. We compare\nvisualizations of DNM with those of t-SNE and LLE on the MNIST and COIL-20 data\nsets. Our experiments show that the DNM can learn efficient representations of\nthe input data, which reflects characteristics of each class. This is shown via\nback-projecting the neurons of the map on the data space.\n",
        "published": "2018",
        "authors": [
            "Mehran Pesteie",
            "Purang Abolmaesumi",
            "Robert Rohling"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.08359v1",
        "title": "Malicious Web Domain Identification using Online Credibility and\n  Performance Data by Considering the Class Imbalance Issue",
        "abstract": "  Purpose: Malicious web domain identification is of significant importance to\nthe security protection of Internet users. With online credibility and\nperformance data, this paper aims to investigate the use of machine learning\ntech-niques for malicious web domain identification by considering the class\nimbalance issue (i.e., there are more benign web domains than malicious ones).\nDesign/methodology/approach: We propose an integrated resampling approach to\nhandle class imbalance by combining the Synthetic Minority Over-sampling\nTEchnique (SMOTE) and Particle Swarm Optimisation (PSO), a population-based\nmeta-heuristic algorithm. We use the SMOTE for over-sampling and PSO for\nunder-sampling. Findings: By applying eight well-known machine learning\nclassifiers, the proposed integrated resampling approach is comprehensively\nexamined using several imbalanced web domain datasets with different imbalance\nratios. Com-pared to five other well-known resampling approaches, experimental\nresults confirm that the proposed approach is highly effective. Practical\nimplications: This study not only inspires the practical use of online\ncredibility and performance data for identifying malicious web domains, but\nalso provides an effective resampling approach for handling the class\nimbal-ance issue in the area of malicious web domain identification.\nOriginality/value: Online credibility and performance data is applied to build\nmalicious web domain identification models using machine learning techniques.\nAn integrated resampling approach is proposed to address the class im-balance\nissue. The performance of the proposed approach is confirmed based on\nreal-world datasets with different imbalance ratios.\n",
        "published": "2018",
        "authors": [
            "Zhongyi Hu",
            "Raymond Chiong",
            "Ilung Pranata",
            "Yukun Bao",
            "Yuqing Lin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.08646v1",
        "title": "SLAYER: Spike Layer Error Reassignment in Time",
        "abstract": "  Configuring deep Spiking Neural Networks (SNNs) is an exciting research\navenue for low power spike event based computation. However, the spike\ngeneration function is non-differentiable and therefore not directly compatible\nwith the standard error backpropagation algorithm. In this paper, we introduce\na new general backpropagation mechanism for learning synaptic weights and\naxonal delays which overcomes the problem of non-differentiability of the spike\nfunction and uses a temporal credit assignment policy for backpropagating error\nto preceding layers. We describe and release a GPU accelerated software\nimplementation of our method which allows training both fully connected and\nconvolutional neural network (CNN) architectures. Using our software, we\ncompare our method against existing SNN based learning approaches and standard\nANN to SNN conversion techniques and show that our method achieves state of the\nart performance for an SNN on the MNIST, NMNIST, DVS Gesture, and TIDIGITS\ndatasets.\n",
        "published": "2018",
        "authors": [
            "Sumit Bam Shrestha",
            "Garrick Orchard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.08651v1",
        "title": "How can deep learning advance computational modeling of sensory\n  information processing?",
        "abstract": "  Deep learning, computational neuroscience, and cognitive science have\noverlapping goals related to understanding intelligence such that perception\nand behaviour can be simulated in computational systems. In neuroimaging,\nmachine learning methods have been used to test computational models of sensory\ninformation processing. Recently, these model comparison techniques have been\nused to evaluate deep neural networks (DNNs) as models of sensory information\nprocessing. However, the interpretation of such model evaluations is muddied by\nimprecise statistical conclusions. Here, we make explicit the types of\nconclusions that can be drawn from these existing model comparison techniques\nand how these conclusions change when the model in question is a DNN. We\ndiscuss how DNNs are amenable to new model comparison techniques that allow for\nstronger conclusions to be made about the computational mechanisms underlying\nsensory information processing.\n",
        "published": "2018",
        "authors": [
            "Jessica A. F. Thompson",
            "Yoshua Bengio",
            "Elia Formisano",
            "Marc Sch\u00f6nwiesner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.11491v2",
        "title": "Empirical Evaluation of Contextual Policy Search with a Comparison-based\n  Surrogate Model and Active Covariance Matrix Adaptation",
        "abstract": "  Contextual policy search (CPS) is a class of multi-task reinforcement\nlearning algorithms that is particularly useful for robotic applications. A\nrecent state-of-the-art method is Contextual Covariance Matrix Adaptation\nEvolution Strategies (C-CMA-ES). It is based on the standard black-box\noptimization algorithm CMA-ES. There are two useful extensions of CMA-ES that\nwe will transfer to C-CMA-ES and evaluate empirically: ACM-ES, which uses a\ncomparison-based surrogate model, and aCMA-ES, which uses an active update of\nthe covariance matrix. We will show that improvements with these methods can be\nimpressive in terms of sample-efficiency, although this is not relevant any\nmore for the robotic domain.\n",
        "published": "2018",
        "authors": [
            "Alexander Fabisch"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.11701v1",
        "title": "Hull Form Optimization with Principal Component Analysis and Deep Neural\n  Network",
        "abstract": "  Designing and modifying complex hull forms for optimal vessel performances\nhave been a major challenge for naval architects. In the present study,\nPrincipal Component Analysis (PCA) is introduced to compress the geometric\nrepresentation of a group of existing vessels, and the resulting principal\nscores are manipulated to generate a large number of derived hull forms, which\nare evaluated computationally for their calm-water performances. The results\nare subsequently used to train a Deep Neural Network (DNN) to accurately\nestablish the relation between different hull forms and their associated\nperformances. Then, based on the fast, parallel DNN-based hull-form evaluation,\nthe large-scale search for optimal hull forms is performed.\n",
        "published": "2018",
        "authors": [
            "Dongchi Yu",
            "Lu Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.11896v1",
        "title": "Smoothed Analysis of Discrete Tensor Decomposition and Assemblies of\n  Neurons",
        "abstract": "  We analyze linear independence of rank one tensors produced by tensor powers\nof randomly perturbed vectors. This enables efficient decomposition of sums of\nhigh-order tensors. Our analysis builds upon [BCMV14] but allows for a wider\nrange of perturbation models, including discrete ones. We give an application\nto recovering assemblies of neurons.\n  Assemblies are large sets of neurons representing specific memories or\nconcepts. The size of the intersection of two assemblies has been shown in\nexperiments to represent the extent to which these memories co-occur or these\nconcepts are related; the phenomenon is called association of assemblies. This\nsuggests that an animal's memory is a complex web of associations, and poses\nthe problem of recovering this representation from cognitive data. Motivated by\nthis problem, we study the following more general question: Can we reconstruct\nthe Venn diagram of a family of sets, given the sizes of their $\\ell$-wise\nintersections? We show that as long as the family of sets is randomly\nperturbed, it is enough for the number of measurements to be polynomially\nlarger than the number of nonempty regions of the Venn diagram to fully\nreconstruct the diagram.\n",
        "published": "2018",
        "authors": [
            "Nima Anari",
            "Constantinos Daskalakis",
            "Wolfgang Maass",
            "Christos H. Papadimitriou",
            "Amin Saberi",
            "Santosh Vempala"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.11914v4",
        "title": "Rademacher Complexity for Adversarially Robust Generalization",
        "abstract": "  Many machine learning models are vulnerable to adversarial attacks; for\nexample, adding adversarial perturbations that are imperceptible to humans can\noften make machine learning models produce wrong predictions with high\nconfidence. Moreover, although we may obtain robust models on the training\ndataset via adversarial training, in some problems the learned models cannot\ngeneralize well to the test data. In this paper, we focus on $\\ell_\\infty$\nattacks, and study the adversarially robust generalization problem through the\nlens of Rademacher complexity. For binary linear classifiers, we prove tight\nbounds for the adversarial Rademacher complexity, and show that the adversarial\nRademacher complexity is never smaller than its natural counterpart, and it has\nan unavoidable dimension dependence, unless the weight vector has bounded\n$\\ell_1$ norm. The results also extend to multi-class linear classifiers. For\n(nonlinear) neural networks, we show that the dimension dependence in the\nadversarial Rademacher complexity also exists. We further consider a surrogate\nadversarial loss for one-hidden layer ReLU network and prove margin bounds for\nthis setting. Our results indicate that having $\\ell_1$ norm constraints on the\nweight matrices might be a potential way to improve generalization in the\nadversarial setting. We demonstrate experimental results that validate our\ntheoretical findings.\n",
        "published": "2018",
        "authors": [
            "Dong Yin",
            "Kannan Ramchandran",
            "Peter Bartlett"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.12411v2",
        "title": "Counting in Language with RNNs",
        "abstract": "  In this paper we examine a possible reason for the LSTM outperforming the GRU\non language modeling and more specifically machine translation. We hypothesize\nthat this has to do with counting. This is a consistent theme across the\nliterature of long term dependence, counting, and language modeling for RNNs.\nUsing the simplified forms of language -- Context-Free and Context-Sensitive\nLanguages -- we show how exactly the LSTM performs its counting based on their\ncell states during inference and why the GRU cannot perform as well.\n",
        "published": "2018",
        "authors": [
            "Heng xin Fun",
            "Sergiy V Bokhnyak",
            "Francesco Saverio Zuppichini"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.00687v4",
        "title": "On the Power and Limitations of Random Features for Understanding Neural\n  Networks",
        "abstract": "  Recently, a spate of papers have provided positive theoretical results for\ntraining over-parameterized neural networks (where the network size is larger\nthan what is needed to achieve low error). The key insight is that with\nsufficient over-parameterization, gradient-based methods will implicitly leave\nsome components of the network relatively unchanged, so the optimization\ndynamics will behave as if those components are essentially fixed at their\ninitial random values. In fact, fixing these explicitly leads to the well-known\napproach of learning with random features. In other words, these techniques\nimply that we can successfully learn with neural networks, whenever we can\nsuccessfully learn with random features. In this paper, we first review these\ntechniques, providing a simple and self-contained analysis for one-hidden-layer\nnetworks. We then argue that despite the impressive positive results, random\nfeature approaches are also inherently limited in what they can explain. In\nparticular, we rigorously show that random features cannot be used to learn\neven a single ReLU neuron with standard Gaussian inputs, unless the network\nsize (or magnitude of the weights) is exponentially large. Since a single\nneuron is learnable with gradient-based methods, we conclude that we are still\nfar from a satisfying general explanation for the empirical success of neural\nnetworks.\n",
        "published": "2019",
        "authors": [
            "Gilad Yehudai",
            "Ohad Shamir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.01070v1",
        "title": "Multimodal Sparse Classifier for Adolescent Brain Age Prediction",
        "abstract": "  The study of healthy brain development helps to better understand the brain\ntransformation and brain connectivity patterns which happen during childhood to\nadulthood. This study presents a sparse machine learning solution across\nwhole-brain functional connectivity (FC) measures of three sets of data,\nderived from resting state functional magnetic resonance imaging (rs-fMRI) and\ntask fMRI data, including a working memory n-back task (nb-fMRI) and an emotion\nidentification task (em-fMRI). These multi-modal image data are collected on a\nsample of adolescents from the Philadelphia Neurodevelopmental Cohort (PNC) for\nthe prediction of brain ages. Due to extremely large variable-to-instance ratio\nof PNC data, a high dimensional matrix with several irrelevant and highly\ncorrelated features is generated and hence a pattern learning approach is\nnecessary to extract significant features. We propose a sparse learner based on\nthe residual errors along the estimation of an inverse problem for the extreme\nlearning machine (ELM) neural network. The purpose of the approach is to\novercome the overlearning problem through pruning of several redundant features\nand their corresponding output weights. The proposed multimodal sparse ELM\nclassifier based on residual errors (RES-ELM) is highly competitive in terms of\nthe classification accuracy compared to its counterparts such as conventional\nELM, and sparse Bayesian learning ELM.\n",
        "published": "2019",
        "authors": [
            "Peyman Hosseinzadeh Kassani",
            "Alexej Gossmann",
            "Yu-Ping Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.03673v3",
        "title": "Every Local Minimum Value is the Global Minimum Value of Induced Model\n  in Non-convex Machine Learning",
        "abstract": "  For nonconvex optimization in machine learning, this article proves that\nevery local minimum achieves the globally optimal value of the perturbable\ngradient basis model at any differentiable point. As a result, nonconvex\nmachine learning is theoretically as supported as convex machine learning with\na handcrafted basis in terms of the loss at differentiable local minima, except\nin the case when a preference is given to the handcrafted basis over the\nperturbable gradient basis. The proofs of these results are derived under mild\nassumptions. Accordingly, the proven results are directly applicable to many\nmachine learning models, including practical deep neural networks, without any\nmodification of practical methods. Furthermore, as special cases of our general\nresults, this article improves or complements several state-of-the-art\ntheoretical results on deep neural networks, deep residual networks, and\noverparameterized deep neural networks with a unified proof technique and novel\ngeometric insights. A special case of our results also contributes to the\ntheoretical foundation of representation learning.\n",
        "published": "2019",
        "authors": [
            "Kenji Kawaguchi",
            "Jiaoyang Huang",
            "Leslie Pack Kaelbling"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.05098v1",
        "title": "Multitask Hopfield Networks",
        "abstract": "  Multitask algorithms typically use task similarity information as a bias to\nspeed up and improve the performance of learning processes. Tasks are learned\njointly, sharing information across them, in order to construct models more\naccurate than those learned separately over single tasks. In this contribution,\nwe present the first multitask model, to our knowledge, based on Hopfield\nNetworks (HNs), named HoMTask. We show that by appropriately building a unique\nHN embedding all tasks, a more robust and effective classification model can be\nlearned. HoMTask is a transductive semi-supervised parametric HN, that\nminimizes an energy function extended to all nodes and to all tasks under\nstudy. We provide theoretical evidence that the optimal parameters\nautomatically estimated by HoMTask make coherent the model itself with the\nprior knowledge (connection weights and node labels). The convergence\nproperties of HNs are preserved, and the fixed point reached by the network\ndynamics gives rise to the prediction of unlabeled nodes. The proposed model\nimproves the classification abilities of singletask HNs on a preliminary\nbenchmark comparison, and achieves competitive performance with\nstate-of-the-art semi-supervised graph-based algorithms.\n",
        "published": "2019",
        "authors": [
            "Marco Frasca",
            "Giuliano Grossi",
            "Giorgio Valentini"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.05760v1",
        "title": "Scalarizing Functions in Bayesian Multiobjective Optimization",
        "abstract": "  Scalarizing functions have been widely used to convert a multiobjective\noptimization problem into a single objective optimization problem. However,\ntheir use in solving (computationally) expensive multi- and many-objective\noptimization problems in Bayesian multiobjective optimization is scarce.\nScalarizing functions can play a crucial role on the quality and number of\nevaluations required when doing the optimization. In this article, we study and\nreview 15 different scalarizing functions in the framework of Bayesian\nmultiobjective optimization and build Gaussian process models (as surrogates,\nmetamodels or emulators) on them. We use expected improvement as infill\ncriterion (or acquisition function) to update the models. In particular, we\ncompare different scalarizing functions and analyze their performance on\nseveral benchmark problems with different number of objectives to be optimized.\nThe review and experiments on different functions provide useful insights when\nusing and selecting a scalarizing function when using a Bayesian multiobjective\noptimization method.\n",
        "published": "2019",
        "authors": [
            "Tinkle Chugh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.05815v1",
        "title": "GP-HD: Using Genetic Programming to Generate Dynamical Systems Models\n  for Health Care",
        "abstract": "  The huge wealth of data in the health domain can be exploited to create\nmodels that predict development of health states over time. Temporal learning\nalgorithms are well suited to learn relationships between health states and\nmake predictions about their future developments. However, these algorithms:\n(1) either focus on learning one generic model for all patients, providing\ngeneral insights but often with limited predictive performance, or (2) learn\nindividualized models from which it is hard to derive generic concepts. In this\npaper, we present a middle ground, namely parameterized dynamical systems\nmodels that are generated from data using a Genetic Programming (GP) framework.\nA fitness function suitable for the health domain is exploited. An evaluation\nof the approach in the mental health domain shows that performance of the model\ngenerated by the GP is on par with a dynamical systems model developed based on\ndomain knowledge, significantly outperforms a generic Long Term Short Term\nMemory (LSTM) model and in some cases also outperforms an individualized LSTM\nmodel.\n",
        "published": "2019",
        "authors": [
            "Mark Hoogendoorn",
            "Ward van Breda",
            "Jeroen Ruwaard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.06960v1",
        "title": "On the Performance of Differential Evolution for Hyperparameter Tuning",
        "abstract": "  Automated hyperparameter tuning aspires to facilitate the application of\nmachine learning for non-experts. In the literature, different optimization\napproaches are applied for that purpose. This paper investigates the\nperformance of Differential Evolution for tuning hyperparameters of supervised\nlearning algorithms for classification tasks. This empirical study involves a\nrange of different machine learning algorithms and datasets with various\ncharacteristics to compare the performance of Differential Evolution with\nSequential Model-based Algorithm Configuration (SMAC), a reference Bayesian\nOptimization approach. The results indicate that Differential Evolution\noutperforms SMAC for most datasets when tuning a given machine learning\nalgorithm - particularly when breaking ties in a first-to-report fashion. Only\nfor the tightest of computational budgets SMAC performs better. On small\ndatasets, Differential Evolution outperforms SMAC by 19% (37% after\ntie-breaking). In a second experiment across a range of representative datasets\ntaken from the literature, Differential Evolution scores 15% (23% after\ntie-breaking) more wins than SMAC.\n",
        "published": "2019",
        "authors": [
            "Mischa Schmidt",
            "Shahd Safarani",
            "Julia Gastinger",
            "Tobias Jacobs",
            "Sebastien Nicolas",
            "Anett Sch\u00fclke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.06991v3",
        "title": "Improved Precision and Recall Metric for Assessing Generative Models",
        "abstract": "  The ability to automatically estimate the quality and coverage of the samples\nproduced by a generative model is a vital requirement for driving algorithm\nresearch. We present an evaluation metric that can separately and reliably\nmeasure both of these aspects in image generation tasks by forming explicit,\nnon-parametric representations of the manifolds of real and generated data. We\ndemonstrate the effectiveness of our metric in StyleGAN and BigGAN by providing\nseveral illustrative examples where existing metrics yield uninformative or\ncontradictory results. Furthermore, we analyze multiple design variants of\nStyleGAN to better understand the relationships between the model architecture,\ntraining methods, and the properties of the resulting sample distribution. In\nthe process, we identify new variants that improve the state-of-the-art. We\nalso perform the first principled analysis of truncation methods and identify\nan improved method. Finally, we extend our metric to estimate the perceptual\nquality of individual samples, and use this to study latent space\ninterpolations.\n",
        "published": "2019",
        "authors": [
            "Tuomas Kynk\u00e4\u00e4nniemi",
            "Tero Karras",
            "Samuli Laine",
            "Jaakko Lehtinen",
            "Timo Aila"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.07964v1",
        "title": "3D Shape Synthesis for Conceptual Design and Optimization Using\n  Variational Autoencoders",
        "abstract": "  We propose a data-driven 3D shape design method that can learn a generative\nmodel from a corpus of existing designs, and use this model to produce a wide\nrange of new designs. The approach learns an encoding of the samples in the\ntraining corpus using an unsupervised variational autoencoder-decoder\narchitecture, without the need for an explicit parametric representation of the\noriginal designs. To facilitate the generation of smooth final surfaces, we\ndevelop a 3D shape representation based on a distance transformation of the\noriginal 3D data, rather than using the commonly utilized binary voxel\nrepresentation. Once established, the generator maps the latent space\nrepresentations to the high-dimensional distance transformation fields, which\nare then automatically surfaced to produce 3D representations amenable to\nphysics simulations or other objective function evaluation modules. We\ndemonstrate our approach for the computational design of gliders that are\noptimized to attain prescribed performance scores. Our results show that when\ncombined with genetic optimization, the proposed approach can generate a rich\nset of candidate concept designs that achieve prescribed functional goals, even\nwhen the original dataset has only a few or no solutions that achieve these\ngoals.\n",
        "published": "2019",
        "authors": [
            "Wentai Zhang",
            "Zhangsihao Yang",
            "Haoliang Jiang",
            "Suyash Nigam",
            "Soji Yamakawa",
            "Tomotake Furuhata",
            "Kenji Shimada",
            "Levent Burak Kara"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.08050v1",
        "title": "Sparseout: Controlling Sparsity in Deep Networks",
        "abstract": "  Dropout is commonly used to help reduce overfitting in deep neural networks.\nSparsity is a potentially important property of neural networks, but is not\nexplicitly controlled by Dropout-based regularization. In this work, we propose\nSparseout a simple and efficient variant of Dropout that can be used to control\nthe sparsity of the activations in a neural network. We theoretically prove\nthat Sparseout is equivalent to an $L_q$ penalty on the features of a\ngeneralized linear model and that Dropout is a special case of Sparseout for\nneural networks. We empirically demonstrate that Sparseout is computationally\ninexpensive and is able to control the desired level of sparsity in the\nactivations. We evaluated Sparseout on image classification and language\nmodelling tasks to see the effect of sparsity on these tasks. We found that\nsparsity of the activations is favorable for language modelling performance\nwhile image classification benefits from denser activations. Sparseout provides\na way to investigate sparsity in state-of-the-art deep learning models. Source\ncode for Sparseout could be found at\n\\url{https://github.com/najeebkhan/sparseout}.\n",
        "published": "2019",
        "authors": [
            "Najeeb Khan",
            "Ian Stavness"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.08378v1",
        "title": "Dynamic Evaluation of Transformer Language Models",
        "abstract": "  This research note combines two methods that have recently improved the state\nof the art in language modeling: Transformers and dynamic evaluation.\nTransformers use stacked layers of self-attention that allow them to capture\nlong range dependencies in sequential data. Dynamic evaluation fits models to\nthe recent sequence history, allowing them to assign higher probabilities to\nre-occurring sequential patterns. By applying dynamic evaluation to\nTransformer-XL models, we improve the state of the art on enwik8 from 0.99 to\n0.94 bits/char, text8 from 1.08 to 1.04 bits/char, and WikiText-103 from 18.3\nto 16.4 perplexity points.\n",
        "published": "2019",
        "authors": [
            "Ben Krause",
            "Emmanuel Kahembwe",
            "Iain Murray",
            "Steve Renals"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.09098v1",
        "title": "Optimal initialization of K-means using Particle Swarm Optimization",
        "abstract": "  This paper proposes the use of an optimization algorithm, namely PSO to\ndecide the initial centroids in K-means, to eventually get better accuracy. The\nvectorized notation of the optimal centroids can be thought of as entities in\nan optimization space, where the accuracy of K-means over a random subset of\nthe data could act as a fitness measure. The resultant optimal vector can be\nused as the initial centroids for K-means.\n",
        "published": "2019",
        "authors": [
            "Ashutosh Mahesh Pednekar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.10400v1",
        "title": "Efficient single input-output layer spiking neural classifier with\n  time-varying weight model",
        "abstract": "  This paper presents a supervised learning algorithm, namely, the Synaptic\nEfficacy Function with Meta-neuron based learning algorithm (SEF-M) for a\nspiking neural network with a time-varying weight model. For a given pattern,\nSEF-M uses the learning algorithm derived from meta-neuron based learning\nalgorithm to determine the change in weights corresponding to each presynaptic\nspike times. The changes in weights modulate the amplitude of a Gaussian\nfunction centred at the same presynaptic spike times. The sum of amplitude\nmodulated Gaussian functions represents the synaptic efficacy functions (or\ntime-varying weight models). The performance of SEF-M is evaluated against\nstate-of-the-art spiking neural network learning algorithms on 10 benchmark\ndatasets from UCI machine learning repository. Performance studies show\nsuperior generalization ability of SEF-M. An ablation study on time-varying\nweight model is conducted using JAFFE dataset. The results of the ablation\nstudy indicate that using a time-varying weight model instead of single weight\nmodel improves the classification accuracy by 14%. Thus, it can be inferred\nthat a single input-output layer spiking neural network with time-varying\nweight model is computationally more efficient than a multi-layer spiking\nneural network with long-term or short-term weight model.\n",
        "published": "2019",
        "authors": [
            "Abeegithan Jeyasothy",
            "Savitha Ramasamy",
            "Suresh Sundaram"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.10931v3",
        "title": "Prediction of Progression to Alzheimer's disease with Deep InfoMax",
        "abstract": "  Arguably, unsupervised learning plays a crucial role in the majority of\nalgorithms for processing brain imaging. A recently introduced unsupervised\napproach Deep InfoMax (DIM) is a promising tool for exploring brain structure\nin a flexible non-linear way. In this paper, we investigate the use of variants\nof DIM in a setting of progression to Alzheimer's disease in comparison with\nsupervised AlexNet and ResNet inspired convolutional neural networks. As a\nbenchmark, we use a classification task between four groups: patients with\nstable, and progressive mild cognitive impairment (MCI), with Alzheimer's\ndisease, and healthy controls. Our dataset is comprised of 828 subjects from\nthe Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Our\nexperiments highlight encouraging evidence of the high potential utility of DIM\nin future neuroimaging studies.\n",
        "published": "2019",
        "authors": [
            "Alex Fedorov",
            "R Devon Hjelm",
            "Anees Abrol",
            "Zening Fu",
            "Yuhui Du",
            "Sergey Plis",
            "Vince D. Calhoun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.11367v1",
        "title": "A novel method for extracting interpretable knowledge from a spiking\n  neural classifier with time-varying synaptic weights",
        "abstract": "  This paper presents a novel method for information interpretability in an\nMC-SEFRON classifier. To develop a method to extract knowledge stored in a\ntrained classifier, first, the binary-class SEFRON classifier developed earlier\nis extended to handle multi-class problems. MC-SEFRON uses the population\nencoding scheme to encode the real-valued input data into spike patterns.\nMC-SEFRON is trained using the same supervised learning rule used in the\nSEFRON. After training, the proposed method extracts the knowledge for a given\nclass stored in the classifier by mapping the weighted postsynaptic potential\nin the time domain to the feature domain as Feature Strength Functions (FSFs).\nA set of FSFs corresponding to each output class represents the extracted\nknowledge from the classifier. This knowledge encoding method is derived to\nmaintain consistency between the classification in the time domain and the\nfeature domain. The correctness of the FSF is quantitatively measured by using\nFSF directly for classification tasks. For a given input, each FSF is sampled\nat the input value to obtain the corresponding feature strength value (FSV).\nThen the aggregated FSVs obtained for each class are used to determine the\noutput class labels during classification. FSVs are also used to interpret the\npredictions during the classification task. Using ten UCI datasets and the\nMNIST dataset, the knowledge extraction method, interpretation and the\nreliability of the FSF are demonstrated. Based on the studies, it can be seen\nthat on an average, the difference in the classification accuracies using the\nFSF directly and those obtained by MC-SEFRON is only around 0.9% & 0.1\\% for\nthe UCI datasets and the MNIST dataset respectively. This clearly shows that\nthe knowledge represented by the FSFs has acceptable reliability and the\ninterpretability of classification using the classifier's knowledge has been\njustified.\n",
        "published": "2019",
        "authors": [
            "Abeegithan Jeyasothy",
            "Suresh Sundaram",
            "Savitha Ramasamy",
            "Narasimhan Sundararajan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.11829v3",
        "title": "Evaluating Recurrent Neural Network Explanations",
        "abstract": "  Recently, several methods have been proposed to explain the predictions of\nrecurrent neural networks (RNNs), in particular of LSTMs. The goal of these\nmethods is to understand the network's decisions by assigning to each input\nvariable, e.g., a word, a relevance indicating to which extent it contributed\nto a particular prediction. In previous works, some of these methods were not\nyet compared to one another, or were evaluated only qualitatively. We close\nthis gap by systematically and quantitatively comparing these methods in\ndifferent settings, namely (1) a toy arithmetic task which we use as a sanity\ncheck, (2) a five-class sentiment prediction of movie reviews, and besides (3)\nwe explore the usefulness of word relevances to build sentence-level\nrepresentations. Lastly, using the method that performed best in our\nexperiments, we show how specific linguistic phenomena such as the negation in\nsentiment analysis reflect in terms of relevance patterns, and how the\nrelevance visualization can help to understand the misclassification of\nindividual samples.\n",
        "published": "2019",
        "authors": [
            "Leila Arras",
            "Ahmed Osman",
            "Klaus-Robert M\u00fcller",
            "Wojciech Samek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.12774v1",
        "title": "Routing Networks and the Challenges of Modular and Compositional\n  Computation",
        "abstract": "  Compositionality is a key strategy for addressing combinatorial complexity\nand the curse of dimensionality. Recent work has shown that compositional\nsolutions can be learned and offer substantial gains across a variety of\ndomains, including multi-task learning, language modeling, visual question\nanswering, machine comprehension, and others. However, such models present\nunique challenges during training when both the module parameters and their\ncomposition must be learned jointly. In this paper, we identify several of\nthese issues and analyze their underlying causes. Our discussion focuses on\nrouting networks, a general approach to this problem, and examines empirically\nthe interplay of these challenges and a variety of design decisions. In\nparticular, we consider the effect of how the algorithm decides on module\ncomposition, how the algorithm updates the modules, and if the algorithm uses\nregularization.\n",
        "published": "2019",
        "authors": [
            "Clemens Rosenbaum",
            "Ignacio Cases",
            "Matthew Riemer",
            "Tim Klinger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.12904v1",
        "title": "Neuromorphic Acceleration for Approximate Bayesian Inference on Neural\n  Networks via Permanent Dropout",
        "abstract": "  As neural networks have begun performing increasingly critical tasks for\nsociety, ranging from driving cars to identifying candidates for drug\ndevelopment, the value of their ability to perform uncertainty quantification\n(UQ) in their predictions has risen commensurately. Permanent dropout, a\npopular method for neural network UQ, involves injecting stochasticity into the\ninference phase of the model and creating many predictions for each of the test\ndata. This shifts the computational and energy burden of deep neural networks\nfrom the training phase to the inference phase. Recent work has demonstrated\nnear-lossless conversion of classical deep neural networks to their spiking\ncounterparts. We use these results to demonstrate the feasibility of conducting\nthe inference phase with permanent dropout on spiking neural networks,\nmitigating the technique's computational and energy burden, which is essential\nfor its use at scale or on edge platforms. We demonstrate the proposed approach\nvia the Nengo spiking neural simulator on a combination drug therapy dataset\nfor cancer treatment, where UQ is critical. Our results indicate that the\nspiking approximation gives a predictive distribution practically\nindistinguishable from that given by the classical network.\n",
        "published": "2019",
        "authors": [
            "Nathan Wycoff",
            "Prasanna Balaprakash",
            "Fangfang Xia"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.02438v2",
        "title": "Rethinking Arithmetic for Deep Neural Networks",
        "abstract": "  We consider efficiency in the implementation of deep neural networks.\nHardware accelerators are gaining interest as machine learning becomes one of\nthe drivers of high-performance computing. In these accelerators, the directed\ngraph describing a neural network can be implemented as a directed graph\ndescribing a Boolean circuit. We make this observation precise, leading\nnaturally to an understanding of practical neural networks as discrete\nfunctions, and show that so-called binarised neural networks are functionally\ncomplete. In general, our results suggest that it is valuable to consider\nBoolean circuits as neural networks, leading to the question of which circuit\ntopologies are promising. We argue that continuity is central to generalisation\nin learning, explore the interaction between data coding, network topology, and\nnode functionality for continuity, and pose some open questions for future\nresearch. As a first step to bridging the gap between continuous and Boolean\nviews of neural network accelerators, we present some recent results from our\nwork on LUTNet, a novel Field-Programmable Gate Array inference approach.\nFinally, we conclude with additional possible fruitful avenues for research\nbridging the continuous and discrete views of neural networks.\n",
        "published": "2019",
        "authors": [
            "George A. Constantinides"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.04101v2",
        "title": "Biologically plausible deep learning -- but how far can we go with\n  shallow networks?",
        "abstract": "  Training deep neural networks with the error backpropagation algorithm is\nconsidered implausible from a biological perspective. Numerous recent\npublications suggest elaborate models for biologically plausible variants of\ndeep learning, typically defining success as reaching around 98% test accuracy\non the MNIST data set. Here, we investigate how far we can go on digit (MNIST)\nand object (CIFAR10) classification with biologically plausible, local learning\nrules in a network with one hidden layer and a single readout layer. The hidden\nlayer weights are either fixed (random or random Gabor filters) or trained with\nunsupervised methods (PCA, ICA or Sparse Coding) that can be implemented by\nlocal learning rules. The readout layer is trained with a supervised, local\nlearning rule. We first implement these models with rate neurons. This\ncomparison reveals, first, that unsupervised learning does not lead to better\nperformance than fixed random projections or Gabor filters for large hidden\nlayers. Second, networks with localized receptive fields perform significantly\nbetter than networks with all-to-all connectivity and can reach backpropagation\nperformance on MNIST. We then implement two of the networks - fixed, localized,\nrandom & random Gabor filters in the hidden layer - with spiking leaky\nintegrate-and-fire neurons and spike timing dependent plasticity to train the\nreadout layer. These spiking models achieve > 98.2% test accuracy on MNIST,\nwhich is close to the performance of rate networks with one hidden layer\ntrained with backpropagation. The performance of our shallow network models is\ncomparable to most current biologically plausible models of deep learning.\nFurthermore, our results with a shallow spiking network provide an important\nreference and suggest the use of datasets other than MNIST for testing the\nperformance of future models of biologically plausible deep learning.\n",
        "published": "2019",
        "authors": [
            "Bernd Illing",
            "Wulfram Gerstner",
            "Johanni Brea"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.04522v2",
        "title": "Accuracy Improvement of Neural Network Training using Particle Swarm\n  Optimization and its Stability Analysis for Classification",
        "abstract": "  Supervised classification is the most active and emerging research trends in\ntoday's scenario. In this view, Artificial Neural Network (ANN) techniques have\nbeen widely employed and growing interest to the researchers day by day. ANN\ntraining aims to find the proper setting of parameters such as weights\n($\\textbf{W}$) and biases ($b$) to properly classify the given data samples.\nThe training process is formulated in an error minimization problem which\nconsists of many local optima in the search landscape. In this paper, an\nenhanced Particle Swarm Optimization is proposed to minimize the error function\nfor classifying real-life data sets. A stability analysis is performed to\nestablish the efficiency of the proposed method for improving classification\naccuracy. The performance measurement such as confusion matrix, $F$-measure and\nconvergence graph indicates the significant improvement in the classification\naccuracy.\n",
        "published": "2019",
        "authors": [
            "Arijit Nandi",
            "Nanda Dulal Jana"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.04545v2",
        "title": "Deep Learning: a new definition of artificial neuron with double weight",
        "abstract": "  Deep learning is a subset of a broader family of machine learning methods\nbased on learning data representations. These models are inspired by human\nbiological nervous systems, even if there are various differences pertaining to\nthe structural and functional properties of biological brains. The elementary\nconstituents of deep learning models are neurons, which can be considered as\nfunctions that receive inputs and produce an output that is a weighted sum of\nthe inputs fed through an activation function. Several models of neurons were\nproposed in the course of the years that are all based on learnable parameters\ncalled weights. In this paper we present a new type of artificial neuron, the\ndouble-weight neuron,characterized by additional learnable weights that lead to\na more complex and accurate system. We tested a feed-forward and convolutional\nneural network consisting of double-weight neurons on the MNIST dataset, and we\ntested a convolution network on the CIFAR-10 dataset. For MNIST we find a\n$\\approx 4\\%$ and $\\approx 1\\%$ improved classification accuracy, respectively,\nwhen compared to a standard feed-forward and convolutional neural network built\nwith the same sets of hyperparameters. For CIFAR-10 we find a $\\approx 12\\%$\nimproved classification accuracy. We thus conclude that this novel artificial\nneuron can be considered as a valuable alternative to common ones.\n",
        "published": "2019",
        "authors": [
            "Adriano Baldeschi",
            "Raffaella Margutti",
            "Adam Miller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.05918v1",
        "title": "A Neural Network-Evolutionary Computational Framework for Remaining\n  Useful Life Estimation of Mechanical Systems",
        "abstract": "  This paper presents a framework for estimating the remaining useful life\n(RUL) of mechanical systems. The framework consists of a multi-layer perceptron\nand an evolutionary algorithm for optimizing the data-related parameters. The\nframework makes use of a strided time window to estimate the RUL for mechanical\ncomponents. Tuning the data-related parameters can become a very time consuming\ntask. The framework presented here automatically reshapes the data such that\nthe efficiency of the model is increased. Furthermore, the complexity of the\nmodel is kept low, e.g. neural networks with few hidden layers and few neurons\nat each layer. Having simple models has several advantages like short training\ntimes and the capacity of being in environments with limited computational\nresources such as embedded systems. The proposed method is evaluated on the\npublicly available C-MAPSS dataset, its accuracy is compared against other\nstate-of-the art methods for the same dataset.\n",
        "published": "2019",
        "authors": [
            "David Laredo",
            "Zhaoyin Chen",
            "Oliver Sch\u00fctze",
            "Jian-Qiao Sun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.07350v1",
        "title": "DeepSwarm: Optimising Convolutional Neural Networks using Swarm\n  Intelligence",
        "abstract": "  In this paper we propose DeepSwarm, a novel neural architecture search (NAS)\nmethod based on Swarm Intelligence principles. At its core DeepSwarm uses Ant\nColony Optimization (ACO) to generate ant population which uses the pheromone\ninformation to collectively search for the best neural architecture.\nFurthermore, by using local and global pheromone update rules our method\nensures the balance between exploitation and exploration. On top of this, to\nmake our method more efficient we combine progressive neural architecture\nsearch with weight reusability. Furthermore, due to the nature of ACO our\nmethod can incorporate heuristic information which can further speed up the\nsearch process. After systematic and extensive evaluation, we discover that on\nthree different datasets (MNIST, Fashion-MNIST, and CIFAR-10) when compared to\nexisting systems our proposed method demonstrates competitive performance.\nFinally, we open source DeepSwarm as a NAS library and hope it can be used by\nmore deep learning researchers and practitioners.\n",
        "published": "2019",
        "authors": [
            "Edvinas Byla",
            "Wei Pang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.07685v1",
        "title": "Learning Compact Neural Networks Using Ordinary Differential Equations\n  as Activation Functions",
        "abstract": "  Most deep neural networks use simple, fixed activation functions, such as\nsigmoids or rectified linear units, regardless of domain or network structure.\nWe introduce differential equation units (DEUs), an improvement to modern\nneural networks, which enables each neuron to learn a particular nonlinear\nactivation function from a family of solutions to an ordinary differential\nequation. Specifically, each neuron may change its functional form during\ntraining based on the behavior of the other parts of the network. We show that\nusing neurons with DEU activation functions results in a more compact network\ncapable of achieving comparable, if not superior, performance when is compared\nto much larger networks.\n",
        "published": "2019",
        "authors": [
            "MohamadAli Torkamani",
            "Phillip Wallis",
            "Shiv Shankar",
            "Amirmohammad Rooshenas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.07785v2",
        "title": "Sparse Transfer Learning via Winning Lottery Tickets",
        "abstract": "  The recently proposed Lottery Ticket Hypothesis of Frankle and Carbin (2019)\nsuggests that the performance of over-parameterized deep networks is due to the\nrandom initialization seeding the network with a small fraction of favorable\nweights. These weights retain their dominant status throughout training -- in a\nvery real sense, this sub-network \"won the lottery\" during initialization. The\nauthors find sub-networks via unstructured magnitude pruning with 85-95% of\nparameters removed that train to the same accuracy as the original network at a\nsimilar speed, which they call winning tickets. In this paper, we extend the\nLottery Ticket Hypothesis to a variety of transfer learning tasks. We show that\nsparse sub-networks with approximately 90-95% of weights removed achieve (and\noften exceed) the accuracy of the original dense network in several realistic\nsettings. We experimentally validate this by transferring the sparse\nrepresentation found via pruning on CIFAR-10 to SmallNORB and FashionMNIST for\nobject recognition tasks.\n",
        "published": "2019",
        "authors": [
            "Rahul Mehta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.08537v1",
        "title": "Adaptive Stochastic Natural Gradient Method for One-Shot Neural\n  Architecture Search",
        "abstract": "  High sensitivity of neural architecture search (NAS) methods against their\ninput such as step-size (i.e., learning rate) and search space prevents\npractitioners from applying them out-of-the-box to their own problems, albeit\nits purpose is to automate a part of tuning process. Aiming at a fast, robust,\nand widely-applicable NAS, we develop a generic optimization framework for NAS.\nWe turn a coupled optimization of connection weights and neural architecture\ninto a differentiable optimization by means of stochastic relaxation. It\naccepts arbitrary search space (widely-applicable) and enables to employ a\ngradient-based simultaneous optimization of weights and architecture (fast). We\npropose a stochastic natural gradient method with an adaptive step-size\nmechanism built upon our theoretical investigation (robust). Despite its\nsimplicity and no problem-dependent parameter tuning, our method exhibited near\nstate-of-the-art performances with low computational budgets both on image\nclassification and inpainting tasks.\n",
        "published": "2019",
        "authors": [
            "Youhei Akimoto",
            "Shinichi Shirakawa",
            "Nozomu Yoshinari",
            "Kento Uchida",
            "Shota Saito",
            "Kouhei Nishida"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.09492v2",
        "title": "Combine PPO with NES to Improve Exploration",
        "abstract": "  We introduce two approaches for combining neural evolution strategy (NES) and\nproximal policy optimization (PPO): parameter transfer and parameter space\nnoise. Parameter transfer is a PPO agent with parameters transferred from a NES\nagent. Parameter space noise is to directly add noise to the PPO agent`s\nparameters. We demonstrate that PPO could benefit from both methods through\nexperimental comparison on discrete action environments as well as continuous\ncontrol tasks\n",
        "published": "2019",
        "authors": [
            "Lianjiang Li",
            "Yunrong Yang",
            "Bingna Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.09950v4",
        "title": "Zero-shot task adaptation by homoiconic meta-mapping",
        "abstract": "  How can deep learning systems flexibly reuse their knowledge? Toward this\ngoal, we propose a new class of challenges, and a class of architectures that\ncan solve them. The challenges are meta-mappings, which involve systematically\ntransforming task behaviors to adapt to new tasks zero-shot. The key to\nachieving these challenges is representing the task being performed in such a\nway that this task representation is itself transformable. We therefore draw\ninspiration from functional programming and recent work in meta-learning to\npropose a class of Homoiconic Meta-Mapping (HoMM) approaches that represent\ndata points and tasks in a shared latent space, and learn to infer\ntransformations of that space. HoMM approaches can be applied to any type of\nmachine learning task. We demonstrate the utility of this perspective by\nexhibiting zero-shot remapping of behavior to adapt to new tasks.\n",
        "published": "2019",
        "authors": [
            "Andrew K. Lampinen",
            "James L. McClelland"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.10094v1",
        "title": "Deep Model Predictive Control with Online Learning for Complex Physical\n  Systems",
        "abstract": "  The control of complex systems is of critical importance in many branches of\nscience, engineering, and industry. Controlling an unsteady fluid flow is\nparticularly important, as flow control is a key enabler for technologies in\nenergy (e.g., wind, tidal, and combustion), transportation (e.g., planes,\ntrains, and automobiles), security (e.g., tracking airborne contamination), and\nhealth (e.g., artificial hearts and artificial respiration). However, the\nhigh-dimensional, nonlinear, and multi-scale dynamics make real-time feedback\ncontrol infeasible. Fortunately, these high-dimensional systems exhibit\ndominant, low-dimensional patterns of activity that can be exploited for\neffective control in the sense that knowledge of the entire state of a system\nis not required. Advances in machine learning have the potential to\nrevolutionize flow control given its ability to extract principled, low-rank\nfeature spaces characterizing such complex systems. We present a novel deep\nlearning model predictive control (DeepMPC) framework that exploits low-rank\nfeatures of the flow in order to achieve considerable improvements to control\nperformance. Instead of predicting the entire fluid state, we use a recurrent\nneural network (RNN) to accurately predict the control relevant quantities of\nthe system. The RNN is then embedded into a MPC framework to construct a\nfeedback loop, and incoming sensor data is used to perform online updates to\nimprove prediction accuracy. The results are validated using varying fluid flow\nexamples of increasing complexity.\n",
        "published": "2019",
        "authors": [
            "Katharina Bieker",
            "Sebastian Peitz",
            "Steven L. Brunton",
            "J. Nathan Kutz",
            "Michael Dellnitz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.10268v2",
        "title": "Loss Surface Modality of Feed-Forward Neural Network Architectures",
        "abstract": "  It has been argued in the past that high-dimensional neural networks do not\nexhibit local minima capable of trapping an optimisation algorithm. However,\nthe relationship between loss surface modality and the neural architecture\nparameters, such as the number of hidden neurons per layer and the number of\nhidden layers, remains poorly understood. This study employs fitness landscape\nanalysis to study the modality of neural network loss surfaces under various\nfeed-forward architecture settings. An increase in the problem dimensionality\nis shown to yield a more searchable and more exploitable loss surface. An\nincrease in the hidden layer width is shown to effectively reduce the number of\nlocal minima, and simplify the shape of the global attractor. An increase in\nthe architecture depth is shown to sharpen the global attractor, thus making it\nmore exploitable.\n",
        "published": "2019",
        "authors": [
            "Anna Sergeevna Bosman",
            "Andries Engelbrecht",
            "Mard\u00e9 Helbig"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.10540v1",
        "title": "Dynamic Cell Structure via Recursive-Recurrent Neural Networks",
        "abstract": "  In a recurrent setting, conventional approaches to neural architecture search\nfind and fix a general model for all data samples and time steps. We propose a\nnovel algorithm that can dynamically search for the structure of cells in a\nrecurrent neural network model. Based on a combination of recurrent and\nrecursive neural networks, our algorithm is able to construct customized cell\nstructures for each data sample and time step, allowing for a more efficient\narchitecture search than existing models. Experiments on three common datasets\nshow that the algorithm discovers high-performance cell architectures and\nachieves better prediction accuracy compared to the GRU structure for language\nmodelling and sentiment analysis.\n",
        "published": "2019",
        "authors": [
            "Xin Qian",
            "Matthew Kennedy",
            "Diego Klabjan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.10585v1",
        "title": "Hebbian-Descent",
        "abstract": "  In this work we propose Hebbian-descent as a biologically plausible learning\nrule for hetero-associative as well as auto-associative learning in single\nlayer artificial neural networks. It can be used as a replacement for gradient\ndescent as well as Hebbian learning, in particular in online learning, as it\ninherits their advantages while not suffering from their disadvantages. We\ndiscuss the drawbacks of Hebbian learning as having problems with correlated\ninput data and not profiting from seeing training patterns several times. For\ngradient descent we identify the derivative of the activation function as\nproblematic especially in online learning. Hebbian-descent addresses these\nproblems by getting rid of the activation function's derivative and by\ncentering, i.e. keeping the neural activities mean free, leading to a\nbiologically plausible update rule that is provably convergent, does not suffer\nfrom the vanishing error term problem, can deal with correlated data, profits\nfrom seeing patterns several times, and enables successful online learning when\ncentering is used. We discuss its relationship to Hebbian learning, contrastive\nlearning, and gradient decent and show that in case of a strictly positive\nderivative of the activation function Hebbian-descent leads to the same update\nrule as gradient descent but for a different loss function. In this case\nHebbian-descent inherits the convergence properties of gradient descent, but we\nalso show empirically that it converges when the derivative of the activation\nfunction is only non-negative, such as for the step function for example.\nFurthermore, in case of the mean squared error loss Hebbian-descent can be\nunderstood as the difference between two Hebb-learning steps, which in case of\nan invertible and integrable activation function actually optimizes a\ngeneralized linear model. ...\n",
        "published": "2019",
        "authors": [
            "Jan Melchior",
            "Laurenz Wiskott"
        ]
    }
]