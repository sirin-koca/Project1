[
    {
        "id": "http://arxiv.org/abs/2002.08032v1",
        "title": "A Fixed point view: A Model-Based Clustering Framework",
        "abstract": "  With the inflation of the data, clustering analysis, as a branch of\nunsupervised learning, lacks unified understanding and application of its\nmathematical law. Based on the view of fixed point, this paper restates the\nmodel-based clustering and proposes a unified clustering framework. In order to\nfind fixed points as cluster centers, the framework iteratively constructs the\ncontraction map, which strongly reveals the convergence mechanism and\ninterconnections among algorithms. By specifying a contraction map, Gaussian\nmixture model (GMM) can be mapped to the framework as an application. We hope\nthe fixed point framework will help the design of future clustering algorithms.\n",
        "published": "2020",
        "authors": [
            "Jianhao Ding",
            "Lansheng Han"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.08071v4",
        "title": "Dissecting Neural ODEs",
        "abstract": "  Continuous deep learning architectures have recently re-emerged as Neural\nOrdinary Differential Equations (Neural ODEs). This infinite-depth approach\ntheoretically bridges the gap between deep learning and dynamical systems,\noffering a novel perspective. However, deciphering the inner working of these\nmodels is still an open challenge, as most applications apply them as generic\nblack-box modules. In this work we \"open the box\", further developing the\ncontinuous-depth formulation with the aim of clarifying the influence of\nseveral design choices on the underlying dynamics.\n",
        "published": "2020",
        "authors": [
            "Stefano Massaroli",
            "Michael Poli",
            "Jinkyoo Park",
            "Atsushi Yamashita",
            "Hajime Asama"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.08645v1",
        "title": "Uncovering Coresets for Classification With Multi-Objective Evolutionary\n  Algorithms",
        "abstract": "  A coreset is a subset of the training set, using which a machine learning\nalgorithm obtains performances similar to what it would deliver if trained over\nthe whole original data. Coreset discovery is an active and open line of\nresearch as it allows improving training speed for the algorithms and may help\nhuman understanding the results. Building on previous works, a novel approach\nis presented: candidate corsets are iteratively optimized, adding and removing\nsamples. As there is an obvious trade-off between limiting training size and\nquality of the results, a multi-objective evolutionary algorithm is used to\nminimize simultaneously the number of points in the set and the classification\nerror. Experimental results on non-trivial benchmarks show that the proposed\napproach is able to deliver results that allow a classifier to obtain lower\nerror and better ability of generalizing on unseen data than state-of-the-art\ncoreset discovery techniques.\n",
        "published": "2020",
        "authors": [
            "Pietro Barbiero",
            "Giovanni Squillero",
            "Alberto Tonda"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.08856v4",
        "title": "Bounding the expected run-time of nonconvex optimization with early\n  stopping",
        "abstract": "  This work examines the convergence of stochastic gradient-based optimization\nalgorithms that use early stopping based on a validation function. The form of\nearly stopping we consider is that optimization terminates when the norm of the\ngradient of a validation function falls below a threshold. We derive conditions\nthat guarantee this stopping rule is well-defined, and provide bounds on the\nexpected number of iterations and gradient evaluations needed to meet this\ncriterion. The guarantee accounts for the distance between the training and\nvalidation sets, measured with the Wasserstein distance. We develop the\napproach in the general setting of a first-order optimization algorithm, with\npossibly biased update directions subject to a geometric drift condition. We\nthen derive bounds on the expected running time for early stopping variants of\nseveral algorithms, including stochastic gradient descent (SGD), decentralized\nSGD (DSGD), and the stochastic variance reduced gradient (SVRG) algorithm.\nFinally, we consider the generalization properties of the iterate returned by\nearly stopping.\n",
        "published": "2020",
        "authors": [
            "Thomas Flynn",
            "Kwang Min Yu",
            "Abid Malik",
            "Nicolas D'Imperio",
            "Shinjae Yoo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.09398v2",
        "title": "It's Not What Machines Can Learn, It's What We Cannot Teach",
        "abstract": "  Can deep neural networks learn to solve any task, and in particular problems\nof high complexity? This question attracts a lot of interest, with recent works\ntackling computationally hard tasks such as the traveling salesman problem and\nsatisfiability. In this work we offer a different perspective on this question.\nGiven the common assumption that $\\textit{NP} \\neq \\textit{coNP}$ we prove that\nany polynomial-time sample generator for an $\\textit{NP}$-hard problem samples,\nin fact, from an easier sub-problem. We empirically explore a case study,\nConjunctive Query Containment, and show how common data generation techniques\ngenerate biased datasets that lead practitioners to over-estimate model\naccuracy. Our results suggest that machine learning approaches that require\ntraining on a dense uniform sampling from the target distribution cannot be\nused to solve computationally hard problems, the reason being the difficulty of\ngenerating sufficiently large and unbiased training sets.\n",
        "published": "2020",
        "authors": [
            "Gal Yehuda",
            "Moshe Gabel",
            "Assaf Schuster"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.09518v2",
        "title": "Memory-Based Graph Networks",
        "abstract": "  Graph neural networks (GNNs) are a class of deep models that operate on data\nwith arbitrary topology represented as graphs. We introduce an efficient memory\nlayer for GNNs that can jointly learn node representations and coarsen the\ngraph. We also introduce two new networks based on this layer: memory-based GNN\n(MemGNN) and graph memory network (GMN) that can learn hierarchical graph\nrepresentations. The experimental results shows that the proposed models\nachieve state-of-the-art results in eight out of nine graph classification and\nregression benchmarks. We also show that the learned representations could\ncorrespond to chemical features in the molecule data. Code and reference\nimplementations are released at: https://github.com/amirkhas/GraphMemoryNet\n",
        "published": "2020",
        "authors": [
            "Amir Hosein Khasahmadi",
            "Kaveh Hassani",
            "Parsa Moradi",
            "Leo Lee",
            "Quaid Morris"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.10365v1",
        "title": "The Early Phase of Neural Network Training",
        "abstract": "  Recent studies have shown that many important aspects of neural network\nlearning take place within the very earliest iterations or epochs of training.\nFor example, sparse, trainable sub-networks emerge (Frankle et al., 2019),\ngradient descent moves into a small subspace (Gur-Ari et al., 2018), and the\nnetwork undergoes a critical period (Achille et al., 2019). Here, we examine\nthe changes that deep neural networks undergo during this early phase of\ntraining. We perform extensive measurements of the network state during these\nearly iterations of training and leverage the framework of Frankle et al.\n(2019) to quantitatively probe the weight distribution and its reliance on\nvarious aspects of the dataset. We find that, within this framework, deep\nnetworks are not robust to reinitializing with random weights while maintaining\nsigns, and that weight distributions are highly non-independent even after only\na few hundred iterations. Despite this behavior, pre-training with blurred\ninputs or an auxiliary self-supervised task can approximate the changes in\nsupervised networks, suggesting that these changes are not inherently\nlabel-dependent, though labels significantly accelerate this process. Together,\nthese results help to elucidate the network changes occurring during this\npivotal initial period of learning.\n",
        "published": "2020",
        "authors": [
            "Jonathan Frankle",
            "David J. Schwab",
            "Ari S. Morcos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.10583v2",
        "title": "Scheduled Restart Momentum for Accelerated Stochastic Gradient Descent",
        "abstract": "  Stochastic gradient descent (SGD) with constant momentum and its variants\nsuch as Adam are the optimization algorithms of choice for training deep neural\nnetworks (DNNs). Since DNN training is incredibly computationally expensive,\nthere is great interest in speeding up the convergence. Nesterov accelerated\ngradient (NAG) improves the convergence rate of gradient descent (GD) for\nconvex optimization using a specially designed momentum; however, it\naccumulates error when an inexact gradient is used (such as in SGD), slowing\nconvergence at best and diverging at worst. In this paper, we propose Scheduled\nRestart SGD (SRSGD), a new NAG-style scheme for training DNNs. SRSGD replaces\nthe constant momentum in SGD by the increasing momentum in NAG but stabilizes\nthe iterations by resetting the momentum to zero according to a schedule. Using\na variety of models and benchmarks for image classification, we demonstrate\nthat, in training DNNs, SRSGD significantly improves convergence and\ngeneralization; for instance in training ResNet200 for ImageNet classification,\nSRSGD achieves an error rate of 20.93% vs. the benchmark of 22.13%. These\nimprovements become more significant as the network grows deeper. Furthermore,\non both CIFAR and ImageNet, SRSGD reaches similar or even better error rates\nwith significantly fewer training epochs compared to the SGD baseline.\n",
        "published": "2020",
        "authors": [
            "Bao Wang",
            "Tan M. Nguyen",
            "Andrea L. Bertozzi",
            "Richard G. Baraniuk",
            "Stanley J. Osher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.11152v1",
        "title": "Fundamental Issues Regarding Uncertainties in Artificial Neural Networks",
        "abstract": "  Artificial Neural Networks (ANNs) implement a specific form of multi-variate\nextrapolation and will generate an output for any input pattern, even when\nthere is no similar training pattern. Extrapolations are not necessarily to be\ntrusted, and in order to support safety critical systems, we require such\nsystems to give an indication of the training sample related uncertainty\nassociated with their output. Some readers may think that this is a well known\nissue which is already covered by the basic principles of pattern recognition.\nWe will explain below how this is not the case and how the conventional\n(Likelihood estimate of) conditional probability of classification does not\ncorrectly assess this uncertainty. We provide a discussion of the standard\ninterpretations of this problem and show how a quantitative approach based upon\nlong standing methods can be practically applied. The methods are illustrated\non the task of early diagnosis of dementing diseases using Magnetic Resonance\nImaging.\n",
        "published": "2020",
        "authors": [
            "Neil A. Thacker",
            "Carole J. Twining",
            "Paul D. Tar",
            "Scott Notley",
            "Visvanathan Ramesh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.11945v1",
        "title": "Is my Neural Network Neuromorphic? Taxonomy, Recent Trends and Future\n  Directions in Neuromorphic Engineering",
        "abstract": "  In this paper, we review recent work published over the last 3 years under\nthe umbrella of Neuromorphic engineering to analyze what are the common\nfeatures among such systems. We see that there is no clear consensus but each\nsystem has one or more of the following features:(1) Analog computing (2) Non\nvonNeumann Architecture and low-precision digital processing (3) Spiking Neural\nNetworks (SNN) with components closely related to biology. We compare recent\nmachine learning accelerator chips to show that indeed analog processing and\nreduced bit precision architectures have best throughput, energy and area\nefficiencies. However, pure digital architectures can also achieve quite high\nefficiencies by just adopting a non von-Neumann architecture. Given the design\nautomation tools for digital hardware design, it raises a question on the\nlikelihood of adoption of analog processing in the near future for industrial\ndesigns. Next, we argue about the importance of defining standards and choosing\nproper benchmarks for the progress of neuromorphic system designs and propose\nsome desired characteristics of such benchmarks. Finally, we show brain-machine\ninterfaces as a potential task that fulfils all the criteria of such\nbenchmarks.\n",
        "published": "2020",
        "authors": [
            "Sumon Kumar Bose",
            "Jyotibdha Acharya",
            "Arindam Basu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.12287v2",
        "title": "Deep Randomized Neural Networks",
        "abstract": "  Randomized Neural Networks explore the behavior of neural systems where the\nmajority of connections are fixed, either in a stochastic or a deterministic\nfashion. Typical examples of such systems consist of multi-layered neural\nnetwork architectures where the connections to the hidden layer(s) are left\nuntrained after initialization. Limiting the training algorithms to operate on\na reduced set of weights inherently characterizes the class of Randomized\nNeural Networks with a number of intriguing features. Among them, the extreme\nefficiency of the resulting learning processes is undoubtedly a striking\nadvantage with respect to fully trained architectures. Besides, despite the\ninvolved simplifications, randomized neural systems possess remarkable\nproperties both in practice, achieving state-of-the-art results in multiple\ndomains, and theoretically, allowing to analyze intrinsic properties of neural\narchitectures (e.g. before training of the hidden layers' connections). In\nrecent years, the study of Randomized Neural Networks has been extended towards\ndeep architectures, opening new research directions to the design of effective\nyet extremely efficient deep learning models in vectorial as well as in more\ncomplex data domains. This chapter surveys all the major aspects regarding the\ndesign and analysis of Randomized Neural Networks, and some of the key results\nwith respect to their approximation capabilities. In particular, we first\nintroduce the fundamentals of randomized neural models in the context of\nfeed-forward networks (i.e., Random Vector Functional Link and equivalent\nmodels) and convolutional filters, before moving to the case of recurrent\nsystems (i.e., Reservoir Computing networks). For both, we focus specifically\non recent results in the domain of deep randomized systems, and (for recurrent\nmodels) their application to structured domains.\n",
        "published": "2020",
        "authors": [
            "Claudio Gallicchio",
            "Simone Scardapane"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.12795v2",
        "title": "The Landscape of Matrix Factorization Revisited",
        "abstract": "  We revisit the landscape of the simple matrix factorization problem. For\nlow-rank matrix factorization, prior work has shown that there exist infinitely\nmany critical points all of which are either global minima or strict saddles.\nAt a strict saddle the minimum eigenvalue of the Hessian is negative. Of\ninterest is whether this minimum eigenvalue is uniformly bounded below zero\nover all strict saddles. To answer this we consider orbits of critical points\nunder the general linear group. For each orbit we identify a representative\npoint, called a canonical point. If a canonical point is a strict saddle, so is\nevery point on its orbit. We derive an expression for the minimum eigenvalue of\nthe Hessian at each canonical strict saddle and use this to show that the\nminimum eigenvalue of the Hessian over the set of strict saddles is not\nuniformly bounded below zero. We also show that a known invariance property of\ngradient flow ensures the solution of gradient flow only encounters critical\npoints on an invariant manifold $\\mathcal{M}_C$ determined by the initial\ncondition. We show that, in contrast to the general situation, the minimum\neigenvalue of strict saddles in $\\mathcal{M}_{0}$ is uniformly bounded below\nzero. We obtain an expression for this bound in terms of the singular values of\nthe matrix being factorized. This bound depends on the size of the nonzero\nsingular values and on the separation between distinct nonzero singular values\nof the matrix.\n",
        "published": "2020",
        "authors": [
            "Hossein Valavi",
            "Sulin Liu",
            "Peter J. Ramadge"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.00817v4",
        "title": "A survey on modern trainable activation functions",
        "abstract": "  In neural networks literature, there is a strong interest in identifying and\ndefining activation functions which can improve neural network performance. In\nrecent years there has been a renovated interest of the scientific community in\ninvestigating activation functions which can be trained during the learning\nprocess, usually referred to as \"trainable\", \"learnable\" or \"adaptable\"\nactivation functions. They appear to lead to better network performance.\nDiverse and heterogeneous models of trainable activation function have been\nproposed in the literature. In this paper, we present a survey of these models.\nStarting from a discussion on the use of the term \"activation function\" in\nliterature, we propose a taxonomy of trainable activation functions, highlight\ncommon and distinctive proprieties of recent and past models, and discuss main\nadvantages and limitations of this type of approach. We show that many of the\nproposed approaches are equivalent to adding neuron layers which use fixed\n(non-trainable) activation functions and some simple local rule that\nconstraints the corresponding weight layers.\n",
        "published": "2020",
        "authors": [
            "Andrea Apicella",
            "Francesco Donnarumma",
            "Francesco Isgr\u00f2",
            "Roberto Prevete"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.01711v1",
        "title": "Dual Stage Classification of Hand Gestures using Surface Electromyogram",
        "abstract": "  Surface electromyography (sEMG) is becoming exceeding useful in applications\ninvolving analysis of human motion such as in human-machine interface,\nassistive technology, healthcare and prosthetic development. The proposed work\npresents a novel dual stage classification approach for classification of\ngrasping gestures from sEMG signals. A statistical assessment of these\nactivities is presented to determine the similar characteristics between the\nconsidered activities. Similar activities are grouped together. In the first\nstage of classification, an activity is identified as belonging to a group,\nwhich is then further classified as one of the activities within the group in\nthe second stage of classification. The performance of the proposed approach is\ncompared to the conventional single stage classification approach in terms of\nclassification accuracies. The classification accuracies obtained using the\nproposed dual stage classification are significantly higher as compared to that\nfor single stage classification.\n",
        "published": "2020",
        "authors": [
            "Karush Suri",
            "Rinki Gupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.01862v1",
        "title": "Complex Amplitude-Phase Boltzmann Machines",
        "abstract": "  We extend the framework of Boltzmann machines to a network of complex-valued\nneurons with variable amplitudes, referred to as Complex Amplitude-Phase\nBoltzmann machine (CAP-BM). The model is capable of performing unsupervised\nlearning on the amplitude and relative phase distribution in complex data. The\nsampling rule of the Gibbs distribution and the learning rules of the model are\npresented. Learning in a Complex Amplitude-Phase restricted Boltzmann machine\n(CAP-RBM) is demonstrated on synthetic complex-valued images, and handwritten\nMNIST digits transformed by a complex wavelet transform. Specifically, we show\nthe necessity of a new amplitude-amplitude coupling term in our model. The\nproposed model is potentially valuable for machine learning tasks involving\ncomplex-valued data with amplitude variation, and for developing algorithms for\nnovel computation hardware, such as coupled oscillators and neuromorphic\nhardware, on which Boltzmann sampling can be executed in the complex domain.\n",
        "published": "2020",
        "authors": [
            "Zengyi Li",
            "Friedrich T. Sommer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.03675v3",
        "title": "Machine Learning on Graphs: A Model and Comprehensive Taxonomy",
        "abstract": "  There has been a surge of recent interest in learning representations for\ngraph-structured data. Graph representation learning methods have generally\nfallen into three main categories, based on the availability of labeled data.\nThe first, network embedding (such as shallow graph embedding or graph\nauto-encoders), focuses on learning unsupervised representations of relational\nstructure. The second, graph regularized neural networks, leverages graphs to\naugment neural network losses with a regularization objective for\nsemi-supervised learning. The third, graph neural networks, aims to learn\ndifferentiable functions over discrete topologies with arbitrary structure.\nHowever, despite the popularity of these areas there has been surprisingly\nlittle work on unifying the three paradigms. Here, we aim to bridge the gap\nbetween graph neural networks, network embedding and graph regularization\nmodels. We propose a comprehensive taxonomy of representation learning methods\nfor graph-structured data, aiming to unify several disparate bodies of work.\nSpecifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which\ngeneralizes popular algorithms for semi-supervised learning on graphs (e.g.\nGraphSage, Graph Convolutional Networks, Graph Attention Networks), and\nunsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc)\ninto a single consistent approach. To illustrate the generality of this\napproach, we fit over thirty existing methods into this framework. We believe\nthat this unifying view both provides a solid foundation for understanding the\nintuition behind these methods, and enables future research in the area.\n",
        "published": "2020",
        "authors": [
            "Ines Chami",
            "Sami Abu-El-Haija",
            "Bryan Perozzi",
            "Christopher R\u00e9",
            "Kevin Murphy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.04155v1",
        "title": "Hybrid Machine Learning Models for Crop Yield Prediction",
        "abstract": "  Prediction of crop yield is essential for food security policymaking,\nplanning, and trade. The objective of the current study is to propose novel\ncrop yield prediction models based on hybrid machine learning methods. In this\nstudy, the performance of the artificial neural networks-imperialist\ncompetitive algorithm (ANN-ICA) and artificial neural networks-gray wolf\noptimizer (ANN-GWO) models for the crop yield prediction are evaluated.\nAccording to the results, ANN-GWO, with R of 0.48, RMSE of 3.19, and MEA of\n26.65, proved a better performance in the crop yield prediction compared to the\nANN-ICA model. The results can be used by either practitioners, researchers or\npolicymakers for food security.\n",
        "published": "2020",
        "authors": [
            "Saeed Nosratabadi",
            "Felde Imre",
            "Karoly Szell",
            "Sina Ardabili",
            "Bertalan Beszedes",
            "Amir Mosavi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.04156v1",
        "title": "Comparison of Evolving Granular Classifiers applied to Anomaly Detection\n  for Predictive Maintenance in Computing Centers",
        "abstract": "  Log-based predictive maintenance of computing centers is a main concern\nregarding the worldwide computing grid that supports the CERN (European\nOrganization for Nuclear Research) physics experiments. A log, as\nevent-oriented adhoc information, is quite often given as unstructured big\ndata. Log data processing is a time-consuming computational task. The goal is\nto grab essential information from a continuously changeable grid environment\nto construct a classification model. Evolving granular classifiers are suited\nto learn from time-varying log streams and, therefore, perform online\nclassification of the severity of anomalies. We formulated a 4-class online\nanomaly classification problem, and employed time windows between landmarks and\ntwo granular computing methods, namely, Fuzzy-set-Based evolving Modeling\n(FBeM) and evolving Granular Neural Network (eGNN), to model and monitor\nlogging activity rate. The results of classification are of utmost importance\nfor predictive maintenance because priority can be given to specific time\nintervals in which the classifier indicates the existence of high or medium\nseverity anomalies.\n",
        "published": "2020",
        "authors": [
            "Leticia Decker",
            "Daniel Leite",
            "Fabio Viola",
            "Daniele Bonacorsi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.04165v1",
        "title": "Towards On-Chip Bayesian Neuromorphic Learning",
        "abstract": "  If edge devices are to be deployed to critical applications where their\ndecisions could have serious financial, political, or public-health\nconsequences, they will need a way to signal when they are not sure how to\nreact to their environment. For instance, a lost delivery drone could make its\nway back to a distribution center or contact the client if it is confused about\nhow exactly to make its delivery, rather than taking the action which is \"most\nlikely\" correct. This issue is compounded for health care or military\napplications. However, the brain-realistic temporal credit assignment problem\nneuromorphic computing algorithms have to solve is difficult. The double role\nweights play in backpropagation-based-learning, dictating how the network\nreacts to both input and feedback, needs to be decoupled. e-prop 1 is a\npromising learning algorithm that tackles this with Broadcast Alignment (a\ntechnique where network weights are replaced with random weights during\nfeedback) and accumulated local information. We investigate under what\nconditions the Bayesian loss term can be expressed in a similar fashion,\nproposing an algorithm that can be computed with only local information as well\nand which is thus no more difficult to implement on hardware. This algorithm is\nexhibited on a store-recall problem, which suggests that it can learn good\nuncertainty on decisions to be made over time.\n",
        "published": "2020",
        "authors": [
            "Nathan Wycoff",
            "Prasanna Balaprakash",
            "Fangfang Xia"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.04168v1",
        "title": "Equilibrium Propagation with Continual Weight Updates",
        "abstract": "  Equilibrium Propagation (EP) is a learning algorithm that bridges Machine\nLearning and Neuroscience, by computing gradients closely matching those of\nBackpropagation Through Time (BPTT), but with a learning rule local in space.\nGiven an input $x$ and associated target $y$, EP proceeds in two phases: in the\nfirst phase neurons evolve freely towards a first steady state; in the second\nphase output neurons are nudged towards $y$ until they reach a second steady\nstate. However, in existing implementations of EP, the learning rule is not\nlocal in time: the weight update is performed after the dynamics of the second\nphase have converged and requires information of the first phase that is no\nlonger available physically. In this work, we propose a version of EP named\nContinual Equilibrium Propagation (C-EP) where neuron and synapse dynamics\noccur simultaneously throughout the second phase, so that the weight update\nbecomes local in time. Such a learning rule local both in space and time opens\nthe possibility of an extremely energy efficient hardware implementation of EP.\nWe prove theoretically that, provided the learning rates are sufficiently\nsmall, at each time step of the second phase the dynamics of neurons and\nsynapses follow the gradients of the loss given by BPTT (Theorem 1). We\ndemonstrate training with C-EP on MNIST and generalize C-EP to neural networks\nwhere neurons are connected by asymmetric connections. We show through\nexperiments that the more the network updates follows the gradients of BPTT,\nthe best it performs in terms of training. These results bring EP a step closer\nto biology by better complying with hardware constraints while maintaining its\nintimate link with backpropagation.\n",
        "published": "2020",
        "authors": [
            "Maxence Ernoult",
            "Julie Grollier",
            "Damien Querlioz",
            "Yoshua Bengio",
            "Benjamin Scellier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.04169v1",
        "title": "Continual Weight Updates and Convolutional Architectures for Equilibrium\n  Propagation",
        "abstract": "  Equilibrium Propagation (EP) is a biologically inspired alternative algorithm\nto backpropagation (BP) for training neural networks. It applies to RNNs fed by\na static input x that settle to a steady state, such as Hopfield networks. EP\nis similar to BP in that in the second phase of training, an error signal\npropagates backwards in the layers of the network, but contrary to BP, the\nlearning rule of EP is spatially local. Nonetheless, EP suffers from two major\nlimitations. On the one hand, due to its formulation in terms of real-time\ndynamics, EP entails long simulation times, which limits its applicability to\npractical tasks. On the other hand, the biological plausibility of EP is\nlimited by the fact that its learning rule is not local in time: the synapse\nupdate is performed after the dynamics of the second phase have converged and\nrequires information of the first phase that is no longer available physically.\nOur work addresses these two issues and aims at widening the spectrum of EP\nfrom standard machine learning models to more bio-realistic neural networks.\nFirst, we propose a discrete-time formulation of EP which enables to simplify\nequations, speed up training and extend EP to CNNs. Our CNN model achieves the\nbest performance ever reported on MNIST with EP. Using the same discrete-time\nformulation, we introduce Continual Equilibrium Propagation (C-EP): the weights\nof the network are adjusted continually in the second phase of training using\nlocal information in space and time. We show that in the limit of slow changes\nof synaptic strengths and small nudging, C-EP is equivalent to BPTT (Theorem\n1). We numerically demonstrate Theorem 1 and C-EP training on MNIST and\ngeneralize it to the bio-realistic situation of a neural network with\nasymmetric connections between neurons.\n",
        "published": "2020",
        "authors": [
            "Maxence Ernoult",
            "Julie Grollier",
            "Damien Querlioz",
            "Yoshua Bengio",
            "Benjamin Scellier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.04170v1",
        "title": "A Neuromorphic Paradigm for Online Unsupervised Clustering",
        "abstract": "  A computational paradigm based on neuroscientific concepts is proposed and\nshown to be capable of online unsupervised clustering. Because it is an online\nmethod, it is readily amenable to streaming realtime applications and is\ncapable of dynamically adjusting to macro-level input changes. All operations,\nboth training and inference, are localized and efficient. The paradigm is\nimplemented as a cognitive column that incorporates five key elements: 1)\ntemporal coding, 2) an excitatory neuron model for inference, 3)\nwinner-take-all inhibition, 4) a column architecture that combines excitation\nand inhibition, 5) localized training via spike timing de-pendent plasticity\n(STDP). These elements are described and discussed, and a prototype column is\ngiven. The prototype column is simulated with a semi-synthetic benchmark and is\nshown to have performance characteristics on par with classic k-means.\nSimulations reveal the inner operation and capabilities of the column with\nemphasis on excitatory neuron response functions and STDP implementations.\n",
        "published": "2020",
        "authors": [
            "James E. Smith"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.04210v2",
        "title": "The critical locus of overparameterized neural networks",
        "abstract": "  Many aspects of the geometry of loss functions in deep learning remain\nmysterious. In this paper, we work toward a better understanding of the\ngeometry of the loss function $L$ of overparameterized feedforward neural\nnetworks. In this setting, we identify several components of the critical locus\nof $L$ and study their geometric properties. For networks of depth $\\ell \\geq\n4$, we identify a locus of critical points we call the star locus $S$. Within\n$S$ we identify a positive-dimensional sublocus $C$ with the property that for\n$p \\in C$, $p$ is a degenerate critical point, and no existing theoretical\nresult guarantees that gradient descent will not converge to $p$. For very wide\nnetworks, we build on earlier work and show that all critical points of $L$ are\ndegenerate, and give lower bounds on the number of zero eigenvalues of the\nHessian at each critical point. For networks that are both deep and very wide,\nwe compare the growth rates of the zero eigenspaces of the Hessian at all the\ndifferent families of critical points that we identify. The results in this\npaper provide a starting point to a more quantitative understanding of the\nproperties of various components of the critical locus of $L$.\n",
        "published": "2020",
        "authors": [
            "Y. Cooper"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.05294v1",
        "title": "Ring Reservoir Neural Networks for Graphs",
        "abstract": "  Machine Learning for graphs is nowadays a research topic of consolidated\nrelevance. Common approaches in the field typically resort to complex deep\nneural network architectures and demanding training algorithms, highlighting\nthe need for more efficient solutions. The class of Reservoir Computing (RC)\nmodels can play an important role in this context, enabling to develop fruitful\ngraph embeddings through untrained recursive architectures. In this paper, we\nstudy progressive simplifications to the design strategy of RC neural networks\nfor graphs. Our core proposal is based on shaping the organization of the\nhidden neurons to follow a ring topology. Experimental results on graph\nclassification tasks indicate that ring-reservoirs architectures enable\nparticularly effective network configurations, showing consistent advantages in\nterms of predictive performance.\n",
        "published": "2020",
        "authors": [
            "Claudio Gallicchio",
            "Alessio Micheli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.05704v1",
        "title": "Fostering Event Compression using Gated Surprise",
        "abstract": "  Our brain receives a dynamically changing stream of sensorimotor data. Yet,\nwe perceive a rather organized world, which we segment into and perceive as\nevents. Computational theories of cognitive science on event-predictive\ncognition suggest that our brain forms generative, event-predictive models by\nsegmenting sensorimotor data into suitable chunks of contextual experiences.\nHere, we introduce a hierarchical, surprise-gated recurrent neural network\narchitecture, which models this process and develops compact compressions of\ndistinct event-like contexts. The architecture contains a contextual LSTM\nlayer, which develops generative compressions of ongoing and subsequent\ncontexts. These compressions are passed into a GRU-like layer, which uses\nsurprise signals to update its recurrent latent state. The latent state is\npassed forward into another LSTM layer, which processes actual dynamic sensory\nflow in the light of the provided latent, contextual compression signals. Our\nmodel shows to develop distinct event compressions and achieves the best\nperformance on multiple event processing tasks. The architecture may be very\nuseful for the further development of resource-efficient learning, hierarchical\nmodel-based reinforcement learning, as well as the development of artificial\nevent-predictive cognition and intelligence.\n",
        "published": "2020",
        "authors": [
            "Dania Humaidan",
            "Sebastian Otte",
            "Martin V. Butz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.05941v1",
        "title": "Training spiking neural networks using reinforcement learning",
        "abstract": "  Neurons in the brain communicate with each other through discrete action\nspikes as opposed to continuous signal transmission in artificial neural\nnetworks. Therefore, the traditional techniques for optimization of parameters\nin neural networks which rely on the assumption of differentiability of\nactivation functions are no longer applicable to modeling the learning\nprocesses in the brain. In this project, we propose biologically-plausible\nalternatives to backpropagation to facilitate the training of spiking neural\nnetworks. We primarily focus on investigating the candidacy of reinforcement\nlearning (RL) rules in solving the spatial and temporal credit assignment\nproblems to enable decision-making in complex tasks. In one approach, we\nconsider each neuron in a multi-layer neural network as an independent RL agent\nforming a different representation of the feature space while the network as a\nwhole forms the representation of the complex policy to solve the task at hand.\nIn other approach, we apply the reparameterization trick to enable\ndifferentiation through stochastic transformations in spiking neural networks.\nWe compare and contrast the two approaches by applying them to traditional RL\ndomains such as gridworld, cartpole and mountain car. Further we also suggest\nvariations and enhancements to enable future research in this area.\n",
        "published": "2020",
        "authors": [
            "Sneha Aenugu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.06284v3",
        "title": "Pruning coupled with learning, ensembles of minimal neural networks, and\n  future of XAI",
        "abstract": "  Pruning coupled with learning aims to optimize the neural network (NN)\nstructure for solving specific problems. This optimization can be used for\nvarious purposes: to prevent overfitting, to save resources for implementation\nand training, to provide explainability of the trained NN, and many others. The\nminimal structure that cannot be pruned further is not unique. Ensemble of\nminimal structures can be used as a committee of intellectual agents that\nsolves problems by voting. Each minimal NN presents an \"empirical knowledge\"\nabout the problem and can be verbalized. The non-uniqueness of such knowledge\nextracted from data is an important property of data-driven Artificial\nIntelligence (AI). In this work, we review an approach to pruning based on the\nprinciple: What controls training should control pruning. This principle is\nexpected to work both for artificial NN and for selection and modification of\nimportant synaptic contacts in brain. In back-propagation artificial NN\nlearning is controlled by the gradient of loss functions. Therefore, the first\norder sensitivity indicators are used for pruning and the algorithms based on\nthese indicators are reviewed. The notion of logically transparent NN was\nintroduced. The approach was illustrated on the problem of political\nforecasting: predicting the results of the US presidential election. Eight\nminimal NN were produced that give different forecasting algorithms. The\nnon-uniqueness of solution can be utilised by creation of expert panels\n(committee). Another use of NN pluralism is to identify areas of input signals\nwhere further data collection is most useful. In Conclusion, we discuss the\npossible future of widely advertised XAI program.\n",
        "published": "2020",
        "authors": [
            "Alexander N. Gorban",
            "Evgeny M. Mirkes"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.06398v2",
        "title": "Implicit Regularization in Deep Learning May Not Be Explainable by Norms",
        "abstract": "  Mathematically characterizing the implicit regularization induced by\ngradient-based optimization is a longstanding pursuit in the theory of deep\nlearning. A widespread hope is that a characterization based on minimization of\nnorms may apply, and a standard test-bed for studying this prospect is matrix\nfactorization (matrix completion via linear neural networks). It is an open\nquestion whether norms can explain the implicit regularization in matrix\nfactorization. The current paper resolves this open question in the negative,\nby proving that there exist natural matrix factorization problems on which the\nimplicit regularization drives all norms (and quasi-norms) towards infinity.\nOur results suggest that, rather than perceiving the implicit regularization\nvia norms, a potentially more useful interpretation is minimization of rank. We\ndemonstrate empirically that this interpretation extends to a certain class of\nnon-linear neural networks, and hypothesize that it may be key to explaining\ngeneralization in deep learning.\n",
        "published": "2020",
        "authors": [
            "Noam Razin",
            "Nadav Cohen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.07360v1",
        "title": "Learning Rate Annealing Can Provably Help Generalization, Even for\n  Convex Problems",
        "abstract": "  Learning rate schedule can significantly affect generalization performance in\nmodern neural networks, but the reasons for this are not yet understood.\nLi-Wei-Ma (2019) recently proved this behavior can exist in a simplified\nnon-convex neural-network setting. In this note, we show that this phenomenon\ncan exist even for convex learning problems -- in particular, linear regression\nin 2 dimensions.\n  We give a toy convex problem where learning rate annealing (large initial\nlearning rate, followed by small learning rate) can lead gradient descent to\nminima with provably better generalization than using a small learning rate\nthroughout. In our case, this occurs due to a combination of the mismatch\nbetween the test and train loss landscapes, and early-stopping.\n",
        "published": "2020",
        "authors": [
            "Preetum Nakkiran"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.07786v1",
        "title": "A flexible, extensible software framework for model compression based on\n  the LC algorithm",
        "abstract": "  We propose a software framework based on the ideas of the\nLearning-Compression (LC) algorithm, that allows a user to compress a neural\nnetwork or other machine learning model using different compression schemes\nwith minimal effort. Currently, the supported compressions include pruning,\nquantization, low-rank methods (including automatically learning the layer\nranks), and combinations of those, and the user can choose different\ncompression types for different parts of a neural network.\n  The LC algorithm alternates two types of steps until convergence: a learning\n(L) step, which trains a model on a dataset (using an algorithm such as SGD);\nand a compression (C) step, which compresses the model parameters (using a\ncompression scheme such as low-rank or quantization). This decoupling of the\n\"machine learning\" aspect from the \"signal compression\" aspect means that\nchanging the model or the compression type amounts to calling the corresponding\nsubroutine in the L or C step, respectively. The library fully supports this by\ndesign, which makes it flexible and extensible. This does not come at the\nexpense of performance: the runtime needed to compress a model is comparable to\nthat of training the model in the first place; and the compressed model is\ncompetitive in terms of prediction accuracy and compression ratio with other\nalgorithms (which are often specialized for specific models or compression\nschemes). The library is written in Python and PyTorch and available in Github.\n",
        "published": "2020",
        "authors": [
            "Yerlan Idelbayev",
            "Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.09319v2",
        "title": "A New Training Pipeline for an Improved Neural Transducer",
        "abstract": "  The RNN transducer is a promising end-to-end model candidate. We compare the\noriginal training criterion with the full marginalization over all alignments,\nto the commonly used maximum approximation, which simplifies, improves and\nspeeds up our training. We also generalize from the original neural network\nmodel and study more powerful models, made possible due to the maximum\napproximation. We further generalize the output label topology to cover RNN-T,\nRNA and CTC. We perform several studies among all these aspects, including a\nstudy on the effect of external alignments. We find that the transducer model\ngeneralizes much better on longer sequences than the attention model. Our final\ntransducer model outperforms our attention model on Switchboard 300h by over 6%\nrelative WER.\n",
        "published": "2020",
        "authors": [
            "Albert Zeyer",
            "Andr\u00e9 Merboldt",
            "Ralf Schl\u00fcter",
            "Hermann Ney"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.10190v4",
        "title": "Feature Purification: How Adversarial Training Performs Robust Deep\n  Learning",
        "abstract": "  Despite the empirical success of using Adversarial Training to defend deep\nlearning models against adversarial perturbations, so far, it still remains\nrather unclear what the principles are behind the existence of adversarial\nperturbations, and what adversarial training does to the neural network to\nremove them.\n  In this paper, we present a principle that we call Feature Purification,\nwhere we show one of the causes of the existence of adversarial examples is the\naccumulation of certain small dense mixtures in the hidden weights during the\ntraining process of a neural network; and more importantly, one of the goals of\nadversarial training is to remove such mixtures to purify hidden weights. We\npresent both experiments on the CIFAR-10 dataset to illustrate this principle,\nand a theoretical result proving that for certain natural classification tasks,\ntraining a two-layer neural network with ReLU activation using randomly\ninitialized gradient descent indeed satisfies this principle.\n  Technically, we give, to the best of our knowledge, the first result proving\nthat the following two can hold simultaneously for training a neural network\nwith ReLU activation. (1) Training over the original data is indeed non-robust\nto small adversarial perturbations of some radius. (2) Adversarial training,\neven with an empirical perturbation algorithm such as FGM, can in fact be\nprovably robust against ANY perturbations of the same radius. Finally, we also\nprove a complexity lower bound, showing that low complexity models such as\nlinear classifiers, low-degree polynomials, or even the neural tangent kernel\nfor this network, CANNOT defend against perturbations of this same radius, no\nmatter what algorithms are used to train them.\n",
        "published": "2020",
        "authors": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.11074v1",
        "title": "An Introduction to Neural Architecture Search for Convolutional Networks",
        "abstract": "  Neural Architecture Search (NAS) is a research field concerned with utilizing\noptimization algorithms to design optimal neural network architectures. There\nare many approaches concerning the architectural search spaces, optimization\nalgorithms, as well as candidate architecture evaluation methods. As the field\nis growing at a continuously increasing pace, it is difficult for a beginner to\ndiscern between major, as well as emerging directions the field has followed.\nIn this work, we provide an introduction to the basic concepts of NAS for\nconvolutional networks, along with the major advances in search spaces,\nalgorithms and evaluation techniques.\n",
        "published": "2020",
        "authors": [
            "George Kyriakides",
            "Konstantinos Margaritis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.13971v1",
        "title": "Separation of Memory and Processing in Dual Recurrent Neural Networks",
        "abstract": "  We explore a neural network architecture that stacks a recurrent layer and a\nfeedforward layer that is also connected to the input, and compare it to\nstandard Elman and LSTM architectures in terms of accuracy and\ninterpretability. When noise is introduced into the activation function of the\nrecurrent units, these neurons are forced into a binary activation regime that\nmakes the networks behave much as finite automata. The resulting models are\nsimpler, easier to interpret and get higher accuracy on different sample\nproblems, including the recognition of regular languages, the computation of\nadditions in different bases and the generation of arithmetic expressions.\n",
        "published": "2020",
        "authors": [
            "Christian Oliva",
            "Luis F. Lago-Fern\u00e1ndez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.00945v1",
        "title": "LAVARNET: Neural Network Modeling of Causal Variable Relationships for\n  Multivariate Time Series Forecasting",
        "abstract": "  Multivariate time series forecasting is of great importance to many\nscientific disciplines and industrial sectors. The evolution of a multivariate\ntime series depends on the dynamics of its variables and the connectivity\nnetwork of causal interrelationships among them. Most of the existing time\nseries models do not account for the causal effects among the system's\nvariables and even if they do they rely just on determining the\nbetween-variables causality network. Knowing the structure of such a complex\nnetwork and even more specifically knowing the exact lagged variables that\ncontribute to the underlying process is crucial for the task of multivariate\ntime series forecasting. The latter is a rather unexplored source of\ninformation to leverage. In this direction, here a novel neural network-based\narchitecture is proposed, termed LAgged VAriable Representation NETwork\n(LAVARNET), which intrinsically estimates the importance of lagged variables\nand combines high dimensional latent representations of them to predict future\nvalues of time series. Our model is compared with other baseline and state of\nthe art neural network architectures on one simulated data set and four real\ndata sets from meteorology, music, solar activity, and finance areas. The\nproposed architecture outperforms the competitive architectures in most of the\nexperiments.\n",
        "published": "2020",
        "authors": [
            "Christos Koutlis",
            "Symeon Papadopoulos",
            "Manos Schinas",
            "Ioannis Kompatsiaris"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.02560v1",
        "title": "Particle Swarm Optimized Federated Learning For Industrial IoT and Smart\n  City Services",
        "abstract": "  Most of the research on Federated Learning (FL) has focused on analyzing\nglobal optimization, privacy, and communication, with limited attention\nfocusing on analyzing the critical matter of performing efficient local\ntraining and inference at the edge devices. One of the main challenges for\nsuccessful and efficient training and inference on edge devices is the careful\nselection of parameters to build local Machine Learning (ML) models. To this\naim, we propose a Particle Swarm Optimization (PSO)-based technique to optimize\nthe hyperparameter settings for the local ML models in an FL environment. We\nevaluate the performance of our proposed technique using two case studies.\nFirst, we consider smart city services and use an experimental transportation\ndataset for traffic prediction as a proxy for this setting. Second, we consider\nIndustrial IoT (IIoT) services and use the real-time telemetry dataset to\npredict the probability that a machine will fail shortly due to component\nfailures. Our experiments indicate that PSO provides an efficient approach for\ntuning the hyperparameters of deep Long short-term memory (LSTM) models when\ncompared to the grid search method. Our experiments illustrate that the number\nof clients-server communication rounds to explore the landscape of\nconfigurations to find the near-optimal parameters are greatly reduced (roughly\nby two orders of magnitude needing only 2%--4% of the rounds compared to state\nof the art non-PSO-based approaches). We also demonstrate that utilizing the\nproposed PSO-based technique to find the near-optimal configurations for FL and\ncentralized learning models does not adversely affect the accuracy of the\nmodels.\n",
        "published": "2020",
        "authors": [
            "Basheer Qolomany",
            "Kashif Ahmad",
            "Ala Al-Fuqaha",
            "Junaid Qadir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.06342v2",
        "title": "Reservoir Memory Machines as Neural Computers",
        "abstract": "  Differentiable neural computers extend artificial neural networks with an\nexplicit memory without interference, thus enabling the model to perform\nclassic computation tasks such as graph traversal. However, such models are\ndifficult to train, requiring long training times and large datasets. In this\nwork, we achieve some of the computational capabilities of differentiable\nneural computers with a model that can be trained very efficiently, namely an\necho state network with an explicit memory without interference. This extension\nenables echo state networks to recognize all regular languages, including those\nthat contractive echo state networks provably can not recognize. Further, we\ndemonstrate experimentally that our model performs comparably to its\nfully-trained deep version on several typical benchmark tasks for\ndifferentiable neural computers.\n",
        "published": "2020",
        "authors": [
            "Benjamin Paa\u00dfen",
            "Alexander Schulz",
            "Terrence C. Stewart",
            "Barbara Hammer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.06762v1",
        "title": "New complex network building methodology for High Level Classification\n  based on attribute-attribute interaction",
        "abstract": "  High-level classification algorithms focus on the interactions between\ninstances. These produce a new form to evaluate and classify data. In this\nprocess, the core is the complex network building methodology because it\ndetermines the metrics to be used for classification. The current methodologies\nuse variations of kNN to produce these graphs. However, this technique ignores\nsome hidden pattern between attributes and require normalization to be\naccurate. In this paper, we propose a new methodology for network building\nbased on attribute-attribute interactions that do not require normalization and\ncapture the hidden patterns of the attributes. The current results show us that\ncould be used to improve some current high-level techniques.\n",
        "published": "2020",
        "authors": [
            "Esteban Wilfredo Vilca Zu\u00f1iga"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.07101v4",
        "title": "Approximate spectral clustering using both reference vectors and\n  topology of the network generated by growing neural gas",
        "abstract": "  Spectral clustering (SC) is one of the most popular clustering methods and\noften outperforms traditional clustering methods. SC uses the eigenvectors of a\nLaplacian matrix calculated from a similarity matrix of a dataset. SC has\nserious drawbacks: the significant increases in the time complexity derived\nfrom the computation of eigenvectors and the memory space complexity to store\nthe similarity matrix. To address the issues, I develop a new approximate\nspectral clustering using the network generated by growing neural gas (GNG),\ncalled ASC with GNG in this study. ASC with GNG uses not only reference vectors\nfor vector quantization but also the topology of the network for extraction of\nthe topological relationship between data points in a dataset. ASC with GNG\ncalculates the similarity matrix from both the reference vectors and the\ntopology of the network generated by GNG. Using the network generated from a\ndataset by GNG, ASC with GNG achieves to reduce the computational and space\ncomplexities and improve clustering quality. In this study, I demonstrate that\nASC with GNG effectively reduces the computational time. Moreover, this study\nshows that ASC with GNG provides equal to or better clustering performance than\nSC.\n",
        "published": "2020",
        "authors": [
            "Kazuhisa Fujita"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.07430v1",
        "title": "An Extensive Experimental Evaluation of Automated Machine Learning\n  Methods for Recommending Classification Algorithms (Extended Version)",
        "abstract": "  This paper presents an experimental comparison among four Automated Machine\nLearning (AutoML) methods for recommending the best classification algorithm\nfor a given input dataset. Three of these methods are based on Evolutionary\nAlgorithms (EAs), and the other is Auto-WEKA, a well-known AutoML method based\non the Combined Algorithm Selection and Hyper-parameter optimisation (CASH)\napproach. The EA-based methods build classification algorithms from a single\nmachine learning paradigm: either decision-tree induction, rule induction, or\nBayesian network classification. Auto-WEKA combines algorithm selection and\nhyper-parameter optimisation to recommend classification algorithms from\nmultiple paradigms. We performed controlled experiments where these four AutoML\nmethods were given the same runtime limit for different values of this limit.\nIn general, the difference in predictive accuracy of the three best AutoML\nmethods was not statistically significant. However, the EA evolving\ndecision-tree induction algorithms has the advantage of producing algorithms\nthat generate interpretable classification models and that are more scalable to\nlarge datasets, by comparison with many algorithms from other learning\nparadigms that can be recommended by Auto-WEKA. We also observed that Auto-WEKA\nhas shown meta-overfitting, a form of overfitting at the meta-learning level,\nrather than at the base-learning level.\n",
        "published": "2020",
        "authors": [
            "M\u00e1rcio P. Basgalupp",
            "Rodrigo C. Barros",
            "Alex G. C. de S\u00e1",
            "Gisele L. Pappa",
            "Rafael G. Mantovani",
            "Andr\u00e9 C. P. L. F. de Carvalho",
            "Alex A. Freitas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.08934v1",
        "title": "Exploiting Heterogeneity in Operational Neural Networks by Synaptic\n  Plasticity",
        "abstract": "  The recently proposed network model, Operational Neural Networks (ONNs), can\ngeneralize the conventional Convolutional Neural Networks (CNNs) that are\nhomogenous only with a linear neuron model. As a heterogenous network model,\nONNs are based on a generalized neuron model that can encapsulate any set of\nnon-linear operators to boost diversity and to learn highly complex and\nmulti-modal functions or spaces with minimal network complexity and training\ndata. However, the default search method to find optimal operators in ONNs, the\nso-called Greedy Iterative Search (GIS) method, usually takes several training\nsessions to find a single operator set per layer. This is not only\ncomputationally demanding, also the network heterogeneity is limited since the\nsame set of operators will then be used for all neurons in each layer. To\naddress this deficiency and exploit a superior level of heterogeneity, in this\nstudy the focus is drawn on searching the best-possible operator set(s) for the\nhidden neurons of the network based on the Synaptic Plasticity paradigm that\nposes the essential learning theory in biological neurons. During training,\neach operator set in the library can be evaluated by their synaptic plasticity\nlevel, ranked from the worst to the best, and an elite ONN can then be\nconfigured using the top ranked operator sets found at each hidden layer.\nExperimental results over highly challenging problems demonstrate that the\nelite ONNs even with few neurons and layers can achieve a superior learning\nperformance than GIS-based ONNs and as a result the performance gap over the\nCNNs further widens.\n",
        "published": "2020",
        "authors": [
            "Serkan Kiranyaz",
            "Junaid Malik",
            "Habib Ben Abdallah",
            "Turker Ince",
            "Alexandros Iosifidis",
            "Moncef Gabbouj"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.11243v1",
        "title": "Tasks, stability, architecture, and compute: Training more effective\n  learned optimizers, and using them to train themselves",
        "abstract": "  Much as replacing hand-designed features with learned functions has\nrevolutionized how we solve perceptual tasks, we believe learned algorithms\nwill transform how we train models. In this work we focus on general-purpose\nlearned optimizers capable of training a wide variety of problems with no\nuser-specified hyperparameters. We introduce a new, neural network\nparameterized, hierarchical optimizer with access to additional features such\nas validation loss to enable automatic regularization. Most learned optimizers\nhave been trained on only a single task, or a small number of tasks. We train\nour optimizers on thousands of tasks, making use of orders of magnitude more\ncompute, resulting in optimizers that generalize better to unseen tasks. The\nlearned optimizers not only perform well, but learn behaviors that are distinct\nfrom existing first order optimizers. For instance, they generate update steps\nthat have implicit regularization and adapt as the problem hyperparameters\n(e.g. batch size) or architecture (e.g. neural network width) change. Finally,\nthese learned optimizers show evidence of being useful for out of distribution\ntasks such as training themselves from scratch.\n",
        "published": "2020",
        "authors": [
            "Luke Metz",
            "Niru Maheswaranathan",
            "C. Daniel Freeman",
            "Ben Poole",
            "Jascha Sohl-Dickstein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.11479v1",
        "title": "Theoretical Analysis of the Advantage of Deepening Neural Networks",
        "abstract": "  We propose two new criteria to understand the advantage of deepening neural\nnetworks. It is important to know the expressivity of functions computable by\ndeep neural networks in order to understand the advantage of deepening neural\nnetworks. Unless deep neural networks have enough expressivity, they cannot\nhave good performance even though learning is successful. In this situation,\nthe proposed criteria contribute to understanding the advantage of deepening\nneural networks since they can evaluate the expressivity independently from the\nefficiency of learning. The first criterion shows the approximation accuracy of\ndeep neural networks to the target function. This criterion has the background\nthat the goal of deep learning is approximating the target function by deep\nneural networks. The second criterion shows the property of linear regions of\nfunctions computable by deep neural networks. This criterion has the background\nthat deep neural networks whose activation functions are piecewise linear are\nalso piecewise linear. Furthermore, by the two criteria, we show that to\nincrease layers is more effective than to increase units at each layer on\nimproving the expressivity of deep neural networks.\n",
        "published": "2020",
        "authors": [
            "Yasushi Esaki",
            "Yuta Nakahara",
            "Toshiyasu Matsushima"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.11510v1",
        "title": "EPNE: Evolutionary Pattern Preserving Network Embedding",
        "abstract": "  Information networks are ubiquitous and are ideal for modeling relational\ndata. Networks being sparse and irregular, network embedding algorithms have\ncaught the attention of many researchers, who came up with numerous embeddings\nalgorithms in static networks. Yet in real life, networks constantly evolve\nover time. Hence, evolutionary patterns, namely how nodes develop itself over\ntime, would serve as a powerful complement to static structures in embedding\nnetworks, on which relatively few works focus. In this paper, we propose EPNE,\na temporal network embedding model preserving evolutionary patterns of the\nlocal structure of nodes. In particular, we analyze evolutionary patterns with\nand without periodicity and design strategies correspondingly to model such\npatterns in time-frequency domains based on causal convolutions. In addition,\nwe propose a temporal objective function which is optimized simultaneously with\nproximity ones such that both temporal and structural information are\npreserved. With the adequate modeling of temporal information, our model is\nable to outperform other competitive methods in various prediction tasks.\n",
        "published": "2020",
        "authors": [
            "Junshan Wang",
            "Yilun Jin",
            "Guojie Song",
            "Xiaojun Ma"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.13266v1",
        "title": "Disentangled Neural Architecture Search",
        "abstract": "  Neural architecture search has shown its great potential in various areas\nrecently. However, existing methods rely heavily on a black-box controller to\nsearch architectures, which suffers from the serious problem of lacking\ninterpretability. In this paper, we propose disentangled neural architecture\nsearch (DNAS) which disentangles the hidden representation of the controller\ninto semantically meaningful concepts, making the neural architecture search\nprocess interpretable. Based on systematical study, we discover the correlation\nbetween network architecture and its performance, and propose a dense-sampling\nstrategy to conduct a targeted search in promising regions that may generate\nwell-performing architectures. We show that: 1) DNAS successfully disentangles\nthe architecture representations, including operation selection, skip\nconnections, and number of layers. 2) Benefiting from interpretability, DNAS\ncan find excellent architectures under different FLOPS restrictions flexibly.\n3) Dense-sampling leads to neural architecture search with higher efficiency\nand better performance. On the NASBench-101 dataset, DNAS achieves\nstate-of-the-art performance of 94.21% using less than 1/13 computational cost\nof baseline methods. On ImageNet dataset, DNAS discovers the competitive\narchitectures that achieves 22.7% test error. our method provides a new\nperspective of understanding neural architecture search.\n",
        "published": "2020",
        "authors": [
            "Xinyue Zheng",
            "Peng Wang",
            "Qigang Wang",
            "Zhongchao Shi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.14670v1",
        "title": "An Online Learning Algorithm for a Neuro-Fuzzy Classifier with\n  Mixed-Attribute Data",
        "abstract": "  General fuzzy min-max neural network (GFMMNN) is one of the efficient\nneuro-fuzzy systems for data classification. However, one of the downsides of\nits original learning algorithms is the inability to handle and learn from the\nmixed-attribute data. While categorical features encoding methods can be used\nwith the GFMMNN learning algorithms, they exhibit a lot of shortcomings. Other\napproaches proposed in the literature are not suitable for on-line learning as\nthey require entire training data available in the learning phase. With the\nrapid change in the volume and velocity of streaming data in many application\nareas, it is increasingly required that the constructed models can learn and\nadapt to the continuous data changes in real-time without the need for their\nfull retraining or access to the historical data. This paper proposes an\nextended online learning algorithm for the GFMMNN. The proposed method can\nhandle the datasets with both continuous and categorical features. The\nextensive experiments confirmed superior and stable classification performance\nof the proposed approach in comparison to other relevant learning algorithms\nfor the GFMM model.\n",
        "published": "2020",
        "authors": [
            "Thanh Tung Khuat",
            "Bogdan Gabrys"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.14695v2",
        "title": "Global convergence of Negative Correlation Extreme Learning Machine",
        "abstract": "  Ensemble approaches introduced in the Extreme Learning Machine (ELM)\nliterature mainly come from methods that relies on data sampling procedures,\nunder the assumption that the training data are heterogeneously enough to set\nup diverse base learners. To overcome this assumption, it was proposed an ELM\nensemble method based on the Negative Correlation Learning (NCL) framework,\ncalled Negative Correlation Extreme Learning Machine (NCELM). This model works\nin two stages: i) different ELMs are generated as base learners with random\nweights in the hidden layer, and ii) a NCL penalty term with the information of\nthe ensemble prediction is introduced in each ELM minimization problem,\nupdating the base learners, iii) second step is iterated until the ensemble\nconverges.\n  Although this NCL ensemble method was validated by an experimental study with\nmultiple benchmark datasets, no information was given on the conditions about\nthis convergence. This paper mathematically presents the sufficient conditions\nto guarantee the global convergence of NCELM. The update of the ensemble in\neach iteration is defined as a contraction mapping function, and through Banach\ntheorem, global convergence of the ensemble is proved.\n",
        "published": "2020",
        "authors": [
            "Carlos Perales-Gonz\u00e1lez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.02159v2",
        "title": "Reverse engineering learned optimizers reveals known and novel\n  mechanisms",
        "abstract": "  Learned optimizers are algorithms that can themselves be trained to solve\noptimization problems. In contrast to baseline optimizers (such as momentum or\nAdam) that use simple update rules derived from theoretical principles, learned\noptimizers use flexible, high-dimensional, nonlinear parameterizations.\nAlthough this can lead to better performance in certain settings, their inner\nworkings remain a mystery. How is a learned optimizer able to outperform a well\ntuned baseline? Has it learned a sophisticated combination of existing\noptimization techniques, or is it implementing completely new behavior? In this\nwork, we address these questions by careful analysis and visualization of\nlearned optimizers. We study learned optimizers trained from scratch on three\ndisparate tasks, and discover that they have learned interpretable mechanisms,\nincluding: momentum, gradient clipping, learning rate schedules, and a new form\nof learning rate adaptation. Moreover, we show how the dynamics of learned\noptimizers enables these behaviors. Our results help elucidate the previously\nmurky understanding of how learned optimizers work, and establish tools for\ninterpreting future learned optimizers.\n",
        "published": "2020",
        "authors": [
            "Niru Maheswaranathan",
            "David Sussillo",
            "Luke Metz",
            "Ruoxi Sun",
            "Jascha Sohl-Dickstein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.03535v1",
        "title": "Contrastive Topographic Models: Energy-based density models applied to\n  the understanding of sensory coding and cortical topography",
        "abstract": "  We address the problem of building theoretical models that help elucidate the\nfunction of the visual brain at computational/algorithmic and\nstructural/mechanistic levels. We seek to understand how the receptive fields\nand topographic maps found in visual cortical areas relate to underlying\ncomputational desiderata. We view the development of sensory systems from the\npopular perspective of probability density estimation; this is motivated by the\nnotion that an effective internal representational scheme is likely to reflect\nthe statistical structure of the environment in which an organism lives. We\napply biologically based constraints on elements of the model.\n  The thesis begins by surveying the relevant literature from the fields of\nneurobiology, theoretical neuroscience, and machine learning. After this review\nwe present our main theoretical and algorithmic developments: we propose a\nclass of probabilistic models, which we refer to as \"energy-based models\", and\nshow equivalences between this framework and various other types of\nprobabilistic model such as Markov random fields and factor graphs; we also\ndevelop and discuss approximate algorithms for performing maximum likelihood\nlearning and inference in our energy based models. The rest of the thesis is\nthen concerned with exploring specific instantiations of such models. By\nperforming constrained optimisation of model parameters to maximise the\nlikelihood of appropriate, naturalistic datasets we are able to qualitatively\nreproduce many of the receptive field and map properties found in vivo, whilst\nsimultaneously learning about statistical regularities in the data.\n",
        "published": "2020",
        "authors": [
            "Simon Osindero"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.03842v1",
        "title": "Universal Activation Function For Machine Learning",
        "abstract": "  This article proposes a Universal Activation Function (UAF) that achieves\nnear optimal performance in quantification, classification, and reinforcement\nlearning (RL) problems. For any given problem, the optimization algorithms are\nable to evolve the UAF to a suitable activation function by tuning the UAF's\nparameters. For the CIFAR-10 classification and VGG-8, the UAF converges to the\nMish like activation function, which has near optimal performance $F_{1} =\n0.9017\\pm0.0040$ when compared to other activation functions. For the\nquantification of simulated 9-gas mixtures in 30 dB signal-to-noise ratio (SNR)\nenvironments, the UAF converges to the identity function, which has near\noptimal root mean square error of $0.4888 \\pm 0.0032$ $\\mu M$. In the\nBipedalWalker-v2 RL dataset, the UAF achieves the 250 reward in $961 \\pm 193$\nepochs, which proves that the UAF converges in the lowest number of epochs.\nFurthermore, the UAF converges to a new activation function in the\nBipedalWalker-v2 RL dataset.\n",
        "published": "2020",
        "authors": [
            "Brosnan Yuen",
            "Minh Tu Hoang",
            "Xiaodai Dong",
            "Tao Lu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.04720v1",
        "title": "Improving Neural Network Training in Low Dimensional Random Bases",
        "abstract": "  Stochastic Gradient Descent (SGD) has proven to be remarkably effective in\noptimizing deep neural networks that employ ever-larger numbers of parameters.\nYet, improving the efficiency of large-scale optimization remains a vital and\nhighly active area of research. Recent work has shown that deep neural networks\ncan be optimized in randomly-projected subspaces of much smaller dimensionality\nthan their native parameter space. While such training is promising for more\nefficient and scalable optimization schemes, its practical application is\nlimited by inferior optimization performance. Here, we improve on recent random\nsubspace approaches as follows: Firstly, we show that keeping the random\nprojection fixed throughout training is detrimental to optimization. We propose\nre-drawing the random subspace at each step, which yields significantly better\nperformance. We realize further improvements by applying independent\nprojections to different parts of the network, making the approximation more\nefficient as network dimensionality grows. To implement these experiments, we\nleverage hardware-accelerated pseudo-random number generation to construct the\nrandom projections on-demand at every optimization step, allowing us to\ndistribute the computation of independent random directions across multiple\nworkers with shared random seeds. This yields significant reductions in memory\nand is up to 10 times faster for the workloads in question.\n",
        "published": "2020",
        "authors": [
            "Frithjof Gressmann",
            "Zach Eaton-Rosen",
            "Carlo Luschi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.07248v2",
        "title": "Self Normalizing Flows",
        "abstract": "  Efficient gradient computation of the Jacobian determinant term is a core\nproblem in many machine learning settings, and especially so in the normalizing\nflow framework. Most proposed flow models therefore either restrict to a\nfunction class with easy evaluation of the Jacobian determinant, or an\nefficient estimator thereof. However, these restrictions limit the performance\nof such density models, frequently requiring significant depth to reach desired\nperformance levels. In this work, we propose Self Normalizing Flows, a flexible\nframework for training normalizing flows by replacing expensive terms in the\ngradient by learned approximate inverses at each layer. This reduces the\ncomputational complexity of each layer's exact update from $\\mathcal{O}(D^3)$\nto $\\mathcal{O}(D^2)$, allowing for the training of flow architectures which\nwere otherwise computationally infeasible, while also providing efficient\nsampling. We show experimentally that such models are remarkably stable and\noptimize to similar data likelihood values as their exact gradient\ncounterparts, while training more quickly and surpassing the performance of\nfunctionally constrained counterparts.\n",
        "published": "2020",
        "authors": [
            "T. Anderson Keller",
            "Jorn W. T. Peters",
            "Priyank Jaini",
            "Emiel Hoogeboom",
            "Patrick Forr\u00e9",
            "Max Welling"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.08895v1",
        "title": "ZORB: A Derivative-Free Backpropagation Algorithm for Neural Networks",
        "abstract": "  Gradient descent and backpropagation have enabled neural networks to achieve\nremarkable results in many real-world applications. Despite ongoing success,\ntraining a neural network with gradient descent can be a slow and strenuous\naffair. We present a simple yet faster training algorithm called Zeroth-Order\nRelaxed Backpropagation (ZORB). Instead of calculating gradients, ZORB uses the\npseudoinverse of targets to backpropagate information. ZORB is designed to\nreduce the time required to train deep neural networks without penalizing\nperformance. To illustrate the speed up, we trained a feed-forward neural\nnetwork with 11 layers on MNIST and observed that ZORB converged 300 times\nfaster than Adam while achieving a comparable error rate, without any\nhyperparameter tuning. We also broaden the scope of ZORB to convolutional\nneural networks, and apply it to subsamples of the CIFAR-10 dataset.\nExperiments on standard classification and regression benchmarks demonstrate\nZORB's advantage over traditional backpropagation with Gradient Descent.\n",
        "published": "2020",
        "authors": [
            "Varun Ranganathan",
            "Alex Lewandowski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.12428v2",
        "title": "Align, then memorise: the dynamics of learning with feedback alignment",
        "abstract": "  Direct Feedback Alignment (DFA) is emerging as an efficient and biologically\nplausible alternative to the ubiquitous backpropagation algorithm for training\ndeep neural networks. Despite relying on random feedback weights for the\nbackward pass, DFA successfully trains state-of-the-art models such as\nTransformers. On the other hand, it notoriously fails to train convolutional\nnetworks. An understanding of the inner workings of DFA to explain these\ndiverging results remains elusive. Here, we propose a theory for the success of\nDFA. We first show that learning in shallow networks proceeds in two steps: an\nalignment phase, where the model adapts its weights to align the approximate\ngradient with the true gradient of the loss function, is followed by a\nmemorisation phase, where the model focuses on fitting the data. This two-step\nprocess has a degeneracy breaking effect: out of all the low-loss solutions in\nthe landscape, a network trained with DFA naturally converges to the solution\nwhich maximises gradient alignment. We also identify a key quantity underlying\nalignment in deep linear networks: the conditioning of the alignment matrices.\nThe latter enables a detailed understanding of the impact of data structure on\nalignment, and suggests a simple explanation for the well-known failure of DFA\nto train convolutional neural networks. Numerical experiments on MNIST and\nCIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and\nshow that the align-then-memorise process occurs sequentially from the bottom\nlayers of the network to the top.\n",
        "published": "2020",
        "authors": [
            "Maria Refinetti",
            "St\u00e9phane d'Ascoli",
            "Ruben Ohana",
            "Sebastian Goldt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.14439v3",
        "title": "Scaling down Deep Learning",
        "abstract": "  Though deep learning models have taken on commercial and political relevance,\nmany aspects of their training and operation remain poorly understood. This has\nsparked interest in \"science of deep learning\" projects, many of which are run\nat scale and require enormous amounts of time, money, and electricity. But how\nmuch of this research really needs to occur at scale? In this paper, we\nintroduce MNIST-1D: a minimalist, low-memory, and low-compute alternative to\nclassic deep learning benchmarks. The training examples are 20 times smaller\nthan MNIST examples yet they differentiate more clearly between linear,\nnonlinear, and convolutional models which attain 32, 68, and 94% accuracy\nrespectively (these models obtain 94, 99+, and 99+% on MNIST). Then we present\nexample use cases which include measuring the spatial inductive biases of\nlottery tickets, observing deep double descent, and metalearning an activation\nfunction.\n",
        "published": "2020",
        "authors": [
            "Sam Greydanus"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.00152v1",
        "title": "Every Model Learned by Gradient Descent Is Approximately a Kernel\n  Machine",
        "abstract": "  Deep learning's successes are often attributed to its ability to\nautomatically discover new representations of the data, rather than relying on\nhandcrafted features like other learning methods. We show, however, that deep\nnetworks learned by the standard gradient descent algorithm are in fact\nmathematically approximately equivalent to kernel machines, a learning method\nthat simply memorizes the data and uses it directly for prediction via a\nsimilarity function (the kernel). This greatly enhances the interpretability of\ndeep network weights, by elucidating that they are effectively a superposition\nof the training examples. The network architecture incorporates knowledge of\nthe target function into the kernel. This improved understanding should lead to\nbetter learning algorithms.\n",
        "published": "2020",
        "authors": [
            "Pedro Domingos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.04231v5",
        "title": "A Deep Generative Model for Molecule Optimization via One Fragment\n  Modification",
        "abstract": "  Molecule optimization is a critical step in drug development to improve\ndesired properties of drug candidates through chemical modification. We\ndeveloped a novel deep generative model Modof over molecular graphs for\nmolecule optimization. Modof modifies a given molecule through the prediction\nof a single site of disconnection at the molecule and the removal and/or\naddition of fragments at that site. A pipeline of multiple, identical Modof\nmodels is implemented into Modof-pipe to modify an input molecule at multiple\ndisconnection sites. Here we show that Modof-pipe is able to retain major\nmolecular scaffolds, allow controls over intermediate optimization steps and\nbetter constrain molecule similarities. Modof-pipe outperforms the\nstate-of-the-art methods on benchmark datasets: without molecular similarity\nconstraints, Modof-pipe achieves 81.2% improvement in octanol-water partition\ncoefficient penalized by synthetic accessibility and ring size; and 51.2%,\n25.6% and 9.2% improvement if the optimized molecules are at least 0.2, 0.4 and\n0.6 similar to those before optimization, respectively. Modof-pipe is further\nenhanced into Modof-pipem to allow modifying one molecule to multiple optimized\nones. Modof-pipem achieves additional performance improvement as at least 17.8%\nbetter than Modof-pipe.\n",
        "published": "2020",
        "authors": [
            "Ziqi Chen",
            "Martin Renqiang Min",
            "Srinivasan Parthasarathy",
            "Xia Ning"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.04322v2",
        "title": "Quality-Diversity Optimization: a novel branch of stochastic\n  optimization",
        "abstract": "  Traditional optimization algorithms search for a single global optimum that\nmaximizes (or minimizes) the objective function. Multimodal optimization\nalgorithms search for the highest peaks in the search space that can be more\nthan one. Quality-Diversity algorithms are a recent addition to the\nevolutionary computation toolbox that do not only search for a single set of\nlocal optima, but instead try to illuminate the search space. In effect, they\nprovide a holistic view of how high-performing solutions are distributed\nthroughout a search space. The main differences with multimodal optimization\nalgorithms are that (1) Quality-Diversity typically works in the behavioral\nspace (or feature space), and not in the genotypic (or parameter) space, and\n(2) Quality-Diversity attempts to fill the whole behavior space, even if the\nniche is not a peak in the fitness landscape. In this chapter, we provide a\ngentle introduction to Quality-Diversity optimization, discuss the main\nrepresentative algorithms, and the main current topics under consideration in\nthe community. Throughout the chapter, we also discuss several successful\napplications of Quality-Diversity algorithms, including deep learning,\nrobotics, and reinforcement learning.\n",
        "published": "2020",
        "authors": [
            "Konstantinos Chatzilygeroudis",
            "Antoine Cully",
            "Vassilis Vassiliades",
            "Jean-Baptiste Mouret"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.08180v2",
        "title": "Squirrel: A Switching Hyperparameter Optimizer",
        "abstract": "  In this short note, we describe our submission to the NeurIPS 2020 BBO\nchallenge. Motivated by the fact that different optimizers work well on\ndifferent problems, our approach switches between different optimizers. Since\nthe team names on the competition's leaderboard were randomly generated\n\"alliteration nicknames\", consisting of an adjective and an animal with the\nsame initial letter, we called our approach the Switching Squirrel, or here,\nshort, Squirrel.\n",
        "published": "2020",
        "authors": [
            "Noor Awad",
            "Gresa Shala",
            "Difan Deng",
            "Neeratyoy Mallik",
            "Matthias Feurer",
            "Katharina Eggensperger",
            "Andre' Biedenkapp",
            "Diederick Vermetten",
            "Hao Wang",
            "Carola Doerr",
            "Marius Lindauer",
            "Frank Hutter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.09816v3",
        "title": "Towards Understanding Ensemble, Knowledge Distillation and\n  Self-Distillation in Deep Learning",
        "abstract": "  We formally study how ensemble of deep learning models can improve test\naccuracy, and how the superior performance of ensemble can be distilled into a\nsingle model using knowledge distillation. We consider the challenging case\nwhere the ensemble is simply an average of the outputs of a few independently\ntrained neural networks with the SAME architecture, trained using the SAME\nalgorithm on the SAME data set, and they only differ by the random seeds used\nin the initialization.\n  We show that ensemble/knowledge distillation in Deep Learning works very\ndifferently from traditional learning theory (such as boosting or NTKs, neural\ntangent kernels). To properly understand them, we develop a theory showing that\nwhen data has a structure we refer to as ``multi-view'', then ensemble of\nindependently trained neural networks can provably improve test accuracy, and\nsuch superior test accuracy can also be provably distilled into a single model\nby training a single model to match the output of the ensemble instead of the\ntrue label. Our result sheds light on how ensemble works in deep learning in a\nway that is completely different from traditional theorems, and how the ``dark\nknowledge'' is hidden in the outputs of the ensemble and can be used in\ndistillation. In the end, we prove that self-distillation can also be viewed as\nimplicitly combining ensemble and knowledge distillation to improve test\naccuracy.\n",
        "published": "2020",
        "authors": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.12294v2",
        "title": "Evolutionary Variational Optimization of Generative Models",
        "abstract": "  We combine two popular optimization approaches to derive learning algorithms\nfor generative models: variational optimization and evolutionary algorithms.\nThe combination is realized for generative models with discrete latents by\nusing truncated posteriors as the family of variational distributions. The\nvariational parameters of truncated posteriors are sets of latent states. By\ninterpreting these states as genomes of individuals and by using the\nvariational lower bound to define a fitness, we can apply evolutionary\nalgorithms to realize the variational loop. The used variational distributions\nare very flexible and we show that evolutionary algorithms can effectively and\nefficiently optimize the variational bound. Furthermore, the variational loop\nis generally applicable (\"black box\") with no analytical derivations required.\nTo show general applicability, we apply the approach to three generative models\n(we use noisy-OR Bayes Nets, Binary Sparse Coding, and Spike-and-Slab Sparse\nCoding). To demonstrate effectiveness and efficiency of the novel variational\napproach, we use the standard competitive benchmarks of image denoising and\ninpainting. The benchmarks allow quantitative comparisons to a wide range of\nmethods including probabilistic approaches, deep deterministic and generative\nnetworks, and non-local image processing methods. In the category of\n\"zero-shot\" learning (when only the corrupted image is used for training), we\nobserved the evolutionary variational algorithm to significantly improve the\nstate-of-the-art in many benchmark settings. For one well-known inpainting\nbenchmark, we also observed state-of-the-art performance across all categories\nof algorithms although we only train on the corrupted image. In general, our\ninvestigations highlight the importance of research on optimization methods for\ngenerative models to achieve performance improvements.\n",
        "published": "2020",
        "authors": [
            "Jakob Drefs",
            "Enrico Guiraud",
            "J\u00f6rg L\u00fccke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.15059v2",
        "title": "Ensembles of Localised Models for Time Series Forecasting",
        "abstract": "  With large quantities of data typically available nowadays, forecasting\nmodels that are trained across sets of time series, known as Global Forecasting\nModels (GFM), are regularly outperforming traditional univariate forecasting\nmodels that work on isolated series. As GFMs usually share the same set of\nparameters across all time series, they often have the problem of not being\nlocalised enough to a particular series, especially in situations where\ndatasets are heterogeneous. We study how ensembling techniques can be used with\ngeneric GFMs and univariate models to solve this issue. Our work systematises\nand compares relevant current approaches, namely clustering series and training\nseparate submodels per cluster, the so-called ensemble of specialists approach,\nand building heterogeneous ensembles of global and local models. We fill some\ngaps in the existing GFM localisation approaches, in particular by\nincorporating varied clustering techniques such as feature-based clustering,\ndistance-based clustering and random clustering, and generalise them to use\ndifferent underlying GFM model types. We then propose a new methodology of\nclustered ensembles where we train multiple GFMs on different clusters of\nseries, obtained by changing the number of clusters and cluster seeds. Using\nFeed-forward Neural Networks, Recurrent Neural Networks, and Pooled Regression\nmodels as the underlying GFMs, in our evaluation on eight publicly available\ndatasets, the proposed models are able to achieve significantly higher accuracy\nthan baseline GFM models and univariate forecasting methods.\n",
        "published": "2020",
        "authors": [
            "Rakshitha Godahewa",
            "Kasun Bandara",
            "Geoffrey I. Webb",
            "Slawek Smyl",
            "Christoph Bergmeir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.00434v2",
        "title": "The Connection Between Approximation, Depth Separation and Learnability\n  in Neural Networks",
        "abstract": "  Several recent works have shown separation results between deep neural\nnetworks, and hypothesis classes with inferior approximation capacity such as\nshallow networks or kernel classes. On the other hand, the fact that deep\nnetworks can efficiently express a target function does not mean that this\ntarget function can be learned efficiently by deep neural networks. In this\nwork we study the intricate connection between learnability and approximation\ncapacity. We show that learnability with deep networks of a target function\ndepends on the ability of simpler classes to approximate the target.\nSpecifically, we show that a necessary condition for a function to be learnable\nby gradient descent on deep neural networks is to be able to approximate the\nfunction, at least in a weak sense, with shallow neural networks. We also show\nthat a class of functions can be learned by an efficient statistical query\nalgorithm if and only if it can be approximated in a weak sense by some kernel\nclass. We give several examples of functions which demonstrate depth\nseparation, and conclude that they cannot be efficiently learned, even by a\nhypothesis class that can efficiently approximate them.\n",
        "published": "2021",
        "authors": [
            "Eran Malach",
            "Gilad Yehudai",
            "Shai Shalev-Shwartz",
            "Ohad Shamir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.01621v4",
        "title": "Depth separation beyond radial functions",
        "abstract": "  High-dimensional depth separation results for neural networks show that\ncertain functions can be efficiently approximated by two-hidden-layer networks\nbut not by one-hidden-layer ones in high-dimensions $d$. Existing results of\nthis type mainly focus on functions with an underlying radial or\none-dimensional structure, which are usually not encountered in practice. The\nfirst contribution of this paper is to extend such results to a more general\nclass of functions, namely functions with piece-wise oscillatory structure, by\nbuilding on the proof strategy of (Eldan and Shamir, 2016). We complement these\nresults by showing that, if the domain radius and the rate of oscillation of\nthe objective function are constant, then approximation by one-hidden-layer\nnetworks holds at a $\\mathrm{poly}(d)$ rate for any fixed error threshold.\n  A common theme in the proofs of depth-separation results is the fact that\none-hidden-layer networks fail to approximate high-energy functions whose\nFourier representation is spread in the domain. On the other hand, existing\napproximation results of a function by one-hidden-layer neural networks rely on\nthe function having a sparse Fourier representation. The choice of the domain\nalso represents a source of gaps between upper and lower approximation bounds.\nFocusing on a fixed approximation domain, namely the sphere $\\mathbb{S}^{d-1}$\nin dimension $d$, we provide a characterisation of both functions which are\nefficiently approximable by one-hidden-layer networks and of functions which\nare provably not, in terms of their Fourier expansion.\n",
        "published": "2021",
        "authors": [
            "Luca Venturi",
            "Samy Jelassi",
            "Tristan Ozuch",
            "Joan Bruna"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.02336v2",
        "title": "On the Approximation Power of Two-Layer Networks of Random ReLUs",
        "abstract": "  This paper considers the following question: how well can depth-two ReLU\nnetworks with randomly initialized bottom-level weights represent smooth\nfunctions? We give near-matching upper- and lower-bounds for\n$L_2$-approximation in terms of the Lipschitz constant, the desired accuracy,\nand the dimension of the problem, as well as similar results in terms of\nSobolev norms. Our positive results employ tools from harmonic analysis and\nridgelet representation theory, while our lower-bounds are based on (robust\nversions of) dimensionality arguments.\n",
        "published": "2021",
        "authors": [
            "Daniel Hsu",
            "Clayton Sanford",
            "Rocco A. Servedio",
            "Emmanouil-Vasileios Vlatakis-Gkaragkounis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.03280v1",
        "title": "Multi-Sample Online Learning for Spiking Neural Networks based on\n  Generalized Expectation Maximization",
        "abstract": "  Spiking Neural Networks (SNNs) offer a novel computational paradigm that\ncaptures some of the efficiency of biological brains by processing through\nbinary neural dynamic activations. Probabilistic SNN models are typically\ntrained to maximize the likelihood of the desired outputs by using unbiased\nestimates of the log-likelihood gradients. While prior work used single-sample\nestimators obtained from a single run of the network, this paper proposes to\nleverage multiple compartments that sample independent spiking signals while\nsharing synaptic weights. The key idea is to use these signals to obtain more\naccurate statistical estimates of the log-likelihood training criterion, as\nwell as of its gradient. The approach is based on generalized\nexpectation-maximization (GEM), which optimizes a tighter approximation of the\nlog-likelihood using importance sampling. The derived online learning algorithm\nimplements a three-factor rule with global per-compartment learning signals.\nExperimental results on a classification task on the neuromorphic MNIST-DVS\ndata set demonstrate significant improvements in terms of log-likelihood,\naccuracy, and calibration when increasing the number of compartments used for\ntraining and inference.\n",
        "published": "2021",
        "authors": [
            "Hyeryung Jang",
            "Osvaldo Simeone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.08792v2",
        "title": "Chance-Constrained Active Inference",
        "abstract": "  Active Inference (ActInf) is an emerging theory that explains perception and\naction in biological agents, in terms of minimizing a free energy bound on\nBayesian surprise. Goal-directed behavior is elicited by introducing prior\nbeliefs on the underlying generative model. In contrast to prior beliefs, which\nconstrain all realizations of a random variable, we propose an alternative\napproach through chance constraints, which allow for a (typically small)\nprobability of constraint violation, and demonstrate how such constraints can\nbe used as intrinsic drivers for goal-directed behavior in ActInf. We\nillustrate how chance-constrained ActInf weights all imposed (prior)\nconstraints on the generative model, allowing e.g., for a trade-off between\nrobust control and empirical chance constraint violation. Secondly, we\ninterpret the proposed solution within a message passing framework.\nInterestingly, the message passing interpretation is not only relevant to the\ncontext of ActInf, but also provides a general purpose approach that can\naccount for chance constraints on graphical models. The chance constraint\nmessage updates can then be readily combined with other pre-derived message\nupdate rules, without the need for custom derivations. The proposed\nchance-constrained message passing framework thus accelerates the search for\nworkable models in general, and can be used to complement message-passing\nformulations on generative neural models.\n",
        "published": "2021",
        "authors": [
            "Thijs van de Laar",
            "Ismail Senoz",
            "Ay\u00e7a \u00d6z\u00e7elikkale",
            "Henk Wymeersch"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.10148v1",
        "title": "A theory of capacity and sparse neural encoding",
        "abstract": "  Motivated by biological considerations, we study sparse neural maps from an\ninput layer to a target layer with sparse activity, and specifically the\nproblem of storing $K$ input-target associations $(x,y)$, or memories, when the\ntarget vectors $y$ are sparse. We mathematically prove that $K$ undergoes a\nphase transition and that in general, and somewhat paradoxically, sparsity in\nthe target layers increases the storage capacity of the map. The target vectors\ncan be chosen arbitrarily, including in random fashion, and the memories can be\nboth encoded and decoded by networks trained using local learning rules,\nincluding the simple Hebb rule. These results are robust under a variety of\nstatistical assumptions on the data. The proofs rely on elegant properties of\nrandom polytopes and sub-gaussian random vector variables. Open problems and\nconnections to capacity theories and polynomial threshold maps are discussed.\n",
        "published": "2021",
        "authors": [
            "Pierre Baldi",
            "Roman Vershynin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.01148v3",
        "title": "E$^2$CM: Early Exit via Class Means for Efficient Supervised and\n  Unsupervised Learning",
        "abstract": "  State-of-the-art neural networks with early exit mechanisms often need\nconsiderable amount of training and fine tuning to achieve good performance\nwith low computational cost. We propose a novel early exit technique, Early\nExit Class Means (E$^2$CM), based on class means of samples. Unlike most\nexisting schemes, E$^2$CM does not require gradient-based training of internal\nclassifiers and it does not modify the base network by any means. This makes it\nparticularly useful for neural network training in low-power devices, as in\nwireless edge networks. We evaluate the performance and overheads of E$^2$CM\nover various base neural networks such as MobileNetV3, EfficientNet, ResNet,\nand datasets such as CIFAR-100, ImageNet, and KMNIST. Our results show that,\ngiven a fixed training time budget, E$^2$CM achieves higher accuracy as\ncompared to existing early exit mechanisms. Moreover, if there are no\nlimitations on the training time budget, E$^2$CM can be combined with an\nexisting early exit scheme to boost the latter's performance, achieving a\nbetter trade-off between computational cost and network accuracy. We also show\nthat E$^2$CM can be used to decrease the computational cost in unsupervised\nlearning tasks.\n",
        "published": "2021",
        "authors": [
            "Alperen G\u00f6rmez",
            "Venkat R. Dasari",
            "Erdem Koyuncu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.08277v1",
        "title": "Representation Theorem for Matrix Product States",
        "abstract": "  In this work, we investigate the universal representation capacity of the\nMatrix Product States (MPS) from the perspective of boolean functions and\ncontinuous functions. We show that MPS can accurately realize arbitrary boolean\nfunctions by providing a construction method of the corresponding MPS structure\nfor an arbitrarily given boolean gate. Moreover, we prove that the function\nspace of MPS with the scale-invariant sigmoidal activation is dense in the\nspace of continuous functions defined on a compact subspace of the\n$n$-dimensional real coordinate space $\\mathbb{R^{n}}$. We study the relation\nbetween MPS and neural networks and show that the MPS with a scale-invariant\nsigmoidal function is equivalent to a one-hidden-layer neural network equipped\nwith a kernel function. We construct the equivalent neural networks for several\nspecific MPS models and show that non-linear kernels such as the polynomial\nkernel which introduces the couplings between different components of the input\ninto the model appear naturally in the equivalent neural networks. At last, we\ndiscuss the realization of the Gaussian Process (GP) with infinitely wide MPS\nby studying their equivalent neural networks.\n",
        "published": "2021",
        "authors": [
            "Erdong Guo",
            "David Draper"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.01177v2",
        "title": "How Powerful are Performance Predictors in Neural Architecture Search?",
        "abstract": "  Early methods in the rapidly developing field of neural architecture search\n(NAS) required fully training thousands of neural networks. To reduce this\nextreme computational cost, dozens of techniques have since been proposed to\npredict the final performance of neural architectures. Despite the success of\nsuch performance prediction methods, it is not well-understood how different\nfamilies of techniques compare to one another, due to the lack of an\nagreed-upon evaluation metric and optimization for different constraints on the\ninitialization time and query time. In this work, we give the first large-scale\nstudy of performance predictors by analyzing 31 techniques ranging from\nlearning curve extrapolation, to weight-sharing, to supervised learning, to\n\"zero-cost\" proxies. We test a number of correlation- and rank-based\nperformance measures in a variety of settings, as well as the ability of each\ntechnique to speed up predictor-based NAS frameworks. Our results act as\nrecommendations for the best predictors to use in different settings, and we\nshow that certain families of predictors can be combined to achieve even better\npredictive power, opening up promising research directions. Our code, featuring\na library of 31 performance predictors, is available at\nhttps://github.com/automl/naslib.\n",
        "published": "2021",
        "authors": [
            "Colin White",
            "Arber Zela",
            "Binxin Ru",
            "Yang Liu",
            "Frank Hutter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.04710v1",
        "title": "Pyramidal Reservoir Graph Neural Network",
        "abstract": "  We propose a deep Graph Neural Network (GNN) model that alternates two types\nof layers. The first type is inspired by Reservoir Computing (RC) and generates\nnew vertex features by iterating a non-linear map until it converges to a fixed\npoint. The second type of layer implements graph pooling operations, that\ngradually reduce the support graph and the vertex features, and further improve\nthe computational efficiency of the RC-based GNN. The architecture is,\ntherefore, pyramidal. In the last layer, the features of the remaining vertices\nare combined into a single vector, which represents the graph embedding.\nThrough a mathematical derivation introduced in this paper, we show formally\nhow graph pooling can reduce the computational complexity of the model and\nspeed-up the convergence of the dynamical updates of the vertex features. Our\nproposed approach to the design of RC-based GNNs offers an advantageous and\nprincipled trade-off between accuracy and complexity, which we extensively\ndemonstrate in experiments on a large set of graph datasets.\n",
        "published": "2021",
        "authors": [
            "Filippo Maria Bianchi",
            "Claudio Gallicchio",
            "Alessio Micheli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.05048v1",
        "title": "Rank-R FNN: A Tensor-Based Learning Model for High-Order Data\n  Classification",
        "abstract": "  An increasing number of emerging applications in data science and engineering\nare based on multidimensional and structurally rich data. The irregularities,\nhowever, of high-dimensional data often compromise the effectiveness of\nstandard machine learning algorithms. We hereby propose the Rank-R Feedforward\nNeural Network (FNN), a tensor-based nonlinear learning model that imposes\nCanonical/Polyadic decomposition on its parameters, thereby offering two core\nadvantages compared to typical machine learning methods. First, it handles\ninputs as multilinear arrays, bypassing the need for vectorization, and can\nthus fully exploit the structural information along every data dimension.\nMoreover, the number of the model's trainable parameters is substantially\nreduced, making it very efficient for small sample setting problems. We\nestablish the universal approximation and learnability properties of Rank-R\nFNN, and we validate its performance on real-world hyperspectral datasets.\nExperimental evaluations show that Rank-R FNN is a computationally inexpensive\nalternative of ordinary FNN that achieves state-of-the-art performance on\nhigher-order tensor data.\n",
        "published": "2021",
        "authors": [
            "Konstantinos Makantasis",
            "Alexandros Georgogiannis",
            "Athanasios Voulodimos",
            "Ioannis Georgoulas",
            "Anastasios Doulamis",
            "Nikolaos Doulamis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.05089v2",
        "title": "The World as a Graph: Improving El Ni\u00f1o Forecasts with Graph Neural\n  Networks",
        "abstract": "  Deep learning-based models have recently outperformed state-of-the-art\nseasonal forecasting models, such as for predicting El Ni\\~no-Southern\nOscillation (ENSO). However, current deep learning models are based on\nconvolutional neural networks which are difficult to interpret and can fail to\nmodel large-scale atmospheric patterns. In comparison, graph neural networks\n(GNNs) are capable of modeling large-scale spatial dependencies and are more\ninterpretable due to the explicit modeling of information flow through edge\nconnections. We propose the first application of graph neural networks to\nseasonal forecasting. We design a novel graph connectivity learning module that\nenables our GNN model to learn large-scale spatial interactions jointly with\nthe actual ENSO forecasting task. Our model, \\graphino, outperforms\nstate-of-the-art deep learning-based models for forecasts up to six months\nahead. Additionally, we show that our model is more interpretable as it learns\nsensible connectivity structures that correlate with the ENSO anomaly pattern.\n",
        "published": "2021",
        "authors": [
            "Salva R\u00fchling Cachay",
            "Emma Erickson",
            "Arthur Fender C. Bucker",
            "Ernest Pokropek",
            "Willa Potosnak",
            "Suyash Bire",
            "Salomey Osei",
            "Bj\u00f6rn L\u00fctjens"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.06135v4",
        "title": "Multivariate Deep Evidential Regression",
        "abstract": "  There is significant need for principled uncertainty reasoning in machine\nlearning systems as they are increasingly deployed in safety-critical domains.\nA new approach with uncertainty-aware neural networks (NNs), based on learning\nevidential distributions for aleatoric and epistemic uncertainties, shows\npromise over traditional deterministic methods and typical Bayesian NNs, yet\nseveral important gaps in the theory and implementation of these networks\nremain. We discuss three issues with a proposed solution to extract aleatoric\nand epistemic uncertainties from regression-based neural networks. The approach\nderives a technique by placing evidential priors over the original Gaussian\nlikelihood function and training the NN to infer the hyperparameters of the\nevidential distribution. Doing so allows for the simultaneous extraction of\nboth uncertainties without sampling or utilization of out-of-distribution data\nfor univariate regression tasks. We describe the outstanding issues in detail,\nprovide a possible solution, and generalize the deep evidential regression\ntechnique for multivariate cases.\n",
        "published": "2021",
        "authors": [
            "Nis Meinert",
            "Alexander Lavin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.00901v2",
        "title": "A Differentiable Point Process with Its Application to Spiking Neural\n  Networks",
        "abstract": "  This paper is concerned about a learning algorithm for a probabilistic model\nof spiking neural networks (SNNs). Jimenez Rezende & Gerstner (2014) proposed a\nstochastic variational inference algorithm to train SNNs with hidden neurons.\nThe algorithm updates the variational distribution using the score function\ngradient estimator, whose high variance often impedes the whole learning\nalgorithm. This paper presents an alternative gradient estimator for SNNs based\non the path-wise gradient estimator. The main technical difficulty is a lack of\na general method to differentiate a realization of an arbitrary point process,\nwhich is necessary to derive the path-wise gradient estimator. We develop a\ndifferentiable point process, which is the technical highlight of this paper,\nand apply it to derive the path-wise gradient estimator for SNNs. We\ninvestigate the effectiveness of our gradient estimator through numerical\nsimulation.\n",
        "published": "2021",
        "authors": [
            "Hiroshi Kajino"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.01101v2",
        "title": "Learning a Single Neuron with Bias Using Gradient Descent",
        "abstract": "  We theoretically study the fundamental problem of learning a single neuron\nwith a bias term ($\\mathbf{x} \\mapsto \\sigma(<\\mathbf{w},\\mathbf{x}> + b)$) in\nthe realizable setting with the ReLU activation, using gradient descent.\nPerhaps surprisingly, we show that this is a significantly different and more\nchallenging problem than the bias-less case (which was the focus of previous\nworks on single neurons), both in terms of the optimization geometry as well as\nthe ability of gradient methods to succeed in some scenarios. We provide a\ndetailed study of this problem, characterizing the critical points of the\nobjective, demonstrating failure cases, and providing positive convergence\nguarantees under different sets of assumptions. To prove our results, we\ndevelop some tools which may be of independent interest, and improve previous\nresults on learning single neurons.\n",
        "published": "2021",
        "authors": [
            "Gal Vardi",
            "Gilad Yehudai",
            "Ohad Shamir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.03007v6",
        "title": "Truthful Self-Play",
        "abstract": "  We present a general framework for evolutionary learning to emergent unbiased\nstate representation without any supervision. Evolutionary frameworks such as\nself-play converge to bad local optima in case of multi-agent reinforcement\nlearning in non-cooperative partially observable environments with\ncommunication due to information asymmetry. Our proposed framework is a simple\nmodification of self-play inspired by mechanism design, also known as {\\em\nreverse game theory}, to elicit truthful signals and make the agents\ncooperative. The key idea is to add imaginary rewards using the peer prediction\nmethod, i.e., a mechanism for evaluating the validity of information exchanged\nbetween agents in a decentralized environment. Numerical experiments with\npredator prey, traffic junction and StarCraft tasks demonstrate that the\nstate-of-the-art performance of our framework.\n",
        "published": "2021",
        "authors": [
            "Shohei Ohsawa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.05566v5",
        "title": "A Neural Tangent Kernel Perspective of GANs",
        "abstract": "  We propose a novel theoretical framework of analysis for Generative\nAdversarial Networks (GANs). We reveal a fundamental flaw of previous analyses\nwhich, by incorrectly modeling GANs' training scheme, are subject to\nill-defined discriminator gradients. We overcome this issue which impedes a\nprincipled study of GAN training, solving it within our framework by taking\ninto account the discriminator's architecture. To this end, we leverage the\ntheory of infinite-width neural networks for the discriminator via its Neural\nTangent Kernel. We characterize the trained discriminator for a wide range of\nlosses and establish general differentiability properties of the network. From\nthis, we derive new insights about the convergence of the generated\ndistribution, advancing our understanding of GANs' training dynamics. We\nempirically corroborate these results via an analysis toolkit based on our\nframework, unveiling intuitions that are consistent with GAN practice.\n",
        "published": "2021",
        "authors": [
            "Jean-Yves Franceschi",
            "Emmanuel de B\u00e9zenac",
            "Ibrahim Ayed",
            "Micka\u00ebl Chen",
            "Sylvain Lamprier",
            "Patrick Gallinari"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.10575v2",
        "title": "EvoGrad: Efficient Gradient-Based Meta-Learning and Hyperparameter\n  Optimization",
        "abstract": "  Gradient-based meta-learning and hyperparameter optimization have seen\nsignificant progress recently, enabling practical end-to-end training of neural\nnetworks together with many hyperparameters. Nevertheless, existing approaches\nare relatively expensive as they need to compute second-order derivatives and\nstore a longer computational graph. This cost prevents scaling them to larger\nnetwork architectures. We present EvoGrad, a new approach to meta-learning that\ndraws upon evolutionary techniques to more efficiently compute hypergradients.\nEvoGrad estimates hypergradient with respect to hyperparameters without\ncalculating second-order gradients, or storing a longer computational graph,\nleading to significant improvements in efficiency. We evaluate EvoGrad on three\nsubstantial recent meta-learning applications, namely cross-domain few-shot\nlearning with feature-wise transformations, noisy label learning with\nMeta-Weight-Net and low-resource cross-lingual learning with meta\nrepresentation transformation. The results show that EvoGrad significantly\nimproves efficiency and enables scaling meta-learning to bigger architectures\nsuch as from ResNet10 to ResNet34.\n",
        "published": "2021",
        "authors": [
            "Ondrej Bohdal",
            "Yongxin Yang",
            "Timothy Hospedales"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.14406v1",
        "title": "Poisoning the Search Space in Neural Architecture Search",
        "abstract": "  Deep learning has proven to be a highly effective problem-solving tool for\nobject detection and image segmentation across various domains such as\nhealthcare and autonomous driving. At the heart of this performance lies neural\narchitecture design which relies heavily on domain knowledge and prior\nexperience on the researchers' behalf. More recently, this process of finding\nthe most optimal architectures, given an initial search space of possible\noperations, was automated by Neural Architecture Search (NAS). In this paper,\nwe evaluate the robustness of one such algorithm known as Efficient NAS (ENAS)\nagainst data agnostic poisoning attacks on the original search space with\ncarefully designed ineffective operations. By evaluating algorithm performance\non the CIFAR-10 dataset, we empirically demonstrate how our novel search space\npoisoning (SSP) approach and multiple-instance poisoning attacks exploit design\nflaws in the ENAS controller to result in inflated prediction error rates for\nchild networks. Our results provide insights into the challenges to surmount in\nusing NAS for more adversarially robust architecture search.\n",
        "published": "2021",
        "authors": [
            "Robert Wu",
            "Nayan Saxena",
            "Rohan Jain"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.01867v2",
        "title": "A Pragmatic Look at Deep Imitation Learning",
        "abstract": "  The introduction of the generative adversarial imitation learning (GAIL)\nalgorithm has spurred the development of scalable imitation learning approaches\nusing deep neural networks. Many of the algorithms that followed used a similar\nprocedure, combining on-policy actor-critic algorithms with inverse\nreinforcement learning. More recently there have been an even larger breadth of\napproaches, most of which use off-policy algorithms. However, with the breadth\nof algorithms, everything from datasets to base reinforcement learning\nalgorithms to evaluation settings can vary, making it difficult to fairly\ncompare them. In this work we re-implement 6 different IL algorithms, updating\n3 of them to be off-policy, base them on a common off-policy algorithm (SAC),\nand evaluate them on a widely-used expert trajectory dataset (D4RL) for the\nmost common benchmark (MuJoCo). After giving all algorithms the same\nhyperparameter optimisation budget, we compare their results for a range of\nexpert trajectories. In summary, GAIL, with all of its improvements,\nconsistently performs well across a range of sample sizes, AdRIL is a simple\ncontender that performs well with one important hyperparameter to tune, and\nbehavioural cloning remains a strong baseline when data is more plentiful.\n",
        "published": "2021",
        "authors": [
            "Kai Arulkumaran",
            "Dan Ogawa Lillrank"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.06980v3",
        "title": "Task-Sensitive Concept Drift Detector with Constraint Embedding",
        "abstract": "  Detecting drifts in data is essential for machine learning applications, as\nchanges in the statistics of processed data typically has a profound influence\non the performance of trained models. Most of the available drift detection\nmethods are either supervised and require access to the true labels during\ninference time, or they are completely unsupervised and aim for changes in\ndistributions without taking label information into account. We propose a novel\ntask-sensitive semi-supervised drift detection scheme, which utilizes label\ninformation while training the initial model, but takes into account that\nsupervised label information is no longer available when using the model during\ninference. It utilizes a constrained low-dimensional embedding representation\nof the input data. This way, it is best suited for the classification task. It\nis able to detect real drift, where the drift affects the classification\nperformance, while it properly ignores virtual drift, where the classification\nperformance is not affected by the drift. In the proposed framework, the actual\nmethod to detect a change in the statistics of incoming data samples can be\nchosen freely. Experimental evaluation on nine benchmarks datasets, with\ndifferent types of drift, demonstrates that the proposed framework can reliably\ndetect drifts, and outperforms state-of-the-art unsupervised drift detection\napproaches.\n",
        "published": "2021",
        "authors": [
            "Andrea Castellani",
            "Sebastian Schmitt",
            "Barbara Hammer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.10573v2",
        "title": "The staircase property: How hierarchical structure can guide deep\n  learning",
        "abstract": "  This paper identifies a structural property of data distributions that\nenables deep neural networks to learn hierarchically. We define the \"staircase\"\nproperty for functions over the Boolean hypercube, which posits that high-order\nFourier coefficients are reachable from lower-order Fourier coefficients along\nincreasing chains. We prove that functions satisfying this property can be\nlearned in polynomial time using layerwise stochastic coordinate descent on\nregular neural networks -- a class of network architectures and initializations\nthat have homogeneity properties. Our analysis shows that for such staircase\nfunctions and neural networks, the gradient-based algorithm learns high-level\nfeatures by greedily combining lower-level features along the depth of the\nnetwork. We further back our theoretical results with experiments showing that\nstaircase functions are also learnable by more standard ResNet architectures\nwith stochastic gradient descent. Both the theoretical and experimental results\nsupport the fact that staircase properties have a role to play in understanding\nthe capabilities of gradient-based learning on regular networks, in contrast to\ngeneral polynomial-size networks that can emulate any SQ or PAC algorithms as\nrecently shown.\n",
        "published": "2021",
        "authors": [
            "Emmanuel Abbe",
            "Enric Boix-Adsera",
            "Matthew Brennan",
            "Guy Bresler",
            "Dheeraj Nagaraj"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.00541v2",
        "title": "Active Inference and Epistemic Value in Graphical Models",
        "abstract": "  The Free Energy Principle (FEP) postulates that biological agents perceive\nand interact with their environment in order to minimize a Variational Free\nEnergy (VFE) with respect to a generative model of their environment. The\ninference of a policy (future control sequence) according to the FEP is known\nas Active Inference (AIF). The AIF literature describes multiple VFE objectives\nfor policy planning that lead to epistemic (information-seeking) behavior.\nHowever, most objectives have limited modeling flexibility. This paper\napproaches epistemic behavior from a constrained Bethe Free Energy (CBFE)\nperspective. Crucially, variational optimization of the CBFE can be expressed\nin terms of message passing on free-form generative models. The key intuition\nbehind the CBFE is that we impose a point-mass constraint on predicted\noutcomes, which explicitly encodes the assumption that the agent will make\nobservations in the future. We interpret the CBFE objective in terms of its\nconstituent behavioral drives. We then illustrate resulting behavior of the\nCBFE by planning and interacting with a simulated T-maze environment.\nSimulations for the T-maze task illustrate how the CBFE agent exhibits an\nepistemic drive, and actively plans ahead to account for the impact of\npredicted outcomes. Compared to an EFE agent, the CBFE agent incurs expected\nreward in significantly more environmental scenarios. We conclude that CBFE\noptimization by message passing suggests a general mechanism for\nepistemic-aware AIF in free-form generative models.\n",
        "published": "2021",
        "authors": [
            "Thijs van de Laar",
            "Magnus Koudahl",
            "Bart van Erp",
            "Bert de Vries"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.09705v4",
        "title": "Neural forecasting at scale",
        "abstract": "  We study the problem of efficiently scaling ensemble-based deep neural\nnetworks for multi-step time series (TS) forecasting on a large set of time\nseries. Current state-of-the-art deep ensemble models have high memory and\ncomputational requirements, hampering their use to forecast millions of TS in\npractical scenarios. We propose N-BEATS(P), a global parallel variant of the\nN-BEATS model designed to allow simultaneous training of multiple univariate TS\nforecasting models. Our model addresses the practical limitations of related\nmodels, reducing the training time by half and memory requirement by a factor\nof 5, while keeping the same level of accuracy in all TS forecasting settings.\nWe have performed multiple experiments detailing the various ways to train our\nmodel and have obtained results that demonstrate its capacity to generalize in\nvarious forecasting conditions and setups.\n",
        "published": "2021",
        "authors": [
            "Philippe Chatigny",
            "Shengrui Wang",
            "Jean-Marc Patenaude",
            "Boris N. Oreshkin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.02491v4",
        "title": "Data-Centric AI Requires Rethinking Data Notion",
        "abstract": "  The transition towards data-centric AI requires revisiting data notions from\nmathematical and implementational standpoints to obtain unified data-centric\nmachine learning packages. Towards this end, this work proposes unifying\nprinciples offered by categorical and cochain notions of data, and discusses\nthe importance of these principles in data-centric AI transition. In the\ncategorical notion, data is viewed as a mathematical structure that we act upon\nvia morphisms to preserve this structure. As for cochain notion, data can be\nviewed as a function defined in a discrete domain of interest and acted upon\nvia operators. While these notions are almost orthogonal, they provide a\nunifying definition to view data, ultimately impacting the way machine learning\npackages are developed, implemented, and utilized by practitioners.\n",
        "published": "2021",
        "authors": [
            "Mustafa Hajij",
            "Ghada Zamzmi",
            "Karthikeyan Natesan Ramamurthy",
            "Aldo Guzman Saenz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.03187v1",
        "title": "On the Optimal Memorization Power of ReLU Neural Networks",
        "abstract": "  We study the memorization power of feedforward ReLU neural networks. We show\nthat such networks can memorize any $N$ points that satisfy a mild separability\nassumption using $\\tilde{O}\\left(\\sqrt{N}\\right)$ parameters. Known\nVC-dimension upper bounds imply that memorizing $N$ samples requires\n$\\Omega(\\sqrt{N})$ parameters, and hence our construction is optimal up to\nlogarithmic factors. We also give a generalized construction for networks with\ndepth bounded by $1 \\leq L \\leq \\sqrt{N}$, for memorizing $N$ samples using\n$\\tilde{O}(N/L)$ parameters. This bound is also optimal up to logarithmic\nfactors. Our construction uses weights with large bit complexity. We prove that\nhaving such a large bit complexity is both necessary and sufficient for\nmemorization with a sub-linear number of parameters.\n",
        "published": "2021",
        "authors": [
            "Gal Vardi",
            "Gilad Yehudai",
            "Ohad Shamir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.05177v2",
        "title": "Learning Division with Neural Arithmetic Logic Modules",
        "abstract": "  To achieve systematic generalisation, it first makes sense to master simple\ntasks such as arithmetic. Of the four fundamental arithmetic operations\n(+,-,$\\times$,$\\div$), division is considered the most difficult for both\nhumans and computers. In this paper we show that robustly learning division in\na systematic manner remains a challenge even at the simplest level of dividing\ntwo numbers. We propose two novel approaches for division which we call the\nNeural Reciprocal Unit (NRU) and the Neural Multiplicative Reciprocal Unit\n(NMRU), and present improvements for an existing division module, the Real\nNeural Power Unit (Real NPU). Experiments in learning division with input\nredundancy on 225 different training sets, find that our proposed modifications\nto the Real NPU obtains an average success of 85.3$\\%$ improving over the\noriginal by 15.1$\\%$. In light of the suggestion above, our NMRU approach can\nfurther improve the success to 91.6$\\%$.\n",
        "published": "2021",
        "authors": [
            "Bhumika Mistry",
            "Katayoun Farrahi",
            "Jonathon Hare"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.08706v3",
        "title": "How and When Random Feedback Works: A Case Study of Low-Rank Matrix\n  Factorization",
        "abstract": "  The success of gradient descent in ML and especially for learning neural\nnetworks is remarkable and robust. In the context of how the brain learns, one\naspect of gradient descent that appears biologically difficult to realize (if\nnot implausible) is that its updates rely on feedback from later layers to\nearlier layers through the same connections. Such bidirected links are\nrelatively few in brain networks, and even when reciprocal connections exist,\nthey may not be equi-weighted. Random Feedback Alignment (Lillicrap et al.,\n2016), where the backward weights are random and fixed, has been proposed as a\nbio-plausible alternative and found to be effective empirically. We investigate\nhow and when feedback alignment (FA) works, focusing on one of the most basic\nproblems with layered structure -- low-rank matrix factorization. In this\nproblem, given a matrix $Y_{n\\times m}$, the goal is to find a low rank\nfactorization $Z_{n \\times r}W_{r \\times m}$ that minimizes the error\n$\\|ZW-Y\\|_F$. Gradient descent solves this problem optimally. We show that FA\nconverges to the optimal solution when $r\\ge \\mbox{rank}(Y)$. We also shed\nlight on how FA works. It is observed empirically that the forward weight\nmatrices and (random) feedback matrices come closer during FA updates. Our\nanalysis rigorously derives this phenomenon and shows how it facilitates\nconvergence of FA*, a closely related variant of FA. We also show that FA can\nbe far from optimal when $r < \\mbox{rank}(Y)$. This is the first provable\nseparation result between gradient descent and FA. Moreover, the\nrepresentations found by gradient descent and FA can be almost orthogonal even\nwhen their error $\\|ZW-Y\\|_F$ is approximately equal. As a corollary, these\nresults also hold for training two-layer linear neural networks when the\ntraining input is isotropic, and the output is a linear function of the input.\n",
        "published": "2021",
        "authors": [
            "Shivam Garg",
            "Santosh S. Vempala"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.14046v1",
        "title": "Neural Tangent Kernel of Matrix Product States: Convergence and\n  Applications",
        "abstract": "  In this work, we study the Neural Tangent Kernel (NTK) of Matrix Product\nStates (MPS) and the convergence of its NTK in the infinite bond dimensional\nlimit. We prove that the NTK of MPS asymptotically converges to a constant\nmatrix during the gradient descent (training) process (and also the\ninitialization phase) as the bond dimensions of MPS go to infinity by the\nobservation that the variation of the tensors in MPS asymptotically goes to\nzero during training in the infinite limit. By showing the\npositive-definiteness of the NTK of MPS, the convergence of MPS during the\ntraining in the function space (space of functions represented by MPS) is\nguaranteed without any extra assumptions of the data set. We then consider the\nsettings of (supervised) Regression with Mean Square Error (RMSE) and\n(unsupervised) Born Machines (BM) and analyze their dynamics in the infinite\nbond dimensional limit. The ordinary differential equations (ODEs) which\ndescribe the dynamics of the responses of MPS in the RMSE and BM are derived\nand solved in the closed-form. For the Regression, we consider Mercer Kernels\n(Gaussian Kernels) and find that the evolution of the mean of the responses of\nMPS follows the largest eigenvalue of the NTK. Due to the orthogonality of the\nkernel functions in BM, the evolution of different modes (samples) decouples\nand the \"characteristic time\" of convergence in training is obtained.\n",
        "published": "2021",
        "authors": [
            "Erdong Guo",
            "David Draper"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.09992v4",
        "title": "Weisfeiler and Leman go Machine Learning: The Story so far",
        "abstract": "  In recent years, algorithms and neural architectures based on the\nWeisfeiler--Leman algorithm, a well-known heuristic for the graph isomorphism\nproblem, have emerged as a powerful tool for machine learning with graphs and\nrelational data. Here, we give a comprehensive overview of the algorithm's use\nin a machine-learning setting, focusing on the supervised regime. We discuss\nthe theoretical background, show how to use it for supervised graph and node\nrepresentation learning, discuss recent extensions, and outline the algorithm's\nconnection to (permutation-)equivariant neural architectures. Moreover, we give\nan overview of current applications and future directions to stimulate further\nresearch.\n",
        "published": "2021",
        "authors": [
            "Christopher Morris",
            "Yaron Lipman",
            "Haggai Maron",
            "Bastian Rieck",
            "Nils M. Kriege",
            "Martin Grohe",
            "Matthias Fey",
            "Karsten Borgwardt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.06153v1",
        "title": "Reconstruction of Incomplete Wildfire Data using Deep Generative Models",
        "abstract": "  We present our submission to the Extreme Value Analysis 2021 Data Challenge\nin which teams were asked to accurately predict distributions of wildfire\nfrequency and size within spatio-temporal regions of missing data. For the\npurpose of this competition we developed a variant of the powerful variational\nautoencoder models dubbed the Conditional Missing data Importance-Weighted\nAutoencoder (CMIWAE). Our deep latent variable generative model requires little\nto no feature engineering and does not necessarily rely on the specifics of\nscoring in the Data Challenge. It is fully trained on incomplete data, with the\nsingle objective to maximize log-likelihood of the observed wildfire\ninformation. We mitigate the effects of the relatively low number of training\nsamples by stochastic sampling from a variational latent variable distribution,\nas well as by ensembling a set of CMIWAE models trained and validated on\ndifferent splits of the provided data. The presented approach is not\ndomain-specific and is amenable to application in other missing data recovery\ntasks with tabular or image-like information conditioned on auxiliary\ninformation.\n",
        "published": "2022",
        "authors": [
            "Tomislav Ivek",
            "Domagoj Vlah"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.11422v2",
        "title": "Fast Moving Natural Evolution Strategy for High-Dimensional Problems",
        "abstract": "  In this work, we propose a new variant of natural evolution strategies (NES)\nfor high-dimensional black-box optimization problems. The proposed method,\nCR-FM-NES, extends a recently proposed state-of-the-art NES, Fast Moving\nNatural Evolution Strategy (FM-NES), in order to be applicable in\nhigh-dimensional problems. CR-FM-NES builds on an idea using a restricted\nrepresentation of a covariance matrix instead of using a full covariance\nmatrix, while inheriting an efficiency of FM-NES. The restricted representation\nof the covariance matrix enables CR-FM-NES to update parameters of a\nmultivariate normal distribution in linear time and space complexity, which can\nbe applied to high-dimensional problems. Our experimental results reveal that\nCR-FM-NES does not lose the efficiency of FM-NES, and on the contrary,\nCR-FM-NES has achieved significant speedup compared to FM-NES on some benchmark\nproblems. Furthermore, our numerical experiments using 200, 600, and\n1000-dimensional benchmark problems demonstrate that CR-FM-NES is effective\nover scalable baseline methods, VD-CMA and Sep-CMA.\n",
        "published": "2022",
        "authors": [
            "Masahiro Nomura",
            "Isao Ono"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.02433v2",
        "title": "The Machine Learning for Combinatorial Optimization Competition (ML4CO):\n  Results and Insights",
        "abstract": "  Combinatorial optimization is a well-established area in operations research\nand computer science. Until recently, its methods have focused on solving\nproblem instances in isolation, ignoring that they often stem from related data\ndistributions in practice. However, recent years have seen a surge of interest\nin using machine learning as a new approach for solving combinatorial problems,\neither directly as solvers or by enhancing exact solvers. Based on this\ncontext, the ML4CO aims at improving state-of-the-art combinatorial\noptimization solvers by replacing key heuristic components. The competition\nfeatured three challenging tasks: finding the best feasible solution, producing\nthe tightest optimality certificate, and giving an appropriate solver\nconfiguration. Three realistic datasets were considered: balanced item\nplacement, workload apportionment, and maritime inventory routing. This last\ndataset was kept anonymous for the contestants.\n",
        "published": "2022",
        "authors": [
            "Maxime Gasse",
            "Quentin Cappart",
            "Jonas Charfreitag",
            "Laurent Charlin",
            "Didier Ch\u00e9telat",
            "Antonia Chmiela",
            "Justin Dumouchelle",
            "Ambros Gleixner",
            "Aleksandr M. Kazachkov",
            "Elias Khalil",
            "Pawel Lichocki",
            "Andrea Lodi",
            "Miles Lubin",
            "Chris J. Maddison",
            "Christopher Morris",
            "Dimitri J. Papageorgiou",
            "Augustin Parjadis",
            "Sebastian Pokutta",
            "Antoine Prouvost",
            "Lara Scavuzzo",
            "Giulia Zarpellon",
            "Linxin Yang",
            "Sha Lai",
            "Akang Wang",
            "Xiaodong Luo",
            "Xiang Zhou",
            "Haohan Huang",
            "Shengcheng Shao",
            "Yuanming Zhu",
            "Dong Zhang",
            "Tao Quan",
            "Zixuan Cao",
            "Yang Xu",
            "Zhewei Huang",
            "Shuchang Zhou",
            "Chen Binbin",
            "He Minggui",
            "Hao Hao",
            "Zhang Zhiyu",
            "An Zhiwu",
            "Mao Kun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.09410v4",
        "title": "A Framework and Benchmark for Deep Batch Active Learning for Regression",
        "abstract": "  The acquisition of labels for supervised learning can be expensive. To\nimprove the sample efficiency of neural network regression, we study active\nlearning methods that adaptively select batches of unlabeled data for labeling.\nWe present a framework for constructing such methods out of (network-dependent)\nbase kernels, kernel transformations, and selection methods. Our framework\nencompasses many existing Bayesian methods based on Gaussian process\napproximations of neural networks as well as non-Bayesian methods.\nAdditionally, we propose to replace the commonly used last-layer features with\nsketched finite-width neural tangent kernels and to combine them with a novel\nclustering method. To evaluate different methods, we introduce an open-source\nbenchmark consisting of 15 large tabular regression data sets. Our proposed\nmethod outperforms the state-of-the-art on our benchmark, scales to large data\nsets, and works out-of-the-box without adjusting the network architecture or\ntraining code. We provide open-source code that includes efficient\nimplementations of all kernels, kernel transformations, and selection methods,\nand can be used for reproducing our results.\n",
        "published": "2022",
        "authors": [
            "David Holzm\u00fcller",
            "Viktor Zaverkin",
            "Johannes K\u00e4stner",
            "Ingo Steinwart"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.11740v17",
        "title": "The Deep learning model of upstream and downstream brain regions Based\n  on Memory Generation-Consolidation-Loss, Synaptic Strength Rebalance and\n  mnemonic spiral",
        "abstract": "  In addition to the shared weights of the synaptic connections, our new neural\nnetwork includes the synaptic effective range weights for both the forward and\nback propagation. We try to simulate the functions of prefrontal lobe,\namygdala, and hippocampus by the Deep learning model of upstream and downstream\nbrain regions(DLMOUADBR). Along the forward propagation, the negative memory\ngradually increases. Along the back propagation, the optimization order will\nincrease. Memory flow may be considered to be the transmission of the rate of\nchange of the architecture, then the nth cortex is the nth derivative of brain\nplasticity. Astrocytic cortex memory persistence factor and astrocytes\nphagocytose synapses inhibit local synaptic accumulation, and the model\ninspires experiments. The memory Generation-Consolidation-Loss model tries to\nexplain 15 phenomena of Alzheimer's disease based on the DLMOUADBR and reverse\nturbulence. We consider the Heart-Brain model to reference to non-classical\nquantum entanglement experiments. And turbulent movement of brain regions\nthrough mnemonic spiral. The study first showed that mnemonic architecture\nformula-logarithmic spiral, turbulent movement in brain regions is only energy\nloss and memory engrams are approximate. This explains the dynamics cause of\nshaping in the geometry of the brain, related to the turbulent movement of the\nlogarithmic spiral of the brain. In simulation, it is possible that thicker\ncortices and more diverse individuals within the brain could have high IQ, but\nthickest cortices and most diverse individuals may have low IQ in simulation\nand tries to give the mechanism of Cognitive impairment.\n",
        "published": "2022",
        "authors": [
            "Jun-Bo Tao",
            "Bai-Qing Sun",
            "Wei-Dong Zhu",
            "Shi-You Qu",
            "Ling-Kun Chen",
            "Jia-Qiang Li",
            "Guo-Qi Li",
            "Chong Wu",
            "Yu Xiong",
            "Jiaxuan Zhou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.11815v1",
        "title": "Clustering units in neural networks: upstream vs downstream information",
        "abstract": "  It has been hypothesized that some form of \"modular\" structure in artificial\nneural networks should be useful for learning, compositionality, and\ngeneralization. However, defining and quantifying modularity remains an open\nproblem. We cast the problem of detecting functional modules into the problem\nof detecting clusters of similar-functioning units. This begs the question of\nwhat makes two units functionally similar. For this, we consider two broad\nfamilies of methods: those that define similarity based on how units respond to\nstructured variations in inputs (\"upstream\"), and those based on how variations\nin hidden unit activations affect outputs (\"downstream\"). We conduct an\nempirical study quantifying modularity of hidden layer representations of\nsimple feedforward, fully connected networks, across a range of\nhyperparameters. For each model, we quantify pairwise associations between\nhidden units in each layer using a variety of both upstream and downstream\nmeasures, then cluster them by maximizing their \"modularity score\" using\nestablished tools from network science. We find two surprising results: first,\ndropout dramatically increased modularity, while other forms of weight\nregularization had more modest effects. Second, although we observe that there\nis usually good agreement about clusters within both upstream methods and\ndownstream methods, there is little agreement about the cluster assignments\nacross these two families of methods. This has important implications for\nrepresentation-learning, as it suggests that finding modular representations\nthat reflect structure in inputs (e.g. disentanglement) may be a distinct goal\nfrom learning modular representations that reflect structure in outputs (e.g.\ncompositionality).\n",
        "published": "2022",
        "authors": [
            "Richard D. Lange",
            "David S. Rolnick",
            "Konrad P. Kording"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.11860v3",
        "title": "Practical tradeoffs between memory, compute, and performance in learned\n  optimizers",
        "abstract": "  Optimization plays a costly and crucial role in developing machine learning\nsystems. In learned optimizers, the few hyperparameters of commonly used\nhand-designed optimizers, e.g. Adam or SGD, are replaced with flexible\nparametric functions. The parameters of these functions are then optimized so\nthat the resulting learned optimizer minimizes a target loss on a chosen class\nof models. Learned optimizers can both reduce the number of required training\nsteps and improve the final test loss. However, they can be expensive to train,\nand once trained can be expensive to use due to computational and memory\noverhead for the optimizer itself. In this work, we identify and quantify the\ndesign features governing the memory, compute, and performance trade-offs for\nmany learned and hand-designed optimizers. We further leverage our analysis to\nconstruct a learned optimizer that is both faster and more memory efficient\nthan previous work. Our model and training code are open source.\n",
        "published": "2022",
        "authors": [
            "Luke Metz",
            "C. Daniel Freeman",
            "James Harrison",
            "Niru Maheswaranathan",
            "Jascha Sohl-Dickstein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.12742v2",
        "title": "Accelerating Bayesian Optimization for Biological Sequence Design with\n  Denoising Autoencoders",
        "abstract": "  Bayesian optimization (BayesOpt) is a gold standard for query-efficient\ncontinuous optimization. However, its adoption for drug design has been\nhindered by the discrete, high-dimensional nature of the decision variables. We\ndevelop a new approach (LaMBO) which jointly trains a denoising autoencoder\nwith a discriminative multi-task Gaussian process head, allowing gradient-based\noptimization of multi-objective acquisition functions in the latent space of\nthe autoencoder. These acquisition functions allow LaMBO to balance the\nexplore-exploit tradeoff over multiple design rounds, and to balance objective\ntradeoffs by optimizing sequences at many different points on the Pareto\nfrontier. We evaluate LaMBO on two small-molecule design tasks, and introduce\nnew tasks optimizing \\emph{in silico} and \\emph{in vitro} properties of\nlarge-molecule fluorescent proteins. In our experiments LaMBO outperforms\ngenetic optimizers and does not require a large pretraining corpus,\ndemonstrating that BayesOpt is practical and effective for biological sequence\ndesign.\n",
        "published": "2022",
        "authors": [
            "Samuel Stanton",
            "Wesley Maddox",
            "Nate Gruver",
            "Phillip Maffettone",
            "Emily Delaney",
            "Peyton Greenside",
            "Andrew Gordon Wilson"
        ]
    }
]