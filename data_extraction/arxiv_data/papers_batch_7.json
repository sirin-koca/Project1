[
    {
        "id": "http://arxiv.org/abs/2112.08933v2",
        "title": "Responsive parallelized architecture for deploying deep learning models\n  in production environments",
        "abstract": "  Recruiters can easily shortlist candidates for jobs via viewing their\ncurriculum vitae (CV) document. Unstructured document CV beholds candidate's\nportfolio and named entities listing details. The main aim of this study is to\ndesign and propose a web oriented, highly responsive, computational pipeline\nthat systematically predicts CV entities using hierarchically-refined label\nattention networks. Deep learning models specialized for named entity\nrecognition were trained on large dataset to predict relevant fields. The\narticle suggests an optimal strategy to use a number of deep learning models in\nparallel and predict in real time. We demonstrate selection of light weight\nmicro web framework using Analytical Hierarchy Processing algorithm and focus\non an approach useful to deploy large deep learning model-based pipelines in\nproduction ready environments using microservices. Deployed models and\narchitecture proposed helped in parsing normal CV in less than 700 milliseconds\nfor sequential flow of requests.\n",
        "published": "2021",
        "authors": [
            "Nikhil Verma",
            "Krishna Prasad"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.01581v1",
        "title": "Interpretable Fusion Analytics Framework for fMRI Connectivity:\n  Self-Attention Mechanism and Latent Space Item-Response Model",
        "abstract": "  There have been several attempts to use deep learning based on brain fMRI\nsignals to classify cognitive impairment diseases. However, deep learning is a\nhidden black box model that makes it difficult to interpret the process of\nclassification. To address this issue, we propose a novel analytical framework\nthat interprets the classification result from deep learning processes. We\nfirst derive the region of interest (ROI) functional connectivity network (FCN)\nby embedding functions based on their similar signal patterns. Then, using the\nself-attention equipped deep learning model, we classify diseases based on\ntheir FCN. Finally, in order to interpret the classification results, we employ\na latent space item-response interaction network model to identify the\nsignificant functions that exhibit distinct connectivity patterns when compared\nto other diseases. The application of this proposed framework to the four types\nof cognitive impairment shows that our approach is valid for determining the\nsignificant ROI functions.\n",
        "published": "2022",
        "authors": [
            "Jeong-Jae Kim",
            "Yeseul Jeon",
            "SuMin Yu",
            "Junggu Choi",
            "Sanghoon Han"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.06904v1",
        "title": "Attention mechanisms for physiological signal deep learning: which\n  attention should we take?",
        "abstract": "  Attention mechanisms are widely used to dramatically improve deep learning\nmodel performance in various fields. However, their general ability to improve\nthe performance of physiological signal deep learning model is immature. In\nthis study, we experimentally analyze four attention mechanisms (e.g.,\nsqueeze-and-excitation, non-local, convolutional block attention module, and\nmulti-head self-attention) and three convolutional neural network (CNN)\narchitectures (e.g., VGG, ResNet, and Inception) for two representative\nphysiological signal prediction tasks: the classification for predicting\nhypotension and the regression for predicting cardiac output (CO). We evaluated\nmultiple combinations for performance and convergence of physiological signal\ndeep learning model. Accordingly, the CNN models with the spatial attention\nmechanism showed the best performance in the classification problem, whereas\nthe channel attention mechanism achieved the lowest error in the regression\nproblem. Moreover, the performance and convergence of the CNN models with\nattention mechanisms were better than stand-alone self-attention models in both\nproblems. Hence, we verified that convolutional operation and attention\nmechanisms are complementary and provide faster convergence time, despite the\nstand-alone self-attention models requiring fewer parameters.\n",
        "published": "2022",
        "authors": [
            "Seong-A Park",
            "Hyung-Chul Lee",
            "Chul-Woo Jung",
            "Hyun-Lim Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.02094v1",
        "title": "A Novel Approach To Network Intrusion Detection System Using Deep\n  Learning For Sdn: Futuristic Approach",
        "abstract": "  Software-Defined Networking (SDN) is the next generation to change the\narchitecture of traditional networks. SDN is one of the promising solutions to\nchange the architecture of internet networks. Attacks become more common due to\nthe centralized nature of SDN architecture. It is vital to provide security for\nthe SDN. In this study, we propose a Network Intrusion Detection System-Deep\nLearning module (NIDS-DL) approach in the context of SDN. Our suggested method\ncombines Network Intrusion Detection Systems (NIDS) with many types of deep\nlearning algorithms. Our approach employs 12 features extracted from 41\nfeatures in the NSL-KDD dataset using a feature selection method. We employed\nclassifiers (CNN, DNN, RNN, LSTM, and GRU). When we compare classifier scores,\nour technique produced accuracy results of (98.63%, 98.53%, 98.13%, 98.04%, and\n97.78%) respectively. The novelty of our new approach (NIDS-DL) uses 5 deep\nlearning classifiers and made pre-processing dataset to harvests the best\nresults. Our proposed approach was successful in binary classification and\ndetecting attacks, implying that our approach (NIDS-DL) might be used with\ngreat efficiency in the future.\n",
        "published": "2022",
        "authors": [
            "Mhmood Radhi Hadi",
            "Adnan Saher Mohammed"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.08171v1",
        "title": "Deep learning for reconstructing protein structures from cryo-EM density\n  maps: recent advances and future directions",
        "abstract": "  Cryo-Electron Microscopy (cryo-EM) has emerged as a key technology to\ndetermine the structure of proteins, particularly large protein complexes and\nassemblies in recent years. A key challenge in cryo-EM data analysis is to\nautomatically reconstruct accurate protein structures from cryo-EM density\nmaps. In this review, we briefly overview various deep learning methods for\nbuilding protein structures from cryo-EM density maps, analyze their impact,\nand discuss the challenges of preparing high-quality data sets for training\ndeep learning models. Looking into the future, more advanced deep learning\nmodels of effectively integrating cryo-EM data with other sources of\ncomplementary data such as protein sequences and AlphaFold-predicted structures\nneed to be developed to further advance the field.\n",
        "published": "2022",
        "authors": [
            "Nabin Giri",
            "Raj S. Roy",
            "Jianlin Cheng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.00319v1",
        "title": "Development of deep biological ages aware of morbidity and mortality\n  based on unsupervised and semi-supervised deep learning approaches",
        "abstract": "  Background: While deep learning technology, which has the capability of\nobtaining latent representations based on large-scale data, can be a potential\nsolution for the discovery of a novel aging biomarker, existing deep learning\nmethods for biological age estimation usually depend on chronological ages and\nlack of consideration of mortality and morbidity that are the most significant\noutcomes of aging. Methods: This paper proposes a novel deep learning model to\nlearn latent representations of biological aging in regard to subjects'\nmorbidity and mortality. The model utilizes health check-up data in addition to\nmorbidity and mortality information to learn the complex relationships between\naging and measured clinical attributes. Findings: The proposed model is\nevaluated on a large dataset of general populations compared with KDM and other\nlearning-based models. Results demonstrate that biological ages obtained by the\nproposed model have superior discriminability of subjects' morbidity and\nmortality.\n",
        "published": "2023",
        "authors": [
            "Seong-Eun Moon",
            "Ji Won Yoon",
            "Shinyoung Joo",
            "Yoohyung Kim",
            "Jae Hyun Bae",
            "Seokho Yoon",
            "Haanju Yoo",
            "Young Min Cho"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.00722v2",
        "title": "A Survey of Deep Learning: From Activations to Transformers",
        "abstract": "  The past decade has witnessed remarkable advancements in deep learning, owing\nto the emergence of various architectures, layers, objectives, and optimization\ntechniques. These consist of a multitude of variations of attention,\nnormalization, skip connections, transformer, and self-supervised learning\nmethods, among others. Our goal is to furnish a comprehensive survey of\nsignificant recent contributions in these domains to individuals with a\nfundamental grasp of deep learning. Our aspiration is that an integrated and\ncomprehensive approach of influential recent works will facilitate the\nformation of new connections between different areas of deep learning. In our\ndiscussion, we discuss multiple patterns that summarize the key strategies for\nmany of the successful innovations over the last decade. We also include a\ndiscussion on recent commercially built, closed-source models such as OpenAI's\nGPT-4 and Google's PaLM 2.\n",
        "published": "2023",
        "authors": [
            "Johannes Schneider",
            "Michalis Vlachos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.02186v1",
        "title": "Causal Deep Learning",
        "abstract": "  Causality has the potential to truly transform the way we solve a large\nnumber of real-world problems. Yet, so far, its potential remains largely\nunlocked since most work so far requires strict assumptions which do not hold\ntrue in practice. To address this challenge and make progress in solving\nreal-world problems, we propose a new way of thinking about causality - we call\nthis causal deep learning. The framework which we propose for causal deep\nlearning spans three dimensions: (1) a structural dimension, which allows\nincomplete causal knowledge rather than assuming either full or no causal\nknowledge; (2) a parametric dimension, which encompasses parametric forms which\nare typically ignored; and finally, (3) a temporal dimension, which explicitly\nallows for situations which capture exposure times or temporal structure.\nTogether, these dimensions allow us to make progress on a variety of real-world\nproblems by leveraging (sometimes incomplete) causal knowledge and/or combining\ndiverse causal deep learning methods. This new framework also enables\nresearchers to compare systematically across existing works as well as identify\npromising research areas which can lead to real-world impact.\n",
        "published": "2023",
        "authors": [
            "Jeroen Berrevoets",
            "Krzysztof Kacprzyk",
            "Zhaozhi Qian",
            "Mihaela van der Schaar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.14615v1",
        "title": "Explainable Artificial Intelligence Architecture for Melanoma Diagnosis\n  Using Indicator Localization and Self-Supervised Learning",
        "abstract": "  Melanoma is a prevalent lethal type of cancer that is treatable if diagnosed\nat early stages of development. Skin lesions are a typical indicator for\ndiagnosing melanoma but they often led to delayed diagnosis due to high\nsimilarities of cancerous and benign lesions at early stages of melanoma. Deep\nlearning (DL) can be used as a solution to classify skin lesion pictures with a\nhigh accuracy, but clinical adoption of deep learning faces a significant\nchallenge. The reason is that the decision processes of deep learning models\nare often uninterpretable which makes them black boxes that are challenging to\ntrust. We develop an explainable deep learning architecture for melanoma\ndiagnosis which generates clinically interpretable visual explanations for its\ndecisions. Our experiments demonstrate that our proposed architectures matches\nclinical explanations significantly better than existing architectures.\n",
        "published": "2023",
        "authors": [
            "Ruitong Sun",
            "Mohammad Rostami"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.17879v2",
        "title": "CoSMo: a Framework to Instantiate Conditioned Process Simulation Models",
        "abstract": "  Process simulation is gaining attention for its ability to assess potential\nperformance improvements and risks associated with business process changes.\nThe existing literature presents various techniques, generally grounded in\nprocess models discovered from event logs or built upon deep learning\nalgorithms. These techniques have specific strengths and limitations.\nTraditional approaches rooted in process models offer increased\ninterpretability, while those using deep learning excel at generalizing changes\nacross large event logs. However, the practical application of deep learning\nfaces challenges related to managing stochasticity and integrating information\nfor what-if analysis. This paper introduces a novel recurrent neural\narchitecture tailored to discover COnditioned process Simulation MOdels (CoSMo)\nbased on user-based constraints or any other nature of a-priori knowledge. This\narchitecture facilitates the simulation of event logs that adhere to specific\nconstraints by incorporating declarative-based rules into the learning phase as\nan attempt to fill the gap of incorporating information into deep learning\nmodels to perform what-if analysis. Experimental validation illustrates CoSMo's\nefficacy in simulating event logs while adhering to predefined declarative\nconditions, emphasizing both control-flow and data-flow perspectives.\n",
        "published": "2023",
        "authors": [
            "Rafael S. Oyamada",
            "Gabriel M. Tavares",
            "Paolo Ceravolo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.14922v2",
        "title": "Supervised and Unsupervised Deep Learning Approaches for EEG Seizure\n  Prediction",
        "abstract": "  Epilepsy affects more than 50 million people worldwide, making it one of the\nworld's most prevalent neurological diseases. The main symptom of epilepsy is\nseizures, which occur abruptly and can cause serious injury or death. The\nability to predict the occurrence of an epileptic seizure could alleviate many\nrisks and stresses people with epilepsy face. We formulate the problem of\ndetecting preictal (or pre-seizure) with reference to normal EEG as a precursor\nto incoming seizure. To this end, we developed several supervised deep learning\napproaches model to identify preictal EEG from normal EEG. We further develop\nnovel unsupervised deep learning approaches to train the models on only normal\nEEG, and detecting pre-seizure EEG as an anomalous event. These deep learning\nmodels were trained and evaluated on two large EEG seizure datasets in a\nperson-specific manner. We found that both supervised and unsupervised\napproaches are feasible; however, their performance varies depending on the\npatient, approach and architecture. This new line of research has the potential\nto develop therapeutic interventions and save human lives.\n",
        "published": "2023",
        "authors": [
            "Zakary Georgis-Yap",
            "Milos R. Popovic",
            "Shehroz S. Khan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.12073v2",
        "title": "GELU Activation Function in Deep Learning: A Comprehensive Mathematical\n  Analysis and Performance",
        "abstract": "  Selecting the most suitable activation function is a critical factor in the\neffectiveness of deep learning models, as it influences their learning\ncapacity, stability, and computational efficiency. In recent years, the\nGaussian Error Linear Unit (GELU) activation function has emerged as a dominant\nmethod, surpassing traditional functions such as the Rectified Linear Unit\n(ReLU) in various applications. This study presents a rigorous mathematical\ninvestigation of the GELU activation function, exploring its differentiability,\nboundedness, stationarity, and smoothness properties in detail. Additionally,\nwe conduct an extensive experimental comparison of the GELU function against a\nbroad range of alternative activation functions, utilizing a residual\nconvolutional network trained on the CIFAR-10, CIFAR-100, and STL-10 datasets\nas the empirical testbed. Our results demonstrate the superior performance of\nGELU compared to other activation functions, establishing its suitability for a\nwide range of deep learning applications. This comprehensive study contributes\nto a more profound understanding of the underlying mathematical properties of\nGELU and provides valuable insights for practitioners aiming to select\nactivation functions that optimally align with their specific objectives and\nconstraints in deep learning.\n",
        "published": "2023",
        "authors": [
            "Minhyeok Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.03067v1",
        "title": "DeepOnto: A Python Package for Ontology Engineering with Deep Learning",
        "abstract": "  Applying deep learning techniques, particularly language models (LMs), in\nontology engineering has raised widespread attention. However, deep learning\nframeworks like PyTorch and Tensorflow are predominantly developed for Python\nprogramming, while widely-used ontology APIs, such as the OWL API and Jena, are\nprimarily Java-based. To facilitate seamless integration of these frameworks\nand APIs, we present Deeponto, a Python package designed for ontology\nengineering. The package encompasses a core ontology processing module founded\non the widely-recognised and reliable OWL API, encapsulating its fundamental\nfeatures in a more \"Pythonic\" manner and extending its capabilities to include\nother essential components including reasoning, verbalisation, normalisation,\nprojection, and more. Building on this module, Deeponto offers a suite of\ntools, resources, and algorithms that support various ontology engineering\ntasks, such as ontology alignment and completion, by harnessing deep learning\nmethodologies, primarily pre-trained LMs. In this paper, we also demonstrate\nthe practical utility of Deeponto through two use-cases: the Digital Health\nCoaching in Samsung Research UK and the Bio-ML track of the Ontology Alignment\nEvaluation Initiative (OAEI).\n",
        "published": "2023",
        "authors": [
            "Yuan He",
            "Jiaoyan Chen",
            "Hang Dong",
            "Ian Horrocks",
            "Carlo Allocca",
            "Taehun Kim",
            "Brahmananda Sapkota"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.05373v1",
        "title": "Classification of sleep stages from EEG, EOG and EMG signals by SSNet",
        "abstract": "  Classification of sleep stages plays an essential role in diagnosing\nsleep-related diseases including Sleep Disorder Breathing (SDB) disease. In\nthis study, we propose an end-to-end deep learning architecture, named SSNet,\nwhich comprises of two deep learning networks based on Convolutional Neuron\nNetworks (CNN) and Long Short Term Memory (LSTM). Both deep learning networks\nextract features from the combination of Electrooculogram (EOG),\nElectroencephalogram (EEG), and Electromyogram (EMG) signals, as each signal\nhas distinct features that help in the classification of sleep stages. The\nfeatures produced by the two-deep learning networks are concatenated to pass to\nthe fully connected layer for the classification. The performance of our\nproposed model is evaluated by using two public datasets Sleep-EDF Expanded\ndataset and ISRUC-Sleep dataset. The accuracy and Kappa coefficient are 96.36%\nand 93.40% respectively, for classifying three classes of sleep stages using\nSleep-EDF Expanded dataset. Whereas, the accuracy and Kappa coefficient are\n96.57% and 83.05% respectively for five classes of sleep stages using Sleep-EDF\nExpanded dataset. Our model achieves the best performance in classifying sleep\nstages when compared with the state-of-the-art techniques.\n",
        "published": "2023",
        "authors": [
            "Haifa Almutairi",
            "Ghulam Mubashar Hassan",
            "Amitava Datta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.16015v1",
        "title": "Physically Explainable Deep Learning for Convective Initiation\n  Nowcasting Using GOES-16 Satellite Observations",
        "abstract": "  Convection initiation (CI) nowcasting remains a challenging problem for both\nnumerical weather prediction models and existing nowcasting algorithms. In this\nstudy, object-based probabilistic deep learning models are developed to predict\nCI based on multichannel infrared GOES-R satellite observations. The data come\nfrom patches surrounding potential CI events identified in Multi-Radar\nMulti-Sensor Doppler weather radar products over the Great Plains region from\nJune and July 2020 and June 2021. An objective radar-based approach is used to\nidentify these events. The deep learning models significantly outperform the\nclassical logistic model at lead times up to 1 hour, especially on the false\nalarm ratio. Through case studies, the deep learning model exhibits the\ndependence on the characteristics of clouds and moisture at multiple levels.\nModel explanation further reveals the model's decision-making process with\ndifferent baselines. The explanation results highlight the importance of\nmoisture and cloud features at different levels depending on the choice of\nbaseline. Our study demonstrates the advantage of using different baselines in\nfurther understanding model behavior and gaining scientific insights.\n",
        "published": "2023",
        "authors": [
            "Da Fan",
            "Steven J. Greybush",
            "David John Gagne II",
            "Eugene E. Clothiaux"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.19957v1",
        "title": "Deep Learning for Spatiotemporal Big Data: A Vision on Opportunities and\n  Challenges",
        "abstract": "  With advancements in GPS, remote sensing, and computational simulation, an\nenormous volume of spatiotemporal data is being collected at an increasing\nspeed from various application domains, spanning Earth sciences, agriculture,\nsmart cities, and public safety. Such emerging geospatial and spatiotemporal\nbig data, coupled with recent advances in deep learning technologies, foster\nnew opportunities to solve problems that have not been possible before. For\ninstance, remote sensing researchers can potentially train a foundation model\nusing Earth imagery big data for numerous land cover and land use modeling\ntasks. Coastal modelers can train AI surrogates to speed up numerical\nsimulations. However, the distinctive characteristics of spatiotemporal big\ndata pose new challenges for deep learning technologies. This vision paper\nintroduces various types of spatiotemporal big data, discusses new research\nopportunities in the realm of deep learning applied to spatiotemporal big data,\nlists the unique challenges, and identifies several future research needs.\n",
        "published": "2023",
        "authors": [
            "Zhe Jiang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.09358v3",
        "title": "Trends in Integration of Vision and Language Research: A Survey of\n  Tasks, Datasets, and Methods",
        "abstract": "  Interest in Artificial Intelligence (AI) and its applications has seen\nunprecedented growth in the last few years. This success can be partly\nattributed to the advancements made in the sub-fields of AI such as machine\nlearning, computer vision, and natural language processing. Much of the growth\nin these fields has been made possible with deep learning, a sub-area of\nmachine learning that uses artificial neural networks. This has created\nsignificant interest in the integration of vision and language. In this survey,\nwe focus on ten prominent tasks that integrate language and vision by\ndiscussing their problem formulation, methods, existing datasets, evaluation\nmeasures, and compare the results obtained with corresponding state-of-the-art\nmethods. Our efforts go beyond earlier surveys which are either task-specific\nor concentrate only on one type of visual content, i.e., image or video.\nFurthermore, we also provide some potential future directions in this field of\nresearch with an anticipation that this survey stimulates innovative thoughts\nand ideas to address the existing challenges and build new applications.\n",
        "published": "2019",
        "authors": [
            "Aditya Mogadala",
            "Marimuthu Kalimuthu",
            "Dietrich Klakow"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.08796v1",
        "title": "Artificial Intelligence for Pediatric Ophthalmology",
        "abstract": "  PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning\n",
        "published": "2019",
        "authors": [
            "Julia E. Reid",
            "Eric Eaton"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.09844v2",
        "title": "Assessing clinical utility of Machine Learning and Artificial\n  Intelligence approaches to analyze speech recordings in Multiple Sclerosis: A\n  Pilot Study",
        "abstract": "  Background: An early diagnosis together with an accurate disease progression\nmonitoring of multiple sclerosis is an important component of successful\ndisease management. Prior studies have established that multiple sclerosis is\ncorrelated with speech discrepancies. Early research using objective acoustic\nmeasurements has discovered measurable dysarthria.\n  Objective: To determine the potential clinical utility of machine learning\nand deep learning/AI approaches for the aiding of diagnosis, biomarker\nextraction and progression monitoring of multiple sclerosis using speech\nrecordings.\n  Methods: A corpus of 65 MS-positive and 66 healthy individuals reading the\nsame text aloud was used for targeted acoustic feature extraction utilizing\nautomatic phoneme segmentation. A series of binary classification models was\ntrained, tuned, and evaluated regarding their Accuracy and area-under-curve.\n  Results: The Random Forest model performed best, achieving an Accuracy of\n0.82 on the validation dataset and an area-under-curve of 0.76 across 5 k-fold\ncycles on the training dataset. 5 out of 7 acoustic features were statistically\nsignificant.\n  Conclusion: Machine learning and artificial intelligence in automatic\nanalyses of voice recordings for aiding MS diagnosis and progression tracking\nseems promising. Further clinical validation of these methods and their mapping\nonto multiple sclerosis progression is needed, as well as a validating utility\nfor English-speaking populations.\n",
        "published": "2021",
        "authors": [
            "Emil Svoboda",
            "Tom\u00e1\u0161 Bo\u0159il",
            "Jan Rusz",
            "Tereza Tykalov\u00e1",
            "Dana Hor\u00e1kov\u00e1",
            "Charles R. G. Guttman",
            "Krastan B. Blagoev",
            "Hiroto Hatabu",
            "Vlad I. Valtchinov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.01156v2",
        "title": "Engineering the Neural Automatic Passenger Counter",
        "abstract": "  Automatic passenger counting (APC) in public transportation has been\napproached with various machine learning and artificial intelligence methods\nsince its introduction in the 1970s. While equivalence testing is becoming more\npopular than difference detection (Student's t-test), the former is much more\ndifficult to pass to ensure low user risk. On the other hand, recent\ndevelopments in artificial intelligence have led to algorithms that promise\nmuch higher counting quality (lower bias). However, gradient-based methods\n(including Deep Learning) have one limitation: they typically run into local\noptima. In this work, we explore and exploit various aspects of machine\nlearning to increase reliability, performance, and counting quality. We perform\na grid search with several fundamental parameters: the selection and size of\nthe training set, which is similar to cross-validation, and the initial network\nweights and randomness during the training process. Using this experiment, we\nshow how aggregation techniques such as ensemble quantiles can reduce bias, and\nwe give an idea of the overall spread of the results. We utilize the test\nsuccess chance, a simulative metric based on the empirical distribution. We\nalso employ a post-training Monte Carlo quantization approach and introduce\ncumulative summation to turn counting into a stationary method and allow\nunbounded counts.\n",
        "published": "2022",
        "authors": [
            "Nico Jahn",
            "Michael Siebert"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.08631v2",
        "title": "SEGEN: Sample-Ensemble Genetic Evolutional Network Model",
        "abstract": "  Deep learning, a rebranding of deep neural network research works, has\nachieved a remarkable success in recent years. With multiple hidden layers,\ndeep learning models aim at computing the hierarchical feature representations\nof the observational data. Meanwhile, due to its severe disadvantages in data\nconsumption, computational resources, parameter tuning costs and the lack of\nresult explainability, deep learning has also suffered from lots of criticism.\nIn this paper, we will introduce a new representation learning model, namely\n\"Sample-Ensemble Genetic Evolutionary Network\" (SEGEN), which can serve as an\nalternative approach to deep learning models. Instead of building one single\ndeep model, based on a set of sampled sub-instances, SEGEN adopts a\ngenetic-evolutionary learning strategy to build a group of unit models\ngenerations by generations. The unit models incorporated in SEGEN can be either\ntraditional machine learning models or the recent deep learning models with a\nmuch \"narrower\" and \"shallower\" architecture. The learning results of each\ninstance at the final generation will be effectively combined from each unit\nmodel via diffusive propagation and ensemble learning strategies. From the\ncomputational perspective, SEGEN requires far less data, fewer computational\nresources and parameter tuning efforts, but has sound theoretic\ninterpretability of the learning process and results. Extensive experiments\nhave been done on several different real-world benchmark datasets, and the\nexperimental results obtained by SEGEN have demonstrated its advantages over\nthe state-of-the-art representation learning models.\n",
        "published": "2018",
        "authors": [
            "Jiawei Zhang",
            "Limeng Cui",
            "Fisher B. Gouza"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.01587v1",
        "title": "GAN Lab: Understanding Complex Deep Generative Models using Interactive\n  Visual Experimentation",
        "abstract": "  Recent success in deep learning has generated immense interest among\npractitioners and students, inspiring many to learn about this new technology.\nWhile visual and interactive approaches have been successfully developed to\nhelp people more easily learn deep learning, most existing tools focus on\nsimpler models. In this work, we present GAN Lab, the first interactive\nvisualization tool designed for non-experts to learn and experiment with\nGenerative Adversarial Networks (GANs), a popular class of complex deep\nlearning models. With GAN Lab, users can interactively train generative models\nand visualize the dynamic training process's intermediate results. GAN Lab\ntightly integrates an model overview graph that summarizes GAN's structure, and\na layered distributions view that helps users interpret the interplay between\nsubmodels. GAN Lab introduces new interactive experimentation features for\nlearning complex deep learning models, such as step-by-step training at\nmultiple levels of abstraction for understanding intricate training dynamics.\nImplemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web\nbrowsers, without the need for installation or specialized hardware, overcoming\na major practical challenge in deploying interactive tools for deep learning.\n",
        "published": "2018",
        "authors": [
            "Minsuk Kahng",
            "Nikhil Thorat",
            "Duen Horng Chau",
            "Fernanda Vi\u00e9gas",
            "Martin Wattenberg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1708.00807v1",
        "title": "Adversarial-Playground: A Visualization Suite Showing How Adversarial\n  Examples Fool Deep Learning",
        "abstract": "  Recent studies have shown that attackers can force deep learning models to\nmisclassify so-called \"adversarial examples\": maliciously generated images\nformed by making imperceptible modifications to pixel values. With growing\ninterest in deep learning for security applications, it is important for\nsecurity experts and users of machine learning to recognize how learning\nsystems may be attacked. Due to the complex nature of deep learning, it is\nchallenging to understand how deep models can be fooled by adversarial\nexamples. Thus, we present a web-based visualization tool,\nAdversarial-Playground, to demonstrate the efficacy of common adversarial\nmethods against a convolutional neural network (CNN) system.\nAdversarial-Playground is educational, modular and interactive. (1) It enables\nnon-experts to compare examples visually and to understand why an adversarial\nexample can fool a CNN-based image classifier. (2) It can help security experts\nexplore more vulnerability of deep learning as a software module. (3) Building\nan interactive visualization is challenging in this domain due to the large\nfeature space of image classification (generating adversarial examples is slow\nin general and visualizing images are costly). Through multiple novel design\nchoices, our tool can provide fast and accurate responses to user requests.\nEmpirically, we find that our client-server division strategy reduced the\nresponse time by an average of 1.5 seconds per sample. Our other innovation, a\nfaster variant of JSMA evasion algorithm, empirically performed twice as fast\nas JSMA and yet maintains a comparable evasion rate.\n  Project source code and data from our experiments available at:\nhttps://github.com/QData/AdversarialDNN-Playground\n",
        "published": "2017",
        "authors": [
            "Andrew P. Norton",
            "Yanjun Qi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.00105v1",
        "title": "Deep learning pipeline for image classification on mobile phones",
        "abstract": "  This article proposes and documents a machine-learning framework and tutorial\nfor classifying images using mobile phones. Compared to computers, the\nperformance of deep learning model performance degrades when deployed on a\nmobile phone and requires a systematic approach to find a model that performs\noptimally on both computers and mobile phones. By following the proposed\npipeline, which consists of various computational tools, simple procedural\nrecipes, and technical considerations, one can bring the power of deep learning\nmedical image classification to mobile devices, potentially unlocking new\ndomains of applications. The pipeline is demonstrated on four different\npublicly available datasets: COVID X-rays, COVID CT scans, leaves, and\ncolorectal cancer. We used two application development frameworks: TensorFlow\nLite (real-time testing) and Flutter (digital image testing) to test the\nproposed pipeline. We found that transferring deep learning models to a mobile\nphone is limited by hardware and classification accuracy drops. To address this\nissue, we proposed this pipeline to find an optimized model for mobile phones.\nFinally, we discuss additional applications and computational concerns related\nto deploying deep-learning models on phones, including real-time analysis and\nimage preprocessing. We believe the associated documentation and code can help\nphysicians and medical experts develop medical image classification\napplications for distribution.\n",
        "published": "2022",
        "authors": [
            "Muhammad Muneeb",
            "Samuel F. Feng",
            "Andreas Henschel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.03868v1",
        "title": "Deep Learning for Brain Age Estimation: A Systematic Review",
        "abstract": "  Over the years, Machine Learning models have been successfully employed on\nneuroimaging data for accurately predicting brain age. Deviations from the\nhealthy brain aging pattern are associated to the accelerated brain aging and\nbrain abnormalities. Hence, efficient and accurate diagnosis techniques are\nrequired for eliciting accurate brain age estimations. Several contributions\nhave been reported in the past for this purpose, resorting to different\ndata-driven modeling methods. Recently, deep neural networks (also referred to\nas deep learning) have become prevalent in manifold neuroimaging studies,\nincluding brain age estimation. In this review, we offer a comprehensive\nanalysis of the literature related to the adoption of deep learning for brain\nage estimation with neuroimaging data. We detail and analyze different deep\nlearning architectures used for this application, pausing at research works\npublished to date quantitatively exploring their application. We also examine\ndifferent brain age estimation frameworks, comparatively exposing their\nadvantages and weaknesses. Finally, the review concludes with an outlook\ntowards future directions that should be followed by prospective studies. The\nultimate goal of this paper is to establish a common and informed reference for\nnewcomers and experienced researchers willing to approach brain age estimation\nby using deep learning models\n",
        "published": "2022",
        "authors": [
            "M. Tanveer",
            "M. A. Ganaie",
            "Iman Beheshti",
            "Tripti Goel",
            "Nehal Ahmad",
            "Kuan-Ting Lai",
            "Kaizhu Huang",
            "Yu-Dong Zhang",
            "Javier Del Ser",
            "Chin-Teng Lin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.07296v1",
        "title": "Optimized Three Deep Learning Models Based-PSO Hyperparameters for\n  Beijing PM2.5 Prediction",
        "abstract": "  Deep learning is a machine learning approach that produces excellent\nperformance in various applications, including natural language processing,\nimage identification, and forecasting. Deep learning network performance\ndepends on the hyperparameter settings. This research attempts to optimize the\ndeep learning architecture of Long short term memory (LSTM), Convolutional\nneural network (CNN), and Multilayer perceptron (MLP) for forecasting tasks\nusing Particle swarm optimization (PSO), a swarm intelligence-based\nmetaheuristic optimization methodology: Proposed M-1 (PSO-LSTM), M-2 (PSO-CNN),\nand M-3 (PSO-MLP). Beijing PM2.5 datasets was analyzed to measure the\nperformance of the proposed models. PM2.5 as a target variable was affected by\ndew point, pressure, temperature, cumulated wind speed, hours of snow, and\nhours of rain. The deep learning network inputs consist of three different\nscenarios: daily, weekly, and monthly. The results show that the proposed M-1\nwith three hidden layers produces the best results of RMSE and MAPE compared to\nthe proposed M-2, M-3, and all the baselines. A recommendation for air\npollution management could be generated by using these optimized models\n",
        "published": "2023",
        "authors": [
            "Andri Pranolo",
            "Yingchi Mao",
            "Aji Prasetya Wibawa",
            "Agung Bella Putra Utama",
            "Felix Andika Dwiyanto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.09615v1",
        "title": "Looking deeper into interpretable deep learning in neuroimaging: a\n  comprehensive survey",
        "abstract": "  Deep learning (DL) models have been popular due to their ability to learn\ndirectly from the raw data in an end-to-end paradigm, alleviating the concern\nof a separate error-prone feature extraction phase. Recent DL-based\nneuroimaging studies have also witnessed a noticeable performance advancement\nover traditional machine learning algorithms. But the challenges of deep\nlearning models still exist because of the lack of transparency in these models\nfor their successful deployment in real-world applications. In recent years,\nExplainable AI (XAI) has undergone a surge of developments mainly to get\nintuitions of how the models reached the decisions, which is essential for\nsafety-critical domains such as healthcare, finance, and law enforcement\nagencies. While the interpretability domain is advancing noticeably,\nresearchers are still unclear about what aspect of model learning a post hoc\nmethod reveals and how to validate its reliability. This paper comprehensively\nreviews interpretable deep learning models in the neuroimaging domain. Firstly,\nwe summarize the current status of interpretability resources in general,\nfocusing on the progression of methods, associated challenges, and opinions.\nSecondly, we discuss how multiple recent neuroimaging studies leveraged model\ninterpretability to capture anatomical and functional brain alterations most\nrelevant to model predictions. Finally, we discuss the limitations of the\ncurrent practices and offer some valuable insights and guidance on how we can\nsteer our future research directions to make deep learning models substantially\ninterpretable and thus advance scientific understanding of brain disorders.\n",
        "published": "2023",
        "authors": [
            "Md. Mahfuzur Rahman",
            "Vince D. Calhoun",
            "Sergey M. Plis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.12057v2",
        "title": "An Experimental Review on Deep Learning Architectures for Time Series\n  Forecasting",
        "abstract": "  In recent years, deep learning techniques have outperformed traditional\nmodels in many machine learning tasks. Deep neural networks have successfully\nbeen applied to address time series forecasting problems, which is a very\nimportant topic in data mining. They have proved to be an effective solution\ngiven their capacity to automatically learn the temporal dependencies present\nin time series. However, selecting the most convenient type of deep neural\nnetwork and its parametrization is a complex task that requires considerable\nexpertise. Therefore, there is a need for deeper studies on the suitability of\nall existing architectures for different forecasting tasks. In this work, we\nface two main challenges: a comprehensive review of the latest works using deep\nlearning for time series forecasting; and an experimental study comparing the\nperformance of the most popular architectures. The comparison involves a\nthorough analysis of seven types of deep learning models in terms of accuracy\nand efficiency. We evaluate the rankings and distribution of results obtained\nwith the proposed models under many different architecture configurations and\ntraining hyperparameters. The datasets used comprise more than 50000 time\nseries divided into 12 different forecasting problems. By training more than\n38000 models on these data, we provide the most extensive deep learning study\nfor time series forecasting. Among all studied models, the results show that\nlong short-term memory (LSTM) and convolutional networks (CNN) are the best\nalternatives, with LSTMs obtaining the most accurate forecasts. CNNs achieve\ncomparable performance with less variability of results under different\nparameter configurations, while also being more efficient.\n",
        "published": "2021",
        "authors": [
            "Pedro Lara-Ben\u00edtez",
            "Manuel Carranza-Garc\u00eda",
            "Jos\u00e9 C. Riquelme"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.12717v1",
        "title": "Benchmarking Bayesian Deep Learning on Diabetic Retinopathy Detection\n  Tasks",
        "abstract": "  Bayesian deep learning seeks to equip deep neural networks with the ability\nto precisely quantify their predictive uncertainty, and has promised to make\ndeep learning more reliable for safety-critical real-world applications. Yet,\nexisting Bayesian deep learning methods fall short of this promise; new methods\ncontinue to be evaluated on unrealistic test beds that do not reflect the\ncomplexities of downstream real-world tasks that would benefit most from\nreliable uncertainty quantification. We propose the RETINA Benchmark, a set of\nreal-world tasks that accurately reflect such complexities and are designed to\nassess the reliability of predictive models in safety-critical scenarios.\nSpecifically, we curate two publicly available datasets of high-resolution\nhuman retina images exhibiting varying degrees of diabetic retinopathy, a\nmedical condition that can lead to blindness, and use them to design a suite of\nautomated diagnosis tasks that require reliable predictive uncertainty\nquantification. We use these tasks to benchmark well-established and\nstate-of-the-art Bayesian deep learning methods on task-specific evaluation\nmetrics. We provide an easy-to-use codebase for fast and easy benchmarking\nfollowing reproducibility and software design principles. We provide\nimplementations of all methods included in the benchmark as well as results\ncomputed over 100 TPU days, 20 GPU days, 400 hyperparameter configurations, and\nevaluation on at least 6 random seeds each.\n",
        "published": "2022",
        "authors": [
            "Neil Band",
            "Tim G. J. Rudner",
            "Qixuan Feng",
            "Angelos Filos",
            "Zachary Nado",
            "Michael W. Dusenberry",
            "Ghassen Jerfel",
            "Dustin Tran",
            "Yarin Gal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.08291v1",
        "title": "Machine Learning Approaches in Agile Manufacturing with Recycled\n  Materials for Sustainability",
        "abstract": "  It is important to develop sustainable processes in materials science and\nmanufacturing that are environmentally friendly. AI can play a significant role\nin decision support here as evident from our earlier research leading to tools\ndeveloped using our proposed machine learning based approaches. Such tools\nserved the purpose of computational estimation and expert systems. This\nresearch addresses environmental sustainability in materials science via\ndecision support in agile manufacturing using recycled and reclaimed materials.\nIt is a safe and responsible way to turn a specific waste stream to value-added\nproducts. We propose to use data-driven methods in AI by applying machine\nlearning models for predictive analysis to guide decision support in\nmanufacturing. This includes harnessing artificial neural networks to study\nparameters affecting heat treatment of materials and impacts on their\nproperties; deep learning via advances such as convolutional neural networks to\nexplore grain size detection; and other classifiers such as Random Forests to\nanalyze phrase fraction detection. Results with all these methods seem\npromising to embark on further work, e.g. ANN yields accuracy around 90\\% for\npredicting micro-structure development as per quench tempering, a heat\ntreatment process. Future work entails several challenges: investigating\nvarious computer vision models (VGG, ResNet etc.) to find optimal accuracy,\nefficiency and robustness adequate for sustainable processes; creating\ndomain-specific tools using machine learning for decision support in agile\nmanufacturing; and assessing impacts on sustainability with metrics\nincorporating the appropriate use of recycled materials as well as the\neffectiveness of developed products. Our work makes impacts on green technology\nfor smart manufacturing, and is motivated by related work in the highly\ninteresting realm of AI for materials science.\n",
        "published": "2023",
        "authors": [
            "Aparna S. Varde",
            "Jianyu Liang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.01909v1",
        "title": "ClimateLearn: Benchmarking Machine Learning for Weather and Climate\n  Modeling",
        "abstract": "  Modeling weather and climate is an essential endeavor to understand the near-\nand long-term impacts of climate change, as well as inform technology and\npolicymaking for adaptation and mitigation efforts. In recent years, there has\nbeen a surging interest in applying data-driven methods based on machine\nlearning for solving core problems such as weather forecasting and climate\ndownscaling. Despite promising results, much of this progress has been impaired\ndue to the lack of large-scale, open-source efforts for reproducibility,\nresulting in the use of inconsistent or underspecified datasets, training\nsetups, and evaluations by both domain scientists and artificial intelligence\nresearchers. We introduce ClimateLearn, an open-source PyTorch library that\nvastly simplifies the training and evaluation of machine learning models for\ndata-driven climate science. ClimateLearn consists of holistic pipelines for\ndataset processing (e.g., ERA5, CMIP6, PRISM), implementation of\nstate-of-the-art deep learning models (e.g., Transformers, ResNets), and\nquantitative and qualitative evaluation for standard weather and climate\nmodeling tasks. We supplement these functionalities with extensive\ndocumentation, contribution guides, and quickstart tutorials to expand access\nand promote community growth. We have also performed comprehensive forecasting\nand downscaling experiments to showcase the capabilities and key features of\nour library. To our knowledge, ClimateLearn is the first large-scale,\nopen-source effort for bridging research in weather and climate modeling with\nmodern machine learning systems. Our library is available publicly at\nhttps://github.com/aditya-grover/climate-learn.\n",
        "published": "2023",
        "authors": [
            "Tung Nguyen",
            "Jason Jewik",
            "Hritik Bansal",
            "Prakhar Sharma",
            "Aditya Grover"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.07753v3",
        "title": "Do Not Sleep on Traditional Machine Learning: Simple and Interpretable\n  Techniques Are Competitive to Deep Learning for Sleep Scoring",
        "abstract": "  Over the last few years, research in automatic sleep scoring has mainly\nfocused on developing increasingly complex deep learning architectures.\nHowever, recently these approaches achieved only marginal improvements, often\nat the expense of requiring more data and more expensive training procedures.\nDespite all these efforts and their satisfactory performance, automatic sleep\nstaging solutions are not widely adopted in a clinical context yet. We argue\nthat most deep learning solutions for sleep scoring are limited in their\nreal-world applicability as they are hard to train, deploy, and reproduce.\nMoreover, these solutions lack interpretability and transparency, which are\noften key to increase adoption rates. In this work, we revisit the problem of\nsleep stage classification using classical machine learning. Results show that\ncompetitive performance can be achieved with a conventional machine learning\npipeline consisting of preprocessing, feature extraction, and a simple machine\nlearning model. In particular, we analyze the performance of a linear model and\na non-linear (gradient boosting) model. Our approach surpasses state-of-the-art\n(that uses the same data) on two public datasets: Sleep-EDF SC-20 (MF1 0.810)\nand Sleep-EDF ST (MF1 0.795), while achieving competitive results on Sleep-EDF\nSC-78 (MF1 0.775) and MASS SS3 (MF1 0.817). We show that, for the sleep stage\nscoring task, the expressiveness of an engineered feature vector is on par with\nthe internally learned representations of deep learning models. This\nobservation opens the door to clinical adoption, as a representative feature\nvector allows to leverage both the interpretability and successful track record\nof traditional machine learning models.\n",
        "published": "2022",
        "authors": [
            "Jeroen Van Der Donckt",
            "Jonas Van Der Donckt",
            "Emiel Deprost",
            "Nicolas Vandenbussche",
            "Michael Rademaker",
            "Gilles Vandewiele",
            "Sofie Van Hoecke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.10293v1",
        "title": "Bosch Deep Learning Hardware Benchmark",
        "abstract": "  The widespread use of Deep Learning (DL) applications in science and industry\nhas created a large demand for efficient inference systems. This has resulted\nin a rapid increase of available Hardware Accelerators (HWAs) making comparison\nchallenging and laborious. To address this, several DL hardware benchmarks have\nbeen proposed aiming at a comprehensive comparison for many models, tasks, and\nhardware platforms. Here, we present our DL hardware benchmark which has been\nspecifically developed for inference on embedded HWAs and tasks required for\nautonomous driving. In addition to previous benchmarks, we propose a new\ngranularity level to evaluate common submodules of DL models, a twofold\nbenchmark procedure that accounts for hardware and model optimizations done by\nHWA manufacturers, and an extended set of performance indicators that can help\nto identify a mismatch between a HWA and the DL models used in our benchmark.\n",
        "published": "2020",
        "authors": [
            "Armin Runge",
            "Thomas Wenzel",
            "Dimitrios Bariamis",
            "Benedikt Sebastian Staffler",
            "Lucas Rego Drumond",
            "Michael Pfeiffer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.06088v1",
        "title": "Neural-Symbolic Computing: An Effective Methodology for Principled\n  Integration of Machine Learning and Reasoning",
        "abstract": "  Current advances in Artificial Intelligence and machine learning in general,\nand deep learning in particular have reached unprecedented impact not only\nacross research communities, but also over popular media channels. However,\nconcerns about interpretability and accountability of AI have been raised by\ninfluential thinkers. In spite of the recent impact of AI, several works have\nidentified the need for principled knowledge representation and reasoning\nmechanisms integrated with deep learning-based systems to provide sound and\nexplainable models for such systems. Neural-symbolic computing aims at\nintegrating, as foreseen by Valiant, two most fundamental cognitive abilities:\nthe ability to learn from the environment, and the ability to reason from what\nhas been learned. Neural-symbolic computing has been an active topic of\nresearch for many years, reconciling the advantages of robust learning in\nneural networks and reasoning and interpretability of symbolic representation.\nIn this paper, we survey recent accomplishments of neural-symbolic computing as\na principled methodology for integrated machine learning and reasoning. We\nillustrate the effectiveness of the approach by outlining the main\ncharacteristics of the methodology: principled integration of neural learning\nwith symbolic knowledge representation and reasoning allowing for the\nconstruction of explainable AI systems. The insights provided by\nneural-symbolic computing shed new light on the increasingly prominent need for\ninterpretable and accountable AI systems.\n",
        "published": "2019",
        "authors": [
            "Artur d'Avila Garcez",
            "Marco Gori",
            "Luis C. Lamb",
            "Luciano Serafini",
            "Michael Spranger",
            "Son N. Tran"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.08268v1",
        "title": "A Comparative Study of Machine Learning and Deep Learning Techniques for\n  Prediction of Co2 Emission in Cars",
        "abstract": "  The most recent concern of all people on Earth is the increase in the\nconcentration of greenhouse gas in the atmosphere. The concentration of these\ngases has risen rapidly over the last century and if the trend continues it can\ncause many adverse climatic changes. There have been ways implemented to curb\nthis by the government by limiting processes that emit a higher amount of CO2,\none such greenhouse gas. However, there is mounting evidence that the CO2\nnumbers supplied by the government do not accurately reflect the performance of\nautomobiles on the road. Our proposal of using artificial intelligence\ntechniques to improve a previously rudimentary process takes a radical tack,\nbut it fits the bill given the situation. To determine which algorithms and\nmodels produce the greatest outcomes, we compared them all and explored a novel\nmethod of ensembling them. Further, this can be used to foretell the rise in\nglobal temperature and to ground crucial policy decisions like the adoption of\nelectric vehicles. To estimate emissions from vehicles, we used machine\nlearning, deep learning, and ensemble learning on a massive dataset.\n",
        "published": "2022",
        "authors": [
            "Samveg Shah",
            "Shubham Thakar",
            "Kashish Jain",
            "Bhavya Shah",
            "Sudhir Dhage"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.12580v1",
        "title": "Assessing Regulatory Risk in Personal Financial Advice Documents: a\n  Pilot Study",
        "abstract": "  Assessing regulatory compliance of personal financial advice is currently a\ncomplex manual process. In Australia, only 5%- 15% of advice documents are\naudited annually and 75% of these are found to be non-compliant(ASI 2018b).\nThis paper describes a pilot with an Australian government regulation agency\nwhere Artificial Intelligence (AI) models based on techniques such natural\nlanguage processing (NLP), machine learning and deep learning were developed to\nmethodically characterise the regulatory risk status of personal financial\nadvice documents. The solution provides traffic light rating of advice\ndocuments for various risk factors enabling comprehensive coverage of documents\nin the review and allowing rapid identification of documents that are at high\nrisk of non-compliance with government regulations. This pilot serves as a case\nstudy of public-private partnership in developing AI systems for government and\npublic sector.\n",
        "published": "2019",
        "authors": [
            "Wanita Sherchan",
            "Simon Harris",
            "Sue Ann Chen",
            "Nebula Alam",
            "Khoi-Nguyen Tran",
            "Adam J. Makarucha",
            "Christopher J. Butler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.02948v1",
        "title": "A Survey on Artificial Intelligence Trends in Spacecraft Guidance\n  Dynamics and Control",
        "abstract": "  The rapid developments of Artificial Intelligence in the last decade are\ninfluencing Aerospace Engineering to a great extent and research in this\ncontext is proliferating. We share our observations on the recent developments\nin the area of Spacecraft Guidance Dynamics and Control, giving selected\nexamples on success stories that have been motivated by mission designs. Our\nfocus is on evolutionary optimisation, tree searches and machine learning,\nincluding deep learning and reinforcement learning as the key technologies and\ndrivers for current and future research in the field. From a high-level\nperspective, we survey various scenarios for which these approaches have been\nsuccessfully applied or are under strong scientific investigation. Whenever\npossible, we highlight the relations and synergies that can be obtained by\ncombining different techniques and projects towards future domains for which\nnewly emerging artificial intelligence techniques are expected to become game\nchangers.\n",
        "published": "2018",
        "authors": [
            "Dario Izzo",
            "Marcus M\u00e4rtens",
            "Binfeng Pan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.07629v2",
        "title": "Deep Learning for Abstract Argumentation Semantics",
        "abstract": "  In this paper, we present a learning-based approach to determining acceptance\nof arguments under several abstract argumentation semantics. More specifically,\nwe propose an argumentation graph neural network (AGNN) that learns a\nmessage-passing algorithm to predict the likelihood of an argument being\naccepted. The experimental results demonstrate that the AGNN can almost\nperfectly predict the acceptability under different semantics and scales well\nfor larger argumentation frameworks. Furthermore, analysing the behaviour of\nthe message-passing algorithm shows that the AGNN learns to adhere to basic\nprinciples of argument semantics as identified in the literature, and can thus\nbe trained to predict extensions under the different semantics - we show how\nthe latter can be done for multi-extension semantics by using AGNNs to guide a\nbasic search. We publish our code at\nhttps://github.com/DennisCraandijk/DL-Abstract-Argumentation\n",
        "published": "2020",
        "authors": [
            "Dennis Craandijk",
            "Floris Bex"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.10899v1",
        "title": "Artificial Intelligence for Satellite Communication: A Review",
        "abstract": "  Satellite communication offers the prospect of service continuity over\nuncovered and under-covered areas, service ubiquity, and service scalability.\nHowever, several challenges must first be addressed to realize these benefits,\nas the resource management, network control, network security, spectrum\nmanagement, and energy usage of satellite networks are more challenging than\nthat of terrestrial networks. Meanwhile, artificial intelligence (AI),\nincluding machine learning, deep learning, and reinforcement learning, has been\nsteadily growing as a research field and has shown successful results in\ndiverse applications, including wireless communication. In particular, the\napplication of AI to a wide variety of satellite communication aspects have\ndemonstrated excellent potential, including beam-hopping, anti-jamming, network\ntraffic forecasting, channel modeling, telemetry mining, ionospheric\nscintillation detecting, interference managing, remote sensing, behavior\nmodeling, space-air-ground integrating, and energy managing. This work thus\nprovides a general overview of AI, its diverse sub-fields, and its\nstate-of-the-art algorithms. Several challenges facing diverse aspects of\nsatellite communication systems are then discussed, and their proposed and\npotential AI-based solutions are presented. Finally, an outlook of field is\ndrawn, and future steps are suggested.\n",
        "published": "2021",
        "authors": [
            "Fares Fourati",
            "Mohamed-Slim Alouini"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.10298v1",
        "title": "Artificial Intelligence-Based Image Reconstruction in Cardiac Magnetic\n  Resonance",
        "abstract": "  Artificial intelligence (AI) and Machine Learning (ML) have shown great\npotential in improving the medical imaging workflow, from image acquisition and\nreconstruction to disease diagnosis and treatment. Particularly, in recent\nyears, there has been a significant growth in the use of AI and ML algorithms,\nespecially Deep Learning (DL) based methods, for medical image reconstruction.\nDL techniques have shown to be competitive and often superior over conventional\nreconstruction methods in terms of both reconstruction quality and\ncomputational efficiency. The use of DL-based image reconstruction also\nprovides promising opportunities to transform the way cardiac images are\nacquired and reconstructed. In this chapter, we will review recent advances in\nDL-based reconstruction techniques for cardiac imaging, with emphasis on\ncardiac magnetic resonance (CMR) image reconstruction. We mainly focus on\nsupervised DL methods for the application, including image post-processing\ntechniques, model-driven approaches and k-space based methods. Current\nlimitations, challenges and future opportunities of DL for cardiac image\nreconstruction are also discussed.\n",
        "published": "2022",
        "authors": [
            "Chen Qin",
            "Daniel Rueckert"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.03279v2",
        "title": "From Knowledge Augmentation to Multi-tasking: Towards Human-like\n  Dialogue Systems",
        "abstract": "  The goal of building dialogue agents that can converse with humans naturally\nhas been a long-standing dream of researchers since the early days of\nartificial intelligence. The well-known Turing Test proposed to judge the\nultimate validity of an artificial intelligence agent on the\nindistinguishability of its dialogues from humans'. It should come as no\nsurprise that human-level dialogue systems are very challenging to build. But,\nwhile early effort on rule-based systems found limited success, the emergence\nof deep learning enabled great advance on this topic.\n  In this thesis, we focus on methods that address the numerous issues that\nhave been imposing the gap between artificial conversational agents and\nhuman-level interlocutors. These methods were proposed and experimented with in\nways that were inspired by general state-of-the-art AI methodologies. But they\nalso targeted the characteristics that dialogue systems possess.\n",
        "published": "2022",
        "authors": [
            "Tom Young"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.09721v1",
        "title": "A new solution and concrete implementation steps for Artificial General\n  Intelligence",
        "abstract": "  At present, the mainstream artificial intelligence generally adopts the\ntechnical path of \"attention mechanism + deep learning\" + \"reinforcement\nlearning\". It has made great progress in the field of AIGC (Artificial\nIntelligence Generated Content), setting off the technical wave of big models[\n2][13 ]. But in areas that need to interact with the actual environment, such\nas elderly care, home nanny, agricultural production, and vehicle driving,\ntrial and error are expensive and a reinforcement learning process that\nrequires much trial and error is difficult to achieve. Therefore, in order to\nachieve Artificial General Intelligence(AGI) that can be applied to any field,\nwe need to use both existing technologies and solve the defects of existing\ntechnologies, so as to further develop the technological wave of artificial\nintelligence. In this paper, we analyze the limitations of the technical route\nof large models, and by addressing these limitations, we propose solutions,\nthus solving the inherent defects of large models. In this paper, we will\nreveal how to achieve true AGI step by step.\n",
        "published": "2023",
        "authors": [
            "Yongcong Chen",
            "Ting Zeng",
            "Jun Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.12031v1",
        "title": "CACTUS: a Comprehensive Abstraction and Classification Tool for\n  Uncovering Structures",
        "abstract": "  The availability of large data sets is providing an impetus for driving\ncurrent artificial intelligent developments. There are, however, challenges for\ndeveloping solutions with small data sets due to practical and cost-effective\ndeployment and the opacity of deep learning models. The Comprehensive\nAbstraction and Classification Tool for Uncovering Structures called CACTUS is\npresented for improved secure analytics by effectively employing explainable\nartificial intelligence. It provides additional support for categorical\nattributes, preserving their original meaning, optimising memory usage, and\nspeeding up the computation through parallelisation. It shows to the user the\nfrequency of the attributes in each class and ranks them by their\ndiscriminative power. Its performance is assessed by application to the\nWisconsin diagnostic breast cancer and Thyroid0387 data sets.\n",
        "published": "2023",
        "authors": [
            "Luca Gherardini",
            "Varun Ravi Varma",
            "Karol Capala",
            "Roger Woods",
            "Jose Sousa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.03380v2",
        "title": "An attempt to generate new bridge types from latent space of variational\n  autoencoder",
        "abstract": "  Try to generate new bridge types using generative artificial intelligence\ntechnology. The grayscale images of the bridge facade with the change of\ncomponent width was rendered by 3dsMax animation software, and then the OpenCV\nmodule performed an appropriate amount of geometric transformation (rotation,\nhorizontal scale, vertical scale) to obtain the image dataset of three-span\nbeam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on\nPython programming language, TensorFlow and Keras deep learning platform\nframework, variational autoencoder was constructed and trained, and\nlow-dimensional bridge-type latent space that is convenient for vector\noperations was obtained. Variational autoencoder can combine two bridge types\non the basis of the original of human into one that is a new bridge type.\nGenerative artificial intelligence technology can assist bridge designers in\nbridge-type innovation, and can be used as copilot.\n",
        "published": "2023",
        "authors": [
            "Hongjun Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.07003v1",
        "title": "RACER: Rational Artificial Intelligence Car-following-model Enhanced by\n  Reality",
        "abstract": "  This paper introduces RACER, the Rational Artificial Intelligence\nCar-following model Enhanced by Reality, a cutting-edge deep learning\ncar-following model, that satisfies partial derivative constraints, designed to\npredict Adaptive Cruise Control (ACC) driving behavior while staying\ntheoretically feasible. Unlike conventional models, RACER effectively\nintegrates Rational Driving Constraints (RDCs), crucial tenets of actual\ndriving, resulting in strikingly accurate and realistic predictions. Against\nestablished models like the Optimal Velocity Relative Velocity (OVRV), a\ncar-following Neural Network (NN), and a car-following Physics-Informed Neural\nNetwork (PINN), RACER excels across key metrics, such as acceleration,\nvelocity, and spacing. Notably, it displays a perfect adherence to the RDCs,\nregistering zero violations, in stark contrast to other models. This study\nhighlights the immense value of incorporating physical constraints within AI\nmodels, especially for augmenting safety measures in transportation. It also\npaves the way for future research to test these models against human driving\ndata, with the potential to guide safer and more rational driving behavior. The\nversatility of the proposed model, including its potential to incorporate\nadditional derivative constraints and broader architectural applications,\nenhances its appeal and broadens its impact within the scientific community.\n",
        "published": "2023",
        "authors": [
            "Tianyi Li",
            "Alexander Halatsis",
            "Raphael Stern"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.00700v1",
        "title": "An attempt to generate new bridge types from latent space of generative\n  adversarial network",
        "abstract": "  Try to generate new bridge types using generative artificial intelligence\ntechnology. Symmetric structured image dataset of three-span beam bridge, arch\nbridge, cable-stayed bridge and suspension bridge are used . Based on Python\nprogramming language, TensorFlow and Keras deep learning platform framework ,\nas well as Wasserstein loss function and Lipschitz constraints, generative\nadversarial network is constructed and trained. From the obtained low\ndimensional bridge-type latent space sampling, new bridge types with asymmetric\nstructures can be generated. Generative adversarial network can create new\nbridge types by organically combining different structural components on the\nbasis of human original bridge types. It has a certain degree of human original\nability. Generative artificial intelligence technology can open up imagination\nspace and inspire humanity.\n",
        "published": "2024",
        "authors": [
            "Hongjun Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.13585v1",
        "title": "Interpreting convolutional networks trained on textual data",
        "abstract": "  There have been many advances in the artificial intelligence field due to the\nemergence of deep learning. In almost all sub-fields, artificial neural\nnetworks have reached or exceeded human-level performance. However, most of the\nmodels are not interpretable. As a result, it is hard to trust their decisions,\nespecially in life and death scenarios. In recent years, there has been a\nmovement toward creating explainable artificial intelligence, but most work to\ndate has concentrated on image processing models, as it is easier for humans to\nperceive visual patterns. There has been little work in other fields like\nnatural language processing. In this paper, we train a convolutional model on\ntextual data and analyze the global logic of the model by studying its filter\nvalues. In the end, we find the most important words in our corpus to our\nmodels logic and remove the rest (95%). New models trained on just the 5% most\nimportant words can achieve the same performance as the original model while\nreducing training time by more than half. Approaches such as this will help us\nto understand NLP models, explain their decisions according to their word\nchoices, and improve them by finding blind spots and biases.\n",
        "published": "2020",
        "authors": [
            "Reza Marzban",
            "Christopher John Crick"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.05593v2",
        "title": "Fast Construction of Correcting Ensembles for Legacy Artificial\n  Intelligence Systems: Algorithms and a Case Study",
        "abstract": "  This paper presents a technology for simple and computationally efficient\nimprovements of a generic Artificial Intelligence (AI) system, including\nMultilayer and Deep Learning neural networks. The improvements are, in essence,\nsmall network ensembles constructed on top of the existing AI architectures.\nTheoretical foundations of the technology are based on Stochastic Separation\nTheorems and the ideas of the concentration of measure. We show that, subject\nto mild technical assumptions on statistical properties of internal signals in\nthe original AI system, the technology enables instantaneous and\ncomputationally efficient removal of spurious and systematic errors with\nprobability close to one on the datasets which are exponentially large in\ndimension. The method is illustrated with numerical examples and a case study\nof ten digits recognition from American Sign Language.\n",
        "published": "2018",
        "authors": [
            "Ivan Y. Tyukin",
            "Alexander N. Gorban",
            "Stephen Green",
            "Danil Prokhorov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.14935v1",
        "title": "AS-XAI: Self-supervised Automatic Semantic Interpretation for CNN",
        "abstract": "  Explainable artificial intelligence (XAI) aims to develop transparent\nexplanatory approaches for \"black-box\" deep learning models. However,it remains\ndifficult for existing methods to achieve the trade-off of the three key\ncriteria in interpretability, namely, reliability, causality, and usability,\nwhich hinder their practical applications. In this paper, we propose a\nself-supervised automatic semantic interpretable explainable artificial\nintelligence (AS-XAI) framework, which utilizes transparent orthogonal\nembedding semantic extraction spaces and row-centered principal component\nanalysis (PCA) for global semantic interpretation of model decisions in the\nabsence of human interference, without additional computational costs. In\naddition, the invariance of filter feature high-rank decomposition is used to\nevaluate model sensitivity to different semantic concepts. Extensive\nexperiments demonstrate that robust and orthogonal semantic spaces can be\nautomatically extracted by AS-XAI, providing more effective global\ninterpretability for convolutional neural networks (CNNs) and generating\nhuman-comprehensible explanations. The proposed approach offers broad\nfine-grained extensible practical applications, including shared semantic\ninterpretation under out-of-distribution (OOD) categories, auxiliary\nexplanations for species that are challenging to distinguish, and\nclassification explanations from various perspectives.\n",
        "published": "2023",
        "authors": [
            "Changqi Sun",
            "Hao Xu",
            "Yuntian Chen",
            "Dongxiao Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.12698v2",
        "title": "Security and Privacy Preserving Deep Learning",
        "abstract": "  Commercial companies that collect user data on a large scale have been the\nmain beneficiaries of this trend since the success of deep learning techniques\nis directly proportional to the amount of data available for training. Massive\ndata collection required for deep learning presents obvious privacy issues.\nUsers personal, highly sensitive data such as photos and voice recordings are\nkept indefinitely by the companies that collect it. Users can neither delete it\nnor restrict the purposes for which it is used. So, data privacy has been a\nvery important concern for governments and companies these days. It gives rise\nto a very interesting challenge since on the one hand, we are pushing further\nand further for high-quality models and accessible data, but on the other hand,\nwe need to keep data safe from both intentional and accidental leakage. The\nmore personal the data is it is more restricted it means some of the most\nimportant social issues cannot be addressed using machine learning because\nresearchers do not have access to proper training data. But by learning how to\nmachine learning that protects privacy we can make a huge difference in solving\nmany social issues like curing disease etc. Deep neural networks are\nsusceptible to various inference attacks as they remember information about\ntheir training data. In this chapter, we introduce differential privacy, which\nensures that different kinds of statistical analyses dont compromise privacy\nand federated learning, training a machine learning model on a data to which we\ndo not have access to.\n",
        "published": "2020",
        "authors": [
            "Saichethan Miriyala Reddy",
            "Saisree Miriyala"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.01023v1",
        "title": "Physicist's Journeys Through the AI World - A Topical Review. There is\n  no royal road to unsupervised learning",
        "abstract": "  Artificial Intelligence (AI), defined in its most simple form, is a\ntechnological tool that makes machines intelligent. Since learning is at the\ncore of intelligence, machine learning poses itself as a core sub-field of AI.\nThen there comes a subclass of machine learning, known as deep learning, to\naddress the limitations of their predecessors. AI has generally acquired its\nprominence over the past few years due to its considerable progress in various\nfields. AI has vastly invaded the realm of research. This has led physicists to\nattentively direct their research towards implementing AI tools. Their central\naim has been to gain better understanding and enrich their intuition. This\nreview article is meant to supplement the previously presented efforts to\nbridge the gap between AI and physics, and take a serious step forward to\nfilter out the \"Babelian\" clashes brought about from such gabs. This\nnecessitates first to have fundamental knowledge about common AI tools. To this\nend, the review's primary focus shall be on deep learning models called\nartificial neural networks. They are deep learning models which train\nthemselves through different learning processes. It discusses also the concept\nof Markov decision processes. Finally, shortcut to the main goal, the review\nthoroughly examines how these neural networks are capable to construct a\nphysical theory describing some observations without applying any previous\nphysical knowledge.\n",
        "published": "2019",
        "authors": [
            "Imad Alhousseini",
            "Wissam Chemissany",
            "Fatima Kleit",
            "Aly Nasrallah"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.09279v1",
        "title": "A Big Data Analysis Framework Using Apache Spark and Deep Learning",
        "abstract": "  With the spreading prevalence of Big Data, many advances have recently been\nmade in this field. Frameworks such as Apache Hadoop and Apache Spark have\ngained a lot of traction over the past decades and have become massively\npopular, especially in industries. It is becoming increasingly evident that\neffective big data analysis is key to solving artificial intelligence problems.\nThus, a multi-algorithm library was implemented in the Spark framework, called\nMLlib. While this library supports multiple machine learning algorithms, there\nis still scope to use the Spark setup efficiently for highly time-intensive and\ncomputationally expensive procedures like deep learning. In this paper, we\npropose a novel framework that combines the distributive computational\nabilities of Apache Spark and the advanced machine learning architecture of a\ndeep multi-layer perceptron (MLP), using the popular concept of Cascade\nLearning. We conduct empirical analysis of our framework on two real world\ndatasets. The results are encouraging and corroborate our proposed framework,\nin turn proving that it is an improvement over traditional big data analysis\nmethods that use either Spark or Deep learning as individual elements.\n",
        "published": "2017",
        "authors": [
            "Anand Gupta",
            "Hardeo Thakur",
            "Ritvik Shrivastava",
            "Pulkit Kumar",
            "Sreyashi Nag"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.03272v3",
        "title": "Privacy-Preserving Deep Learning via Weight Transmission",
        "abstract": "  This paper considers the scenario that multiple data owners wish to apply a\nmachine learning method over the combined dataset of all owners to obtain the\nbest possible learning output but do not want to share the local datasets owing\nto privacy concerns. We design systems for the scenario that the stochastic\ngradient descent (SGD) algorithm is used as the machine learning method because\nSGD (or its variants) is at the heart of recent deep learning techniques over\nneural networks. Our systems differ from existing systems in the following\nfeatures: {\\bf (1)} any activation function can be used, meaning that no\nprivacy-preserving-friendly approximation is required; {\\bf (2)} gradients\ncomputed by SGD are not shared but the weight parameters are shared instead;\nand {\\bf (3)} robustness against colluding parties even in the extreme case\nthat only one honest party exists. We prove that our systems, while\nprivacy-preserving, achieve the same learning accuracy as SGD and hence retain\nthe merit of deep learning with respect to accuracy. Finally, we conduct\nseveral experiments using benchmark datasets, and show that our systems\noutperform previous system in terms of learning accuracies.\n",
        "published": "2018",
        "authors": [
            "Le Trieu Phong",
            "Tran Thi Phuong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.06461v1",
        "title": "Fake News Detection in Spanish Using Deep Learning Techniques",
        "abstract": "  This paper addresses the problem of fake news detection in Spanish using\nMachine Learning techniques. It is fundamentally the same problem tackled for\nthe English language; however, there is not a significant amount of publicly\navailable and adequately labeled fake news in Spanish to effectively train a\nMachine Learning model, similarly to those proposed for the English language.\nTherefore, this work explores different training strategies and architectures\nto establish a baseline for further research in this area. Four datasets were\nused, two in English and two in Spanish, and four experimental schemes were\ntested, including a baseline with classical Machine Learning models, trained\nand validated using a small dataset in Spanish. The remaining schemes include\nstate-of-the-art Deep Learning models trained (or fine-tuned) and validated in\nEnglish, trained and validated in Spanish, and fitted in English and validated\nwith automatic translated Spanish sentences. The Deep Learning architectures\nwere built on top of different pre-trained Word Embedding representations,\nincluding GloVe, ELMo, BERT, and BETO (a BERT version trained on a large corpus\nin Spanish). According to the results, the best strategy was a combination of a\npre-trained BETO model and a Recurrent Neural Network based on LSTM layers,\nyielding an accuracy of up to 80%; nonetheless, a baseline model using a Random\nForest estimator obtained similar outcomes. Additionally, the translation\nstrategy did not yield acceptable results because of the propagation error;\nthere was also observed a significant difference in models performance when\ntrained in English or Spanish, mainly attributable to the number of samples\navailable for each language.\n",
        "published": "2021",
        "authors": [
            "Kevin Mart\u00ednez-Gallego",
            "Andr\u00e9s M. \u00c1lvarez-Ortiz",
            "Juli\u00e1n D. Arias-Londo\u00f1o"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.13341v1",
        "title": "Learning principle and mathematical realization of the learning\n  mechanism in the brain",
        "abstract": "  While deep learning has achieved remarkable success, there is no clear\nexplanation about why it works so well. In order to discuss this question\nquantitatively, we need a mathematical framework that explains what learning is\nin the first place. After several considerations, we succeeded in constructing\na mathematical framework that can provide a unified understanding of all types\nof learning, including deep learning and learning in the brain. We call it\nlearning principle, and it follows that all learning is equivalent to\nestimating the probability of input data. We not only derived this principle,\nbut also mentioned its application to actual machine learning models. For\nexample, we found that conventional supervised learning is equivalent to\nestimating conditional probabilities, and succeeded in making supervised\nlearning more effective and generalized. We also proposed a new method of\ndefining the values of estimated probability using differentiation, and showed\nthat unsupervised learning can be performed on arbitrary dataset without any\nprior knowledge. Namely, this method is a general-purpose machine learning in\nthe true sense. Moreover, we succeeded in describing the learning mechanism in\nthe brain by considering the time evolution of a fully or partially connected\nmodel and applying this new method. The learning principle provides solutions\nto many unsolved problems in deep learning and cognitive neuroscience.\n",
        "published": "2023",
        "authors": [
            "Taisuke Katayose"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.06092v2",
        "title": "A Dataset and Architecture for Visual Reasoning with a Working Memory",
        "abstract": "  A vexing problem in artificial intelligence is reasoning about events that\noccur in complex, changing visual stimuli such as in video analysis or game\nplay. Inspired by a rich tradition of visual reasoning and memory in cognitive\npsychology and neuroscience, we developed an artificial, configurable visual\nquestion and answer dataset (COG) to parallel experiments in humans and\nanimals. COG is much simpler than the general problem of video analysis, yet it\naddresses many of the problems relating to visual and logical reasoning and\nmemory -- problems that remain challenging for modern deep learning\narchitectures. We additionally propose a deep learning architecture that\nperforms competitively on other diagnostic VQA datasets (i.e. CLEVR) as well as\neasy settings of the COG dataset. However, several settings of COG result in\ndatasets that are progressively more challenging to learn. After training, the\nnetwork can zero-shot generalize to many new tasks. Preliminary analyses of the\nnetwork architectures trained on COG demonstrate that the network accomplishes\nthe task in a manner interpretable to humans.\n",
        "published": "2018",
        "authors": [
            "Guangyu Robert Yang",
            "Igor Ganichev",
            "Xiao-Jing Wang",
            "Jonathon Shlens",
            "David Sussillo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.05227v1",
        "title": "Deep Learning Framework for Wireless Systems: Applications to Optical\n  Wireless Communications",
        "abstract": "  Optical wireless communication (OWC) is a promising technology for future\nwireless communications owing to its potentials for cost-effective network\ndeployment and high data rate. There are several implementation issues in the\nOWC which have not been encountered in radio frequency wireless communications.\nFirst, practical OWC transmitters need an illumination control on color,\nintensity, and luminance, etc., which poses complicated modulation design\nchallenges. Furthermore, signal-dependent properties of optical channels raise\nnon-trivial challenges both in modulation and demodulation of the optical\nsignals. To tackle such difficulties, deep learning (DL) technologies can be\napplied for optical wireless transceiver design. This article addresses recent\nefforts on DL-based OWC system designs. A DL framework for emerging image\nsensor communication is proposed and its feasibility is verified by simulation.\nFinally, technical challenges and implementation issues for the DL-based\noptical wireless technology are discussed.\n",
        "published": "2018",
        "authors": [
            "Hoon Lee",
            "Sang Hyun Lee",
            "Tony Q. S. Quek",
            "Inkyu Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.00110v3",
        "title": "Classical Planning in Deep Latent Space",
        "abstract": "  Current domain-independent, classical planners require symbolic models of the\nproblem domain and instance as input, resulting in a knowledge acquisition\nbottleneck. Meanwhile, although deep learning has achieved significant success\nin many fields, the knowledge is encoded in a subsymbolic representation which\nis incompatible with symbolic systems such as planners. We propose Latplan, an\nunsupervised architecture combining deep learning and classical planning. Given\nonly an unlabeled set of image pairs showing a subset of transitions allowed in\nthe environment (training inputs), Latplan learns a complete propositional PDDL\naction model of the environment. Later, when a pair of images representing the\ninitial and the goal states (planning inputs) is given, Latplan finds a plan to\nthe goal state in a symbolic latent space and returns a visualized plan\nexecution. We evaluate Latplan using image-based versions of 6 planning\ndomains: 8-puzzle, 15-Puzzle, Blocksworld, Sokoban and Two variations of\nLightsOut.\n",
        "published": "2021",
        "authors": [
            "Masataro Asai",
            "Hiroshi Kajino",
            "Alex Fukunaga",
            "Christian Muise"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.12375v4",
        "title": "Geometric Deep Learning on Molecular Representations",
        "abstract": "  Geometric deep learning (GDL), which is based on neural network architectures\nthat incorporate and process symmetry information, has emerged as a recent\nparadigm in artificial intelligence. GDL bears particular promise in molecular\nmodeling applications, in which various molecular representations with\ndifferent symmetry properties and levels of abstraction exist. This review\nprovides a structured and harmonized overview of molecular GDL, highlighting\nits applications in drug discovery, chemical synthesis prediction, and quantum\nchemistry. Emphasis is placed on the relevance of the learned molecular\nfeatures and their complementarity to well-established molecular descriptors.\nThis review provides an overview of current challenges and opportunities, and\npresents a forecast of the future of GDL for molecular sciences.\n",
        "published": "2021",
        "authors": [
            "Kenneth Atz",
            "Francesca Grisoni",
            "Gisbert Schneider"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.11452v3",
        "title": "Deep Learning Reproducibility and Explainable AI (XAI)",
        "abstract": "  The nondeterminism of Deep Learning (DL) training algorithms and its\ninfluence on the explainability of neural network (NN) models are investigated\nin this work with the help of image classification examples. To discuss the\nissue, two convolutional neural networks (CNN) have been trained and their\nresults compared. The comparison serves the exploration of the feasibility of\ncreating deterministic, robust DL models and deterministic explainable\nartificial intelligence (XAI) in practice. Successes and limitation of all here\ncarried out efforts are described in detail. The source code of the attained\ndeterministic models has been listed in this work. Reproducibility is indexed\nas a development-phase-component of the Model Governance Framework, proposed by\nthe EU within their excellence in AI approach. Furthermore, reproducibility is\na requirement for establishing causality for the interpretation of model\nresults and building of trust towards the overwhelming expansion of AI systems\napplications. Problems that have to be solved on the way to reproducibility and\nways to deal with some of them, are examined in this work.\n",
        "published": "2022",
        "authors": [
            "A. -M. Leventi-Peetz",
            "T. \u00d6streich"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.12058v1",
        "title": "Exploring the Unfairness of DP-SGD Across Settings",
        "abstract": "  End users and regulators require private and fair artificial intelligence\nmodels, but previous work suggests these objectives may be at odds. We use the\nCivilComments to evaluate the impact of applying the {\\em de facto} standard\napproach to privacy, DP-SGD, across several fairness metrics. We evaluate three\nimplementations of DP-SGD: for dimensionality reduction (PCA), linear\nclassification (logistic regression), and robust deep learning (Group-DRO). We\nestablish a negative, logarithmic correlation between privacy and fairness in\nthe case of linear classification and robust deep learning. DP-SGD had no\nsignificant impact on fairness for PCA, but upon inspection, also did not seem\nto lead to private representations.\n",
        "published": "2022",
        "authors": [
            "Frederik Noe",
            "Rasmus Herskind",
            "Anders S\u00f8gaard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.12139v1",
        "title": "Testing Deep Learning Models: A First Comparative Study of Multiple\n  Testing Techniques",
        "abstract": "  Deep Learning (DL) has revolutionized the capabilities of vision-based\nsystems (VBS) in critical applications such as autonomous driving, robotic\nsurgery, critical infrastructure surveillance, air and maritime traffic\ncontrol, etc. By analyzing images, voice, videos, or any type of complex\nsignals, DL has considerably increased the situation awareness of these\nsystems. At the same time, while relying more and more on trained DL models,\nthe reliability and robustness of VBS have been challenged and it has become\ncrucial to test thoroughly these models to assess their capabilities and\npotential errors. To discover faults in DL models, existing software testing\nmethods have been adapted and refined accordingly. In this article, we provide\nan overview of these software testing methods, namely differential,\nmetamorphic, mutation, and combinatorial testing, as well as adversarial\nperturbation testing and review some challenges in their deployment for\nboosting perception systems used in VBS. We also provide a first experimental\ncomparative study on a classical benchmark used in VBS and discuss its results.\n",
        "published": "2022",
        "authors": [
            "Mohit Kumar Ahuja",
            "Arnaud Gotlieb",
            "Helge Spieker"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.06133v1",
        "title": "Neuro-Symbolic AI: An Emerging Class of AI Workloads and their\n  Characterization",
        "abstract": "  Neuro-symbolic artificial intelligence is a novel area of AI research which\nseeks to combine traditional rules-based AI approaches with modern deep\nlearning techniques. Neuro-symbolic models have already demonstrated the\ncapability to outperform state-of-the-art deep learning models in domains such\nas image and video reasoning. They have also been shown to obtain high accuracy\nwith significantly less training data than traditional models. Due to the\nrecency of the field's emergence and relative sparsity of published results,\nthe performance characteristics of these models are not well understood. In\nthis paper, we describe and analyze the performance characteristics of three\nrecent neuro-symbolic models. We find that symbolic models have less potential\nparallelism than traditional neural models due to complex control flow and\nlow-operational-intensity operations, such as scalar multiplication and tensor\naddition. However, the neural aspect of computation dominates the symbolic part\nin cases where they are clearly separable. We also find that data movement\nposes a potential bottleneck, as it does in many ML workloads.\n",
        "published": "2021",
        "authors": [
            "Zachary Susskind",
            "Bryce Arden",
            "Lizy K. John",
            "Patrick Stockton",
            "Eugene B. John"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.09791v1",
        "title": "Prediction of severe thunderstorm events with ensemble deep learning and\n  radar data",
        "abstract": "  The problem of nowcasting extreme weather events can be addressed by applying\neither numerical methods for the solution of dynamic model equations or\ndata-driven artificial intelligence algorithms. Within this latter framework,\nthe present paper illustrates how a deep learning method, exploiting videos of\nradar reflectivity frames as input, can be used to realize a warning machine\nable to sound timely alarms of possible severe thunderstorm events. From a\ntechnical viewpoint, the computational core of this approach is the use of a\nvalue-weighted skill score for both transforming the probabilistic outcomes of\nthe deep neural network into binary classification and assessing the\nforecasting performances. The warning machine has been validated against\nweather radar data recorded in the Liguria region, in Italy,\n",
        "published": "2021",
        "authors": [
            "Sabrina Guastavino",
            "Michele Piana",
            "Marco Tizzi",
            "Federico Cassola",
            "Antonio Iengo",
            "Davide Sacchetti",
            "Enrico Solazzo",
            "Federico Benvenuto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.11739v2",
        "title": "A Survey of Natural Language Generation",
        "abstract": "  This paper offers a comprehensive review of the research on Natural Language\nGeneration (NLG) over the past two decades, especially in relation to\ndata-to-text generation and text-to-text generation deep learning methods, as\nwell as new applications of NLG technology. This survey aims to (a) give the\nlatest synthesis of deep learning research on the NLG core tasks, as well as\nthe architectures adopted in the field; (b) detail meticulously and\ncomprehensively various NLG tasks and datasets, and draw attention to the\nchallenges in NLG evaluation, focusing on different evaluation methods and\ntheir relationships; (c) highlight some future emphasis and relatively recent\nresearch issues that arise due to the increasing synergy between NLG and other\nartificial intelligence areas, such as computer vision, text and computational\ncreativity.\n",
        "published": "2021",
        "authors": [
            "Chenhe Dong",
            "Yinghui Li",
            "Haifan Gong",
            "Miaoxin Chen",
            "Junxin Li",
            "Ying Shen",
            "Min Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.08812v1",
        "title": "Self-Supervised Deep Learning to Enhance Breast Cancer Detection on\n  Screening Mammography",
        "abstract": "  A major limitation in applying deep learning to artificial intelligence (AI)\nsystems is the scarcity of high-quality curated datasets. We investigate strong\naugmentation based self-supervised learning (SSL) techniques to address this\nproblem. Using breast cancer detection as an example, we first identify a\nmammogram-specific transformation paradigm and then systematically compare four\nrecent SSL methods representing a diversity of approaches. We develop a method\nto convert a pretrained model from making predictions on uniformly tiled\npatches to whole images, and an attention-based pooling method that improves\nthe classification performance. We found that the best SSL model substantially\noutperformed the baseline supervised model. The best SSL model also improved\nthe data efficiency of sample labeling by nearly 4-fold and was highly\ntransferrable from one dataset to another. SSL represents a major breakthrough\nin computer vision and may help the AI for medical imaging field to shift away\nfrom supervised learning and dependency on scarce labels.\n",
        "published": "2022",
        "authors": [
            "John D. Miller",
            "Vignesh A. Arasu",
            "Albert X. Pu",
            "Laurie R. Margolies",
            "Weiva Sieh",
            "Li Shen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.00617v1",
        "title": "Visual explanations for polyp detection: How medical doctors assess\n  intrinsic versus extrinsic explanations",
        "abstract": "  Deep learning has in recent years achieved immense success in all areas of\ncomputer vision and has the potential of assisting medical doctors in analyzing\nvisual content for disease and other abnormalities. However, the current state\nof deep learning is very much a black box, making medical professionals highly\nskeptical about integrating these methods into clinical practice. Several\nmethods have been proposed in order to shine some light onto these black boxes,\nbut there is no consensus on the opinion of the medical doctors that will\nconsume these explanations. This paper presents a study asking medical doctors\nabout their opinion of current state-of-the-art explainable artificial\nintelligence methods when applied to a gastrointestinal disease detection use\ncase. We compare two different categories of explanation methods, intrinsic and\nextrinsic, and gauge their opinion of the current value of these explanations.\nThe results indicate that intrinsic explanations are preferred and that\nexplanation.\n",
        "published": "2022",
        "authors": [
            "Steven Hicks",
            "Andrea Stor\u00e5s",
            "Michael Riegler",
            "Cise Midoglu",
            "Malek Hammou",
            "Thomas de Lange",
            "Sravanthi Parasa",
            "P\u00e5l Halvorsen",
            "Inga Str\u00fcmke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.13012v2",
        "title": "TSEM: Temporally Weighted Spatiotemporal Explainable Neural Network for\n  Multivariate Time Series",
        "abstract": "  Deep learning has become a one-size-fits-all solution for technical and\nbusiness domains thanks to its flexibility and adaptability. It is implemented\nusing opaque models, which unfortunately undermines the outcome\ntrustworthiness. In order to have a better understanding of the behavior of a\nsystem, particularly one driven by time series, a look inside a deep learning\nmodel so-called posthoc eXplainable Artificial Intelligence (XAI) approaches,\nis important. There are two major types of XAI for time series data, namely\nmodel-agnostic and model-specific. Model-specific approach is considered in\nthis work. While other approaches employ either Class Activation Mapping (CAM)\nor Attention Mechanism, we merge the two strategies into a single system,\nsimply called the Temporally Weighted Spatiotemporal Explainable Neural Network\nfor Multivariate Time Series (TSEM). TSEM combines the capabilities of RNN and\nCNN models in such a way that RNN hidden units are employed as attention\nweights for the CNN feature maps temporal axis. The result shows that TSEM\noutperforms XCM. It is similar to STAM in terms of accuracy, while also\nsatisfying a number of interpretability criteria, including causality,\nfidelity, and spatiotemporality.\n",
        "published": "2022",
        "authors": [
            "Anh-Duy Pham",
            "Anastassia Kuestenmacher",
            "Paul G. Ploeger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.13845v1",
        "title": "Raising the Bar in Graph-level Anomaly Detection",
        "abstract": "  Graph-level anomaly detection has become a critical topic in diverse areas,\nsuch as financial fraud detection and detecting anomalous activities in social\nnetworks. While most research has focused on anomaly detection for visual data\nsuch as images, where high detection accuracies have been obtained, existing\ndeep learning approaches for graphs currently show considerably worse\nperformance. This paper raises the bar on graph-level anomaly detection, i.e.,\nthe task of detecting abnormal graphs in a set of graphs. By drawing on ideas\nfrom self-supervised learning and transformation learning, we present a new\ndeep learning approach that significantly improves existing deep one-class\napproaches by fixing some of their known problems, including hypersphere\ncollapse and performance flip. Experiments on nine real-world data sets\ninvolving nine techniques reveal that our method achieves an average\nperformance improvement of 11.8% AUC compared to the best existing approach.\n",
        "published": "2022",
        "authors": [
            "Chen Qiu",
            "Marius Kloft",
            "Stephan Mandt",
            "Maja Rudolph"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.04295v1",
        "title": "Explainable AI (XAI) in Biomedical Signal and Image Processing: Promises\n  and Challenges",
        "abstract": "  Artificial intelligence has become pervasive across disciplines and fields,\nand biomedical image and signal processing is no exception. The growing and\nwidespread interest on the topic has triggered a vast research activity that is\nreflected in an exponential research effort. Through study of massive and\ndiverse biomedical data, machine and deep learning models have revolutionized\nvarious tasks such as modeling, segmentation, registration, classification and\nsynthesis, outperforming traditional techniques. However, the difficulty in\ntranslating the results into biologically/clinically interpretable information\nis preventing their full exploitation in the field. Explainable AI (XAI)\nattempts to fill this translational gap by providing means to make the models\ninterpretable and providing explanations. Different solutions have been\nproposed so far and are gaining increasing interest from the community. This\npaper aims at providing an overview on XAI in biomedical data processing and\npoints to an upcoming Special Issue on Deep Learning in Biomedical Image and\nSignal Processing of the IEEE Signal Processing Magazine that is going to\nappear in March 2022.\n",
        "published": "2022",
        "authors": [
            "Guang Yang",
            "Arvind Rao",
            "Christine Fernandez-Maloigne",
            "Vince Calhoun",
            "Gloria Menegaz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.13738v1",
        "title": "Scalable, Distributed AI Frameworks: Leveraging Cloud Computing for\n  Enhanced Deep Learning Performance and Efficiency",
        "abstract": "  In recent years, the integration of artificial intelligence (AI) and cloud\ncomputing has emerged as a promising avenue for addressing the growing\ncomputational demands of AI applications. This paper presents a comprehensive\nstudy of scalable, distributed AI frameworks leveraging cloud computing for\nenhanced deep learning performance and efficiency. We first provide an overview\nof popular AI frameworks and cloud services, highlighting their respective\nstrengths and weaknesses. Next, we delve into the critical aspects of data\nstorage and management in cloud-based AI systems, discussing data\npreprocessing, feature engineering, privacy, and security. We then explore\nparallel and distributed training techniques for AI models, focusing on model\npartitioning, communication strategies, and cloud-based training architectures.\n  In subsequent chapters, we discuss optimization strategies for AI workloads\nin the cloud, covering load balancing, resource allocation, auto-scaling, and\nperformance benchmarking. We also examine AI model deployment and serving in\nthe cloud, outlining containerization, serverless deployment options, and\nmonitoring best practices. To ensure the cost-effectiveness of cloud-based AI\nsolutions, we present a thorough analysis of costs, optimization strategies,\nand case studies showcasing successful deployments. Finally, we summarize the\nkey findings of this study, discuss the challenges and limitations of\ncloud-based AI, and identify emerging trends and future research opportunities\nin the field.\n",
        "published": "2023",
        "authors": [
            "Neelesh Mungoli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.16235v1",
        "title": "Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A\n  Survey",
        "abstract": "  For a long time, biology and neuroscience fields have been a great source of\ninspiration for computer scientists, towards the development of Artificial\nIntelligence (AI) technologies. This survey aims at providing a comprehensive\nreview of recent biologically-inspired approaches for AI. After introducing the\nmain principles of computation and synaptic plasticity in biological neurons,\nwe provide a thorough presentation of Spiking Neural Network (SNN) models, and\nwe highlight the main challenges related to SNN training, where traditional\nbackprop-based optimization is not directly applicable. Therefore, we discuss\nrecent bio-inspired training methods, which pose themselves as alternatives to\nbackprop, both for traditional and spiking networks. Bio-Inspired Deep Learning\n(BIDL) approaches towards advancing the computational capabilities and\nbiological plausibility of current models.\n",
        "published": "2023",
        "authors": [
            "Gabriele Lagani",
            "Fabrizio Falchi",
            "Claudio Gennaro",
            "Giuseppe Amato"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.13072v1",
        "title": "Weakly Supervised Reasoning by Neuro-Symbolic Approaches",
        "abstract": "  Deep learning has largely improved the performance of various natural\nlanguage processing (NLP) tasks. However, most deep learning models are\nblack-box machinery, and lack explicit interpretation. In this chapter, we will\nintroduce our recent progress on neuro-symbolic approaches to NLP, which\ncombines different schools of AI, namely, symbolism and connectionism.\nGenerally, we will design a neural system with symbolic latent structures for\nan NLP task, and apply reinforcement learning or its relaxation to perform\nweakly supervised reasoning in the downstream task. Our framework has been\nsuccessfully applied to various tasks, including table query reasoning,\nsyntactic structure reasoning, information extraction reasoning, and rule\nreasoning. For each application, we will introduce the background, our\napproach, and experimental results.\n",
        "published": "2023",
        "authors": [
            "Xianggen Liu",
            "Zhengdong Lu",
            "Lili Mou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.07112v1",
        "title": "Generating High-Resolution Regional Precipitation Using Conditional\n  Diffusion Model",
        "abstract": "  Climate downscaling is a crucial technique within climate research, serving\nto project low-resolution (LR) climate data to higher resolutions (HR).\nPrevious research has demonstrated the effectiveness of deep learning for\ndownscaling tasks. However, most deep learning models for climate downscaling\nmay not perform optimally for high scaling factors (i.e., 4x, 8x) due to their\nlimited ability to capture the intricate details required for generating HR\nclimate data. Furthermore, climate data behaves differently from image data,\nnecessitating a nuanced approach when employing deep generative models. In\nresponse to these challenges, this paper presents a deep generative model for\ndownscaling climate data, specifically precipitation on a regional scale. We\nemploy a denoising diffusion probabilistic model (DDPM) conditioned on multiple\nLR climate variables. The proposed model is evaluated using precipitation data\nfrom the Community Earth System Model (CESM) v1.2.2 simulation. Our results\ndemonstrate significant improvements over existing baselines, underscoring the\neffectiveness of the conditional diffusion model in downscaling climate data.\n",
        "published": "2023",
        "authors": [
            "Naufal Shidqi",
            "Chaeyoon Jeong",
            "Sungwon Park",
            "Elke Zeller",
            "Arjun Babu Nellikkattil",
            "Karandeep Singh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.01672v3",
        "title": "Model-Aided Wireless Artificial Intelligence: Embedding Expert Knowledge\n  in Deep Neural Networks Towards Wireless Systems Optimization",
        "abstract": "  Deep learning based on artificial neural networks is a powerful machine\nlearning method that, in the last few years, has been successfully used to\nrealize tasks, e.g., image classification, speech recognition, translation of\nlanguages, etc., that are usually simple to execute by human beings but\nextremely difficult to perform by machines. This is one of the reasons why deep\nlearning is considered to be one of the main enablers to realize the notion of\nartificial intelligence. In order to identify the best architecture of an\nartificial neural network that allows one to fit input-output data pairs, the\ncurrent methodology in deep learning methods consists of employing a\ndata-driven approach. Once the artificial neural network is trained, it is\ncapable of responding to never-observed inputs by providing the optimum output\nbased on past acquired knowledge. In this context, a recent trend in the deep\nlearning community is to complement pure data-driven approaches with prior\ninformation based on expert knowledge. In this work, we describe two methods\nthat implement this strategy, which aim at optimizing wireless communication\nnetworks. In addition, we illustrate numerical results in order to assess the\nperformance of the proposed approaches compared with pure data-driven\nimplementations.\n",
        "published": "2018",
        "authors": [
            "Alessio Zappone",
            "Marco Di Renzo",
            "M\u00e9rouane Debbah",
            "Thanh Tu Lam",
            "Xuewen Qian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.05699v1",
        "title": "Development of Deep Learning Based Natural Language Processing Model for\n  Turkish",
        "abstract": "  Natural language is one of the most fundamental features that distinguish\npeople from other living things and enable people to communicate each other.\nLanguage is a tool that enables people to express their feelings and thoughts\nand to transfers cultures through generations. Texts and audio are examples of\nnatural language in daily life. In the natural language, many words disappear\nin time, on the other hand new words are derived. Therefore, while the process\nof natural language processing (NLP) is complex even for human, it is difficult\nto process in computer system. The area of linguistics examines how people use\nlanguage. NLP, which requires the collaboration of linguists and computer\nscientists, plays an important role in human computer interaction. Studies in\nNLP have increased with the use of artificial intelligence technologies in the\nfield of linguistics. With the deep learning methods which are one of the\nartificial intelligence study areas, platforms close to natural language are\nbeing developed. Developed platforms for language comprehension, machine\ntranslation and part of speech (POS) tagging benefit from deep learning\nmethods. Recurrent Neural Network (RNN), one of the deep learning\narchitectures, is preferred for processing sequential data such as text or\naudio data. In this study, Turkish POS tagging model has been proposed by using\nBidirectional Long-Short Term Memory (BLSTM) which is an RNN type. The proposed\nPOS tagging model is provided to natural language researchers with a platform\nthat allows them to perform and use their own analysis. In the development\nphase of the platform developed by using BLSTM, the error rate of the POS\ntagger has been reduced by taking feedback with expert opinion.\n",
        "published": "2019",
        "authors": [
            "Baris Baburoglu",
            "Adem Tekerek",
            "Mehmet Tekerek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.11629v1",
        "title": "Logical Interpretations of Autoencoders",
        "abstract": "  The unification of low-level perception and high-level reasoning is a\nlong-standing problem in artificial intelligence, which has the potential to\nnot only bring the areas of logic and learning closer together but also\ndemonstrate how abstract concepts might emerge from sensory data. Precisely\nbecause deep learning methods dominate perception-based learning, including\nvision, speech, and linguistic grammar, there is fast-growing literature on how\nto integrate symbolic reasoning and deep learning. Broadly, efforts seem to\nfall into three camps: those focused on defining a logic whose formulas capture\ndeep learning, ones that integrate symbolic constraints in deep learning, and\nothers that allow neural computations and symbolic reasoning to co-exist\nseparately, to enjoy the strengths of both worlds. In this paper, we identify\nanother dimension to this inquiry: what do the hidden layers really capture,\nand how can we reason about that logically? In particular, we consider\nautoencoders that are widely used for dimensionality reduction and inject a\nsymbolic generative framework onto the feature layer. This allows us, among\nother things, to generate example images for a class to get a sense of what was\nlearned. Moreover, the modular structure of the proposed model makes it\npossible to learn relations over multiple images at a time, as well as handle\nnoisy labels. Our empirical evaluations show the promise of this inquiry.\n",
        "published": "2019",
        "authors": [
            "Anton Fuxjaeger",
            "Vaishak Belle"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.04766v3",
        "title": "Explainable Deep Learning Methods in Medical Image Classification: A\n  Survey",
        "abstract": "  The remarkable success of deep learning has prompted interest in its\napplication to medical imaging diagnosis. Even though state-of-the-art deep\nlearning models have achieved human-level accuracy on the classification of\ndifferent types of medical data, these models are hardly adopted in clinical\nworkflows, mainly due to their lack of interpretability. The black-box-ness of\ndeep learning models has raised the need for devising strategies to explain the\ndecision process of these models, leading to the creation of the topic of\neXplainable Artificial Intelligence (XAI). In this context, we provide a\nthorough survey of XAI applied to medical imaging diagnosis, including visual,\ntextual, example-based and concept-based explanation methods. Moreover, this\nwork reviews the existing medical imaging datasets and the existing metrics for\nevaluating the quality of the explanations. In addition, we include a\nperformance comparison among a set of report generation-based methods. Finally,\nthe major challenges in applying XAI to medical imaging and the future research\ndirections on the topic are also discussed.\n",
        "published": "2022",
        "authors": [
            "Cristiano Patr\u00edcio",
            "Jo\u00e3o C. Neves",
            "Lu\u00eds F. Teixeira"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.13540v2",
        "title": "Artificial Intelligence for Sustainability: Facilitating Sustainable\n  Smart Product-Service Systems with Computer Vision",
        "abstract": "  The usage and impact of deep learning for cleaner production and\nsustainability purposes remain little explored. This work shows how deep\nlearning can be harnessed to increase sustainability in production and product\nusage. Specifically, we utilize deep learning-based computer vision to\ndetermine the wear states of products. The resulting insights serve as a basis\nfor novel product-service systems with improved integration and result\norientation. Moreover, these insights are expected to facilitate product usage\nimprovements and R&D innovations. We demonstrate our approach on two products:\nmachining tools and rotating X-ray anodes. From a technical standpoint, we show\nthat it is possible to recognize the wear state of these products using\ndeep-learning-based computer vision. In particular, we detect wear through\nmicroscopic images of the two products. We utilize a U-Net for semantic\nsegmentation to detect wear based on pixel granularity. The resulting mean dice\ncoefficients of 0.631 and 0.603 demonstrate the feasibility of the proposed\napproach. Consequently, experts can now make better decisions, for example, to\nimprove the machining process parameters. To assess the impact of the proposed\napproach on environmental sustainability, we perform life cycle assessments\nthat show gains for both products. The results indicate that the emissions of\nCO2 equivalents are reduced by 12% for machining tools and by 44% for rotating\nanodes. This work can serve as a guideline and inspire researchers and\npractitioners to utilize computer vision in similar scenarios to develop\nsustainable smart product-service systems and enable cleaner production.\n",
        "published": "2023",
        "authors": [
            "Jannis Walk",
            "Niklas K\u00fchl",
            "Michael Saidani",
            "J\u00fcrgen Schatte"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.10991v2",
        "title": "Dense Sample Deep Learning",
        "abstract": "  Deep Learning (DL) , a variant of the neural network algorithms originally\nproposed in the 1980s, has made surprising progress in Artificial Intelligence\n(AI), ranging from language translation, protein folding, autonomous cars, and\nmore recently human-like language models (CHATbots), all that seemed\nintractable until very recently. Despite the growing use of Deep Learning (DL)\nnetworks, little is actually understood about the learning mechanisms and\nrepresentations that makes these networks effective across such a diverse range\nof applications. Part of the answer must be the huge scale of the architecture\nand of course the large scale of the data, since not much has changed since\n1987. But the nature of deep learned representations remain largely unknown.\nUnfortunately training sets with millions or billions of tokens have unknown\ncombinatorics and Networks with millions or billions of hidden units cannot\neasily be visualized and their mechanisms cannot be easily revealed. In this\npaper, we explore these questions with a large (1.24M weights; VGG) DL in a\nnovel high density sample task (5 unique tokens with at minimum 500 exemplars\nper token) which allows us to more carefully follow the emergence of category\nstructure and feature construction. We use various visualization methods for\nfollowing the emergence of the classification and the development of the\ncoupling of feature detectors and structures that provide a type of graphical\nbootstrapping, From these results we harvest some basic observations of the\nlearning dynamics of DL and propose a new theory of complex feature\nconstruction based on our results.\n",
        "published": "2023",
        "authors": [
            "Stephen Jos\u00e8 Hanson",
            "Vivek Yadav",
            "Catherine Hanson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.03402v2",
        "title": "A Survey on Data Collection for Machine Learning: a Big Data -- AI\n  Integration Perspective",
        "abstract": "  Data collection is a major bottleneck in machine learning and an active\nresearch topic in multiple communities. There are largely two reasons data\ncollection has recently become a critical issue. First, as machine learning is\nbecoming more widely-used, we are seeing new applications that do not\nnecessarily have enough labeled data. Second, unlike traditional machine\nlearning, deep learning techniques automatically generate features, which saves\nfeature engineering costs, but in return may require larger amounts of labeled\ndata. Interestingly, recent research in data collection comes not only from the\nmachine learning, natural language, and computer vision communities, but also\nfrom the data management community due to the importance of handling large\namounts of data. In this survey, we perform a comprehensive study of data\ncollection from a data management point of view. Data collection largely\nconsists of data acquisition, data labeling, and improvement of existing data\nor models. We provide a research landscape of these operations, provide\nguidelines on which technique to use when, and identify interesting research\nchallenges. The integration of machine learning and data management for data\ncollection is part of a larger trend of Big data and Artificial Intelligence\n(AI) integration and opens many opportunities for new research.\n",
        "published": "2018",
        "authors": [
            "Yuji Roh",
            "Geon Heo",
            "Steven Euijong Whang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.11323v3",
        "title": "A novel machine learning based framework for detection of Autism\n  Spectrum Disorder (ASD)",
        "abstract": "  Computer vision and machine learning are the linchpin of field of automation.\nThe medicine industry has adopted numerous methods to discover the root causes\nof many diseases in order to automate detection process. But, the biomarkers of\nAutism Spectrum Disorder (ASD) are still unknown, let alone automating its\ndetection. Studies from the neuroscience domain highlighted the fact that\ncorpus callosum and intracranial brain volume holds significant information for\ndetection of ASD. Such results and studies are not tested and verified by\nscientists working in the domain of computer vision / machine learning. Thus,\nin this study we have proposed a machine learning based framework for automatic\ndetection of ASD using features extracted from corpus callosum and intracranial\nbrain volume from ABIDE dataset. Corpus callosum and intracranial brain volume\ndata is obtained from T1-weighted MRI scans. Our proposed framework first\ncalculates weights of features extracted from Corpus callosum and intracranial\nbrain volume data. This step ensures to utilize discriminative capabilities of\nonly those features that will help in robust recognition of ASD. Then,\nconventional machine learning algorithm (conventional refers to algorithms\nother than deep learning) is applied on features that are most significant in\nterms of discriminative capabilities for recognition of ASD. Finally, for\nbenchmarking and to verify potential of deep learning on analyzing neuroimaging\ndata i.e. T1-weighted MRI scans, we have done experiment with state of the art\ndeep learning architecture i.e. VGG16 . We have used transfer learning approach\nto use already trained VGG16 model for detection of ASD. This is done to help\nreaders understand benefits and bottlenecks of using deep learning approach for\nanalyzing neuroimaging data which is difficult to record in large enough\nquantity for deep learning.\n",
        "published": "2019",
        "authors": [
            "Hamza Sharif",
            "Rizwan Ahmed Khan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.09475v1",
        "title": "Deep Reinforcement Learning for Clinical Decision Support: A Brief\n  Survey",
        "abstract": "  Owe to the recent advancements in Artificial Intelligence especially deep\nlearning, many data-driven decision support systems have been implemented to\nfacilitate medical doctors in delivering personalized care. We focus on the\ndeep reinforcement learning (DRL) models in this paper. DRL models have\ndemonstrated human-level or even superior performance in the tasks of computer\nvision and game playings, such as Go and Atari game. However, the adoption of\ndeep reinforcement learning techniques in clinical decision optimization is\nstill rare. We present the first survey that summarizes reinforcement learning\nalgorithms with Deep Neural Networks (DNN) on clinical decision support. We\nalso discuss some case studies, where different DRL algorithms were applied to\naddress various clinical challenges. We further compare and contrast the\nadvantages and limitations of various DRL algorithms and present a preliminary\nguide on how to choose the appropriate DRL algorithm for particular clinical\napplications.\n",
        "published": "2019",
        "authors": [
            "Siqi Liu",
            "Kee Yuan Ngiam",
            "Mengling Feng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.13516v1",
        "title": "Towards Comprehensive Recommender Systems: Time-Aware\n  UnifiedcRecommendations Based on Listwise Ranking of Implicit Cross-Network\n  Data",
        "abstract": "  The abundance of information in web applications make recommendation\nessential for users as well as applications. Despite the effectiveness of\nexisting recommender systems, we find two major limitations that reduce their\noverall performance: (1) inability to provide timely recommendations for both\nnew and existing users by considering the dynamic nature of user preferences,\nand (2) not fully optimized for the ranking task when using implicit feedback.\nTherefore, we propose a novel deep learning based unified cross-network\nsolution to mitigate cold-start and data sparsity issues and provide timely\nrecommendations for new and existing users.Furthermore, we consider the ranking\nproblem under implicit feedback as a classification task, and propose a generic\npersonalized listwise optimization criterion for implicit data to effectively\nrank a list of items. We illustrate our cross-network model using Twitter\nauxiliary information for recommendations on YouTube target network. Extensive\ncomparisons against multiple time aware and cross-network base-lines show that\nthe proposed solution is superior in terms of accuracy, novelty and diversity.\nFurthermore, experiments conducted on the popular MovieLens dataset suggest\nthat the proposed listwise ranking method outperforms existing state-of-the-art\nranking techniques.\n",
        "published": "2020",
        "authors": [
            "Dilruk Perera",
            "Roger Zimmermann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.15551v1",
        "title": "Investigating the Robustness of Artificial Intelligent Algorithms with\n  Mixture Experiments",
        "abstract": "  Artificial intelligent (AI) algorithms, such as deep learning and XGboost,\nare used in numerous applications including computer vision, autonomous\ndriving, and medical diagnostics. The robustness of these AI algorithms is of\ngreat interest as inaccurate prediction could result in safety concerns and\nlimit the adoption of AI systems. In this paper, we propose a framework based\non design of experiments to systematically investigate the robustness of AI\nclassification algorithms. A robust classification algorithm is expected to\nhave high accuracy and low variability under different application scenarios.\nThe robustness can be affected by a wide range of factors such as the imbalance\nof class labels in the training dataset, the chosen prediction algorithm, the\nchosen dataset of the application, and a change of distribution in the training\nand test datasets. To investigate the robustness of AI classification\nalgorithms, we conduct a comprehensive set of mixture experiments to collect\nprediction performance results. Then statistical analyses are conducted to\nunderstand how various factors affect the robustness of AI classification\nalgorithms. We summarize our findings and provide suggestions to practitioners\nin AI applications.\n",
        "published": "2020",
        "authors": [
            "Jiayi Lian",
            "Laura Freeman",
            "Yili Hong",
            "Xinwei Deng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.10265v1",
        "title": "Superiorities of Deep Extreme Learning Machines against Convolutional\n  Neural Networks",
        "abstract": "  Deep Learning (DL) is a machine learning procedure for artificial\nintelligence that analyzes the input data in detail by increasing neuron sizes\nand number of the hidden layers. DL has a popularity with the common\nimprovements on the graphical processing unit capabilities. Increasing number\nof the neuron sizes at each layer and hidden layers is directly related to the\ncomputation time and training speed of the classifier models. The\nclassification parameters including neuron weights, output weights, and biases\nneed to be optimized for obtaining an optimum model. Most of the popular DL\nalgorithms require long training times for optimization of the parameters with\nfeature learning progresses and back-propagated training procedures. Reducing\nthe training time and providing a real-time decision system are the basic focus\npoints of the novel approaches. Deep Extreme Learning machines (Deep ELM)\nclassifier model is one of the fastest and effective way to meet fast\nclassification problems. In this study, Deep ELM model, its superiorities and\nweaknesses are discussed, the problems that are more suitable for the\nclassifiers against Convolutional neural network based DL algorithms.\n",
        "published": "2021",
        "authors": [
            "Gokhan Altan",
            "Yakup Kutlu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.03616v3",
        "title": "A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for\n  Abstractive Text Summarization",
        "abstract": "  In this paper, we propose a deep learning approach to tackle the automatic\nsummarization tasks by incorporating topic information into the convolutional\nsequence-to-sequence (ConvS2S) model and using self-critical sequence training\n(SCST) for optimization. Through jointly attending to topics and word-level\nalignment, our approach can improve coherence, diversity, and informativeness\nof generated summaries via a biased probability generation mechanism. On the\nother hand, reinforcement training, like SCST, directly optimizes the proposed\nmodel with respect to the non-differentiable metric ROUGE, which also avoids\nthe exposure bias during inference. We carry out the experimental evaluation\nwith state-of-the-art methods over the Gigaword, DUC-2004, and LCSTS datasets.\nThe empirical results demonstrate the superiority of our proposed method in the\nabstractive summarization.\n",
        "published": "2018",
        "authors": [
            "Li Wang",
            "Junlin Yao",
            "Yunzhe Tao",
            "Li Zhong",
            "Wei Liu",
            "Qiang Du"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.04053v1",
        "title": "The 30-Year Cycle In The AI Debate",
        "abstract": "  In the last couple of years, the rise of Artificial Intelligence and the\nsuccesses of academic breakthroughs in the field have been inescapable. Vast\nsums of money have been thrown at AI start-ups. Many existing tech companies --\nincluding the giants like Google, Amazon, Facebook, and Microsoft -- have\nopened new research labs. The rapid changes in these everyday work and\nentertainment tools have fueled a rising interest in the underlying technology\nitself; journalists write about AI tirelessly, and companies -- of tech nature\nor not -- brand themselves with AI, Machine Learning or Deep Learning whenever\nthey get a chance. Confronting squarely this media coverage, several analysts\nare starting to voice concerns about over-interpretation of AI's blazing\nsuccesses and the sometimes poor public reporting on the topic. This paper\nreviews briefly the track-record in AI and Machine Learning and finds this\npattern of early dramatic successes, followed by philosophical critique and\nunexpected difficulties, if not downright stagnation, returning almost to the\nclock in 30-year cycles since 1958.\n",
        "published": "2018",
        "authors": [
            "Jean-Marie Chauvet"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.03595v1",
        "title": "Federated AI lets a team imagine together: Federated Learning of GANs",
        "abstract": "  Envisioning a new imaginative idea together is a popular human need.\nImagining together as a team can often lead to breakthrough ideas, but the\ncollaboration effort can also be challenging, especially when the team members\nare separated by time and space. What if there is a AI that can assist the team\nto collaboratively envision new ideas?. Is it possible to develop a working\nmodel of such an AI? This paper aims to design such an intelligence. This paper\nproposes a approach to design a creative and collaborative intelligence by\nemploying a form of distributed machine learning approach called Federated\nLearning along with fusion on Generative Adversarial Networks, GAN. This\ncollaborative creative AI presents a new paradigm in AI, one that lets a team\nof two or more to come together to imagine and envision ideas that synergies\nwell with interests of all members of the team. In short, this paper explores\nthe design of a novel type of AI paradigm, called Federated AI Imagination, one\nthat lets geographically distributed teams to collaboratively imagine.\n",
        "published": "2019",
        "authors": [
            "Rajagopal. A",
            "Nirmala. V"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.05270v1",
        "title": "Artificial Intelligence Enabled Material Behavior Prediction",
        "abstract": "  Artificial Intelligence and Machine Learning algorithms have considerable\npotential to influence the prediction of material properties. Additive\nmaterials have a unique property prediction challenge in the form of surface\nroughness effects on fatigue behavior of structural components. Traditional\napproaches using finite element methods to calculate stress risers associated\nwith additively built surfaces have been challenging due to the computational\nresources required, often taking over a day to calculate a single sample\nprediction. To address this performance challenge, Deep Learning has been\nemployed to enable low cycle fatigue life prediction in additive materials in a\nmatter of seconds.\n",
        "published": "2019",
        "authors": [
            "Timothy Hanlon",
            "Johan Reimann",
            "Monica A. Soare",
            "Anjali Singhal",
            "James Grande",
            "Marc Edgar",
            "Kareem S. Aggour",
            "Joseph Vinciquerra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.01058v2",
        "title": "Explaining the Predictions of Any Image Classifier via Decision Trees",
        "abstract": "  Despite outstanding contribution to the significant progress of Artificial\nIntelligence (AI), deep learning models remain mostly black boxes, which are\nextremely weak in explainability of the reasoning process and prediction\nresults. Explainability is not only a gateway between AI and society but also a\npowerful tool to detect flaws in the model and biases in the data. Local\nInterpretable Model-agnostic Explanation (LIME) is a recent approach that uses\nan interpretable model to form a local explanation for the individual\nprediction result. The current implementation of LIME adopts the linear\nregression as its interpretable function. However, being so restricted and\nusually over-simplifying the relationships, linear models fail in situations\nwhere nonlinear associations and interactions exist among features and\nprediction results. This paper implements a decision Tree-based LIME approach,\nwhich uses a decision tree model to form an interpretable representation that\nis locally faithful to the original model. Tree-LIME approach can capture\nnonlinear interactions among features in the data and creates plausible\nexplanations. Various experiments show that the Tree-LIME explanation of\nmultiple black-box models can achieve more reliable performance in terms of\nunderstandability, fidelity, and efficiency.\n",
        "published": "2019",
        "authors": [
            "Sheng Shi",
            "Xinfeng Zhang",
            "Wei Fan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.07434v1",
        "title": "A Modified Perturbed Sampling Method for Local Interpretable\n  Model-agnostic Explanation",
        "abstract": "  Explainability is a gateway between Artificial Intelligence and society as\nthe current popular deep learning models are generally weak in explaining the\nreasoning process and prediction results. Local Interpretable Model-agnostic\nExplanation (LIME) is a recent technique that explains the predictions of any\nclassifier faithfully by learning an interpretable model locally around the\nprediction. However, the sampling operation in the standard implementation of\nLIME is defective. Perturbed samples are generated from a uniform distribution,\nignoring the complicated correlation between features. This paper proposes a\nnovel Modified Perturbed Sampling operation for LIME (MPS-LIME), which is\nformalized as the clique set construction problem. In image classification,\nMPS-LIME converts the superpixel image into an undirected graph. Various\nexperiments show that the MPS-LIME explanation of the black-box model achieves\nmuch better performance in terms of understandability, fidelity, and\nefficiency.\n",
        "published": "2020",
        "authors": [
            "Sheng Shi",
            "Xinfeng Zhang",
            "Wei Fan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.05876v2",
        "title": "Neurosymbolic AI: The 3rd Wave",
        "abstract": "  Current advances in Artificial Intelligence (AI) and Machine Learning (ML)\nhave achieved unprecedented impact across research communities and industry.\nNevertheless, concerns about trust, safety, interpretability and accountability\nof AI were raised by influential thinkers. Many have identified the need for\nwell-founded knowledge representation and reasoning to be integrated with deep\nlearning and for sound explainability. Neural-symbolic computing has been an\nactive area of research for many years seeking to bring together robust\nlearning in neural networks with reasoning and explainability via symbolic\nrepresentations for network models. In this paper, we relate recent and early\nresearch results in neurosymbolic AI with the objective of identifying the key\ningredients of the next wave of AI systems. We focus on research that\nintegrates in a principled way neural network-based learning with symbolic\nknowledge representation and logical reasoning. The insights provided by 20\nyears of neural-symbolic computing are shown to shed new light onto the\nincreasingly prominent role of trust, safety, interpretability and\naccountability of AI. We also identify promising directions and challenges for\nthe next decade of AI research from the perspective of neural-symbolic systems.\n",
        "published": "2020",
        "authors": [
            "Artur d'Avila Garcez",
            "Luis C. Lamb"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.14405v1",
        "title": "Shape-based Feature Engineering for Solar Flare Prediction",
        "abstract": "  Solar flares are caused by magnetic eruptions in active regions (ARs) on the\nsurface of the sun. These events can have significant impacts on human\nactivity, many of which can be mitigated with enough advance warning from good\nforecasts. To date, machine learning-based flare-prediction methods have\nemployed physics-based attributes of the AR images as features; more recently,\nthere has been some work that uses features deduced automatically by deep\nlearning methods (such as convolutional neural networks). We describe a suite\nof novel shape-based features extracted from magnetogram images of the Sun\nusing the tools of computational topology and computational geometry. We\nevaluate these features in the context of a multi-layer perceptron (MLP) neural\nnetwork and compare their performance against the traditional physics-based\nattributes. We show that these abstract shape-based features outperform the\nfeatures chosen by the human experts, and that a combination of the two feature\nsets improves the forecasting capability even further.\n",
        "published": "2020",
        "authors": [
            "Varad Deshmukh",
            "Thomas Berger",
            "James Meiss",
            "Elizabeth Bradley"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.01792v1",
        "title": "A Survey on Understanding, Visualizations, and Explanation of Deep\n  Neural Networks",
        "abstract": "  Recent advancements in machine learning and signal processing domains have\nresulted in an extensive surge of interest in Deep Neural Networks (DNNs) due\nto their unprecedented performance and high accuracy for different and\nchallenging problems of significant engineering importance. However, when such\ndeep learning architectures are utilized for making critical decisions such as\nthe ones that involve human lives (e.g., in control systems and medical\napplications), it is of paramount importance to understand, trust, and in one\nword \"explain\" the argument behind deep models' decisions. In many\napplications, artificial neural networks (including DNNs) are considered as\nblack-box systems, which do not provide sufficient clue on their internal\nprocessing actions. Although some recent efforts have been initiated to explain\nthe behaviors and decisions of deep networks, explainable artificial\nintelligence (XAI) domain, which aims at reasoning about the behavior and\ndecisions of DNNs, is still in its infancy. The aim of this paper is to provide\na comprehensive overview on Understanding, Visualization, and Explanation of\nthe internal and overall behavior of DNNs.\n",
        "published": "2021",
        "authors": [
            "Atefeh Shahroudnejad"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.03048v1",
        "title": "Detecting Spurious Correlations with Sanity Tests for Artificial\n  Intelligence Guided Radiology Systems",
        "abstract": "  Artificial intelligence (AI) has been successful at solving numerous problems\nin machine perception. In radiology, AI systems are rapidly evolving and show\nprogress in guiding treatment decisions, diagnosing, localizing disease on\nmedical images, and improving radiologists' efficiency. A critical component to\ndeploying AI in radiology is to gain confidence in a developed system's\nefficacy and safety. The current gold standard approach is to conduct an\nanalytical validation of performance on a generalization dataset from one or\nmore institutions, followed by a clinical validation study of the system's\nefficacy during deployment. Clinical validation studies are time-consuming, and\nbest practices dictate limited re-use of analytical validation data, so it is\nideal to know ahead of time if a system is likely to fail analytical or\nclinical validation. In this paper, we describe a series of sanity tests to\nidentify when a system performs well on development data for the wrong reasons.\nWe illustrate the sanity tests' value by designing a deep learning system to\nclassify pancreatic cancer seen in computed tomography scans.\n",
        "published": "2021",
        "authors": [
            "Usman Mahmood",
            "Robik Shrestha",
            "David D. B. Bates",
            "Lorenzo Mannelli",
            "Giuseppe Corrias",
            "Yusuf Erdi",
            "Christopher Kanan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.02468v1",
        "title": "A Novel Approach for Semiconductor Etching Process with Inductive Biases",
        "abstract": "  The etching process is one of the most important processes in semiconductor\nmanufacturing. We have introduced the state-of-the-art deep learning model to\npredict the etching profiles. However, the significant problems violating\nphysics have been found through various techniques such as explainable\nartificial intelligence and representation of prediction uncertainty. To\naddress this problem, this paper presents a novel approach to apply the\ninductive biases for etching process. We demonstrate that our approach fits the\nmeasurement faster than physical simulator while following the physical\nbehavior. Our approach would bring a new opportunity for better etching process\nwith higher accuracy and lower cost.\n",
        "published": "2021",
        "authors": [
            "Sanghoon Myung",
            "Hyunjae Jang",
            "Byungseon Choi",
            "Jisu Ryu",
            "Hyuk Kim",
            "Sang Wuk Park",
            "Changwook Jeong",
            "Dae Sin Kim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.11072v1",
        "title": "Techniques for Symbol Grounding with SATNet",
        "abstract": "  Many experts argue that the future of artificial intelligence is limited by\nthe field's ability to integrate symbolic logical reasoning into deep learning\narchitectures. The recently proposed differentiable MAXSAT solver, SATNet, was\na breakthrough in its capacity to integrate with a traditional neural network\nand solve visual reasoning problems. For instance, it can learn the rules of\nSudoku purely from image examples. Despite its success, SATNet was shown to\nsuccumb to a key challenge in neurosymbolic systems known as the Symbol\nGrounding Problem: the inability to map visual inputs to symbolic variables\nwithout explicit supervision (\"label leakage\"). In this work, we present a\nself-supervised pre-training pipeline that enables SATNet to overcome this\nlimitation, thus broadening the class of problems that SATNet architectures can\nsolve to include datasets where no intermediary labels are available at all. We\ndemonstrate that our method allows SATNet to attain full accuracy even with a\nharder problem setup that prevents any label leakage. We additionally introduce\na proofreading method that further improves the performance of SATNet\narchitectures, beating the state-of-the-art on Visual Sudoku.\n",
        "published": "2021",
        "authors": [
            "Sever Topan",
            "David Rolnick",
            "Xujie Si"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.09618v1",
        "title": "Personalised Federated Learning: A Combinational Approach",
        "abstract": "  Federated learning (FL) is a distributed machine learning approach involving\nmultiple clients collaboratively training a shared model. Such a system has the\nadvantage of more training data from multiple clients, but data can be\nnon-identically and independently distributed (non-i.i.d.). Privacy and\nintegrity preserving features such as differential privacy (DP) and robust\naggregation (RA) are commonly used in FL. In this work, we show that on common\ndeep learning tasks, the performance of FL models differs amongst clients and\nsituations, and FL models can sometimes perform worse than local models due to\nnon-i.i.d. data. Secondly, we show that incorporating DP and RA degrades\nperformance further. Then, we conduct an ablation study on the performance\nimpact of different combinations of common personalization approaches for FL,\nsuch as finetuning, mixture-of-experts ensemble, multi-task learning, and\nknowledge distillation. It is observed that certain combinations of\npersonalization approaches are more impactful in certain scenarios while others\nalways improve performance, and combination approaches are better than\nindividual ones. Most clients obtained better performance with combined\npersonalized FL and recover from performance degradation caused by non-i.i.d.\ndata, DP, and RA.\n",
        "published": "2021",
        "authors": [
            "Sone Kyaw Pye",
            "Han Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.09231v2",
        "title": "Machine Learning Featurizations for AI Hacking of Political Systems",
        "abstract": "  What would the inputs be to a machine whose output is the destabilization of\na robust democracy, or whose emanations could disrupt the political power of\nnations? In the recent essay \"The Coming AI Hackers,\" Schneier (2021) proposed\na future application of artificial intelligences to discover, manipulate, and\nexploit vulnerabilities of social, economic, and political systems at speeds\nfar greater than humans' ability to recognize and respond to such threats. This\nwork advances the concept by applying to it theory from machine learning,\nhypothesizing some possible \"featurization\" (input specification and\ntransformation) frameworks for AI hacking. Focusing on the political domain, we\ndevelop graph and sequence data representations that would enable the\napplication of a range of deep learning models to predict attributes and\noutcomes of political, particularly legislative, systems. We explore possible\ndata models, datasets, predictive tasks, and actionable applications associated\nwith each framework. We speculate about the likely practical impact and\nfeasibility of such models, and conclude by discussing their ethical\nimplications.\n",
        "published": "2021",
        "authors": [
            "Nathan E Sanders",
            "Bruce Schneier"
        ]
    }
]