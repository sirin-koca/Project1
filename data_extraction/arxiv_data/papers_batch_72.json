[
    {
        "id": "http://arxiv.org/abs/2012.14601v2",
        "title": "Emergent Symbols through Binding in External Memory",
        "abstract": "  A key aspect of human intelligence is the ability to infer abstract rules\ndirectly from high-dimensional sensory data, and to do so given only a limited\namount of training experience. Deep neural network algorithms have proven to be\na powerful tool for learning directly from high-dimensional data, but currently\nlack this capacity for data-efficient induction of abstract rules, leading some\nto argue that symbol-processing mechanisms will be necessary to account for\nthis capacity. In this work, we take a step toward bridging this gap by\nintroducing the Emergent Symbol Binding Network (ESBN), a recurrent network\naugmented with an external memory that enables a form of variable-binding and\nindirection. This binding mechanism allows symbol-like representations to\nemerge through the learning process without the need to explicitly incorporate\nsymbol-processing machinery, enabling the ESBN to learn rules in a manner that\nis abstracted away from the particular entities to which those rules apply.\nAcross a series of tasks, we show that this architecture displays nearly\nperfect generalization of learned rules to novel entities given only a limited\nnumber of training examples, and outperforms a number of other competitive\nneural network architectures.\n",
        "published": "2020",
        "authors": [
            "Taylor W. Webb",
            "Ishan Sinha",
            "Jonathan D. Cohen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.01645v4",
        "title": "Generating images from caption and vice versa via CLIP-Guided Generative\n  Latent Space Search",
        "abstract": "  In this research work we present CLIP-GLaSS, a novel zero-shot framework to\ngenerate an image (or a caption) corresponding to a given caption (or image).\nCLIP-GLaSS is based on the CLIP neural network, which, given an image and a\ndescriptive caption, provides similar embeddings. Differently, CLIP-GLaSS takes\na caption (or an image) as an input, and generates the image (or the caption)\nwhose CLIP embedding is the most similar to the input one. This optimal image\n(or caption) is produced via a generative network, after an exploration by a\ngenetic algorithm. Promising results are shown, based on the experimentation of\nthe image Generators BigGAN and StyleGAN2, and of the text Generator GPT2\n",
        "published": "2021",
        "authors": [
            "Federico A. Galatolo",
            "Mario G. C. A. Cimino",
            "Gigliola Vaglini"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.01968v1",
        "title": "A Bayesian Neural Network based on Dropout Regulation",
        "abstract": "  Bayesian Neural Networks (BNN) have recently emerged in the Deep Learning\nworld for dealing with uncertainty estimation in classification tasks, and are\nused in many application domains such as astrophysics, autonomous driving...BNN\nassume a prior over the weights of a neural network instead of point estimates,\nenabling in this way the estimation of both aleatoric and epistemic uncertainty\nof the model prediction.Moreover, a particular type of BNN, namely MC Dropout,\nassumes a Bernoulli distribution on the weights by using Dropout.Several\nattempts to optimize the dropout rate exist, e.g. using a variational\napproach.In this paper, we present a new method called \"Dropout Regulation\"\n(DR), which consists of automatically adjusting the dropout rate during\ntraining using a controller as used in automation.DR allows for a precise\nestimation of the uncertainty which is comparable to the state-of-the-art while\nremaining simple to implement.\n",
        "published": "2021",
        "authors": [
            "Claire Theobald",
            "Fr\u00e9d\u00e9ric Pennerath",
            "Brieuc Conan-Guez",
            "Miguel Couceiro",
            "Amedeo Napoli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.04231v1",
        "title": "Neurogenetic Programming Framework for Explainable Reinforcement\n  Learning",
        "abstract": "  Automatic programming, the task of generating computer programs compliant\nwith a specification without a human developer, is usually tackled either via\ngenetic programming methods based on mutation and recombination of programs, or\nvia neural language models. We propose a novel method that combines both\napproaches using a concept of a virtual neuro-genetic programmer: using\nevolutionary methods as an alternative to gradient descent for neural network\ntraining}, or scrum team. We demonstrate its ability to provide performant and\nexplainable solutions for various OpenAI Gym tasks, as well as inject expert\nknowledge into the otherwise data-driven search for solutions.\n",
        "published": "2021",
        "authors": [
            "Vadim Liventsev",
            "Aki H\u00e4rm\u00e4",
            "Milan Petkovi\u0107"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.07503v2",
        "title": "One-shot learning for the long term: consolidation with an artificial\n  hippocampal algorithm",
        "abstract": "  Standard few-shot experiments involve learning to efficiently match\npreviously unseen samples by class. We claim that few-shot learning should be\nlong term, assimilating knowledge for the future, without forgetting previous\nconcepts. In the mammalian brain, the hippocampus is understood to play a\nsignificant role in this process, by learning rapidly and consolidating\nknowledge to the neocortex incrementally over a short period. In this research\nwe tested whether an artificial hippocampal algorithm (AHA), could be used with\na conventional Machine Learning (ML) model that learns incrementally analogous\nto the neocortex, to achieve one-shot learning both short and long term. The\nresults demonstrated that with the addition of AHA, the system could learn in\none-shot and consolidate the knowledge for the long term without catastrophic\nforgetting. This study is one of the first examples of using a CLS model of\nhippocampus to consolidate memories, and it constitutes a step toward few-shot\ncontinual learning.\n",
        "published": "2021",
        "authors": [
            "Gideon Kowadlo",
            "Abdelrahman Ahmed",
            "David Rawlinson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.12076v1",
        "title": "Perspective: Purposeful Failure in Artificial Life and Artificial\n  Intelligence",
        "abstract": "  Complex systems fail. I argue that failures can be a blueprint characterizing\nliving organisms and biological intelligence, a control mechanism to increase\ncomplexity in evolutionary simulations, and an alternative to classical fitness\noptimization. Imitating biological successes in Artificial Life and Artificial\nIntelligence can be misleading; imitating failures offers a path towards\nunderstanding and emulating life it in artificial systems.\n",
        "published": "2021",
        "authors": [
            "Lana Sinapayen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.00180v2",
        "title": "Incorporating Domain Knowledge into Deep Neural Networks",
        "abstract": "  We present a survey of ways in which domain-knowledge has been included when\nconstructing models with neural networks. The inclusion of domain-knowledge is\nof special interest not just to constructing scientific assistants, but also,\nmany other areas that involve understanding data using human-machine\ncollaboration. In many such instances, machine-based model construction may\nbenefit significantly from being provided with human-knowledge of the domain\nencoded in a sufficiently precise form. This paper examines two broad\napproaches to encode such knowledge--as logical and numerical constraints--and\ndescribes techniques and results obtained in several sub-categories under each\nof these approaches.\n",
        "published": "2021",
        "authors": [
            "Tirtharaj Dash",
            "Sharad Chitlangia",
            "Aditya Ahuja",
            "Ashwin Srinivasan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.00424v1",
        "title": "SpikeDyn: A Framework for Energy-Efficient Spiking Neural Networks with\n  Continual and Unsupervised Learning Capabilities in Dynamic Environments",
        "abstract": "  Spiking Neural Networks (SNNs) bear the potential of efficient unsupervised\nand continual learning capabilities because of their biological plausibility,\nbut their complexity still poses a serious research challenge to enable their\nenergy-efficient design for resource-constrained scenarios (like embedded\nsystems, IoT-Edge, etc.). We propose SpikeDyn, a comprehensive framework for\nenergy-efficient SNNs with continual and unsupervised learning capabilities in\ndynamic environments, for both the training and inference phases. It is\nachieved through the following multiple diverse mechanisms: 1) reduction of\nneuronal operations, by replacing the inhibitory neurons with direct lateral\ninhibitions; 2) a memory- and energy-constrained SNN model search algorithm\nthat employs analytical models to estimate the memory footprint and energy\nconsumption of different candidate SNN models and selects a Pareto-optimal SNN\nmodel; and 3) a lightweight continual and unsupervised learning algorithm that\nemploys adaptive learning rates, adaptive membrane threshold potential, weight\ndecay, and reduction of spurious updates. Our experimental results show that,\nfor a network with 400 excitatory neurons, our SpikeDyn reduces the energy\nconsumption on average by 51% for training and by 37% for inference, as\ncompared to the state-of-the-art. Due to the improved learning algorithm,\nSpikeDyn provides on avg. 21% accuracy improvement over the state-of-the-art,\nfor classifying the most recently learned task, and by 8% on average for the\npreviously learned tasks.\n",
        "published": "2021",
        "authors": [
            "Rachmad Vidya Wicaksana Putra",
            "Muhammad Shafique"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.01301v2",
        "title": "Multi-Objective Evolutionary Design of Composite Data-Driven Models",
        "abstract": "  In this paper, a multi-objective approach for the design of composite\ndata-driven mathematical models is proposed. It allows automating the\nidentification of graph-based heterogeneous pipelines that consist of different\nblocks: machine learning models, data preprocessing blocks, etc. The\nimplemented approach is based on a parameter-free genetic algorithm (GA) for\nmodel design called GPComp@Free. It is developed to be part of automated\nmachine learning solutions and to increase the efficiency of the modeling\npipeline automation. A set of experiments was conducted to verify the\ncorrectness and efficiency of the proposed approach and substantiate the\nselected solutions. The experimental results confirm that a multi-objective\napproach to the model design allows achieving better diversity and quality of\nobtained models. The implemented approach is available as a part of the\nopen-source AutoML framework FEDOT.\n",
        "published": "2021",
        "authors": [
            "Iana S. Polonskaia",
            "Nikolay O. Nikitin",
            "Ilia Revin",
            "Pavel Vychuzhanin",
            "Anna V. Kalyuzhnaya"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.01730v1",
        "title": "Graph-Time Convolutional Neural Networks",
        "abstract": "  Spatiotemporal data can be represented as a process over a graph, which\ncaptures their spatial relationships either explicitly or implicitly. How to\nleverage such a structure for learning representations is one of the key\nchallenges when working with graphs. In this paper, we represent the\nspatiotemporal relationships through product graphs and develop a first\nprinciple graph-time convolutional neural network (GTCNN). The GTCNN is a\ncompositional architecture with each layer comprising a graph-time\nconvolutional module, a graph-time pooling module, and a nonlinearity. We\ndevelop a graph-time convolutional filter by following the shift-and-sum\nprinciples of the convolutional operator to learn higher-level features over\nthe product graph. The product graph itself is parametric so that we can learn\nalso the spatiotemporal coupling from data. We develop a zero-pad pooling that\npreserves the spatial graph (the prior about the data) while reducing the\nnumber of active nodes and the parameters. Experimental results with synthetic\nand real data corroborate the different components and compare with baseline\nand state-of-the-art solutions.\n",
        "published": "2021",
        "authors": [
            "Elvin Isufi",
            "Gabriele Mazzola"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.02881v1",
        "title": "Bad and good errors: value-weighted skill scores in deep ensemble\n  learning",
        "abstract": "  In this paper we propose a novel approach to realize forecast verification.\nSpecifically, we introduce a strategy for assessing the severity of forecast\nerrors based on the evidence that, on the one hand, a false alarm just\nanticipating an occurring event is better than one in the middle of consecutive\nnon-occurring events, and that, on the other hand, a miss of an isolated event\nhas a worse impact than a miss of a single event, which is part of several\nconsecutive occurrences. Relying on this idea, we introduce a novel definition\nof confusion matrix and skill scores giving greater importance to the value of\nthe prediction rather than to its quality. Then, we introduce a deep ensemble\nlearning procedure for binary classification, in which the probabilistic\noutcomes of a neural network are clustered via optimization of these\nvalue-weighted skill scores. We finally show the performances of this approach\nin the case of three applications concerned with pollution, space weather and\nstock prize forecasting.\n",
        "published": "2021",
        "authors": [
            "Sabrina Guastavino",
            "Michele Piana",
            "Federico Benvenuto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.03526v1",
        "title": "Meta Learning Black-Box Population-Based Optimizers",
        "abstract": "  The no free lunch theorem states that no model is better suited to every\nproblem. A question that arises from this is how to design methods that propose\noptimizers tailored to specific problems achieving state-of-the-art\nperformance. This paper addresses this issue by proposing the use of\nmeta-learning to infer population-based black-box optimizers that can\nautomatically adapt to specific classes of problems. We suggest a general\nmodeling of population-based algorithms that result in Learning-to-Optimize\nPOMDP (LTO-POMDP), a meta-learning framework based on a specific partially\nobservable Markov decision process (POMDP). From that framework's formulation,\nwe propose to parameterize the algorithm using deep recurrent neural networks\nand use a meta-loss function based on stochastic algorithms' performance to\ntrain efficient data-driven optimizers over several related optimization tasks.\nThe learned optimizers' performance based on this implementation is assessed on\nvarious black-box optimization tasks and hyperparameter tuning of machine\nlearning models. Our results revealed that the meta-loss function encourages a\nlearned algorithm to alter its search behavior so that it can easily fit into a\nnew context. Thus, it allows better generalization and higher sample efficiency\nthan state-of-the-art generic optimization algorithms, such as the Covariance\nmatrix adaptation evolution strategy (CMA-ES).\n",
        "published": "2021",
        "authors": [
            "Hugo Siqueira Gomes",
            "Benjamin L\u00e9ger",
            "Christian Gagn\u00e9"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.04710v3",
        "title": "Cluster-based Input Weight Initialization for Echo State Networks",
        "abstract": "  Echo State Networks (ESNs) are a special type of recurrent neural networks\n(RNNs), in which the input and recurrent connections are traditionally\ngenerated randomly, and only the output weights are trained. Despite the recent\nsuccess of ESNs in various tasks of audio, image and radar recognition, we\npostulate that a purely random initialization is not the ideal way of\ninitializing ESNs. The aim of this work is to propose an unsupervised\ninitialization of the input connections using the $K$-Means algorithm on the\ntraining data. We show that for a large variety of datasets this initialization\nperforms equivalently or superior than a randomly initialized ESN whilst\nneeding significantly less reservoir neurons. Furthermore, we discuss that this\napproach provides the opportunity to estimate a suitable size of the reservoir\nbased on prior knowledge about the data.\n",
        "published": "2021",
        "authors": [
            "Peter Steiner",
            "Azarakhsh Jalalvand",
            "Peter Birkholz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.05636v1",
        "title": "A Gradient Estimator for Time-Varying Electrical Networks with\n  Non-Linear Dissipation",
        "abstract": "  We propose a method for extending the technique of equilibrium propagation\nfor estimating gradients in fixed-point neural networks to the more general\nsetting of directed, time-varying neural networks by modeling them as\nelectrical circuits. We use electrical circuit theory to construct a Lagrangian\ncapable of describing deep, directed neural networks modeled using nonlinear\ncapacitors and inductors, linear resistors and sources, and a special class of\nnonlinear dissipative elements called fractional memristors. We then derive an\nestimator for the gradient of the physical parameters of the network, such as\nsynapse conductances, with respect to an arbitrary loss function. This\nestimator is entirely local, in that it only depends on information locally\navailable to each synapse. We conclude by suggesting methods for extending\nthese results to networks of biologically plausible neurons, e.g.\nHodgkin-Huxley neurons.\n",
        "published": "2021",
        "authors": [
            "Jack Kendall"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.06123v1",
        "title": "The whole brain architecture approach: Accelerating the development of\n  artificial general intelligence by referring to the brain",
        "abstract": "  The vastness of the design space created by the combination of a large number\nof computational mechanisms, including machine learning, is an obstacle to\ncreating an artificial general intelligence (AGI). Brain-inspired AGI\ndevelopment, in other words, cutting down the design space to look more like a\nbiological brain, which is an existing model of a general intelligence, is a\npromising plan for solving this problem. However, it is difficult for an\nindividual to design a software program that corresponds to the entire brain\nbecause the neuroscientific data required to understand the architecture of the\nbrain are extensive and complicated. The whole-brain architecture approach\ndivides the brain-inspired AGI development process into the task of designing\nthe brain reference architecture (BRA) -- the flow of information and the\ndiagram of corresponding components -- and the task of developing each\ncomponent using the BRA. This is called BRA-driven development. Another\ndifficulty lies in the extraction of the operating principles necessary for\nreproducing the cognitive-behavioral function of the brain from neuroscience\ndata. Therefore, this study proposes the Structure-constrained Interface\nDecomposition (SCID) method, which is a hypothesis-building method for creating\na hypothetical component diagram consistent with neuroscientific findings. The\napplication of this approach has begun for building various regions of the\nbrain. Moving forward, we will examine methods of evaluating the biological\nplausibility of brain-inspired software. This evaluation will also be used to\nprioritize different computational mechanisms, which should be merged,\nassociated with the same regions of the brain.\n",
        "published": "2021",
        "authors": [
            "Hiroshi Yamakawa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.06846v1",
        "title": "Policy Search with Rare Significant Events: Choosing the Right Partner\n  to Cooperate with",
        "abstract": "  This paper focuses on a class of reinforcement learning problems where\nsignificant events are rare and limited to a single positive reward per\nepisode. A typical example is that of an agent who has to choose a partner to\ncooperate with, while a large number of partners are simply not interested in\ncooperating, regardless of what the agent has to offer. We address this problem\nin a continuous state and action space with two different kinds of search\nmethods: a gradient policy search method and a direct policy search method\nusing an evolution strategy. We show that when significant events are rare,\ngradient information is also scarce, making it difficult for policy gradient\nsearch methods to find an optimal policy, with or without a deep neural\narchitecture. On the other hand, we show that direct policy search methods are\ninvariant to the rarity of significant events, which is yet another\nconfirmation of the unique role evolutionary algorithms has to play as a\nreinforcement learning method.\n",
        "published": "2021",
        "authors": [
            "Paul Ecoffet",
            "Nicolas Fontbonne",
            "Jean-Baptiste Andr\u00e9",
            "Nicolas Bredeche"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.08143v2",
        "title": "Constrained plasticity reserve as a natural way to control frequency and\n  weights in spiking neural networks",
        "abstract": "  Biological neurons have adaptive nature and perform complex computations\ninvolving the filtering of redundant information. However, most common neural\ncell models, including biologically plausible, such as Hodgkin-Huxley or\nIzhikevich, do not possess predictive dynamics on a single-cell level.\nMoreover, the modern rules of synaptic plasticity or interconnections weights\nadaptation also do not provide grounding for the ability of neurons to adapt to\nthe ever-changing input signal intensity. While natural neuron synaptic growth\nis precisely controlled and restricted by protein supply and recycling, weight\ncorrection rules such as widely used STDP are efficiently unlimited in change\nrate and scale. The present article introduces new mechanics of interconnection\nbetween neuron firing rate homeostasis and weight change through STDP growth\nbounded by abstract protein reserve, controlled by the intracellular\noptimization algorithm. We show how these cellular dynamics help neurons filter\nout the intense noise signals to help neurons keep a stable firing rate. We\nalso examine that such filtering does not affect the ability of neurons to\nrecognize the correlated inputs in unsupervised mode. Such an approach might be\nused in the machine learning domain to improve the robustness of AI systems.\n",
        "published": "2021",
        "authors": [
            "Oleg Nikitin",
            "Olga Lukyanova",
            "Alex Kunin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.08408v1",
        "title": "A Fast Heuristic for Gateway Location in Wireless Backhaul of 5G\n  Ultra-Dense Networks",
        "abstract": "  In 5G Ultra-Dense Networks, a distributed wireless backhaul is an attractive\nsolution for forwarding traffic to the core. The macro-cell coverage area is\ndivided into many small cells. A few of these cells are designated as gateways\nand are linked to the core by high-capacity fiber optic links. Each small cell\nis associated with one gateway and all small cells forward their traffic to\ntheir respective gateway through multi-hop mesh networks. We investigate the\ngateway location problem and show that finding near-optimal gateway locations\nimproves the backhaul network capacity. An exact p-median integer linear\nprogram is formulated for comparison with our novel K-GA heuristic that\ncombines a Genetic Algorithm (GA) with K-means clustering to find near-optimal\ngateway locations. We compare the performance of KGA with six other approaches\nin terms of average number of hops and backhaul network capacity at different\nnode densities through extensive Monte Carlo simulations. All approaches are\ntested in various user distribution scenarios, including uniform distribution,\nbivariate Gaussian distribution, and cluster distribution. In all cases K-GA\nprovides near-optimal results, achieving average number of hops and backhaul\nnetwork capacity within 2% of optimal while saving an average of 95% of the\nexecution time.\n",
        "published": "2021",
        "authors": [
            "Mital Raithatha",
            "Aizaz U. Chaudhry",
            "Roshdy H. M. Hafez",
            "John W. Chinneck"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.08878v2",
        "title": "Learning without gradient descent encoded by the dynamics of a\n  neurobiological model",
        "abstract": "  The success of state-of-the-art machine learning is essentially all based on\ndifferent variations of gradient descent algorithms that minimize some version\nof a cost or loss function. A fundamental limitation, however, is the need to\ntrain these systems in either supervised or unsupervised ways by exposing them\nto typically large numbers of training examples. Here, we introduce a\nfundamentally novel conceptual approach to machine learning that takes\nadvantage of a neurobiologically derived model of dynamic signaling,\nconstrained by the geometric structure of a network. We show that MNIST images\ncan be uniquely encoded and classified by the dynamics of geometric networks\nwith nearly state-of-the-art accuracy in an unsupervised way, and without the\nneed for any training.\n",
        "published": "2021",
        "authors": [
            "Vivek Kurien George",
            "Vikash Morar",
            "Weiwei Yang",
            "Jonathan Larson",
            "Bryan Tower",
            "Shweti Mahajan",
            "Arkin Gupta",
            "Christopher White",
            "Gabriel A. Silva"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.10790v2",
        "title": "Quality Evolvability ES: Evolving Individuals With a Distribution of\n  Well Performing and Diverse Offspring",
        "abstract": "  One of the most important lessons from the success of deep learning is that\nlearned representations tend to perform much better at any task compared to\nrepresentations we design by hand. Yet evolution of evolvability algorithms,\nwhich aim to automatically learn good genetic representations, have received\nrelatively little attention, perhaps because of the large amount of\ncomputational power they require. The recent method Evolvability ES allows\ndirect selection for evolvability with little computation. However, it can only\nbe used to solve problems where evolvability and task performance are aligned.\nWe propose Quality Evolvability ES, a method that simultaneously optimizes for\ntask performance and evolvability and without this restriction. Our proposed\napproach Quality Evolvability has similar motivation to Quality Diversity\nalgorithms, but with some important differences. While Quality Diversity aims\nto find an archive of diverse and well-performing, but potentially genetically\ndistant individuals, Quality Evolvability aims to find a single individual with\na diverse and well-performing distribution of offspring. By doing so Quality\nEvolvability is forced to discover more evolvable representations. We\ndemonstrate on robotic locomotion control tasks that Quality Evolvability ES,\nsimilarly to Quality Diversity methods, can learn faster than objective-based\nmethods and can handle deceptive problems.\n",
        "published": "2021",
        "authors": [
            "Adam Katona",
            "Daniel W. Franks",
            "James Alfred Walker"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.11177v1",
        "title": "A Deep Neural Network Surrogate Modeling Benchmark for Temperature Field\n  Prediction of Heat Source Layout",
        "abstract": "  Thermal issue is of great importance during layout design of heat source\ncomponents in systems engineering, especially for high functional-density\nproducts. Thermal analysis generally needs complex simulation, which leads to\nan unaffordable computational burden to layout optimization as it iteratively\nevaluates different schemes. Surrogate modeling is an effective way to\nalleviate computation complexity. However, temperature field prediction (TFP)\nwith complex heat source layout (HSL) input is an ultra-high dimensional\nnonlinear regression problem, which brings great difficulty to traditional\nregression models. The Deep neural network (DNN) regression method is a\nfeasible way for its good approximation performance. However, it faces great\nchallenges in both data preparation for sample diversity and uniformity in the\nlayout space with physical constraints, and proper DNN model selection and\ntraining for good generality, which necessitates efforts of both layout\ndesigner and DNN experts. To advance this cross-domain research, this paper\nproposes a DNN based HSL-TFP surrogate modeling task benchmark. With\nconsideration for engineering applicability, sample generation, dataset\nevaluation, DNN model, and surrogate performance metrics, are thoroughly\nstudied. Experiments are conducted with ten representative state-of-the-art DNN\nmodels. Detailed discussion on baseline results is provided and future\nprospects are analyzed for DNN based HSL-TFP tasks.\n",
        "published": "2021",
        "authors": [
            "Xianqi Chen",
            "Xiaoyu Zhao",
            "Zhiqiang Gong",
            "Jun Zhang",
            "Weien Zhou",
            "Xiaoqian Chen",
            "Wen Yao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.11715v1",
        "title": "Transforming Exploratory Creativity with DeLeNoX",
        "abstract": "  We introduce DeLeNoX (Deep Learning Novelty Explorer), a system that\nautonomously creates artifacts in constrained spaces according to its own\nevolving interestingness criterion. DeLeNoX proceeds in alternating phases of\nexploration and transformation. In the exploration phases, a version of novelty\nsearch augmented with constraint handling searches for maximally diverse\nartifacts using a given distance function. In the transformation phases, a deep\nlearning autoencoder learns to compress the variation between the found\nartifacts into a lower-dimensional space. The newly trained encoder is then\nused as the basis for a new distance function, transforming the criteria for\nthe next exploration phase. In the current paper, we apply DeLeNoX to the\ncreation of spaceships suitable for use in two-dimensional arcade-style\ncomputer games, a representative problem in procedural content generation in\ngames. We also situate DeLeNoX in relation to the distinction between\nexploratory and transformational creativity, and in relation to Schmidhuber's\ntheory of creativity through the drive for compression progress.\n",
        "published": "2021",
        "authors": [
            "Antonios Liapis",
            "Hector P. Martinez",
            "Julian Togelius",
            "Georgios N. Yannakakis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.11726v1",
        "title": "SuSketch: Surrogate Models of Gameplay as a Design Assistant",
        "abstract": "  This paper introduces SuSketch, a design tool for first person shooter\nlevels. SuSketch provides the designer with gameplay predictions for two\ncompeting players of specific character classes. The interface allows the\ndesigner to work side-by-side with an artificially intelligent creator and to\nreceive varied types of feedback such as path information, predicted balance\nbetween players in a complete playthrough, or a predicted heatmap of the\nlocations of player deaths. The system also proactively designs alternatives to\nthe level and class pairing, and presents them to the designer as suggestions\nthat improve the predicted balance of the game. SuSketch offers a new way of\nintegrating machine learning into mixed-initiative co-creation tools, as a\nsurrogate of human play trained on a large corpus of artificial playtraces. A\nuser study with 16 game developers indicated that the tool was easy to use, but\nalso highlighted a need to make SuSketch more accessible and more explainable.\n",
        "published": "2021",
        "authors": [
            "Panagiotis Migkotzidis",
            "Antonios Liapis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.12564v2",
        "title": "Linear Constraints Learning for Spiking Neurons",
        "abstract": "  We introduce a new supervised learning algorithm based to train spiking\nneural networks for classification. The algorithm overcomes a limitation of\nexisting multi-spike learning methods: it solves the problem of interference\nbetween interacting output spikes during a learning trial. This problem of\nlearning interference causes learning performance in existing approaches to\ndecrease as the number of output spikes increases, and represents an important\nlimitation in existing multi-spike learning approaches. We address learning\ninterference by introducing a novel mechanism to balance the magnitudes of\nweight adjustments during learning, which in theory allows every spike to\nsimultaneously converge to their desired timings. Our results indicate that our\nmethod achieves significantly higher memory capacity and faster convergence\ncompared to existing approaches for multi-spike classification. In the\nubiquitous Iris and MNIST datasets, our algorithm achieves competitive\npredictive performance with state-of-the-art approaches.\n",
        "published": "2021",
        "authors": [
            "Huy Le Nguyen",
            "Dominique Chu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.13861v1",
        "title": "Hierarchical Program-Triggered Reinforcement Learning Agents For\n  Automated Driving",
        "abstract": "  Recent advances in Reinforcement Learning (RL) combined with Deep Learning\n(DL) have demonstrated impressive performance in complex tasks, including\nautonomous driving. The use of RL agents in autonomous driving leads to a\nsmooth human-like driving experience, but the limited interpretability of Deep\nReinforcement Learning (DRL) creates a verification and certification\nbottleneck. Instead of relying on RL agents to learn complex tasks, we propose\nHPRL - Hierarchical Program-triggered Reinforcement Learning, which uses a\nhierarchy consisting of a structured program along with multiple RL agents,\neach trained to perform a relatively simple task. The focus of verification\nshifts to the master program under simple guarantees from the RL agents,\nleading to a significantly more interpretable and verifiable implementation as\ncompared to a complex RL agent. The evaluation of the framework is demonstrated\non different driving tasks, and NHTSA precrash scenarios using CARLA, an\nopen-source dynamic urban simulation environment.\n",
        "published": "2021",
        "authors": [
            "Briti Gangopadhyay",
            "Harshit Soora",
            "Pallab Dasgupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.14886v2",
        "title": "Generalization over different cellular automata rules learned by a deep\n  feed-forward neural network",
        "abstract": "  To test generalization ability of a class of deep neural networks, we\nrandomly generate a large number of different rule sets for 2-D cellular\nautomata (CA), based on John Conway's Game of Life. Using these rules, we\ncompute several trajectories for each CA instance. A deep convolutional\nencoder-decoder network with short and long range skip connections is trained\non various generated CA trajectories to predict the next CA state given its\nprevious states. Results show that the network is able to learn the rules of\nvarious, complex cellular automata and generalize to unseen configurations. To\nsome extent, the network shows generalization to rule sets and neighborhood\nsizes that were not seen during the training at all. Code to reproduce the\nexperiments is publicly available at:\nhttps://github.com/SLAMPAI/generalization-cellular-automata\n",
        "published": "2021",
        "authors": [
            "Marcel Aach",
            "Jens Henrik Goebbert",
            "Jenia Jitsev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.15692v1",
        "title": "Self-Constructing Neural Networks Through Random Mutation",
        "abstract": "  The search for neural architecture is producing many of the most exciting\nresults in artificial intelligence. It has increasingly become apparent that\ntask-specific neural architecture plays a crucial role for effectively solving\nproblems. This paper presents a simple method for learning neural architecture\nthrough random mutation. This method demonstrates 1) neural architecture may be\nlearned during the agent's lifetime, 2) neural architecture may be constructed\nover a single lifetime without any initial connections or neurons, and 3)\narchitectural modifications enable rapid adaptation to dynamic and novel task\nscenarios. Starting without any neurons or connections, this method constructs\na neural architecture capable of high-performance on several tasks. The\nlifelong learning capabilities of this method are demonstrated in an\nenvironment without episodic resets, even learning with constantly changing\nmorphology, limb disablement, and changing task goals all without losing\nlocomotion capabilities.\n",
        "published": "2021",
        "authors": [
            "Samuel Schmidgall"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.02032v3",
        "title": "Artificial Neural Network Modeling for Airline Disruption Management",
        "abstract": "  Since the 1970s, most airlines have incorporated computerized support for\nmanaging disruptions during flight schedule execution. However, existing\nplatforms for airline disruption management (ADM) employ monolithic system\ndesign methods that rely on the creation of specific rules and requirements\nthrough explicit optimization routines, before a system that meets the\nspecifications is designed. Thus, current platforms for ADM are unable to\nreadily accommodate additional system complexities resulting from the\nintroduction of new capabilities, such as the introduction of unmanned aerial\nsystems (UAS), operations and infrastructure, to the system. To this end, we\nuse historical data on airline scheduling and operations recovery to develop a\nsystem of artificial neural networks (ANNs), which describe a predictive\ntransfer function model (PTFM) for promptly estimating the recovery impact of\ndisruption resolutions at separate phases of flight schedule execution during\nADM. Furthermore, we provide a modular approach for assessing and executing the\nPTFM by employing a parallel ensemble method to develop generative routines\nthat amalgamate the system of ANNs. Our modular approach ensures that current\nindustry standards for tardiness in flight schedule execution during ADM are\nsatisfied, while accurately estimating appropriate time-based performance\nmetrics for the separate phases of flight schedule execution.\n",
        "published": "2021",
        "authors": [
            "Kolawole Ogunsina",
            "Wendy A. Okolo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.03624v1",
        "title": "Post-Hoc Domain Adaptation via Guided Data Homogenization",
        "abstract": "  Addressing shifts in data distributions is an important prerequisite for the\ndeployment of deep learning models to real-world settings. A general approach\nto this problem involves the adjustment of models to a new domain through\ntransfer learning. However, in many cases, this is not applicable in a post-hoc\nmanner to deployed models and further parameter adjustments jeopardize safety\ncertifications that were established beforehand. In such a context, we propose\nto deal with changes in the data distribution via guided data homogenization\nwhich shifts the burden of adaptation from the model to the data. This approach\nmakes use of information about the training data contained implicitly in the\ndeep learning model to learn a domain transfer function. This allows for a\ntargeted deployment of models to unknown scenarios without changing the model\nitself. We demonstrate the potential of data homogenization through experiments\non the CIFAR-10 and MNIST data sets.\n",
        "published": "2021",
        "authors": [
            "Kurt Willis",
            "Luis Oala"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.03760v1",
        "title": "A Reinforcement Learning Environment For Job-Shop Scheduling",
        "abstract": "  Scheduling is a fundamental task occurring in various automated systems\napplications, e.g., optimal schedules for machines on a job shop allow for a\nreduction of production costs and waste. Nevertheless, finding such schedules\nis often intractable and cannot be achieved by Combinatorial Optimization\nProblem (COP) methods within a given time limit. Recent advances of Deep\nReinforcement Learning (DRL) in learning complex behavior enable new COP\napplication possibilities. This paper presents an efficient DRL environment for\nJob-Shop Scheduling -- an important problem in the field. Furthermore, we\ndesign a meaningful and compact state representation as well as a novel, simple\ndense reward function, closely related to the sparse make-span minimization\ncriteria used by COP methods. We demonstrate that our approach significantly\noutperforms existing DRL methods on classic benchmark instances, coming close\nto state-of-the-art COP approaches.\n",
        "published": "2021",
        "authors": [
            "Pierre Tassel",
            "Martin Gebser",
            "Konstantin Schekotihin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.03936v1",
        "title": "BR-NS: an Archive-less Approach to Novelty Search",
        "abstract": "  As open-ended learning based on divergent search algorithms such as Novelty\nSearch (NS) draws more and more attention from the research community, it is\nnatural to expect that its application to increasingly complex real-world\nproblems will require the exploration to operate in higher dimensional Behavior\nSpaces which will not necessarily be Euclidean. Novelty Search traditionally\nrelies on k-nearest neighbours search and an archive of previously visited\nbehavior descriptors which are assumed to live in a Euclidean space. This is\nproblematic because of a number of issues. On one hand, Euclidean distance and\nNearest-neighbour search are known to behave differently and become less\nmeaningful in high dimensional spaces. On the other hand, the archive has to be\nbounded since, memory considerations aside, the computational complexity of\nfinding nearest neighbours in that archive grows linearithmically with its\nsize. A sub-optimal bound can result in \"cycling\" in the behavior space, which\ninhibits the progress of the exploration. Furthermore, the performance of NS\ndepends on a number of algorithmic choices and hyperparameters, such as the\nstrategies to add or remove elements to the archive and the number of\nneighbours to use in k-nn search. In this paper, we discuss an alternative\napproach to novelty estimation, dubbed Behavior Recognition based Novelty\nSearch (BR-NS), which does not require an archive, makes no assumption on the\nmetrics that can be defined in the behavior space and does not rely on nearest\nneighbours search. We conduct experiments to gain insight into its feasibility\nand dynamics as well as potential advantages over archive-based NS in terms of\ntime complexity.\n",
        "published": "2021",
        "authors": [
            "Achkan Salehi",
            "Alexandre Coninx",
            "Stephane Doncieux"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.04768v1",
        "title": "Selection-Expansion: A Unifying Framework for Motion-Planning and\n  Diversity Search Algorithms",
        "abstract": "  Reinforcement learning agents need a reward signal to learn successful\npolicies. When this signal is sparse or the corresponding gradient is\ndeceptive, such agents need a dedicated mechanism to efficiently explore their\nsearch space without relying on the reward. Looking for a large diversity of\nbehaviors or using Motion Planning (MP) algorithms are two options in this\ncontext. In this paper, we build on the common roots between these two options\nto investigate the properties of two diversity search algorithms, the Novelty\nSearch and the Goal Exploration Process algorithms. These algorithms look for\ndiversity in an outcome space or behavioral space which is generally\nhand-designed to represent what matters for a given task. The relation to MP\nalgorithms reveals that the smoothness, or lack of smoothness of the mapping\nbetween the policy parameter space and the outcome space plays a key role in\nthe search efficiency. In particular, we show empirically that, if the mapping\nis smooth enough, i.e. if two close policies in the parameter space lead to\nsimilar outcomes, then diversity algorithms tend to inherit exploration\nproperties of MP algorithms. By contrast, if it is not, diversity algorithms\nlose these properties and their performance strongly depends on specific\nheuristics, notably filtering mechanisms that discard some of the explored\npolicies.\n",
        "published": "2021",
        "authors": [
            "Alexandre Chenu",
            "Nicolas Perrin-Gilbert",
            "St\u00e9phane Doncieux",
            "Olivier Sigaud"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.07715v1",
        "title": "Quantum Architecture Search via Deep Reinforcement Learning",
        "abstract": "  Recent advances in quantum computing have drawn considerable attention to\nbuilding realistic application for and using quantum computers. However,\ndesigning a suitable quantum circuit architecture requires expert knowledge.\nFor example, it is non-trivial to design a quantum gate sequence for generating\na particular quantum state with as fewer gates as possible. We propose a\nquantum architecture search framework with the power of deep reinforcement\nlearning (DRL) to address this challenge. In the proposed framework, the DRL\nagent can only access the Pauli-$X$, $Y$, $Z$ expectation values and a\npredefined set of quantum operations for learning the target quantum state, and\nis optimized by the advantage actor-critic (A2C) and proximal policy\noptimization (PPO) algorithms. We demonstrate a successful generation of\nquantum gate sequences for multi-qubit GHZ states without encoding any\nknowledge of quantum physics in the agent. The design of our framework is\nrather general and can be employed with other DRL architectures or optimization\nmethods to study gate synthesis and compilation for many quantum states.\n",
        "published": "2021",
        "authors": [
            "En-Jui Kuo",
            "Yao-Lung L. Fang",
            "Samuel Yen-Chi Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.08048v1",
        "title": "A Novel Surrogate-assisted Evolutionary Algorithm Applied to\n  Partition-based Ensemble Learning",
        "abstract": "  We propose a novel surrogate-assisted Evolutionary Algorithm for solving\nexpensive combinatorial optimization problems. We integrate a surrogate model,\nwhich is used for fitness value estimation, into a state-of-the-art P3-like\nvariant of the Gene-Pool Optimal Mixing Algorithm (GOMEA) and adapt the\nresulting algorithm for solving non-binary combinatorial problems. We test the\nproposed algorithm on an ensemble learning problem. Ensembling several models\nis a common Machine Learning technique to achieve better performance. We\nconsider ensembles of several models trained on disjoint subsets of a dataset.\nFinding the best dataset partitioning is naturally a combinatorial non-binary\noptimization problem. Fitness function evaluations can be extremely expensive\nif complex models, such as Deep Neural Networks, are used as learners in an\nensemble. Therefore, the number of fitness function evaluations is typically\nlimited, necessitating expensive optimization techniques. In our experiments we\nuse five classification datasets from the OpenML-CC18 benchmark and\nSupport-vector Machines as learners in an ensemble. The proposed algorithm\ndemonstrates better performance than alternative approaches, including Bayesian\noptimization algorithms. It manages to find better solutions using just several\nthousand fitness function evaluations for an ensemble learning problem with up\nto 500 variables.\n",
        "published": "2021",
        "authors": [
            "Arkadiy Dushatskiy",
            "Tanja Alderliesten",
            "Peter A. N. Bosman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.09163v1",
        "title": "Bidirectional Interaction between Visual and Motor Generative Models\n  using Predictive Coding and Active Inference",
        "abstract": "  In this work, we build upon the Active Inference (AIF) and Predictive Coding\n(PC) frameworks to propose a neural architecture comprising a generative model\nfor sensory prediction, and a distinct generative model for motor trajectories.\nWe highlight how sequences of sensory predictions can act as rails guiding\nlearning, control and online adaptation of motor trajectories. We furthermore\ninquire the effects of bidirectional interactions between the motor and the\nvisual modules. The architecture is tested on the control of a simulated\nrobotic arm learning to reproduce handwritten letters.\n",
        "published": "2021",
        "authors": [
            "Louis Annabi",
            "Alexandre Pitti",
            "Mathias Quoy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.09943v2",
        "title": "The principle of weight divergence facilitation for unsupervised pattern\n  recognition in spiking neural networks",
        "abstract": "  Parallels between the signal processing tasks and biological neurons lead to\nan understanding of the principles of self-organized optimization of input\nsignal recognition. In the present paper, we discuss such similarities among\nbiological and technical systems. We propose adding the well-known STDP\nsynaptic plasticity rule to direct the weight modification towards the state\nassociated with the maximal difference between background noise and correlated\nsignals. We use the principle of physically constrained weight growth as a\nbasis for such weights' modification control. It is proposed that the existence\nand production of bio-chemical 'substances' needed for plasticity development\nrestrict a biological synaptic straight modification. In this paper, the\ninformation about the noise-to-signal ratio controls such a substances'\nproduction and storage and drives the neuron's synaptic pressures towards the\nstate with the best signal-to-noise ratio. We consider several experiments with\ndifferent input signal regimes to understand the functioning of the proposed\napproach.\n",
        "published": "2021",
        "authors": [
            "Oleg Nikitin",
            "Olga Lukyanova",
            "Alex Kunin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.11169v1",
        "title": "Noise-Robust Deep Spiking Neural Networks with Temporal Information",
        "abstract": "  Spiking neural networks (SNNs) have emerged as energy-efficient neural\nnetworks with temporal information. SNNs have shown a superior efficiency on\nneuromorphic devices, but the devices are susceptible to noise, which hinders\nthem from being applied in real-world applications. Several studies have\nincreased noise robustness, but most of them considered neither deep SNNs nor\ntemporal information. In this paper, we investigate the effect of noise on deep\nSNNs with various neural coding methods and present a noise-robust deep SNN\nwith temporal information. With the proposed methods, we have achieved a deep\nSNN that is efficient and robust to spike deletion and jitter.\n",
        "published": "2021",
        "authors": [
            "Seongsik Park",
            "Dongjin Lee",
            "Sungroh Yoon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.00672v1",
        "title": "What Matters for Adversarial Imitation Learning?",
        "abstract": "  Adversarial imitation learning has become a popular framework for imitation\nin continuous control. Over the years, several variations of its components\nwere proposed to enhance the performance of the learned policies as well as the\nsample complexity of the algorithm. In practice, these choices are rarely\ntested all together in rigorous empirical studies. It is therefore difficult to\ndiscuss and understand what choices, among the high-level algorithmic options\nas well as low-level implementation details, matter. To tackle this issue, we\nimplement more than 50 of these choices in a generic adversarial imitation\nlearning framework and investigate their impacts in a large-scale study (>500k\ntrained agents) with both synthetic and human-generated demonstrations. While\nmany of our findings confirm common practices, some of them are surprising or\neven contradict prior work. In particular, our results suggest that artificial\ndemonstrations are not a good proxy for human data and that the very common\npractice of evaluating imitation algorithms only with synthetic demonstrations\nmay lead to algorithms which perform poorly in the more realistic scenarios\nwith human demonstrations.\n",
        "published": "2021",
        "authors": [
            "Manu Orsini",
            "Anton Raichuk",
            "L\u00e9onard Hussenot",
            "Damien Vincent",
            "Robert Dadashi",
            "Sertan Girgin",
            "Matthieu Geist",
            "Olivier Bachem",
            "Olivier Pietquin",
            "Marcin Andrychowicz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.01741v3",
        "title": "Lifetime policy reuse and the importance of task capacity",
        "abstract": "  A long-standing challenge in artificial intelligence is lifelong\nreinforcement learning, where learners are given many tasks in sequence and\nmust transfer knowledge between tasks while avoiding catastrophic forgetting.\nPolicy reuse and other multi-policy reinforcement learning techniques can learn\nmultiple tasks but may generate many policies. This paper presents two novel\ncontributions, namely 1) Lifetime Policy Reuse, a model-agnostic policy reuse\nalgorithm that avoids generating many policies by optimising a fixed number of\nnear-optimal policies through a combination of policy optimisation and adaptive\npolicy selection; and 2) the task capacity, a measure for the maximal number of\ntasks that a policy can accurately solve. Comparing two state-of-the-art\nbase-learners, the results demonstrate the importance of Lifetime Policy Reuse\nand task capacity based pre-selection on an 18-task partially observable Pacman\ndomain and a Cartpole domain of up to 125 tasks.\n",
        "published": "2021",
        "authors": [
            "David M. Bossens",
            "Adam J. Sobey"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.01958v3",
        "title": "Multiplierless MP-Kernel Machine For Energy-efficient Edge Devices",
        "abstract": "  We present a novel framework for designing multiplierless kernel machines\nthat can be used on resource-constrained platforms like intelligent edge\ndevices. The framework uses a piecewise linear (PWL) approximation based on a\nmargin propagation (MP) technique and uses only addition/subtraction, shift,\ncomparison, and register underflow/overflow operations. We propose a\nhardware-friendly MP-based inference and online training algorithm that has\nbeen optimized for a Field Programmable Gate Array (FPGA) platform. Our FPGA\nimplementation eliminates the need for DSP units and reduces the number of\nLUTs. By reusing the same hardware for inference and training, we show that the\nplatform can overcome classification errors and local minima artifacts that\nresult from the MP approximation. The implementation of this proposed\nmultiplierless MP-kernel machine on FPGA results in an estimated energy\nconsumption of 13.4 pJ and power consumption of 107 mW with ~9k LUTs and FFs\neach for a 256 x 32 sized kernel making it superior in terms of power,\nperformance, and area compared to other comparable implementations.\n",
        "published": "2021",
        "authors": [
            "Abhishek Ramdas Nair",
            "Pallab Kumar Nath",
            "Shantanu Chakrabartty",
            "Chetan Singh Thakur"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.02626v4",
        "title": "Dynamics of specialization in neural modules under resource constraints",
        "abstract": "  It has long been believed that the brain is highly modular both in terms of\nstructure and function, although recent evidence has led some to question the\nextent of both types of modularity. We used artificial neural networks to test\nthe hypothesis that structural modularity is sufficient to guarantee functional\nspecialization, and find that in general, this doesn't necessarily hold except\nat extreme levels. We then systematically tested which features of the\nenvironment and network do lead to the emergence of specialization. We used a\nsimple toy environment, task and network, allowing us precise control, and show\nthat in this setup, several distinct measures of specialization give\nqualitatively similar results. We further find that (1) specialization can only\nemerge in environments where features of that environment are meaningfully\nseparable, (2) specialization preferentially emerges when the network is\nstrongly resource-constrained, and (3) these findings are qualitatively similar\nacross different network architectures, but the quantitative relationships\ndepends on the architecture type. Finally, we show that functional\nspecialization varies dynamically across time, and demonstrate that these\ndynamics depend on both the timing and bandwidth of information flow in the\nnetwork. We conclude that a static notion of specialization, based on\nstructural modularity, is likely too simple a framework for understanding\nintelligence in situations of real-world complexity, from biology to\nbrain-inspired neuromorphic systems. We propose that thoroughly stress testing\ncandidate definitions of functional modularity in simplified scenarios before\nextending to more complex data, network models and electrophysiological\nrecordings is likely to be a fruitful approach.\n",
        "published": "2021",
        "authors": [
            "Gabriel B\u00e9na",
            "Dan F. M. Goodman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.03394v1",
        "title": "A generative model for molecule generation based on chemical reaction\n  trees",
        "abstract": "  Deep generative models have been shown powerful in generating novel molecules\nwith desired chemical properties via their representations such as strings,\ntrees or graphs. However, these models are limited in recommending synthetic\nroutes for the generated molecules in practice. We propose a generative model\nto generate molecules via multi-step chemical reaction trees. Specifically, our\nmodel first propose a chemical reaction tree with predicted reaction templates\nand commercially available molecules (starting molecules), and then perform\nforward synthetic steps to obtain product molecules. Experiments show that our\nmodel can generate chemical reactions whose product molecules are with desired\nchemical properties. Also, the complete synthetic routes for these product\nmolecules are provided.\n",
        "published": "2021",
        "authors": [
            "Dai Hai Nguyen",
            "Koji Tsuda"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.06061v1",
        "title": "Data-driven battery operation for energy arbitrage using rainbow deep\n  reinforcement learning",
        "abstract": "  As the world seeks to become more sustainable, intelligent solutions are\nneeded to increase the penetration of renewable energy. In this paper, the\nmodel-free deep reinforcement learning algorithm Rainbow Deep Q-Networks is\nused to control a battery in a small microgrid to perform energy arbitrage and\nmore efficiently utilise solar and wind energy sources. The grid operates with\nits own demand and renewable generation based on a dataset collected at Keele\nUniversity, as well as using dynamic energy pricing from a real wholesale\nenergy market. Four scenarios are tested including using demand and price\nforecasting produced with local weather data. The algorithm and its\nsubcomponents are evaluated against two continuous control benchmarks with\nRainbow able to outperform all other method. This research shows the importance\nof using the distributional approach for reinforcement learning when working\nwith complex environments and reward functions, as well as how it can be used\nto visualise and contextualise the agent's behaviour for real-world\napplications.\n",
        "published": "2021",
        "authors": [
            "Daniel J. B. Harrold",
            "Jun Cao",
            "Zhong Fan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.06232v6",
        "title": "GDI: Rethinking What Makes Reinforcement Learning Different From\n  Supervised Learning",
        "abstract": "  Deep Q Network (DQN) firstly kicked the door of deep reinforcement learning\n(DRL) via combining deep learning (DL) with reinforcement learning (RL), which\nhas noticed that the distribution of the acquired data would change during the\ntraining process. DQN found this property might cause instability for training,\nso it proposed effective methods to handle the downside of the property.\nInstead of focusing on the unfavourable aspects, we find it critical for RL to\nease the gap between the estimated data distribution and the ground truth data\ndistribution while supervised learning (SL) fails to do so. From this new\nperspective, we extend the basic paradigm of RL called the Generalized Policy\nIteration (GPI) into a more generalized version, which is called the\nGeneralized Data Distribution Iteration (GDI). We see massive RL algorithms and\ntechniques can be unified into the GDI paradigm, which can be considered as one\nof the special cases of GDI. We provide theoretical proof of why GDI is better\nthan GPI and how it works. Several practical algorithms based on GDI have been\nproposed to verify the effectiveness and extensiveness of it. Empirical\nexperiments prove our state-of-the-art (SOTA) performance on Arcade Learning\nEnvironment (ALE), wherein our algorithm has achieved 9620.98% mean human\nnormalized score (HNS), 1146.39% median HNS and 22 human world record\nbreakthroughs (HWRB) using only 200M training frames. Our work aims to lead the\nRL research to step into the journey of conquering the human world records and\nseek real superhuman agents on both performance and efficiency.\n",
        "published": "2021",
        "authors": [
            "Jiajun Fan",
            "Changnan Xiao",
            "Yue Huang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.07030v2",
        "title": "The Backpropagation Algorithm Implemented on Spiking Neuromorphic\n  Hardware",
        "abstract": "  The capabilities of natural neural systems have inspired new generations of\nmachine learning algorithms as well as neuromorphic very large-scale integrated\n(VLSI) circuits capable of fast, low-power information processing. However, it\nhas been argued that most modern machine learning algorithms are not\nneurophysiologically plausible. In particular, the workhorse of modern deep\nlearning, the backpropagation algorithm, has proven difficult to translate to\nneuromorphic hardware. In this study, we present a neuromorphic, spiking\nbackpropagation algorithm based on synfire-gated dynamical information\ncoordination and processing, implemented on Intel's Loihi neuromorphic research\nprocessor. We demonstrate a proof-of-principle three-layer circuit that learns\nto classify digits from the MNIST dataset. To our knowledge, this is the first\nwork to show a Spiking Neural Network (SNN) implementation of the\nbackpropagation algorithm that is fully on-chip, without a computer in the\nloop. It is competitive in accuracy with off-chip trained SNNs and achieves an\nenergy-delay product suitable for edge computing. This implementation shows a\npath for using in-memory, massively parallel neuromorphic processors for\nlow-power, low-latency implementation of modern deep learning applications.\n",
        "published": "2021",
        "authors": [
            "Alpha Renner",
            "Forrest Sheldon",
            "Anatoly Zlotnik",
            "Louis Tao",
            "Andrew Sornborger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.11463v1",
        "title": "A Logical Neural Network Structure With More Direct Mapping From Logical\n  Relations",
        "abstract": "  Logical relations widely exist in human activities. Human use them for making\njudgement and decision according to various conditions, which are embodied in\nthe form of \\emph{if-then} rules. As an important kind of cognitive\nintelligence, it is prerequisite of representing and storing logical relations\nrightly into computer systems so as to make automatic judgement and decision,\nespecially for high-risk domains like medical diagnosis. However, current\nnumeric ANN (Artificial Neural Network) models are good at perceptual\nintelligence such as image recognition while they are not good at cognitive\nintelligence such as logical representation, blocking the further application\nof ANN. To solve it, researchers have tried to design logical ANN models to\nrepresent and store logical relations. Although there are some advances in this\nresearch area, recent works still have disadvantages because the structures of\nthese logical ANN models still don't map more directly with logical relations\nwhich will cause the corresponding logical relations cannot be read out from\ntheir network structures. Therefore, in order to represent logical relations\nmore clearly by the neural network structure and to read out logical relations\nfrom it, this paper proposes a novel logical ANN model by designing the new\nlogical neurons and links in demand of logical representation. Compared with\nthe recent works on logical ANN models, this logical ANN model has more clear\ncorresponding with logical relations using the more direct mapping method\nherein, thus logical relations can be read out following the connection\npatterns of the network structure. Additionally, less neurons are used.\n",
        "published": "2021",
        "authors": [
            "Gang Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.11804v1",
        "title": "Evo* 2021 -- Late-Breaking Abstracts Volume",
        "abstract": "  Volume with the Late-Breaking Abstracts submitted to the Evo* 2021\nConference, held online from 7 to 9 of April 2021. These papers present ongoing\nresearch and preliminary results investigating on the application of different\napproaches of Bioinspired Methods (mainly Evolutionary Computation) to\ndifferent problems, most of them real world ones.\n",
        "published": "2021",
        "authors": [
            "A. M. Mora",
            "A. I. Esparcia-Alc\u00e1zar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.12891v2",
        "title": "Encoding Involutory Invariances in Neural Networks",
        "abstract": "  In certain situations, neural networks are trained upon data that obey\nunderlying symmetries. However, the predictions do not respect the symmetries\nexactly unless embedded in the network structure. In this work, we introduce\narchitectures that embed a special kind of symmetry namely, invariance with\nrespect to involutory linear/affine transformations up to parity $p=\\pm 1$. We\nprovide rigorous theorems to show that the proposed network ensures such an\ninvariance and present qualitative arguments for a special universal\napproximation theorem. An adaption of our techniques to CNN tasks for datasets\nwith inherent horizontal/vertical reflection symmetry is demonstrated.\nExtensive experiments indicate that the proposed model outperforms baseline\nfeed-forward and physics-informed neural networks while identically respecting\nthe underlying symmetry.\n",
        "published": "2021",
        "authors": [
            "Anwesh Bhattacharya",
            "Marios Mattheakis",
            "Pavlos Protopapas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.00700v3",
        "title": "Piecewise Linear Units Improve Deep Neural Networks",
        "abstract": "  The activation function is at the heart of a deep neural networks\nnonlinearity; the choice of the function has great impact on the success of\ntraining. Currently, many practitioners prefer the Rectified Linear Unit (ReLU)\ndue to its simplicity and reliability, despite its few drawbacks. While most\nprevious functions proposed to supplant ReLU have been hand-designed, recent\nwork on learning the function during training has shown promising results. In\nthis paper we propose an adaptive piecewise linear activation function, the\nPiecewise Linear Unit (PiLU), which can be learned independently for each\ndimension of the neural network. We demonstrate how PiLU is a generalised\nrectifier unit and note its similarities with the Adaptive Piecewise Linear\nUnits, namely adaptive and piecewise linear. Across a distribution of 30\nexperiments, we show that for the same model architecture, hyperparameters, and\npre-processing, PiLU significantly outperforms ReLU: reducing classification\nerror by 18.53% on CIFAR-10 and 13.13% on CIFAR-100, for a minor increase in\nthe number of neurons. Further work should be dedicated to exploring\ngeneralised piecewise linear units, as well as verifying these results across\nother challenging domains and larger problems.\n",
        "published": "2021",
        "authors": [
            "Jordan Inturrisi",
            "Sui Yang Khoo",
            "Abbas Kouzani",
            "Riccardo Pagliarella"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.02428v2",
        "title": "A Method for Medical Data Analysis Using the LogNNet for Clinical\n  Decision Support Systems and Edge Computing in Healthcare",
        "abstract": "  Edge computing is a fast-growing and much needed technology in healthcare.\nThe problem of implementing artificial intelligence on edge devices is the\ncomplexity and high resource intensity of the most known neural network data\nanalysis methods and algorithms. The difficulty of implementing these methods\non low-power microcontrollers with small memory size calls for the development\nof new effective algorithms for neural networks. This study presents a new\nmethod for analyzing medical data based on the LogNNet neural network, which\nuses chaotic mappings to transform input information. The method effectively\nsolves classification problems and calculates risk factors for the presence of\na disease in a patient according to a set of medical health indicators. The\nefficiency of LogNNet in assessing perinatal risk is illustrated on\ncardiotocogram data obtained from the UC Irvine machine learning repository.\nThe classification accuracy reaches ~91% with the ~3-10 kB of RAM used on the\nArduino microcontroller. Using the LogNNet network trained on a publicly\navailable database of the Israeli Ministry of Health, a service concept for\nCOVID-19 express testing is provided. A classification accuracy of ~95% is\nachieved, and ~0.6 kB of RAM is used. In all examples, the model is tested\nusing standard classification quality metrics: precision, recall, and\nF1-measure. The LogNNet architecture allows the implementation of artificial\nintelligence on medical peripherals of the Internet of Things with low RAM\nresources and can be used in clinical decision support systems.\n",
        "published": "2021",
        "authors": [
            "Andrei Velichko"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.05149v1",
        "title": "Logic Explained Networks",
        "abstract": "  The large and still increasing popularity of deep learning clashes with a\nmajor limit of neural network architectures, that consists in their lack of\ncapability in providing human-understandable motivations of their decisions. In\nsituations in which the machine is expected to support the decision of human\nexperts, providing a comprehensible explanation is a feature of crucial\nimportance. The language used to communicate the explanations must be formal\nenough to be implementable in a machine and friendly enough to be\nunderstandable by a wide audience. In this paper, we propose a general approach\nto Explainable Artificial Intelligence in the case of neural architectures,\nshowing how a mindful design of the networks leads to a family of interpretable\ndeep learning models called Logic Explained Networks (LENs). LENs only require\ntheir inputs to be human-understandable predicates, and they provide\nexplanations in terms of simple First-Order Logic (FOL) formulas involving such\npredicates. LENs are general enough to cover a large number of scenarios.\nAmongst them, we consider the case in which LENs are directly used as special\nclassifiers with the capability of being explainable, or when they act as\nadditional networks with the role of creating the conditions for making a\nblack-box classifier explainable by FOL formulas. Despite supervised learning\nproblems are mostly emphasized, we also show that LENs can learn and provide\nexplanations in unsupervised learning settings. Experimental results on several\ndatasets and tasks show that LENs may yield better classifications than\nestablished white-box models, such as decision trees and Bayesian rule lists,\nwhile providing more compact and meaningful explanations.\n",
        "published": "2021",
        "authors": [
            "Gabriele Ciravegna",
            "Pietro Barbiero",
            "Francesco Giannini",
            "Marco Gori",
            "Pietro Li\u00f3",
            "Marco Maggini",
            "Stefano Melacci"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.08846v1",
        "title": "Personalized next-best action recommendation with multi-party\n  interaction learning for automated decision-making",
        "abstract": "  Automated next-best action recommendation for each customer in a sequential,\ndynamic and interactive context has been widely needed in natural, social and\nbusiness decision-making. Personalized next-best action recommendation must\ninvolve past, current and future customer demographics and circumstances\n(states) and behaviors, long-range sequential interactions between customers\nand decision-makers, multi-sequence interactions between states, behaviors and\nactions, and their reactions to their counterpart's actions. No existing\nmodeling theories and tools, including Markovian decision processes, user and\nbehavior modeling, deep sequential modeling, and personalized sequential\nrecommendation, can quantify such complex decision-making on a personal level.\nWe take a data-driven approach to learn the next-best actions for personalized\ndecision-making by a reinforced coupled recurrent neural network (CRN). CRN\nrepresents multiple coupled dynamic sequences of a customer's historical and\ncurrent states, responses to decision-makers' actions, decision rewards to\nactions, and learns long-term multi-sequence interactions between parties\n(customer and decision-maker). Next-best actions are then recommended on each\ncustomer at a time point to change their state for an optimal decision-making\nobjective. Our study demonstrates the potential of personalized deep learning\nof multi-sequence interactions and automated dynamic intervention for\npersonalized decision-making in complex systems.\n",
        "published": "2021",
        "authors": [
            "Longbing Cao",
            "Chengzhang Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.09659v1",
        "title": "Evolutionary Ensemble Learning for Multivariate Time Series Prediction",
        "abstract": "  Multivariate time series (MTS) prediction plays a key role in many fields\nsuch as finance, energy and transport, where each individual time series\ncorresponds to the data collected from a certain data source, so-called\nchannel. A typical pipeline of building an MTS prediction model (PM) consists\nof selecting a subset of channels among all available ones, extracting features\nfrom the selected channels, and building a PM based on the extracted features,\nwhere each component involves certain optimization tasks, i.e., selection of\nchannels, feature extraction (FE) methods, and PMs as well as configuration of\nthe selected FE method and PM. Accordingly, pursuing the best prediction\nperformance corresponds to optimizing the pipeline by solving all of its\ninvolved optimization problems. This is a non-trivial task due to the vastness\nof the solution space. Different from most of the existing works which target\nat optimizing certain components of the pipeline, we propose a novel\nevolutionary ensemble learning framework to optimize the entire pipeline in a\nholistic manner. In this framework, a specific pipeline is encoded as a\ncandidate solution and a multi-objective evolutionary algorithm is applied\nunder different population sizes to produce multiple Pareto optimal sets\n(POSs). Finally, selective ensemble learning is designed to choose the optimal\nsubset of solutions from the POSs and combine them to yield final prediction by\nusing greedy sequential selection and least square methods. We implement the\nproposed framework and evaluate our implementation on two real-world\napplications, i.e., electricity consumption prediction and air quality\nprediction. The performance comparison with state-of-the-art techniques\ndemonstrates the superiority of the proposed approach.\n",
        "published": "2021",
        "authors": [
            "Hui Song",
            "A. K. Qin",
            "Flora D. Salim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.09817v1",
        "title": "Electroencephalogram Signal Processing with Independent Component\n  Analysis and Cognitive Stress Classification using Convolutional Neural\n  Networks",
        "abstract": "  Electroencephalogram (EEG) is the recording which is the result due to the\nactivity of bio-electrical signals that is acquired from electrodes placed on\nthe scalp. In Electroencephalogram signal(EEG) recordings, the signals obtained\nare contaminated predominantly by the Electrooculogram(EOG) signal. Since this\nartifact has higher magnitude compared to EEG signals, these noise signals have\nto be removed in order to have a better understanding regarding the functioning\nof a human brain for applications such as medical diagnosis. This paper\nproposes an idea of using Independent Component Analysis(ICA) along with\ncross-correlation to de-noise EEG signal. This is done by selecting the\ncomponent based on the cross-correlation coefficient with a threshold value and\nreducing its effect instead of zeroing it out completely, thus reducing the\ninformation loss. The results of the recorded data show that this algorithm can\neliminate the EOG signal artifact with little loss in EEG data. The denoising\nis verified by an increase in SNR value and the decrease in cross-correlation\ncoefficient value. The denoised signals are used to train an Artificial Neural\nNetwork(ANN) which would examine the features of the input EEG signal and\npredict the stress levels of the individual.\n",
        "published": "2021",
        "authors": [
            "Venkatakrishnan Sutharsan",
            "Alagappan Swaminathan",
            "Saisrinivasan Ramachandran",
            "Madan Kumar Lakshmanan",
            "Balaji Mahadevan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.09914v1",
        "title": "Genetic Programming for Manifold Learning: Preserving Local Topology",
        "abstract": "  Manifold learning methods are an invaluable tool in today's world of\nincreasingly huge datasets. Manifold learning algorithms can discover a much\nlower-dimensional representation (embedding) of a high-dimensional dataset\nthrough non-linear transformations that preserve the most important structure\nof the original data. State-of-the-art manifold learning methods directly\noptimise an embedding without mapping between the original space and the\ndiscovered embedded space. This makes interpretability - a key requirement in\nexploratory data analysis - nearly impossible. Recently, genetic programming\nhas emerged as a very promising approach to manifold learning by evolving\nfunctional mappings from the original space to an embedding. However, genetic\nprogramming-based manifold learning has struggled to match the performance of\nother approaches. In this work, we propose a new approach to using genetic\nprogramming for manifold learning, which preserves local topology. This is\nexpected to significantly improve performance on tasks where local\nneighbourhood structure (topology) is paramount. We compare our proposed\napproach with various baseline manifold learning methods and find that it often\noutperforms other methods, including a clear improvement over previous genetic\nprogramming approaches. These results are particularly promising, given the\npotential interpretability and reusability of the evolved mappings.\n",
        "published": "2021",
        "authors": [
            "Andrew Lensen",
            "Bing Xue",
            "Mengjie Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.10078v1",
        "title": "Distilling Neuron Spike with High Temperature in Reinforcement Learning\n  Agents",
        "abstract": "  Spiking neural network (SNN), compared with depth neural network (DNN), has\nfaster processing speed, lower energy consumption and more biological\ninterpretability, which is expected to approach Strong AI. Reinforcement\nlearning is similar to learning in biology. It is of great significance to\nstudy the combination of SNN and RL. We propose the reinforcement learning\nmethod of spike distillation network (SDN) with STBP. This method uses\ndistillation to effectively avoid the weakness of STBP, which can achieve SOTA\nperformance in classification, and can obtain a smaller, faster convergence and\nlower power consumption SNN reinforcement learning model. Experiments show that\nour method can converge faster than traditional SNN reinforcement learning and\nDNN reinforcement learning methods, about 1000 epochs faster, and obtain SNN\n200 times smaller than DNN. We also deploy SDN to the PKU nc64c chip, which\nproves that SDN has lower power consumption than DNN, and the power consumption\nof SDN is more than 600 times lower than DNN on large-scale devices. SDN\nprovides a new way of SNN reinforcement learning, and can achieve SOTA\nperformance, which proves the possibility of further development of SNN\nreinforcement learning.\n",
        "published": "2021",
        "authors": [
            "Ling Zhang",
            "Jian Cao",
            "Yuan Zhang",
            "Bohan Zhou",
            "Shuo Feng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.12284v4",
        "title": "The Devil is in the Detail: Simple Tricks Improve Systematic\n  Generalization of Transformers",
        "abstract": "  Recently, many datasets have been proposed to test the systematic\ngeneralization ability of neural networks. The companion baseline Transformers,\ntypically trained with default hyper-parameters from standard tasks, are shown\nto fail dramatically. Here we demonstrate that by revisiting model\nconfigurations as basic as scaling of embeddings, early stopping, relative\npositional embedding, and Universal Transformer variants, we can drastically\nimprove the performance of Transformers on systematic generalization. We report\nimprovements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics\ndataset. Our models improve accuracy from 50% to 85% on the PCFG productivity\nsplit, and from 35% to 81% on COGS. On SCAN, relative positional embedding\nlargely mitigates the EOS decision problem (Newman et al., 2020), yielding 100%\naccuracy on the length split with a cutoff at 26. Importantly, performance\ndifferences between these models are typically invisible on the IID data split.\nThis calls for proper generalization validation sets for developing neural\nnetworks that generalize systematically. We publicly release the code to\nreproduce our results.\n",
        "published": "2021",
        "authors": [
            "R\u00f3bert Csord\u00e1s",
            "Kazuki Irie",
            "J\u00fcrgen Schmidhuber"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.01394v2",
        "title": "Topographic VAEs learn Equivariant Capsules",
        "abstract": "  In this work we seek to bridge the concepts of topographic organization and\nequivariance in neural networks. To accomplish this, we introduce the\nTopographic VAE: a novel method for efficiently training deep generative models\nwith topographically organized latent variables. We show that such a model\nindeed learns to organize its activations according to salient characteristics\nsuch as digit class, width, and style on MNIST. Furthermore, through\ntopographic organization over time (i.e. temporal coherence), we demonstrate\nhow predefined latent space transformation operators can be encouraged for\nobserved transformed input sequences -- a primitive form of unsupervised\nlearned equivariance. We demonstrate that this model successfully learns sets\nof approximately equivariant features (i.e. \"capsules\") directly from sequences\nand achieves higher likelihood on correspondingly transforming test sequences.\nEquivariance is verified quantitatively by measuring the approximate\ncommutativity of the inference network and the sequence transformations.\nFinally, we demonstrate approximate equivariance to complex transformations,\nexpanding upon the capabilities of existing group equivariant neural networks.\n",
        "published": "2021",
        "authors": [
            "T. Anderson Keller",
            "Max Welling"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.02157v2",
        "title": "Learning with Holographic Reduced Representations",
        "abstract": "  Holographic Reduced Representations (HRR) are a method for performing\nsymbolic AI on top of real-valued vectors by associating each vector with an\nabstract concept, and providing mathematical operations to manipulate vectors\nas if they were classic symbolic objects. This method has seen little use\noutside of older symbolic AI work and cognitive science. Our goal is to revisit\nthis approach to understand if it is viable for enabling a hybrid\nneural-symbolic approach to learning as a differentiable component of a deep\nlearning architecture. HRRs today are not effective in a differentiable\nsolution due to numerical instability, a problem we solve by introducing a\nprojection step that forces the vectors to exist in a well behaved point in\nspace. In doing so we improve the concept retrieval efficacy of HRRs by over\n$100\\times$. Using multi-label classification we demonstrate how to leverage\nthe symbolic HRR properties to develop an output layer and loss function that\nis able to learn effectively, and allows us to investigate some of the pros and\ncons of an HRR neuro-symbolic learning approach. Our code can be found at\nhttps://github.com/NeuromorphicComputationResearchProgram/Learning-with-Holographic-Reduced-Representations\n",
        "published": "2021",
        "authors": [
            "Ashwinkumar Ganesan",
            "Hang Gao",
            "Sunil Gandhi",
            "Edward Raff",
            "Tim Oates",
            "James Holt",
            "Mark McLean"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.06133v1",
        "title": "Neuro-Symbolic AI: An Emerging Class of AI Workloads and their\n  Characterization",
        "abstract": "  Neuro-symbolic artificial intelligence is a novel area of AI research which\nseeks to combine traditional rules-based AI approaches with modern deep\nlearning techniques. Neuro-symbolic models have already demonstrated the\ncapability to outperform state-of-the-art deep learning models in domains such\nas image and video reasoning. They have also been shown to obtain high accuracy\nwith significantly less training data than traditional models. Due to the\nrecency of the field's emergence and relative sparsity of published results,\nthe performance characteristics of these models are not well understood. In\nthis paper, we describe and analyze the performance characteristics of three\nrecent neuro-symbolic models. We find that symbolic models have less potential\nparallelism than traditional neural models due to complex control flow and\nlow-operational-intensity operations, such as scalar multiplication and tensor\naddition. However, the neural aspect of computation dominates the symbolic part\nin cases where they are clearly separable. We also find that data movement\nposes a potential bottleneck, as it does in many ML workloads.\n",
        "published": "2021",
        "authors": [
            "Zachary Susskind",
            "Bryce Arden",
            "Lizy K. John",
            "Patrick Stockton",
            "Eugene B. John"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.06139v1",
        "title": "Application of Machine Learning in Early Recommendation of Cardiac\n  Resynchronization Therapy",
        "abstract": "  Heart failure (HF) is a leading cause of morbidity, mortality, and health\ncare costs. Prolonged conduction through the myocardium can occur with HF, and\na device-driven approach, termed cardiac resynchronization therapy (CRT), can\nimprove left ventricular (LV) myocardial conduction patterns. While a\nfunctional benefit of CRT has been demonstrated, a large proportion of HF\npatients (30-50%) receiving CRT do not show sufficient improvement. Moreover,\nidentifying HF patients that would benefit from CRT prospectively remains a\nclinical challenge. Accordingly, strategies to effectively predict those HF\npatients that would derive a functional benefit from CRT holds great medical\nand socio-economic importance. Thus, we used machine learning methods of\nclassifying HF patients, namely Cluster Analysis, Decision Trees, and\nArtificial neural networks, to develop predictive models of individual outcomes\nfollowing CRT. Clinical, functional, and biomarker data were collected in HF\npatients before and following CRT. A prospective 6-month endpoint of a\nreduction in LV volume was defined as a CRT response. Using this approach (418\nresponders, 412 non-responders), each with 56 parameters, we could classify HF\npatients based on their response to CRT with more than 95% success. We have\ndemonstrated that using machine learning approaches can identify HF patients\nwith a high probability of a positive CRT response (95% accuracy), and of equal\nimportance, identify those HF patients that would not derive a functional\nbenefit from CRT. Developing this approach into a clinical algorithm to assist\nin clinical decision-making regarding the use of CRT in HF patients would\npotentially improve outcomes and reduce health care costs.\n",
        "published": "2021",
        "authors": [
            "Brendan E. Odigwe",
            "Francis G. Spinale",
            "Homayoun Valafar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.06826v2",
        "title": "Few-shot Quality-Diversity Optimization",
        "abstract": "  In the past few years, a considerable amount of research has been dedicated\nto the exploitation of previous learning experiences and the design of Few-shot\nand Meta Learning approaches, in problem domains ranging from Computer Vision\nto Reinforcement Learning based control. A notable exception, where to the best\nof our knowledge, little to no effort has been made in this direction is\nQuality-Diversity (QD) optimization. QD methods have been shown to be effective\ntools in dealing with deceptive minima and sparse rewards in Reinforcement\nLearning. However, they remain costly due to their reliance on inherently\nsample inefficient evolutionary processes. We show that, given examples from a\ntask distribution, information about the paths taken by optimization in\nparameter space can be leveraged to build a prior population, which when used\nto initialize QD methods in unseen environments, allows for few-shot\nadaptation. Our proposed method does not require backpropagation. It is simple\nto implement and scale, and furthermore, it is agnostic to the underlying\nmodels that are being trained. Experiments carried in both sparse and dense\nreward settings using robotic manipulation and navigation benchmarks show that\nit considerably reduces the number of generations that are required for QD\noptimization in these environments.\n",
        "published": "2021",
        "authors": [
            "Achkan Salehi",
            "Alexandre Coninx",
            "Stephane Doncieux"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.10376v1",
        "title": "Learning through structure: towards deep neuromorphic knowledge graph\n  embeddings",
        "abstract": "  Computing latent representations for graph-structured data is an ubiquitous\nlearning task in many industrial and academic applications ranging from\nmolecule synthetization to social network analysis and recommender systems.\nKnowledge graphs are among the most popular and widely used data\nrepresentations related to the Semantic Web. Next to structuring factual\nknowledge in a machine-readable format, knowledge graphs serve as the backbone\nof many artificial intelligence applications and allow the ingestion of context\ninformation into various learning algorithms. Graph neural networks attempt to\nencode graph structures in low-dimensional vector spaces via a message passing\nheuristic between neighboring nodes. Over the recent years, a multitude of\ndifferent graph neural network architectures demonstrated ground-breaking\nperformances in many learning tasks. In this work, we propose a strategy to map\ndeep graph learning architectures for knowledge graph reasoning to neuromorphic\narchitectures. Based on the insight that randomly initialized and untrained\n(i.e., frozen) graph neural networks are able to preserve local graph\nstructures, we compose a frozen neural network with shallow knowledge graph\nembedding models. We experimentally show that already on conventional computing\nhardware, this leads to a significant speedup and memory reduction while\nmaintaining a competitive performance level. Moreover, we extend the frozen\narchitecture to spiking neural networks, introducing a novel, event-based and\nhighly sparse knowledge graph embedding algorithm that is suitable for\nimplementation in neuromorphic hardware.\n",
        "published": "2021",
        "authors": [
            "Victor Caceres Chian",
            "Marcel Hildebrandt",
            "Thomas Runkler",
            "Dominik Dold"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.10617v2",
        "title": "Solving Large Steiner Tree Problems in Graphs for Cost-Efficient\n  Fiber-To-The-Home Network Expansion",
        "abstract": "  The expansion of Fiber-To-The-Home (FTTH) networks creates high costs due to\nexpensive excavation procedures. Optimizing the planning process and minimizing\nthe cost of the earth excavation work therefore lead to large savings.\nMathematically, the FTTH network problem can be described as a minimum Steiner\nTree problem. Even though the Steiner Tree problem has already been\ninvestigated intensively in the last decades, it might be further optimized\nwith the help of new computing paradigms and emerging approaches. This work\nstudies upcoming technologies, such as Quantum Annealing, Simulated Annealing\nand nature-inspired methods like Evolutionary Algorithms or slime-mold-based\noptimization. Additionally, we investigate partitioning and simplifying\nmethods. Evaluated on several real-life problem instances, we could outperform\na traditional, widely-used baseline (NetworkX Approximate Solver) on most of\nthe domains. Prior partitioning of the initial graph and the presented\nslime-mold-based approach were especially valuable for a cost-efficient\napproximation. Quantum Annealing seems promising, but was limited by the number\nof available qubits.\n",
        "published": "2021",
        "authors": [
            "Tobias M\u00fcller",
            "Kyrill Schmid",
            "Dani\u00eblle Schuman",
            "Thomas Gabor",
            "Markus Friedrich",
            "Marc Geitz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.13297v1",
        "title": "GANG-MAM: GAN based enGine for Modifying Android Malware",
        "abstract": "  Malware detectors based on machine learning are vulnerable to adversarial\nattacks. Generative Adversarial Networks (GAN) are architectures based on\nNeural Networks that could produce successful adversarial samples. The interest\ntowards this technology is quickly growing. In this paper, we propose a system\nthat produces a feature vector for making an Android malware strongly evasive\nand then modify the malicious program accordingly. Such a system could have a\ntwofold contribution: it could be used to generate datasets to validate systems\nfor detecting GAN-based malware and to enlarge the training and testing dataset\nfor making more robust malware classifiers.\n",
        "published": "2021",
        "authors": [
            "Renjith G",
            "Sonia Laudanna",
            "Aji S",
            "Corrado Aaron Visaggio",
            "Vinod P"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.14155v1",
        "title": "Customs Fraud Detection in the Presence of Concept Drift",
        "abstract": "  Capturing the changing trade pattern is critical in customs fraud detection.\nAs new goods are imported and novel frauds arise, a drift-aware fraud detection\nsystem is needed to detect both known frauds and unknown frauds within a\nlimited budget. The current paper proposes ADAPT, an adaptive selection method\nthat controls the balance between exploitation and exploration strategies used\nfor customs fraud detection. ADAPT makes use of the model performance trends\nand the amount of concept drift to determine the best exploration ratio at\nevery time. Experiments on data from four countries over several years show\nthat each country requires a different amount of exploration for maintaining\nits fraud detection system. We find the system with ADAPT can gradually adapt\nto the dataset and find the appropriate amount of exploration ratio with high\nperformance.\n",
        "published": "2021",
        "authors": [
            "Tung-Duong Mai",
            "Kien Hoang",
            "Aitolkyn Baigutanova",
            "Gaukhartas Alina",
            "Sundong Kim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.00367v1",
        "title": "Multi Expression Programming -- an in-depth description",
        "abstract": "  Multi Expression Programming (MEP) is a Genetic Programming variant that uses\na linear representation of chromosomes. MEP individuals are strings of genes\nencoding complex computer programs. When MEP individuals encode expressions,\ntheir representation is similar to the way in which compilers translate $C$ or\n$Pascal$ expressions into machine code. A unique MEP feature is the ability to\nstore multiple solutions of a problem in a single chromosome. Usually, the best\nsolution is chosen for fitness assignment. When solving symbolic regression or\nclassification problems (or any other problems for which the training set is\nknown before the problem is solved) MEP has the same complexity as other\ntechniques storing a single solution in a chromosome (such as GP, CGP, GEP or\nGE). Evaluation of the expressions encoded into an MEP individual can be\nperformed by a single parsing of the chromosome. Offspring obtained by\ncrossover and mutation is always syntactically correct MEP individuals\n(computer programs). Thus, no extra processing for repairing newly obtained\nindividuals is needed.\n",
        "published": "2021",
        "authors": [
            "Mihai Oltean"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.00385v2",
        "title": "Neural Dependency Coding inspired Multimodal Fusion",
        "abstract": "  Information integration from different modalities is an active area of\nresearch. Human beings and, in general, biological neural systems are quite\nadept at using a multitude of signals from different sensory perceptive fields\nto interact with the environment and each other. Recent work in deep fusion\nmodels via neural networks has led to substantial improvements over unimodal\napproaches in areas like speech recognition, emotion recognition and analysis,\ncaptioning and image description. However, such research has mostly focused on\narchitectural changes allowing for fusion of different modalities while keeping\nthe model complexity manageable. Inspired by recent neuroscience ideas about\nmultisensory integration and processing, we investigate the effect of synergy\nmaximizing loss functions. Experiments on multimodal sentiment analysis tasks:\nCMU-MOSI and CMU-MOSEI with different models show that our approach provides a\nconsistent performance boost.\n",
        "published": "2021",
        "authors": [
            "Shiv Shankar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.00468v1",
        "title": "New Evolutionary Computation Models and their Applications to Machine\n  Learning",
        "abstract": "  Automatic Programming is one of the most important areas of computer science\nresearch today. Hardware speed and capability have increased exponentially, but\nthe software is years behind. The demand for software has also increased\nsignificantly, but it is still written in old fashion: by using humans.\n  There are multiple problems when the work is done by humans: cost, time,\nquality. It is costly to pay humans, it is hard to keep them satisfied for a\nlong time, it takes a lot of time to teach and train them and the quality of\ntheir output is in most cases low (in software, mostly due to bugs).\n  The real advances in human civilization appeared during the industrial\nrevolutions. Before the first revolution, most people worked in agriculture.\nToday, very few percent of people work in this field.\n  A similar revolution must appear in the computer programming field.\nOtherwise, we will have so many people working in this field as we had in the\npast working in agriculture.\n  How do people know how to write computer programs? Very simple: by learning.\nCan we do the same for software? Can we put the software to learn how to write\nsoftware?\n  It seems that is possible (to some degree) and the term is called Machine\nLearning. It was first coined in 1959 by the first person who made a computer\nperform a serious learning task, namely, Arthur Samuel.\n  However, things are not so easy as in humans (well, truth to be said - for\nsome humans it is impossible to learn how to write software). So far we do not\nhave software that can learn perfectly to write software. We have some\nparticular cases where some programs do better than humans, but the examples\nare sporadic at best. Learning from experience is difficult for computer\nprograms. Instead of trying to simulate how humans teach humans how to write\ncomputer programs, we can simulate nature.\n",
        "published": "2021",
        "authors": [
            "Mihai Oltean"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.01470v1",
        "title": "Implementation of Parallel Simplified Swarm Optimization in CUDA",
        "abstract": "  As the acquisition cost of the graphics processing unit (GPU) has decreased,\npersonal computers (PC) can handle optimization problems nowadays. In\noptimization computing, intelligent swarm algorithms (SIAs) method is suitable\nfor parallelization. However, a GPU-based Simplified Swarm Optimization\nAlgorithm has never been proposed. Accordingly, this paper proposed Parallel\nSimplified Swarm Optimization (PSSO) based on the CUDA platform considering\ncomputational ability and versatility. In PSSO, the theoretical value of time\ncomplexity of fitness function is O (tNm). There are t iterations and N fitness\nfunctions, each of which required pair comparisons m times. pBests and gBest\nhave the resource preemption when updating in previous studies. As the\nexperiment results showed, the time complexity has successfully reduced by an\norder of magnitude of N, and the problem of resource preemption was avoided\nentirely.\n",
        "published": "2021",
        "authors": [
            "Wei-Chang Yeh",
            "Zhenyao Liu",
            "Shi-Yi Tan",
            "Shang-Ke Huang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.01765v1",
        "title": "Rapid training of deep neural networks without skip connections or\n  normalization layers using Deep Kernel Shaping",
        "abstract": "  Using an extended and formalized version of the Q/C map analysis of Poole et\nal. (2016), along with Neural Tangent Kernel theory, we identify the main\npathologies present in deep networks that prevent them from training fast and\ngeneralizing to unseen data, and show how these can be avoided by carefully\ncontrolling the \"shape\" of the network's initialization-time kernel function.\nWe then develop a method called Deep Kernel Shaping (DKS), which accomplishes\nthis using a combination of precise parameter initialization, activation\nfunction transformations, and small architectural tweaks, all of which preserve\nthe model class. In our experiments we show that DKS enables SGD training of\nresidual networks without normalization layers on Imagenet and CIFAR-10\nclassification tasks at speeds comparable to standard ResNetV2 and Wide-ResNet\nmodels, with only a small decrease in generalization performance. And when\nusing K-FAC as the optimizer, we achieve similar results for networks without\nskip connections. Our results apply for a large variety of activation\nfunctions, including those which traditionally perform very badly, such as the\nlogistic sigmoid. In addition to DKS, we contribute a detailed analysis of skip\nconnections, normalization layers, special activation functions like RELU and\nSELU, and various initialization schemes, explaining their effectiveness as\nalternative (and ultimately incomplete) ways of \"shaping\" the network's\ninitialization-time kernel.\n",
        "published": "2021",
        "authors": [
            "James Martens",
            "Andy Ballard",
            "Guillaume Desjardins",
            "Grzegorz Swirszcz",
            "Valentin Dalibard",
            "Jascha Sohl-Dickstein",
            "Samuel S. Schoenholz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.02775v1",
        "title": "NEWRON: A New Generalization of the Artificial Neuron to Enhance the\n  Interpretability of Neural Networks",
        "abstract": "  In this work, we formulate NEWRON: a generalization of the McCulloch-Pitts\nneuron structure. This new framework aims to explore additional desirable\nproperties of artificial neurons. We show that some specializations of NEWRON\nallow the network to be interpretable with no change in their expressiveness.\nBy just inspecting the models produced by our NEWRON-based networks, we can\nunderstand the rules governing the task. Extensive experiments show that the\nquality of the generated models is better than traditional interpretable models\nand in line or better than standard neural networks.\n",
        "published": "2021",
        "authors": [
            "Federico Siciliano",
            "Maria Sofia Bucarelli",
            "Gabriele Tolomei",
            "Fabrizio Silvestri"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.03171v3",
        "title": "Assemblies of neurons learn to classify well-separated distributions",
        "abstract": "  An assembly is a large population of neurons whose synchronous firing is\nhypothesized to represent a memory, concept, word, and other cognitive\ncategories. Assemblies are believed to provide a bridge between high-level\ncognitive phenomena and low-level neural activity. Recently, a computational\nsystem called the Assembly Calculus (AC), with a repertoire of biologically\nplausible operations on assemblies, has been shown capable of simulating\narbitrary space-bounded computation, but also of simulating complex cognitive\nphenomena such as language, reasoning, and planning. However, the mechanism\nwhereby assemblies can mediate learning has not been known. Here we present\nsuch a mechanism, and prove rigorously that, for simple classification problems\ndefined on distributions of labeled assemblies, a new assembly representing\neach class can be reliably formed in response to a few stimuli from the class;\nthis assembly is henceforth reliably recalled in response to new stimuli from\nthe same class. Furthermore, such class assemblies will be distinguishable as\nlong as the respective classes are reasonably separated -- for example, when\nthey are clusters of similar assemblies. To prove these results, we draw on\nrandom graph theory with dynamic edge weights to estimate sequences of\nactivated vertices, yielding strong generalizations of previous calculations\nand theorems in this field over the past five years. These theorems are backed\nup by experiments demonstrating the successful formation of assemblies which\nrepresent concept classes on synthetic data drawn from such distributions, and\nalso on MNIST, which lends itself to classification through one assembly per\ndigit. Seen as a learning algorithm, this mechanism is entirely online,\ngeneralizes from very few samples, and requires only mild supervision -- all\nkey attributes of learning in a model of the brain.\n",
        "published": "2021",
        "authors": [
            "Max Dabagia",
            "Christos H. Papadimitriou",
            "Santosh S. Vempala"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.03431v2",
        "title": "Cloud Failure Prediction with Hierarchical Temporal Memory: An Empirical\n  Assessment",
        "abstract": "  Hierarchical Temporal Memory (HTM) is an unsupervised learning algorithm\ninspired by the features of the neocortex that can be used to continuously\nprocess stream data and detect anomalies, without requiring a large amount of\ndata for training nor requiring labeled data. HTM is also able to continuously\nlearn from samples, providing a model that is always up-to-date with respect to\nobservations. These characteristics make HTM particularly suitable for\nsupporting online failure prediction in cloud systems, which are systems with a\ndynamically changing behavior that must be monitored to anticipate problems.\nThis paper presents the first systematic study that assesses HTM in the context\nof failure prediction. The results that we obtained considering 72\nconfigurations of HTM applied to 12 different types of faults introduced in the\nClearwater cloud system show that HTM can help to predict failures with\nsufficient effectiveness (F-measure = 0.76), representing an interesting\npractical alternative to (semi-)supervised algorithms.\n",
        "published": "2021",
        "authors": [
            "Oliviero Riganelli",
            "Paolo Saltarel",
            "Alessandro Tundo",
            "Marco Mobilio",
            "Leonardo Mariani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.07732v4",
        "title": "The Neural Data Router: Adaptive Control Flow in Transformers Improves\n  Systematic Generalization",
        "abstract": "  Despite progress across a broad range of applications, Transformers have\nlimited success in systematic generalization. The situation is especially\nfrustrating in the case of algorithmic tasks, where they often fail to find\nintuitive solutions that route relevant information to the right node/operation\nat the right time in the grid represented by Transformer columns. To facilitate\nthe learning of useful control flow, we propose two modifications to the\nTransformer architecture, copy gate and geometric attention. Our novel Neural\nData Router (NDR) achieves 100% length generalization accuracy on the classic\ncompositional table lookup task, as well as near-perfect accuracy on the simple\narithmetic task and a new variant of ListOps testing for generalization across\ncomputational depths. NDR's attention and gating patterns tend to be\ninterpretable as an intuitive form of neural routing. Our code is public.\n",
        "published": "2021",
        "authors": [
            "R\u00f3bert Csord\u00e1s",
            "Kazuki Irie",
            "J\u00fcrgen Schmidhuber"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.08058v2",
        "title": "Quantifying Local Specialization in Deep Neural Networks",
        "abstract": "  A neural network is locally specialized to the extent that parts of its\ncomputational graph (i.e. structure) can be abstractly represented as\nperforming some comprehensible sub-task relevant to the overall task (i.e.\nfunctionality). Are modern deep neural networks locally specialized? How can\nthis be quantified? In this paper, we consider the problem of taking a neural\nnetwork whose neurons are partitioned into clusters, and quantifying how\nfunctionally specialized the clusters are. We propose two proxies for this:\nimportance, which reflects how crucial sets of neurons are to network\nperformance; and coherence, which reflects how consistently their neurons\nassociate with features of the inputs. To measure these proxies, we develop a\nset of statistical methods based on techniques conventionally used to interpret\nindividual neurons. We apply the proxies to partitionings generated by\nspectrally clustering a graph representation of the network's neurons with\nedges determined either by network weights or correlations of activations. We\nshow that these partitionings, even ones based only on weights (i.e. strictly\nfrom non-runtime analysis), reveal groups of neurons that are important and\ncoherent. These results suggest that graph-based partitioning can reveal local\nspecialization and that statistical methods can be used to automatedly screen\nfor sets of neurons that can be understood abstractly.\n",
        "published": "2021",
        "authors": [
            "Shlomi Hod",
            "Daniel Filan",
            "Stephen Casper",
            "Andrew Critch",
            "Stuart Russell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.08771v1",
        "title": "An LSTM-based Plagiarism Detection via Attention Mechanism and a\n  Population-based Approach for Pre-Training Parameters with imbalanced Classes",
        "abstract": "  Plagiarism is one of the leading problems in academic and industrial\nenvironments, which its goal is to find the similar items in a typical document\nor source code. This paper proposes an architecture based on a Long Short-Term\nMemory (LSTM) and attention mechanism called LSTM-AM-ABC boosted by a\npopulation-based approach for parameter initialization. Gradient-based\noptimization algorithms such as back-propagation (BP) are widely used in the\nliterature for learning process in LSTM, attention mechanism, and feed-forward\nneural network, while they suffer from some problems such as getting stuck in\nlocal optima. To tackle this problem, population-based metaheuristic (PBMH)\nalgorithms can be used. To this end, this paper employs a PBMH algorithm,\nartificial bee colony (ABC), to moderate the problem. Our proposed algorithm\ncan find the initial values for model learning in all LSTM, attention\nmechanism, and feed-forward neural network, simultaneously. In other words, ABC\nalgorithm finds a promising point for starting BP algorithm. For evaluation, we\ncompare our proposed algorithm with both conventional and population-based\nmethods. The results clearly show that the proposed method can provide\ncompetitive performance.\n",
        "published": "2021",
        "authors": [
            "Seyed Vahid Moravvej",
            "Seyed Jalaleddin Mousavirad",
            "Mahshid Helali Moghadam",
            "Mehrdad Saadatmand"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.15056v2",
        "title": "Brain-inspired feature exaggeration in generative replay for continual\n  learning",
        "abstract": "  The catastrophic forgetting of previously learnt classes is one of the main\nobstacles to the successful development of a reliable and accurate generative\ncontinual learning model. When learning new classes, the internal\nrepresentation of previously learnt ones can often be overwritten, resulting in\nthe model's \"memory\" of earlier classes being lost over time. Recent\ndevelopments in neuroscience have uncovered a method through which the brain\navoids its own form of memory interference. Applying a targeted exaggeration of\nthe differences between features of similar, yet competing memories, the brain\ncan more easily distinguish and recall them. In this paper, the application of\nsuch exaggeration, via the repulsion of replayed samples belonging to competing\nclasses, is explored. Through the development of a 'reconstruction repulsion'\nloss, this paper presents a new state-of-the-art performance on the\nclassification of early classes in the class-incremental learning dataset\nCIFAR100.\n",
        "published": "2021",
        "authors": [
            "Jack Millichamp",
            "Xi Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.00053v2",
        "title": "Symbolic Regression via Neural-Guided Genetic Programming Population\n  Seeding",
        "abstract": "  Symbolic regression is the process of identifying mathematical expressions\nthat fit observed output from a black-box process. It is a discrete\noptimization problem generally believed to be NP-hard. Prior approaches to\nsolving the problem include neural-guided search (e.g. using reinforcement\nlearning) and genetic programming. In this work, we introduce a hybrid\nneural-guided/genetic programming approach to symbolic regression and other\ncombinatorial optimization problems. We propose a neural-guided component used\nto seed the starting population of a random restart genetic programming\ncomponent, gradually learning better starting populations. On a number of\ncommon benchmark tasks to recover underlying expressions from a dataset, our\nmethod recovers 65% more expressions than a recently published top-performing\nmodel using the same experimental setup. We demonstrate that running many\ngenetic programming generations without interdependence on the neural-guided\ncomponent performs better for symbolic regression than alternative formulations\nwhere the two are more strongly coupled. Finally, we introduce a new set of 22\nsymbolic regression benchmark problems with increased difficulty over existing\nbenchmarks. Source code is provided at\nwww.github.com/brendenpetersen/deep-symbolic-optimization.\n",
        "published": "2021",
        "authors": [
            "T. Nathan Mundhenk",
            "Mikel Landajuela",
            "Ruben Glatt",
            "Claudio P. Santiago",
            "Daniel M. Faissol",
            "Brenden K. Petersen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.00134v3",
        "title": "Context Meta-Reinforcement Learning via Neuromodulation",
        "abstract": "  Meta-reinforcement learning (meta-RL) algorithms enable agents to adapt\nquickly to tasks from few samples in dynamic environments. Such a feat is\nachieved through dynamic representations in an agent's policy network (obtained\nvia reasoning about task context, model parameter updates, or both). However,\nobtaining rich dynamic representations for fast adaptation beyond simple\nbenchmark problems is challenging due to the burden placed on the policy\nnetwork to accommodate different policies. This paper addresses the challenge\nby introducing neuromodulation as a modular component to augment a standard\npolicy network that regulates neuronal activities in order to produce efficient\ndynamic representations for task adaptation. The proposed extension to the\npolicy network is evaluated across multiple discrete and continuous control\nenvironments of increasing complexity. To prove the generality and benefits of\nthe extension in meta-RL, the neuromodulated network was applied to two\nstate-of-the-art meta-RL algorithms (CAVIA and PEARL). The result demonstrates\nthat meta-RL augmented with neuromodulation produces significantly better\nresult and richer dynamic representations in comparison to the baselines.\n",
        "published": "2021",
        "authors": [
            "Eseoghene Ben-Iwhiwhu",
            "Jeffery Dick",
            "Nicholas A. Ketz",
            "Praveen K. Pilly",
            "Andrea Soltoggio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.01456v1",
        "title": "WaveSense: Efficient Temporal Convolutions with Spiking Neural Networks\n  for Keyword Spotting",
        "abstract": "  Ultra-low power local signal processing is a crucial aspect for edge\napplications on always-on devices. Neuromorphic processors emulating spiking\nneural networks show great computational power while fulfilling the limited\npower budget as needed in this domain. In this work we propose spiking neural\ndynamics as a natural alternative to dilated temporal convolutions. We extend\nthis idea to WaveSense, a spiking neural network inspired by the WaveNet\narchitecture. WaveSense uses simple neural dynamics, fixed time-constants and a\nsimple feed-forward architecture and hence is particularly well suited for a\nneuromorphic implementation. We test the capabilities of this model on several\ndatasets for keyword-spotting. The results show that the proposed network beats\nthe state of the art of other spiking neural networks and reaches near\nstate-of-the-art performance of artificial neural networks such as CNNs and\nLSTMs.\n",
        "published": "2021",
        "authors": [
            "Philipp Weidel",
            "Sadique Sheik"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.02557v1",
        "title": "A Meta-Learned Neuron model for Continual Learning",
        "abstract": "  Continual learning is the ability to acquire new knowledge without forgetting\nthe previously learned one, assuming no further access to past training data.\nNeural network approximators trained with gradient descent are known to fail in\nthis setting as they must learn from a stream of data-points sampled from a\nstationary distribution to converge. In this work, we replace the standard\nneuron by a meta-learned neuron model whom inference and update rules are\noptimized to minimize catastrophic interference. Our approach can memorize\ndataset-length sequences of training samples, and its learning capabilities\ngeneralize to any domain. Unlike previous continual learning methods, our\nmethod does not make any assumption about how tasks are constructed, delivered\nand how they relate to each other: it simply absorbs and retains training\nsamples one by one, whether the stream of input data is time-correlated or not.\n",
        "published": "2021",
        "authors": [
            "Rodrigue Siry"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.03524v1",
        "title": "A Data-driven Approach to Neural Architecture Search Initialization",
        "abstract": "  Algorithmic design in neural architecture search (NAS) has received a lot of\nattention, aiming to improve performance and reduce computational cost. Despite\nthe great advances made, few authors have proposed to tailor initialization\ntechniques for NAS. However, literature shows that a good initial set of\nsolutions facilitate finding the optima. Therefore, in this study, we propose a\ndata-driven technique to initialize a population-based NAS algorithm.\nParticularly, we proposed a two-step methodology. First, we perform a\ncalibrated clustering analysis of the search space, and second, we extract the\ncentroids and use them to initialize a NAS algorithm. We benchmark our proposed\napproach against random and Latin hypercube sampling initialization using three\npopulation-based algorithms, namely a genetic algorithm, evolutionary\nalgorithm, and aging evolution, on CIFAR-10. More specifically, we use\nNAS-Bench-101 to leverage the availability of NAS benchmarks. The results show\nthat compared to random and Latin hypercube sampling, the proposed\ninitialization technique enables achieving significant long-term improvements\nfor two of the search baselines, and sometimes in various search scenarios\n(various training budgets). Moreover, we analyze the distributions of solutions\nobtained and find that that the population provided by the data-driven\ninitialization technique enables retrieving local optima (maxima) of high\nfitness and similar configurations.\n",
        "published": "2021",
        "authors": [
            "Kalifou Ren\u00e9 Traor\u00e9",
            "Andr\u00e9s Camero",
            "Xiao Xiang Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.04879v2",
        "title": "EvoLearner: Learning Description Logics with Evolutionary Algorithms",
        "abstract": "  Classifying nodes in knowledge graphs is an important task, e.g., for\npredicting missing types of entities, predicting which molecules cause cancer,\nor predicting which drugs are promising treatment candidates. While black-box\nmodels often achieve high predictive performance, they are only post-hoc and\nlocally explainable and do not allow the learned model to be easily enriched\nwith domain knowledge. Towards this end, learning description logic concepts\nfrom positive and negative examples has been proposed. However, learning such\nconcepts often takes a long time and state-of-the-art approaches provide\nlimited support for literal data values, although they are crucial for many\napplications. In this paper, we propose EvoLearner - an evolutionary approach\nto learn concepts in ALCQ(D), which is the attributive language with complement\n(ALC) paired with qualified cardinality restrictions (Q) and data properties\n(D). We contribute a novel initialization method for the initial population:\nstarting from positive examples, we perform biased random walks and translate\nthem to description logic concepts. Moreover, we improve support for data\nproperties by maximizing information gain when deciding where to split the\ndata. We show that our approach significantly outperforms the state of the art\non the benchmarking framework SML-Bench for structured machine learning. Our\nablation study confirms that this is due to our novel initialization method and\nsupport for data properties.\n",
        "published": "2021",
        "authors": [
            "Stefan Heindorf",
            "Lukas Bl\u00fcbaum",
            "Nick D\u00fcsterhus",
            "Till Werner",
            "Varun Nandkumar Golani",
            "Caglar Demir",
            "Axel-Cyrille Ngonga Ngomo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.06679v2",
        "title": "deepstruct -- linking deep learning and graph theory",
        "abstract": "  deepstruct connects deep learning models and graph theory such that different\ngraph structures can be imposed on neural networks or graph structures can be\nextracted from trained neural network models. For this, deepstruct provides\ndeep neural network models with different restrictions which can be created\nbased on an initial graph. Further, tools to extract graph structures from\ntrained models are available. This step of extracting graphs can be\ncomputationally expensive even for models of just a few dozen thousand\nparameters and poses a challenging problem. deepstruct supports research in\npruning, neural architecture search, automated network design and structure\nanalysis of neural networks.\n",
        "published": "2021",
        "authors": [
            "Julian Stier",
            "Michael Granitzer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.06726v1",
        "title": "One model Packs Thousands of Items with Recurrent Conditional Query\n  Learning",
        "abstract": "  Recent studies have revealed that neural combinatorial optimization (NCO) has\nadvantages over conventional algorithms in many combinatorial optimization\nproblems such as routing, but it is less efficient for more complicated\noptimization tasks such as packing which involves mutually conditioned action\nspaces. In this paper, we propose a Recurrent Conditional Query Learning (RCQL)\nmethod to solve both 2D and 3D packing problems. We first embed states by a\nrecurrent encoder, and then adopt attention with conditional queries from\nprevious actions. The conditional query mechanism fills the information gap\nbetween learning steps, which shapes the problem as a Markov decision process.\nBenefiting from the recurrence, a single RCQL model is capable of handling\ndifferent sizes of packing problems. Experiment results show that RCQL can\neffectively learn strong heuristics for offline and online strip packing\nproblems (SPPs), outperforming a wide range of baselines in space utilization\nratio. RCQL reduces the average bin gap ratio by 1.83% in offline 2D 40-box\ncases and 7.84% in 3D cases compared with state-of-the-art methods. Meanwhile,\nour method also achieves 5.64% higher space utilization ratio for SPPs with\n1000 items than the state of the art.\n",
        "published": "2021",
        "authors": [
            "Dongda Li",
            "Zhaoquan Gu",
            "Yuexuan Wang",
            "Changwei Ren",
            "Francis C. M. Lau"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.06942v1",
        "title": "Predictive coding, precision and natural gradients",
        "abstract": "  There is an increasing convergence between biologically plausible\ncomputational models of inference and learning with local update rules and the\nglobal gradient-based optimization of neural network models employed in machine\nlearning. One particularly exciting connection is the correspondence between\nthe locally informed optimization in predictive coding networks and the error\nbackpropagation algorithm that is used to train state-of-the-art deep\nartificial neural networks. Here we focus on the related, but still largely\nunder-explored connection between precision weighting in predictive coding\nnetworks and the Natural Gradient Descent algorithm for deep neural networks.\nPrecision-weighted predictive coding is an interesting candidate for scaling up\nuncertainty-aware optimization -- particularly for models with large parameter\nspaces -- due to its distributed nature of the optimization process and the\nunderlying local approximation of the Fisher information metric, the adaptive\nlearning rate that is central to Natural Gradient Descent. Here, we show that\nhierarchical predictive coding networks with learnable precision indeed are\nable to solve various supervised and unsupervised learning tasks with\nperformance comparable to global backpropagation with natural gradients and\noutperform their classical gradient descent counterpart on tasks where high\namounts of noise are embedded in data or label inputs. When applied to\nunsupervised auto-encoding of image inputs, the deterministic network produces\nhierarchically organized and disentangled embeddings, hinting at the close\nconnections between predictive coding and hierarchical variational inference.\n",
        "published": "2021",
        "authors": [
            "Andre Ofner",
            "Raihan Kabir Ratul",
            "Suhita Ghosh",
            "Sebastian Stober"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.07138v1",
        "title": "Towards One Shot Search Space Poisoning in Neural Architecture Search",
        "abstract": "  We evaluate the robustness of a Neural Architecture Search (NAS) algorithm\nknown as Efficient NAS (ENAS) against data agnostic poisoning attacks on the\noriginal search space with carefully designed ineffective operations. We\nempirically demonstrate how our one shot search space poisoning approach\nexploits design flaws in the ENAS controller to degrade predictive performance\non classification tasks. With just two poisoning operations injected into the\nsearch space, we inflate prediction error rates for child networks upto 90% on\nthe CIFAR-10 dataset.\n",
        "published": "2021",
        "authors": [
            "Nayan Saxena",
            "Robert Wu",
            "Rohan Jain"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.08679v1",
        "title": "Automatically detecting anomalous exoplanet transits",
        "abstract": "  Raw light curve data from exoplanet transits is too complex to naively apply\ntraditional outlier detection methods. We propose an architecture which\nestimates a latent representation of both the main transit and residual\ndeviations with a pair of variational autoencoders. We show, using two\nfabricated datasets, that our latent representations of anomalous transit\nresiduals are significantly more amenable to outlier detection than raw data or\nthe latent representation of a traditional variational autoencoder. We then\napply our method to real exoplanet transit data. Our study is the first which\nautomatically identifies anomalous exoplanet transit light curves. We\nadditionally release three first-of-their-kind datasets to enable further\nresearch.\n",
        "published": "2021",
        "authors": [
            "Christoph J. H\u00f6nes",
            "Benjamin Kurt Miller",
            "Ana M. Heras",
            "Bernard H. Foing"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.08792v2",
        "title": "PredProp: Bidirectional Stochastic Optimization with Precision Weighted\n  Predictive Coding",
        "abstract": "  We present PredProp, a method for optimization of weights and states in\npredictive coding networks (PCNs) based on the precision of propagated errors\nand neural activity. PredProp jointly addresses inference and learning via\nstochastic gradient descent and adaptively weights parameter updates by\napproximate curvature. Due to the relation between propagated error covariance\nand the Fisher information matrix, PredProp implements approximate Natural\nGradient Descent. We demonstrate PredProp's effectiveness in the context of\ndense decoder networks and simple image benchmark datasets. We found that\nPredProp performs favorably over Adam, a widely used adaptive learning rate\noptimizer in the tested configurations. Furthermore, available optimization\nmethods for weight parameters benefit from using PredProp's error precision\nduring inference. Since hierarchical predictive coding layers are optimised\nindividually using local errors, the required precisions factorize over\nhierarchical layers. Extending beyond classical PCNs with a single set of\ndecoder layers per hierarchical layer, we also generalize PredProp to deep\nneural networks in each PCN layer by additionally factorizing over the weights\nin each PCN layer.\n",
        "published": "2021",
        "authors": [
            "Andr\u00e9 Ofner",
            "Sebastian Stober"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.10831v1",
        "title": "Learning by Active Forgetting for Neural Networks",
        "abstract": "  Remembering and forgetting mechanisms are two sides of the same coin in a\nhuman learning-memory system. Inspired by human brain memory mechanisms, modern\nmachine learning systems have been working to endow machine with lifelong\nlearning capability through better remembering while pushing the forgetting as\nthe antagonist to overcome. Nevertheless, this idea might only see the half\npicture. Up until very recently, increasing researchers argue that a brain is\nborn to forget, i.e., forgetting is a natural and active process for abstract,\nrich, and flexible representations. This paper presents a learning model by\nactive forgetting mechanism with artificial neural networks. The active\nforgetting mechanism (AFM) is introduced to a neural network via a\n\"plug-and-play\" forgetting layer (P\\&PF), consisting of groups of inhibitory\nneurons with Internal Regulation Strategy (IRS) to adjust the extinction rate\nof themselves via lateral inhibition mechanism and External Regulation Strategy\n(ERS) to adjust the extinction rate of excitatory neurons via inhibition\nmechanism. Experimental studies have shown that the P\\&PF offers surprising\nbenefits: self-adaptive structure, strong generalization, long-term learning\nand memory, and robustness to data and parameter perturbation. This work sheds\nlight on the importance of forgetting in the learning process and offers new\nperspectives to understand the underlying mechanisms of neural networks.\n",
        "published": "2021",
        "authors": [
            "Jian Peng",
            "Xian Sun",
            "Min Deng",
            "Chao Tao",
            "Bo Tang",
            "Wenbo Li",
            "Guohua Wu",
            " QingZhu",
            "Yu Liu",
            "Tao Lin",
            "Haifeng Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.11647v1",
        "title": "Inducing Functions through Reinforcement Learning without Task\n  Specification",
        "abstract": "  We report a bio-inspired framework for training a neural network through\nreinforcement learning to induce high level functions within the network. Based\non the interpretation that animals have gained their cognitive functions such\nas object recognition - without ever being specifically trained for - as a\nresult of maximizing their fitness to the environment, we place our agent in an\nenvironment where developing certain functions may facilitate decision making.\nThe experimental results show that high level functions, such as image\nclassification and hidden variable estimation, can be naturally and\nsimultaneously induced without any pre-training or specifying them.\n",
        "published": "2021",
        "authors": [
            "Junmo Cho",
            "Dong-Hwan Lee",
            "Young-Gyu Yoon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.11964v1",
        "title": "Reviewing continual learning from the perspective of human-level\n  intelligence",
        "abstract": "  Humans' continual learning (CL) ability is closely related to Stability\nVersus Plasticity Dilemma that describes how humans achieve ongoing learning\ncapacity and preservation for learned information. The notion of CL has always\nbeen present in artificial intelligence (AI) since its births. This paper\nproposes a comprehensive review of CL. Different from previous reviews that\nmainly focus on the catastrophic forgetting phenomenon in CL, this paper\nsurveys CL from a more macroscopic perspective based on the Stability Versus\nPlasticity mechanism. Analogous to biological counterpart, \"smart\" AI agents\nare supposed to i) remember previously learned information (information\nretrospection); ii) infer on new information continuously (information\nprospection:); iii) transfer useful information (information transfer), to\nachieve high-level CL. According to the taxonomy, evaluation metrics,\nalgorithms, applications as well as some open issues are then introduced. Our\nmain contributions concern i) rechecking CL from the level of artificial\ngeneral intelligence; ii) providing a detailed and extensive overview on CL\ntopics; iii) presenting some novel ideas on the potential development of CL.\n",
        "published": "2021",
        "authors": [
            "Yifan Chang",
            "Wenbo Li",
            "Jian Peng",
            "Bo Tang",
            "Yu Kang",
            "Yinjie Lei",
            "Yuanmiao Gui",
            "Qing Zhu",
            "Yu Liu",
            "Haifeng Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.12606v1",
        "title": "Deep metric learning improves lab of origin prediction of genetically\n  engineered plasmids",
        "abstract": "  Genome engineering is undergoing unprecedented development and is now\nbecoming widely available. To ensure responsible biotechnology innovation and\nto reduce misuse of engineered DNA sequences, it is vital to develop tools to\nidentify the lab-of-origin of engineered plasmids. Genetic engineering\nattribution (GEA), the ability to make sequence-lab associations, would support\nforensic experts in this process. Here, we propose a method, based on metric\nlearning, that ranks the most likely labs-of-origin whilst simultaneously\ngenerating embeddings for plasmid sequences and labs. These embeddings can be\nused to perform various downstream tasks, such as clustering DNA sequences and\nlabs, as well as using them as features in machine learning models. Our\napproach employs a circular shift augmentation approach and is able to\ncorrectly rank the lab-of-origin $90\\%$ of the time within its top 10\npredictions - outperforming all current state-of-the-art approaches. We also\ndemonstrate that we can perform few-shot-learning and obtain $76\\%$ top-10\naccuracy using only $10\\%$ of the sequences. This means, we outperform the\nprevious CNN approach using only one-tenth of the data. We also demonstrate\nthat we are able to extract key signatures in plasmid sequences for particular\nlabs, allowing for an interpretable examination of the model's outputs.\n",
        "published": "2021",
        "authors": [
            "Igor M. Soares",
            "Fernando H. F. Camargo",
            "Adriano Marques",
            "Oliver M. Crook"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.13187v1",
        "title": "Information Bottleneck-Based Hebbian Learning Rule Naturally Ties\n  Working Memory and Synaptic Updates",
        "abstract": "  Artificial neural networks have successfully tackled a large variety of\nproblems by training extremely deep networks via back-propagation. A direct\napplication of back-propagation to spiking neural networks contains\nbiologically implausible components, like the weight transport problem or\nseparate inference and learning phases. Various methods address different\ncomponents individually, but a complete solution remains intangible. Here, we\ntake an alternate approach that avoids back-propagation and its associated\nissues entirely. Recent work in deep learning proposed independently training\neach layer of a network via the information bottleneck (IB). Subsequent studies\nnoted that this layer-wise approach circumvents error propagation across\nlayers, leading to a biologically plausible paradigm. Unfortunately, the IB is\ncomputed using a batch of samples. The prior work addresses this with a weight\nupdate that only uses two samples (the current and previous sample). Our work\ntakes a different approach by decomposing the weight update into a local and\nglobal component. The local component is Hebbian and only depends on the\ncurrent sample. The global component computes a layer-wise modulatory signal\nthat depends on a batch of samples. We show that this modulatory signal can be\nlearned by an auxiliary circuit with working memory (WM) like a reservoir.\nThus, we can use batch sizes greater than two, and the batch size determines\nthe required capacity of the WM. To the best of our knowledge, our rule is the\nfirst biologically plausible mechanism to directly couple synaptic updates with\na WM of the task. We evaluate our rule on synthetic datasets and image\nclassification datasets like MNIST, and we explore the effect of the WM\ncapacity on learning performance. We hope our work is a first-step towards\nunderstanding the mechanistic role of memory in learning.\n",
        "published": "2021",
        "authors": [
            "Kyle Daruwalla",
            "Mikko Lipasti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.14683v1",
        "title": "Anomaly Localization in Model Gradients Under Backdoor Attacks Against\n  Federated Learning",
        "abstract": "  Inserting a backdoor into the joint model in federated learning (FL) is a\nrecent threat raising concerns. Existing studies mostly focus on developing\neffective countermeasures against this threat, assuming that backdoored local\nmodels, if any, somehow reveal themselves by anomalies in their gradients.\nHowever, this assumption needs to be elaborated by identifying specifically\nwhich gradients are more likely to indicate an anomaly to what extent under\nwhich conditions. This is an important issue given that neural network models\nusually have huge parametric space and consist of a large number of weights. In\nthis study, we make a deep gradient-level analysis on the expected variations\nin model gradients under several backdoor attack scenarios against FL. Our main\nnovel finding is that backdoor-induced anomalies in local model updates\n(weights or gradients) appear in the final layer bias weights of the malicious\nlocal models. We support and validate our findings by both theoretical and\nexperimental analysis in various FL settings. We also investigate the impact of\nthe number of malicious clients, learning rate, and malicious data rate on the\nobserved anomaly. Our implementation is publicly available\\footnote{\\url{\nhttps://github.com/ArcelikAcikKaynak/Federated_Learning.git}}.\n",
        "published": "2021",
        "authors": [
            "Zeki Bilgin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.02342v1",
        "title": "Overcome Anterograde Forgetting with Cycled Memory Networks",
        "abstract": "  Learning from a sequence of tasks for a lifetime is essential for an agent\ntowards artificial general intelligence. This requires the agent to\ncontinuously learn and memorize new knowledge without interference. This paper\nfirst demonstrates a fundamental issue of lifelong learning using neural\nnetworks, named anterograde forgetting, i.e., preserving and transferring\nmemory may inhibit the learning of new knowledge. This is attributed to the\nfact that the learning capacity of a neural network will be reduced as it keeps\nmemorizing historical knowledge, and the fact that conceptual confusion may\noccur as it transfers irrelevant old knowledge to the current task. This work\nproposes a general framework named Cycled Memory Networks (CMN) to address the\nanterograde forgetting in neural networks for lifelong learning. The CMN\nconsists of two individual memory networks to store short-term and long-term\nmemories to avoid capacity shrinkage. A transfer cell is designed to connect\nthese two memory networks, enabling knowledge transfer from the long-term\nmemory network to the short-term memory network to mitigate the conceptual\nconfusion, and a memory consolidation mechanism is developed to integrate\nshort-term knowledge into the long-term memory network for knowledge\naccumulation. Experimental results demonstrate that the CMN can effectively\naddress the anterograde forgetting on several task-related, task-conflict,\nclass-incremental and cross-domain benchmarks.\n",
        "published": "2021",
        "authors": [
            "Jian Peng",
            "Dingqi Ye",
            "Bo Tang",
            "Yinjie Lei",
            "Yu Liu",
            "Haifeng Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.03378v2",
        "title": "Differentiable Generalised Predictive Coding",
        "abstract": "  This paper deals with differentiable dynamical models congruent with neural\nprocess theories that cast brain function as the hierarchical refinement of an\ninternal generative model explaining observations. Our work extends existing\nimplementations of gradient-based predictive coding with automatic\ndifferentiation and allows to integrate deep neural networks for non-linear\nstate parameterization. Gradient-based predictive coding optimises inferred\nstates and weights locally in for each layer by optimising precision-weighted\nprediction errors that propagate from stimuli towards latent states.\nPredictions flow backwards, from latent states towards lower layers. The model\nsuggested here optimises hierarchical and dynamical predictions of latent\nstates. Hierarchical predictions encode expected content and hierarchical\nstructure. Dynamical predictions capture changes in the encoded content along\nwith higher order derivatives. Hierarchical and dynamical predictions interact\nand address different aspects of the same latent states. We apply the model to\nvarious perception and planning tasks on sequential data and show their mutual\ndependence. In particular, we demonstrate how learning sampling distances in\nparallel address meaningful locations data sampled at discrete time steps. We\ndiscuss possibilities to relax the assumption of linear hierarchies in favor of\nmore flexible graph structure with emergent properties. We compare the granular\nstructure of the model with canonical microcircuits describing predictive\ncoding in biological networks and review the connection to Markov Blankets as a\ntool to characterize modularity. A final section sketches out ideas for\nefficient perception and planning in nested spatio-temporal hierarchies.\n",
        "published": "2021",
        "authors": [
            "Andr\u00e9 Ofner",
            "Sebastian Stober"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.03518v2",
        "title": "Genetic Algorithm for Constrained Molecular Inverse Design",
        "abstract": "  A genetic algorithm is suitable for exploring large search spaces as it finds\nan approximate solution. Because of this advantage, genetic algorithm is\neffective in exploring vast and unknown space such as molecular search space.\nThough the algorithm is suitable for searching vast chemical space, it is\ndifficult to optimize pharmacological properties while maintaining molecular\nsubstructure. To solve this issue, we introduce a genetic algorithm featuring a\nconstrained molecular inverse design. The proposed algorithm successfully\nproduces valid molecules for crossover and mutation. Furthermore, it optimizes\nspecific properties while adhering to structural constraints using a two-phase\noptimization. Experiments prove that our algorithm effectively finds molecules\nthat satisfy specific properties while maintaining structural constraints.\n",
        "published": "2021",
        "authors": [
            "Yurim Lee",
            "Gydam Choi",
            "Minsung Yoon",
            "Cheongwon Kim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.08645v4",
        "title": "Learning Interpretable Models Through Multi-Objective Neural\n  Architecture Search",
        "abstract": "  Monumental advances in deep learning have led to unprecedented achievements\nacross various domains. While the performance of deep neural networks is\nindubitable, the architectural design and interpretability of such models are\nnontrivial. Research has been introduced to automate the design of neural\nnetwork architectures through neural architecture search (NAS). Recent progress\nhas made these methods more pragmatic by exploiting distributed computation and\nnovel optimization algorithms. However, there is little work in optimizing\narchitectures for interpretability. To this end, we propose a multi-objective\ndistributed NAS framework that optimizes for both task performance and\n\"introspectability,\" a surrogate metric for aspects of interpretability. We\nleverage the non-dominated sorting genetic algorithm (NSGA-II) and explainable\nAI (XAI) techniques to reward architectures that can be better comprehended by\ndomain experts. The framework is evaluated on several image classification\ndatasets. We demonstrate that jointly optimizing for task error and\nintrospectability leads to more disentangled and debuggable architectures that\nperform within tolerable error.\n",
        "published": "2021",
        "authors": [
            "Zachariah Carmichael",
            "Tim Moon",
            "Sam Ade Jacobs"
        ]
    }
]