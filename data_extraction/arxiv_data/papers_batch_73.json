[
    {
        "id": "http://arxiv.org/abs/2112.09697v1",
        "title": "On the Evolution of the MCTS Upper Confidence Bounds for Trees by Means\n  of Evolutionary Algorithms in the Game of Carcassonne",
        "abstract": "  Monte Carlo Tree Search (MCTS) is a sampling best-first method to search for\noptimal decisions. The MCTS's popularity is based on its extraordinary results\nin the challenging two-player based game Go, a game considered much harder than\nChess and that until very recently was considered infeasible for Artificial\nIntelligence methods. The success of MCTS depends heavily on how the tree is\nbuilt and the selection process plays a fundamental role in this. One\nparticular selection mechanism that has proved to be reliable is based on the\nUpper Confidence Bounds for Trees, commonly referred as UCT. The UCT attempts\nto nicely balance exploration and exploitation by considering the values stored\nin the statistical tree of the MCTS. However, some tuning of the MCTS UCT is\nnecessary for this to work well. In this work, we use Evolutionary Algorithms\n(EAs) to evolve mathematical expressions with the goal to substitute the UCT\nmathematical expression. We compare our proposed approach, called Evolution\nStrategy in MCTS (ES-MCTS) against five variants of the MCTS UCT, three\nvariants of the star-minimax family of algorithms as well as a random\ncontroller in the Game of Carcassonne. We also use a variant of our proposed\nEA-based controller, dubbed ES partially integrated in MCTS. We show how the\nES-MCTS controller, is able to outperform all these 10 intelligent controllers,\nincluding robust MCTS UCT controllers.\n",
        "published": "2021",
        "authors": [
            "Edgar Galv\u00e1n",
            "Gavin Simpson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.10930v1",
        "title": "Compact Multi-level Sparse Neural Networks with Input Independent\n  Dynamic Rerouting",
        "abstract": "  Deep neural networks (DNNs) have shown to provide superb performance in many\nreal life applications, but their large computation cost and storage\nrequirement have prevented them from being deployed to many edge and\ninternet-of-things (IoT) devices. Sparse deep neural networks, whose majority\nweight parameters are zeros, can substantially reduce the computation\ncomplexity and memory consumption of the models. In real-use scenarios, devices\nmay suffer from large fluctuations of the available computation and memory\nresources under different environment, and the quality of service (QoS) is\ndifficult to maintain due to the long tail inferences with large latency.\nFacing the real-life challenges, we propose to train a sparse model that\nsupports multiple sparse levels. That is, a hierarchical structure of weights\nare satisfied such that the locations and the values of the non-zero parameters\nof the more-sparse sub-model area subset of the less-sparse sub-model. In this\nway, one can dynamically select the appropriate sparsity level during\ninference, while the storage cost is capped by the least sparse sub-model. We\nhave verified our methodologies on a variety of DNN models and tasks, including\nthe ResNet-50, PointNet++, GNMT, and graph attention networks. We obtain sparse\nsub-models with an average of 13.38% weights and 14.97% FLOPs, while the\naccuracies are as good as their dense counterparts. More-sparse sub-models with\n5.38% weights and 4.47% of FLOPs, which are subsets of the less-sparse ones,\ncan be obtained with only 3.25% relative accuracy loss.\n",
        "published": "2021",
        "authors": [
            "Minghai Qin",
            "Tianyun Zhang",
            "Fei Sun",
            "Yen-Kuang Chen",
            "Makan Fardad",
            "Yanzhi Wang",
            "Yuan Xie"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.12251v1",
        "title": "ML4CO: Is GCNN All You Need? Graph Convolutional Neural Networks Produce\n  Strong Baselines For Combinatorial Optimization Problems, If Tuned and\n  Trained Properly, on Appropriate Data",
        "abstract": "  The 2021 NeurIPS Machine Learning for Combinatorial Optimization (ML4CO)\ncompetition was designed with the goal of improving state-of-the-art\ncombinatorial optimization solvers by replacing key heuristic components with\nmachine learning models. The competition's main scientific question was the\nfollowing: is machine learning a viable option for improving traditional\ncombinatorial optimization solvers on specific problem distributions, when\nhistorical data is available? This was motivated by the fact that in many\npractical scenarios, the data changes only slightly between the repetitions of\na combinatorial optimization problem, and this is an area where machine\nlearning models are particularly powerful at. This paper summarizes the\nsolution and lessons learned by the Huawei EI-OROAS team in the dual task of\nthe competition. The submission of our team achieved the second place in the\nfinal ranking, with a very close distance to the first spot. In addition, our\nsolution was ranked first consistently for several weekly leaderboard updates\nbefore the final evaluation. We provide insights gained from a large number of\nexperiments, and argue that a simple Graph Convolutional Neural Network (GCNNs)\ncan achieve state-of-the-art results if trained and tuned properly.\n",
        "published": "2021",
        "authors": [
            "Amin Banitalebi-Dehkordi",
            "Yong Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.13896v1",
        "title": "Two Sparsities Are Better Than One: Unlocking the Performance Benefits\n  of Sparse-Sparse Networks",
        "abstract": "  In principle, sparse neural networks should be significantly more efficient\nthan traditional dense networks. Neurons in the brain exhibit two types of\nsparsity; they are sparsely interconnected and sparsely active. These two types\nof sparsity, called weight sparsity and activation sparsity, when combined,\noffer the potential to reduce the computational cost of neural networks by two\norders of magnitude. Despite this potential, today's neural networks deliver\nonly modest performance benefits using just weight sparsity, because\ntraditional computing hardware cannot efficiently process sparse networks. In\nthis article we introduce Complementary Sparsity, a novel technique that\nsignificantly improves the performance of dual sparse networks on existing\nhardware. We demonstrate that we can achieve high performance running\nweight-sparse networks, and we can multiply those speedups by incorporating\nactivation sparsity. Using Complementary Sparsity, we show up to 100X\nimprovement in throughput and energy efficiency performing inference on FPGAs.\nWe analyze scalability and resource tradeoffs for a variety of kernels typical\nof commercial convolutional networks such as ResNet-50 and MobileNetV2. Our\nresults with Complementary Sparsity suggest that weight plus activation\nsparsity can be a potent combination for efficiently scaling future AI models.\n",
        "published": "2021",
        "authors": [
            "Kevin Lee Hunter",
            "Lawrence Spracklen",
            "Subutai Ahmad"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.14834v3",
        "title": "Training Quantized Deep Neural Networks via Cooperative Coevolution",
        "abstract": "  This work considers a challenging Deep Neural Network(DNN) quantization task\nthat seeks to train quantized DNNs without involving any full-precision\noperations. Most previous quantization approaches are not applicable to this\ntask since they rely on full-precision gradients to update network weights. To\nfill this gap, in this work we advocate using Evolutionary Algorithms (EAs) to\nsearch for the optimal low-bits weights of DNNs. To efficiently solve the\ninduced large-scale discrete problem, we propose a novel EA based on\ncooperative coevolution that repeatedly groups the network weights based on the\nconfidence in their values and focuses on optimizing the ones with the least\nconfidence. To the best of our knowledge, this is the first work that applies\nEAs to train quantized DNNs. Experiments show that our approach surpasses\nprevious quantization approaches and can train a 4-bit ResNet-20 on the\nCifar-10 dataset with the same test accuracy as its full-precision counterpart.\n",
        "published": "2021",
        "authors": [
            "Fu Peng",
            "Shengcai Liu",
            "Ning Lu",
            "Ke Tang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.15475v1",
        "title": "Shift-Equivariant Similarity-Preserving Hypervector Representations of\n  Sequences",
        "abstract": "  Hyperdimensional Computing (HDC), also known as Vector-Symbolic Architectures\n(VSA), is a promising framework for the development of cognitive architectures\nand artificial intelligence systems, as well as for technical applications and\nemerging neuromorphic and nanoscale hardware. HDC/VSA operate with\nhypervectors, i.e., distributed vector representations of large fixed dimension\n(usually > 1000). One of the key ingredients of HDC/VSA are the methods for\nencoding data of various types (from numeric scalars and vectors to graphs)\ninto hypervectors. In this paper, we propose an approach for the formation of\nhypervectors of sequences that provides both an equivariance with respect to\nthe shift of sequences and preserves the similarity of sequences with identical\nelements at nearby positions. Our methods represent the sequence elements by\ncompositional hypervectors and exploit permutations of hypervectors for\nrepresenting the order of sequence elements. We experimentally explored the\nproposed representations using a diverse set of tasks with data in the form of\nsymbolic strings. Although our approach is feature-free as it forms the\nhypervector of a sequence from the hypervectors of its symbols at their\npositions, it demonstrated the performance on a par with the methods that apply\nvarious features, such as subsequences. The proposed techniques were designed\nfor the HDC/VSA model known as Sparse Binary Distributed Representations.\nHowever, they can be adapted to hypervectors in formats of other HDC/VSA\nmodels, as well as for representing sequences of types other than symbolic\nstrings.\n",
        "published": "2021",
        "authors": [
            "Dmitri A. Rachkovskij"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.00042v2",
        "title": "Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in\n  Dynamic Environments",
        "abstract": "  A key challenge for AI is to build embodied systems that operate in\ndynamically changing environments. Such systems must adapt to changing task\ncontexts and learn continuously. Although standard deep learning systems\nachieve state of the art results on static benchmarks, they often struggle in\ndynamic scenarios. In these settings, error signals from multiple contexts can\ninterfere with one another, ultimately leading to a phenomenon known as\ncatastrophic forgetting. In this article we investigate biologically inspired\narchitectures as solutions to these problems. Specifically, we show that the\nbiophysical properties of dendrites and local inhibitory systems enable\nnetworks to dynamically restrict and route information in a context-specific\nmanner. Our key contributions are as follows. First, we propose a novel\nartificial neural network architecture that incorporates active dendrites and\nsparse representations into the standard deep learning framework. Next, we\nstudy the performance of this architecture on two separate benchmarks requiring\ntask-based adaptation: Meta-World, a multi-task reinforcement learning\nenvironment where a robotic agent must learn to solve a variety of manipulation\ntasks simultaneously; and a continual learning benchmark in which the model's\nprediction task changes throughout training. Analysis on both benchmarks\ndemonstrates the emergence of overlapping but distinct and sparse subnetworks,\nallowing the system to fluidly learn multiple tasks with minimal forgetting.\nOur neural implementation marks the first time a single architecture has\nachieved competitive results on both multi-task and continual learning\nsettings. Our research sheds light on how biological properties of neurons can\ninform deep learning systems to address dynamic scenarios that are typically\nimpossible for traditional ANNs to solve.\n",
        "published": "2021",
        "authors": [
            "Abhiram Iyer",
            "Karan Grewal",
            "Akash Velu",
            "Lucas Oliveira Souza",
            "Jeremy Forest",
            "Subutai Ahmad"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.00093v1",
        "title": "Distributed Evolution Strategies Using TPUs for Meta-Learning",
        "abstract": "  Meta-learning traditionally relies on backpropagation through entire tasks to\niteratively improve a model's learning dynamics. However, this approach is\ncomputationally intractable when scaled to complex tasks. We propose a\ndistributed evolutionary meta-learning strategy using Tensor Processing Units\n(TPUs) that is highly parallel and scalable to arbitrarily long tasks with no\nincrease in memory cost. Using a Prototypical Network trained with evolution\nstrategies on the Omniglot dataset, we achieved an accuracy of 98.4% on a\n5-shot classification problem. Our algorithm used as much as 40 times less\nmemory than automatic differentiation to compute the gradient, with the\nresulting model achieving accuracy within 1.3% of a backpropagation-trained\nequivalent (99.6%). We observed better classification accuracy as high as 99.1%\nwith larger population configurations. We further experimentally validate the\nstability and performance of ES-ProtoNet across a variety of training\nconditions (varying population size, model size, number of workers, shot, way,\nES hyperparameters, etc.). Our contributions are twofold: we provide the first\nassessment of evolutionary meta-learning in a supervised setting, and create a\ngeneral framework for distributed evolution strategies on TPUs.\n",
        "published": "2022",
        "authors": [
            "Alex Sheng",
            "Derek He"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.02158v2",
        "title": "Introducing Randomized High Order Fuzzy Cognitive Maps as Reservoir\n  Computing Models: A Case Study in Solar Energy and Load Forecasting",
        "abstract": "  Fuzzy Cognitive Maps (FCMs) have emerged as an interpretable signed weighted\ndigraph method consisting of nodes (concepts) and weights which represent the\ndependencies among the concepts. Although FCMs have attained considerable\nachievements in various time series prediction applications, designing an FCM\nmodel with time-efficient training method is still an open challenge. Thus,\nthis paper introduces a novel univariate time series forecasting technique,\nwhich is composed of a group of randomized high order FCM models labeled\nR-HFCM. The novelty of the proposed R-HFCM model is relevant to merging the\nconcepts of FCM and Echo State Network (ESN) as an efficient and particular\nfamily of Reservoir Computing (RC) models, where the least squares algorithm is\napplied to train the model. From another perspective, the structure of R-HFCM\nconsists of the input layer, reservoir layer, and output layer in which only\nthe output layer is trainable while the weights of each sub-reservoir\ncomponents are selected randomly and keep constant during the training process.\nAs case studies, this model considers solar energy forecasting with public data\nfor Brazilian solar stations as well as Malaysia dataset, which includes hourly\nelectric load and temperature data of the power supply company of the city of\nJohor in Malaysia. The experiment also includes the effect of the map size,\nactivation function, the presence of bias and the size of the reservoir on the\naccuracy of R-HFCM method. The obtained results confirm the outperformance of\nthe proposed R-HFCM model in comparison to the other methods. This study\nprovides evidence that FCM can be a new way to implement a reservoir of\ndynamics in time series modelling.\n",
        "published": "2022",
        "authors": [
            "Omid Orang",
            "Petr\u00f4nio C\u00e2ndido de Lima Silva",
            "Frederico Gadelha Guimar\u00e3es"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.02297v2",
        "title": "Time Series Forecasting Using Fuzzy Cognitive Maps: A Survey",
        "abstract": "  Among various soft computing approaches for time series forecasting, Fuzzy\nCognitive Maps (FCM) have shown remarkable results as a tool to model and\nanalyze the dynamics of complex systems. FCM have similarities to recurrent\nneural networks and can be classified as a neuro-fuzzy method. In other words,\nFCMs are a mixture of fuzzy logic, neural network, and expert system aspects,\nwhich act as a powerful tool for simulating and studying the dynamic behavior\nof complex systems. The most interesting features are knowledge\ninterpretability, dynamic characteristics and learning capability. The goal of\nthis survey paper is mainly to present an overview on the most relevant and\nrecent FCM-based time series forecasting models proposed in the literature. In\naddition, this article considers an introduction on the fundamentals of FCM\nmodel and learning methodologies. Also, this survey provides some ideas for\nfuture research to enhance the capabilities of FCM in order to cover some\nchallenges in the real-world experiments such as handling non-stationary data\nand scalability issues. Moreover, equipping FCMs with fast learning algorithms\nis one of the major concerns in this area.\n",
        "published": "2022",
        "authors": [
            "Omid Orang",
            "Petr\u00f4nio C\u00e2ndido de Lima e Silva",
            "Frederico Gadelha Guimar\u00e3es"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.07208v1",
        "title": "Enhanced Self-Organizing Map Solution for the Traveling Salesman Problem",
        "abstract": "  Using an enhanced Self-Organizing Map method, we provided suboptimal\nsolutions to the Traveling Salesman Problem. Besides, we employed\nhyperparameter tuning to identify the most critical features in the algorithm.\nAll improvements in the benchmark work brought consistent results and may\ninspire future efforts to improve this algorithm and apply it to different\nproblems.\n",
        "published": "2021",
        "authors": [
            "Joao P. A. Dantas",
            "Andre N. Costa",
            "Marcos R. O. A. Maximo",
            "Takashi Yoneyama"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.09699v2",
        "title": "EASY: Ensemble Augmented-Shot Y-shaped Learning: State-Of-The-Art\n  Few-Shot Classification with Simple Ingredients",
        "abstract": "  Few-shot learning aims at leveraging knowledge learned by one or more deep\nlearning models, in order to obtain good classification performance on new\nproblems, where only a few labeled samples per class are available. Recent\nyears have seen a fair number of works in the field, introducing methods with\nnumerous ingredients. A frequent problem, though, is the use of suboptimally\ntrained models to extract knowledge, leading to interrogations on whether\nproposed approaches bring gains compared to using better initial models without\nthe introduced ingredients. In this work, we propose a simple methodology, that\nreaches or even beats state of the art performance on multiple standardized\nbenchmarks of the field, while adding almost no hyperparameters or parameters\nto those used for training the initial deep learning models on the generic\ndataset. This methodology offers a new baseline on which to propose (and fairly\ncompare) new techniques or adapt existing ones.\n",
        "published": "2022",
        "authors": [
            "Yassir Bendou",
            "Yuqing Hu",
            "Raphael Lafargue",
            "Giulia Lioi",
            "Bastien Pasdeloup",
            "St\u00e9phane Pateux",
            "Vincent Gripon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.09754v1",
        "title": "Deep Reinforcement Learning with Spiking Q-learning",
        "abstract": "  With the help of special neuromorphic hardware, spiking neural networks\n(SNNs) are expected to realize artificial intelligence with less energy\nconsumption. It provides a promising energy-efficient way for realistic control\ntasks by combing SNNs and deep reinforcement learning (RL). There are only a\nfew existing SNN-based RL methods at present. Most of them either lack\ngeneralization ability or employ Artificial Neural Networks (ANNs) to estimate\nvalue function in training. The former needs to tune numerous hyper-parameters\nfor each scenario, and the latter limits the application of different types of\nRL algorithm and ignores the large energy consumption in training. To develop a\nrobust spike-based RL method, we draw inspiration from non-spiking interneurons\nfound in insects and propose the deep spiking Q-network (DSQN), using the\nmembrane voltage of non-spiking neurons as the representation of Q-value, which\ncan directly learn robust policies from high-dimensional sensory inputs using\nend-to-end RL. Experiments conducted on 17 Atari games demonstrate the\neffectiveness of DSQN by outperforming the ANN-based deep Q-network (DQN) in\nmost games. Moreover, the experimental results show superior learning stability\nand robustness to adversarial attacks of DSQN.\n",
        "published": "2022",
        "authors": [
            "Ding Chen",
            "Peixi Peng",
            "Tiejun Huang",
            "Yonghong Tian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.10355v3",
        "title": "Neural Architecture Search for Spiking Neural Networks",
        "abstract": "  Spiking Neural Networks (SNNs) have gained huge attention as a potential\nenergy-efficient alternative to conventional Artificial Neural Networks (ANNs)\ndue to their inherent high-sparsity activation. However, most prior SNN methods\nuse ANN-like architectures (e.g., VGG-Net or ResNet), which could provide\nsub-optimal performance for temporal sequence processing of binary information\nin SNNs. To address this, in this paper, we introduce a novel Neural\nArchitecture Search (NAS) approach for finding better SNN architectures.\nInspired by recent NAS approaches that find the optimal architecture from\nactivation patterns at initialization, we select the architecture that can\nrepresent diverse spike activation patterns across different data samples\nwithout training. Moreover, to further leverage the temporal information among\nthe spikes, we search for feed forward connections as well as backward\nconnections (i.e., temporal feedback connections) between layers.\nInterestingly, SNASNet found by our search algorithm achieves higher\nperformance with backward connections, demonstrating the importance of\ndesigning SNN architecture for suitably using temporal information. We conduct\nextensive experiments on three image recognition benchmarks where we show that\nSNASNet achieves state-of-the-art performance with significantly lower\ntimesteps (5 timesteps). Code is available at Github.\n",
        "published": "2022",
        "authors": [
            "Youngeun Kim",
            "Yuhang Li",
            "Hyoungseob Park",
            "Yeshwanth Venkatesha",
            "Priyadarshini Panda"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.12493v1",
        "title": "A new Sparse Auto-encoder based Framework using Grey Wolf Optimizer for\n  Data Classification Problem",
        "abstract": "  One of the most important properties of deep auto-encoders (DAEs) is their\ncapability to extract high level features from row data. Hence, especially\nrecently, the autoencoders are preferred to be used in various classification\nproblems such as image and voice recognition, computer security, medical data\nanalysis, etc. Despite, its popularity and high performance, the training phase\nof autoencoders is still a challenging task, involving to select best\nparameters that let the model to approach optimal results. Different training\napproaches are applied to train sparse autoencoders. Previous studies and\npreliminary experiments reveal that those approaches may present remarkable\nresults in same problems but also disappointing results can be obtained in\nother complex problems. Metaheuristic algorithms have emerged over the last two\ndecades and are becoming an essential part of contemporary optimization\ntechniques. Gray wolf optimization (GWO) is one of the current of those\nalgorithms and is applied to train sparse auto-encoders for this study. This\nmodel is validated by employing several popular Gene expression databases.\nResults are compared with previous state-of-the art methods studied with the\nsame data sets and also are compared with other popular metaheuristic\nalgorithms, namely, Genetic Algorithms (GA), Particle Swarm Optimization (PSO)\nand Artificial Bee Colony (ABC). Results reveal that the performance of the\ntrained model using GWO outperforms on both conventional models and models\ntrained with most popular metaheuristic algorithms.\n",
        "published": "2022",
        "authors": [
            "Ahmad Mozaffer Karim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.12738v3",
        "title": "AutoSNN: Towards Energy-Efficient Spiking Neural Networks",
        "abstract": "  Spiking neural networks (SNNs) that mimic information transmission in the\nbrain can energy-efficiently process spatio-temporal information through\ndiscrete and sparse spikes, thereby receiving considerable attention. To\nimprove accuracy and energy efficiency of SNNs, most previous studies have\nfocused solely on training methods, and the effect of architecture has rarely\nbeen studied. We investigate the design choices used in the previous studies in\nterms of the accuracy and number of spikes and figure out that they are not\nbest-suited for SNNs. To further improve the accuracy and reduce the spikes\ngenerated by SNNs, we propose a spike-aware neural architecture search\nframework called AutoSNN. We define a search space consisting of architectures\nwithout undesirable design choices. To enable the spike-aware architecture\nsearch, we introduce a fitness that considers both the accuracy and number of\nspikes. AutoSNN successfully searches for SNN architectures that outperform\nhand-crafted SNNs in accuracy and energy efficiency. We thoroughly demonstrate\nthe effectiveness of AutoSNN on various datasets including neuromorphic\ndatasets.\n",
        "published": "2022",
        "authors": [
            "Byunggook Na",
            "Jisoo Mok",
            "Seongsik Park",
            "Dongjin Lee",
            "Hyeokjun Choe",
            "Sungroh Yoon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.13305v1",
        "title": "Optimizing LLVM Pass Sequences with Shackleton: A Linear Genetic\n  Programming Framework",
        "abstract": "  In this paper we introduce Shackleton as a generalized framework enabling the\napplication of linear genetic programming -- a technique under the umbrella of\nevolutionary algorithms -- to a variety of use cases. We also explore here a\nnovel application for this class of methods: optimizing sequences of LLVM\noptimization passes. The algorithm underpinning Shackleton is discussed, with\nan emphasis on the effects of different features unique to the framework when\napplied to LLVM pass sequences. Combined with analysis of different\nhyperparameter settings, we report the results on automatically optimizing pass\nsequences using Shackleton for two software applications at differing\ncomplexity levels. Finally, we reflect on the advantages and limitations of our\ncurrent implementation and lay out a path for further improvements. These\nimprovements aim to surpass hand-crafted solutions with an automatic discovery\nmethod for an optimal pass sequence.\n",
        "published": "2022",
        "authors": [
            "Hannah Peeler",
            "Shuyue Stella Li",
            "Andrew N. Sloss",
            "Kenneth N. Reid",
            "Yuan Yuan",
            "Wolfgang Banzhaf"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.01027v1",
        "title": "Learning in Sparse Rewards settings through Quality-Diversity algorithms",
        "abstract": "  In the Reinforcement Learning (RL) framework, the learning is guided through\na reward signal. This means that in situations of sparse rewards the agent has\nto focus on exploration, in order to discover which action, or set of actions\nleads to the reward. RL agents usually struggle with this. Exploration is the\nfocus of Quality-Diversity (QD) methods. In this thesis, we approach the\nproblem of sparse rewards with these algorithms, and in particular with Novelty\nSearch (NS). This is a method that only focuses on the diversity of the\npossible policies behaviors. The first part of the thesis focuses on learning a\nrepresentation of the space in which the diversity of the policies is\nevaluated. In this regard, we propose the TAXONS algorithm, a method that\nlearns a low-dimensional representation of the search space through an\nAutoEncoder. While effective, TAXONS still requires information on when to\ncapture the observation used to learn said space. For this, we study multiple\nways, and in particular the signature transform, to encode information about\nthe whole trajectory of observations. The thesis continues with the\nintroduction of the SERENE algorithm, a method that can efficiently focus on\nthe interesting parts of the search space. This method separates the\nexploration of the search space from the exploitation of the reward through a\ntwo-alternating-steps approach. The exploration is performed through NS. Any\ndiscovered reward is then locally exploited through emitters. The third and\nfinal contribution combines TAXONS and SERENE into a single approach: STAX.\nThroughout this thesis, we introduce methods that lower the amount of prior\ninformation needed in sparse rewards settings. These contributions are a\npromising step towards the development of methods that can autonomously explore\nand find high-performance policies in a variety of sparse rewards settings.\n",
        "published": "2022",
        "authors": [
            "Giuseppe Paolo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.01465v1",
        "title": "Deep Q-network using reservoir computing with multi-layered readout",
        "abstract": "  Recurrent neural network (RNN) based reinforcement learning (RL) is used for\nlearning context-dependent tasks and has also attracted attention as a method\nwith remarkable learning performance in recent research. However, RNN-based RL\nhas some issues that the learning procedures tend to be more computationally\nexpensive, and training with backpropagation through time (BPTT) is unstable\nbecause of vanishing/exploding gradients problem. An approach with replay\nmemory introducing reservoir computing has been proposed, which trains an agent\nwithout BPTT and avoids these issues. The basic idea of this approach is that\nobservations from the environment are input to the reservoir network, and both\nthe observation and the reservoir output are stored in the memory. This paper\nshows that the performance of this method improves by using a multi-layered\nneural network for the readout layer, which regularly consists of a single\nlinear layer. The experimental results show that using multi-layered readout\nimproves the learning performance of four classical control tasks that require\ntime-series processing.\n",
        "published": "2022",
        "authors": [
            "Toshitaka Matsuki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.07852v3",
        "title": "Block-Recurrent Transformers",
        "abstract": "  We introduce the Block-Recurrent Transformer, which applies a transformer\nlayer in a recurrent fashion along a sequence, and has linear complexity with\nrespect to sequence length. Our recurrent cell operates on blocks of tokens\nrather than single tokens during training, and leverages parallel computation\nwithin a block in order to make efficient use of accelerator hardware. The cell\nitself is strikingly simple. It is merely a transformer layer: it uses\nself-attention and cross-attention to efficiently compute a recurrent function\nover a large set of state vectors and tokens. Our design was inspired in part\nby LSTM cells, and it uses LSTM-style gates, but it scales the typical LSTM\ncell up by several orders of magnitude. Our implementation of recurrence has\nthe same cost in both computation time and parameter count as a conventional\ntransformer layer, but offers dramatically improved perplexity in language\nmodeling tasks over very long sequences. Our model out-performs a long-range\nTransformer XL baseline by a wide margin, while running twice as fast. We\ndemonstrate its effectiveness on PG19 (books), arXiv papers, and GitHub source\ncode. Our code has been released as open source.\n",
        "published": "2022",
        "authors": [
            "DeLesley Hutchins",
            "Imanol Schlag",
            "Yuhuai Wu",
            "Ethan Dyer",
            "Behnam Neyshabur"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.10093v5",
        "title": "Deep reinforcement learning guided graph neural networks for brain\n  network analysis",
        "abstract": "  Modern neuroimaging techniques, such as diffusion tensor imaging (DTI) and\nfunctional magnetic resonance imaging (fMRI), enable us to model the human\nbrain as a brain network or connectome. Capturing brain networks' structural\ninformation and hierarchical patterns is essential for understanding brain\nfunctions and disease states. Recently, the promising network representation\nlearning capability of graph neural networks (GNNs) has prompted many GNN-based\nmethods for brain network analysis to be proposed. Specifically, these methods\napply feature aggregation and global pooling to convert brain network instances\ninto meaningful low-dimensional representations used for downstream brain\nnetwork analysis tasks. However, existing GNN-based methods often neglect that\nbrain networks of different subjects may require various aggregation iterations\nand use GNN with a fixed number of layers to learn all brain networks.\nTherefore, how to fully release the potential of GNNs to promote brain network\nanalysis is still non-trivial. To solve this problem, we propose a novel brain\nnetwork representation framework, namely BN-GNN, which searches for the optimal\nGNN architecture for each brain network. Concretely, BN-GNN employs deep\nreinforcement learning (DRL) to train a meta-policy to automatically determine\nthe optimal number of feature aggregations (reflected in the number of GNN\nlayers) required for a given brain network. Extensive experiments on eight\nreal-world brain network datasets demonstrate that our proposed BN-GNN improves\nthe performance of traditional GNNs on different brain network analysis tasks.\n",
        "published": "2022",
        "authors": [
            "Xusheng Zhao",
            "Jia Wu",
            "Hao Peng",
            "Amin Beheshti",
            "Jessica J. M. Monaghan",
            "David McAlpine",
            "Heivet Hernandez-Perez",
            "Mark Dras",
            "Qiong Dai",
            "Yangyang Li",
            "Philip S. Yu",
            "Lifang He"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.10941v1",
        "title": "Transfer Dynamics in Emergent Evolutionary Curricula",
        "abstract": "  PINSKY is a system for open-ended learning through neuroevolution in\ngame-based domains. It builds on the Paired Open-Ended Trailblazer (POET)\nsystem, which originally explored learning and environment generation for\nbipedal walkers, and adapts it to games in the General Video Game AI (GVGAI)\nsystem. Previous work showed that by co-evolving levels and neural network\npolicies, levels could be found for which successful policies could not be\ncreated via optimization alone. Studied in the realm of Artificial Life as a\npotentially open-ended alternative to gradient-based fitness, minimal criteria\n(MC)-based selection helps foster diversity in evolutionary populations. The\nmain question addressed by this paper is how the open-ended learning actually\nworks, focusing in particular on the role of transfer of policies from one\nevolutionary branch (\"species\") to another. We analyze the dynamics of the\nsystem through creating phylogenetic trees, analyzing evolutionary trajectories\nof policies, and temporally breaking down transfers according to species type.\nFurthermore, we analyze the impact of the minimal criterion on generated level\ndiversity and inter-species transfer. The most insightful finding is that\ninter-species transfer, while rare, is crucial to the system's success.\n",
        "published": "2022",
        "authors": [
            "Aaron Dharna",
            "Amy K Hoover",
            "Julian Togelius",
            "L. B. Soros"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.11629v1",
        "title": "On Neural Network Equivalence Checking using SMT Solvers",
        "abstract": "  Two pretrained neural networks are deemed equivalent if they yield similar\noutputs for the same inputs. Equivalence checking of neural networks is of\ngreat importance, due to its utility in replacing learning-enabled components\nwith equivalent ones, when there is need to fulfill additional requirements or\nto address security threats, as is the case for example when using knowledge\ndistillation, adversarial training etc. SMT solvers can potentially provide\nsolutions to the problem of neural network equivalence checking that will be\nsound and complete, but as it is expected any such solution is associated with\nsignificant limitations with respect to the size of neural networks to be\nchecked. This work presents a first SMT-based encoding of the equivalence\nchecking problem, explores its utility and limitations and proposes avenues for\nfuture research and improvements towards more scalable and practically\napplicable solutions. We present experimental results that shed light to the\naforementioned issues, for diverse types of neural network models (classifiers\nand regression networks) and equivalence criteria, towards a general and\napplication-independent equivalence checking approach.\n",
        "published": "2022",
        "authors": [
            "Charis Eleftheriadis",
            "Nikolaos Kekatos",
            "Panagiotis Katsaros",
            "Stavros Tripakis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.12026v4",
        "title": "Machine Learning Testing in an ADAS Case Study Using\n  Simulation-Integrated Bio-Inspired Search-Based Testing",
        "abstract": "  This paper presents an extended version of Deeper, a search-based\nsimulation-integrated test solution that generates failure-revealing test\nscenarios for testing a deep neural network-based lane-keeping system. In the\nnewly proposed version, we utilize a new set of bio-inspired search algorithms,\ngenetic algorithm (GA), $({\\mu}+{\\lambda})$ and $({\\mu},{\\lambda})$ evolution\nstrategies (ES), and particle swarm optimization (PSO), that leverage a quality\npopulation seed and domain-specific cross-over and mutation operations tailored\nfor the presentation model used for modeling the test scenarios. In order to\ndemonstrate the capabilities of the new test generators within Deeper, we carry\nout an empirical evaluation and comparison with regard to the results of five\nparticipating tools in the cyber-physical systems testing competition at SBST\n2021. Our evaluation shows the newly proposed test generators in Deeper not\nonly represent a considerable improvement on the previous version but also\nprove to be effective and efficient in provoking a considerable number of\ndiverse failure-revealing test scenarios for testing an ML-driven lane-keeping\nsystem. They can trigger several failures while promoting test scenario\ndiversity, under a limited test time budget, high target failure severity, and\nstrict speed limit constraints.\n",
        "published": "2022",
        "authors": [
            "Mahshid Helali Moghadam",
            "Markus Borg",
            "Mehrdad Saadatmand",
            "Seyed Jalaleddin Mousavirad",
            "Markus Bohlin",
            "Bj\u00f6rn Lisper"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.13108v2",
        "title": "Explainable Artificial Intelligence for Exhaust Gas Temperature of\n  Turbofan Engines",
        "abstract": "  Data-driven modeling is an imperative tool in various industrial\napplications, including many applications in the sectors of aeronautics and\ncommercial aviation. These models are in charge of providing key insights, such\nas which parameters are important on a specific measured outcome or which\nparameter values we should expect to observe given a set of input parameters.\nAt the same time, however, these models rely heavily on assumptions (e.g.,\nstationarity) or are \"black box\" (e.g., deep neural networks), meaning that\nthey lack interpretability of their internal working and can be viewed only in\nterms of their inputs and outputs. An interpretable alternative to the \"black\nbox\" models and with considerably less assumptions is symbolic regression (SR).\nSR searches for the optimal model structure while simultaneously optimizing the\nmodel's parameters without relying on an a-priori model structure. In this\nwork, we apply SR on real-life exhaust gas temperature (EGT) data, collected at\nhigh frequencies through the entire flight, in order to uncover meaningful\nalgebraic relationships between the EGT and other measurable engine parameters.\nThe experimental results exhibit promising model accuracy, as well as\nexplainability returning an absolute difference of 3{\\deg}C compared to the\nground truth and demonstrating consistency from an engineering perspective.\n",
        "published": "2022",
        "authors": [
            "Marios Kefalas",
            "Juan de Santiago Rojo Jr.",
            "Asteris Apostolidis",
            "Dirk van den Herik",
            "Bas van Stein",
            "Thomas B\u00e4ck"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.13190v1",
        "title": "GEMA: An open-source Python library for self-organizing-maps",
        "abstract": "  Organizations have realized the importance of data analysis and its benefits.\nThis in combination with Machine Learning algorithms has allowed to solve\nproblems more easily, making these processes less time-consuming. Neural\nnetworks are the Machine Learning technique that is recently obtaining very\ngood best results. This paper describes an open-source Python library called\nGEMA developed to work with a type of neural network model called\nSelf-Organizing-Maps. GEMA is freely available under GNU General Public License\nat GitHub (https://github.com/ufvceiec/GEMA). The library has been evaluated in\ndifferent a particular use case obtaining accurate results.\n",
        "published": "2022",
        "authors": [
            "Alvaro J. Garcia-Tejedor",
            "Alberto Nogales"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.13392v1",
        "title": "Automated Algorithm Selection: from Feature-Based to Feature-Free\n  Approaches",
        "abstract": "  We propose a novel technique for algorithm-selection, applicable to\noptimisation domains in which there is implicit sequential information\nencapsulated in the data, e.g., in online bin-packing. Specifically we train\ntwo types of recurrent neural networks to predict a packing heuristic in online\nbin-packing, selecting from four well-known heuristics. As input, the RNN\nmethods only use the sequence of item-sizes. This contrasts to typical\napproaches to algorithm-selection which require a model to be trained using\ndomain-specific instance features that need to be first derived from the input\ndata. The RNN approaches are shown to be capable of achieving within 5% of the\noracle performance on between 80.88% to 97.63% of the instances, depending on\nthe dataset. They are also shown to outperform classical machine learning\nmodels trained using derived features. Finally, we hypothesise that the\nproposed methods perform well when the instances exhibit some implicit\nstructure that results in discriminatory performance with respect to a set of\nheuristics. We test this hypothesis by generating fourteen new datasets with\nincreasing levels of structure, and show that there is a critical threshold of\nstructure required before algorithm-selection delivers benefit.\n",
        "published": "2022",
        "authors": [
            "Mohamad Alissa",
            "Kevin Sim",
            "Emma Hart"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.13424v2",
        "title": "Dealing with Sparse Rewards Using Graph Neural Networks",
        "abstract": "  Deep reinforcement learning in partially observable environments is a\ndifficult task in itself, and can be further complicated by a sparse reward\nsignal. Most tasks involving navigation in three-dimensional environments\nprovide the agent with extremely limited information. Typically, the agent\nreceives a visual observation input from the environment and is rewarded once\nat the end of the episode. A good reward function could substantially improve\nthe convergence of reinforcement learning algorithms for such tasks. The\nclassic approach to increase the density of the reward signal is to augment it\nwith supplementary rewards. This technique is called the reward shaping. In\nthis study, we propose two modifications of one of the recent reward shaping\nmethods based on graph convolutional networks: the first involving advanced\naggregation functions, and the second utilizing the attention mechanism. We\nempirically validate the effectiveness of our solutions for the task of\nnavigation in a 3D environment with sparse rewards. For the solution featuring\nattention mechanism, we are also able to show that the learned attention is\nconcentrated on edges corresponding to important transitions in 3D environment.\n",
        "published": "2022",
        "authors": [
            "Matvey Gerasyov",
            "Ilya Makarov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.13573v2",
        "title": "Unsupervised Learning of Temporal Abstractions with Slot-based\n  Transformers",
        "abstract": "  The discovery of reusable sub-routines simplifies decision-making and\nplanning in complex reinforcement learning problems. Previous approaches\npropose to learn such temporal abstractions in a purely unsupervised fashion\nthrough observing state-action trajectories gathered from executing a policy.\nHowever, a current limitation is that they process each trajectory in an\nentirely sequential manner, which prevents them from revising earlier decisions\nabout sub-routine boundary points in light of new incoming information. In this\nwork we propose SloTTAr, a fully parallel approach that integrates sequence\nprocessing Transformers with a Slot Attention module and adaptive computation\nfor learning about the number of such sub-routines in an unsupervised fashion.\nWe demonstrate how SloTTAr is capable of outperforming strong baselines in\nterms of boundary point discovery, even for sequences containing variable\namounts of sub-routines, while being up to 7x faster to train on existing\nbenchmarks.\n",
        "published": "2022",
        "authors": [
            "Anand Gopalakrishnan",
            "Kazuki Irie",
            "J\u00fcrgen Schmidhuber",
            "Sjoerd van Steenkiste"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.16538v1",
        "title": "Machine Learning Approaches for Non-Intrusive Home Absence Detection\n  Based on Appliance Electrical Use",
        "abstract": "  Home absence detection is an emerging field on smart home installations.\nIdentifying whether or not the residents of the house are present, is important\nin numerous scenarios. Possible scenarios include but are not limited to:\nelderly people living alone, people suffering from dementia, home quarantine.\nThe majority of published papers focus on either pressure / door sensors or\ncameras in order to detect outing events. Although the aforementioned\napproaches provide solid results, they are intrusive and require modifications\nfor sensor placement. In our work, appliance electrical use is investigated as\na means for detecting the presence or absence of residents. The energy use is\nthe result of power disaggregation, a non intrusive / non invasive sensing\nmethod. Since a dataset providing energy data and ground truth for home absence\nis not available, artificial outing events were introduced on the UK-DALE\ndataset, a well known dataset for Non Intrusive Load Monitoring (NILM). Several\nmachine learning algorithms were evaluated using the generated dataset.\nBenchmark results have shown that home absence detection using appliance power\nconsumption is feasible.\n",
        "published": "2022",
        "authors": [
            "Athanasios Lentzas",
            "Dimitris Vrakas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.00049v1",
        "title": "AKF-SR: Adaptive Kalman Filtering-based Successor Representation",
        "abstract": "  Recent studies in neuroscience suggest that Successor Representation\n(SR)-based models provide adaptation to changes in the goal locations or reward\nfunction faster than model-free algorithms, together with lower computational\ncost compared to that of model-based algorithms. However, it is not known how\nsuch representation might help animals to manage uncertainty in their\ndecision-making. Existing methods for SR learning do not capture uncertainty\nabout the estimated SR. In order to address this issue, the paper presents a\nKalman filter-based SR framework, referred to as Adaptive Kalman\nFiltering-based Successor Representation (AKF-SR). First, Kalman temporal\ndifference approach, which is a combination of the Kalman filter and the\ntemporal difference method, is used within the AKF-SR framework to cast the SR\nlearning procedure into a filtering problem to benefit from the uncertainty\nestimation of the SR, and also decreases in memory requirement and sensitivity\nto model's parameters in comparison to deep neural network-based algorithms. An\nadaptive Kalman filtering approach is then applied within the proposed AKF-SR\nframework in order to tune the measurement noise covariance and measurement\nmapping function of Kalman filter as the most important parameters affecting\nthe filter's performance. Moreover, an active learning method that exploits the\nestimated uncertainty of the SR to form the behaviour policy leading to more\nvisits to less certain values is proposed to improve the overall performance of\nan agent in terms of received rewards while interacting with its environment.\n",
        "published": "2022",
        "authors": [
            "Parvin Malekzadeh",
            "Mohammad Salimibeni",
            "Ming Hou",
            "Arash Mohammadi",
            "Konstantinos N. Plataniotis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.00619v2",
        "title": "Maze Learning using a Hyperdimensional Predictive Processing Cognitive\n  Architecture",
        "abstract": "  We present the COGnitive Neural GENerative system (CogNGen), a cognitive\narchitecture that combines two neurobiologically-plausible, computational\nmodels: predictive processing and hyperdimensional/vector-symbolic models. We\ndraw inspiration from architectures such as ACT-R and Spaun/Nengo. CogNGen is\nin broad agreement with these, providing a level of detail between ACT-R's\nhigh-level symbolic description of human cognition and Spaun's low-level\nneurobiological description, furthermore creating the groundwork for designing\nagents that learn continually from diverse tasks and model human performance at\nlarger scales than what is possible with current systems. We test CogNGen on\nfour maze-learning tasks, including those that test memory and planning, and\nfind that CogNGen matches performance of deep reinforcement learning models and\nexceeds on a task designed to test memory.\n",
        "published": "2022",
        "authors": [
            "Alexander Ororbia",
            "M. Alex Kelly"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.01852v1",
        "title": "A Data-Driven Framework for Identifying Investment Opportunities in\n  Private Equity",
        "abstract": "  The core activity of a Private Equity (PE) firm is to invest into companies\nin order to provide the investors with profit, usually within 4-7 years. To\ninvest into a company or not is typically done manually by looking at various\nperformance indicators of the company and then making a decision often based on\ninstinct. This process is rather unmanageable given the large number of\ncompanies to potentially invest. Moreover, as more data about company\nperformance indicators becomes available and the number of different indicators\none may want to consider increases, manual crawling and assessment of\ninvestment opportunities becomes inefficient and ultimately impossible. To\naddress these issues, this paper proposes a framework for automated data-driven\nscreening of investment opportunities and thus the recommendation of businesses\nto invest in. The framework draws on data from several sources to assess the\nfinancial and managerial position of a company, and then uses an explainable\nartificial intelligence (XAI) engine to suggest investment recommendations. The\nrobustness of the model is validated using different AI algorithms, class\nimbalance-handling methods, and features extracted from the available data\nsources.\n",
        "published": "2022",
        "authors": [
            "Samantha Petersone",
            "Alwin Tan",
            "Richard Allmendinger",
            "Sujit Roy",
            "James Hales"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.01916v1",
        "title": "Domain-Aware Contrastive Knowledge Transfer for Multi-domain Imbalanced\n  Data",
        "abstract": "  In many real-world machine learning applications, samples belong to a set of\ndomains e.g., for product reviews each review belongs to a product category. In\nthis paper, we study multi-domain imbalanced learning (MIL), the scenario that\nthere is imbalance not only in classes but also in domains. In the MIL setting,\ndifferent domains exhibit different patterns and there is a varying degree of\nsimilarity and divergence among domains posing opportunities and challenges for\ntransfer learning especially when faced with limited or insufficient training\ndata. We propose a novel domain-aware contrastive knowledge transfer method\ncalled DCMI to (1) identify the shared domain knowledge to encourage positive\ntransfer among similar domains (in particular from head domains to tail\ndomains); (2) isolate the domain-specific knowledge to minimize the negative\ntransfer from dissimilar domains. We evaluated the performance of DCMI on three\ndifferent datasets showing significant improvements in different MIL scenarios.\n",
        "published": "2022",
        "authors": [
            "Zixuan Ke",
            "Mohammad Kachuee",
            "Sungjin Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.02183v1",
        "title": "Optimising Communication Overhead in Federated Learning Using NSGA-II",
        "abstract": "  Federated learning is a training paradigm according to which a server-based\nmodel is cooperatively trained using local models running on edge devices and\nensuring data privacy. These devices exchange information that induces a\nsubstantial communication load, which jeopardises the functioning efficiency.\nThe difficulty of reducing this overhead stands in achieving this without\ndecreasing the model's efficiency (contradictory relation). To do so, many\nworks investigated the compression of the pre/mid/post-trained models and the\ncommunication rounds, separately, although they jointly contribute to the\ncommunication overload. Our work aims at optimising communication overhead in\nfederated learning by (I) modelling it as a multi-objective problem and (II)\napplying a multi-objective optimization algorithm (NSGA-II) to solve it. To the\nbest of the author's knowledge, this is the first work that \\texttt{(I)}\nexplores the add-in that evolutionary computation could bring for solving such\na problem, and \\texttt{(II)} considers both the neuron and devices features\ntogether. We perform the experimentation by simulating a server/client\narchitecture with 4 slaves. We investigate both convolutional and\nfully-connected neural networks with 12 and 3 layers, 887,530 and 33,400\nweights, respectively. We conducted the validation on the \\texttt{MNIST}\ndataset containing 70,000 images. The experiments have shown that our proposal\ncould reduce communication by 99% and maintain an accuracy equal to the one\nobtained by the FedAvg Algorithm that uses 100% of communications.\n",
        "published": "2022",
        "authors": [
            "Jos\u00e9 \u00c1ngel Morell",
            "Zakaria Abdelmoiz Dahi",
            "Francisco Chicano",
            "Gabriel Luque",
            "Enrique Alba"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.03899v1",
        "title": "Optimizing Coordinative Schedules for Tanker Terminals: An Intelligent\n  Large Spatial-Temporal Data-Driven Approach -- Part 1",
        "abstract": "  In this study, a novel coordinative scheduling optimization approach is\nproposed to enhance port efficiency by reducing average wait time and\nturnaround time. The proposed approach consists of enhanced particle swarm\noptimization (ePSO) as kernel and augmented firefly algorithm (AFA) as global\noptimal search. Two paradigm methods of the proposed approach are investigated,\nwhich are batch method and rolling horizon method. The experimental results\nshow that both paradigm methods of proposed approach can effectively enhance\nport efficiency. The average wait time could be significantly reduced by 86.0%\n- 95.5%, and the average turnaround time could eventually save 38.2% - 42.4%\nwith respect to historical benchmarks. Moreover, the paradigm method of rolling\nhorizon could reduce to 20 mins on running time over 3-month datasets, rather\nthan 4 hrs on batch method at corresponding maximum performance.\n",
        "published": "2022",
        "authors": [
            "Deqing Zhai",
            "Xiuju Fu",
            "Xiao Feng Yin",
            "Haiyan Xu",
            "Wanbing Zhang",
            "Ning Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.03955v1",
        "title": "Optimizing Coordinative Schedules for Tanker Terminals: An Intelligent\n  Large Spatial-Temporal Data-Driven Approach -- Part 2",
        "abstract": "  In this study, a novel coordinative scheduling optimization approach is\nproposed to enhance port efficiency by reducing weighted average turnaround\ntime. The proposed approach is developed as a heuristic algorithm applied and\ninvestigated through different observation windows with weekly rolling horizon\nparadigm method. The experimental results show that the proposed approach is\neffective and promising on mitigating the turnaround time of vessels. The\nresults demonstrate that largest potential savings of turnaround time (weighted\naverage) are around 17 hours (28%) reduction on baseline of 1-week observation,\n45 hours (37%) reduction on baseline of 2-week observation and 70 hours (40%)\nreduction on baseline of 3-week observation. Even though the experimental\nresults are based on historical datasets, the results potentially present\nsignificant benefits if real-time applications were applied under a quadratic\ncomputational complexity.\n",
        "published": "2022",
        "authors": [
            "Deqing Zhai",
            "Xiuju Fu",
            "Xiao Feng Yin",
            "Haiyan Xu",
            "Wanbing Zhang",
            "Ning Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.04085v2",
        "title": "Predicting Berth Stay for Tanker Terminals: A Systematic and Dynamic\n  Approach",
        "abstract": "  Given the trend of digitization and increasing number of maritime transport,\nprediction of vessel berth stay has been triggered for requirements of\noperation research and scheduling optimization problem in the era of maritime\nbig data, which takes a significant part in port efficiency and maritime\nlogistics enhancement. This study proposes a systematic and dynamic approach of\npredicting berth stay for tanker terminals. The approach covers three\ninnovative aspects: 1) Data source employed is multi-faceted, including cargo\noperation data from tanker terminals, time-series data from automatic\nidentification system (AIS), etc. 2) The process of berth stay is decomposed\ninto multiple blocks according to data analysis and information extraction\ninnovatively, and practical operation scenarios are also developed accordingly.\n3) The predictive models of berth stay are developed on the basis of prior data\nanalysis and information extraction under two methods, including regression and\ndecomposed distribution. The models are evaluated under four dynamic scenarios\nwith certain designated cargoes among two different terminals. The evaluation\nresults show that the proposed approach can predict berth stay with the\naccuracy up to 98.81% validated by historical baselines, and also demonstrate\nthe proposed approach has dynamic capability of predicting berth stay among the\nscenarios. The model may be potentially applied for short-term pilot-booking or\nscheduling optimizations within a reasonable time frame for advancement of port\nintelligence and logistics efficiency.\n",
        "published": "2022",
        "authors": [
            "Deqing Zhai",
            "Xiuju Fu",
            "Xiao Feng Yin",
            "Haiyan Xu",
            "Wanbing Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.04338v1",
        "title": "Fuzzy temporal convolutional neural networks in P300-based\n  Brain-computer interface for smart home interaction",
        "abstract": "  The processing and classification of electroencephalographic signals (EEG)\nare increasingly performed using deep learning frameworks, such as\nconvolutional neural networks (CNNs), to generate abstract features from brain\ndata, automatically paving the way for remarkable classification prowess.\nHowever, EEG patterns exhibit high variability across time and uncertainty due\nto noise. It is a significant problem to be addressed in P300-based Brain\nComputer Interface (BCI) for smart home interaction. It operates in a\nnon-optimal natural environment where added noise is often present. In this\nwork, we propose a sequential unification of temporal convolutional networks\n(TCNs) modified to EEG signals, LSTM cells, with a fuzzy neural block (FNB),\nwhich we called EEG-TCFNet. Fuzzy components may enable a higher tolerance to\nnoisy conditions. We applied three different architectures comparing the effect\nof using block FNB to classify a P300 wave to build a BCI for smart home\ninteraction with healthy and post-stroke individuals. Our results reported a\nmaximum classification accuracy of 98.6% and 74.3% using the proposed method of\nEEG-TCFNet in subject-dependent strategy and subject-independent strategy,\nrespectively. Overall, FNB usage in all three CNN topologies outperformed those\nwithout FNB. In addition, we compared the addition of FNB to other\nstate-of-the-art methods and obtained higher classification accuracies on\naccount of the integration with FNB. The remarkable performance of the proposed\nmodel, EEG-TCFNet, and the general integration of fuzzy units to other\nclassifiers would pave the way for enhanced P300-based BCIs for smart home\ninteraction within natural settings.\n",
        "published": "2022",
        "authors": [
            "Christian Flores Vega",
            "Jonathan Quevedo",
            "Elmer Escand\u00f3n",
            "Mehrin Kiani",
            "Weiping Ding",
            "Javier Andreu-Perez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.06250v3",
        "title": "Large-scale multi-objective influence maximisation with network\n  downscaling",
        "abstract": "  Finding the most influential nodes in a network is a computationally hard\nproblem with several possible applications in various kinds of network-based\nproblems. While several methods have been proposed for tackling the influence\nmaximisation (IM) problem, their runtime typically scales poorly when the\nnetwork size increases. Here, we propose an original method, based on network\ndownscaling, that allows a multi-objective evolutionary algorithm (MOEA) to\nsolve the IM problem on a reduced scale network, while preserving the relevant\nproperties of the original network. The downscaled solution is then upscaled to\nthe original network, using a mechanism based on centrality metrics such as\nPageRank. Our results on eight large networks (including two with $\\sim$50k\nnodes) demonstrate the effectiveness of the proposed method with a more than\n10-fold runtime gain compared to the time needed on the original network, and\nan up to $82\\%$ time reduction compared to CELF.\n",
        "published": "2022",
        "authors": [
            "Elia Cunegatti",
            "Giovanni Iacca",
            "Doina Bucur"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.10231v1",
        "title": "Handling Imbalanced Classification Problems With Support Vector Machines\n  via Evolutionary Bilevel Optimization",
        "abstract": "  Support vector machines (SVMs) are popular learning algorithms to deal with\nbinary classification problems. They traditionally assume equal\nmisclassification costs for each class; however, real-world problems may have\nan uneven class distribution. This article introduces EBCS-SVM: evolutionary\nbilevel cost-sensitive SVMs. EBCS-SVM handles imbalanced classification\nproblems by simultaneously learning the support vectors and optimizing the SVM\nhyperparameters, which comprise the kernel parameter and misclassification\ncosts. The resulting optimization problem is a bilevel problem, where the lower\nlevel determines the support vectors and the upper level the hyperparameters.\nThis optimization problem is solved using an evolutionary algorithm (EA) at the\nupper level and sequential minimal optimization (SMO) at the lower level. These\ntwo methods work in a nested fashion, that is, the optimal support vectors help\nguide the search of the hyperparameters, and the lower level is initialized\nbased on previous successful solutions. The proposed method is assessed using\n70 datasets of imbalanced classification and compared with several\nstate-of-the-art methods. The experimental results, supported by a Bayesian\ntest, provided evidence of the effectiveness of EBCS-SVM when working with\nhighly imbalanced datasets.\n",
        "published": "2022",
        "authors": [
            "Alejandro Rosales-P\u00e9rez",
            "Salvador Garc\u00eda",
            "Francisco Herrera"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.10438v3",
        "title": "EVOTER: Evolution of Transparent Explainable Rule-sets",
        "abstract": "  Most AI systems are black boxes generating reasonable outputs for given\ninputs. Some domains, however, have explainability and trustworthiness\nrequirements that cannot be directly met by these approaches. Various methods\nhave therefore been developed to interpret black-box models after training.\nThis paper advocates an alternative approach where the models are transparent\nand explainable to begin with. This approach, EVOTER, evolves rule-sets based\non simple logical expressions. The approach is evaluated in several\nprediction/classification and prescription/policy search domains with and\nwithout a surrogate. It is shown to discover meaningful rule sets that perform\nsimilarly to black-box models. The rules can provide insight into the domain,\nand make biases hidden in the data explicit. It may also be possible to edit\nthem directly to remove biases and add constraints. EVOTER thus forms a\npromising foundation for building trustworthy AI systems for real-world\napplications in the future.\n",
        "published": "2022",
        "authors": [
            "Hormoz Shahrzad",
            "Babak Hodjat",
            "Risto Miikkulainen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.11674v1",
        "title": "HyperNCA: Growing Developmental Networks with Neural Cellular Automata",
        "abstract": "  In contrast to deep reinforcement learning agents, biological neural networks\nare grown through a self-organized developmental process. Here we propose a new\nhypernetwork approach to grow artificial neural networks based on neural\ncellular automata (NCA). Inspired by self-organising systems and\ninformation-theoretic approaches to developmental biology, we show that our\nHyperNCA method can grow neural networks capable of solving common\nreinforcement learning tasks. Finally, we explore how the same approach can be\nused to build developmental metamorphosis networks capable of transforming\ntheir weights to solve variations of the initial RL task.\n",
        "published": "2022",
        "authors": [
            "Elias Najarro",
            "Shyam Sudhakaran",
            "Claire Glanois",
            "Sebastian Risi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.11843v2",
        "title": "A Computational Theory of Learning Flexible Reward-Seeking Behavior with\n  Place Cells",
        "abstract": "  An important open question in computational neuroscience is how various\nspatially tuned neurons, such as place cells, are used to support the learning\nof reward-seeking behavior of an animal. Existing computational models either\nlack biological plausibility or fall short of behavioral flexibility when\nenvironments change. In this paper, we propose a computational theory that\nachieves behavioral flexibility with better biological plausibility. We first\ntrain a mixture of Gaussian distributions to model the ensemble of firing\nfields of place cells. Then we propose a Hebbian-like rule to learn the\nsynaptic strength matrix among place cells. This matrix is interpreted as the\ntransition rate matrix of a continuous time Markov chain to generate the\nsequential replay of place cells. During replay, the synaptic strengths from\nplace cells to medium spiny neurons (MSN) are learned by a temporal-difference\nlike rule to store place-reward associations. After replay, the activation of\nMSN will ramp up when an animal approaches the rewarding place, so the animal\ncan move along the direction where the MSN activation is increasing to find the\nrewarding place. We implement our theory into a high-fidelity virtual rat in\nthe MuJoCo physics simulator. In a complex maze, the rat shows significantly\nbetter learning efficiency and behavioral flexibility than a rat that\nimplements a neuroscience-inspired reinforcement learning algorithm, deep\nQ-network.\n",
        "published": "2022",
        "authors": [
            "Yuanxiang Gao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.12244v1",
        "title": "Hybridised Loss Functions for Improved Neural Network Generalisation",
        "abstract": "  Loss functions play an important role in the training of artificial neural\nnetworks (ANNs), and can affect the generalisation ability of the ANN model,\namong other properties. Specifically, it has been shown that the cross entropy\nand sum squared error loss functions result in different training dynamics, and\nexhibit different properties that are complementary to one another. It has\npreviously been suggested that a hybrid of the entropy and sum squared error\nloss functions could combine the advantages of the two functions, while\nlimiting their disadvantages. The effectiveness of such hybrid loss functions\nis investigated in this study. It is shown that hybridisation of the two loss\nfunctions improves the generalisation ability of the ANNs on all problems\nconsidered. The hybrid loss function that starts training with the sum squared\nerror loss function and later switches to the cross entropy error loss function\nis shown to either perform the best on average, or to not be significantly\ndifferent than the best loss function tested for all problems considered. This\nstudy shows that the minima discovered by the sum squared error loss function\ncan be further exploited by switching to cross entropy error loss function. It\ncan thus be concluded that hybridisation of the two loss functions could lead\nto better performance in ANNs.\n",
        "published": "2022",
        "authors": [
            "Matthew C. Dickson",
            "Anna S. Bosman",
            "Katherine M. Malan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.13250v1",
        "title": "Watts: Infrastructure for Open-Ended Learning",
        "abstract": "  This paper proposes a framework called Watts for implementing, comparing, and\nrecombining open-ended learning (OEL) algorithms. Motivated by modularity and\nalgorithmic flexibility, Watts atomizes the components of OEL systems to\npromote the study of and direct comparisons between approaches. Examining\nimplementations of three OEL algorithms, the paper introduces the modules of\nthe framework. The hope is for Watts to enable benchmarking and to explore new\ntypes of OEL algorithms. The repo is available at\n\\url{https://github.com/aadharna/watts}\n",
        "published": "2022",
        "authors": [
            "Aaron Dharna",
            "Charlie Summers",
            "Rohin Dasari",
            "Julian Togelius",
            "Amy K. Hoover"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.14008v1",
        "title": "Biologically-inspired neuronal adaptation improves learning in neural\n  networks",
        "abstract": "  Since humans still outperform artificial neural networks on many tasks,\ndrawing inspiration from the brain may help to improve current machine learning\nalgorithms. Contrastive Hebbian Learning (CHL) and Equilibrium Propagation (EP)\nare biologically plausible algorithms that update weights using only local\ninformation (without explicitly calculating gradients) and still achieve\nperformance comparable to conventional backpropagation. In this study, we\naugmented CHL and EP with Adjusted Adaptation, inspired by the adaptation\neffect observed in neurons, in which a neuron's response to a given stimulus is\nadjusted after a short time. We add this adaptation feature to multilayer\nperceptrons and convolutional neural networks trained on MNIST and CIFAR-10.\nSurprisingly, adaptation improved the performance of these networks. We discuss\nthe biological inspiration for this idea and investigate why Neuronal\nAdaptation could be an important brain mechanism to improve the stability and\naccuracy of learning.\n",
        "published": "2022",
        "authors": [
            "Yoshimasa Kubo",
            "Eric Chalmers",
            "Artur Luczak"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.01625v1",
        "title": "Toward Robust Spiking Neural Network Against Adversarial Perturbation",
        "abstract": "  As spiking neural networks (SNNs) are deployed increasingly in real-world\nefficiency critical applications, the security concerns in SNNs attract more\nattention. Currently, researchers have already demonstrated an SNN can be\nattacked with adversarial examples. How to build a robust SNN becomes an urgent\nissue. Recently, many studies apply certified training in artificial neural\nnetworks (ANNs), which can improve the robustness of an NN model promisely.\nHowever, existing certifications cannot transfer to SNNs directly because of\nthe distinct neuron behavior and input formats for SNNs. In this work, we first\ndesign S-IBP and S-CROWN that tackle the non-linear functions in SNNs' neuron\nmodeling. Then, we formalize the boundaries for both digital and spike inputs.\nFinally, we demonstrate the efficiency of our proposed robust training method\nin different datasets and model architectures. Based on our experiment, we can\nachieve a maximum $37.7\\%$ attack error reduction with $3.7\\%$ original\naccuracy loss. To the best of our knowledge, this is the first analysis on\nrobust training of SNNs.\n",
        "published": "2022",
        "authors": [
            "Ling Liang",
            "Kaidi Xu",
            "Xing Hu",
            "Lei Deng",
            "Yuan Xie"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.02100v1",
        "title": "MAD: Self-Supervised Masked Anomaly Detection Task for Multivariate Time\n  Series",
        "abstract": "  In this paper, we introduce Masked Anomaly Detection (MAD), a general\nself-supervised learning task for multivariate time series anomaly detection.\nWith the increasing availability of sensor data from industrial systems, being\nable to detecting anomalies from streams of multivariate time series data is of\nsignificant importance. Given the scarcity of anomalies in real-world\napplications, the majority of literature has been focusing on modeling\nnormality. The learned normal representations can empower anomaly detection as\nthe model has learned to capture certain key underlying data regularities. A\ntypical formulation is to learn a predictive model, i.e., use a window of time\nseries data to predict future data values. In this paper, we propose an\nalternative self-supervised learning task. By randomly masking a portion of the\ninputs and training a model to estimate them using the remaining ones, MAD is\nan improvement over the traditional left-to-right next step prediction (NSP)\ntask. Our experimental results demonstrate that MAD can achieve better anomaly\ndetection rates over traditional NSP approaches when using exactly the same\nneural network (NN) base models, and can be modified to run as fast as NSP\nmodels during test time on the same hardware, thus making it an ideal upgrade\nfor many existing NSP-based NN anomaly detection models.\n",
        "published": "2022",
        "authors": [
            "Yiwei Fu",
            "Feng Xue"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.02115v1",
        "title": "Axonal Delay As a Short-Term Memory for Feed Forward Deep Spiking Neural\n  Networks",
        "abstract": "  The information of spiking neural networks (SNNs) are propagated between the\nadjacent biological neuron by spikes, which provides a computing paradigm with\nthe promise of simulating the human brain. Recent studies have found that the\ntime delay of neurons plays an important role in the learning process.\nTherefore, configuring the precise timing of the spike is a promising direction\nfor understanding and improving the transmission process of temporal\ninformation in SNNs. However, most of the existing learning methods for spiking\nneurons are focusing on the adjustment of synaptic weight, while very few\nresearch has been working on axonal delay. In this paper, we verify the\neffectiveness of integrating time delay into supervised learning and propose a\nmodule that modulates the axonal delay through short-term memory. To this end,\na rectified axonal delay (RAD) module is integrated with the spiking model to\nalign the spike timing and thus improve the characterization learning ability\nof temporal features. Experiments on three neuromorphic benchmark datasets :\nNMNIST, DVS Gesture and N-TIDIGITS18 show that the proposed method achieves the\nstate-of-the-art performance while using the fewest parameters.\n",
        "published": "2022",
        "authors": [
            "Pengfei Sun",
            "Longwei Zhu",
            "Dick Botteldooren"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.03162v1",
        "title": "Geodesics, Non-linearities and the Archive of Novelty Search",
        "abstract": "  The Novelty Search (NS) algorithm was proposed more than a decade ago.\nHowever, the mechanisms behind its empirical success are still not well\nformalized/understood. This short note focuses on the effects of the archive on\nexploration. Experimental evidence from a few application domains suggests that\narchive-based NS performs in general better than when Novelty is solely\ncomputed with respect to the population. An argument that is often encountered\nin the literature is that the archive prevents exploration from backtracking or\ncycling, i.e. from revisiting previously encountered areas in the behavior\nspace. We argue that this is not a complete or accurate explanation as\nbacktracking - beside often being desirable - can actually be enabled by the\narchive. Through low-dimensional/analytical examples, we show that a key effect\nof the archive is that it counterbalances the exploration biases that result,\namong other factors, from the use of inadequate behavior metrics and the\nnon-linearities of the behavior mapping. Our observations seem to hint that\nattributing a more active role to the archive in sampling can be beneficial.\n",
        "published": "2022",
        "authors": [
            "Achkan Salehi",
            "Alexandre Coninx",
            "Stephane Doncieux"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.04586v1",
        "title": "Towards Optimal VPU Compiler Cost Modeling by using Neural Networks to\n  Infer Hardware Performances",
        "abstract": "  Calculating the most efficient schedule of work in a neural network compiler\nis a difficult task. There are many parameters to be accounted for that can\npositively or adversely affect that schedule depending on their configuration -\nHow work is shared between distributed targets, the subdivision of tensors to\nfit in memory, toggling the enablement of optimizations, etc. Traditionally,\nneural network compilers determine how to set these values by building a graph\nof choices and choosing the path with minimal 'cost'. These choices and their\ncorresponding costs are usually determined by an algorithm crafted by engineers\nwith a deep knowledge of the target platform. However, when the amount of\noptions available to a compiler is large, it is very difficult to ensure that\nthese models consistently produce an optimal schedule for all scenarios, whilst\nstill completing compilation in an acceptable timeframe. This paper presents\n'VPUNN' - a neural network-based cost model trained on low-level task profiling\nthat consistently outperforms the state-of-the-art cost modeling in Intel's\nline of VPU processors.\n",
        "published": "2022",
        "authors": [
            "Ian Frederick Vigogne Goodbody Hunter",
            "Alessandro Palla",
            "Sebastian Eusebiu Nagy",
            "Richard Richmond",
            "Kyle McAdoo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.06978v3",
        "title": "Efficient Off-Policy Reinforcement Learning via Brain-Inspired Computing",
        "abstract": "  Reinforcement Learning (RL) has opened up new opportunities to enhance\nexisting smart systems that generally include a complex decision-making\nprocess. However, modern RL algorithms, e.g., Deep Q-Networks (DQN), are based\non deep neural networks, resulting in high computational costs. In this paper,\nwe propose QHD, an off-policy value-based Hyperdimensional Reinforcement\nLearning, that mimics brain properties toward robust and real-time learning.\nQHD relies on a lightweight brain-inspired model to learn an optimal policy in\nan unknown environment. On both desktop and power-limited embedded platforms,\nQHD achieves significantly better overall efficiency than DQN while providing\nhigher or comparable rewards. QHD is also suitable for highly-efficient\nreinforcement learning with great potential for online and real-time learning.\nOur solution supports a small experience replay batch size that provides 12.3\ntimes speedup compared to DQN while ensuring minimal quality loss. Our\nevaluation shows QHD capability for real-time learning, providing 34.6 times\nspeedup and significantly better quality of learning than DQN.\n",
        "published": "2022",
        "authors": [
            "Yang Ni",
            "Danny Abraham",
            "Mariam Issa",
            "Yeseong Kim",
            "Pietro Mercati",
            "Mohsen Imani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.07292v3",
        "title": "A Computational Framework of Cortical Microcircuits Approximates\n  Sign-concordant Random Backpropagation",
        "abstract": "  Several recent studies attempt to address the biological implausibility of\nthe well-known backpropagation (BP) method. While promising methods such as\nfeedback alignment, direct feedback alignment, and their variants like\nsign-concordant feedback alignment tackle BP's weight transport problem, their\nvalidity remains controversial owing to a set of other unsolved issues. In this\nwork, we answer the question of whether it is possible to realize random\nbackpropagation solely based on mechanisms observed in neuroscience. We propose\na hypothetical framework consisting of a new microcircuit architecture and its\nsupporting Hebbian learning rules. Comprising three types of cells and two\ntypes of synaptic connectivity, the proposed microcircuit architecture computes\nand propagates error signals through local feedback connections and supports\nthe training of multi-layered spiking neural networks with a globally defined\nspiking error function. We employ the Hebbian rule operating in local\ncompartments to update synaptic weights and achieve supervised learning in a\nbiologically plausible manner. Finally, we interpret the proposed framework\nfrom an optimization point of view and show its equivalence to sign-concordant\nfeedback alignment. The proposed framework is benchmarked on several datasets\nincluding MNIST and CIFAR10, demonstrating promising BP-comparable accuracy.\n",
        "published": "2022",
        "authors": [
            "Yukun Yang",
            "Peng Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.07592v1",
        "title": "Qualitative Differences Between Evolutionary Strategies and\n  Reinforcement Learning Methods for Control of Autonomous Agents",
        "abstract": "  In this paper we analyze the qualitative differences between evolutionary\nstrategies and reinforcement learning algorithms by focusing on two popular\nstate-of-the-art algorithms: the OpenAI-ES evolutionary strategy and the\nProximal Policy Optimization (PPO) reinforcement learning algorithm -- the most\nsimilar methods of the two families. We analyze how the methods differ with\nrespect to: (i) general efficacy, (ii) ability to cope with sparse rewards,\n(iii) propensity/capacity to discover minimal solutions, (iv) dependency on\nreward shaping, and (v) ability to cope with variations of the environmental\nconditions. The analysis of the performance and of the behavioral strategies\ndisplayed by the agents trained with the two methods on benchmark problems\nenable us to demonstrate qualitative differences which were not identified in\nprevious studies, to identify the relative weakness of the two methods, and to\npropose ways to ameliorate some of those weakness. We show that the\ncharacteristics of the reward function has a strong impact which vary\nqualitatively not only for the OpenAI-ES and the PPO but also for alternative\nreinforcement learning algorithms, thus demonstrating the importance of\noptimizing the characteristic of the reward function to the algorithm used.\n",
        "published": "2022",
        "authors": [
            "Nicola Milano",
            "Stefano Nolfi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.07868v1",
        "title": "Minimal Neural Network Models for Permutation Invariant Agents",
        "abstract": "  Organisms in nature have evolved to exhibit flexibility in face of changes to\nthe environment and/or to themselves. Artificial neural networks (ANNs) have\nproven useful for controlling of artificial agents acting in environments.\nHowever, most ANN models used for reinforcement learning-type tasks have a\nrigid structure that does not allow for varying input sizes. Further, they fail\ncatastrophically if inputs are presented in an ordering unseen during\noptimization. We find that these two ANN inflexibilities can be mitigated and\ntheir solutions are simple and highly related. For permutation invariance, no\noptimized parameters can be tied to a specific index of the input elements. For\nsize invariance, inputs must be projected onto a common space that does not\ngrow with the number of projections. Based on these restrictions, we construct\na conceptually simple model that exhibit flexibility most ANNs lack. We\ndemonstrate the model's properties on multiple control problems, and show that\nit can cope with even very rapid permutations of input indices, as well as\nchanges in input size. Ablation studies show that is possible to achieve these\nproperties with simple feedforward structures, but that it is much easier to\noptimize recurrent structures.\n",
        "published": "2022",
        "authors": [
            "Joachim Winther Pedersen",
            "Sebastian Risi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.08358v1",
        "title": "Perturbation of Deep Autoencoder Weights for Model Compression and\n  Classification of Tabular Data",
        "abstract": "  Fully connected deep neural networks (DNN) often include redundant weights\nleading to overfitting and high memory requirements. Additionally, the\nperformance of DNN is often challenged by traditional machine learning models\nin tabular data classification. In this paper, we propose periodical\nperturbations (prune and regrow) of DNN weights, especially at the\nself-supervised pre-training stage of deep autoencoders. The proposed weight\nperturbation strategy outperforms dropout learning in four out of six tabular\ndata sets in downstream classification tasks. The L1 or L2 regularization of\nweights at the same pretraining stage results in inferior classification\nperformance compared to dropout or our weight perturbation routine. Unlike\ndropout learning, the proposed weight perturbation routine additionally\nachieves 15% to 40% sparsity across six tabular data sets for the compression\nof deep pretrained models. Our experiments reveal that a pretrained deep\nautoencoder with weight perturbation or dropout can outperform traditional\nmachine learning in tabular data classification when fully connected DNN fails\nmiserably. However, traditional machine learning models appear superior to any\ndeep models when a tabular data set contains uncorrelated variables. Therefore,\nthe success of deep models can be attributed to the inevitable presence of\ncorrelated variables in real-world data sets.\n",
        "published": "2022",
        "authors": [
            "Manar Samad",
            "Sakib Abrar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.08645v1",
        "title": "Need is All You Need: Homeostatic Neural Networks Adapt to Concept Shift",
        "abstract": "  In living organisms, homeostasis is the natural regulation of internal states\naimed at maintaining conditions compatible with life. Typical artificial\nsystems are not equipped with comparable regulatory features. Here, we\nintroduce an artificial neural network that incorporates homeostatic features.\nIts own computing substrate is placed in a needful and vulnerable relation to\nthe very objects over which it computes. For example, artificial neurons\nperforming classification of MNIST digits or Fashion-MNIST articles of clothing\nmay receive excitatory or inhibitory effects, which alter their own learning\nrate as a direct result of perceiving and classifying the digits. In this\nscenario, accurate recognition is desirable to the agent itself because it\nguides decisions to regulate its vulnerable internal states and functionality.\nCounterintuitively, the addition of vulnerability to a learner does not\nnecessarily impair its performance. On the contrary, self-regulation in\nresponse to vulnerability confers benefits under certain conditions. We show\nthat homeostatic design confers increased adaptability under concept shift, in\nwhich the relationships between labels and data change over time, and that the\ngreatest advantages are obtained under the highest rates of shift. This\nnecessitates the rapid un-learning of past associations and the re-learning of\nnew ones. We also demonstrate the superior abilities of homeostatic learners in\nenvironments with dynamically changing rates of concept shift. Our homeostatic\ndesign exposes the artificial neural network's thinking machinery to the\nconsequences of its own \"thoughts\", illustrating the advantage of putting one's\nown \"skin in the game\" to improve fluid intelligence.\n",
        "published": "2022",
        "authors": [
            "Kingson Man",
            "Antonio Damasio",
            "Hartmut Neven"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.10116v1",
        "title": "Evolving SimGANs to Improve Abnormal Electrocardiogram Classification",
        "abstract": "  Machine Learning models are used in a wide variety of domains. However,\nmachine learning methods often require a large amount of data in order to be\nsuccessful. This is especially troublesome in domains where collecting\nreal-world data is difficult and/or expensive. Data simulators do exist for\nmany of these domains, but they do not sufficiently reflect the real world data\ndue to factors such as a lack of real-world noise. Recently generative\nadversarial networks (GANs) have been modified to refine simulated image data\ninto data that better fits the real world distribution, using the SimGAN\nmethod. While evolutionary computing has been used for GAN evolution, there are\ncurrently no frameworks that can evolve a SimGAN. In this paper we (1) extend\nthe SimGAN method to refine one-dimensional data, (2) modify Easy Cartesian\nGenetic Programming (ezCGP), an evolutionary computing framework, to create\nSimGANs that more accurately refine simulated data, and (3) create new\nfeature-based quantitative metrics to evaluate refined data. We also use our\nframework to augment an electrocardiogram (ECG) dataset, a domain that suffers\nfrom the issues previously mentioned. In particular, while healthy ECGs can be\nsimulated there are no current simulators of abnormal ECGs. We show that by\nusing an evolved SimGAN to refine simulated healthy ECG data to mimic\nreal-world abnormal ECGs, we can improve the accuracy of abnormal ECG\nclassifiers.\n",
        "published": "2022",
        "authors": [
            "Gabriel Wang",
            "Anish Thite",
            "Rodd Talebi",
            "Anthony D'Achille",
            "Alex Mussa",
            "Jason Zutty"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.10118v1",
        "title": "An Artificial Neural Network Functionalized by Evolution",
        "abstract": "  The topology of artificial neural networks has a significant effect on their\nperformance. Characterizing efficient topology is a field of promising research\nin Artificial Intelligence. However, it is not a trivial task and it is mainly\nexperimented on through convolutional neural networks. We propose a hybrid\nmodel which combines the tensor calculus of feed-forward neural networks with\nPseudo-Darwinian mechanisms. This allows for finding topologies that are well\nadapted for elaboration of strategies, control problems or pattern recognition\ntasks. In particular, the model can provide adapted topologies at early\nevolutionary stages, and 'structural convergence', which can found applications\nin robotics, big-data and artificial life.\n",
        "published": "2022",
        "authors": [
            "Fabien Furfaro",
            "Avner Bar-Hen",
            "Geoffroy Berthelot"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.10320v1",
        "title": "Nothing makes sense in deep learning, except in the light of evolution",
        "abstract": "  Deep Learning (DL) is a surprisingly successful branch of machine learning.\nThe success of DL is usually explained by focusing analysis on a particular\nrecent algorithm and its traits. Instead, we propose that an explanation of the\nsuccess of DL must look at the population of all algorithms in the field and\nhow they have evolved over time. We argue that cultural evolution is a useful\nframework to explain the success of DL. In analogy to biology, we use\n`development' to mean the process converting the pseudocode or text description\nof an algorithm into a fully trained model. This includes writing the\nprogramming code, compiling and running the program, and training the model. If\nall parts of the process don't align well then the resultant model will be\nuseless (if the code runs at all!). This is a constraint. A core component of\nevolutionary developmental biology is the concept of deconstraints -- these are\nmodification to the developmental process that avoid complete failure by\nautomatically accommodating changes in other components. We suggest that many\nimportant innovations in DL, from neural networks themselves to hyperparameter\noptimization and AutoGrad, can be seen as developmental deconstraints. These\ndeconstraints can be very helpful to both the particular algorithm in how it\nhandles challenges in implementation and the overall field of DL in how easy it\nis for new ideas to be generated. We highlight how our perspective can both\nadvance DL and lead to new insights for evolutionary biology.\n",
        "published": "2022",
        "authors": [
            "Artem Kaznatcheev",
            "Konrad Paul Kording"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.11636v1",
        "title": "Forecasting of Non-Stationary Sales Time Series Using Deep Learning",
        "abstract": "  The paper describes the deep learning approach for forecasting non-stationary\ntime series with using time trend correction in a neural network model. Along\nwith the layers for predicting sales values, the neural network model includes\na subnetwork block for the prediction weight for a time trend term which is\nadded to a predicted sales value. The time trend term is considered as a\nproduct of the predicted weight value and normalized time value. The results\nshow that the forecasting accuracy can be essentially improved for\nnon-stationary sales with time trends using the trend correction block in the\ndeep learning model.\n",
        "published": "2022",
        "authors": [
            "Bohdan M. Pavlyshenko"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.12295v1",
        "title": "lpSpikeCon: Enabling Low-Precision Spiking Neural Network Processing for\n  Efficient Unsupervised Continual Learning on Autonomous Agents",
        "abstract": "  Recent advances have shown that SNN-based systems can efficiently perform\nunsupervised continual learning due to their bio-plausible learning rule, e.g.,\nSpike-Timing-Dependent Plasticity (STDP). Such learning capabilities are\nespecially beneficial for use cases like autonomous agents (e.g., robots and\nUAVs) that need to continuously adapt to dynamically changing\nscenarios/environments, where new data gathered directly from the environment\nmay have novel features that should be learned online. Current state-of-the-art\nworks employ high-precision weights (i.e., 32 bit) for both training and\ninference phases, which pose high memory and energy costs thereby hindering\nefficient embedded implementations of such systems for battery-driven mobile\nautonomous systems. On the other hand, precision reduction may jeopardize the\nquality of unsupervised continual learning due to information loss. Towards\nthis, we propose lpSpikeCon, a novel methodology to enable low-precision SNN\nprocessing for efficient unsupervised continual learning on\nresource-constrained autonomous agents/systems. Our lpSpikeCon methodology\nemploys the following key steps: (1) analyzing the impacts of training the SNN\nmodel under unsupervised continual learning settings with reduced weight\nprecision on the inference accuracy; (2) leveraging this study to identify SNN\nparameters that have a significant impact on the inference accuracy; and (3)\ndeveloping an algorithm for searching the respective SNN parameter values that\nimprove the quality of unsupervised continual learning. The experimental\nresults show that our lpSpikeCon can reduce weight memory of the SNN model by\n8x (i.e., by judiciously employing 4-bit weights) for performing online\ntraining with unsupervised continual learning and achieve no accuracy loss in\nthe inference phase, as compared to the baseline model with 32-bit weights\nacross different network sizes.\n",
        "published": "2022",
        "authors": [
            "Rachmad Vidya Wicaksana Putra",
            "Muhammad Shafique"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.12718v3",
        "title": "DPSNN: A Differentially Private Spiking Neural Network with Temporal\n  Enhanced Pooling",
        "abstract": "  Privacy protection is a crucial issue in machine learning algorithms, and the\ncurrent privacy protection is combined with traditional artificial neural\nnetworks based on real values. Spiking neural network (SNN), the new generation\nof artificial neural networks, plays a crucial role in many fields. Therefore,\nresearch on the privacy protection of SNN is urgently needed. This paper\ncombines the differential privacy(DP) algorithm with SNN and proposes a\ndifferentially private spiking neural network (DPSNN). The SNN uses discrete\nspike sequences to transmit information, combined with the gradient noise\nintroduced by DP so that SNN maintains strong privacy protection. At the same\ntime, to make SNN maintain high performance while obtaining high privacy\nprotection, we propose the temporal enhanced pooling (TEP) method. It fully\nintegrates the temporal information of SNN into the spatial information\ntransfer, which enables SNN to perform better information transfer. We conduct\nexperiments on static and neuromorphic datasets, and the experimental results\nshow that our algorithm still maintains high performance while providing strong\nprivacy protection.\n",
        "published": "2022",
        "authors": [
            "Jihang Wang",
            "Dongcheng Zhao",
            "Guobin Shen",
            "Qian Zhang",
            "Yi Zeng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.13440v1",
        "title": "The Neuro-Symbolic Brain",
        "abstract": "  Neural networks promote a distributed representation with no clear place for\nsymbols. Despite this, we propose that symbols are manufactured simply by\ntraining a sparse random noise as a self-sustaining attractor in a feedback\nspiking neural network. This way, we can generate many of what we shall call\nprime attractors, and the networks that support them are like registers holding\na symbolic value, and we call them registers. Like symbols, prime attractors\nare atomic and devoid of any internal structure. Moreover, the winner-take-all\nmechanism naturally implemented by spiking neurons enables registers to recover\na prime attractor within a noisy signal. Using this faculty, when considering\ntwo connected registers, an input one and an output one, it is possible to bind\nin one shot using a Hebbian rule the attractor active on the output to the\nattractor active on the input. Thus, whenever an attractor is active on the\ninput, it induces its bound attractor on the output; even though the signal\ngets blurrier with more bindings, the winner-take-all filtering faculty can\nrecover the bound prime attractor. However, the capacity is still limited. It\nis also possible to unbind in one shot, restoring the capacity taken by that\nbinding. This mechanism serves as a basis for working memory, turning prime\nattractors into variables. Also, we use a random second-order network to\namalgamate the prime attractors held by two registers to bind the prime\nattractor held by a third register to them in one shot, de facto implementing a\nhash table. Furthermore, we introduce the register switch box composed of\nregisters to move the content of one register to another. Then, we use spiking\nneurons to build a toy symbolic computer based on the above. The technics used\nsuggest ways to design extrapolating, reusable, sample-efficient deep learning\nnetworks at the cost of structural priors.\n",
        "published": "2022",
        "authors": [
            "Robert Liz\u00e9e"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.13585v1",
        "title": "Learning in Feedback-driven Recurrent Spiking Neural Networks using\n  full-FORCE Training",
        "abstract": "  Feedback-driven recurrent spiking neural networks (RSNNs) are powerful\ncomputational models that can mimic dynamical systems. However, the presence of\na feedback loop from the readout to the recurrent layer de-stabilizes the\nlearning mechanism and prevents it from converging. Here, we propose a\nsupervised training procedure for RSNNs, where a second network is introduced\nonly during the training, to provide hint for the target dynamics. The proposed\ntraining procedure consists of generating targets for both recurrent and\nreadout layers (i.e., for a full RSNN system). It uses the recursive least\nsquare-based First-Order and Reduced Control Error (FORCE) algorithm to fit the\nactivity of each layer to its target. The proposed full-FORCE training\nprocedure reduces the amount of modifications needed to keep the error between\nthe output and target close to zero. These modifications control the feedback\nloop, which causes the training to converge. We demonstrate the improved\nperformance and noise robustness of the proposed full-FORCE training procedure\nto model 8 dynamical systems using RSNNs with leaky integrate and fire (LIF)\nneurons and rate coding. For energy-efficient hardware implementation, an\nalternative time-to-first-spike (TTFS) coding is implemented for the full-\nFORCE training procedure. Compared to rate coding, full-FORCE with TTFS coding\ngenerates fewer spikes and facilitates faster convergence to the target\ndynamics.\n",
        "published": "2022",
        "authors": [
            "Ankita Paul",
            "Stefan Wagner",
            "Anup Das"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.13881v1",
        "title": "Automated Dynamic Algorithm Configuration",
        "abstract": "  The performance of an algorithm often critically depends on its parameter\nconfiguration. While a variety of automated algorithm configuration methods\nhave been proposed to relieve users from the tedious and error-prone task of\nmanually tuning parameters, there is still a lot of untapped potential as the\nlearned configuration is static, i.e., parameter settings remain fixed\nthroughout the run. However, it has been shown that some algorithm parameters\nare best adjusted dynamically during execution, e.g., to adapt to the current\npart of the optimization landscape. Thus far, this is most commonly achieved\nthrough hand-crafted heuristics. A promising recent alternative is to\nautomatically learn such dynamic parameter adaptation policies from data. In\nthis article, we give the first comprehensive account of this new field of\nautomated dynamic algorithm configuration (DAC), present a series of recent\nadvances, and provide a solid foundation for future research in this field.\nSpecifically, we (i) situate DAC in the broader historical context of AI\nresearch; (ii) formalize DAC as a computational problem; (iii) identify the\nmethods used in prior-art to tackle this problem; (iv) conduct empirical case\nstudies for using DAC in evolutionary optimization, AI planning, and machine\nlearning.\n",
        "published": "2022",
        "authors": [
            "Steven Adriaensen",
            "Andr\u00e9 Biedenkapp",
            "Gresa Shala",
            "Noor Awad",
            "Theresa Eimer",
            "Marius Lindauer",
            "Frank Hutter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.14420v1",
        "title": "Fault-Aware Design and Training to Enhance DNNs Reliability with\n  Zero-Overhead",
        "abstract": "  Deep Neural Networks (DNNs) enable a wide series of technological\nadvancements, ranging from clinical imaging, to predictive industrial\nmaintenance and autonomous driving. However, recent findings indicate that\ntransient hardware faults may corrupt the models prediction dramatically. For\ninstance, the radiation-induced misprediction probability can be so high to\nimpede a safe deployment of DNNs models at scale, urging the need for efficient\nand effective hardening solutions. In this work, we propose to tackle the\nreliability issue both at training and model design time. First, we show that\nvanilla models are highly affected by transient faults, that can induce a\nperformances drop up to 37%. Hence, we provide three zero-overhead solutions,\nbased on DNN re-design and re-train, that can improve DNNs reliability to\ntransient faults up to one order of magnitude. We complement our work with\nextensive ablation studies to quantify the gain in performances of each\nhardening component.\n",
        "published": "2022",
        "authors": [
            "Niccol\u00f2 Cavagnero",
            "Fernando Dos Santos",
            "Marco Ciccone",
            "Giuseppe Averta",
            "Tatiana Tommasi",
            "Paolo Rech"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.15409v1",
        "title": "Painful intelligence: What AI can tell us about human suffering",
        "abstract": "  This book uses the modern theory of artificial intelligence (AI) to\nunderstand human suffering or mental pain. Both humans and sophisticated AI\nagents process information about the world in order to achieve goals and obtain\nrewards, which is why AI can be used as a model of the human brain and mind.\nThis book intends to make the theory accessible to a relatively general\naudience, requiring only some relevant scientific background. The book starts\nwith the assumption that suffering is mainly caused by frustration. Frustration\nmeans the failure of an agent (whether AI or human) to achieve a goal or a\nreward it wanted or expected. Frustration is inevitable because of the\noverwhelming complexity of the world, limited computational resources, and\nscarcity of good data. In particular, such limitations imply that an agent\nacting in the real world must cope with uncontrollability, unpredictability,\nand uncertainty, which all lead to frustration. Fundamental in such modelling\nis the idea of learning, or adaptation to the environment. While AI uses\nmachine learning, humans and animals adapt by a combination of evolutionary\nmechanisms and ordinary learning. Even frustration is fundamentally an error\nsignal that the system uses for learning. This book explores various aspects\nand limitations of learning algorithms and their implications regarding\nsuffering. At the end of the book, the computational theory is used to derive\nvarious interventions or training methods that will reduce suffering in humans.\nThe amount of frustration is expressed by a simple equation which indicates how\nit can be reduced. The ensuing interventions are very similar to those proposed\nby Buddhist and Stoic philosophy, and include mindfulness meditation.\nTherefore, this book can be interpreted as an exposition of a computational\ntheory justifying why such philosophies and meditation reduce human suffering.\n",
        "published": "2022",
        "authors": [
            "Aapo Hyv\u00e4rinen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.15569v2",
        "title": "GSR: A Generalized Symbolic Regression Approach",
        "abstract": "  Identifying the mathematical relationships that best describe a dataset\nremains a very challenging problem in machine learning, and is known as\nSymbolic Regression (SR). In contrast to neural networks which are often\ntreated as black boxes, SR attempts to gain insight into the underlying\nrelationships between the independent variables and the target variable of a\ngiven dataset by assembling analytical functions. In this paper, we present\nGSR, a Generalized Symbolic Regression approach, by modifying the conventional\nSR optimization problem formulation, while keeping the main SR objective\nintact. In GSR, we infer mathematical relationships between the independent\nvariables and some transformation of the target variable. We constrain our\nsearch space to a weighted sum of basis functions, and propose a genetic\nprogramming approach with a matrix-based encoding scheme. We show that our GSR\nmethod is competitive with strong SR benchmark methods, achieving promising\nexperimental performance on the well-known SR benchmark problem sets. Finally,\nwe highlight the strengths of GSR by introducing SymSet, a new SR benchmark set\nwhich is more challenging relative to the existing benchmarks.\n",
        "published": "2022",
        "authors": [
            "Tony Tohme",
            "Dehong Liu",
            "Kamal Youcef-Toumi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.15663v1",
        "title": "Multi-task Optimization Based Co-training for Electricity Consumption\n  Prediction",
        "abstract": "  Real-world electricity consumption prediction may involve different tasks,\ne.g., prediction for different time steps ahead or different geo-locations.\nThese tasks are often solved independently without utilizing some common\nproblem-solving knowledge that could be extracted and shared among these tasks\nto augment the performance of solving each task. In this work, we propose a\nmulti-task optimization (MTO) based co-training (MTO-CT) framework, where the\nmodels for solving different tasks are co-trained via an MTO paradigm in which\nsolving each task may benefit from the knowledge gained from when solving some\nother tasks to help its solving process. MTO-CT leverages long short-term\nmemory (LSTM) based model as the predictor where the knowledge is represented\nvia connection weights and biases. In MTO-CT, an inter-task knowledge transfer\nmodule is designed to transfer knowledge between different tasks, where the\nmost helpful source tasks are selected by using the probability matching and\nstochastic universal selection, and evolutionary operations like mutation and\ncrossover are performed for reusing the knowledge from selected source tasks in\na target task. We use electricity consumption data from five states in\nAustralia to design two sets of tasks at different scales: a) one-step ahead\nprediction for each state (five tasks) and b) 6-step, 12-step, 18-step, and\n24-step ahead prediction for each state (20 tasks). The performance of MTO-CT\nis evaluated on solving each of these two sets of tasks in comparison to\nsolving each task in the set independently without knowledge sharing under the\nsame settings, which demonstrates the superiority of MTO-CT in terms of\nprediction accuracy.\n",
        "published": "2022",
        "authors": [
            "Hui Song",
            "A. K. Qin",
            "Chenggang Yan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.00738v2",
        "title": "Composition of Relational Features with an Application to Explaining\n  Black-Box Predictors",
        "abstract": "  Relational machine learning programs like those developed in Inductive Logic\nProgramming (ILP) offer several advantages: (1) The ability to model complex\nrelationships amongst data instances; (2) The use of domain-specific relations\nduring model construction; and (3) The models constructed are human-readable,\nwhich is often one step closer to being human-understandable. However, these\nILP-like methods have not been able to capitalise fully on the rapid hardware,\nsoftware and algorithmic developments fuelling current developments in deep\nneural networks. In this paper, we treat relational features as functions and\nuse the notion of generalised composition of functions to derive complex\nfunctions from simpler ones. We formulate the notion of a set of\n$\\text{M}$-simple features in a mode language $\\text{M}$ and identify two\ncomposition operators ($\\rho_1$ and $\\rho_2$) from which all possible complex\nfeatures can be derived. We use these results to implement a form of\n\"explainable neural network\" called Compositional Relational Machines, or CRMs,\nwhich are labelled directed-acyclic graphs. The vertex-label for any vertex $j$\nin the CRM contains a feature-function $f_j$ and a continuous activation\nfunction $g_j$. If $j$ is a \"non-input\" vertex, then $f_j$ is the composition\nof features associated with vertices in the direct predecessors of $j$. Our\nfocus is on CRMs in which input vertices (those without any direct\npredecessors) all have $\\text{M}$-simple features in their vertex-labels. We\nprovide a randomised procedure for constructing and learning such CRMs. Using a\nnotion of explanations based on the compositional structure of features in a\nCRM, we provide empirical evidence on synthetic data of the ability to identify\nappropriate explanations; and demonstrate the use of CRMs as 'explanation\nmachines' for black-box models that do not provide explanations for their\npredictions.\n",
        "published": "2022",
        "authors": [
            "Ashwin Srinivasan",
            "A Baskar",
            "Tirtharaj Dash",
            "Devanshu Shah"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.01176v1",
        "title": "From Cities to Series: Complex Networks and Deep Learning for Improved\n  Spatial and Temporal Analytics*",
        "abstract": "  Graphs have often been used to answer questions about the interaction between\nreal-world entities by taking advantage of their capacity to represent complex\ntopologies. Complex networks are known to be graphs that capture such\nnon-trivial topologies; they are able to represent human phenomena such as\nepidemic processes, the dynamics of populations, and the urbanization of\ncities. The investigation of complex networks has been extrapolated to many\nfields of science, with particular emphasis on computing techniques, including\nartificial intelligence. In such a case, the analysis of the interaction\nbetween entities of interest is transposed to the internal learning of\nalgorithms, a paradigm whose investigation is able to expand the state of the\nart in Computer Science. By exploring this paradigm, this thesis puts together\ncomplex networks and machine learning techniques to improve the understanding\nof the human phenomena observed in pandemics, pendular migration, and street\nnetworks. Accordingly, we contribute with: (i) a new neural network\narchitecture capable of modeling dynamic processes observed in spatial and\ntemporal data with applications in epidemics propagation, weather forecasting,\nand patient monitoring in intensive care units; (ii) a machine-learning\nmethodology for analyzing and predicting links in the scope of human mobility\nbetween all the cities of Brazil; and, (iii) techniques for identifying\ninconsistencies in the urban planning of cities while tracking the most\ninfluential vertices, with applications over Brazilian and worldwide cities. We\nobtained results sustained by sound evidence of advances to the state of the\nart in artificial intelligence, rigorous formalisms, and ample experimentation.\nOur findings rely upon real-world applications in a range of domains,\ndemonstrating the applicability of our methodologies.\n",
        "published": "2022",
        "authors": [
            "Gabriel Spadon",
            "Jose F. Rodrigues-Jr"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.01261v1",
        "title": "Entangled Residual Mappings",
        "abstract": "  Residual mappings have been shown to perform representation learning in the\nfirst layers and iterative feature refinement in higher layers. This interplay,\ncombined with their stabilizing effect on the gradient norms, enables them to\ntrain very deep networks. In this paper, we take a step further and introduce\nentangled residual mappings to generalize the structure of the residual\nconnections and evaluate their role in iterative learning representations. An\nentangled residual mapping replaces the identity skip connections with\nspecialized entangled mappings such as orthogonal, sparse, and structural\ncorrelation matrices that share key attributes (eigenvalues, structure, and\nJacobian norm) with identity mappings. We show that while entangled mappings\ncan preserve the iterative refinement of features across various deep models,\nthey influence the representation learning process in convolutional networks\ndifferently than attention-based models and recurrent neural networks. In\ngeneral, we find that for CNNs and Vision Transformers entangled sparse mapping\ncan help generalization while orthogonal mappings hurt performance. For\nrecurrent networks, orthogonal residual mappings form an inductive bias for\ntime-variant sequences, which degrades accuracy on time-invariant tasks.\n",
        "published": "2022",
        "authors": [
            "Mathias Lechner",
            "Ramin Hasani",
            "Zahra Babaiee",
            "Radu Grosu",
            "Daniela Rus",
            "Thomas A. Henzinger",
            "Sepp Hochreiter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.03192v4",
        "title": "Generalized Data Distribution Iteration",
        "abstract": "  To obtain higher sample efficiency and superior final performance\nsimultaneously has been one of the major challenges for deep reinforcement\nlearning (DRL). Previous work could handle one of these challenges but\ntypically failed to address them concurrently. In this paper, we try to tackle\nthese two challenges simultaneously. To achieve this, we firstly decouple these\nchallenges into two classic RL problems: data richness and\nexploration-exploitation trade-off. Then, we cast these two problems into the\ntraining data distribution optimization problem, namely to obtain desired\ntraining data within limited interactions, and address them concurrently via i)\nexplicit modeling and control of the capacity and diversity of behavior policy\nand ii) more fine-grained and adaptive control of selective/sampling\ndistribution of the behavior policy using a monotonic data distribution\noptimization. Finally, we integrate this process into Generalized Policy\nIteration (GPI) and obtain a more general framework called Generalized Data\nDistribution Iteration (GDI). We use the GDI framework to introduce\noperator-based versions of well-known RL methods from DQN to Agent57.\nTheoretical guarantee of the superiority of GDI compared with GPI is concluded.\nWe also demonstrate our state-of-the-art (SOTA) performance on Arcade Learning\nEnvironment (ALE), wherein our algorithm has achieved 9620.33% mean human\nnormalized score (HNS), 1146.39% median HNS and surpassed 22 human world\nrecords using only 200M training frames. Our performance is comparable to\nAgent57's while we consume 500 times less data. We argue that there is still a\nlong way to go before obtaining real superhuman agents in ALE.\n",
        "published": "2022",
        "authors": [
            "Jiajun Fan",
            "Changnan Xiao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.03312v1",
        "title": "Neuro-Nav: A Library for Neurally-Plausible Reinforcement Learning",
        "abstract": "  In this work we propose Neuro-Nav, an open-source library for neurally\nplausible reinforcement learning (RL). RL is among the most common modeling\nframeworks for studying decision making, learning, and navigation in biological\norganisms. In utilizing RL, cognitive scientists often handcraft environments\nand agents to meet the needs of their particular studies. On the other hand,\nartificial intelligence researchers often struggle to find benchmarks for\nneurally and biologically plausible representation and behavior (e.g., in\ndecision making or navigation). In order to streamline this process across both\nfields with transparency and reproducibility, Neuro-Nav offers a set of\nstandardized environments and RL algorithms drawn from canonical behavioral and\nneural studies in rodents and humans. We demonstrate that the toolkit\nreplicates relevant findings from a number of studies across both cognitive\nscience and RL literatures. We furthermore describe ways in which the library\ncan be extended with novel algorithms (including deep RL) and environments to\naddress future research needs of the field.\n",
        "published": "2022",
        "authors": [
            "Arthur Juliani",
            "Samuel Barnett",
            "Brandon Davis",
            "Margaret Sereno",
            "Ida Momennejad"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.03654v1",
        "title": "Solving the Spike Feature Information Vanishing Problem in Spiking Deep\n  Q Network with Potential Based Normalization",
        "abstract": "  Brain inspired spiking neural networks (SNNs) have been successfully applied\nto many pattern recognition domains. The SNNs based deep structure have\nachieved considerable results in perceptual tasks, such as image\nclassification, target detection. However, the application of deep SNNs in\nreinforcement learning (RL) tasks is still a problem to be explored. Although\nthere have been previous studies on the combination of SNNs and RL, most of\nthem focus on robotic control problems with shallow networks or using ANN-SNN\nconversion method to implement spiking deep Q Network (SDQN). In this work, we\nmathematically analyzed the problem of the disappearance of spiking signal\nfeatures in SDQN and proposed a potential based layer normalization(pbLN)\nmethod to directly train spiking deep Q networks. Experiment shows that\ncompared with state-of-art ANN-SNN conversion method and other SDQN works, the\nproposed pbLN spiking deep Q networks (PL-SDQN) achieved better performance on\nAtari game tasks.\n",
        "published": "2022",
        "authors": [
            "Yinqian Sun",
            "Yi Zeng",
            "Yang Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.04199v3",
        "title": "Deep Surrogate Assisted Generation of Environments",
        "abstract": "  Recent progress in reinforcement learning (RL) has started producing\ngenerally capable agents that can solve a distribution of complex environments.\nThese agents are typically tested on fixed, human-authored environments. On the\nother hand, quality diversity (QD) optimization has been proven to be an\neffective component of environment generation algorithms, which can generate\ncollections of high-quality environments that are diverse in the resulting\nagent behaviors. However, these algorithms require potentially expensive\nsimulations of agents on newly generated environments. We propose Deep\nSurrogate Assisted Generation of Environments (DSAGE), a sample-efficient QD\nenvironment generation algorithm that maintains a deep surrogate model for\npredicting agent behaviors in new environments. Results in two benchmark\ndomains show that DSAGE significantly outperforms existing QD environment\ngeneration algorithms in discovering collections of environments that elicit\ndiverse behaviors of a state-of-the-art RL agent and a planning agent. Our\nsource code and videos are available at https://dsagepaper.github.io/.\n",
        "published": "2022",
        "authors": [
            "Varun Bhatt",
            "Bryon Tjanaka",
            "Matthew C. Fontaine",
            "Stefanos Nikolaidis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.04804v2",
        "title": "Theoretical Error Performance Analysis for Variational Quantum Circuit\n  Based Functional Regression",
        "abstract": "  The noisy intermediate-scale quantum (NISQ) devices enable the implementation\nof the variational quantum circuit (VQC) for quantum neural networks (QNN).\nAlthough the VQC-based QNN has succeeded in many machine learning tasks, the\nrepresentation and generalization powers of VQC still require further\ninvestigation, particularly when the dimensionality of classical inputs is\nconcerned. In this work, we first put forth an end-to-end quantum neural\nnetwork, TTN-VQC, which consists of a quantum tensor network based on a\ntensor-train network (TTN) for dimensionality reduction and a VQC for\nfunctional regression. Then, we aim at the error performance analysis for the\nTTN-VQC in terms of representation and generalization powers. We also\ncharacterize the optimization properties of TTN-VQC by leveraging the\nPolyak-Lojasiewicz (PL) condition. Moreover, we conduct the experiments of\nfunctional regression on a handwritten digit classification dataset to justify\nour theoretical analysis.\n",
        "published": "2022",
        "authors": [
            "Jun Qi",
            "Chao-Han Huck Yang",
            "Pin-Yu Chen",
            "Min-Hsiu Hsieh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.04928v5",
        "title": "GAMR: A Guided Attention Model for (visual) Reasoning",
        "abstract": "  Humans continue to outperform modern AI systems in their ability to flexibly\nparse and understand complex visual scenes. Here, we present a novel module for\nvisual reasoning, the Guided Attention Model for (visual) Reasoning (GAMR),\nwhich instantiates an active vision theory -- positing that the brain solves\ncomplex visual reasoning problems dynamically -- via sequences of attention\nshifts to select and route task-relevant visual information into memory.\nExperiments on an array of visual reasoning tasks and datasets demonstrate\nGAMR's ability to learn visual routines in a robust and sample-efficient\nmanner. In addition, GAMR is shown to be capable of zero-shot generalization on\ncompletely novel reasoning tasks. Overall, our work provides computational\nsupport for cognitive theories that postulate the need for a critical interplay\nbetween attention and memory to dynamically maintain and manipulate\ntask-relevant visual information to solve complex visual reasoning tasks.\n",
        "published": "2022",
        "authors": [
            "Mohit Vaishnav",
            "Thomas Serre"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.04951v2",
        "title": "Evolutionary Echo State Network: evolving reservoirs in the Fourier\n  space",
        "abstract": "  The Echo State Network (ESN) is a class of Recurrent Neural Network with a\nlarge number of hidden-hidden weights (in the so-called reservoir). Canonical\nESN and its variations have recently received significant attention due to\ntheir remarkable success in the modeling of non-linear dynamical systems. The\nreservoir is randomly connected with fixed weights that don't change in the\nlearning process. Only the weights from reservoir to output are trained. Since\nthe reservoir is fixed during the training procedure, we may wonder if the\ncomputational power of the recurrent structure is fully harnessed. In this\narticle, we propose a new computational model of the ESN type, that represents\nthe reservoir weights in the Fourier space and performs a fine-tuning of these\nweights applying genetic algorithms in the frequency domain. The main interest\nis that this procedure will work in a much smaller space compared to the\nclassical ESN, thus providing a dimensionality reduction transformation of the\ninitial method. The proposed technique allows us to exploit the benefits of the\nlarge recurrent structure avoiding the training problems of gradient-based\nmethod. We provide a detailed experimental study that demonstrates the good\nperformances of our approach with well-known chaotic systems and real-world\ndata.\n",
        "published": "2022",
        "authors": [
            "Sebastian Basterrech",
            "Gerardo Rubino"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.05056v1",
        "title": "On Neural Architecture Inductive Biases for Relational Tasks",
        "abstract": "  Current deep learning approaches have shown good in-distribution\ngeneralization performance, but struggle with out-of-distribution\ngeneralization. This is especially true in the case of tasks involving abstract\nrelations like recognizing rules in sequences, as we find in many intelligence\ntests. Recent work has explored how forcing relational representations to\nremain distinct from sensory representations, as it seems to be the case in the\nbrain, can help artificial systems. Building on this work, we further explore\nand formalize the advantages afforded by 'partitioned' representations of\nrelations and sensory details, and how this inductive bias can help recompose\nlearned relational structure in newly encountered settings. We introduce a\nsimple architecture based on similarity scores which we name Compositional\nRelational Network (CoRelNet). Using this model, we investigate a series of\ninductive biases that ensure abstract relations are learned and represented\ndistinctly from sensory data, and explore their effects on out-of-distribution\ngeneralization for a series of relational psychophysics tasks. We find that\nsimple architectural choices can outperform existing models in\nout-of-distribution generalization. Together, these results show that\npartitioning relational representations from other information streams may be a\nsimple way to augment existing network architectures' robustness when\nperforming out-of-distribution relational computations.\n",
        "published": "2022",
        "authors": [
            "Giancarlo Kerg",
            "Sarthak Mittal",
            "David Rolnick",
            "Yoshua Bengio",
            "Blake Richards",
            "Guillaume Lajoie"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.05497v3",
        "title": "Mutation Models: Learning to Generate Levels by Imitating Evolution",
        "abstract": "  Search-based procedural content generation (PCG) is a well-known method for\nlevel generation in games. Its key advantage is that it is generic and able to\nsatisfy functional constraints. However, due to the heavy computational costs\nto run these algorithms online, search-based PCG is rarely utilized for\nreal-time generation. In this paper, we introduce mutation models, a new type\nof iterative level generator based on machine learning. We train a model to\nimitate the evolutionary process and use the trained model to generate levels.\nThis trained model is able to modify noisy levels sequentially to create better\nlevels without the need for a fitness function during inference. We evaluate\nour trained models on a 2D maze generation task. We compare several different\nversions of the method: training the models either at the end of evolution\n(normal evolution) or every 100 generations (assisted evolution) and using the\nmodel as a mutation function during evolution. Using the assisted evolution\nprocess, the final trained models are able to generate mazes with a success\nrate of 99% and high diversity of 86%. The trained model is many times faster\nthan the evolutionary process it was trained on. This work opens the door to a\nnew way of learning level generators guided by an evolutionary process, meaning\nautomatic creation of generators with specifiable constraints and objectives\nthat are fast enough for runtime deployment in games.\n",
        "published": "2022",
        "authors": [
            "Ahmed Khalifa",
            "Michael Cerny Green",
            "Julian Togelius"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.06178v3",
        "title": "Efficient recurrent architectures through activity sparsity and sparse\n  back-propagation through time",
        "abstract": "  Recurrent neural networks (RNNs) are well suited for solving sequence tasks\nin resource-constrained systems due to their expressivity and low computational\nrequirements. However, there is still a need to bridge the gap between what\nRNNs are capable of in terms of efficiency and performance and real-world\napplication requirements. The memory and computational requirements arising\nfrom propagating the activations of all the neurons at every time step to every\nconnected neuron, together with the sequential dependence of activations,\ncontribute to the inefficiency of training and using RNNs. We propose a\nsolution inspired by biological neuron dynamics that makes the communication\nbetween RNN units sparse and discrete. This makes the backward pass with\nbackpropagation through time (BPTT) computationally sparse and efficient as\nwell. We base our model on the gated recurrent unit (GRU), extending it with\nunits that emit discrete events for communication triggered by a threshold so\nthat no information is communicated to other units in the absence of events. We\nshow theoretically that the communication between units, and hence the\ncomputation required for both the forward and backward passes, scales with the\nnumber of events in the network. Our model achieves efficiency without\ncompromising task performance, demonstrating competitive performance compared\nto state-of-the-art recurrent network models in real-world tasks, including\nlanguage modeling. The dynamic activity sparsity mechanism also makes our model\nwell suited for novel energy-efficient neuromorphic hardware. Code is available\nat https://github.com/KhaleelKhan/EvNN/.\n",
        "published": "2022",
        "authors": [
            "Anand Subramoney",
            "Khaleelulla Khan Nazeer",
            "Mark Sch\u00f6ne",
            "Christian Mayr",
            "David Kappel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.06355v1",
        "title": "Anomaly Detection and Inter-Sensor Transfer Learning on Smart\n  Manufacturing Datasets",
        "abstract": "  Smart manufacturing systems are being deployed at a growing rate because of\ntheir ability to interpret a wide variety of sensed information and act on the\nknowledge gleaned from system observations. In many cases, the principal goal\nof the smart manufacturing system is to rapidly detect (or anticipate) failures\nto reduce operational cost and eliminate downtime. This often boils down to\ndetecting anomalies within the sensor date acquired from the system. The smart\nmanufacturing application domain poses certain salient technical challenges. In\nparticular, there are often multiple types of sensors with varying capabilities\nand costs. The sensor data characteristics change with the operating point of\nthe environment or machines, such as, the RPM of the motor. The anomaly\ndetection process therefore has to be calibrated near an operating point. In\nthis paper, we analyze four datasets from sensors deployed from manufacturing\ntestbeds. We evaluate the performance of several traditional and ML-based\nforecasting models for predicting the time series of sensor data. Then,\nconsidering the sparse data from one kind of sensor, we perform transfer\nlearning from a high data rate sensor to perform defect type classification.\nTaken together, we show that predictive failure classification can be achieved,\nthus paving the way for predictive maintenance.\n",
        "published": "2022",
        "authors": [
            "Mustafa Abdallah",
            "Byung-Gun Joung",
            "Wo Jae Lee",
            "Charilaos Mousoulis",
            "John W. Sutherland",
            "Saurabh Bagchi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.06903v1",
        "title": "A Local Optima Network Analysis of the Feedforward Neural Architecture\n  Space",
        "abstract": "  This study investigates the use of local optima network (LON) analysis, a\nderivative of the fitness landscape of candidate solutions, to characterise and\nvisualise the neural architecture space. The search space of feedforward neural\nnetwork architectures with up to three layers, each with up to 10 neurons, is\nfully enumerated by evaluating trained model performance on a selection of data\nsets. Extracted LONs, while heterogeneous across data sets, all exhibit simple\nglobal structures, with single global funnels in all cases but one. These\nresults yield early indication that LONs may provide a viable paradigm by which\nto analyse and optimise neural architectures.\n",
        "published": "2022",
        "authors": [
            "Isak Potgieter",
            "Christopher W. Cleghorn",
            "Anna S. Bosman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.08656v1",
        "title": "tinySNN: Towards Memory- and Energy-Efficient Spiking Neural Networks",
        "abstract": "  Larger Spiking Neural Network (SNN) models are typically favorable as they\ncan offer higher accuracy. However, employing such models on the resource- and\nenergy-constrained embedded platforms is inefficient. Towards this, we present\na tinySNN framework that optimizes the memory and energy requirements of SNN\nprocessing in both the training and inference phases, while keeping the\naccuracy high. It is achieved by reducing the SNN operations, improving the\nlearning quality, quantizing the SNN parameters, and selecting the appropriate\nSNN model. Furthermore, our tinySNN quantizes different SNN parameters (i.e.,\nweights and neuron parameters) to maximize the compression while exploring\ndifferent combinations of quantization schemes, precision levels, and rounding\nschemes to find the model that provides acceptable accuracy. The experimental\nresults demonstrate that our tinySNN significantly reduces the memory footprint\nand the energy consumption of SNNs without accuracy loss as compared to the\nbaseline network. Therefore, our tinySNN effectively compresses the given SNN\nmodel to achieve high accuracy in a memory- and energy-efficient manner, hence\nenabling the employment of SNNs for the resource- and energy-constrained\nembedded applications.\n",
        "published": "2022",
        "authors": [
            "Rachmad Vidya Wicaksana Putra",
            "Muhammad Shafique"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.08888v1",
        "title": "Fast Population-Based Reinforcement Learning on a Single Machine",
        "abstract": "  Training populations of agents has demonstrated great promise in\nReinforcement Learning for stabilizing training, improving exploration and\nasymptotic performance, and generating a diverse set of solutions. However,\npopulation-based training is often not considered by practitioners as it is\nperceived to be either prohibitively slow (when implemented sequentially), or\ncomputationally expensive (if agents are trained in parallel on independent\naccelerators). In this work, we compare implementations and revisit previous\nstudies to show that the judicious use of compilation and vectorization allows\npopulation-based training to be performed on a single machine with one\naccelerator with minimal overhead compared to training a single agent. We also\nshow that, when provided with a few accelerators, our protocols extend to large\npopulation sizes for applications such as hyperparameter tuning. We hope that\nthis work and the public release of our code will encourage practitioners to\nuse population-based learning more frequently for their research and\napplications.\n",
        "published": "2022",
        "authors": [
            "Arthur Flajolet",
            "Claire Bizon Monroc",
            "Karim Beguir",
            "Thomas Pierrot"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.10540v4",
        "title": "Rethinking Symbolic Regression Datasets and Benchmarks for Scientific\n  Discovery",
        "abstract": "  This paper revisits datasets and evaluation criteria for Symbolic Regression\n(SR), specifically focused on its potential for scientific discovery. Focused\non a set of formulas used in the existing datasets based on Feynman Lectures on\nPhysics, we recreate 120 datasets to discuss the performance of symbolic\nregression for scientific discovery (SRSD). For each of the 120 SRSD datasets,\nwe carefully review the properties of the formula and its variables to design\nreasonably realistic sampling ranges of values so that our new SRSD datasets\ncan be used for evaluating the potential of SRSD such as whether or not an SR\nmethod can (re)discover physical laws from such datasets. We also create\nanother 120 datasets that contain dummy variables to examine whether SR methods\ncan choose necessary variables only. Besides, we propose to use normalized edit\ndistances (NED) between a predicted equation and the true equation trees for\naddressing a critical issue that existing SR metrics are either binary or\nerrors between the target values and an SR model's predicted values for a given\ninput. We conduct benchmark experiments on our new SRSD datasets using various\nrepresentative SR methods. The experimental results show that we provide a more\nrealistic performance evaluation, and our user study shows that the NED\ncorrelates with human judges significantly more than an existing SR metric.\n",
        "published": "2022",
        "authors": [
            "Yoshitomo Matsubara",
            "Naoya Chiba",
            "Ryo Igarashi",
            "Yoshitaka Ushiku"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.10593v1",
        "title": "A Survey on Computational Intelligence-based Transfer Learning",
        "abstract": "  The goal of transfer learning (TL) is providing a framework for exploiting\nacquired knowledge from source to target data. Transfer learning approaches\ncompared to traditional machine learning approaches are capable of modeling\nbetter data patterns from the current domain. However, vanilla TL needs\nperformance improvements by using computational intelligence-based TL. This\npaper studies computational intelligence-based transfer learning techniques and\ncategorizes them into neural network-based, evolutionary algorithm-based, swarm\nintelligence-based and fuzzy logic-based transfer learning.\n",
        "published": "2022",
        "authors": [
            "Mohamad Zamini",
            "Eunjin Kim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.10609v1",
        "title": "Autoencoder-based Attribute Noise Handling Method for Medical Data",
        "abstract": "  Medical datasets are particularly subject to attribute noise, that is,\nmissing and erroneous values. Attribute noise is known to be largely\ndetrimental to learning performances. To maximize future learning performances\nit is primordial to deal with attribute noise before any inference. We propose\na simple autoencoder-based preprocessing method that can correct mixed-type\ntabular data corrupted by attribute noise. No other method currently exists to\nhandle attribute noise in tabular data. We experimentally demonstrate that our\nmethod outperforms both state-of-the-art imputation methods and noise\ncorrection methods on several real-world medical datasets.\n",
        "published": "2022",
        "authors": [
            "Thomas Ranvier",
            "Haytham Elgazel",
            "Emmanuel Coquery",
            "Khalid Benabdeslem"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.11198v3",
        "title": "General Univariate Estimation-of-Distribution Algorithms",
        "abstract": "  We propose a general formulation of a univariate estimation-of-distribution\nalgorithm (EDA). It naturally incorporates the three classic univariate EDAs\n\\emph{compact genetic algorithm}, \\emph{univariate marginal distribution\nalgorithm} and \\emph{population-based incremental learning} as well as the\n\\emph{max-min ant system} with iteration-best update. Our unified description\nof the existing algorithms allows a unified analysis of these; we demonstrate\nthis by providing an analysis of genetic drift that immediately gives the\nexisting results proven separately for the four algorithms named above. Our\ngeneral model also includes EDAs that are more efficient than the existing ones\nand these may not be difficult to find as we demonstrate for the OneMax and\nLeadingOnes benchmarks.\n",
        "published": "2022",
        "authors": [
            "Benjamin Doerr",
            "Marc Dufay"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.13448v2",
        "title": "Distinguishing Learning Rules with Brain Machine Interfaces",
        "abstract": "  Despite extensive theoretical work on biologically plausible learning rules,\nclear evidence about whether and how such rules are implemented in the brain\nhas been difficult to obtain. We consider biologically plausible supervised-\nand reinforcement-learning rules and ask whether changes in network activity\nduring learning can be used to determine which learning rule is being used.\nSupervised learning requires a credit-assignment model estimating the mapping\nfrom neural activity to behavior, and, in a biological organism, this model\nwill inevitably be an imperfect approximation of the ideal mapping, leading to\na bias in the direction of the weight updates relative to the true gradient.\nReinforcement learning, on the other hand, requires no credit-assignment model\nand tends to make weight updates following the true gradient direction. We\nderive a metric to distinguish between learning rules by observing changes in\nthe network activity during learning, given that the mapping from brain to\nbehavior is known by the experimenter. Because brain-machine interface (BMI)\nexperiments allow for precise knowledge of this mapping, we model a\ncursor-control BMI task using recurrent neural networks, showing that learning\nrules can be distinguished in simulated experiments using only observations\nthat a neuroscience experimenter would plausibly have access to.\n",
        "published": "2022",
        "authors": [
            "Jacob P. Portes",
            "Christian Schmid",
            "James M. Murray"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.13623v3",
        "title": "Learning Controllable 3D Level Generators",
        "abstract": "  Procedural Content Generation via Reinforcement Learning (PCGRL) foregoes the\nneed for large human-authored data-sets and allows agents to train explicitly\non functional constraints, using computable, user-defined measures of quality\ninstead of target output. We explore the application of PCGRL to 3D domains, in\nwhich content-generation tasks naturally have greater complexity and potential\npertinence to real-world applications. Here, we introduce several PCGRL tasks\nfor the 3D domain, Minecraft (Mojang Studios, 2009). These tasks will challenge\nRL-based generators using affordances often found in 3D environments, such as\njumping, multiple dimensional movement, and gravity. We train an agent to\noptimize each of these tasks to explore the capabilities of previous research\nin PCGRL. This agent is able to generate relatively complex and diverse levels,\nand generalize to random initial states and control targets. Controllability\ntests in the presented tasks demonstrate their utility to analyze success and\nfailure for 3D generators.\n",
        "published": "2022",
        "authors": [
            "Zehua Jiang",
            "Sam Earle",
            "Michael Cerny Green",
            "Julian Togelius"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.14048v1",
        "title": "Short-Term Plasticity Neurons Learning to Learn and Forget",
        "abstract": "  Short-term plasticity (STP) is a mechanism that stores decaying memories in\nsynapses of the cerebral cortex. In computing practice, STP has been used, but\nmostly in the niche of spiking neurons, even though theory predicts that it is\nthe optimal solution to certain dynamic tasks. Here we present a new type of\nrecurrent neural unit, the STP Neuron (STPN), which indeed turns out strikingly\npowerful. Its key mechanism is that synapses have a state, propagated through\ntime by a self-recurrent connection-within-the-synapse. This formulation\nenables training the plasticity with backpropagation through time, resulting in\na form of learning to learn and forget in the short term. The STPN outperforms\nall tested alternatives, i.e. RNNs, LSTMs, other models with fast weights, and\ndifferentiable plasticity. We confirm this in both supervised and reinforcement\nlearning (RL), and in tasks such as Associative Retrieval, Maze Exploration,\nAtari video games, and MuJoCo robotics. Moreover, we calculate that, in\nneuromorphic or biological circuits, the STPN minimizes energy consumption\nacross models, as it depresses individual synapses dynamically. Based on these,\nbiological STP may have been a strong evolutionary attractor that maximizes\nboth efficiency and computational power. The STPN now brings these neuromorphic\nadvantages also to a broad spectrum of machine learning practice. Code is\navailable at https://github.com/NeuromorphicComputing/stpn\n",
        "published": "2022",
        "authors": [
            "Hector Garcia Rodriguez",
            "Qinghai Guo",
            "Timoleon Moraitis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.00705v1",
        "title": "Multivariate Time Series Anomaly Detection with Few Positive Samples",
        "abstract": "  Given the scarcity of anomalies in real-world applications, the majority of\nliterature has been focusing on modeling normality. The learned representations\nenable anomaly detection as the normality model is trained to capture certain\nkey underlying data regularities under normal circumstances. In practical\nsettings, particularly industrial time series anomaly detection, we often\nencounter situations where a large amount of normal operation data is available\nalong with a small number of anomaly events collected over time. This practical\nsituation calls for methodologies to leverage these small number of anomaly\nevents to create a better anomaly detector. In this paper, we introduce two\nmethodologies to address the needs of this practical situation and compared\nthem with recently developed state of the art techniques. Our proposed methods\nanchor on representative learning of normal operation with autoregressive (AR)\nmodel along with loss components to encourage representations that separate\nnormal versus few positive examples. We applied the proposed methods to two\nindustrial anomaly detection datasets and demonstrated effective performance in\ncomparison with approaches from literature. Our study also points out\nadditional challenges with adopting such methods in practical applications.\n",
        "published": "2022",
        "authors": [
            "Feng Xue",
            "Weizhong Yan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.03485v2",
        "title": "On Non-Linear operators for Geometric Deep Learning",
        "abstract": "  This work studies operators mapping vector and scalar fields defined over a\nmanifold $\\mathcal{M}$, and which commute with its group of diffeomorphisms\n$\\text{Diff}(\\mathcal{M})$. We prove that in the case of scalar fields\n$L^p_\\omega(\\mathcal{M,\\mathbb{R}})$, those operators correspond to point-wise\nnon-linearities, recovering and extending known results on $\\mathbb{R}^d$. In\nthe context of Neural Networks defined over $\\mathcal{M}$, it indicates that\npoint-wise non-linear operators are the only universal family that commutes\nwith any group of symmetries, and justifies their systematic use in combination\nwith dedicated linear operators commuting with specific symmetries. In the case\nof vector fields $L^p_\\omega(\\mathcal{M},T\\mathcal{M})$, we show that those\noperators are solely the scalar multiplication. It indicates that\n$\\text{Diff}(\\mathcal{M})$ is too rich and that there is no universal class of\nnon-linear operators to motivate the design of Neural Networks over the\nsymmetries of $\\mathcal{M}$.\n",
        "published": "2022",
        "authors": [
            "Gr\u00e9goire Sergeant-Perthuis",
            "Jakob Maier",
            "Joan Bruna",
            "Edouard Oyallon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.03577v1",
        "title": "Automatic Synthesis of Neurons for Recurrent Neural Nets",
        "abstract": "  We present a new class of neurons, ARNs, which give a cross entropy on test\ndata that is up to three times lower than the one achieved by carefully\noptimized LSTM neurons. The explanations for the huge improvements that often\nare achieved are elaborate skip connections through time, up to four internal\nmemory states per neuron and a number of novel activation functions including\nsmall quadratic forms. The new neurons were generated using automatic\nprogramming and are formulated as pure functional programs that easily can be\ntransformed. We present experimental results for eight datasets and found\nexcellent improvements for seven of them, but LSTM remained the best for one\ndataset. The results are so promising that automatic programming to generate\nnew neurons should become part of the standard operating procedure for any\nmachine learning practitioner who works on time series data such as sensor\nsignals.\n",
        "published": "2022",
        "authors": [
            "Roland Olsson",
            "Chau Tran",
            "Lars Magnusson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.04121v1",
        "title": "Braid-based architecture search",
        "abstract": "  In this article, we propose the approach to structural optimization of neural\nnetworks, based on the braid theory. The paper describes the basics of braid\ntheory as applied to the description of graph structures of neural networks. It\nis shown how networks of various topologies can be built using braid structures\nbetween layers of neural networks. The operation of a neural network based on\nthe braid theory is compared with a homogeneous deep neural network and a\nnetwork with random intersections between layers that do not correspond to the\nordering of the braids. Results are obtained showing the advantage of\nbraid-based networks over comparable architectures in classification problems.\n",
        "published": "2022",
        "authors": [
            "Olga Lukyanova",
            "Oleg Nikitin",
            "Alex Kunin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.04467v1",
        "title": "Noisy Heuristics NAS: A Network Morphism based Neural Architecture\n  Search using Heuristics",
        "abstract": "  Network Morphism based Neural Architecture Search (NAS) is one of the most\nefficient methods, however, knowing where and when to add new neurons or remove\ndis-functional ones is generally left to black-box Reinforcement Learning\nmodels. In this paper, we present a new Network Morphism based NAS called Noisy\nHeuristics NAS which uses heuristics learned from manually developing neural\nnetwork models and inspired by biological neuronal dynamics. Firstly, we add\nnew neurons randomly and prune away some to select only the best fitting\nneurons. Secondly, we control the number of layers in the network using the\nrelationship of hidden units to the number of input-output connections. Our\nmethod can increase or decrease the capacity or non-linearity of models online\nwhich is specified with a few meta-parameters by the user. Our method\ngeneralizes both on toy datasets and on real-world data sets such as MNIST,\nCIFAR-10, and CIFAR-100. The performance is comparable to the hand-engineered\narchitecture ResNet-18 with the similar parameters.\n",
        "published": "2022",
        "authors": [
            "Suman Sapkota",
            "Binod Bhattarai"
        ]
    }
]