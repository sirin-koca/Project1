[
    {
        "id": "http://arxiv.org/abs/2203.15776v1",
        "title": "Efficiently Evolving Swarm Behaviors Using Grammatical Evolution With\n  PPA-style Behavior Trees",
        "abstract": "  Evolving swarm behaviors with artificial agents is computationally expensive\nand challenging. Because reward structures are often sparse in swarm problems,\nonly a few simulations among hundreds evolve successful swarm behaviors.\nAdditionally, swarm evolutionary algorithms typically rely on ad hoc fitness\nstructures, and novel fitness functions need to be designed for each swarm\ntask. This paper evolves swarm behaviors by systematically combining\nPostcondition-Precondition-Action (PPA) canonical Behavior Trees (BT) with a\nGrammatical Evolution. The PPA structure replaces ad hoc reward structures with\nsystematic postcondition checks, which allows a common grammar to learn\nsolutions to different tasks using only environmental cues and BT feedback. The\nstatic performance of learned behaviors is poor because no agent learns all\nnecessary subtasks, but performance while evolving is excellent because agents\ncan quickly change behaviors in new contexts. The evolving algorithm succeeded\nin 75\\% of learning trials for both foraging and nest maintenance tasks, an\neight-fold improvement over prior work.\n",
        "published": "2022",
        "authors": [
            "Aadesh Neupane",
            "Michael A. Goodrich"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.03618v1",
        "title": "Adaptive Combination of a Genetic Algorithm and Novelty Search for Deep\n  Neuroevolution",
        "abstract": "  Evolutionary Computation (EC) has been shown to be able to quickly train Deep\nArtificial Neural Networks (DNNs) to solve Reinforcement Learning (RL)\nproblems. While a Genetic Algorithm (GA) is well-suited for exploiting reward\nfunctions that are neither deceptive nor sparse, it struggles when the reward\nfunction is either of those. To that end, Novelty Search (NS) has been shown to\nbe able to outperform gradient-following optimizers in some cases, while\nunder-performing in others. We propose a new algorithm: Explore-Exploit\n$\\gamma$-Adaptive Learner ($E^2\\gamma AL$, or EyAL). By preserving a\ndynamically-sized niche of novelty-seeking agents, the algorithm manages to\nmaintain population diversity, exploiting the reward signal when possible and\nexploring otherwise. The algorithm combines both the exploitation power of a GA\nand the exploration power of NS, while maintaining their simplicity and\nelegance. Our experiments show that EyAL outperforms NS in most scenarios,\nwhile being on par with a GA -- and in some scenarios it can outperform both.\nEyAL also allows the substitution of the exploiting component (GA) and the\nexploring component (NS) with other algorithms, e.g., Evolution Strategy and\nSurprise Search, thus opening the door for future research.\n",
        "published": "2022",
        "authors": [
            "Eyal Segal",
            "Moshe Sipper"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.08484v2",
        "title": "Emergent communication enhances foraging behaviour in evolved swarms\n  controlled by Spiking Neural Networks",
        "abstract": "  Social insects such as ants communicate via pheromones which allows them to\ncoordinate their activity and solve complex tasks as a swarm, e.g. foraging for\nfood. This behavior was shaped through evolutionary processes. In computational\nmodels, self-coordination in swarms has been implemented using probabilistic or\nsimple action rules to shape the decision of each agent and the collective\nbehavior. However, manual tuned decision rules may limit the behavior of the\nswarm. In this work we investigate the emergence of self-coordination and\ncommunication in evolved swarms without defining any explicit rule. We evolve a\nswarm of agents representing an ant colony. We use an evolutionary algorithm to\noptimize a spiking neural network (SNN) which serves as an artificial brain to\ncontrol the behavior of each agent. The goal of the evolved colony is to find\noptimal ways to forage for food and return it to the nest in the shortest\namount of time. In the evolutionary phase, the ants are able to learn to\ncollaborate by depositing pheromone near food piles and near the nest to guide\nother ants. The pheromone usage is not manually encoded into the network;\ninstead, this behavior is established through the optimization procedure. We\nobserve that pheromone-based communication enables the ants to perform better\nin comparison to colonies where communication via pheromone did not emerge. We\nassess the foraging performance by comparing the SNN based model to a rule\nbased system. Our results show that the SNN based model can efficiently\ncomplete the foraging task in a short amount of time. Our approach illustrates\nself coordination via pheromone emerges as a result of the network\noptimization. This work serves as a proof of concept for the possibility of\ncreating complex applications utilizing SNNs as underlying architectures for\nmulti-agent interactions where communication and self-coordination is desired.\n",
        "published": "2022",
        "authors": [
            "Cristian Jimenez Romero",
            "Alper Yegenoglu",
            "Aar\u00f3n P\u00e9rez Mart\u00edn",
            "Sandra Diaz-Pier",
            "Abigail Morrison"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.14624v2",
        "title": "An Auction-based Coordination Strategy for Task-Constrained Multi-Agent\n  Stochastic Planning with Submodular Rewards",
        "abstract": "  In many domains such as transportation and logistics, search and rescue, or\ncooperative surveillance, tasks are pending to be allocated with the\nconsideration of possible execution uncertainties. Existing task coordination\nalgorithms either ignore the stochastic process or suffer from the\ncomputational intensity. Taking advantage of the weakly coupled feature of the\nproblem and the opportunity for coordination in advance, we propose a\ndecentralized auction-based coordination strategy using a newly formulated\nscore function which is generated by forming the problem into task-constrained\nMarkov decision processes (MDPs). The proposed method guarantees convergence\nand at least 50% optimality in the premise of a submodular reward function.\nFurthermore, for the implementation on large-scale applications, an approximate\nvariant of the proposed method, namely Deep Auction, is also suggested with the\nuse of neural networks, which is evasive of the troublesome for constructing\nMDPs. Inspired by the well-known actor-critic architecture, two Transformers\nare used to map observations to action probabilities and cumulative rewards\nrespectively. Finally, we demonstrate the performance of the two proposed\napproaches in the context of drone deliveries, where the stochastic planning\nfor the drone league is cast into a stochastic price-collecting Vehicle Routing\nProblem (VRP) with time windows. Simulation results are compared with\nstate-of-the-art methods in terms of solution quality, planning efficiency and\nscalability.\n",
        "published": "2022",
        "authors": [
            "Ruifan Liu",
            "Hyo-Sang Shin",
            "Binbin Yan",
            "Antonios Tsourdos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.01862v1",
        "title": "Emergent Resource Exchange and Tolerated Theft Behavior using\n  Multi-Agent Reinforcement Learning",
        "abstract": "  For decades, the evolution of cooperation has piqued the interest of numerous\nacademic disciplines such as game theory, economics, biology, and computer\nscience. In this work, we demonstrate the emergence of a novel and effective\nresource exchange protocol formed by dropping and picking up resources in a\nforaging environment. This form of cooperation is made possible by the\nintroduction of a campfire, which adds an extended period of congregation and\ndowntime for agents to explore otherwise unlikely interactions. We find that\nthe agents learn to avoid getting cheated by their exchange partners, but not\nalways from a third party. We also observe the emergence of behavior analogous\nto tolerated theft, despite the lack of any punishment, combat, or larceny\nmechanism in the environment.\n",
        "published": "2023",
        "authors": [
            "Jack Garbus",
            "Jordan Pollack"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1609.01926v1",
        "title": "A modular architecture for transparent computation in Recurrent Neural\n  Networks",
        "abstract": "  Computation is classically studied in terms of automata, formal languages and\nalgorithms; yet, the relation between neural dynamics and symbolic\nrepresentations and operations is still unclear in traditional eliminative\nconnectionism. Therefore, we suggest a unique perspective on this central\nissue, to which we would like to refer as to transparent connectionism, by\nproposing accounts of how symbolic computation can be implemented in neural\nsubstrates. In this study we first introduce a new model of dynamics on a\nsymbolic space, the versatile shift, showing that it supports the real-time\nsimulation of a range of automata. We then show that the Goedelization of\nversatile shifts defines nonlinear dynamical automata, dynamical systems\nevolving on a vectorial space. Finally, we present a mapping between nonlinear\ndynamical automata and recurrent artificial neural networks. The mapping\ndefines an architecture characterized by its granular modularity, where data,\nsymbolic operations and their control are not only distinguishable in\nactivation space, but also spatially localizable in the network itself, while\nmaintaining a distributed encoding of symbolic representations. The resulting\nnetworks simulate automata in real-time and are programmed directly, in absence\nof network training. To discuss the unique characteristics of the architecture\nand their consequences, we present two examples: i) the design of a Central\nPattern Generator from a finite-state locomotive controller, and ii) the\ncreation of a network simulating a system of interactive automata that supports\nthe parsing of garden-path sentences as investigated in psycholinguistics\nexperiments.\n",
        "published": "2016",
        "authors": [
            "Giovanni Sirio Carmantini",
            "Peter beim Graben",
            "Mathieu Desroches",
            "Serafim Rodrigues"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.05005v1",
        "title": "graph2vec: Learning Distributed Representations of Graphs",
        "abstract": "  Recent works on representation learning for graph structured data\npredominantly focus on learning distributed representations of graph\nsubstructures such as nodes and subgraphs. However, many graph analytics tasks\nsuch as graph classification and clustering require representing entire graphs\nas fixed length feature vectors. While the aforementioned approaches are\nnaturally unequipped to learn such representations, graph kernels remain as the\nmost effective way of obtaining them. However, these graph kernels use\nhandcrafted features (e.g., shortest paths, graphlets, etc.) and hence are\nhampered by problems such as poor generalization. To address this limitation,\nin this work, we propose a neural embedding framework named graph2vec to learn\ndata-driven distributed representations of arbitrary sized graphs. graph2vec's\nembeddings are learnt in an unsupervised manner and are task agnostic. Hence,\nthey could be used for any downstream task such as graph classification,\nclustering and even seeding supervised representation learning approaches. Our\nexperiments on several benchmark and large real-world datasets show that\ngraph2vec achieves significant improvements in classification and clustering\naccuracies over substructure representation learning approaches and are\ncompetitive with state-of-the-art graph kernels.\n",
        "published": "2017",
        "authors": [
            "Annamalai Narayanan",
            "Mahinthan Chandramohan",
            "Rajasekar Venkatesan",
            "Lihui Chen",
            "Yang Liu",
            "Shantanu Jaiswal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.03550v1",
        "title": "When BERT Meets Quantum Temporal Convolution Learning for Text\n  Classification in Heterogeneous Computing",
        "abstract": "  The rapid development of quantum computing has demonstrated many unique\ncharacteristics of quantum advantages, such as richer feature representation\nand more secured protection on model parameters. This work proposes a vertical\nfederated learning architecture based on variational quantum circuits to\ndemonstrate the competitive performance of a quantum-enhanced pre-trained BERT\nmodel for text classification. In particular, our proposed hybrid\nclassical-quantum model consists of a novel random quantum temporal convolution\n(QTC) learning framework replacing some layers in the BERT-based decoder. Our\nexperiments on intent classification show that our proposed BERT-QTC model\nattains competitive experimental results in the Snips and ATIS spoken language\ndatasets. Particularly, the BERT-QTC boosts the performance of the existing\nquantum circuit-based language model in two text classification datasets by\n1.57% and 1.52% relative improvements. Furthermore, BERT-QTC can be feasibly\ndeployed on both existing commercial-accessible quantum computation hardware\nand CPU-based interface for ensuring data isolation.\n",
        "published": "2022",
        "authors": [
            "Chao-Han Huck Yang",
            "Jun Qi",
            "Samuel Yen-Chi Chen",
            "Yu Tsao",
            "Pin-Yu Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.14746v1",
        "title": "FlowTransformer: A Transformer Framework for Flow-based Network\n  Intrusion Detection Systems",
        "abstract": "  This paper presents the FlowTransformer framework, a novel approach for\nimplementing transformer-based Network Intrusion Detection Systems (NIDSs).\nFlowTransformer leverages the strengths of transformer models in identifying\nthe long-term behaviour and characteristics of networks, which are often\noverlooked by most existing NIDSs. By capturing these complex patterns in\nnetwork traffic, FlowTransformer offers a flexible and efficient tool for\nresearchers and practitioners in the cybersecurity community who are seeking to\nimplement NIDSs using transformer-based models. FlowTransformer allows the\ndirect substitution of various transformer components, including the input\nencoding, transformer, classification head, and the evaluation of these across\nany flow-based network dataset. To demonstrate the effectiveness and efficiency\nof the FlowTransformer framework, we utilise it to provide an extensive\nevaluation of various common transformer architectures, such as GPT 2.0 and\nBERT, on three commonly used public NIDS benchmark datasets. We provide results\nfor accuracy, model size and speed. A key finding of our evaluation is that the\nchoice of classification head has the most significant impact on the model\nperformance. Surprisingly, Global Average Pooling, which is commonly used in\ntext classification, performs very poorly in the context of NIDS. In addition,\nwe show that model size can be reduced by over 50\\%, and inference and training\ntimes improved, with no loss of accuracy, by making specific choices of input\nencoding and classification head instead of other commonly used alternatives.\n",
        "published": "2023",
        "authors": [
            "Liam Daly Manocchio",
            "Siamak Layeghy",
            "Wai Weng Lo",
            "Gayan K. Kulatilleke",
            "Mohanad Sarhan",
            "Marius Portmann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.05502v1",
        "title": "Bringing order into the realm of Transformer-based language models for\n  artificial intelligence and law",
        "abstract": "  Transformer-based language models (TLMs) have widely been recognized to be a\ncutting-edge technology for the successful development of deep-learning-based\nsolutions to problems and applications that require natural language processing\nand understanding. Like for other textual domains, TLMs have indeed pushed the\nstate-of-the-art of AI approaches for many tasks of interest in the legal\ndomain. Despite the first Transformer model being proposed about six years ago,\nthere has been a rapid progress of this technology at an unprecedented rate,\nwhereby BERT and related models represent a major reference, also in the legal\ndomain. This article provides the first systematic overview of TLM-based\nmethods for AI-driven problems and tasks in the legal sphere. A major goal is\nto highlight research advances in this field so as to understand, on the one\nhand, how the Transformers have contributed to the success of AI in supporting\nlegal processes, and on the other hand, what are the current limitations and\nopportunities for further research development.\n",
        "published": "2023",
        "authors": [
            "Candida M. Greco",
            "Andrea Tagarelli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.13025v1",
        "title": "Powerset multi-class cross entropy loss for neural speaker diarization",
        "abstract": "  Since its introduction in 2019, the whole end-to-end neural diarization\n(EEND) line of work has been addressing speaker diarization as a frame-wise\nmulti-label classification problem with permutation-invariant training. Despite\nEEND showing great promise, a few recent works took a step back and studied the\npossible combination of (local) supervised EEND diarization with (global)\nunsupervised clustering. Yet, these hybrid contributions did not question the\noriginal multi-label formulation. We propose to switch from multi-label (where\nany two speakers can be active at the same time) to powerset multi-class\nclassification (where dedicated classes are assigned to pairs of overlapping\nspeakers). Through extensive experiments on 9 different benchmarks, we show\nthat this formulation leads to significantly better performance (mostly on\noverlapping speech) and robustness to domain mismatch, while eliminating the\ndetection threshold hyperparameter, critical for the multi-label formulation.\n",
        "published": "2023",
        "authors": [
            "Alexis Plaquet",
            "Herv\u00e9 Bredin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.05670v2",
        "title": "Bank distress in the news: Describing events through deep learning",
        "abstract": "  While many models are purposed for detecting the occurrence of significant\nevents in financial systems, the task of providing qualitative detail on the\ndevelopments is not usually as well automated. We present a deep learning\napproach for detecting relevant discussion in text and extracting natural\nlanguage descriptions of events. Supervised by only a small set of event\ninformation, comprising entity names and dates, the model is leveraged by\nunsupervised learning of semantic vector representations on extensive text\ndata. We demonstrate applicability to the study of financial risk based on news\n(6.6M articles), particularly bank distress and government interventions (243\nevents), where indices can signal the level of bank-stress-related reporting at\nthe entity level, or aggregated at national or European level, while being\ncoupled with explanations. Thus, we exemplify how text, as timely, widely\navailable and descriptive data, can serve as a useful complementary source of\ninformation for financial and systemic risk analytics.\n",
        "published": "2016",
        "authors": [
            "Samuel R\u00f6nnqvist",
            "Peter Sarlin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1203.4345v1",
        "title": "Robust Filtering and Smoothing with Gaussian Processes",
        "abstract": "  We propose a principled algorithm for robust Bayesian filtering and smoothing\nin nonlinear stochastic dynamic systems when both the transition function and\nthe measurement function are described by non-parametric Gaussian process (GP)\nmodels. GPs are gaining increasing importance in signal processing, machine\nlearning, robotics, and control for representing unknown system functions by\nposterior probability distributions. This modern way of \"system identification\"\nis more robust than finding point estimates of a parametric function\nrepresentation. In this article, we present a principled algorithm for robust\nanalytic smoothing in GP dynamic systems, which are increasingly used in\nrobotics and control. Our numerical evaluations demonstrate the robustness of\nthe proposed approach in situations where other state-of-the-art Gaussian\nfilters and smoothers can fail.\n",
        "published": "2012",
        "authors": [
            "Marc Peter Deisenroth",
            "Ryan Turner",
            "Marco F. Huber",
            "Uwe D. Hanebeck",
            "Carl Edward Rasmussen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1303.2912v3",
        "title": "Integrated Pre-Processing for Bayesian Nonlinear System Identification\n  with Gaussian Processes",
        "abstract": "  We introduce GP-FNARX: a new model for nonlinear system identification based\non a nonlinear autoregressive exogenous model (NARX) with filtered regressors\n(F) where the nonlinear regression problem is tackled using sparse Gaussian\nprocesses (GP). We integrate data pre-processing with system identification\ninto a fully automated procedure that goes from raw data to an identified\nmodel. Both pre-processing parameters and GP hyper-parameters are tuned by\nmaximizing the marginal likelihood of the probabilistic model. We obtain a\nBayesian model of the system's dynamics which is able to report its uncertainty\nin regions where the data is scarce. The automated approach, the modeling of\nuncertainty and its relatively low computational cost make of GP-FNARX a good\ncandidate for applications in robotics and adaptive control.\n",
        "published": "2013",
        "authors": [
            "Roger Frigola",
            "Carl Edward Rasmussen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.08212v1",
        "title": "Physical problem solving: Joint planning with symbolic, geometric, and\n  dynamic constraints",
        "abstract": "  In this paper, we present a new task that investigates how people interact\nwith and make judgments about towers of blocks. In Experiment~1, participants\nin the lab solved a series of problems in which they had to re-configure three\nblocks from an initial to a final configuration. We recorded whether they used\none hand or two hands to do so. In Experiment~2, we asked participants online\nto judge whether they think the person in the lab used one or two hands. The\nresults revealed a close correspondence between participants' actions in the\nlab, and the mental simulations of participants online. To explain\nparticipants' actions and mental simulations, we develop a model that plans\nover a symbolic representation of the situation, executes the plan using a\ngeometric solver, and checks the plan's feasibility by taking into account the\nphysical constraints of the scene. Our model explains participants' actions and\njudgments to a high degree of quantitative accuracy.\n",
        "published": "2017",
        "authors": [
            "Ilker Yildirim",
            "Tobias Gerstenberg",
            "Basil Saeed",
            "Marc Toussaint",
            "Josh Tenenbaum"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.08130v1",
        "title": "Discover Life Skills for Planning with Bandits via Observing and\n  Learning How the World Works",
        "abstract": "  We propose a novel approach for planning agents to compose abstract skills\nvia observing and learning from historical interactions with the world. Our\nframework operates in a Markov state-space model via a set of actions under\nunknown pre-conditions. We formulate skills as high-level abstract policies\nthat propose action plans based on the current state. Each policy learns new\nplans by observing the states' transitions while the agent interacts with the\nworld. Such an approach automatically learns new plans to achieve specific\nintended effects, but the success of such plans is often dependent on the\nstates in which they are applicable. Therefore, we formulate the evaluation of\nsuch plans as infinitely many multi-armed bandit problems, where we balance the\nallocation of resources on evaluating the success probability of existing arms\nand exploring new options. The result is a planner capable of automatically\nlearning robust high-level skills under a noisy environment; such skills\nimplicitly learn the action pre-condition without explicit knowledge. We show\nthat this planning approach is experimentally very competitive in\nhigh-dimensional state space domains.\n",
        "published": "2022",
        "authors": [
            "Tin Lai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.11070v2",
        "title": "Universal Morphology Control via Contextual Modulation",
        "abstract": "  Learning a universal policy across different robot morphologies can\nsignificantly improve learning efficiency and generalization in continuous\ncontrol. However, it poses a challenging multi-task reinforcement learning\nproblem, as the optimal policy may be quite different across robots and\ncritically depend on the morphology. Existing methods utilize graph neural\nnetworks or transformers to handle heterogeneous state and action spaces across\ndifferent morphologies, but pay little attention to the dependency of a robot's\ncontrol policy on its morphology context. In this paper, we propose a\nhierarchical architecture to better model this dependency via contextual\nmodulation, which includes two key submodules: (1) Instead of enforcing hard\nparameter sharing across robots, we use hypernetworks to generate\nmorphology-dependent control parameters; (2) We propose a fixed attention\nmechanism that solely depends on the morphology to modulate the interactions\nbetween different limbs in a robot. Experimental results show that our method\nnot only improves learning performance on a diverse set of training robots, but\nalso generalizes better to unseen morphologies in a zero-shot fashion.\n",
        "published": "2023",
        "authors": [
            "Zheng Xiong",
            "Jacob Beck",
            "Shimon Whiteson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.08841v1",
        "title": "Leveraging Optimal Transport for Enhanced Offline Reinforcement Learning\n  in Surgical Robotic Environments",
        "abstract": "  Most Reinforcement Learning (RL) methods are traditionally studied in an\nactive learning setting, where agents directly interact with their\nenvironments, observe action outcomes, and learn through trial and error.\nHowever, allowing partially trained agents to interact with real physical\nsystems poses significant challenges, including high costs, safety risks, and\nthe need for constant supervision. Offline RL addresses these cost and safety\nconcerns by leveraging existing datasets and reducing the need for\nresource-intensive real-time interactions. Nevertheless, a substantial\nchallenge lies in the demand for these datasets to be meticulously annotated\nwith rewards. In this paper, we introduce Optimal Transport Reward (OTR)\nlabelling, an innovative algorithm designed to assign rewards to offline\ntrajectories, using a small number of high-quality expert demonstrations. The\ncore principle of OTR involves employing Optimal Transport (OT) to calculate an\noptimal alignment between an unlabeled trajectory from the dataset and an\nexpert demonstration. This alignment yields a similarity measure that is\neffectively interpreted as a reward signal. An offline RL algorithm can then\nutilize these reward signals to learn a policy. This approach circumvents the\nneed for handcrafted rewards, unlocking the potential to harness vast datasets\nfor policy learning. Leveraging the SurRoL simulation platform tailored for\nsurgical robot learning, we generate datasets and employ them to train policies\nusing the OTR algorithm. By demonstrating the efficacy of OTR in a different\ndomain, we emphasize its versatility and its potential to expedite RL\ndeployment across a wide range of fields.\n",
        "published": "2023",
        "authors": [
            "Maryam Zare",
            "Parham M. Kebria",
            "Abbas Khosravi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1510.00331v3",
        "title": "Multimodal Hierarchical Dirichlet Process-based Active Perception",
        "abstract": "  In this paper, we propose an active perception method for recognizing object\ncategories based on the multimodal hierarchical Dirichlet process (MHDP). The\nMHDP enables a robot to form object categories using multimodal information,\ne.g., visual, auditory, and haptic information, which can be observed by\nperforming actions on an object. However, performing many actions on a target\nobject requires a long time. In a real-time scenario, i.e., when the time is\nlimited, the robot has to determine the set of actions that is most effective\nfor recognizing a target object. We propose an MHDP-based active perception\nmethod that uses the information gain (IG) maximization criterion and lazy\ngreedy algorithm. We show that the IG maximization criterion is optimal in the\nsense that the criterion is equivalent to a minimization of the expected\nKullback--Leibler divergence between a final recognition state and the\nrecognition state after the next set of actions. However, a straightforward\ncalculation of IG is practically impossible. Therefore, we derive an efficient\nMonte Carlo approximation method for IG by making use of a property of the\nMHDP. We also show that the IG has submodular and non-decreasing properties as\na set function because of the structure of the graphical model of the MHDP.\nTherefore, the IG maximization problem is reduced to a submodular maximization\nproblem. This means that greedy and lazy greedy algorithms are effective and\nhave a theoretical justification for their performance. We conducted an\nexperiment using an upper-torso humanoid robot and a second one using synthetic\ndata. The experimental results show that the method enables the robot to select\na set of actions that allow it to recognize target objects quickly and\naccurately. The results support our theoretical outcomes.\n",
        "published": "2015",
        "authors": [
            "Tadahiro Taniguchi",
            "Toshiaki Takano",
            "Ryo Yoshino"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1605.09735v1",
        "title": "Information Theoretically Aided Reinforcement Learning for Embodied\n  Agents",
        "abstract": "  Reinforcement learning for embodied agents is a challenging problem. The\naccumulated reward to be optimized is often a very rugged function, and\ngradient methods are impaired by many local optimizers. We demonstrate, in an\nexperimental setting, that incorporating an intrinsic reward can smoothen the\noptimization landscape while preserving the global optimizers of interest. We\nshow that policy gradient optimization for locomotion in a complex morphology\nis significantly improved when supplementing the extrinsic reward by an\nintrinsic reward defined in terms of the mutual information of time consecutive\nsensor readings.\n",
        "published": "2016",
        "authors": [
            "Guido Montufar",
            "Keyan Ghazi-Zahedi",
            "Nihat Ay"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.05767v2",
        "title": "Motion Planning Networks",
        "abstract": "  Fast and efficient motion planning algorithms are crucial for many\nstate-of-the-art robotics applications such as self-driving cars. Existing\nmotion planning methods become ineffective as their computational complexity\nincreases exponentially with the dimensionality of the motion planning problem.\nTo address this issue, we present Motion Planning Networks (MPNet), a neural\nnetwork-based novel planning algorithm. The proposed method encodes the given\nworkspaces directly from a point cloud measurement and generates the end-to-end\ncollision-free paths for the given start and goal configurations. We evaluate\nMPNet on various 2D and 3D environments including the planning of a 7 DOF\nBaxter robot manipulator. The results show that MPNet is not only consistently\ncomputationally efficient in all environments but also generalizes to\ncompletely unseen environments. The results also show that the computation time\nof MPNet consistently remains less than 1 second in all presented experiments,\nwhich is significantly lower than existing state-of-the-art motion planning\nalgorithms.\n",
        "published": "2018",
        "authors": [
            "Ahmed H. Qureshi",
            "Anthony Simeonov",
            "Mayur J. Bency",
            "Michael C. Yip"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.09735v3",
        "title": "Deep Haptic Model Predictive Control for Robot-Assisted Dressing",
        "abstract": "  Robot-assisted dressing offers an opportunity to benefit the lives of many\npeople with disabilities, such as some older adults. However, robots currently\nlack common sense about the physical implications of their actions on people.\nThe physical implications of dressing are complicated by non-rigid garments,\nwhich can result in a robot indirectly applying high forces to a person's body.\nWe present a deep recurrent model that, when given a proposed action by the\nrobot, predicts the forces a garment will apply to a person's body. We also\nshow that a robot can provide better dressing assistance by using this model\nwith model predictive control. The predictions made by our model only use\nhaptic and kinematic observations from the robot's end effector, which are\nreadily attainable. Collecting training data from real world physical\nhuman-robot interaction can be time consuming, costly, and put people at risk.\nInstead, we train our predictive model using data collected in an entirely\nself-supervised fashion from a physics-based simulation. We evaluated our\napproach with a PR2 robot that attempted to pull a hospital gown onto the arms\nof 10 human participants. With a 0.2s prediction horizon, our controller\nsucceeded at high rates and lowered applied force while navigating the garment\naround a persons fist and elbow without getting caught. Shorter prediction\nhorizons resulted in significantly reduced performance with the sleeve catching\non the participants' fists and elbows, demonstrating the value of our model's\npredictions. These behaviors of mitigating catches emerged from our deep\npredictive model and the controller objective function, which primarily\npenalizes high forces.\n",
        "published": "2017",
        "authors": [
            "Zackory Erickson",
            "Henry M. Clever",
            "Greg Turk",
            "C. Karen Liu",
            "Charles C. Kemp"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1708.06374v6",
        "title": "On a Formal Model of Safe and Scalable Self-driving Cars",
        "abstract": "  In recent years, car makers and tech companies have been racing towards self\ndriving cars. It seems that the main parameter in this race is who will have\nthe first car on the road. The goal of this paper is to add to the equation two\nadditional crucial parameters. The first is standardization of safety assurance\n--- what are the minimal requirements that every self-driving car must satisfy,\nand how can we verify these requirements. The second parameter is scalability\n--- engineering solutions that lead to unleashed costs will not scale to\nmillions of cars, which will push interest in this field into a niche academic\ncorner, and drive the entire field into a \"winter of autonomous driving\". In\nthe first part of the paper we propose a white-box, interpretable, mathematical\nmodel for safety assurance, which we call Responsibility-Sensitive Safety\n(RSS). In the second part we describe a design of a system that adheres to our\nsafety assurance requirements and is scalable to millions of cars.\n",
        "published": "2017",
        "authors": [
            "Shai Shalev-Shwartz",
            "Shaked Shammah",
            "Amnon Shashua"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.11164v1",
        "title": "Decentralized Deep Reinforcement Learning for a Distributed and Adaptive\n  Locomotion Controller of a Hexapod Robot",
        "abstract": "  Locomotion is a prime example for adaptive behavior in animals and biological\ncontrol principles have inspired control architectures for legged robots. While\nmachine learning has been successfully applied to many tasks in recent years,\nDeep Reinforcement Learning approaches still appear to struggle when applied to\nreal world robots in continuous control tasks and in particular do not appear\nas robust solutions that can handle uncertainties well. Therefore, there is a\nnew interest in incorporating biological principles into such learning\narchitectures. While inducing a hierarchical organization as found in motor\ncontrol has shown already some success, we here propose a decentralized\norganization as found in insect motor control for coordination of different\nlegs. A decentralized and distributed architecture is introduced on a simulated\nhexapod robot and the details of the controller are learned through Deep\nReinforcement Learning. We first show that such a concurrent local structure is\nable to learn better walking behavior. Secondly, that the simpler organization\nis learned faster compared to holistic approaches.\n",
        "published": "2020",
        "authors": [
            "Malte Schilling",
            "Kai Konen",
            "Frank W. Ohl",
            "Timo Korthals"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.02040v3",
        "title": "Towards Learning to Play Piano with Dexterous Hands and Touch",
        "abstract": "  The virtuoso plays the piano with passion, poetry and extraordinary technical\nability. As Liszt said (a virtuoso)must call up scent and blossom, and breathe\nthe breath of life. The strongest robots that can play a piano are based on a\ncombination of specialized robot hands/piano and hardcoded planning algorithms.\nIn contrast to that, in this paper, we demonstrate how an agent can learn\ndirectly from machine-readable music score to play the piano with dexterous\nhands on a simulated piano using reinforcement learning (RL) from scratch. We\ndemonstrate the RL agents can not only find the correct key position but also\ndeal with various rhythmic, volume and fingering, requirements. We achieve this\nby using a touch-augmented reward and a novel curriculum of tasks. We conclude\nby carefully studying the important aspects to enable such learning algorithms\nand that can potentially shed light on future research in this direction.\n",
        "published": "2021",
        "authors": [
            "Huazhe Xu",
            "Yuping Luo",
            "Shaoxiong Wang",
            "Trevor Darrell",
            "Roberto Calandra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1401.1465v1",
        "title": "Cortical prediction markets",
        "abstract": "  We investigate cortical learning from the perspective of mechanism design.\nFirst, we show that discretizing standard models of neurons and synaptic\nplasticity leads to rational agents maximizing simple scoring rules. Second,\nour main result is that the scoring rules are proper, implying that neurons\nfaithfully encode expected utilities in their synaptic weights and encode\nhigh-scoring outcomes in their spikes. Third, with this foundation in hand, we\npropose a biologically plausible mechanism whereby neurons backpropagate\nincentives which allows them to optimize their usefulness to the rest of\ncortex. Finally, experiments show that networks that backpropagate incentives\ncan learn simple tasks.\n",
        "published": "2014",
        "authors": [
            "David Balduzzi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.14002v1",
        "title": "A Distributed Model-Free Algorithm for Multi-hop Ride-sharing using Deep\n  Reinforcement Learning",
        "abstract": "  The growth of autonomous vehicles, ridesharing systems, and self driving\ntechnology will bring a shift in the way ride hailing platforms plan out their\nservices. However, these advances in technology coupled with road congestion,\nenvironmental concerns, fuel usage, vehicles emissions, and the high cost of\nthe vehicle usage have brought more attention to better utilize the use of\nvehicles and their capacities. In this paper, we propose a novel multi-hop\nride-sharing (MHRS) algorithm that uses deep reinforcement learning to learn\noptimal vehicle dispatch and matching decisions by interacting with the\nexternal environment. By allowing customers to transfer between vehicles, i.e.,\nride with one vehicle for sometime and then transfer to another one, MHRS helps\nin attaining 30\\% lower cost and 20\\% more efficient utilization of fleets, as\ncompared to the ride-sharing algorithms. This flexibility of multi-hop feature\ngives a seamless experience to customers and ride-sharing companies, and thus\nimproves ride-sharing services.\n",
        "published": "2019",
        "authors": [
            "Ashutosh Singh",
            "Abubakr Alabbasi",
            "Vaneet Aggarwal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.00498v1",
        "title": "Optimization for Reinforcement Learning: From Single Agent to\n  Cooperative Agents",
        "abstract": "  This article reviews recent advances in multi-agent reinforcement learning\nalgorithms for large-scale control systems and communication networks, which\nlearn to communicate and cooperate. We provide an overview of this emerging\nfield, with an emphasis on the decentralized setting under different\ncoordination protocols. We highlight the evolution of reinforcement learning\nalgorithms from single-agent to multi-agent systems, from a distributed\noptimization perspective, and conclude with future directions and challenges,\nin the hope to catalyze the growing synergy among distributed optimization,\nsignal processing, and reinforcement learning communities.\n",
        "published": "2019",
        "authors": [
            "Donghwan Lee",
            "Niao He",
            "Parameswaran Kamalaruban",
            "Volkan Cevher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.13122v7",
        "title": "Towards Regulated Deep Learning",
        "abstract": "  Regulation of Multi-Agent Systems (MAS) and Declarative Electronic\nInstitutions (DEIs) was a multidisciplinary research topic of the past decade\ninvolving (Physical and Software) Agents and Law since the beginning, but\nrecently evolved towards News-claimed Robot Lawyer since 2016. One of these\nfirst proposals of restricting the behaviour of Software Agents was Electronic\nInstitutions.However, with the recent reformulation of Artificial Neural\nNetworks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal\nissues regarding the use of DL has raised concerns in the Artificial\nIntelligence (AI) Community. Now that the Regulation of MAS is almost correctly\naddressed, we propose the Regulation of Artificial Neural Networks as\nAgent-based Training of a special type of regulated Artificial Neural Network\nthat we call Institutional Neural Network (INN).The main purpose of this paper\nis to bring attention to Artificial Teaching (AT) and to give a tentative\nanswer showing a proof-of-concept implementation of Regulated Deep Learning\n(RDL). This paper introduces the former concept and provide $I^*$, a language\npreviously used to model declaratively and extend Electronic Institutions, as a\nmeans to regulate the execution of Artificial Neural Networks and their\ninteractions with Artificial Teachers (ATs)\n",
        "published": "2019",
        "authors": [
            "Andr\u00e9s Garc\u00eda-Camino"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.06580v3",
        "title": "Online Learning in Iterated Prisoner's Dilemma to Mimic Human Behavior",
        "abstract": "  As an important psychological and social experiment, the Iterated Prisoner's\nDilemma (IPD) treats the choice to cooperate or defect as an atomic action. We\npropose to study the behaviors of online learning algorithms in the Iterated\nPrisoner's Dilemma (IPD) game, where we investigate the full spectrum of\nreinforcement learning agents: multi-armed bandits, contextual bandits and\nreinforcement learning. We evaluate them based on a tournament of iterated\nprisoner's dilemma where multiple agents can compete in a sequential fashion.\nThis allows us to analyze the dynamics of policies learned by multiple\nself-interested independent reward-driven agents, and also allows us study the\ncapacity of these algorithms to fit the human behaviors. Results suggest that\nconsidering the current situation to make decision is the worst in this kind of\nsocial dilemma game. Multiples discoveries on online learning behaviors and\nclinical validations are stated, as an effort to connect artificial\nintelligence algorithms with human behaviors and their abnormal states in\nneuropsychiatric conditions.\n",
        "published": "2020",
        "authors": [
            "Baihan Lin",
            "Djallel Bouneffouf",
            "Guillermo Cecchi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.13699v2",
        "title": "FlexPool: A Distributed Model-Free Deep Reinforcement Learning Algorithm\n  for Joint Passengers & Goods Transportation",
        "abstract": "  The growth in online goods delivery is causing a dramatic surge in urban\nvehicle traffic from last-mile deliveries. On the other hand, ride-sharing has\nbeen on the rise with the success of ride-sharing platforms and increased\nresearch on using autonomous vehicle technologies for routing and matching. The\nfuture of urban mobility for passengers and goods relies on leveraging new\nmethods that minimize operational costs and environmental footprints of\ntransportation systems.\n  This paper considers combining passenger transportation with goods delivery\nto improve vehicle-based transportation. Even though the problem has been\nstudied with a defined dynamics model of the transportation system environment,\nthis paper considers a model-free approach that has been demonstrated to be\nadaptable to new or erratic environment dynamics. We propose FlexPool, a\ndistributed model-free deep reinforcement learning algorithm that jointly\nserves passengers & goods workloads by learning optimal dispatch policies from\nits interaction with the environment. The proposed algorithm pools passengers\nfor a ride-sharing service and delivers goods using a multi-hop transit method.\nThese flexibilities decrease the fleet's operational cost and environmental\nfootprint while maintaining service levels for passengers and goods. Through\nsimulations on a realistic multi-agent urban mobility platform, we demonstrate\nthat FlexPool outperforms other model-free settings in serving the demands from\npassengers & goods. FlexPool achieves 30% higher fleet utilization and 35%\nhigher fuel efficiency in comparison to (i) model-free approaches where\nvehicles transport a combination of passengers & goods without the use of\nmulti-hop transit, and (ii) model-free approaches where vehicles exclusively\ntransport either passengers or goods.\n",
        "published": "2020",
        "authors": [
            "Kaushik Manchella",
            "Abhishek K. Umrawal",
            "Vaneet Aggarwal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.14358v2",
        "title": "Faster Game Solving via Predictive Blackwell Approachability: Connecting\n  Regret Matching and Mirror Descent",
        "abstract": "  Blackwell approachability is a framework for reasoning about repeated games\nwith vector-valued payoffs. We introduce predictive Blackwell approachability,\nwhere an estimate of the next payoff vector is given, and the decision maker\ntries to achieve better performance based on the accuracy of that estimator. In\norder to derive algorithms that achieve predictive Blackwell approachability,\nwe start by showing a powerful connection between four well-known algorithms.\nFollow-the-regularized-leader (FTRL) and online mirror descent (OMD) are the\nmost prevalent regret minimizers in online convex optimization. In spite of\nthis prevalence, the regret matching (RM) and regret matching+ (RM+) algorithms\nhave been preferred in the practice of solving large-scale games (as the local\nregret minimizers within the counterfactual regret minimization framework). We\nshow that RM and RM+ are the algorithms that result from running FTRL and OMD,\nrespectively, to select the halfspace to force at all times in the underlying\nBlackwell approachability game. By applying the predictive variants of FTRL or\nOMD to this connection, we obtain predictive Blackwell approachability\nalgorithms, as well as predictive variants of RM and RM+. In experiments across\n18 common zero-sum extensive-form benchmark games, we show that predictive RM+\ncoupled with counterfactual regret minimization converges vastly faster than\nthe fastest prior algorithms (CFR+, DCFR, LCFR) across all games but two of the\npoker games, sometimes by two or more orders of magnitude.\n",
        "published": "2020",
        "authors": [
            "Gabriele Farina",
            "Christian Kroer",
            "Tuomas Sandholm"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.07385v2",
        "title": "Autonomous synthesis of metastable materials",
        "abstract": "  Autonomous experimentation enabled by artificial intelligence (AI) offers a\nnew paradigm for accelerating scientific discovery. Non-equilibrium materials\nsynthesis is emblematic of complex, resource-intensive experimentation whose\nacceleration would be a watershed for materials discovery and development. The\nmapping of non-equilibrium synthesis phase diagrams has recently been\naccelerated via high throughput experimentation but still limits materials\nresearch because the parameter space is too vast to be exhaustively explored.\nWe demonstrate accelerated synthesis and exploration of metastable materials\nthrough hierarchical autonomous experimentation governed by the Scientific\nAutonomous Reasoning Agent (SARA). SARA integrates robotic materials synthesis\nand characterization along with a hierarchy of AI methods that efficiently\nreveal the structure of processing phase diagrams. SARA designs lateral\ngradient laser spike annealing (lg-LSA) experiments for parallel materials\nsynthesis and employs optical spectroscopy to rapidly identify phase\ntransitions. Efficient exploration of the multi-dimensional parameter space is\nachieved with nested active learning (AL) cycles built upon advanced machine\nlearning models that incorporate the underlying physics of the experiments as\nwell as end-to-end uncertainty quantification. With this, and the coordination\nof AL at multiple scales, SARA embodies AI harnessing of complex scientific\ntasks. We demonstrate its performance by autonomously mapping synthesis phase\nboundaries for the Bi$_2$O$_3$ system, leading to orders-of-magnitude\nacceleration in establishment of a synthesis phase diagram that includes\nconditions for kinetically stabilizing $\\delta$-Bi$_2$O$_3$ at room\ntemperature, a critical development for electrochemical technologies such as\nsolid oxide fuel cells.\n",
        "published": "2021",
        "authors": [
            "Sebastian Ament",
            "Maximilian Amsler",
            "Duncan R. Sutherland",
            "Ming-Chiang Chang",
            "Dan Guevarra",
            "Aine B. Connolly",
            "John M. Gregoire",
            "Michael O. Thompson",
            "Carla P. Gomes",
            "R. Bruce van Dover"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.08074v2",
        "title": "Flocking and Collision Avoidance for a Dynamic Squad of Fixed-Wing UAVs\n  Using Deep Reinforcement Learning",
        "abstract": "  Developing the flocking behavior for a dynamic squad of fixed-wing UAVs is\nstill a challenge due to kinematic complexity and environmental uncertainty. In\nthis paper, we deal with the decentralized flocking and collision avoidance\nproblem through deep reinforcement learning (DRL). Specifically, we formulate a\ndecentralized DRL-based decision making framework from the perspective of every\nfollower, where a collision avoidance mechanism is integrated into the flocking\ncontroller. Then, we propose a novel reinforcement learning algorithm PS-CACER\nfor training a shared control policy for all the followers. Besides, we design\na plug-n-play embedding module based on convolutional neural networks and the\nattention mechanism. As a result, the variable-length system state can be\nencoded into a fixed-length embedding vector, which makes the learned DRL\npolicy independent with the number and the order of followers. Finally,\nnumerical simulation results demonstrate the effectiveness of the proposed\nmethod, and the learned policies can be directly transferred to semi-physical\nsimulation without any parameter finetuning.\n",
        "published": "2021",
        "authors": [
            "Chao Yan",
            "Xiaojia Xiang",
            "Chang Wang",
            "Zhen Lan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.03371v1",
        "title": "The Synergy of Complex Event Processing and Tiny Machine Learning in\n  Industrial IoT",
        "abstract": "  Focusing on comprehensive networking, big data, and artificial intelligence,\nthe Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness\nin factory operations. Various sensors and field devices play a central role,\nas they generate a vast amount of real-time data that can provide insights into\nmanufacturing. The synergy of complex event processing (CEP) and machine\nlearning (ML) has been developed actively in the last years in IIoT to identify\npatterns in heterogeneous data streams and fuse raw data into tangible facts.\nIn a traditional compute-centric paradigm, the raw field data are continuously\nsent to the cloud and processed centrally. As IIoT devices become increasingly\npervasive and ubiquitous, concerns are raised since transmitting such amount of\ndata is energy-intensive, vulnerable to be intercepted, and subjected to high\nlatency. The data-centric paradigm can essentially solve these problems by\nempowering IIoT to perform decentralized on-device ML and CEP, keeping data\nprimarily on edge devices and minimizing communications. However, this is no\nmean feat because most IIoT edge devices are designed to be computationally\nconstrained with low power consumption. This paper proposes a framework that\nexploits ML and CEP's synergy at the edge in distributed sensor networks. By\nleveraging tiny ML and micro CEP, we shift the computation from the cloud to\nthe power-constrained IIoT devices and allow users to adapt the on-device ML\nmodel and the CEP reasoning logic flexibly on the fly without requiring to\nreupload the whole program. Lastly, we evaluate the proposed solution and show\nits effectiveness and feasibility using an industrial use case of machine\nsafety monitoring.\n",
        "published": "2021",
        "authors": [
            "Haoyu Ren",
            "Darko Anicic",
            "Thomas Runkler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.08158v2",
        "title": "The Confluence of Networks, Games and Learning",
        "abstract": "  Recent years have witnessed significant advances in technologies and services\nin modern network applications, including smart grid management, wireless\ncommunication, cybersecurity as well as multi-agent autonomous systems.\nConsidering the heterogeneous nature of networked entities, emerging network\napplications call for game-theoretic models and learning-based approaches in\norder to create distributed network intelligence that responds to uncertainties\nand disruptions in a dynamic or an adversarial environment. This paper\narticulates the confluence of networks, games and learning, which establishes a\ntheoretical underpinning for understanding multi-agent decision-making over\nnetworks. We provide an selective overview of game-theoretic learning\nalgorithms within the framework of stochastic approximation theory, and\nassociated applications in some representative contexts of modern network\nsystems, such as the next generation wireless communication networks, the smart\ngrid and distributed machine learning. In addition to existing research works\non game-theoretic learning over networks, we highlight several new angles and\nresearch endeavors on learning in games that are related to recent developments\nin artificial intelligence. Some of the new angles extrapolate from our own\nresearch interests. The overall objective of the paper is to provide the reader\na clear picture of the strengths and challenges of adopting game-theoretic\nlearning methods within the context of network systems, and further to identify\nfruitful future research directions on both theoretical and applied studies.\n",
        "published": "2021",
        "authors": [
            "Tao Li",
            "Guanze Peng",
            "Quanyan Zhu",
            "Tamer Basar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.13284v1",
        "title": "A Modular and Transferable Reinforcement Learning Framework for the\n  Fleet Rebalancing Problem",
        "abstract": "  Mobility on demand (MoD) systems show great promise in realizing flexible and\nefficient urban transportation. However, significant technical challenges arise\nfrom operational decision making associated with MoD vehicle dispatch and fleet\nrebalancing. For this reason, operators tend to employ simplified algorithms\nthat have been demonstrated to work well in a particular setting. To help\nbridge the gap between novel and existing methods, we propose a modular\nframework for fleet rebalancing based on model-free reinforcement learning (RL)\nthat can leverage an existing dispatch method to minimize system cost. In\nparticular, by treating dispatch as part of the environment dynamics, a\ncentralized agent can learn to intermittently direct the dispatcher to\nreposition free vehicles and mitigate against fleet imbalance. We formulate RL\nstate and action spaces as distributions over a grid partitioning of the\noperating area, making the framework scalable and avoiding the complexities\nassociated with multiagent RL. Numerical experiments, using real-world trip and\nnetwork data, demonstrate that this approach has several distinct advantages\nover baseline methods including: improved system cost; high degree of\nadaptability to the selected dispatch method; and the ability to perform\nscale-invariant transfer learning between problem instances with similar\nvehicle and request distributions.\n",
        "published": "2021",
        "authors": [
            "Erotokritos Skordilis",
            "Yi Hou",
            "Charles Tripp",
            "Matthew Moniot",
            "Peter Graf",
            "David Biagioni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.14329v1",
        "title": "GINA: Neural Relational Inference From Independent Snapshots",
        "abstract": "  Dynamical systems in which local interactions among agents give rise to\ncomplex emerging phenomena are ubiquitous in nature and society. This work\nexplores the problem of inferring the unknown interaction structure\n(represented as a graph) of such a system from measurements of its constituent\nagents or individual components (represented as nodes). We consider a setting\nwhere the underlying dynamical model is unknown and where different\nmeasurements (i.e., snapshots) may be independent (e.g., may stem from\ndifferent experiments). We propose GINA (Graph Inference Network Architecture),\na graph neural network (GNN) to simultaneously learn the latent interaction\ngraph and, conditioned on the interaction graph, the prediction of a node's\nobservable state based on adjacent vertices. GINA is based on the hypothesis\nthat the ground truth interaction graph -- among all other potential graphs --\nallows to predict the state of a node, given the states of its neighbors, with\nthe highest accuracy. We test this hypothesis and demonstrate GINA's\neffectiveness on a wide range of interaction graphs and dynamical processes.\n",
        "published": "2021",
        "authors": [
            "Gerrit Gro\u00dfmann",
            "Julian Zimmerlin",
            "Michael Backenk\u00f6hler",
            "Verena Wolf"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.04487v1",
        "title": "ARC: Adversarially Robust Control Policies for Autonomous Vehicles",
        "abstract": "  Deep neural networks have demonstrated their capability to learn control\npolicies for a variety of tasks. However, these neural network-based policies\nhave been shown to be susceptible to exploitation by adversarial agents.\nTherefore, there is a need to develop techniques to learn control policies that\nare robust against adversaries. We introduce Adversarially Robust Control\n(ARC), which trains the protagonist policy and the adversarial policy\nend-to-end on the same loss. The aim of the protagonist is to maximise this\nloss, whilst the adversary is attempting to minimise it. We demonstrate the\nproposed ARC training in a highway driving scenario, where the protagonist\ncontrols the follower vehicle whilst the adversary controls the lead vehicle.\nBy training the protagonist against an ensemble of adversaries, it learns a\nsignificantly more robust control policy, which generalises to a variety of\nadversarial strategies. The approach is shown to reduce the amount of\ncollisions against new adversaries by up to 90.25%, compared to the original\npolicy. Moreover, by utilising an auxiliary distillation loss, we show that the\nfine-tuned control policy shows no drop in performance across its original\ntraining distribution.\n",
        "published": "2021",
        "authors": [
            "Sampo Kuutti",
            "Saber Fallah",
            "Richard Bowden"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.09113v1",
        "title": "How to Manage Tiny Machine Learning at Scale: An Industrial Perspective",
        "abstract": "  Tiny machine learning (TinyML) has gained widespread popularity where machine\nlearning (ML) is democratized on ubiquitous microcontrollers, processing sensor\ndata everywhere in real-time. To manage TinyML in the industry, where mass\ndeployment happens, we consider the hardware and software constraints, ranging\nfrom available onboard sensors and memory size to ML-model architectures and\nruntime platforms. However, Internet of Things (IoT) devices are typically\ntailored to specific tasks and are subject to heterogeneity and limited\nresources. Moreover, TinyML models have been developed with different\nstructures and are often distributed without a clear understanding of their\nworking principles, leading to a fragmented ecosystem. Considering these\nchallenges, we propose a framework using Semantic Web technologies to enable\nthe joint management of TinyML models and IoT devices at scale, from modeling\ninformation to discovering possible combinations and benchmarking, and\neventually facilitate TinyML component exchange and reuse. We present an\nontology (semantic schema) for neural network models aligned with the World\nWide Web Consortium (W3C) Thing Description, which semantically describes IoT\ndevices. Furthermore, a Knowledge Graph of 23 publicly available ML models and\nsix IoT devices were used to demonstrate our concept in three case studies, and\nwe shared the code and examples to enhance reproducibility:\nhttps://github.com/Haoyu-R/How-to-Manage-TinyML-at-Scale\n",
        "published": "2022",
        "authors": [
            "Haoyu Ren",
            "Darko Anicic",
            "Thomas Runkler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.10122v2",
        "title": "HCMD-zero: Learning Value Aligned Mechanisms from Data",
        "abstract": "  Artificial learning agents are mediating a larger and larger number of\ninteractions among humans, firms, and organizations, and the intersection\nbetween mechanism design and machine learning has been heavily investigated in\nrecent years. However, mechanism design methods often make strong assumptions\non how participants behave (e.g. rationality), on the kind of knowledge\ndesigners have access to a priori (e.g. access to strong baseline mechanisms),\nor on what the goal of the mechanism should be (e.g. total welfare). Here we\nintroduce HCMD-zero, a general purpose method to construct mechanisms making\nnone of these three assumptions. HCMD-zero learns to mediate interactions among\nparticipants and adjusts the mechanism parameters to make itself more likely to\nbe preferred by participants. It does so by remaining engaged in an electoral\ncontest with copies of itself, thereby accessing direct feedback from\nparticipants. We test our method on a stylized resource allocation game that\nhighlights the tension between productivity, equality and the temptation to\nfree ride. HCMD-zero produces a mechanism that is preferred by human\nparticipants over a strong baseline, it does so automatically, without\nrequiring prior knowledge, and using human behavioral trajectories sparingly\nand effectively. Our analysis shows HCMD-zero consistently makes the mechanism\npolicy more and more likely to be preferred by human participants over the\ncourse of training, and that it results in a mechanism with an interpretable\nand intuitive policy.\n",
        "published": "2022",
        "authors": [
            "Jan Balaguer",
            "Raphael Koster",
            "Ari Weinstein",
            "Lucy Campbell-Gillingham",
            "Christopher Summerfield",
            "Matthew Botvinick",
            "Andrea Tacchetti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.10135v1",
        "title": "The Good Shepherd: An Oracle Agent for Mechanism Design",
        "abstract": "  From social networks to traffic routing, artificial learning agents are\nplaying a central role in modern institutions. We must therefore understand how\nto leverage these systems to foster outcomes and behaviors that align with our\nown values and aspirations. While multiagent learning has received considerable\nattention in recent years, artificial agents have been primarily evaluated when\ninteracting with fixed, non-learning co-players. While this evaluation scheme\nhas merit, it fails to capture the dynamics faced by institutions that must\ndeal with adaptive and continually learning constituents. Here we address this\nlimitation, and construct agents (\"mechanisms\") that perform well when\nevaluated over the learning trajectory of their adaptive co-players\n(\"participants\"). The algorithm we propose consists of two nested learning\nloops: an inner loop where participants learn to best respond to fixed\nmechanisms; and an outer loop where the mechanism agent updates its policy\nbased on experience. We report the performance of our mechanism agents when\npaired with both artificial learning agents and humans as co-players. Our\nresults show that our mechanisms are able to shepherd the participants\nstrategies towards favorable outcomes, indicating a path for modern\ninstitutions to effectively and automatically influence the strategies and\nbehaviors of their constituents.\n",
        "published": "2022",
        "authors": [
            "Jan Balaguer",
            "Raphael Koster",
            "Christopher Summerfield",
            "Andrea Tacchetti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.03868v3",
        "title": "Partner Approximating Learners (PAL): Simulation-Accelerated Learning\n  with Explicit Partner Modeling in Multi-Agent Domains",
        "abstract": "  Mixed cooperative-competitive control scenarios such as human-machine\ninteraction with individual goals of the interacting partners are very\nchallenging for reinforcement learning agents. In order to contribute towards\nintuitive human-machine collaboration, we focus on problems in the continuous\nstate and control domain where no explicit communication is considered and the\nagents do not know the others' goals or control laws but only sense their\ncontrol inputs retrospectively. Our proposed framework combines a learned\npartner model based on online data with a reinforcement learning agent that is\ntrained in a simulated environment including the partner model. Thus, we\novercome drawbacks of independent learners and, in addition, benefit from a\nreduced amount of real world data required for reinforcement learning which is\nvital in the human-machine context. We finally analyze an example that\ndemonstrates the merits of our proposed framework which learns fast due to the\nsimulated environment and adapts to the continuously changing partner due to\nthe partner approximation.\n",
        "published": "2019",
        "authors": [
            "Florian K\u00f6pf",
            "Alexander Nitsch",
            "Michael Flad",
            "S\u00f6ren Hohmann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.11783v2",
        "title": "Scalable Deep Reinforcement Learning for Routing and Spectrum Access in\n  Physical Layer",
        "abstract": "  This paper proposes a novel scalable reinforcement learning approach for\nsimultaneous routing and spectrum access in wireless ad-hoc networks. In most\nprevious works on reinforcement learning for network optimization, the network\ntopology is assumed to be fixed, and a different agent is trained for each\ntransmission node -- this limits scalability and generalizability. Further,\nrouting and spectrum access are typically treated as separate tasks. Moreover,\nthe optimization objective is usually a cumulative metric along the route,\ne.g., number of hops or delay. In this paper, we account for the physical-layer\nsignal-to-interference-plus-noise ratio (SINR) in a wireless network and\nfurther show that bottleneck objective such as the minimum SINR along the route\ncan also be optimized effectively using reinforcement learning. Specifically,\nwe propose a scalable approach in which a single agent is associated with each\nflow and makes routing and spectrum access decisions as it moves along the\nfrontier nodes. The agent is trained according to the physical-layer\ncharacteristics of the environment using a novel rewarding scheme based on the\nMonte Carlo estimation of the future bottleneck SINR. It learns to avoid\ninterference by intelligently making joint routing and spectrum allocation\ndecisions based on the geographical location information of the neighbouring\nnodes.\n",
        "published": "2020",
        "authors": [
            "Wei Cui",
            "Wei Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.00847v1",
        "title": "CoordiQ : Coordinated Q-learning for Electric Vehicle Charging\n  Recommendation",
        "abstract": "  Electric vehicles have been rapidly increasing in usage, but stations to\ncharge them have not always kept up with demand, so efficient routing of\nvehicles to stations is critical to operating at maximum efficiency. Deciding\nwhich stations to recommend drivers to is a complex problem with a multitude of\npossible recommendations, volatile usage patterns and temporally extended\nconsequences of recommendations. Reinforcement learning offers a powerful\nparadigm for solving sequential decision-making problems, but traditional\nmethods may struggle with sample efficiency due to the high number of possible\nactions. By developing a model that allows complex representations of actions,\nwe improve outcomes for users of our system by over 30% when compared to\nexisting baselines in a simulation. If implemented widely, these better\nrecommendations can globally save over 4 million person-hours of waiting and\ndriving each year.\n",
        "published": "2021",
        "authors": [
            "Carter Blum",
            "Hao Liu",
            "Hui Xiong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.12895v1",
        "title": "Computational Performance of Deep Reinforcement Learning to find Nash\n  Equilibria",
        "abstract": "  We test the performance of deep deterministic policy gradient (DDPG), a deep\nreinforcement learning algorithm, able to handle continuous state and action\nspaces, to learn Nash equilibria in a setting where firms compete in prices.\nThese algorithms are typically considered model-free because they do not\nrequire transition probability functions (as in e.g., Markov games) or\npredefined functional forms. Despite being model-free, a large set of\nparameters are utilized in various steps of the algorithm. These are e.g.,\nlearning rates, memory buffers, state-space dimensioning, normalizations, or\nnoise decay rates and the purpose of this work is to systematically test the\neffect of these parameter configurations on convergence to the analytically\nderived Bertrand equilibrium. We find parameter choices that can reach\nconvergence rates of up to 99%. The reliable convergence may make the method a\nuseful tool to study strategic behavior of firms even in more complex settings.\nKeywords: Bertrand Equilibrium, Competition in Uniform Price Auctions, Deep\nDeterministic Policy Gradient Algorithm, Parameter Sensitivity Analysis\n",
        "published": "2021",
        "authors": [
            "Christoph Graf",
            "Viktor Zobernig",
            "Johannes Schmidt",
            "Claude Kl\u00f6ckl"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.03051v1",
        "title": "ScheduleNet: Learn to solve multi-agent scheduling problems with\n  reinforcement learning",
        "abstract": "  We propose ScheduleNet, a RL-based real-time scheduler, that can solve\nvarious types of multi-agent scheduling problems. We formulate these problems\nas a semi-MDP with episodic reward (makespan) and learn ScheduleNet, a\ndecentralized decision-making policy that can effectively coordinate multiple\nagents to complete tasks. The decision making procedure of ScheduleNet\nincludes: (1) representing the state of a scheduling problem with the\nagent-task graph, (2) extracting node embeddings for agent and tasks nodes, the\nimportant relational information among agents and tasks, by employing the\ntype-aware graph attention (TGA), and (3) computing the assignment probability\nwith the computed node embeddings. We validate the effectiveness of ScheduleNet\nas a general learning-based scheduler for solving various types of multi-agent\nscheduling tasks, including multiple salesman traveling problem (mTSP) and job\nshop scheduling problem (JSP).\n",
        "published": "2021",
        "authors": [
            "Junyoung Park",
            "Sanjar Bakhtiyar",
            "Jinkyoo Park"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.15905v2",
        "title": "Faithful Edge Federated Learning: Scalability and Privacy",
        "abstract": "  Federated learning enables machine learning algorithms to be trained over a\nnetwork of multiple decentralized edge devices without requiring the exchange\nof local datasets. Successfully deploying federated learning requires ensuring\nthat agents (e.g., mobile devices) faithfully execute the intended algorithm,\nwhich has been largely overlooked in the literature. In this study, we first\nuse risk bounds to analyze how the key feature of federated learning,\nunbalanced and non-i.i.d. data, affects agents' incentives to voluntarily\nparticipate and obediently follow traditional federated learning algorithms. To\nbe more specific, our analysis reveals that agents with less typical data\ndistributions and relatively more samples are more likely to opt out of or\ntamper with federated learning algorithms. To this end, we formulate the first\nfaithful implementation problem of federated learning and design two faithful\nfederated learning mechanisms which satisfy economic properties, scalability,\nand privacy. Further, the time complexity of computing all agents' payments in\nthe number of agents is $\\mathcal{O}(1)$. First, we design a Faithful Federated\nLearning (FFL) mechanism which approximates the Vickrey-Clarke-Groves (VCG)\npayments via an incremental computation. We show that it achieves (probably\napproximate) optimality, faithful implementation, voluntary participation, and\nsome other economic properties (such as budget balance). Second, by\npartitioning agents into several subsets, we present a scalable VCG mechanism\napproximation. We further design a scalable and Differentially Private FFL\n(DP-FFL) mechanism, the first differentially private faithful mechanism, that\nmaintains the economic properties. Our mechanism enables one to make three-way\nperformance tradeoffs among privacy, the iterations needed, and payment\naccuracy loss.\n",
        "published": "2021",
        "authors": [
            "Meng Zhang",
            "Ermin Wei",
            "Randall Berry"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.07259v2",
        "title": "Modeling the effects of environmental and perceptual uncertainty using\n  deterministic reinforcement learning dynamics with partial observability",
        "abstract": "  Assessing the systemic effects of uncertainty that arises from agents'\npartial observation of the true states of the world is critical for\nunderstanding a wide range of scenarios. Yet, previous modeling work on agent\nlearning and decision-making either lacks a systematic way to describe this\nsource of uncertainty or puts the focus on obtaining optimal policies using\ncomplex models of the world that would impose an unrealistically high cognitive\ndemand on real agents. In this work we aim to efficiently describe the emergent\nbehavior of biologically plausible and parsimonious learning agents faced with\npartially observable worlds. Therefore we derive and present deterministic\nreinforcement learning dynamics where the agents observe the true state of the\nenvironment only partially. We showcase the broad applicability of our dynamics\nacross different classes of partially observable agent-environment systems. We\nfind that partial observability creates unintuitive benefits in a number of\nspecific contexts, pointing the way to further research on a general\nunderstanding of such effects. For instance, partially observant agents can\nlearn better outcomes faster, in a more stable way and even overcome social\ndilemmas. Furthermore, our method allows the application of dynamical systems\ntheory to partially observable multiagent leaning. In this regard we find the\nemergence of catastrophic limit cycles, a critical slowing down of the learning\nprocesses between reward regimes and the separation of the learning dynamics\ninto fast and slow directions, all caused by partial observability. Therefore,\nthe presented dynamics have the potential to become a formal, yet practical,\nlightweight and robust tool for researchers in biology, social science and\nmachine learning to systematically investigate the effects of interacting\npartially observant agents.\n",
        "published": "2021",
        "authors": [
            "Wolfram Barfuss",
            "Richard P. Mann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.03906v4",
        "title": "Nash Convergence of Mean-Based Learning Algorithms in First Price\n  Auctions",
        "abstract": "  Understanding the convergence properties of learning dynamics in repeated\nauctions is a timely and important question in the area of learning in\nauctions, with numerous applications in, e.g., online advertising markets. This\nwork focuses on repeated first price auctions where bidders with fixed values\nfor the item learn to bid using mean-based algorithms -- a large class of\nonline learning algorithms that include popular no-regret algorithms such as\nMultiplicative Weights Update and Follow the Perturbed Leader. We completely\ncharacterize the learning dynamics of mean-based algorithms, in terms of\nconvergence to a Nash equilibrium of the auction, in two senses: (1)\ntime-average: the fraction of rounds where bidders play a Nash equilibrium\napproaches 1 in the limit; (2)last-iterate: the mixed strategy profile of\nbidders approaches a Nash equilibrium in the limit. Specifically, the results\ndepend on the number of bidders with the highest value: - If the number is at\nleast three, the bidding dynamics almost surely converges to a Nash equilibrium\nof the auction, both in time-average and in last-iterate. - If the number is\ntwo, the bidding dynamics almost surely converges to a Nash equilibrium in\ntime-average but not necessarily in last-iterate. - If the number is one, the\nbidding dynamics may not converge to a Nash equilibrium in time-average nor in\nlast-iterate. Our discovery opens up new possibilities in the study of\nconvergence dynamics of learning algorithms.\n",
        "published": "2021",
        "authors": [
            "Xiaotie Deng",
            "Xinyan Hu",
            "Tao Lin",
            "Weiqiang Zheng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.10655v2",
        "title": "Socialbots on Fire: Modeling Adversarial Behaviors of Socialbots via\n  Multi-Agent Hierarchical Reinforcement Learning",
        "abstract": "  Socialbots are software-driven user accounts on social platforms, acting\nautonomously (mimicking human behavior), with the aims to influence the\nopinions of other users or spread targeted misinformation for particular goals.\nAs socialbots undermine the ecosystem of social platforms, they are often\nconsidered harmful. As such, there have been several computational efforts to\nauto-detect the socialbots. However, to our best knowledge, the adversarial\nnature of these socialbots has not yet been studied. This begs a question \"can\nadversaries, controlling socialbots, exploit AI techniques to their advantage?\"\nTo this question, we successfully demonstrate that indeed it is possible for\nadversaries to exploit computational learning mechanism such as reinforcement\nlearning (RL) to maximize the influence of socialbots while avoiding being\ndetected. We first formulate the adversarial socialbot learning as a\ncooperative game between two functional hierarchical RL agents. While one agent\ncurates a sequence of activities that can avoid the detection, the other agent\naims to maximize network influence by selectively connecting with right users.\nOur proposed policy networks train with a vast amount of synthetic graphs and\ngeneralize better than baselines on unseen real-life graphs both in terms of\nmaximizing network influence (up to +18%) and sustainable stealthiness (up to\n+40% undetectability) under a strong bot detector (with 90% detection\naccuracy). During inference, the complexity of our approach scales linearly,\nindependent of a network's structure and the virality of news. This makes our\napproach a practical adversarial attack when deployed in a real-life setting.\n",
        "published": "2021",
        "authors": [
            "Thai Le",
            "Long Tran-Thanh",
            "Dongwon Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.13287v1",
        "title": "Towards Realistic Market Simulations: a Generative Adversarial Networks\n  Approach",
        "abstract": "  Simulated environments are increasingly used by trading firms and investment\nbanks to evaluate trading strategies before approaching real markets.\nBacktesting, a widely used approach, consists of simulating experimental\nstrategies while replaying historical market scenarios. Unfortunately, this\napproach does not capture the market response to the experimental agents'\nactions. In contrast, multi-agent simulation presents a natural bottom-up\napproach to emulating agent interaction in financial markets. It allows to set\nup pools of traders with diverse strategies to mimic the financial market\ntrader population, and test the performance of new experimental strategies.\nSince individual agent-level historical data is typically proprietary and not\navailable for public use, it is difficult to calibrate multiple market agents\nto obtain the realism required for testing trading strategies. To addresses\nthis challenge we propose a synthetic market generator based on Conditional\nGenerative Adversarial Networks (CGANs) trained on real aggregate-level\nhistorical data. A CGAN-based \"world\" agent can generate meaningful orders in\nresponse to an experimental agent. We integrate our synthetic market generator\ninto ABIDES, an open source simulator of financial markets. By means of\nextensive simulations we show that our proposal outperforms previous work in\nterms of stylized facts reflecting market responsiveness and realism.\n",
        "published": "2021",
        "authors": [
            "Andrea Coletta",
            "Matteo Prata",
            "Michele Conti",
            "Emanuele Mercanti",
            "Novella Bartolini",
            "Aymeric Moulin",
            "Svitlana Vyetrenko",
            "Tucker Balch"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.00987v1",
        "title": "Modelling the transition to a low-carbon energy supply",
        "abstract": "  A transition to a low-carbon electricity supply is crucial to limit the\nimpacts of climate change. Reducing carbon emissions could help prevent the\nworld from reaching a tipping point, where runaway emissions are likely.\nRunaway emissions could lead to extremes in weather conditions around the world\n-- especially in problematic regions unable to cope with these conditions.\nHowever, the movement to a low-carbon energy supply can not happen\ninstantaneously due to the existing fossil-fuel infrastructure and the\nrequirement to maintain a reliable energy supply. Therefore, a low-carbon\ntransition is required, however, the decisions various stakeholders should make\nover the coming decades to reduce these carbon emissions are not obvious. This\nis due to many long-term uncertainties, such as electricity, fuel and\ngeneration costs, human behaviour and the size of electricity demand. A well\nchoreographed low-carbon transition is, therefore, required between all of the\nheterogenous actors in the system, as opposed to changing the behaviour of a\nsingle, centralised actor. The objective of this thesis is to create a novel,\nopen-source agent-based model to better understand the manner in which the\nwhole electricity market reacts to different factors using state-of-the-art\nmachine learning and artificial intelligence methods. In contrast to other\nworks, this thesis looks at both the long-term and short-term impact that\ndifferent behaviours have on the electricity market by using these\nstate-of-the-art methods.\n",
        "published": "2021",
        "authors": [
            "Alexander Kell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.04686v1",
        "title": "Reinforcement Learning for Mixed Autonomy Intersections",
        "abstract": "  We propose a model-free reinforcement learning method for controlling mixed\nautonomy traffic in simulated traffic networks with through-traffic-only\ntwo-way and four-way intersections. Our method utilizes multi-agent policy\ndecomposition which allows decentralized control based on local observations\nfor an arbitrary number of controlled vehicles. We demonstrate that, even\nwithout reward shaping, reinforcement learning learns to coordinate the\nvehicles to exhibit traffic signal-like behaviors, achieving near-optimal\nthroughput with 33-50% controlled vehicles. With the help of multi-task\nlearning and transfer learning, we show that this behavior generalizes across\ninflow rates and size of the traffic network. Our code, models, and videos of\nresults are available at\nhttps://github.com/ZhongxiaYan/mixed_autonomy_intersections.\n",
        "published": "2021",
        "authors": [
            "Zhongxia Yan",
            "Cathy Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.05969v1",
        "title": "PowerGridworld: A Framework for Multi-Agent Reinforcement Learning in\n  Power Systems",
        "abstract": "  We present the PowerGridworld software package to provide users with a\nlightweight, modular, and customizable framework for creating\npower-systems-focused, multi-agent Gym environments that readily integrate with\nexisting training frameworks for reinforcement learning (RL). Although many\nframeworks exist for training multi-agent RL (MARL) policies, none can rapidly\nprototype and develop the environments themselves, especially in the context of\nheterogeneous (composite, multi-device) power systems where power flow\nsolutions are required to define grid-level variables and costs. PowerGridworld\nis an open-source software package that helps to fill this gap. To highlight\nPowerGridworld's key features, we present two case studies and demonstrate\nlearning MARL policies using both OpenAI's multi-agent deep deterministic\npolicy gradient (MADDPG) and RLLib's proximal policy optimization (PPO)\nalgorithms. In both cases, at least some subset of agents incorporates elements\nof the power flow solution at each time step as part of their reward (negative\ncost) structures.\n",
        "published": "2021",
        "authors": [
            "David Biagioni",
            "Xiangyu Zhang",
            "Dylan Wald",
            "Deepthi Vaidhynathan",
            "Rohit Chintala",
            "Jennifer King",
            "Ahmed S. Zamzam"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.10898v2",
        "title": "Renewable energy integration and microgrid energy trading using\n  multi-agent deep reinforcement learning",
        "abstract": "  In this paper, multi-agent reinforcement learning is used to control a hybrid\nenergy storage system working collaboratively to reduce the energy costs of a\nmicrogrid through maximising the value of renewable energy and trading. The\nagents must learn to control three different types of energy storage system\nsuited for short, medium, and long-term storage under fluctuating demand,\ndynamic wholesale energy prices, and unpredictable renewable energy generation.\nTwo case studies are considered: the first looking at how the energy storage\nsystems can better integrate renewable energy generation under dynamic pricing,\nand the second with how those same agents can be used alongside an aggregator\nagent to sell energy to self-interested external microgrids looking to reduce\ntheir own energy bills. This work found that the centralised learning with\ndecentralised execution of the multi-agent deep deterministic policy gradient\nand its state-of-the-art variants allowed the multi-agent methods to perform\nsignificantly better than the control from a single global agent. It was also\nfound that using separate reward functions in the multi-agent approach\nperformed much better than using a single control agent. Being able to trade\nwith the other microgrids, rather than just selling back to the utility grid,\nalso was found to greatly increase the grid's savings.\n",
        "published": "2021",
        "authors": [
            "Daniel J. B. Harrold",
            "Jun Cao",
            "Zhong Fan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.14737v4",
        "title": "Beyond Time-Average Convergence: Near-Optimal Uncoupled Online Learning\n  via Clairvoyant Multiplicative Weights Update",
        "abstract": "  In this paper, we provide a novel and simple algorithm, Clairvoyant\nMultiplicative Weights Updates (CMWU) for regret minimization in general games.\nCMWU effectively corresponds to the standard MWU algorithm but where all\nagents, when updating their mixed strategies, use the payoff profiles based on\ntomorrow's behavior, i.e. the agents are clairvoyant. CMWU achieves constant\nregret of $\\ln(m)/\\eta$ in all normal-form games with m actions and fixed\nstep-sizes $\\eta$. Although CMWU encodes in its definition a fixed point\ncomputation, which in principle could result in dynamics that are neither\ncomputationally efficient nor uncoupled, we show that both of these issues can\nbe largely circumvented. Specifically, as long as the step-size $\\eta$ is upper\nbounded by $\\frac{1}{(n-1)V}$, where $n$ is the number of agents and $[0,V]$ is\nthe payoff range, then the CMWU updates can be computed linearly fast via a\ncontraction map. This implementation results in an uncoupled online learning\ndynamic that admits a $O (\\log T)$-sparse sub-sequence where each agent\nexperiences at most $O(nV\\log m)$ regret. This implies that the CMWU dynamics\nconverge with rate $O(nV \\log m \\log T / T)$ to a \\textit{Coarse Correlated\nEquilibrium}. The latter improves on the current state-of-the-art convergence\nrate of \\textit{uncoupled online learning dynamics}\n\\cite{daskalakis2021near,anagnostides2021near}.\n",
        "published": "2021",
        "authors": [
            "Georgios Piliouras",
            "Ryann Sim",
            "Stratis Skoulakis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.01039v3",
        "title": "How global observation works in Federated Learning: Integrating vertical\n  training into Horizontal Federated Learning",
        "abstract": "  Federated learning (FL) has recently emerged as a transformative paradigm\nthat jointly train a model with distributed data sets in IoT while avoiding the\nneed for central data collection. Due to the limited observation range, such\ndata sets can only reflect local information, which limits the quality of\ntrained models. In practice, the global information and local observations\nwould require a joint consideration for learning to make a reasonable policy.\nHowever, in horizontal FL, the central agency only acts as a model aggregator\nwithout utilizing its global observation to further improve the model. This\ncould significantly degrade the performance in some missions such as traffic\nflow prediction in network systems, where the global information may enhance\nthe accuracy. Meanwhile, the global feature may not be directly transmitted to\nagents for data security. How to utilize the global observation residing in the\ncentral agency while protecting its safety thus rises up as an important\nproblem in FL. In this paper, we develop a vertical-horizontal federated\nlearning (VHFL) process, where the global feature is shared with the agents in\na procedure similar to that of vertical FL without any extra communication\nrounds. By considering the delay and packet loss, we will analyze VHFL\nconvergence and validate its performance by experiments. It is shown that the\nproposed VHFL could enhance the accuracy compared with horizontal FL while\nstill protecting the security of global data.\n",
        "published": "2021",
        "authors": [
            "Shuo Wan",
            "Jiaxun Lu",
            "Pingyi Fan",
            "Yunfeng Shao",
            "Chenghui Peng",
            "Khaled B. Letaief"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.07569v2",
        "title": "Cooperation for Scalable Supervision of Autonomy in Mixed Traffic",
        "abstract": "  Advances in autonomy offer the potential for dramatic positive outcomes in a\nnumber of domains, yet enabling their safe deployment remains an open problem.\nThis work's motivating question is: In safety-critical settings, can we avoid\nthe need to have one human supervise one machine at all times? The work\nformalizes this scalable supervision problem by considering remotely located\nhuman supervisors and investigating how autonomous agents can cooperate to\nachieve safety. This article focuses on the safety-critical context of\nautonomous vehicles (AVs) merging into traffic consisting of a mixture of AVs\nand human drivers. The analysis establishes high reliability upper bounds on\nhuman supervision requirements. It further shows that AV cooperation can\nimprove supervision reliability by orders of magnitude and counterintuitively\nrequires fewer supervisors (per AV) as more AVs are adopted. These analytical\nresults leverage queuing-theoretic analysis, order statistics, and a\nconservative, reachability-based approach. A key takeaway is the potential\nvalue of cooperation in enabling the deployment of autonomy at scale. While\nthis work focuses on AVs, the scalable supervision framework may be of\nindependent interest to a broader array of autonomous control challenges.\n",
        "published": "2021",
        "authors": [
            "Cameron Hickert",
            "Sirui Li",
            "Cathy Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.01182v1",
        "title": "Modelling Cournot Games as Multi-agent Multi-armed Bandits",
        "abstract": "  We investigate the use of a multi-agent multi-armed bandit (MA-MAB) setting\nfor modeling repeated Cournot oligopoly games, where the firms acting as agents\nchoose from the set of arms representing production quantity (a discrete\nvalue). Agents interact with separate and independent bandit problems. In this\nformulation, each agent makes sequential choices among arms to maximize its own\nreward. Agents do not have any information about the environment; they can only\nsee their own rewards after taking an action. However, the market demand is a\nstationary function of total industry output, and random entry or exit from the\nmarket is not allowed. Given these assumptions, we found that an\n$\\epsilon$-greedy approach offers a more viable learning mechanism than other\ntraditional MAB approaches, as it does not require any additional knowledge of\nthe system to operate. We also propose two novel approaches that take advantage\nof the ordered action space: $\\epsilon$-greedy+HL and $\\epsilon$-greedy+EL.\nThese new approaches help firms to focus on more profitable actions by\neliminating less profitable choices and hence are designed to optimize the\nexploration. We use computer simulations to study the emergence of various\nequilibria in the outcomes and do the empirical analysis of joint cumulative\nregrets.\n",
        "published": "2022",
        "authors": [
            "Kshitija Taywade",
            "Brent Harrison",
            "Adib Bagh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.01733v1",
        "title": "Modeling Human Driver Interactions Using an Infinite Policy Space\n  Through Gaussian Processes",
        "abstract": "  This paper proposes a method for modeling human driver interactions that\nrelies on multi-output gaussian processes. The proposed method is developed as\na refinement of the game theoretical hierarchical reasoning approach called\n\"level-k reasoning\" which conventionally assigns discrete levels of behaviors\nto agents. Although it is shown to be an effective modeling tool, the level-k\nreasoning approach may pose undesired constraints for predicting human decision\nmaking due to a limited number (usually 2 or 3) of driver policies it extracts.\nThe proposed approach is put forward to fill this gap in the literature by\nintroducing a continuous domain framework that enables an infinite policy\nspace. By using the approach presented in this paper, more accurate driver\nmodels can be obtained, which can then be employed for creating high fidelity\nsimulation platforms for the validation of autonomous vehicle control\nalgorithms. The proposed method is validated on a real traffic dataset and\ncompared with the conventional level-k approach to demonstrate its\ncontributions and implications.\n",
        "published": "2022",
        "authors": [
            "Cem Okan Yaldiz",
            "Yildiray Yildiz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.06924v1",
        "title": "A Synthetic Prediction Market for Estimating Confidence in Published\n  Work",
        "abstract": "  Explainably estimating confidence in published scholarly work offers\nopportunity for faster and more robust scientific progress. We develop a\nsynthetic prediction market to assess the credibility of published claims in\nthe social and behavioral sciences literature. We demonstrate our system and\ndetail our findings using a collection of known replication projects. We\nsuggest that this work lays the foundation for a research agenda that\ncreatively uses AI for peer review.\n",
        "published": "2021",
        "authors": [
            "Sarah Rajtmajer",
            "Christopher Griffin",
            "Jian Wu",
            "Robert Fraleigh",
            "Laxmaan Balaji",
            "Anna Squicciarini",
            "Anthony Kwasnica",
            "David Pennock",
            "Michael McLaughlin",
            "Timothy Fritton",
            "Nishanth Nakshatri",
            "Arjun Menon",
            "Sai Ajay Modukuri",
            "Rajal Nivargi",
            "Xin Wei",
            "C. Lee Giles"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.12561v1",
        "title": "Learning Eco-Driving Strategies at Signalized Intersections",
        "abstract": "  Signalized intersections in arterial roads result in persistent vehicle\nidling and excess accelerations, contributing to fuel consumption and CO2\nemissions. There has thus been a line of work studying eco-driving control\nstrategies to reduce fuel consumption and emission levels at intersections.\nHowever, methods to devise effective control strategies across a variety of\ntraffic settings remain elusive. In this paper, we propose a reinforcement\nlearning (RL) approach to learn effective eco-driving control strategies. We\nanalyze the potential impact of a learned strategy on fuel consumption, CO2\nemission, and travel time and compare with naturalistic driving and model-based\nbaselines. We further demonstrate the generalizability of the learned policies\nunder mixed traffic scenarios. Simulation results indicate that scenarios with\n100% penetration of connected autonomous vehicles (CAV) may yield as high as\n18% reduction in fuel consumption and 25% reduction in CO2 emission levels\nwhile even improving travel speed by 20%. Furthermore, results indicate that\neven 25% CAV penetration can bring at least 50% of the total fuel and emission\nreduction benefits.\n",
        "published": "2022",
        "authors": [
            "Vindula Jayawardana",
            "Cathy Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.03844v1",
        "title": "Scalable Joint Learning of Wireless Multiple-Access Policies and their\n  Signaling",
        "abstract": "  In this paper, we apply an multi-agent reinforcement learning (MARL)\nframework allowing the base station (BS) and the user equipments (UEs) to\njointly learn a channel access policy and its signaling in a wireless multiple\naccess scenario. In this framework, the BS and UEs are reinforcement learning\n(RL) agents that need to cooperate in order to deliver data. The comparison\nwith a contention-free and a contention-based baselines shows that our\nframework achieves a superior performance in terms of goodput even in high\ntraffic situations while maintaining a low collision rate. The scalability of\nthe proposed method is studied, since it is a major problem in MARL and this\npaper provides the first results in order to address it.\n",
        "published": "2022",
        "authors": [
            "Mateus P. Mota",
            "Alvaro Valcarce",
            "Jean-Marie Gorce"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.08781v2",
        "title": "Reinforcement Learning for Economic Policy: A New Frontier?",
        "abstract": "  Agent-based computational economics is a field with a rich academic history,\nyet one which has struggled to enter mainstream policy design toolboxes,\nplagued by the challenges associated with representing a complex and dynamic\nreality. The field of Reinforcement Learning (RL), too, has a rich history, and\nhas recently been at the centre of several exponential developments. Modern RL\nimplementations have been able to achieve unprecedented levels of\nsophistication, handling previously unthinkable degrees of complexity. This\nreview surveys the historical barriers of classical agent-based techniques in\neconomic modelling, and contemplates whether recent developments in RL can\novercome any of them.\n",
        "published": "2022",
        "authors": [
            "Callum Rhys Tilbury"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.09714v2",
        "title": "Differentiable Agent-based Epidemiology",
        "abstract": "  Mechanistic simulators are an indispensable tool for epidemiology to explore\nthe behavior of complex, dynamic infections under varying conditions and\nnavigate uncertain environments. Agent-based models (ABMs) are an increasingly\npopular simulation paradigm that can represent the heterogeneity of contact\ninteractions with granular detail and agency of individual behavior. However,\nconventional ABM frameworks are not differentiable and present challenges in\nscalability; due to which it is non-trivial to connect them to auxiliary data\nsources. In this paper, we introduce GradABM: a scalable, differentiable design\nfor agent-based modeling that is amenable to gradient-based learning with\nautomatic differentiation. GradABM can quickly simulate million-size\npopulations in few seconds on commodity hardware, integrate with deep neural\nnetworks and ingest heterogeneous data sources. This provides an array of\npractical benefits for calibration, forecasting, and evaluating policy\ninterventions. We demonstrate the efficacy of GradABM via extensive experiments\nwith real COVID-19 and influenza datasets.\n",
        "published": "2022",
        "authors": [
            "Ayush Chopra",
            "Alexander Rodr\u00edguez",
            "Jayakumar Subramanian",
            "Arnau Quera-Bofarull",
            "Balaji Krishnamurthy",
            "B. Aditya Prakash",
            "Ramesh Raskar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.08798v1",
        "title": "Neural Payoff Machines: Predicting Fair and Stable Payoff Allocations\n  Among Team Members",
        "abstract": "  In many multi-agent settings, participants can form teams to achieve\ncollective outcomes that may far surpass their individual capabilities.\nMeasuring the relative contributions of agents and allocating them shares of\nthe reward that promote long-lasting cooperation are difficult tasks.\nCooperative game theory offers solution concepts identifying distribution\nschemes, such as the Shapley value, that fairly reflect the contribution of\nindividuals to the performance of the team or the Core, which reduces the\nincentive of agents to abandon their team. Applications of such methods include\nidentifying influential features and sharing the costs of joint ventures or\nteam formation. Unfortunately, using these solutions requires tackling a\ncomputational barrier as they are hard to compute, even in restricted settings.\nIn this work, we show how cooperative game-theoretic solutions can be distilled\ninto a learned model by training neural networks to propose fair and stable\npayoff allocations. We show that our approach creates models that can\ngeneralize to games far from the training distribution and can predict\nsolutions for more players than observed during training. An important\napplication of our framework is Explainable AI: our approach can be used to\nspeed-up Shapley value computations on many instances.\n",
        "published": "2022",
        "authors": [
            "Daphne Cornelisse",
            "Thomas Rood",
            "Mateusz Malinowski",
            "Yoram Bachrach",
            "Tal Kachman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.12779v1",
        "title": "Battery and Hydrogen Energy Storage Control in a Smart Energy Network\n  with Flexible Energy Demand using Deep Reinforcement Learning",
        "abstract": "  Smart energy networks provide for an effective means to accommodate high\npenetrations of variable renewable energy sources like solar and wind, which\nare key for deep decarbonisation of energy production. However, given the\nvariability of the renewables as well as the energy demand, it is imperative to\ndevelop effective control and energy storage schemes to manage the variable\nenergy generation and achieve desired system economics and environmental goals.\nIn this paper, we introduce a hybrid energy storage system composed of battery\nand hydrogen energy storage to handle the uncertainties related to electricity\nprices, renewable energy production and consumption. We aim to improve\nrenewable energy utilisation and minimise energy costs and carbon emissions\nwhile ensuring energy reliability and stability within the network. To achieve\nthis, we propose a multi-agent deep deterministic policy gradient approach,\nwhich is a deep reinforcement learning-based control strategy to optimise the\nscheduling of the hybrid energy storage system and energy demand in real-time.\nThe proposed approach is model-free and does not require explicit knowledge and\nrigorous mathematical models of the smart energy network environment.\nSimulation results based on real-world data show that: (i) integration and\noptimised operation of the hybrid energy storage system and energy demand\nreduces carbon emissions by 78.69%, improves cost savings by 23.5% and\nrenewable energy utilisation by over 13.2% compared to other baseline models\nand (ii) the proposed algorithm outperforms the state-of-the-art self-learning\nalgorithms like deep-Q network.\n",
        "published": "2022",
        "authors": [
            "Cephas Samende",
            "Zhong Fan",
            "Jun Cao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.08494v2",
        "title": "Who Reviews The Reviewers? A Multi-Level Jury Problem",
        "abstract": "  We consider the problem of determining a binary ground truth using advice\nfrom a group of independent reviewers (experts) who express their guess about a\nground truth correctly with some independent probability (competence). In this\nsetting, when all reviewers are competent (competence greater than one-half),\nthe Condorcet Jury Theorem tells us that adding more reviewers increases the\noverall accuracy, and if all competences are known, then there exists an\noptimal weighting of the reviewers. However, in practical settings, reviewers\nmay be noisy or incompetent, i.e., competence below half, and the number of\nexperts may be small, so the asymptotic Condorcet Jury Theorem is not\npractically relevant. In such cases we explore appointing one or more chairs\n(judges) who determine the weight of each reviewer for aggregation, creating\nmultiple levels. However, these chairs may be unable to correctly identify the\ncompetence of the reviewers they oversee, and therefore unable to compute the\noptimal weighting. We give conditions when a set of chairs is able to weight\nthe reviewers optimally, and depending on the competence distribution of the\nagents, give results about when it is better to have more chairs or more\nreviewers. Through numerical simulations we show that in some cases it is\nbetter to have more chairs, but in many cases it is better to have more\nreviewers.\n",
        "published": "2022",
        "authors": [
            "Ben Abramowitz",
            "Omer Lev",
            "Nicholas Mattei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.09986v2",
        "title": "Pandering in a Flexible Representative Democracy",
        "abstract": "  In representative democracies, the election of new representatives in regular\nelection cycles is meant to prevent corruption and other misbehavior by elected\nofficials and to keep them accountable in service of the ``will of the people.\"\nThis democratic ideal can be undermined when candidates are dishonest when\ncampaigning for election over these multiple cycles or rounds of voting. Much\nof the work on COMSOC to date has investigated strategic actions in only a\nsingle round. We introduce a novel formal model of \\emph{pandering}, or\nstrategic preference reporting by candidates seeking to be elected, and examine\nthe resilience of two democratic voting systems to pandering within a single\nround and across multiple rounds. The two voting systems we compare are\nRepresentative Democracy (RD) and Flexible Representative Democracy (FRD). For\neach voting system, our analysis centers on the types of strategies candidates\nemploy and how voters update their views of candidates based on how the\ncandidates have pandered in the past. We provide theoretical results on the\ncomplexity of pandering in our setting for a single cycle, formulate our\nproblem for multiple cycles as a Markov Decision Process, and use reinforcement\nlearning to study the effects of pandering by both single candidates and groups\nof candidates across a number of rounds.\n",
        "published": "2022",
        "authors": [
            "Xiaolin Sun",
            "Jacob Masur",
            "Ben Abramowitz",
            "Nicholas Mattei",
            "Zizhan Zheng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.05742v1",
        "title": "Where to go: Agent Guidance with Deep Reinforcement Learning in A\n  City-Scale Online Ride-Hailing Service",
        "abstract": "  Online ride-hailing services have become a prevalent transportation system\nacross the world. In this paper, we study a challenging problem of how to\ndirect vacant taxis around a city such that supplies and demands can be\nbalanced in online ride-hailing services. We design a new reward scheme that\nconsiders multiple performance metrics of online ride-hailing services. We also\npropose a novel deep reinforcement learning method named Deep-Q-Network with\nAction Mask (AM-DQN) masking off unnecessary actions in various locations such\nthat agents can learn much faster and more efficiently. We conduct extensive\nexperiments using a city-scale dataset from Chicago. Several popular heuristic\nand learning methods are also implemented as baselines for comparison. The\nresults of the experiments show that the AM-DQN attains the best performances\nof all methods with respect to average failure rate, average waiting time for\ncustomers, and average idle search time for vacant taxis.\n",
        "published": "2022",
        "authors": [
            "Jiyao Li",
            "Vicki H. Allan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.02593v1",
        "title": "Multi-Agent Reinforcement Learning for Fast-Timescale Demand Response of\n  Residential Loads",
        "abstract": "  To integrate high amounts of renewable energy resources, electrical power\ngrids must be able to cope with high amplitude, fast timescale variations in\npower generation. Frequency regulation through demand response has the\npotential to coordinate temporally flexible loads, such as air conditioners, to\ncounteract these variations. Existing approaches for discrete control with\ndynamic constraints struggle to provide satisfactory performance for fast\ntimescale action selection with hundreds of agents. We propose a decentralized\nagent trained with multi-agent proximal policy optimization with localized\ncommunication. We explore two communication frameworks: hand-engineered, or\nlearned through targeted multi-agent communication. The resulting policies\nperform well and robustly for frequency regulation, and scale seamlessly to\narbitrary numbers of houses for constant processing times.\n",
        "published": "2023",
        "authors": [
            "Vincent Mai",
            "Philippe Maisonneuve",
            "Tianyu Zhang",
            "Hadi Nekoei",
            "Liam Paull",
            "Antoine Lesage-Landry"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.11835v3",
        "title": "Reinforcement Learning for Combining Search Methods in the Calibration\n  of Economic ABMs",
        "abstract": "  Calibrating agent-based models (ABMs) in economics and finance typically\ninvolves a derivative-free search in a very large parameter space. In this\nwork, we benchmark a number of search methods in the calibration of a\nwell-known macroeconomic ABM on real data, and further assess the performance\nof \"mixed strategies\" made by combining different methods. We find that methods\nbased on random-forest surrogates are particularly efficient, and that\ncombining search methods generally increases performance since the biases of\nany single method are mitigated. Moving from these observations, we propose a\nreinforcement learning (RL) scheme to automatically select and combine search\nmethods on-the-fly during a calibration run. The RL agent keeps exploiting a\nspecific method only as long as this keeps performing well, but explores new\nstrategies when the specific method reaches a performance plateau. The\nresulting RL search scheme outperforms any other method or method combination\ntested, and does not rely on any prior information or trial and error\nprocedure.\n",
        "published": "2023",
        "authors": [
            "Aldo Glielmo",
            "Marco Favorito",
            "Debmallya Chanda",
            "Domenico Delli Gatti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.12899v2",
        "title": "Multi-Agent Reinforcement Learning with Common Policy for Antenna Tilt\n  Optimization",
        "abstract": "  This paper presents a method for optimizing wireless networks by adjusting\ncell parameters that affect both the performance of the cell being optimized\nand the surrounding cells. The method uses multiple reinforcement learning\nagents that share a common policy and take into account information from\nneighboring cells to determine the state and reward. In order to avoid\nimpairing network performance during the initial stages of learning, agents are\npre-trained in an earlier phase of offline learning. During this phase, an\ninitial policy is obtained using feedback from a static network simulator and\nconsidering a wide variety of scenarios. Finally, agents can intelligently tune\nthe cell parameters of a test network by suggesting small incremental changes,\nslowly guiding the network toward an optimal configuration. The agents propose\noptimal changes using the experience gained with the simulator in the\npre-training phase, but they can also continue to learn from current network\nreadings after each change. The results show how the proposed approach\nsignificantly improves the performance gains already provided by expert\nsystem-based methods when applied to remote antenna tilt optimization. The\nsignificant gains of this approach have truly been observed when compared with\na similar method in which the state and reward do not incorporate information\nfrom neighboring cells.\n",
        "published": "2023",
        "authors": [
            "Adriano Mendo",
            "Jose Outes-Carnero",
            "Yak Ng-Molina",
            "Juan Ramiro-Moreno"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.14094v1",
        "title": "Combating Uncertainties in Wind and Distributed PV Energy Sources Using\n  Integrated Reinforcement Learning and Time-Series Forecasting",
        "abstract": "  Renewable energy sources, such as wind and solar power, are increasingly\nbeing integrated into smart grid systems. However, when compared to traditional\nenergy resources, the unpredictability of renewable energy generation poses\nsignificant challenges for both electricity providers and utility companies.\nFurthermore, the large-scale integration of distributed energy resources (such\nas PV systems) creates new challenges for energy management in microgrids. To\ntackle these issues, we propose a novel framework with two objectives: (i)\ncombating uncertainty of renewable energy in smart grid by leveraging\ntime-series forecasting with Long-Short Term Memory (LSTM) solutions, and (ii)\nestablishing distributed and dynamic decision-making framework with multi-agent\nreinforcement learning using Deep Deterministic Policy Gradient (DDPG)\nalgorithm. The proposed framework considers both objectives concurrently to\nfully integrate them, while considering both wholesale and retail markets,\nthereby enabling efficient energy management in the presence of uncertain and\ndistributed renewable energy sources. Through extensive numerical simulations,\nwe demonstrate that the proposed solution significantly improves the profit of\nload serving entities (LSE) by providing a more accurate wind generation\nforecast. Furthermore, our results demonstrate that households with PV and\nbattery installations can increase their profits by using intelligent battery\ncharge/discharge actions determined by the DDPG agents.\n",
        "published": "2023",
        "authors": [
            "Arman Ghasemi",
            "Amin Shojaeighadikolaei",
            "Morteza Hashemi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.06109v2",
        "title": "On the Fusion Strategies for Federated Decision Making",
        "abstract": "  We consider the problem of information aggregation in federated decision\nmaking, where a group of agents collaborate to infer the underlying state of\nnature without sharing their private data with the central processor or each\nother. We analyze the non-Bayesian social learning strategy in which agents\nincorporate their individual observations into their opinions (i.e.,\nsoft-decisions) with Bayes rule, and the central processor aggregates these\nopinions by arithmetic or geometric averaging. Building on our previous work,\nwe establish that both pooling strategies result in asymptotic normality\ncharacterization of the system, which, for instance, can be utilized to derive\napproximate expressions for the error probability. We verify the theoretical\nfindings with simulations and compare both strategies.\n",
        "published": "2023",
        "authors": [
            "Mert Kayaalp",
            "Yunus Inan",
            "Visa Koivunen",
            "Emre Telatar",
            "Ali H. Sayed"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.17052v1",
        "title": "A Framework for Incentivized Collaborative Learning",
        "abstract": "  Collaborations among various entities, such as companies, research labs, AI\nagents, and edge devices, have become increasingly crucial for achieving\nmachine learning tasks that cannot be accomplished by a single entity alone.\nThis is likely due to factors such as security constraints, privacy concerns,\nand limitations in computation resources. As a result, collaborative learning\n(CL) research has been gaining momentum. However, a significant challenge in\npractical applications of CL is how to effectively incentivize multiple\nentities to collaborate before any collaboration occurs. In this study, we\npropose ICL, a general framework for incentivized collaborative learning, and\nprovide insights into the critical issue of when and why incentives can improve\ncollaboration performance. Furthermore, we show the broad applicability of ICL\nto specific cases in federated learning, assisted learning, and multi-armed\nbandit with both theory and experimental results.\n",
        "published": "2023",
        "authors": [
            "Xinran Wang",
            "Qi Le",
            "Ahmad Faraz Khan",
            "Jie Ding",
            "Ali Anwar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.06087v1",
        "title": "Learning Not to Spoof",
        "abstract": "  As intelligent trading agents based on reinforcement learning (RL) gain\nprevalence, it becomes more important to ensure that RL agents obey laws,\nregulations, and human behavioral expectations. There is substantial literature\nconcerning the aversion of obvious catastrophes like crashing a helicopter or\nbankrupting a trading account, but little around the avoidance of subtle\nnon-normative behavior for which there are examples, but no programmable\ndefinition. Such behavior may violate legal or regulatory, rather than physical\nor monetary, constraints.\n  In this article, I consider a series of experiments in which an intelligent\nstock trading agent maximizes profit but may also inadvertently learn to spoof\nthe market in which it participates. I first inject a hand-coded spoofing agent\nto a multi-agent market simulation and learn to recognize spoofing activity\nsequences. Then I replace the hand-coded spoofing trader with a simple\nprofit-maximizing RL agent and observe that it independently discovers spoofing\nas the optimal strategy. Finally, I introduce a method to incorporate the\nrecognizer as normative guide, shaping the agent's perceived rewards and\naltering its selected actions. The agent remains profitable while avoiding\nspoofing behaviors that would result in even higher profit. After presenting\nthe empirical results, I conclude with some recommendations. The method should\ngeneralize to the reduction of any unwanted behavior for which a recognizer can\nbe learned.\n",
        "published": "2023",
        "authors": [
            "David Byrd"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.08041v1",
        "title": "On Faking a Nash Equilibrium",
        "abstract": "  We characterize offline data poisoning attacks on Multi-Agent Reinforcement\nLearning (MARL), where an attacker may change a data set in an attempt to\ninstall a (potentially fictitious) unique Markov-perfect Nash equilibrium. We\npropose the unique Nash set, namely the set of games, specified by their Q\nfunctions, with a specific joint policy being the unique Nash equilibrium. The\nunique Nash set is central to poisoning attacks because the attack is\nsuccessful if and only if data poisoning pushes all plausible games inside it.\nThe unique Nash set generalizes the reward polytope commonly used in inverse\nreinforcement learning to MARL. For zero-sum Markov games, both the inverse\nNash set and the set of plausible games induced by data are polytopes in the Q\nfunction space. We exhibit a linear program to efficiently compute the optimal\npoisoning attack. Our work sheds light on the structure of data poisoning\nattacks on offline MARL, a necessary step before one can design more robust\nMARL algorithms.\n",
        "published": "2023",
        "authors": [
            "Young Wu",
            "Jeremy McMahan",
            "Xiaojin Zhu",
            "Qiaomin Xie"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.08794v1",
        "title": "Non-Stationary Policy Learning for Multi-Timescale Multi-Agent\n  Reinforcement Learning",
        "abstract": "  In multi-timescale multi-agent reinforcement learning (MARL), agents interact\nacross different timescales. In general, policies for time-dependent behaviors,\nsuch as those induced by multiple timescales, are non-stationary. Learning\nnon-stationary policies is challenging and typically requires sophisticated or\ninefficient algorithms. Motivated by the prevalence of this control problem in\nreal-world complex systems, we introduce a simple framework for learning\nnon-stationary policies for multi-timescale MARL. Our approach uses available\ninformation about agent timescales to define a periodic time encoding. In\ndetail, we theoretically demonstrate that the effects of non-stationarity\nintroduced by multiple timescales can be learned by a periodic multi-agent\npolicy. To learn such policies, we propose a policy gradient algorithm that\nparameterizes the actor and critic with phase-functioned neural networks, which\nprovide an inductive bias for periodicity. The framework's ability to\neffectively learn multi-timescale policies is validated on a gridworld and\nbuilding energy management environment.\n",
        "published": "2023",
        "authors": [
            "Patrick Emami",
            "Xiangyu Zhang",
            "David Biagioni",
            "Ahmed S. Zamzam"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.16228v1",
        "title": "Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand\n  System: A Multi-Agent Reinforcement Learning Approach",
        "abstract": "  Electric autonomous vehicles (EAVs) are getting attention in future\nautonomous mobility-on-demand (AMoD) systems due to their economic and societal\nbenefits. However, EAVs' unique charging patterns (long charging time, high\ncharging frequency, unpredictable charging behaviors, etc.) make it challenging\nto accurately predict the EAVs supply in E-AMoD systems. Furthermore, the\nmobility demand's prediction uncertainty makes it an urgent and challenging\ntask to design an integrated vehicle balancing solution under supply and demand\nuncertainties. Despite the success of reinforcement learning-based E-AMoD\nbalancing algorithms, state uncertainties under the EV supply or mobility\ndemand remain unexplored. In this work, we design a multi-agent reinforcement\nlearning (MARL)-based framework for EAVs balancing in E-AMoD systems, with\nadversarial agents to model both the EAVs supply and mobility demand\nuncertainties that may undermine the vehicle balancing solutions. We then\npropose a robust E-AMoD Balancing MARL (REBAMA) algorithm to train a robust\nEAVs balancing policy to balance both the supply-demand ratio and charging\nutilization rate across the whole city. Experiments show that our proposed\nrobust method performs better compared with a non-robust MARL method that does\nnot consider state uncertainties; it improves the reward, charging utilization\nfairness, and supply-demand fairness by 19.28%, 28.18%, and 3.97%,\nrespectively. Compared with a robust optimization-based method, the proposed\nMARL algorithm can improve the reward, charging utilization fairness, and\nsupply-demand fairness by 8.21%, 8.29%, and 9.42%, respectively.\n",
        "published": "2023",
        "authors": [
            "Sihong He",
            "Shuo Han",
            "Fei Miao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.11456v1",
        "title": "Generative Agent-Based Modeling: Unveiling Social System Dynamics\n  through Coupling Mechanistic Models with Generative Artificial Intelligence",
        "abstract": "  We discuss the emerging new opportunity for building feedback-rich\ncomputational models of social systems using generative artificial\nintelligence. Referred to as Generative Agent-Based Models (GABMs), such\nindividual-level models utilize large language models such as ChatGPT to\nrepresent human decision-making in social settings. We provide a GABM case in\nwhich human behavior can be incorporated in simulation models by coupling a\nmechanistic model of human interactions with a pre-trained large language\nmodel. This is achieved by introducing a simple GABM of social norm diffusion\nin an organization. For educational purposes, the model is intentionally kept\nsimple. We examine a wide range of scenarios and the sensitivity of the results\nto several changes in the prompt. We hope the article and the model serve as a\nguide for building useful diffusion models that include realistic human\nreasoning and decision-making.\n",
        "published": "2023",
        "authors": [
            "Navid Ghaffarzadegan",
            "Aritra Majumdar",
            "Ross Williams",
            "Niyousha Hosseinichimeh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.19647v2",
        "title": "Fast swap regret minimization and applications to approximate correlated\n  equilibria",
        "abstract": "  We give a simple and computationally efficient algorithm that, for any\nconstant $\\varepsilon>0$, obtains $\\varepsilon T$-swap regret within only $T =\n\\mathsf{polylog}(n)$ rounds; this is an exponential improvement compared to the\nsuper-linear number of rounds required by the state-of-the-art algorithm, and\nresolves the main open problem of [Blum and Mansour 2007]. Our algorithm has an\nexponential dependence on $\\varepsilon$, but we prove a new, matching lower\nbound.\n  Our algorithm for swap regret implies faster convergence to\n$\\varepsilon$-Correlated Equilibrium ($\\varepsilon$-CE) in several regimes: For\nnormal form two-player games with $n$ actions, it implies the first uncoupled\ndynamics that converges to the set of $\\varepsilon$-CE in polylogarithmic\nrounds; a $\\mathsf{polylog}(n)$-bit communication protocol for $\\varepsilon$-CE\nin two-player games (resolving an open problem mentioned by\n[Babichenko-Rubinstein'2017, Goos-Rubinstein'2018, Ganor-CS'2018]); and an\n$\\tilde{O}(n)$-query algorithm for $\\varepsilon$-CE (resolving an open problem\nof [Babichenko'2020] and obtaining the first separation between\n$\\varepsilon$-CE and $\\varepsilon$-Nash equilibrium in the query complexity\nmodel).\n  For extensive-form games, our algorithm implies a PTAS for $\\mathit{normal}$\n$\\mathit{form}$ $\\mathit{correlated}$ $\\mathit{equilibria}$, a solution concept\noften conjectured to be computationally intractable (e.g. [Stengel-Forges'08,\nFujii'23]).\n",
        "published": "2023",
        "authors": [
            "Binghui Peng",
            "Aviad Rubinstein"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.10468v1",
        "title": "Using Cooperative Game Theory to Prune Neural Networks",
        "abstract": "  We show how solution concepts from cooperative game theory can be used to\ntackle the problem of pruning neural networks.\n  The ever-growing size of deep neural networks (DNNs) increases their\nperformance, but also their computational requirements. We introduce a method\ncalled Game Theory Assisted Pruning (GTAP), which reduces the neural network's\nsize while preserving its predictive accuracy. GTAP is based on eliminating\nneurons in the network based on an estimation of their joint impact on the\nprediction quality through game theoretic solutions. Specifically, we use a\npower index akin to the Shapley value or Banzhaf index, tailored using a\nprocedure similar to Dropout (commonly used to tackle overfitting problems in\nmachine learning).\n  Empirical evaluation of both feedforward networks and convolutional neural\nnetworks shows that this method outperforms existing approaches in the achieved\ntradeoff between the number of parameters and model accuracy.\n",
        "published": "2023",
        "authors": [
            "Mauricio Diaz-Ortiz Jr",
            "Benjamin Kempinski",
            "Daphne Cornelisse",
            "Yoram Bachrach",
            "Tal Kachman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.17165v1",
        "title": "(Ir)rationality in AI: State of the Art, Research Challenges and Open\n  Questions",
        "abstract": "  The concept of rationality is central to the field of artificial\nintelligence. Whether we are seeking to simulate human reasoning, or the goal\nis to achieve bounded optimality, we generally seek to make artificial agents\nas rational as possible. Despite the centrality of the concept within AI, there\nis no unified definition of what constitutes a rational agent. This article\nprovides a survey of rationality and irrationality in artificial intelligence,\nand sets out the open questions in this area. The understanding of rationality\nin other fields has influenced its conception within artificial intelligence,\nin particular work in economics, philosophy and psychology. Focusing on the\nbehaviour of artificial agents, we consider irrational behaviours that can\nprove to be optimal in certain scenarios. Some methods have been developed to\ndeal with irrational agents, both in terms of identification and interaction,\nhowever work in this area remains limited. Methods that have up to now been\ndeveloped for other purposes, namely adversarial scenarios, may be adapted to\nsuit interactions with artificial agents. We further discuss the interplay\nbetween human and artificial agents, and the role that rationality plays within\nthis interaction; many questions remain in this area, relating to potentially\nirrational behaviour of both humans and artificial agents.\n",
        "published": "2023",
        "authors": [
            "Olivia Macmillan-Scott",
            "Mirco Musolesi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.04504v1",
        "title": "Coordination-free Decentralised Federated Learning on Complex Networks:\n  Overcoming Heterogeneity",
        "abstract": "  Federated Learning (FL) is a well-known framework for successfully performing\na learning task in an edge computing scenario where the devices involved have\nlimited resources and incomplete data representation. The basic assumption of\nFL is that the devices communicate directly or indirectly with a parameter\nserver that centrally coordinates the whole process, overcoming several\nchallenges associated with it. However, in highly pervasive edge scenarios, the\npresence of a central controller that oversees the process cannot always be\nguaranteed, and the interactions (i.e., the connectivity graph) between devices\nmight not be predetermined, resulting in a complex network structure. Moreover,\nthe heterogeneity of data and devices further complicates the learning process.\nThis poses new challenges from a learning standpoint that we address by\nproposing a communication-efficient Decentralised Federated Learning (DFL)\nalgorithm able to cope with them. Our solution allows devices communicating\nonly with their direct neighbours to train an accurate model, overcoming the\nheterogeneity induced by data and different training histories. Our results\nshow that the resulting local models generalise better than those trained with\ncompeting approaches, and do so in a more communication-efficient way.\n",
        "published": "2023",
        "authors": [
            "Lorenzo Valerio",
            "Chiara Boldrini",
            "Andrea Passarella",
            "J\u00e1nos Kert\u00e9sz",
            "M\u00e1rton Karsai",
            "Gerardo I\u00f1iguez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.11718v1",
        "title": "Human-Machine Teaming for UAVs: An Experimentation Platform",
        "abstract": "  Full automation is often not achievable or desirable in critical systems with\nhigh-stakes decisions. Instead, human-AI teams can achieve better results. To\nresearch, develop, evaluate, and validate algorithms suited for such teaming,\nlightweight experimentation platforms that enable interactions between humans\nand multiple AI agents are necessary. However, there are limited examples of\nsuch platforms for defense environments. To address this gap, we present the\nCogment human-machine teaming experimentation platform, which implements\nhuman-machine teaming (HMT) use cases that features heterogeneous multi-agent\nsystems and can involve learning AI agents, static AI agents, and humans. It is\nbuilt on the Cogment platform and has been used for academic research,\nincluding work presented at the ALA workshop at AAMAS this year [1]. With this\nplatform, we hope to facilitate further research on human-machine teaming in\ncritical systems and defense environments.\n",
        "published": "2023",
        "authors": [
            "Laila El Moujtahid",
            "Sai Krishna Gottipati",
            "Clod\u00e9ric Mars",
            "Matthew E. Taylor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.07808v2",
        "title": "Quantum Memristors in Quantum Photonics",
        "abstract": "  We propose a method to build quantum memristors in quantum photonic\nplatforms. We firstly design an effective beam splitter, which is tunable in\nreal-time, by means of a Mach-Zehnder-type array with two equal 50:50 beam\nsplitters and a tunable retarder, which allows us to control its reflectivity.\nThen, we show that this tunable beam splitter, when equipped with weak\nmeasurements and classical feedback, behaves as a quantum memristor. Indeed, in\norder to prove its quantumness, we show how to codify quantum information in\nthe coherent beams. Moreover, we estimate the memory capability of the quantum\nmemristor. Finally, we show the feasibility of the proposed setup in integrated\nquantum photonics.\n",
        "published": "2017",
        "authors": [
            "M. Sanz",
            "L. Lamata",
            "E. Solano"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.02029v3",
        "title": "Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention\n  and Spatial Memory",
        "abstract": "  The role of robots in society keeps expanding, bringing with it the necessity\nof interacting and communicating with humans. In order to keep such interaction\nintuitive, we provide automatic wayfinding based on verbal navigational\ninstructions. Our first contribution is the creation of a large-scale dataset\nwith verbal navigation instructions. To this end, we have developed an\ninteractive visual navigation environment based on Google Street View; we\nfurther design an annotation method to highlight mined anchor landmarks and\nlocal directions between them in order to help annotators formulate typical,\nhuman references to those. The annotation task was crowdsourced on the AMT\nplatform, to construct a new Talk2Nav dataset with $10,714$ routes. Our second\ncontribution is a new learning method. Inspired by spatial cognition research\non the mental conceptualization of navigational instructions, we introduce a\nsoft dual attention mechanism defined over the segmented language instructions\nto jointly extract two partial instructions -- one for matching the next\nupcoming visual landmark and the other for matching the local directions to the\nnext landmark. On the similar lines, we also introduce spatial memory scheme to\nencode the local directional transitions. Our work takes advantage of the\nadvance in two lines of research: mental formalization of verbal navigational\ninstructions and training neural network agents for automatic way finding.\nExtensive experiments show that our method significantly outperforms previous\nnavigation methods. For demo video, dataset and code, please refer to our\nproject page: https://www.trace.ethz.ch/publications/2019/talk2nav/index.html\n",
        "published": "2019",
        "authors": [
            "Arun Balajee Vasudevan",
            "Dengxin Dai",
            "Luc Van Gool"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.02857v2",
        "title": "Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous\n  Environments",
        "abstract": "  We develop a language-guided navigation task set in a continuous 3D\nenvironment where agents must execute low-level actions to follow natural\nlanguage navigation directions. By being situated in continuous environments,\nthis setting lifts a number of assumptions implicit in prior work that\nrepresents environments as a sparse graph of panoramas with edges corresponding\nto navigability. Specifically, our setting drops the presumptions of known\nenvironment topologies, short-range oracle navigation, and perfect agent\nlocalization. To contextualize this new task, we develop models that mirror\nmany of the advances made in prior settings as well as single-modality\nbaselines. While some of these techniques transfer, we find significantly lower\nabsolute performance in the continuous setting -- suggesting that performance\nin prior `navigation-graph' settings may be inflated by the strong implicit\nassumptions.\n",
        "published": "2020",
        "authors": [
            "Jacob Krantz",
            "Erik Wijmans",
            "Arjun Majumdar",
            "Dhruv Batra",
            "Stefan Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.05526v2",
        "title": "Learning Articulated Motion Models from Visual and Lingual Signals",
        "abstract": "  In order for robots to operate effectively in homes and workplaces, they must\nbe able to manipulate the articulated objects common within environments built\nfor and by humans. Previous work learns kinematic models that prescribe this\nmanipulation from visual demonstrations. Lingual signals, such as natural\nlanguage descriptions and instructions, offer a complementary means of\nconveying knowledge of such manipulation models and are suitable to a wide\nrange of interactions (e.g., remote manipulation). In this paper, we present a\nmultimodal learning framework that incorporates both visual and lingual\ninformation to estimate the structure and parameters that define kinematic\nmodels of articulated objects. The visual signal takes the form of an RGB-D\nimage stream that opportunistically captures object motion in an unprepared\nscene. Accompanying natural language descriptions of the motion constitute the\nlingual signal. We present a probabilistic language model that uses word\nembeddings to associate lingual verbs with their corresponding kinematic\nstructures. By exploiting the complementary nature of the visual and lingual\ninput, our method infers correct kinematic structures for various multiple-part\nobjects on which the previous state-of-the-art, visual-only system fails. We\nevaluate our multimodal learning framework on a dataset comprised of a variety\nof household objects, and demonstrate a 36% improvement in model accuracy over\nthe vision-only baseline.\n",
        "published": "2015",
        "authors": [
            "Zhengyang Wu",
            "Mohit Bansal",
            "Matthew R. Walter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.00811v1",
        "title": "Target-dependent UNITER: A Transformer-Based Multimodal Language\n  Comprehension Model for Domestic Service Robots",
        "abstract": "  Currently, domestic service robots have an insufficient ability to interact\nnaturally through language. This is because understanding human instructions is\ncomplicated by various ambiguities and missing information. In existing\nmethods, the referring expressions that specify the relationships between\nobjects are insufficiently modeled. In this paper, we propose Target-dependent\nUNITER, which learns the relationship between the target object and other\nobjects directly by focusing on the relevant regions within an image, rather\nthan the whole image. Our method is an extension of the UNITER-based\nTransformer that can be pretrained on general-purpose datasets. We extend the\nUNITER approach by introducing a new architecture for handling the target\ncandidates. Our model is validated on two standard datasets, and the results\nshow that Target-dependent UNITER outperforms the baseline method in terms of\nclassification accuracy.\n",
        "published": "2021",
        "authors": [
            "Shintaro Ishikawa",
            "Komei Sugiura"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.03807v1",
        "title": "Sim-to-Real Transfer for Vision-and-Language Navigation",
        "abstract": "  We study the challenging problem of releasing a robot in a previously unseen\nenvironment, and having it follow unconstrained natural language navigation\ninstructions. Recent work on the task of Vision-and-Language Navigation (VLN)\nhas achieved significant progress in simulation. To assess the implications of\nthis work for robotics, we transfer a VLN agent trained in simulation to a\nphysical robot. To bridge the gap between the high-level discrete action space\nlearned by the VLN agent, and the robot's low-level continuous action space, we\npropose a subgoal model to identify nearby waypoints, and use domain\nrandomization to mitigate visual domain differences. For accurate sim and real\ncomparisons in parallel environments, we annotate a 325m2 office space with\n1.3km of navigation instructions, and create a digitized replica in simulation.\nWe find that sim-to-real transfer to an environment not seen in training is\nsuccessful if an occupancy map and navigation graph can be collected and\nannotated in advance (success rate of 46.8% vs. 55.9% in sim), but much more\nchallenging in the hardest setting with no prior mapping at all (success rate\nof 22.5%).\n",
        "published": "2020",
        "authors": [
            "Peter Anderson",
            "Ayush Shrivastava",
            "Joanne Truong",
            "Arjun Majumdar",
            "Devi Parikh",
            "Dhruv Batra",
            "Stefan Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.03940v1",
        "title": "Disambiguating Affective Stimulus Associations for Robot Perception and\n  Dialogue",
        "abstract": "  Effectively recognising and applying emotions to interactions is a highly\ndesirable trait for social robots. Implicitly understanding how subjects\nexperience different kinds of actions and objects in the world is crucial for\nnatural HRI interactions, with the possibility to perform positive actions and\navoid negative actions. In this paper, we utilize the NICO robot's appearance\nand capabilities to give the NICO the ability to model a coherent affective\nassociation between a perceived auditory stimulus and a temporally asynchronous\nemotion expression. This is done by combining evaluations of emotional valence\nfrom vision and language. NICO uses this information to make decisions about\nwhen to extend conversations in order to accrue more affective information if\nthe representation of the association is not coherent. Our primary contribution\nis providing a NICO robot with the ability to learn the affective associations\nbetween a perceived auditory stimulus and an emotional expression. NICO is able\nto do this for both individual subjects and specific stimuli, with the aid of\nan emotion-driven dialogue system that rectifies emotional expression\nincoherences. The robot is then able to use this information to determine a\nsubject's enjoyment of perceived auditory stimuli in a real HRI scenario.\n",
        "published": "2021",
        "authors": [
            "Henrique Siqueira",
            "Alexander Sutherland",
            "Pablo Barros",
            "Mattias Kerzel",
            "Sven Magg",
            "Stefan Wermter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.11945v1",
        "title": "SASRA: Semantically-aware Spatio-temporal Reasoning Agent for\n  Vision-and-Language Navigation in Continuous Environments",
        "abstract": "  This paper presents a novel approach for the Vision-and-Language Navigation\n(VLN) task in continuous 3D environments, which requires an autonomous agent to\nfollow natural language instructions in unseen environments. Existing\nend-to-end learning-based VLN methods struggle at this task as they focus\nmostly on utilizing raw visual observations and lack the semantic\nspatio-temporal reasoning capabilities which is crucial in generalizing to new\nenvironments. In this regard, we present a hybrid transformer-recurrence model\nwhich focuses on combining classical semantic mapping techniques with a\nlearning-based method. Our method creates a temporal semantic memory by\nbuilding a top-down local ego-centric semantic map and performs cross-modal\ngrounding to align map and language modalities to enable effective learning of\nVLN policy. Empirical results in a photo-realistic long-horizon simulation\nenvironment show that the proposed approach outperforms a variety of\nstate-of-the-art methods and baselines with over 22% relative improvement in\nSPL in prior unseen environments.\n",
        "published": "2021",
        "authors": [
            "Muhammad Zubair Irshad",
            "Niluthpol Chowdhury Mithun",
            "Zachary Seymour",
            "Han-Pang Chiu",
            "Supun Samarasekera",
            "Rakesh Kumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.02764v1",
        "title": "Bridging the Gap Between Learning in Discrete and Continuous\n  Environments for Vision-and-Language Navigation",
        "abstract": "  Most existing works in vision-and-language navigation (VLN) focus on either\ndiscrete or continuous environments, training agents that cannot generalize\nacross the two. The fundamental difference between the two setups is that\ndiscrete navigation assumes prior knowledge of the connectivity graph of the\nenvironment, so that the agent can effectively transfer the problem of\nnavigation with low-level controls to jumping from node to node with high-level\nactions by grounding to an image of a navigable direction. To bridge the\ndiscrete-to-continuous gap, we propose a predictor to generate a set of\ncandidate waypoints during navigation, so that agents designed with high-level\nactions can be transferred to and trained in continuous environments. We refine\nthe connectivity graph of Matterport3D to fit the continuous\nHabitat-Matterport3D, and train the waypoints predictor with the refined graphs\nto produce accessible waypoints at each time step. Moreover, we demonstrate\nthat the predicted waypoints can be augmented during training to diversify the\nviews and paths, and therefore enhance agent's generalization ability. Through\nextensive experiments we show that agents navigating in continuous environments\nwith predicted waypoints perform significantly better than agents using\nlow-level actions, which reduces the absolute discrete-to-continuous gap by\n11.76% Success Weighted by Path Length (SPL) for the Cross-Modal Matching Agent\nand 18.24% SPL for the Recurrent VLN-BERT. Our agents, trained with a simple\nimitation learning objective, outperform previous methods by a large margin,\nachieving new state-of-the-art results on the testing environments of the\nR2R-CE and the RxR-CE datasets.\n",
        "published": "2022",
        "authors": [
            "Yicong Hong",
            "Zun Wang",
            "Qi Wu",
            "Stephen Gould"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.06822v1",
        "title": "Grounding Commands for Autonomous Vehicles via Layer Fusion with\n  Region-specific Dynamic Layer Attention",
        "abstract": "  Grounding a command to the visual environment is an essential ingredient for\ninteractions between autonomous vehicles and humans. In this work, we study the\nproblem of language grounding for autonomous vehicles, which aims to localize a\nregion in a visual scene according to a natural language command from a\npassenger. Prior work only employs the top layer representations of a\nvision-and-language pre-trained model to predict the region referred to by the\ncommand. However, such a method omits the useful features encoded in other\nlayers, and thus results in inadequate understanding of the input scene and\ncommand. To tackle this limitation, we present the first layer fusion approach\nfor this task. Since different visual regions may require distinct types of\nfeatures to disambiguate them from each other, we further propose the\nregion-specific dynamic (RSD) layer attention to adaptively fuse the multimodal\ninformation across layers for each region. Extensive experiments on the\nTalk2Car benchmark demonstrate that our approach helps predict more accurate\nregions and outperforms state-of-the-art methods.\n",
        "published": "2022",
        "authors": [
            "Hou Pong Chan",
            "Mingxi Guo",
            "Cheng-Zhong Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.00889v1",
        "title": "Moment-based Adversarial Training for Embodied Language Comprehension",
        "abstract": "  In this paper, we focus on a vision-and-language task in which a robot is\ninstructed to execute household tasks. Given an instruction such as \"Rinse off\na mug and place it in the coffee maker,\" the robot is required to locate the\nmug, wash it, and put it in the coffee maker. This is challenging because the\nrobot needs to break down the instruction sentences into subgoals and execute\nthem in the correct order. On the ALFRED benchmark, the performance of\nstate-of-the-art methods is still far lower than that of humans. This is\npartially because existing methods sometimes fail to infer subgoals that are\nnot explicitly specified in the instruction sentences. We propose Moment-based\nAdversarial Training (MAT), which uses two types of moments for perturbation\nupdates in adversarial training. We introduce MAT to the embedding spaces of\nthe instruction, subgoals, and state representations to handle their varieties.\nWe validated our method on the ALFRED benchmark, and the results demonstrated\nthat our method outperformed the baseline method for all the metrics on the\nbenchmark.\n",
        "published": "2022",
        "authors": [
            "Shintaro Ishikawa",
            "Komei Sugiura"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.08522v2",
        "title": "VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation",
        "abstract": "  Benefiting from language flexibility and compositionality, humans naturally\nintend to use language to command an embodied agent for complex tasks such as\nnavigation and object manipulation. In this work, we aim to fill the blank of\nthe last mile of embodied agents -- object manipulation by following human\nguidance, e.g., \"move the red mug next to the box while keeping it upright.\" To\nthis end, we introduce an Automatic Manipulation Solver (AMSolver) system and\nbuild a Vision-and-Language Manipulation benchmark (VLMbench) based on it,\ncontaining various language instructions on categorized robotic manipulation\ntasks. Specifically, modular rule-based task templates are created to\nautomatically generate robot demonstrations with language instructions,\nconsisting of diverse object shapes and appearances, action types, and motion\nconstraints. We also develop a keypoint-based model 6D-CLIPort to deal with\nmulti-view observations and language input and output a sequence of 6 degrees\nof freedom (DoF) actions. We hope the new simulator and benchmark will\nfacilitate future research on language-guided robotic manipulation.\n",
        "published": "2022",
        "authors": [
            "Kaizhi Zheng",
            "Xiaotong Chen",
            "Odest Chadwicke Jenkins",
            "Xin Eric Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.03087v3",
        "title": "Iterative Vision-and-Language Navigation",
        "abstract": "  We present Iterative Vision-and-Language Navigation (IVLN), a paradigm for\nevaluating language-guided agents navigating in a persistent environment over\ntime. Existing Vision-and-Language Navigation (VLN) benchmarks erase the\nagent's memory at the beginning of every episode, testing the ability to\nperform cold-start navigation with no prior information. However, deployed\nrobots occupy the same environment for long periods of time. The IVLN paradigm\naddresses this disparity by training and evaluating VLN agents that maintain\nmemory across tours of scenes that consist of up to 100 ordered\ninstruction-following Room-to-Room (R2R) episodes, each defined by an\nindividual language instruction and a target path. We present discrete and\ncontinuous Iterative Room-to-Room (IR2R) benchmarks comprising about 400 tours\neach in 80 indoor scenes. We find that extending the implicit memory of\nhigh-performing transformer VLN agents is not sufficient for IVLN, but agents\nthat build maps can benefit from environment persistence, motivating a renewed\nfocus on map-building agents in VLN.\n",
        "published": "2022",
        "authors": [
            "Jacob Krantz",
            "Shurjo Banerjee",
            "Wang Zhu",
            "Jason Corso",
            "Peter Anderson",
            "Stefan Lee",
            "Jesse Thomason"
        ]
    }
]