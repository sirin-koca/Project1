[
    {
        "id": "http://arxiv.org/abs/2112.14146v1",
        "title": "Towards continual task learning in artificial neural networks: current\n  approaches and insights from neuroscience",
        "abstract": "  The innate capacity of humans and other animals to learn a diverse, and often\ninterfering, range of knowledge and skills throughout their lifespan is a\nhallmark of natural intelligence, with obvious evolutionary motivations. In\nparallel, the ability of artificial neural networks (ANNs) to learn across a\nrange of tasks and domains, combining and re-using learned representations\nwhere required, is a clear goal of artificial intelligence. This capacity,\nwidely described as continual learning, has become a prolific subfield of\nresearch in machine learning. Despite the numerous successes of deep learning\nin recent years, across domains ranging from image recognition to machine\ntranslation, such continual task learning has proved challenging. Neural\nnetworks trained on multiple tasks in sequence with stochastic gradient descent\noften suffer from representational interference, whereby the learned weights\nfor a given task effectively overwrite those of previous tasks in a process\ntermed catastrophic forgetting. This represents a major impediment to the\ndevelopment of more generalised artificial learning systems, capable of\naccumulating knowledge over time and task space, in a manner analogous to\nhumans. A repository of selected papers and implementations accompanying this\nreview can be found at https://github.com/mccaffary/continual-learning.\n",
        "published": "2021",
        "authors": [
            "David McCaffary"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.16464v2",
        "title": "Towards Interpretable Deep Reinforcement Learning Models via Inverse\n  Reinforcement Learning",
        "abstract": "  Artificial intelligence, particularly through recent advancements in deep\nlearning, has achieved exceptional performances in many tasks in fields such as\nnatural language processing and computer vision. In addition to desirable\nevaluation metrics, a high level of interpretability is often required for\nthese models to be reliably utilized. Therefore, explanations that offer\ninsight into the process by which a model maps its inputs onto its outputs are\nmuch sought-after. Unfortunately, the current black box nature of machine\nlearning models is still an unresolved issue and this very nature prevents\nresearchers from learning and providing explicative descriptions for a model's\nbehavior and final predictions. In this work, we propose a novel framework\nutilizing Adversarial Inverse Reinforcement Learning that can provide global\nexplanations for decisions made by a Reinforcement Learning model and capture\nintuitive tendencies that the model follows by summarizing the model's\ndecision-making process.\n",
        "published": "2022",
        "authors": [
            "Sean Xie",
            "Soroush Vosoughi",
            "Saeed Hassanpour"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.09435v4",
        "title": "Adversarial random forests for density estimation and generative\n  modeling",
        "abstract": "  We propose methods for density estimation and data synthesis using a novel\nform of unsupervised random forests. Inspired by generative adversarial\nnetworks, we implement a recursive procedure in which trees gradually learn\nstructural properties of the data through alternating rounds of generation and\ndiscrimination. The method is provably consistent under minimal assumptions.\nUnlike classic tree-based alternatives, our approach provides smooth\n(un)conditional densities and allows for fully synthetic data generation. We\nachieve comparable or superior performance to state-of-the-art probabilistic\ncircuits and deep learning models on various tabular data benchmarks while\nexecuting about two orders of magnitude faster on average. An accompanying\n$\\texttt{R}$ package, $\\texttt{arf}$, is available on $\\texttt{CRAN}$.\n",
        "published": "2022",
        "authors": [
            "David S. Watson",
            "Kristin Blesch",
            "Jan Kapar",
            "Marvin N. Wright"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.11146v1",
        "title": "VTrackIt: A Synthetic Self-Driving Dataset with Infrastructure and\n  Pooled Vehicle Information",
        "abstract": "  Artificial intelligence solutions for Autonomous Vehicles (AVs) have been\ndeveloped using publicly available datasets such as Argoverse, ApolloScape,\nLevel5, and NuScenes. One major limitation of these datasets is the absence of\ninfrastructure and/or pooled vehicle information like lane line type, vehicle\nspeed, traffic signs, and intersections. Such information is necessary and not\ncomplementary to eliminating high-risk edge cases. The rapid advancements in\nVehicle-to-Infrastructure and Vehicle-to-Vehicle technologies show promise that\ninfrastructure and pooled vehicle information will soon be accessible in near\nreal-time. Taking a leap in the future, we introduce the first comprehensive\nsynthetic dataset with intelligent infrastructure and pooled vehicle\ninformation for advancing the next generation of AVs, named VTrackIt. We also\nintroduce the first deep learning model (InfraGAN) for trajectory predictions\nthat considers such information. Our experiments with InfraGAN show that the\ncomprehensive information offered by VTrackIt reduces the number of high-risk\nedge cases. The VTrackIt dataset is available upon request under the Creative\nCommons CC BY-NC-SA 4.0 license at http://vtrackit.irda.club.\n",
        "published": "2022",
        "authors": [
            "Mayuresh Savargaonkar",
            "Abdallah Chehade"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.07645v1",
        "title": "Evaluating Distribution System Reliability with Hyperstructures Graph\n  Convolutional Nets",
        "abstract": "  Nowadays, it is broadly recognized in the power system community that to meet\nthe ever expanding energy sector's needs, it is no longer possible to rely\nsolely on physics-based models and that reliable, timely and sustainable\noperation of energy systems is impossible without systematic integration of\nartificial intelligence (AI) tools. Nevertheless, the adoption of AI in power\nsystems is still limited, while integration of AI particularly into\ndistribution grid investment planning is still an uncharted territory. We make\nthe first step forward to bridge this gap by showing how graph convolutional\nnetworks coupled with the hyperstructures representation learning framework can\nbe employed for accurate, reliable, and computationally efficient distribution\ngrid planning with resilience objectives. We further propose a Hyperstructures\nGraph Convolutional Neural Networks (Hyper-GCNNs) to capture hidden higher\norder representations of distribution networks with attention mechanism. Our\nnumerical experiments show that the proposed Hyper-GCNNs approach yields\nsubstantial gains in computational efficiency compared to the prevailing\nmethodology in distribution grid planning and also noticeably outperforms seven\nstate-of-the-art models from deep learning (DL) community.\n",
        "published": "2022",
        "authors": [
            "Yuzhou Chen",
            "Tian Jiang",
            "Miguel Heleno",
            "Alexandre Moreira",
            "Yulia R. Gel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.03052v1",
        "title": "AI Maintenance: A Robustness Perspective",
        "abstract": "  With the advancements in machine learning (ML) methods and compute resources,\nartificial intelligence (AI) empowered systems are becoming a prevailing\ntechnology. However, current AI technology such as deep learning is not\nflawless. The significantly increased model complexity and data scale incur\nintensified challenges when lacking trustworthiness and transparency, which\ncould create new risks and negative impacts. In this paper, we carve out AI\nmaintenance from the robustness perspective. We start by introducing some\nhighlighted robustness challenges in the AI lifecycle and motivating AI\nmaintenance by making analogies to car maintenance. We then propose an AI model\ninspection framework to detect and mitigate robustness risks. We also draw\ninspiration from vehicle autonomy to define the levels of AI robustness\nautomation. Our proposal for AI maintenance facilitates robustness assessment,\nstatus tracking, risk scanning, model hardening, and regulation throughout the\nAI lifecycle, which is an essential milestone toward building sustainable and\ntrustworthy AI ecosystems.\n",
        "published": "2023",
        "authors": [
            "Pin-Yu Chen",
            "Payel Das"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.00117v1",
        "title": "Real Estate Property Valuation using Self-Supervised Vision Transformers",
        "abstract": "  The use of Artificial Intelligence (AI) in the real estate market has been\ngrowing in recent years. In this paper, we propose a new method for property\nvaluation that utilizes self-supervised vision transformers, a recent\nbreakthrough in computer vision and deep learning. Our proposed algorithm uses\na combination of machine learning, computer vision and hedonic pricing models\ntrained on real estate data to estimate the value of a given property. We\ncollected and pre-processed a data set of real estate properties in the city of\nBoulder, Colorado and used it to train, validate and test our algorithm. Our\ndata set consisted of qualitative images (including house interiors, exteriors,\nand street views) as well as quantitative features such as the number of\nbedrooms, bathrooms, square footage, lot square footage, property age, crime\nrates, and proximity to amenities. We evaluated the performance of our model\nusing metrics such as Root Mean Squared Error (RMSE). Our findings indicate\nthat these techniques are able to accurately predict the value of properties,\nwith a low RMSE. The proposed algorithm outperforms traditional appraisal\nmethods that do not leverage property images and has the potential to be used\nin real-world applications.\n",
        "published": "2023",
        "authors": [
            "Mahdieh Yazdani",
            "Maziar Raissi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.00311v1",
        "title": "Medical Pathologies Prediction : Systematic Review and Proposed Approach",
        "abstract": "  The healthcare sector is an important pillar of every community, numerous\nresearch studies have been carried out in this context to optimize medical\nprocesses and improve care quality and facilitate patient management. In this\narticle we have analyzed and examined different works concerning the\nexploitation of the most recent technologies such as big data, artificial\nintelligence, machine learning, and deep learning for the improvement of health\ncare, which enabled us to propose our general approach concentrating on the\ncollection, preprocessing and clustering of medical data to facilitate access,\nafter analysis, to the patients and health professionals to predict the most\nfrequent pathologies with better precision within a notable timeframe.\n  keywords: Healthcare, big data, artificial intelligence, automatic language\nprocessing, data mining, predictive models.\n",
        "published": "2023",
        "authors": [
            "Chaimae Taoussi",
            "Imad Hafidi",
            "Abdelmoutalib Metrane"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.11880v1",
        "title": "The State of the Art in transformer fault diagnosis with artificial\n  intelligence and Dissolved Gas Analysis: A Review of the Literature",
        "abstract": "  Transformer fault diagnosis (TFD) is a critical aspect of power system\nmaintenance and management. This review paper provides a comprehensive overview\nof the current state of the art in TFD using artificial intelligence (AI) and\ndissolved gas analysis (DGA). The paper presents an analysis of recent\nadvancements in this field, including the use of deep learning algorithms and\nadvanced data analytics techniques, and their potential impact on TFD and the\npower industry as a whole. The review also highlights the benefits and\nlimitations of different approaches to transformer fault diagnosis, including\nrule-based systems, expert systems, neural networks, and machine learning\nalgorithms. Overall, this review aims to provide valuable insights into the\nimportance of TFD and the role of AI in ensuring the reliable operation of\npower systems.\n",
        "published": "2023",
        "authors": [
            "Yuyan Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.03025v2",
        "title": "AI Techniques for Cone Beam Computed Tomography in Dentistry: Trends and\n  Practices",
        "abstract": "  Cone-beam computed tomography (CBCT) is a popular imaging modality in\ndentistry for diagnosing and planning treatment for a variety of oral diseases\nwith the ability to produce detailed, three-dimensional images of the teeth,\njawbones, and surrounding structures. CBCT imaging has emerged as an essential\ndiagnostic tool in dentistry. CBCT imaging has seen significant improvements in\nterms of its diagnostic value, as well as its accuracy and efficiency, with the\nmost recent development of artificial intelligence (AI) techniques. This paper\nreviews recent AI trends and practices in dental CBCT imaging. AI has been used\nfor lesion detection, malocclusion classification, measurement of buccal bone\nthickness, and classification and segmentation of teeth, alveolar bones,\nmandibles, landmarks, contours, and pharyngeal airways using CBCT images.\nMainly machine learning algorithms, deep learning algorithms, and\nsuper-resolution techniques are used for these tasks. This review focuses on\nthe potential of AI techniques to transform CBCT imaging in dentistry, which\nwould improve both diagnosis and treatment planning. Finally, we discuss the\nchallenges and limitations of artificial intelligence in dentistry and CBCT\nimaging.\n",
        "published": "2023",
        "authors": [
            "Saba Sarwar",
            "Suraiya Jabin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.12816v2",
        "title": "XAI-TRIS: Non-linear image benchmarks to quantify false positive\n  post-hoc attribution of feature importance",
        "abstract": "  The field of 'explainable' artificial intelligence (XAI) has produced highly\ncited methods that seek to make the decisions of complex machine learning (ML)\nmethods 'understandable' to humans, for example by attributing 'importance'\nscores to input features. Yet, a lack of formal underpinning leaves it unclear\nas to what conclusions can safely be drawn from the results of a given XAI\nmethod and has also so far hindered the theoretical verification and empirical\nvalidation of XAI methods. This means that challenging non-linear problems,\ntypically solved by deep neural networks, presently lack appropriate remedies.\nHere, we craft benchmark datasets for three different non-linear classification\nscenarios, in which the important class-conditional features are known by\ndesign, serving as ground truth explanations. Using novel quantitative metrics,\nwe benchmark the explanation performance of a wide set of XAI methods across\nthree deep learning model architectures. We show that popular XAI methods are\noften unable to significantly outperform random performance baselines and edge\ndetection methods. Moreover, we demonstrate that explanations derived from\ndifferent model architectures can be vastly different; thus, prone to\nmisinterpretation even under controlled conditions.\n",
        "published": "2023",
        "authors": [
            "Benedict Clark",
            "Rick Wilming",
            "Stefan Haufe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.05411v1",
        "title": "Explainable AI applications in the Medical Domain: a systematic review",
        "abstract": "  Artificial Intelligence in Medicine has made significant progress with\nemerging applications in medical imaging, patient care, and other areas. While\nthese applications have proven successful in retrospective studies, very few of\nthem were applied in practice.The field of Medical AI faces various challenges,\nin terms of building user trust, complying with regulations, using data\nethically.Explainable AI (XAI) aims to enable humans understand AI and trust\nits results. This paper presents a literature review on the recent developments\nof XAI solutions for medical decision support, based on a representative sample\nof 198 articles published in recent years. The systematic synthesis of the\nrelevant articles resulted in several findings. (1) model-agnostic XAI\ntechniques were mostly employed in these solutions, (2) deep learning models\nare utilized more than other types of machine learning models, (3)\nexplainability was applied to promote trust, but very few works reported the\nphysicians participation in the loop, (4) visual and interactive user interface\nis more useful in understanding the explanation and the recommendation of the\nsystem. More research is needed in collaboration between medical and AI\nexperts, that could guide the development of suitable frameworks for the\ndesign, implementation, and evaluation of XAI solutions in medicine.\n",
        "published": "2023",
        "authors": [
            "Nicoletta Prentzas",
            "Antonis Kakas",
            "Constantinos S. Pattichis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.09854v1",
        "title": "SurvTimeSurvival: Survival Analysis On The Patient With Multiple\n  Visits/Records",
        "abstract": "  The accurate prediction of survival times for patients with severe diseases\nremains a critical challenge despite recent advances in artificial\nintelligence. This study introduces \"SurvTimeSurvival: Survival Analysis On\nPatients With Multiple Visits/Records\", utilizing the Transformer model to not\nonly handle the complexities of time-varying covariates but also covariates\ndata. We also tackle the data sparsity issue common to survival analysis\ndatasets by integrating synthetic data generation into the learning process of\nour model. We show that our method outperforms state-of-the-art deep learning\napproaches on both covariates and time-varying covariates datasets. Our\napproach aims not only to enhance the understanding of individual patient\nsurvival trajectories across various medical conditions, thereby improving\nprediction accuracy, but also to play a pivotal role in designing clinical\ntrials and creating new treatments.\n",
        "published": "2023",
        "authors": [
            "Hung Le",
            "Ong Eng-Jon",
            "Bober Miroslaw"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.00656v2",
        "title": "Simple Transferability Estimation for Regression Tasks",
        "abstract": "  We consider transferability estimation, the problem of estimating how well\ndeep learning models transfer from a source to a target task. We focus on\nregression tasks, which received little previous attention, and propose two\nsimple and computationally efficient approaches that estimate transferability\nbased on the negative regularized mean squared error of a linear regression\nmodel. We prove novel theoretical results connecting our approaches to the\nactual transferability of the optimal target models obtained from the transfer\nlearning process. Despite their simplicity, our approaches significantly\noutperform existing state-of-the-art regression transferability estimators in\nboth accuracy and efficiency. On two large-scale keypoint regression\nbenchmarks, our approaches yield 12% to 36% better results on average while\nbeing at least 27% faster than previous state-of-the-art methods.\n",
        "published": "2023",
        "authors": [
            "Cuong N. Nguyen",
            "Phong Tran",
            "Lam Si Tung Ho",
            "Vu Dinh",
            "Anh T. Tran",
            "Tal Hassner",
            "Cuong V. Nguyen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.06082v1",
        "title": "XAI meets Biology: A Comprehensive Review of Explainable AI in\n  Bioinformatics Applications",
        "abstract": "  Artificial intelligence (AI), particularly machine learning and deep learning\nmodels, has significantly impacted bioinformatics research by offering powerful\ntools for analyzing complex biological data. However, the lack of\ninterpretability and transparency of these models presents challenges in\nleveraging these models for deeper biological insights and for generating\ntestable hypotheses. Explainable AI (XAI) has emerged as a promising solution\nto enhance the transparency and interpretability of AI models in\nbioinformatics. This review provides a comprehensive analysis of various XAI\ntechniques and their applications across various bioinformatics domains\nincluding DNA, RNA, and protein sequence analysis, structural analysis, gene\nexpression and genome analysis, and bioimaging analysis. We introduce the most\npertinent machine learning and XAI methods, then discuss their diverse\napplications and address the current limitations of available XAI tools. By\noffering insights into XAI's potential and challenges, this review aims to\nfacilitate its practical implementation in bioinformatics research and help\nresearchers navigate the landscape of XAI tools.\n",
        "published": "2023",
        "authors": [
            "Zhongliang Zhou",
            "Mengxuan Hu",
            "Mariah Salcedo",
            "Nathan Gravel",
            "Wayland Yeung",
            "Aarya Venkat",
            "Dongliang Guo",
            "Jielu Zhang",
            "Natarajan Kannan",
            "Sheng Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.11507v1",
        "title": "Explain To Decide: A Human-Centric Review on the Role of Explainable\n  Artificial Intelligence in AI-assisted Decision Making",
        "abstract": "  The unprecedented performance of machine learning models in recent years,\nparticularly Deep Learning and transformer models, has resulted in their\napplication in various domains such as finance, healthcare, and education.\nHowever, the models are error-prone and cannot be used autonomously, especially\nin decision-making scenarios where, technically or ethically, the cost of error\nis high. Moreover, because of the black-box nature of these models, it is\nfrequently difficult for the end user to comprehend the models' outcomes and\nunderlying processes to trust and use the model outcome to make a decision.\nExplainable Artificial Intelligence (XAI) aids end-user understanding of the\nmodel by utilizing approaches, including visualization techniques, to explain\nand interpret the inner workings of the model and how it arrives at a result.\nAlthough numerous research studies have been conducted recently focusing on the\nperformance of models and the XAI approaches, less work has been done on the\nimpact of explanations on human-AI team performance. This paper surveyed the\nrecent empirical studies on XAI's impact on human-AI decision-making,\nidentified the challenges, and proposed future research directions.\n",
        "published": "2023",
        "authors": [
            "Milad Rogha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.01342v1",
        "title": "Securing the Digital World: Protecting smart infrastructures and digital\n  industries with Artificial Intelligence (AI)-enabled malware and intrusion\n  detection",
        "abstract": "  The last decades have been characterized by unprecedented technological\nadvances, many of them powered by modern technologies such as Artificial\nIntelligence (AI) and Machine Learning (ML). The world has become more\ndigitally connected than ever, but we face major challenges. One of the most\nsignificant is cybercrime, which has emerged as a global threat to governments,\nbusinesses, and civil societies. The pervasiveness of digital technologies\ncombined with a constantly shifting technological foundation has created a\ncomplex and powerful playground for cybercriminals, which triggered a surge in\ndemand for intelligent threat detection systems based on machine and deep\nlearning. This paper investigates AI-based cyber threat detection to protect\nour modern digital ecosystems. The primary focus is on evaluating ML-based\nclassifiers and ensembles for anomaly-based malware detection and network\nintrusion detection and how to integrate those models in the context of network\nsecurity, mobile security, and IoT security. The discussion highlights the\nchallenges when deploying and integrating AI-enabled cybersecurity solutions\ninto existing enterprise systems and IT infrastructures, including options to\novercome those challenges. Finally, the paper provides future research\ndirections to further increase the security and resilience of our modern\ndigital industries, infrastructures, and ecosystems.\n",
        "published": "2023",
        "authors": [
            "Marc Schmitt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1608.06581v1",
        "title": "Fathom: Reference Workloads for Modern Deep Learning Methods",
        "abstract": "  Deep learning has been popularized by its recent successes on challenging\nartificial intelligence problems. One of the reasons for its dominance is also\nan ongoing challenge: the need for immense amounts of computational power.\nHardware architects have responded by proposing a wide array of promising\nideas, but to date, the majority of the work has focused on specific algorithms\nin somewhat narrow application domains. While their specificity does not\ndiminish these approaches, there is a clear need for more flexible solutions.\nWe believe the first step is to examine the characteristics of cutting edge\nmodels from across the deep learning community.\n  Consequently, we have assembled Fathom: a collection of eight archetypal deep\nlearning workloads for study. Each of these models comes from a seminal work in\nthe deep learning community, ranging from the familiar deep convolutional\nneural network of Krizhevsky et al., to the more exotic memory networks from\nFacebook's AI research group. Fathom has been released online, and this paper\nfocuses on understanding the fundamental performance characteristics of each\nmodel. We use a set of application-level modeling tools built around the\nTensorFlow deep learning framework in order to analyze the behavior of the\nFathom workloads. We present a breakdown of where time is spent, the\nsimilarities between the performance profiles of our models, an analysis of\nbehavior in inference and training, and the effects of parallelism on scaling.\n",
        "published": "2016",
        "authors": [
            "Robert Adolf",
            "Saketh Rama",
            "Brandon Reagen",
            "Gu-Yeon Wei",
            "David Brooks"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.00121v1",
        "title": "FPGA-based Accelerators of Deep Learning Networks for Learning and\n  Classification: A Review",
        "abstract": "  Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\n",
        "published": "2019",
        "authors": [
            "Ahmad Shawahna",
            "Sadiq M. Sait",
            "Aiman El-Maleh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.09624v2",
        "title": "Optimism in the Face of Adversity: Understanding and Improving Deep\n  Learning through Adversarial Robustness",
        "abstract": "  Driven by massive amounts of data and important advances in computational\nresources, new deep learning systems have achieved outstanding results in a\nlarge spectrum of applications. Nevertheless, our current theoretical\nunderstanding on the mathematical foundations of deep learning lags far behind\nits empirical success. Towards solving the vulnerability of neural networks,\nhowever, the field of adversarial robustness has recently become one of the\nmain sources of explanations of our deep models. In this article, we provide an\nin-depth review of the field of adversarial robustness in deep learning, and\ngive a self-contained introduction to its main notions. But, in contrast to the\nmainstream pessimistic perspective of adversarial robustness, we focus on the\nmain positive aspects that it entails. We highlight the intuitive connection\nbetween adversarial examples and the geometry of deep neural networks, and\neventually explore how the geometric study of adversarial examples can serve as\na powerful tool to understand deep learning. Furthermore, we demonstrate the\nbroad applicability of adversarial robustness, providing an overview of the\nmain emerging applications of adversarial robustness beyond security. The goal\nof this article is to provide readers with a set of new perspectives to\nunderstand deep learning, and to supply them with intuitive tools and insights\non how to use adversarial robustness to improve it.\n",
        "published": "2020",
        "authors": [
            "Guillermo Ortiz-Jimenez",
            "Apostolos Modas",
            "Seyed-Mohsen Moosavi-Dezfooli",
            "Pascal Frossard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.04073v2",
        "title": "Deeplite Neutrino: An End-to-End Framework for Constrained Deep Learning\n  Model Optimization",
        "abstract": "  Designing deep learning-based solutions is becoming a race for training\ndeeper models with a greater number of layers. While a large-size deeper model\ncould provide competitive accuracy, it creates a lot of logistical challenges\nand unreasonable resource requirements during development and deployment. This\nhas been one of the key reasons for deep learning models not being excessively\nused in various production environments, especially in edge devices. There is\nan immediate requirement for optimizing and compressing these deep learning\nmodels, to enable on-device intelligence. In this research, we introduce a\nblack-box framework, Deeplite Neutrino for production-ready optimization of\ndeep learning models. The framework provides an easy mechanism for the\nend-users to provide constraints such as a tolerable drop in accuracy or target\nsize of the optimized models, to guide the whole optimization process. The\nframework is easy to include in an existing production pipeline and is\navailable as a Python Package, supporting PyTorch and Tensorflow libraries. The\noptimization performance of the framework is shown across multiple benchmark\ndatasets and popular deep learning models. Further, the framework is currently\nused in production and the results and testimonials from several clients are\nsummarized.\n",
        "published": "2021",
        "authors": [
            "Anush Sankaran",
            "Olivier Mastropietro",
            "Ehsan Saboori",
            "Yasser Idris",
            "Davis Sawyer",
            "MohammadHossein AskariHemmat",
            "Ghouthi Boukli Hacene"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.05839v4",
        "title": "BigDL: A Distributed Deep Learning Framework for Big Data",
        "abstract": "  This paper presents BigDL (a distributed deep learning framework for Apache\nSpark), which has been used by a variety of users in the industry for building\ndeep learning applications on production big data platforms. It allows deep\nlearning applications to run on the Apache Hadoop/Spark cluster so as to\ndirectly process the production data, and as a part of the end-to-end data\nanalysis pipeline for deployment and management. Unlike existing deep learning\nframeworks, BigDL implements distributed, data parallel training directly on\ntop of the functional compute model (with copy-on-write and coarse-grained\noperations) of Spark. We also share real-world experience and \"war stories\" of\nusers that have adopted BigDL to address their challenges(i.e., how to easily\nbuild end-to-end data analysis and deep learning pipelines for their production\ndata).\n",
        "published": "2018",
        "authors": [
            "Jason Dai",
            "Yiheng Wang",
            "Xin Qiu",
            "Ding Ding",
            "Yao Zhang",
            "Yanzhang Wang",
            "Xianyan Jia",
            "Cherry Zhang",
            "Yan Wan",
            "Zhichao Li",
            "Jiao Wang",
            "Shengsheng Huang",
            "Zhongyuan Wu",
            "Yang Wang",
            "Yuhao Yang",
            "Bowen She",
            "Dongjie Shi",
            "Qi Lu",
            "Kai Huang",
            "Guoqiong Song"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.00078v2",
        "title": "Deep Learning for Audio Signal Processing",
        "abstract": "  Given the recent surge in developments of deep learning, this article\nprovides a review of the state-of-the-art deep learning techniques for audio\nsignal processing. Speech, music, and environmental sound processing are\nconsidered side-by-side, in order to point out similarities and differences\nbetween the domains, highlighting general methods, problems, key references,\nand potential for cross-fertilization between areas. The dominant feature\nrepresentations (in particular, log-mel spectra and raw waveform) and deep\nlearning models are reviewed, including convolutional neural networks, variants\nof the long short-term memory architecture, as well as more audio-specific\nneural network models. Subsequently, prominent deep learning application areas\nare covered, i.e. audio recognition (automatic speech recognition, music\ninformation retrieval, environmental sound detection, localization and\ntracking) and synthesis and transformation (source separation, audio\nenhancement, generative models for speech, sound, and music synthesis).\nFinally, key issues and future questions regarding deep learning applied to\naudio signal processing are identified.\n",
        "published": "2019",
        "authors": [
            "Hendrik Purwins",
            "Bo Li",
            "Tuomas Virtanen",
            "Jan Schl\u00fcter",
            "Shuo-yiin Chang",
            "Tara Sainath"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.11188v1",
        "title": "Integrating Deep Learning in Domain Sciences at Exascale",
        "abstract": "  This paper presents some of the current challenges in designing deep learning\nartificial intelligence (AI) and integrating it with traditional\nhigh-performance computing (HPC) simulations. We evaluate existing packages for\ntheir ability to run deep learning models and applications on large-scale HPC\nsystems efficiently, identify challenges, and propose new asynchronous\nparallelization and optimization techniques for current large-scale\nheterogeneous systems and upcoming exascale systems. These developments, along\nwith existing HPC AI software capabilities, have been integrated into MagmaDNN,\nan open-source HPC deep learning framework. Many deep learning frameworks are\ntargeted at data scientists and fall short in providing quality integration\ninto existing HPC workflows. This paper discusses the necessities of an HPC\ndeep learning framework and how those needs can be provided (e.g., as in\nMagmaDNN) through a deep integration with existing HPC libraries, such as MAGMA\nand its modular memory management, MPI, CuBLAS, CuDNN, MKL, and HIP.\nAdvancements are also illustrated through the use of algorithmic enhancements\nin reduced- and mixed-precision, as well as asynchronous optimization methods.\nFinally, we present illustrations and potential solutions for enhancing\ntraditional compute- and data-intensive applications at ORNL and UTK with AI.\nThe approaches and future challenges are illustrated in materials science,\nimaging, and climate applications.\n",
        "published": "2020",
        "authors": [
            "Rick Archibald",
            "Edmond Chow",
            "Eduardo D'Azevedo",
            "Jack Dongarra",
            "Markus Eisenbach",
            "Rocco Febbo",
            "Florent Lopez",
            "Daniel Nichols",
            "Stanimire Tomov",
            "Kwai Wong",
            "Junqi Yin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2112.03411v1",
        "title": "Extrapolation Frameworks in Cognitive Psychology Suitable for Study of\n  Image Classification Models",
        "abstract": "  We study the functional task of deep learning image classification models and\nshow that image classification requires extrapolation capabilities. This\nsuggests that new theories have to be developed for the understanding of deep\nlearning as the current theory assumes models are solely interpolating, leaving\nmany questions about them unanswered. We investigate the pixel space and also\nthe feature spaces extracted from images by trained models (in their hidden\nlayers, including the 64-dimensional feature space in the last hidden layer of\npre-trained residual neural networks), and also the feature space extracted by\nwavelets/shearlets. In all these domains, testing samples considerably fall\noutside the convex hull of training sets, and image classification requires\nextrapolation. In contrast to the deep learning literature, in cognitive\nscience, psychology, and neuroscience, extrapolation and learning are often\nstudied in tandem. Moreover, many aspects of human visual cognition and\nbehavior are reported to involve extrapolation. We propose a novel\nextrapolation framework for the mathematical study of deep learning models. In\nour framework, we use the term extrapolation in this specific way of\nextrapolating outside the convex hull of training set (in the pixel space or\nfeature space) but within the specific scope defined by the training data, the\nsame way extrapolation is defined in many studies in cognitive science. We\nexplain that our extrapolation framework can provide novel answers to open\nresearch problems about deep learning including their over-parameterization,\ntheir training regime, out-of-distribution detection, etc. We also see that the\nextent of extrapolation is negligible in learning tasks where deep learning is\nreported to have no advantage over simple models.\n",
        "published": "2021",
        "authors": [
            "Roozbeh Yousefzadeh",
            "Jessica A. Mollick"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.03210v1",
        "title": "Atlas-powered deep learning (ADL) -- application to diffusion weighted\n  MRI",
        "abstract": "  Deep learning has a great potential for estimating biomarkers in diffusion\nweighted magnetic resonance imaging (dMRI). Atlases, on the other hand, are a\nunique tool for modeling the spatio-temporal variability of biomarkers. In this\npaper, we propose the first framework to exploit both deep learning and atlases\nfor biomarker estimation in dMRI. Our framework relies on non-linear diffusion\ntensor registration to compute biomarker atlases and to estimate atlas\nreliability maps. We also use nonlinear tensor registration to align the atlas\nto a subject and to estimate the error of this alignment. We use the biomarker\natlas, atlas reliability map, and alignment error map, in addition to the dMRI\nsignal, as inputs to a deep learning model for biomarker estimation. We use our\nframework to estimate fractional anisotropy and neurite orientation dispersion\nfrom down-sampled dMRI data on a test cohort of 70 newborn subjects. Results\nshow that our method significantly outperforms standard estimation methods as\nwell as recent deep learning techniques. Our method is also more robust to\nstronger measurement down-sampling factors. Our study shows that the advantages\nof deep learning and atlases can be synergistically combined to achieve\nunprecedented accuracy in biomarker estimation from dMRI data.\n",
        "published": "2022",
        "authors": [
            "Davood Karimi",
            "Ali Gholipour"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.10498v1",
        "title": "Design Automation for Fast, Lightweight, and Effective Deep Learning\n  Models: A Survey",
        "abstract": "  Deep learning technologies have demonstrated remarkable effectiveness in a\nwide range of tasks, and deep learning holds the potential to advance a\nmultitude of applications, including in edge computing, where deep models are\ndeployed on edge devices to enable instant data processing and response. A key\nchallenge is that while the application of deep models often incurs substantial\nmemory and computational costs, edge devices typically offer only very limited\nstorage and computational capabilities that may vary substantially across\ndevices. These characteristics make it difficult to build deep learning\nsolutions that unleash the potential of edge devices while complying with their\nconstraints. A promising approach to addressing this challenge is to automate\nthe design of effective deep learning models that are lightweight, require only\na little storage, and incur only low computational overheads. This survey\noffers comprehensive coverage of studies of design automation techniques for\ndeep learning models targeting edge computing. It offers an overview and\ncomparison of key metrics that are used commonly to quantify the proficiency of\nmodels in terms of effectiveness, lightness, and computational costs. The\nsurvey then proceeds to cover three categories of the state-of-the-art of deep\nmodel design automation techniques: automated neural architecture search,\nautomated model compression, and joint automated design and compression.\nFinally, the survey covers open issues and directions for future research.\n",
        "published": "2022",
        "authors": [
            "Dalin Zhang",
            "Kaixuan Chen",
            "Yan Zhao",
            "Bin Yang",
            "Lina Yao",
            "Christian S. Jensen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.11228v3",
        "title": "Why Deep Learning's Performance Data Are Misleading",
        "abstract": "  This is a theoretical paper, as a companion paper of the keynote talk at the\nsame conference AIEE 2023. In contrast to conscious learning, many projects in\nAI have employed so-called \"deep learning\" many of which seemed to give\nimpressive performance. This paper explains that such performance data are\ndeceptively inflated due to two misconducts: \"data deletion\" and \"test on\ntraining set\". This paper clarifies \"data deletion\" and \"test on training set\"\nin deep learning and why they are misconducts. A simple classification method\nis defined, called Nearest Neighbor With Threshold (NNWT). A theorem is\nestablished that the NNWT method reaches a zero error on any validation set and\nany test set using the two misconducts, as long as the test set is in the\npossession of the author and both the amount of storage space and the time of\ntraining are finite but unbounded like with many deep learning methods.\nHowever, many deep learning methods, like the NNWT method, are all not\ngeneralizable since they have never been tested by a true test set. Why? The\nso-called \"test set\" was used in the Post-Selection step of the training stage.\nThe evidence that misconducts actually took place in many deep learning\nprojects is beyond the scope of this paper.\n",
        "published": "2022",
        "authors": [
            "Juyang Weng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.05677v1",
        "title": "Application of Deep Learning on Single-Cell RNA-sequencing Data\n  Analysis: A Review",
        "abstract": "  Single-cell RNA-sequencing (scRNA-seq) has become a routinely used technique\nto quantify the gene expression profile of thousands of single cells\nsimultaneously. Analysis of scRNA-seq data plays an important role in the study\nof cell states and phenotypes, and has helped elucidate biological processes,\nsuch as those occurring during development of complex organisms and improved\nour understanding of disease states, such as cancer, diabetes, and COVID, among\nothers. Deep learning, a recent advance of artificial intelligence that has\nbeen used to address many problems involving large datasets, has also emerged\nas a promising tool for scRNA-seq data analysis, as it has a capacity to\nextract informative, compact features from noisy, heterogeneous, and\nhigh-dimensional scRNA-seq data to improve downstream analysis. The present\nreview aims at surveying recently developed deep learning techniques in\nscRNA-seq data analysis, identifying key steps within the scRNA-seq data\nanalysis pipeline that have been advanced by deep learning, and explaining the\nbenefits of deep learning over more conventional analysis tools. Finally, we\nsummarize the challenges in current deep learning approaches faced within\nscRNA-seq data and discuss potential directions for improvements in deep\nalgorithms for scRNA-seq data analysis.\n",
        "published": "2022",
        "authors": [
            "Matthew Brendel",
            "Chang Su",
            "Zilong Bai",
            "Hao Zhang",
            "Olivier Elemento",
            "Fei Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.12002v1",
        "title": "Explainability of Traditional and Deep Learning Models on Longitudinal\n  Healthcare Records",
        "abstract": "  Recent advances in deep learning have led to interest in training deep\nlearning models on longitudinal healthcare records to predict a range of\nmedical events, with models demonstrating high predictive performance.\nPredictive performance is necessary but insufficient, however, with\nexplanations and reasoning from models required to convince clinicians for\nsustained use. Rigorous evaluation of explainability is often missing, as\ncomparisons between models (traditional versus deep) and various explainability\nmethods have not been well-studied. Furthermore, ground truths needed to\nevaluate explainability can be highly subjective depending on the clinician's\nperspective. Our work is one of the first to evaluate explainability\nperformance between and within traditional (XGBoost) and deep learning (LSTM\nwith Attention) models on both a global and individual per-prediction level on\nlongitudinal healthcare data. We compared explainability using three popular\nmethods: 1) SHapley Additive exPlanations (SHAP), 2) Layer-Wise Relevance\nPropagation (LRP), and 3) Attention. These implementations were applied on\nsynthetically generated datasets with designed ground-truths and a real-world\nmedicare claims dataset. We showed that overall, LSTMs with SHAP or LRP\nprovides superior explainability compared to XGBoost on both the global and\nlocal level, while LSTM with dot-product attention failed to produce reasonable\nones. With the explosion of the volume of healthcare data and deep learning\nprogress, the need to evaluate explainability will be pivotal towards\nsuccessful adoption of deep learning models in healthcare settings.\n",
        "published": "2022",
        "authors": [
            "Lin Lee Cheong",
            "Tesfagabir Meharizghi",
            "Wynona Black",
            "Yang Guang",
            "Weilin Meng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.10663v2",
        "title": "Transfer Learning in Deep Learning Models for Building Load Forecasting:\n  Case of Limited Data",
        "abstract": "  Precise load forecasting in buildings could increase the bill savings\npotential and facilitate optimized strategies for power generation planning.\nWith the rapid evolution of computer science, data-driven techniques, in\nparticular the Deep Learning models, have become a promising solution for the\nload forecasting problem. These models have showed accurate forecasting\nresults; however, they need abundance amount of historical data to maintain the\nperformance. Considering the new buildings and buildings with low resolution\nmeasuring equipment, it is difficult to get enough historical data from them,\nleading to poor forecasting performance. In order to adapt Deep Learning models\nfor buildings with limited and scarce data, this paper proposes a\nBuilding-to-Building Transfer Learning framework to overcome the problem and\nenhance the performance of Deep Learning models. The transfer learning approach\nwas applied to a new technique known as Transformer model due to its efficacy\nin capturing data trends. The performance of the algorithm was tested on a\nlarge commercial building with limited data. The result showed that the\nproposed approach improved the forecasting accuracy by 56.8% compared to the\ncase of conventional deep learning where training from scratch is used. The\npaper also compared the proposed Transformer model to other sequential deep\nlearning models such as Long-short Term Memory (LSTM) and Recurrent Neural\nNetwork (RNN). The accuracy of the transformer model outperformed other models\nby reducing the root mean square error to 0.009, compared to LSTM with 0.011\nand RNN with 0.051.\n",
        "published": "2023",
        "authors": [
            "Menna Nawar",
            "Moustafa Shomer",
            "Samy Faddel",
            "Huangjie Gong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.05665v1",
        "title": "Exploring Deep Learning Approaches to Predict Person and Vehicle Trips:\n  An Analysis of NHTS Data",
        "abstract": "  Modern transportation planning relies heavily on accurate predictions of\nperson and vehicle trips. However, traditional planning models often fail to\naccount for the intricacies and dynamics of travel behavior, leading to\nless-than-optimal accuracy in these predictions. This study explores the\npotential of deep learning techniques to transform the way we approach trip\npredictions, and ultimately, transportation planning. Utilizing a comprehensive\ndataset from the National Household Travel Survey (NHTS), we developed and\ntrained a deep learning model for predicting person and vehicle trips. The\nproposed model leverages the vast amount of information in the NHTS data,\ncapturing complex, non-linear relationships that were previously overlooked by\ntraditional models. As a result, our deep learning model achieved an impressive\naccuracy of 98% for person trip prediction and 96% for vehicle trip estimation.\nThis represents a significant improvement over the performances of traditional\ntransportation planning models, thereby demonstrating the power of deep\nlearning in this domain. The implications of this study extend beyond just more\naccurate predictions. By enhancing the accuracy and reliability of trip\nprediction models, planners can formulate more effective, data-driven\ntransportation policies, infrastructure, and services. As such, our research\nunderscores the need for the transportation planning field to embrace advanced\ntechniques like deep learning. The detailed methodology, along with a thorough\ndiscussion of the results and their implications, are presented in the\nsubsequent sections of this paper.\n",
        "published": "2023",
        "authors": [
            "Kojo Adu-Gyamfi",
            "Sharma Anuj"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.04686v1",
        "title": "Machine Learning in Artificial Intelligence: Towards a Common\n  Understanding",
        "abstract": "  The application of \"machine learning\" and \"artificial intelligence\" has\nbecome popular within the last decade. Both terms are frequently used in\nscience and media, sometimes interchangeably, sometimes with different\nmeanings. In this work, we aim to clarify the relationship between these terms\nand, in particular, to specify the contribution of machine learning to\nartificial intelligence. We review relevant literature and present a conceptual\nframework which clarifies the role of machine learning to build (artificial)\nintelligent agents. Hence, we seek to provide more terminological clarity and a\nstarting point for (interdisciplinary) discussions and future research.\n",
        "published": "2020",
        "authors": [
            "Niklas K\u00fchl",
            "Marc Goutier",
            "Robin Hirt",
            "Gerhard Satzger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.03759v1",
        "title": "Representation Learning for Appliance Recognition: A Comparison to\n  Classical Machine Learning",
        "abstract": "  Non-intrusive load monitoring (NILM) aims at energy consumption and appliance\nstate information retrieval from aggregated consumption measurements, with the\nhelp of signal processing and machine learning algorithms. Representation\nlearning with deep neural networks is successfully applied to several related\ndisciplines. The main advantage of representation learning lies in replacing an\nexpert-driven, hand-crafted feature extraction with hierarchical learning from\nmany representations in raw data format. In this paper, we show how the NILM\nprocessing-chain can be improved, reduced in complexity and alternatively\ndesigned with recent deep learning algorithms. On the basis of an event-based\nappliance recognition approach, we evaluate seven different classification\nmodels: a classical machine learning approach that is based on a hand-crafted\nfeature extraction, three different deep neural network architectures for\nautomated feature extraction on raw waveform data, as well as three baseline\napproaches for raw data processing. We evaluate all approaches on two\nlarge-scale energy consumption datasets with more than 50,000 events of 44\nappliances. We show that with the use of deep learning, we are able to reach\nand surpass the performance of the state-of-the-art classical machine learning\napproach for appliance recognition with an F-Score of 0.75 and 0.86 compared to\n0.69 and 0.87 of the classical approach.\n",
        "published": "2022",
        "authors": [
            "Matthias Kahl",
            "Daniel Jorde",
            "Hans-Arno Jacobsen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.03086v1",
        "title": "Deep Learning in Computational Biology: Advancements, Challenges, and\n  Future Outlook",
        "abstract": "  Deep learning has become a powerful tool in computational biology,\nrevolutionising the analysis and interpretation of biological data over time.\nIn our article review, we delve into various aspects of deep learning in\ncomputational biology. Specifically, we examine its history, advantages, and\nchallenges. Our focus is on two primary applications: DNA sequence\nclassification and prediction, as well as protein structure prediction from\nsequence data. Additionally, we provide insights into the outlook for this\nfield. To fully harness the potential of deep learning in computational\nbiology, it is crucial to address the challenges that come with it. These\nchallenges include the requirement for large, labelled datasets and the\ninterpretability of deep learning models. The use of deep learning in the\nanalysis of DNA sequences has brought about a significant transformation in the\ndetection of genomic variants and the analysis of gene expression. This has\ngreatly contributed to the advancement of personalised medicine and drug\ndiscovery. Convolutional neural networks (CNNs) have been shown to be highly\naccurate in predicting genetic variations and gene expression levels. Deep\nlearning techniques are used for analysing epigenetic data, including DNA\nmethylation and histone modifications. This provides valuable insights into\nmetabolic conditions and gene regulation. The field of protein structure\nprediction has been significantly impacted by deep learning, which has enabled\naccurate determination of the three-dimensional shape of proteins and\nprediction of their interactions. The future of deep learning in computational\nbiology looks promising. With the development of advanced deep learning models\nand interpretation techniques, there is potential to overcome current\nchallenges and further our understanding of biological systems.\n",
        "published": "2023",
        "authors": [
            "Suresh Kumar",
            "Dhanyashri Guruparan",
            "Pavithren Aaron",
            "Philemon Telajan",
            "Kavinesh Mahadevan",
            "Dinesh Davagandhi",
            "Ong Xin Yue"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1509.08731v1",
        "title": "Variational Information Maximisation for Intrinsically Motivated\n  Reinforcement Learning",
        "abstract": "  The mutual information is a core statistical quantity that has applications\nin all areas of machine learning, whether this is in training of density models\nover multiple data modalities, in maximising the efficiency of noisy\ntransmission channels, or when learning behaviour policies for exploration by\nartificial agents. Most learning algorithms that involve optimisation of the\nmutual information rely on the Blahut-Arimoto algorithm --- an enumerative\nalgorithm with exponential complexity that is not suitable for modern machine\nlearning applications. This paper provides a new approach for scalable\noptimisation of the mutual information by merging techniques from variational\ninference and deep learning. We develop our approach by focusing on the problem\nof intrinsically-motivated learning, where the mutual information forms the\ndefinition of a well-known internal drive known as empowerment. Using a\nvariational lower bound on the mutual information, combined with convolutional\nnetworks for handling visual input streams, we develop a stochastic\noptimisation algorithm that allows for scalable information maximisation and\nempowerment-based reasoning directly from pixels to actions.\n",
        "published": "2015",
        "authors": [
            "Shakir Mohamed",
            "Danilo Jimenez Rezende"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.01258v1",
        "title": "Agreement-based Learning",
        "abstract": "  Model selection is a problem that has occupied machine learning researchers\nfor a long time. Recently, its importance has become evident through\napplications in deep learning. We propose an agreement-based learning framework\nthat prevents many of the pitfalls associated with model selection. It relies\non coupling the training of multiple models by encouraging them to agree on\ntheir predictions while training. In contrast with other model selection and\ncombination approaches used in machine learning, the proposed framework is\ninspired by human learning. We also propose a learning algorithm defined within\nthis framework which manages to significantly outperform alternatives in\npractice, and whose performance improves further with the availability of\nunlabeled data. Finally, we describe a number of potential directions for\ndeveloping more flexible agreement-based learning algorithms.\n",
        "published": "2018",
        "authors": [
            "Emmanouil Antonios Platanios"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.03002v1",
        "title": "Domain Adaptive Generation of Aircraft on Satellite Imagery via\n  Simulated and Unsupervised Learning",
        "abstract": "  Object detection and classification for aircraft are the most important tasks\nin the satellite image analysis. The success of modern detection and\nclassification methods has been based on machine learning and deep learning.\nOne of the key requirements for those learning processes is huge data to train.\nHowever, there is an insufficient portion of aircraft since the targets are on\nmilitary action and oper- ation. Considering the characteristics of satellite\nimagery, this paper attempts to provide a framework of the simulated and\nunsupervised methodology without any additional su- pervision or physical\nassumptions. Finally, the qualitative and quantitative analysis revealed a\npotential to replenish insufficient data for machine learning platform for\nsatellite image analysis.\n",
        "published": "2018",
        "authors": [
            "Junghoon Seo",
            "Seunghyun Jeon",
            "Taegyun Jeon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.06052v1",
        "title": "Human-like machine learning: limitations and suggestions",
        "abstract": "  This paper attempts to address the issues of machine learning in its current\nimplementation. It is known that machine learning algorithms require a\nsignificant amount of data for training purposes, whereas recent developments\nin deep learning have increased this requirement dramatically. The performance\nof an algorithm depends on the quality of data and hence, algorithms are as\ngood as the data they are trained on. Supervised learning is developed based on\nhuman learning processes by analysing named (i.e. annotated) objects, scenes\nand actions. Whether training on large quantities of data (i.e. big data) is\nthe right or the wrong approach, is debatable. The fact is, that training\nalgorithms the same way we learn ourselves, comes with limitations. This paper\ndiscusses the issues around applying a human-like approach to train algorithms\nand the implications of this approach when using limited data. Several current\nstudies involving non-data-driven algorithms and natural examples are also\ndiscussed and certain alternative approaches are suggested.\n",
        "published": "2018",
        "authors": [
            "Georgios Mastorakis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.12560v2",
        "title": "An Introduction to Deep Reinforcement Learning",
        "abstract": "  Deep reinforcement learning is the combination of reinforcement learning (RL)\nand deep learning. This field of research has been able to solve a wide range\nof complex decision-making tasks that were previously out of reach for a\nmachine. Thus, deep RL opens up many new applications in domains such as\nhealthcare, robotics, smart grids, finance, and many more. This manuscript\nprovides an introduction to deep reinforcement learning models, algorithms and\ntechniques. Particular focus is on the aspects related to generalization and\nhow deep RL can be used for practical applications. We assume the reader is\nfamiliar with basic machine learning concepts.\n",
        "published": "2018",
        "authors": [
            "Vincent Francois-Lavet",
            "Peter Henderson",
            "Riashat Islam",
            "Marc G. Bellemare",
            "Joelle Pineau"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.11184v1",
        "title": "HEIDL: Learning Linguistic Expressions with Deep Learning and\n  Human-in-the-Loop",
        "abstract": "  While the role of humans is increasingly recognized in machine learning\ncommunity, representation of and interaction with models in current\nhuman-in-the-loop machine learning (HITL-ML) approaches are too low-level and\nfar-removed from human's conceptual models. We demonstrate HEIDL, a prototype\nHITL-ML system that exposes the machine-learned model through high-level,\nexplainable linguistic expressions formed of predicates representing semantic\nstructure of text. In HEIDL, human's role is elevated from simply evaluating\nmodel predictions to interpreting and even updating the model logic directly by\nenabling interaction with rule predicates themselves. Raising the currency of\ninteraction to such semantic levels calls for new interaction paradigms between\nhumans and machines that result in improved productivity for text analytics\nmodel development process. Moreover, by involving humans in the process, the\nhuman-machine co-created models generalize better to unseen data as domain\nexperts are able to instill their expertise by extrapolating from what has been\nlearned by automated algorithms from few labelled data.\n",
        "published": "2019",
        "authors": [
            "Yiwei Yang",
            "Eser Kandogan",
            "Yunyao Li",
            "Walter S. Lasecki",
            "Prithviraj Sen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.03905v2",
        "title": "ChainerRL: A Deep Reinforcement Learning Library",
        "abstract": "  In this paper, we introduce ChainerRL, an open-source deep reinforcement\nlearning (DRL) library built using Python and the Chainer deep learning\nframework. ChainerRL implements a comprehensive set of DRL algorithms and\ntechniques drawn from state-of-the-art research in the field. To foster\nreproducible research, and for instructional purposes, ChainerRL provides\nscripts that closely replicate the original papers' experimental settings and\nreproduce published benchmark results for several algorithms. Lastly, ChainerRL\noffers a visualization tool that enables the qualitative inspection of trained\nagents. The ChainerRL source code can be found on GitHub:\nhttps://github.com/chainer/chainerrl.\n",
        "published": "2019",
        "authors": [
            "Yasuhiro Fujita",
            "Prabhat Nagarajan",
            "Toshiki Kataoka",
            "Takahiro Ishikawa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.00753v2",
        "title": "Opportunities and Challenges in Deep Learning Adversarial Robustness: A\n  Survey",
        "abstract": "  As we seek to deploy machine learning models beyond virtual and controlled\ndomains, it is critical to analyze not only the accuracy or the fact that it\nworks most of the time, but if such a model is truly robust and reliable. This\npaper studies strategies to implement adversary robustly trained algorithms\ntowards guaranteeing safety in machine learning algorithms. We provide a\ntaxonomy to classify adversarial attacks and defenses, formulate the Robust\nOptimization problem in a min-max setting and divide it into 3 subcategories,\nnamely: Adversarial (re)Training, Regularization Approach, and Certified\nDefenses. We survey the most recent and important results in adversarial\nexample generation, defense mechanisms with adversarial (re)Training as their\nmain defense against perturbations. We also survey mothods that add\nregularization terms that change the behavior of the gradient, making it harder\nfor attackers to achieve their objective. Alternatively, we've surveyed methods\nwhich formally derive certificates of robustness by exactly solving the\noptimization problem or by approximations using upper or lower bounds. In\naddition, we discuss the challenges faced by most of the recent algorithms\npresenting future research perspectives.\n",
        "published": "2020",
        "authors": [
            "Samuel Henrique Silva",
            "Peyman Najafirad"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.06493v1",
        "title": "FLHub: a Federated Learning model sharing service",
        "abstract": "  As easy-to-use deep learning libraries such as Tensorflow and Pytorch are\npopular, it has become convenient to develop machine learning models. Due to\nprivacy issues with centralized machine learning, recently, federated learning\nin the distributed computing framework is attracting attention. The central\nserver does not collect sensitive and personal data from clients in federated\nlearning, but it only aggregates the model parameters. Though federated\nlearning helps protect privacy, it is difficult for machine learning developers\nto share the models that they could utilize for different-domain applications.\nIn this paper, we propose a federated learning model sharing service named\nFederated Learning Hub (FLHub). Users can upload, download, and contribute the\nmodel developed by other developers similarly to GitHub. We demonstrate that a\nforked model can finish training faster than the existing model and that\nlearning progressed more quickly for each federated round.\n",
        "published": "2022",
        "authors": [
            "Hyunsu Mun",
            "Youngseok Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1611.07567v1",
        "title": "Feature Importance Measure for Non-linear Learning Algorithms",
        "abstract": "  Complex problems may require sophisticated, non-linear learning methods such\nas kernel machines or deep neural networks to achieve state of the art\nprediction accuracies. However, high prediction accuracies are not the only\nobjective to consider when solving problems using machine learning. Instead,\nparticular scientific applications require some explanation of the learned\nprediction function. Unfortunately, most methods do not come with out of the\nbox straight forward interpretation. Even linear prediction functions are not\nstraight forward to explain if features exhibit complex correlation structure.\n  In this paper, we propose the Measure of Feature Importance (MFI). MFI is\ngeneral and can be applied to any arbitrary learning machine (including kernel\nmachines and deep learning). MFI is intrinsically non-linear and can detect\nfeatures that by itself are inconspicuous and only impact the prediction\nfunction through their interaction with other features. Lastly, MFI can be used\nfor both --- model-based feature importance and instance-based feature\nimportance (i.e, measuring the importance of a feature for a particular data\npoint).\n",
        "published": "2016",
        "authors": [
            "Marina M. -C. Vidovic",
            "Nico G\u00f6rnitz",
            "Klaus-Robert M\u00fcller",
            "Marius Kloft"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.08744v2",
        "title": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales",
        "abstract": "  We introduce Microsoft Machine Learning for Apache Spark (MMLSpark), an\necosystem of enhancements that expand the Apache Spark distributed computing\nlibrary to tackle problems in Deep Learning, Micro-Service Orchestration,\nGradient Boosting, Model Interpretability, and other areas of modern\ncomputation. Furthermore, we present a novel system called Spark Serving that\nallows users to run any Apache Spark program as a distributed, sub-millisecond\nlatency web service backed by their existing Spark Cluster. All MMLSpark\ncontributions have the same API to enable simple composition across frameworks\nand usage across batch, streaming, and RESTful web serving scenarios on static,\nelastic, or serverless clusters. We showcase MMLSpark by creating a method for\ndeep object detection capable of learning without human labeled data and\ndemonstrate its effectiveness for Snow Leopard conservation.\n",
        "published": "2018",
        "authors": [
            "Mark Hamilton",
            "Sudarshan Raghunathan",
            "Ilya Matiach",
            "Andrew Schonhoffer",
            "Anand Raman",
            "Eli Barzilay",
            "Karthik Rajendran",
            "Dalitso Banda",
            "Casey Jisoo Hong",
            "Manon Knoertzer",
            "Ben Brodsky",
            "Minsoo Thigpen",
            "Janhavi Suresh Mahajan",
            "Courtney Cochrane",
            "Abhiram Eswaran",
            "Ari Green"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.09165v1",
        "title": "A framework for the extraction of Deep Neural Networks by leveraging\n  public data",
        "abstract": "  Machine learning models trained on confidential datasets are increasingly\nbeing deployed for profit. Machine Learning as a Service (MLaaS) has made such\nmodels easily accessible to end-users. Prior work has developed model\nextraction attacks, in which an adversary extracts an approximation of MLaaS\nmodels by making black-box queries to it. However, none of these works is able\nto satisfy all the three essential criteria for practical model extraction: (1)\nthe ability to work on deep learning models, (2) the non-requirement of domain\nknowledge and (3) the ability to work with a limited query budget. We design a\nmodel extraction framework that makes use of active learning and large public\ndatasets to satisfy them. We demonstrate that it is possible to use this\nframework to steal deep classifiers trained on a variety of datasets from image\nand text domains. By querying a model via black-box access for its top\nprediction, our framework improves performance on an average over a uniform\nnoise baseline by 4.70x for image tasks and 2.11x for text tasks respectively,\nwhile using only 30% (30,000 samples) of the public dataset at its disposal.\n",
        "published": "2019",
        "authors": [
            "Soham Pal",
            "Yash Gupta",
            "Aditya Shukla",
            "Aditya Kanade",
            "Shirish Shevade",
            "Vinod Ganapathy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.00872v1",
        "title": "How deep the machine learning can be",
        "abstract": "  Today we live in the age of artificial intelligence and machine learning;\nfrom small startups to HW or SW giants, everyone wants to build machine\nintelligence chips, applications. The task, however, is hard: not only because\nof the size of the problem: the technology one can utilize (and the paradigm it\nis based upon) strongly degrades the chances to succeed efficiently. Today the\nsingle-processor performance practically reached the limits the laws of nature\nenable. The only feasible way to achieve the needed high computing performance\nseems to be parallelizing many sequentially working units. The laws of the\n(massively) parallelized computing, however, are different from those\nexperienced in connection with assembling and utilizing systems comprising\njust-a-few single processors. As machine learning is mostly based on the\nconventional computing (processors), we scrutinize the (known, but somewhat\nfaded) laws of the parallel computing, concerning AI. This paper attempts to\nreview some of the caveats, especially concerning scaling the computing\nperformance of the AI solutions.\n",
        "published": "2020",
        "authors": [
            "J\u00e1nos V\u00e9gh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.13885v1",
        "title": "Hyperbolic Manifold Regression",
        "abstract": "  Geometric representation learning has recently shown great promise in several\nmachine learning settings, ranging from relational learning to language\nprocessing and generative models. In this work, we consider the problem of\nperforming manifold-valued regression onto an hyperbolic space as an\nintermediate component for a number of relevant machine learning applications.\nIn particular, by formulating the problem of predicting nodes of a tree as a\nmanifold regression task in the hyperbolic space, we propose a novel\nperspective on two challenging tasks: 1) hierarchical classification via label\nembeddings and 2) taxonomy extension of hyperbolic representations. To address\nthe regression problem we consider previous methods as well as proposing two\nnovel approaches that are computationally more advantageous: a parametric deep\nlearning model that is informed by the geodesics of the target space and a\nnon-parametric kernel-method for which we also prove excess risk bounds. Our\nexperiments show that the strategy of leveraging the hyperbolic geometry is\npromising. In particular, in the taxonomy expansion setting, we find that the\nhyperbolic-based estimators significantly outperform methods performing\nregression in the ambient Euclidean space.\n",
        "published": "2020",
        "authors": [
            "Gian Maria Marconi",
            "Lorenzo Rosasco",
            "Carlo Ciliberto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.00853v2",
        "title": "Sea Ice Forecasting using Attention-based Ensemble LSTM",
        "abstract": "  Accurately forecasting Arctic sea ice from subseasonal to seasonal scales has\nbeen a major scientific effort with fundamental challenges at play. In addition\nto physics-based earth system models, researchers have been applying multiple\nstatistical and machine learning models for sea ice forecasting. Looking at the\npotential of data-driven sea ice forecasting, we propose an attention-based\nLong Short Term Memory (LSTM) ensemble method to predict monthly sea ice extent\nup to 1 month ahead. Using daily and monthly satellite retrieved sea ice data\nfrom NSIDC and atmospheric and oceanic variables from ERA5 reanalysis product\nfor 39 years, we show that our multi-temporal ensemble method outperforms\nseveral baseline and recently proposed deep learning models. This will\nsubstantially improve our ability in predicting future Arctic sea ice changes,\nwhich is fundamental for forecasting transporting routes, resource development,\ncoastal erosion, threats to Arctic coastal communities and wildlife.\n",
        "published": "2021",
        "authors": [
            "Sahara Ali",
            "Yiyi Huang",
            "Xin Huang",
            "Jianwu Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.08345v1",
        "title": "Learning Enhanced Optimisation for Routing Problems",
        "abstract": "  Deep learning approaches have shown promising results in solving routing\nproblems. However, there is still a substantial gap in solution quality between\nmachine learning and operations research algorithms. Recently, another line of\nresearch has been introduced that fuses the strengths of machine learning and\noperational research algorithms. In particular, search perturbation operators\nhave been used to improve the solution. Nevertheless, using the perturbation\nmay not guarantee a quality solution. This paper presents \"Learning to Guide\nLocal Search\" (L2GLS), a learning-based approach for routing problems that uses\na penalty term and reinforcement learning to adaptively adjust search efforts.\nL2GLS combines local search (LS) operators' strengths with penalty terms to\nescape local optimals. Routing problems have many practical applications, often\npresetting larger instances that are still challenging for many existing\nalgorithms introduced in the learning to optimise field. We show that L2GLS\nachieves the new state-of-the-art results on larger TSP and CVRP over other\nmachine learning methods.\n",
        "published": "2021",
        "authors": [
            "Nasrin Sultana",
            "Jeffrey Chan",
            "Tabinda Sarwar",
            "Babak Abbasi",
            "A. K. Qin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.12529v2",
        "title": "A Deep Learning Approach to Probabilistic Forecasting of Weather",
        "abstract": "  We discuss an approach to probabilistic forecasting based on two chained\nmachine-learning steps: a dimensional reduction step that learns a reduction\nmap of predictor information to a low-dimensional space in a manner designed to\npreserve information about forecast quantities; and a density estimation step\nthat uses the probabilistic machine learning technique of normalizing flows to\ncompute the joint probability density of reduced predictors and forecast\nquantities. This joint density is then renormalized to produce the conditional\nforecast distribution. In this method, probabilistic calibration testing plays\nthe role of a regularization procedure, preventing overfitting in the second\nstep, while effective dimensional reduction from the first step is the source\nof forecast sharpness. We verify the method using a 22-year 1-hour cadence time\nseries of Weather Research and Forecasting (WRF) simulation data of surface\nwind on a grid.\n",
        "published": "2022",
        "authors": [
            "Nick Rittler",
            "Carlo Graziani",
            "Jiali Wang",
            "Rao Kotamarthi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.13535v1",
        "title": "Machine Learning for Violence Risk Assessment Using Dutch Clinical Notes",
        "abstract": "  Violence risk assessment in psychiatric institutions enables interventions to\navoid violence incidents. Clinical notes written by practitioners and available\nin electronic health records are valuable resources capturing unique\ninformation, but are seldom used to their full potential. We explore\nconventional and deep machine learning methods to assess violence risk in\npsychiatric patients using practitioner notes. The performance of our best\nmodels is comparable to the currently used questionnaire-based method, with an\narea under the Receiver Operating Characteristic curve of approximately 0.8. We\nfind that the deep-learning model BERTje performs worse than conventional\nmachine learning methods. We also evaluate our data and our classifiers to\nunderstand the performance of our models better. This is particularly important\nfor the applicability of evaluated classifiers to new data, and is also of\ngreat interest to practitioners, due to the increased availability of new data\nin electronic format.\n",
        "published": "2022",
        "authors": [
            "Pablo Mosteiro",
            "Emil Rijcken",
            "Kalliopi Zervanou",
            "Uzay Kaymak",
            "Floortje Scheepers",
            "Marco Spruit"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.13572v2",
        "title": "Membership Inference Attacks via Adversarial Examples",
        "abstract": "  The raise of machine learning and deep learning led to significant\nimprovement in several domains. This change is supported by both the dramatic\nrise in computation power and the collection of large datasets. Such massive\ndatasets often include personal data which can represent a threat to privacy.\nMembership inference attacks are a novel direction of research which aims at\nrecovering training data used by a learning algorithm. In this paper, we\ndevelop a mean to measure the leakage of training data leveraging a quantity\nappearing as a proxy of the total variation of a trained model near its\ntraining samples. We extend our work by providing a novel defense mechanism.\nOur contributions are supported by empirical evidence through convincing\nnumerical experiments.\n",
        "published": "2022",
        "authors": [
            "Hamid Jalalzai",
            "Elie Kadoche",
            "R\u00e9mi Leluc",
            "Vincent Plassier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.11080v2",
        "title": "SurvSHAP(t): Time-dependent explanations of machine learning survival\n  models",
        "abstract": "  Machine and deep learning survival models demonstrate similar or even\nimproved time-to-event prediction capabilities compared to classical\nstatistical learning methods yet are too complex to be interpreted by humans.\nSeveral model-agnostic explanations are available to overcome this issue;\nhowever, none directly explain the survival function prediction. In this paper,\nwe introduce SurvSHAP(t), the first time-dependent explanation that allows for\ninterpreting survival black-box models. It is based on SHapley Additive\nexPlanations with solid theoretical foundations and a broad adoption among\nmachine learning practitioners. The proposed methods aim to enhance precision\ndiagnostics and support domain experts in making decisions. Experiments on\nsynthetic and medical data confirm that SurvSHAP(t) can detect variables with a\ntime-dependent effect, and its aggregation is a better determinant of the\nimportance of variables for a prediction than SurvLIME. SurvSHAP(t) is\nmodel-agnostic and can be applied to all models with functional output. We\nprovide an accessible implementation of time-dependent explanations in Python\nat http://github.com/MI2DataLab/survshap.\n",
        "published": "2022",
        "authors": [
            "Mateusz Krzyzi\u0144ski",
            "Miko\u0142aj Spytek",
            "Hubert Baniecki",
            "Przemys\u0142aw Biecek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.08119v1",
        "title": "A Survey on Knowledge Graph-based Methods for Automated Driving",
        "abstract": "  Automated driving is one of the most active research areas in computer\nscience. Deep learning methods have made remarkable breakthroughs in machine\nlearning in general and in automated driving (AD)in particular. However, there\nare still unsolved problems to guarantee reliability and safety of automated\nsystems, especially to effectively incorporate all available information and\nknowledge in the driving task. Knowledge graphs (KG) have recently gained\nsignificant attention from both industry and academia for applications that\nbenefit by exploiting structured, dynamic, and relational data. The complexity\nof graph-structured data with complex relationships and inter-dependencies\nbetween objects has posed significant challenges to existing machine learning\nalgorithms. However, recent progress in knowledge graph embeddings and graph\nneural networks allows to applying machine learning to graph-structured data.\nTherefore, we motivate and discuss the potential benefit of KGs applied to the\nmain tasks of AD including 1) ontologies 2) perception, 3) scene understanding,\n4) motion planning, and 5) validation. Then, we survey, analyze and categorize\nontologies and KG-based approaches for AD. We discuss current research\nchallenges and propose promising future research directions for KG-based\nsolutions for AD.\n",
        "published": "2022",
        "authors": [
            "Juergen Luettin",
            "Sebastian Monka",
            "Cory Henson",
            "Lavdim Halilaj"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.14387v1",
        "title": "Machine Learning Algorithms for Time Series Analysis and Forecasting",
        "abstract": "  Time series data is being used everywhere, from sales records to patients'\nhealth evolution metrics. The ability to deal with this data has become a\nnecessity, and time series analysis and forecasting are used for the same.\nEvery Machine Learning enthusiast would consider these as very important tools,\nas they deepen the understanding of the characteristics of data. Forecasting is\nused to predict the value of a variable in the future, based on its past\noccurrences. A detailed survey of the various methods that are used for\nforecasting has been presented in this paper. The complete process of\nforecasting, from preprocessing to validation has also been explained\nthoroughly. Various statistical and deep learning models have been considered,\nnotably, ARIMA, Prophet and LSTMs. Hybrid versions of Machine Learning models\nhave also been explored and elucidated. Our work can be used by anyone to\ndevelop a good understanding of the forecasting process, and to identify\nvarious state of the art models which are being used today.\n",
        "published": "2022",
        "authors": [
            "Rameshwar Garg",
            "Shriya Barpanda",
            "Girish Rao Salanke N S",
            "Ramya S"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.07299v1",
        "title": "Supervised Machine Learning for Breast Cancer Risk Factors Analysis and\n  Survival Prediction",
        "abstract": "  The choice of the most effective treatment may eventually be influenced by\nbreast cancer survival prediction. To predict the chances of a patient\nsurviving, a variety of techniques were employed, such as statistical, machine\nlearning, and deep learning models. In the current study, 1904 patient records\nfrom the METABRIC dataset were utilized to predict a 5-year breast cancer\nsurvival using a machine learning approach. In this study, we compare the\noutcomes of seven classification models to evaluate how well they perform using\nthe following metrics: recall, AUC, confusion matrix, accuracy, precision,\nfalse positive rate, and true positive rate. The findings demonstrate that the\nclassifiers for Logistic Regression (LR), Support Vector Machines (SVM),\nDecision Tree (DT), Random Forest (RD), Extremely Randomized Trees (ET),\nK-Nearest Neighbor (KNN), and Adaptive Boosting (AdaBoost) can accurately\npredict the survival rate of the tested samples, which is 75,4\\%, 74,7\\%,\n71,5\\%, 75,5\\%, 70,3\\%, and 78 percent.\n",
        "published": "2023",
        "authors": [
            "Khaoula Chtouki",
            "Maryem Rhanoui",
            "Mounia Mikram",
            "Kamelia Amazian",
            "Siham Yousfi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.03175v1",
        "title": "Infusing Lattice Symmetry Priors in Attention Mechanisms for\n  Sample-Efficient Abstract Geometric Reasoning",
        "abstract": "  The Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) and its most\nrecent language-complete instantiation (LARC) has been postulated as an\nimportant step towards general AI. Yet, even state-of-the-art machine learning\nmodels struggle to achieve meaningful performance on these problems, falling\nbehind non-learning based approaches. We argue that solving these tasks\nrequires extreme generalization that can only be achieved by proper accounting\nfor core knowledge priors. As a step towards this goal, we focus on geometry\npriors and introduce LatFormer, a model that incorporates lattice symmetry\npriors in attention masks. We show that, for any transformation of the\nhypercubic lattice, there exists a binary attention mask that implements that\ngroup action. Hence, our study motivates a modification to the standard\nattention mechanism, where attention weights are scaled using soft masks\ngenerated by a convolutional network. Experiments on synthetic geometric\nreasoning show that LatFormer requires 2 orders of magnitude fewer data than\nstandard attention and transformers. Moreover, our results on ARC and LARC\ntasks that incorporate geometric priors provide preliminary evidence that these\ncomplex datasets do not lie out of the reach of deep learning models.\n",
        "published": "2023",
        "authors": [
            "Mattia Atzeni",
            "Mrinmaya Sachan",
            "Andreas Loukas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.03363v1",
        "title": "A reading survey on adversarial machine learning: Adversarial attacks\n  and their understanding",
        "abstract": "  Deep Learning has empowered us to train neural networks for complex data with\nhigh performance. However, with the growing research, several vulnerabilities\nin neural networks have been exposed. A particular branch of research,\nAdversarial Machine Learning, exploits and understands some of the\nvulnerabilities that cause the neural networks to misclassify for near original\ninput. A class of algorithms called adversarial attacks is proposed to make the\nneural networks misclassify for various tasks in different domains. With the\nextensive and growing research in adversarial attacks, it is crucial to\nunderstand the classification of adversarial attacks. This will help us\nunderstand the vulnerabilities in a systematic order and help us to mitigate\nthe effects of adversarial attacks. This article provides a survey of existing\nadversarial attacks and their understanding based on different perspectives. We\nalso provide a brief overview of existing adversarial defences and their\nlimitations in mitigating the effect of adversarial attacks. Further, we\nconclude with a discussion on the future research directions in the field of\nadversarial machine learning.\n",
        "published": "2023",
        "authors": [
            "Shashank Kotyan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.01592v2",
        "title": "Les Houches Lectures on Deep Learning at Large & Infinite Width",
        "abstract": "  These lectures, presented at the 2022 Les Houches Summer School on\nStatistical Physics and Machine Learning, focus on the infinite-width limit and\nlarge-width regime of deep neural networks. Topics covered include various\nstatistical and dynamical properties of these networks. In particular, the\nlecturers discuss properties of random deep neural networks; connections\nbetween trained deep neural networks, linear models, kernels, and Gaussian\nprocesses that arise in the infinite-width limit; and perturbative and\nnon-perturbative treatments of large but finite-width networks, at\ninitialization and after training.\n",
        "published": "2023",
        "authors": [
            "Yasaman Bahri",
            "Boris Hanin",
            "Antonin Brossollet",
            "Vittorio Erba",
            "Christian Keup",
            "Rosalba Pacelli",
            "James B. Simon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.09024v1",
        "title": "Deep Learning Approaches for Forecasting Strawberry Yields and Prices\n  Using Satellite Images and Station-Based Soil Parameters",
        "abstract": "  Computational tools for forecasting yields and prices for fresh produce have\nbeen based on traditional machine learning approaches or time series modelling.\nWe propose here an alternate approach based on deep learning algorithms for\nforecasting strawberry yields and prices in Santa Barbara county, California.\nBuilding the proposed forecasting model comprises three stages: first, the\nstation-based ensemble model (ATT-CNN-LSTM-SeriesNet_Ens) with its compound\ndeep learning components, SeriesNet with Gated Recurrent Unit (GRU) and\nConvolutional Neural Network LSTM with Attention layer (Att-CNN-LSTM), are\ntrained and tested using the station-based soil temperature and moisture data\nof SantaBarbara as input and the corresponding strawberry yields or prices as\noutput. Secondly, the remote sensing ensemble model (SIM_CNN-LSTM_Ens), which\nis an ensemble model of Convolutional NeuralNetwork LSTM (CNN-LSTM) models, is\ntrained and tested using satellite images of the same county as input mapped to\nthe same yields and prices as output. These two ensembles forecast strawberry\nyields and prices with minimal forecasting errors and highest model correlation\nfor five weeks ahead forecasts.Finally, the forecasts of these two models are\nensembled to have a final forecasted value for yields and prices by introducing\na voting ensemble. Based on an aggregated performance measure (AGM), it is\nfound that this voting ensemble not only enhances the forecasting performance\nby 5% compared to its best performing component model but also outperforms the\nDeep Learning (DL) ensemble model found in literature by 33% for forecasting\nyields and 21% for forecasting prices\n",
        "published": "2021",
        "authors": [
            "Mohita Chaudhary",
            "Mohamed Sadok Gastli",
            "Lobna Nassar",
            "Fakhri Karray"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.06131v1",
        "title": "Locality Guided Neural Networks for Explainable Artificial Intelligence",
        "abstract": "  In current deep network architectures, deeper layers in networks tend to\ncontain hundreds of independent neurons which makes it hard for humans to\nunderstand how they interact with each other. By organizing the neurons by\ncorrelation, humans can observe how clusters of neighbouring neurons interact\nwith each other. In this paper, we propose a novel algorithm for back\npropagation, called Locality Guided Neural Network(LGNN) for training networks\nthat preserves locality between neighbouring neurons within each layer of a\ndeep network. Heavily motivated by Self-Organizing Map (SOM), the goal is to\nenforce a local topology on each layer of a deep network such that neighbouring\nneurons are highly correlated with each other. This method contributes to the\ndomain of Explainable Artificial Intelligence (XAI), which aims to alleviate\nthe black-box nature of current AI methods and make them understandable by\nhumans. Our method aims to achieve XAI in deep learning without changing the\nstructure of current models nor requiring any post processing. This paper\nfocuses on Convolutional Neural Networks (CNNs), but can theoretically be\napplied to any type of deep learning architecture. In our experiments, we train\nvarious VGG and Wide ResNet (WRN) networks for image classification on\nCIFAR100. In depth analyses presenting both qualitative and quantitative\nresults demonstrate that our method is capable of enforcing a topology on each\nlayer while achieving a small increase in classification accuracy\n",
        "published": "2020",
        "authors": [
            "Randy Tan",
            "Naimul Khan",
            "Ling Guan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.06479v2",
        "title": "Advances in Machine and Deep Learning for Modeling and Real-time\n  Detection of Multi-Messenger Sources",
        "abstract": "  We live in momentous times. The science community is empowered with an\narsenal of cosmic messengers to study the Universe in unprecedented detail.\nGravitational waves, electromagnetic waves, neutrinos and cosmic rays cover a\nwide range of wavelengths and time scales. Combining and processing these\ndatasets that vary in volume, speed and dimensionality requires new modes of\ninstrument coordination, funding and international collaboration with a\nspecialized human and technological infrastructure. In tandem with the advent\nof large-scale scientific facilities, the last decade has experienced an\nunprecedented transformation in computing and signal processing algorithms. The\ncombination of graphics processing units, deep learning, and the availability\nof open source, high-quality datasets, have powered the rise of artificial\nintelligence. This digital revolution now powers a multi-billion dollar\nindustry, with far-reaching implications in technology and society. In this\nchapter we describe pioneering efforts to adapt artificial intelligence\nalgorithms to address computational grand challenges in Multi-Messenger\nAstrophysics. We review the rapid evolution of these disruptive algorithms,\nfrom the first class of algorithms introduced in early 2017, to the\nsophisticated algorithms that now incorporate domain expertise in their\narchitectural design and optimization schemes. We discuss the importance of\nscientific visualization and extreme-scale computing in reducing\ntime-to-insight and obtaining new knowledge from the interplay between models\nand data.\n",
        "published": "2021",
        "authors": [
            "E. A. Huerta",
            "Zhizhen Zhao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.13635v4",
        "title": "Logic Tensor Networks",
        "abstract": "  Artificial Intelligence agents are required to learn from their surroundings\nand to reason about the knowledge that has been learned in order to make\ndecisions. While state-of-the-art learning from data typically uses\nsub-symbolic distributed representations, reasoning is normally useful at a\nhigher level of abstraction with the use of a first-order logic language for\nknowledge representation. As a result, attempts at combining symbolic AI and\nneural computation into neural-symbolic systems have been on the increase. In\nthis paper, we present Logic Tensor Networks (LTN), a neurosymbolic formalism\nand computational model that supports learning and reasoning through the\nintroduction of a many-valued, end-to-end differentiable first-order logic\ncalled Real Logic as a representation language for deep learning. We show that\nLTN provides a uniform language for the specification and the computation of\nseveral AI tasks such as data clustering, multi-label classification,\nrelational learning, query answering, semi-supervised learning, regression and\nembedding learning. We implement and illustrate each of the above tasks with a\nnumber of simple explanatory examples using TensorFlow 2. Keywords:\nNeurosymbolic AI, Deep Learning and Reasoning, Many-valued Logic.\n",
        "published": "2020",
        "authors": [
            "Samy Badreddine",
            "Artur d'Avila Garcez",
            "Luciano Serafini",
            "Michael Spranger"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.05149v1",
        "title": "Logic Explained Networks",
        "abstract": "  The large and still increasing popularity of deep learning clashes with a\nmajor limit of neural network architectures, that consists in their lack of\ncapability in providing human-understandable motivations of their decisions. In\nsituations in which the machine is expected to support the decision of human\nexperts, providing a comprehensible explanation is a feature of crucial\nimportance. The language used to communicate the explanations must be formal\nenough to be implementable in a machine and friendly enough to be\nunderstandable by a wide audience. In this paper, we propose a general approach\nto Explainable Artificial Intelligence in the case of neural architectures,\nshowing how a mindful design of the networks leads to a family of interpretable\ndeep learning models called Logic Explained Networks (LENs). LENs only require\ntheir inputs to be human-understandable predicates, and they provide\nexplanations in terms of simple First-Order Logic (FOL) formulas involving such\npredicates. LENs are general enough to cover a large number of scenarios.\nAmongst them, we consider the case in which LENs are directly used as special\nclassifiers with the capability of being explainable, or when they act as\nadditional networks with the role of creating the conditions for making a\nblack-box classifier explainable by FOL formulas. Despite supervised learning\nproblems are mostly emphasized, we also show that LENs can learn and provide\nexplanations in unsupervised learning settings. Experimental results on several\ndatasets and tasks show that LENs may yield better classifications than\nestablished white-box models, such as decision trees and Bayesian rule lists,\nwhile providing more compact and meaningful explanations.\n",
        "published": "2021",
        "authors": [
            "Gabriele Ciravegna",
            "Pietro Barbiero",
            "Francesco Giannini",
            "Marco Gori",
            "Pietro Li\u00f3",
            "Marco Maggini",
            "Stefano Melacci"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.08296v2",
        "title": "Deep-Learning-Empowered Inverse Design for Freeform Reconfigurable\n  Metasurfaces",
        "abstract": "  The past decade has witnessed the advances of artificial intelligence with\nvarious applications in engineering. Recently, artificial neural network\nempowered inverse design for metasurfaces has been developed that can design\non-demand meta-atoms with diverse shapes and high performance, where the design\nprocess based on artificial intelligence is fast and automatic. However, once\nthe inverse-designed static meta-atom is fabricated, the function of the\nmetasurface is fixed. Reconfigurable metasurfaces can realize dynamic\nfunctions, while applying artificial intelligence to design practical\nreconfigurable meta-atoms inversely has not been reported yet. Here, we present\na deep-learning-empowered inverse design method for freeform reconfigurable\nmetasurfaces, which can generate on-demand reconfigurable coding meta-atoms at\nself-defined frequency bands. To reduce the scale of dataset, a decoupling\nmethod of the reconfigurable meta-atom based on microwave network theory is\nproposed at first, which can convert the inverse design process for\nreconfigurable coding meta-atoms to the inverse design for static structures. A\nconvolutional neural network model is trained to predict the responses of\nfree-shaped meta-atoms, and the genetic algorithm is applied to generate the\noptimal structure patterns rapidly. As a demonstration of concept, several\ninverse-designed examples are generated with different self-defined spectrum\nresponses in microwave band, and an inverse-designed wideband reconfigurable\nmetasurface prototype is fabricated and measured for beam scanning applications\nwith broad bandwidth. Our work paves the way for the fast and automatic design\nprocess of high-performance reconfigurable metasurfaces.\n",
        "published": "2022",
        "authors": [
            "Changhao Liu",
            "Fan Yang",
            "Maokun Li",
            "Shenheng Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.07996v1",
        "title": "A Smart System for Selection of Optimal Product Images in E-Commerce",
        "abstract": "  In e-commerce, content quality of the product catalog plays a key role in\ndelivering a satisfactory experience to the customers. In particular, visual\ncontent such as product images influences customers' engagement and purchase\ndecisions. With the rapid growth of e-commerce and the advent of artificial\nintelligence, traditional content management systems are giving way to\nautomated scalable systems. In this paper, we present a machine learning driven\nvisual content management system for extremely large e-commerce catalogs. For a\ngiven product, the system aggregates images from various suppliers, understands\nand analyzes them to produce a superior image set with optimal image count and\nquality, and arranges them in an order tailored to the demands of the\ncustomers. The system makes use of an array of technologies, ranging from deep\nlearning to traditional computer vision, at different stages of analysis. In\nthis paper, we outline how the system works and discuss the unique challenges\nrelated to applying machine learning techniques to real-world data from\ne-commerce domain. We emphasize how we tune state-of-the-art image\nclassification techniques to develop solutions custom made for a massive,\ndiverse, and constantly evolving product catalog. We also provide the details\nof how we measure the system's impact on various customer engagement metrics.\n",
        "published": "2018",
        "authors": [
            "Abon Chaudhuri",
            "Paolo Messina",
            "Samrat Kokkula",
            "Aditya Subramanian",
            "Abhinandan Krishnan",
            "Shreyansh Gandhi",
            "Alessandro Magnani",
            "Venkatesh Kandaswamy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.05504v2",
        "title": "Melding the Data-Decisions Pipeline: Decision-Focused Learning for\n  Combinatorial Optimization",
        "abstract": "  Creating impact in real-world settings requires artificial intelligence\ntechniques to span the full pipeline from data, to predictive models, to\ndecisions. These components are typically approached separately: a machine\nlearning model is first trained via a measure of predictive accuracy, and then\nits predictions are used as input into an optimization algorithm which produces\na decision. However, the loss function used to train the model may easily be\nmisaligned with the end goal, which is to make the best decisions possible.\nHand-tuning the loss function to align with optimization is a difficult and\nerror-prone process (which is often skipped entirely).\n  We focus on combinatorial optimization problems and introduce a general\nframework for decision-focused learning, where the machine learning model is\ndirectly trained in conjunction with the optimization algorithm to produce\nhigh-quality decisions. Technically, our contribution is a means of integrating\ncommon classes of discrete optimization problems into deep learning or other\npredictive models, which are typically trained via gradient descent. The main\nidea is to use a continuous relaxation of the discrete problem to propagate\ngradients through the optimization procedure. We instantiate this framework for\ntwo broad classes of combinatorial problems: linear programs and submodular\nmaximization. Experimental results across a variety of domains show that\ndecision-focused learning often leads to improved optimization performance\ncompared to traditional methods. We find that standard measures of accuracy are\nnot a reliable proxy for a predictive model's utility in optimization, and our\nmethod's ability to specify the true goal as the model's training objective\nyields substantial dividends across a range of decision problems.\n",
        "published": "2018",
        "authors": [
            "Bryan Wilder",
            "Bistra Dilkina",
            "Milind Tambe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.05606v1",
        "title": "Neuromorphic Processing and Sensing: Evolutionary Progression of AI to\n  Spiking",
        "abstract": "  The increasing rise in machine learning and deep learning applications is\nrequiring ever more computational resources to successfully meet the growing\ndemands of an always-connected, automated world. Neuromorphic technologies\nbased on Spiking Neural Network algorithms hold the promise to implement\nadvanced artificial intelligence using a fraction of the computations and power\nrequirements by modeling the functioning, and spiking, of the human brain. With\nthe proliferation of tools and platforms aiding data scientists and machine\nlearning engineers to develop the latest innovations in artificial and deep\nneural networks, a transition to a new paradigm will require building from the\ncurrent well-established foundations. This paper explains the theoretical\nworkings of neuromorphic technologies based on spikes, and overviews the\nstate-of-art in hardware processors, software platforms and neuromorphic\nsensing devices. A progression path is paved for current machine learning\nspecialists to update their skillset, as well as classification or predictive\nmodels from the current generation of deep neural networks to SNNs. This can be\nachieved by leveraging existing, specialized hardware in the form of SpiNNaker\nand the Nengo migration toolkit. First-hand, experimental results of converting\na VGG-16 neural network to an SNN are shared. A forward gaze into industrial,\nmedical and commercial applications that can readily benefit from SNNs wraps up\nthis investigation into the neuromorphic computing future.\n",
        "published": "2020",
        "authors": [
            "Philippe Reiter",
            "Geet Rose Jose",
            "Spyridon Bizmpikis",
            "Ionela-Ancu\u0163a C\u00eerjil\u0103"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.09512v1",
        "title": "Applying Genetic Programming to Improve Interpretability in Machine\n  Learning Models",
        "abstract": "  Explainable Artificial Intelligence (or xAI) has become an important research\ntopic in the fields of Machine Learning and Deep Learning. In this paper, we\npropose a Genetic Programming (GP) based approach, named Genetic Programming\nExplainer (GPX), to the problem of explaining decisions computed by AI systems.\nThe method generates a noise set located in the neighborhood of the point of\ninterest, whose prediction should be explained, and fits a local explanation\nmodel for the analyzed sample. The tree structure generated by GPX provides a\ncomprehensible analytical, possibly non-linear, symbolic expression which\nreflects the local behavior of the complex model. We considered three machine\nlearning techniques that can be recognized as complex black-box models: Random\nForest, Deep Neural Network and Support Vector Machine in twenty data sets for\nregression and classifications problems. Our results indicate that the GPX is\nable to produce more accurate understanding of complex models than the state of\nthe art. The results validate the proposed approach as a novel way to deploy GP\nto improve interpretability.\n",
        "published": "2020",
        "authors": [
            "Leonardo Augusto Ferreira",
            "Frederico Gadelha Guimar\u00e3es",
            "Rodrigo Silva"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.08864v1",
        "title": "Development of cloud, digital technologies and the introduction of chip\n  technologies",
        "abstract": "  Hardly any other area of research has recently attracted as much attention as\nmachine learning (ML) through the rapid advances in artificial intelligence\n(AI). This publication provides a short introduction to practical concepts and\nmethods of machine learning, problems and emerging research questions, as well\nas an overview of the participants, an overview of the application areas and\nthe socio-economic framework conditions of the research.\n  In expert circles, ML is used as a key technology for modern artificial\nintelligence techniques, which is why AI and ML are often used interchangeably,\nespecially in an economic context. Machine learning and, in particular, deep\nlearning (DL) opens up entirely new possibilities in automatic language\nprocessing, image analysis, medical diagnostics, process management and\ncustomer management. One of the important aspects in this article is\nchipization. Due to the rapid development of digitalization, the number of\napplications will continue to grow as digital technologies advance. In the\nfuture, machines will more and more provide results that are important for\ndecision making. To this end, it is important to ensure the safety, reliability\nand sufficient traceability of automated decision-making processes from the\ntechnological side. At the same time, it is necessary to ensure that ML\napplications are compatible with legal issues such as responsibility and\nliability for algorithmic decisions, as well as technically feasible. Its\nformulation and regulatory implementation is an important and complex issue\nthat requires an interdisciplinary approach. Last but not least, public\nacceptance is critical to the continued diffusion of machine learning processes\nin applications. This requires widespread public discussion and the involvement\nof various social groups.\n",
        "published": "2020",
        "authors": [
            "Ali R. Baghirzade"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.09215v1",
        "title": "A Systematic Review of Machine Learning Techniques for Cattle\n  Identification: Datasets, Methods and Future Directions",
        "abstract": "  Increased biosecurity and food safety requirements may increase demand for\nefficient traceability and identification systems of livestock in the supply\nchain. The advanced technologies of machine learning and computer vision have\nbeen applied in precision livestock management, including critical disease\ndetection, vaccination, production management, tracking, and health monitoring.\nThis paper offers a systematic literature review (SLR) of vision-based cattle\nidentification. More specifically, this SLR is to identify and analyse the\nresearch related to cattle identification using Machine Learning (ML) and Deep\nLearning (DL). For the two main applications of cattle detection and cattle\nidentification, all the ML based papers only solve cattle identification\nproblems. However, both detection and identification problems were studied in\nthe DL based papers. Based on our survey report, the most used ML models for\ncattle identification were support vector machine (SVM), k-nearest neighbour\n(KNN), and artificial neural network (ANN). Convolutional neural network (CNN),\nresidual network (ResNet), Inception, You Only Look Once (YOLO), and Faster\nR-CNN were popular DL models in the selected papers. Among these papers, the\nmost distinguishing features were the muzzle prints and coat patterns of\ncattle. Local binary pattern (LBP), speeded up robust features (SURF),\nscale-invariant feature transform (SIFT), and Inception or CNN were identified\nas the most used feature extraction methods.\n",
        "published": "2022",
        "authors": [
            "Md Ekramul Hossain",
            "Muhammad Ashad Kabir",
            "Lihong Zheng",
            "Dave L. Swain",
            "Shawn McGrath",
            "Jonathan Medway"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.11316v1",
        "title": "Open Problems in Applied Deep Learning",
        "abstract": "  This work formulates the machine learning mechanism as a bi-level\noptimization problem. The inner level optimization loop entails minimizing a\nproperly chosen loss function evaluated on the training data. This is nothing\nbut the well-studied training process in pursuit of optimal model parameters.\nThe outer level optimization loop is less well-studied and involves maximizing\na properly chosen performance metric evaluated on the validation data. This is\nwhat we call the \"iteration process\", pursuing optimal model hyper-parameters.\nAmong many other degrees of freedom, this process entails model engineering\n(e.g., neural network architecture design) and management, experiment tracking,\ndataset versioning and augmentation. The iteration process could be automated\nvia Automatic Machine Learning (AutoML) or left to the intuitions of machine\nlearning students, engineers, and researchers. Regardless of the route we take,\nthere is a need to reduce the computational cost of the iteration step and as a\ndirect consequence reduce the carbon footprint of developing artificial\nintelligence algorithms. Despite the clean and unified mathematical formulation\nof the iteration step as a bi-level optimization problem, its solutions are\ncase specific and complex. This work will consider such cases while increasing\nthe level of complexity from supervised learning to semi-supervised,\nself-supervised, unsupervised, few-shot, federated, reinforcement, and\nphysics-informed learning. As a consequence of this exercise, this proposal\nsurfaces a plethora of open problems in the field, many of which can be\naddressed in parallel.\n",
        "published": "2023",
        "authors": [
            "Maziar Raissi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.05543v1",
        "title": "Walk4Me: Telehealth Community Mobility Assessment, An Automated System\n  for Early Diagnosis and Disease Progression",
        "abstract": "  We introduce Walk4Me, a telehealth community mobility assessment system\ndesigned to facilitate early diagnosis, severity, and progression\nidentification. Our system achieves this by 1) enabling early diagnosis, 2)\nidentifying early indicators of clinical severity, and 3) quantifying and\ntracking the progression of the disease across the ambulatory phase of the\ndisease. To accomplish this, we employ an Artificial Intelligence (AI)-based\ndetection of gait characteristics in patients and typically developing peers.\nOur system remotely and in real-time collects data from device sensors (e.g.,\nacceleration from a mobile device, etc.) using our novel Walk4Me API. Our web\napplication extracts temporal/spatial gait characteristics and raw data signal\ncharacteristics and then employs traditional machine learning and deep learning\ntechniques to identify patterns that can 1) identify patients with gait\ndisturbances associated with disease, 2) describe the degree of mobility\nlimitation, and 3) identify characteristics that change over time with disease\nprogression. We have identified several machine learning techniques that\ndifferentiate between patients and typically-developing subjects with 100%\naccuracy across the age range studied, and we have also identified\ncorresponding temporal/spatial gait characteristics associated with each group.\nOur work demonstrates the potential of utilizing the latest advances in mobile\ndevice and machine learning technology to measure clinical outcomes regardless\nof the point of care, inform early clinical diagnosis and treatment\ndecision-making, and monitor disease progression.\n",
        "published": "2023",
        "authors": [
            "Albara Ah Ramli",
            "Xin Liu",
            "Erik K. Henricson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.09574v3",
        "title": "LymphoML: An interpretable artificial intelligence-based method\n  identifies morphologic features that correlate with lymphoma subtype",
        "abstract": "  The accurate classification of lymphoma subtypes using hematoxylin and eosin\n(H&E)-stained tissue is complicated by the wide range of morphological features\nthese cancers can exhibit. We present LymphoML - an interpretable machine\nlearning method that identifies morphologic features that correlate with\nlymphoma subtypes. Our method applies steps to process H&E-stained tissue\nmicroarray cores, segment nuclei and cells, compute features encompassing\nmorphology, texture, and architecture, and train gradient-boosted models to\nmake diagnostic predictions. LymphoML's interpretable models, developed on a\nlimited volume of H&E-stained tissue, achieve non-inferior diagnostic accuracy\nto pathologists using whole-slide images and outperform black box deep-learning\non a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using\nSHapley Additive exPlanation (SHAP) analysis, we assess the impact of each\nfeature on model prediction and find that nuclear shape features are most\ndiscriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma\n(F1-score: 74.5%). Finally, we provide the first demonstration that a model\ncombining features from H&E-stained tissue with features from a standardized\npanel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a\n46-stain panel (86.1%).\n",
        "published": "2023",
        "authors": [
            "Vivek Shankar",
            "Xiaoli Yang",
            "Vrishab Krishna",
            "Brent Tan",
            "Oscar Silva",
            "Rebecca Rojansky",
            "Andrew Ng",
            "Fabiola Valvert",
            "Edward Briercheck",
            "David Weinstock",
            "Yasodha Natkunam",
            "Sebastian Fernandez-Pol",
            "Pranav Rajpurkar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.11880v1",
        "title": "Predicting the Computational Cost of Deep Learning Models",
        "abstract": "  Deep learning is rapidly becoming a go-to tool for many artificial\nintelligence problems due to its ability to outperform other approaches and\neven humans at many problems. Despite its popularity we are still unable to\naccurately predict the time it will take to train a deep learning network to\nsolve a given problem. This training time can be seen as the product of the\ntraining time per epoch and the number of epochs which need to be performed to\nreach the desired level of accuracy. Some work has been carried out to predict\nthe training time for an epoch -- most have been based around the assumption\nthat the training time is linearly related to the number of floating point\noperations required. However, this relationship is not true and becomes\nexacerbated in cases where other activities start to dominate the execution\ntime. Such as the time to load data from memory or loss of performance due to\nnon-optimal parallel execution. In this work we propose an alternative approach\nin which we train a deep learning network to predict the execution time for\nparts of a deep learning network. Timings for these individual parts can then\nbe combined to provide a prediction for the whole execution time. This has\nadvantages over linear approaches as it can model more complex scenarios. But,\nalso, it has the ability to predict execution times for scenarios unseen in the\ntraining data. Therefore, our approach can be used not only to infer the\nexecution time for a batch, or entire epoch, but it can also support making a\nwell-informed choice for the appropriate hardware and model.\n",
        "published": "2018",
        "authors": [
            "Daniel Justus",
            "John Brennan",
            "Stephen Bonner",
            "Andrew Stephen McGough"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.00966v1",
        "title": "AI visualization in Nanoscale Microscopy",
        "abstract": "  Artificial Intelligence & Nanotechnology are promising areas for the future\nof humanity. While Deep Learning based Computer Vision has found applications\nin many fields from medicine to automotive, its application in nanotechnology\ncan open doors for new scientific discoveries. Can we apply AI to explore\nobjects that our eyes can't see such as nano scale sized objects? An AI\nplatform to visualize nanoscale patterns learnt by a Deep Learning neural\nnetwork can open new frontiers for nanotechnology. The objective of this paper\nis to develop a Deep Learning based visualization system on images of\nnanomaterials obtained by scanning electron microscope. This paper contributes\nan AI platform to enable any nanoscience researcher to use AI in visual\nexploration of nanoscale morphologies of nanomaterials. This AI is developed by\na technique of visualizing intermediate activations of a Convolutional\nAutoEncoder. In this method, a nano scale specimen image is transformed into\nits feature representations by a Convolution Neural Network. The Convolutional\nAutoEncoder is trained on 100% SEM dataset, and then CNN visualization is\napplied. This AI generates various conceptual feature representations of the\nnanomaterial.\n  While Deep Learning based image classification of SEM images are widely\npublished in literature, there are not much publications that have visualized\nDeep neural networks of nanomaterials. There is a significant opportunity to\ngain insights from the learnings extracted by machine learning. This paper\nunlocks the potential of applying Deep Learning based Visualization on electron\nmicroscopy to offer AI extracted features and architectural patterns of various\nnanomaterials. This is a contribution in Explainable AI in nano scale objects.\nThis paper contributes an open source AI with reproducible results at URL\n(https://sites.google.com/view/aifornanotechnology)\n",
        "published": "2022",
        "authors": [
            "Rajagopal A",
            "Nirmala V",
            "Andrew J",
            "Arun Muthuraj Vedamanickam."
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.01172v1",
        "title": "Value-laden Disciplinary Shifts in Machine Learning",
        "abstract": "  As machine learning models are increasingly used for high-stakes decision\nmaking, scholars have sought to intervene to ensure that such models do not\nencode undesirable social and political values. However, little attention thus\nfar has been given to how values influence the machine learning discipline as a\nwhole. How do values influence what the discipline focuses on and the way it\ndevelops? If undesirable values are at play at the level of the discipline,\nthen intervening on particular models will not suffice to address the problem.\nInstead, interventions at the disciplinary-level are required. This paper\nanalyzes the discipline of machine learning through the lens of philosophy of\nscience. We develop a conceptual framework to evaluate the process through\nwhich types of machine learning models (e.g. neural networks, support vector\nmachines, graphical models) become predominant. The rise and fall of\nmodel-types is often framed as objective progress. However, such disciplinary\nshifts are more nuanced. First, we argue that the rise of a model-type is\nself-reinforcing--it influences the way model-types are evaluated. For example,\nthe rise of deep learning was entangled with a greater focus on evaluations in\ncompute-rich and data-rich environments. Second, the way model-types are\nevaluated encodes loaded social and political values. For example, a greater\nfocus on evaluations in compute-rich and data-rich environments encodes values\nabout centralization of power, privacy, and environmental concerns.\n",
        "published": "2019",
        "authors": [
            "Ravit Dotan",
            "Smitha Milli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.08497v1",
        "title": "The Next Big Thing(s) in Unsupervised Machine Learning: Five Lessons\n  from Infant Learning",
        "abstract": "  After a surge in popularity of supervised Deep Learning, the desire to reduce\nthe dependence on curated, labelled data sets and to leverage the vast\nquantities of unlabelled data available recently triggered renewed interest in\nunsupervised learning algorithms. Despite a significantly improved performance\ndue to approaches such as the identification of disentangled latent\nrepresentations, contrastive learning, and clustering optimisations, the\nperformance of unsupervised machine learning still falls short of its\nhypothesised potential. Machine learning has previously taken inspiration from\nneuroscience and cognitive science with great success. However, this has mostly\nbeen based on adult learners with access to labels and a vast amount of prior\nknowledge. In order to push unsupervised machine learning forward, we argue\nthat developmental science of infant cognition might hold the key to unlocking\nthe next generation of unsupervised learning approaches. Conceptually, human\ninfant learning is the closest biological parallel to artificial unsupervised\nlearning, as infants too must learn useful representations from unlabelled\ndata. In contrast to machine learning, these new representations are learned\nrapidly and from relatively few examples. Moreover, infants learn robust\nrepresentations that can be used flexibly and efficiently in a number of\ndifferent tasks and contexts. We identify five crucial factors enabling\ninfants' quality and speed of learning, assess the extent to which these have\nalready been exploited in machine learning, and propose how further adoption of\nthese factors can give rise to previously unseen performance levels in\nunsupervised learning.\n",
        "published": "2020",
        "authors": [
            "Lorijn Zaadnoordijk",
            "Tarek R. Besold",
            "Rhodri Cusack"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.10613v1",
        "title": "The Right Tool for the Job: Open-Source Auditing Tools in Machine\n  Learning",
        "abstract": "  In recent years, discussions about fairness in machine learning, AI ethics\nand algorithm audits have increased. Many entities have developed framework\nguidance to establish a baseline rubric for fairness and accountability.\nHowever, in spite of increased discussions and multiple frameworks, algorithm\nand data auditing still remain difficult to execute in practice. Many\nopen-source auditing tools are available, but users aren't always aware of the\ntools, what they are useful for, or how to access them. Model auditing and\nevaluation are not frequently emphasized skills in machine learning. There are\nalso legal reasons for the proactive adoption of these tools that extend beyond\nthe desire for greater fairness in machine learning. There are positive social\nissues of public perception and goodwill that matter in our highly connected\nglobal society. Greater awareness of these tools and the reasons for actively\nutilizing them may be helpful to the entire continuum of programmers, data\nscientists, engineers, researchers, users and consumers of AI and machine\nlearning products. It is important for everyone to better understand the input\nand output differentials, how they are occurring, and what can be done to\npromote FATE (fairness, accountability, transparency, and ethics) in machine-\nand deep learning. The ability to freely access open-source auditing tools\nremoves barriers to fairness assessment at the most basic levels of machine\nlearning. This paper aims to reinforce the urgent need to actually use these\ntools and provides motivations for doing so. The exemplary tools highlighted\nherein are open-source with software or code-base repositories available that\ncan be used immediately by anyone worldwide.\n",
        "published": "2022",
        "authors": [
            "Cherie M Poland"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1507.01839v2",
        "title": "Dependency-based Convolutional Neural Networks for Sentence Embedding",
        "abstract": "  In sentence modeling and classification, convolutional neural network\napproaches have recently achieved state-of-the-art results, but all such\nefforts process word vectors sequentially and neglect long-distance\ndependencies. To exploit both deep learning and linguistic structures, we\npropose a tree-based convolutional neural network model which exploit various\nlong-distance relationships between words. Our model improves the sequential\nbaselines on all three sentiment and question classification tasks, and\nachieves the highest published accuracy on TREC.\n",
        "published": "2015",
        "authors": [
            "Mingbo Ma",
            "Liang Huang",
            "Bing Xiang",
            "Bowen Zhou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1608.02717v1",
        "title": "Mean Box Pooling: A Rich Image Representation and Output Embedding for\n  the Visual Madlibs Task",
        "abstract": "  We present Mean Box Pooling, a novel visual representation that pools over\nCNN representations of a large number, highly overlapping object proposals. We\nshow that such representation together with nCCA, a successful multimodal\nembedding technique, achieves state-of-the-art performance on the Visual\nMadlibs task. Moreover, inspired by the nCCA's objective function, we extend\nclassical CNN+LSTM approach to train the network by directly maximizing the\nsimilarity between the internal representation of the deep learning\narchitecture and candidate answers. Again, such approach achieves a significant\nimprovement over the prior work that also uses CNN+LSTM approach on Visual\nMadlibs.\n",
        "published": "2016",
        "authors": [
            "Ashkan Mokarian",
            "Mateusz Malinowski",
            "Mario Fritz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1701.04143v1",
        "title": "Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks",
        "abstract": "  Deep learning classifiers are known to be inherently vulnerable to\nmanipulation by intentionally perturbed inputs, named adversarial examples. In\nthis work, we establish that reinforcement learning techniques based on Deep\nQ-Networks (DQNs) are also vulnerable to adversarial input perturbations, and\nverify the transferability of adversarial examples across different DQN models.\nFurthermore, we present a novel class of attacks based on this vulnerability\nthat enable policy manipulation and induction in the learning process of DQNs.\nWe propose an attack mechanism that exploits the transferability of adversarial\nexamples to implement policy induction attacks on DQNs, and demonstrate its\nefficacy and impact through experimental study of a game-learning scenario.\n",
        "published": "2017",
        "authors": [
            "Vahid Behzadan",
            "Arslan Munir"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.05116v1",
        "title": "Tuning Modular Networks with Weighted Losses for Hand-Eye Coordination",
        "abstract": "  This paper introduces an end-to-end fine-tuning method to improve hand-eye\ncoordination in modular deep visuo-motor policies (modular networks) where each\nmodule is trained independently. Benefiting from weighted losses, the\nfine-tuning method significantly improves the performance of the policies for a\nrobotic planar reaching task.\n",
        "published": "2017",
        "authors": [
            "Fangyi Zhang",
            "J\u00fcrgen Leitner",
            "Michael Milford",
            "Peter I. Corke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.07962v2",
        "title": "pix2code: Generating Code from a Graphical User Interface Screenshot",
        "abstract": "  Transforming a graphical user interface screenshot created by a designer into\ncomputer code is a typical task conducted by a developer in order to build\ncustomized software, websites, and mobile applications. In this paper, we show\nthat deep learning methods can be leveraged to train a model end-to-end to\nautomatically generate code from a single input image with over 77% of accuracy\nfor three different platforms (i.e. iOS, Android and web-based technologies).\n",
        "published": "2017",
        "authors": [
            "Tony Beltramelli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1706.05744v2",
        "title": "Learning Hierarchical Information Flow with Recurrent Neural Modules",
        "abstract": "  We propose ThalNet, a deep learning model inspired by neocortical\ncommunication via the thalamus. Our model consists of recurrent neural modules\nthat send features through a routing center, endowing the modules with the\nflexibility to share features over multiple time steps. We show that our model\nlearns to route information hierarchically, processing input data by a chain of\nmodules. We observe common architectures, such as feed forward neural networks\nand skip connections, emerging as special cases of our architecture, while\nnovel connectivity patterns are learned for the text8 compression task. Our\nmodel outperforms standard recurrent neural networks on several sequential\nbenchmarks.\n",
        "published": "2017",
        "authors": [
            "Danijar Hafner",
            "Alex Irpan",
            "James Davidson",
            "Nicolas Heess"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.01968v4",
        "title": "Faster Deep Q-learning using Neural Episodic Control",
        "abstract": "  The research on deep reinforcement learning which estimates Q-value by deep\nlearning has been attracted the interest of researchers recently. In deep\nreinforcement learning, it is important to efficiently learn the experiences\nthat an agent has collected by exploring environment. We propose NEC2DQN that\nimproves learning speed of a poor sample efficiency algorithm such as DQN by\nusing good one such as NEC at the beginning of learning. We show it is able to\nlearn faster than Double DQN or N-step DQN in the experiments of Pong.\n",
        "published": "2018",
        "authors": [
            "Daichi Nishio",
            "Satoshi Yamane"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.05385v1",
        "title": "Fooling OCR Systems with Adversarial Text Images",
        "abstract": "  We demonstrate that state-of-the-art optical character recognition (OCR)\nbased on deep learning is vulnerable to adversarial images. Minor modifications\nto images of printed text, which do not change the meaning of the text to a\nhuman reader, cause the OCR system to \"recognize\" a different text where\ncertain words chosen by the adversary are replaced by their semantic opposites.\nThis completely changes the meaning of the output produced by the OCR system\nand by the NLP applications that use OCR for preprocessing their inputs.\n",
        "published": "2018",
        "authors": [
            "Congzheng Song",
            "Vitaly Shmatikov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.00258v1",
        "title": "A Survey of Domain Adaptation for Neural Machine Translation",
        "abstract": "  Neural machine translation (NMT) is a deep learning based approach for\nmachine translation, which yields the state-of-the-art translation performance\nin scenarios where large-scale parallel corpora are available. Although the\nhigh-quality and domain-specific translation is crucial in the real world,\ndomain-specific corpora are usually scarce or nonexistent, and thus vanilla NMT\nperforms poorly in such scenarios. Domain adaptation that leverages both\nout-of-domain parallel corpora as well as monolingual corpora for in-domain\ntranslation, is very important for domain-specific translation. In this paper,\nwe give a comprehensive survey of the state-of-the-art domain adaptation\ntechniques for NMT.\n",
        "published": "2018",
        "authors": [
            "Chenhui Chu",
            "Rui Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.08122v1",
        "title": "A New Approach for Resource Scheduling with Deep Reinforcement Learning",
        "abstract": "  With the rapid development of deep learning, deep reinforcement learning\n(DRL) began to appear in the field of resource scheduling in recent years.\nBased on the previous research on DRL in the literature, we introduce online\nresource scheduling algorithm DeepRM2 and the offline resource scheduling\nalgorithm DeepRM_Off. Compared with the state-of-the-art DRL algorithm DeepRM\nand heuristic algorithms, our proposed algorithms have faster convergence speed\nand better scheduling efficiency with regarding to average slowdown time, job\ncompletion time and rewards.\n",
        "published": "2018",
        "authors": [
            "Yufei Ye",
            "Xiaoqin Ren",
            "Jin Wang",
            "Lingxiao Xu",
            "Wenxia Guo",
            "Wenqiang Huang",
            "Wenhong Tian"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.11761v1",
        "title": "A First Experiment on Including Text Literals in KGloVe",
        "abstract": "  Graph embedding models produce embedding vectors for entities and relations\nin Knowledge Graphs, often without taking literal properties into account. We\nshow an initial idea based on the combination of global graph structure with\nadditional information provided by textual information in properties. Our\ninitial experiment shows that this approach might be useful, but does not\nclearly outperform earlier approaches when evaluated on machine learning tasks.\n",
        "published": "2018",
        "authors": [
            "Michael Cochez",
            "Martina Garofalo",
            "J\u00e9r\u00f4me Len\u00dfen",
            "Maria Angela Pellegrino"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.00516v1",
        "title": "Gaussian Filter in CRF Based Semantic Segmentation",
        "abstract": "  Artificial intelligence is making great changes in academy and industry with\nthe fast development of deep learning, which is a branch of machine learning\nand statistical learning. Fully convolutional network [1] is the standard model\nfor semantic segmentation. Conditional random fields coded as CNN [2] or RNN\n[3] and connected with FCN has been successfully applied in object detection\n[4]. In this paper, we introduce a multi-resolution neural network for FCN and\napply Gaussian filter to the extended CRF kernel neighborhood and the label\nimage to reduce the oscillating effect of CRF neural network segmentation, thus\nachieve higher precision and faster training speed.\n",
        "published": "2017",
        "authors": [
            "Yichi Gu",
            "Qisheng Wu",
            "Jing Li",
            "Kai Cheng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.09994v1",
        "title": "Premise Selection for Theorem Proving by Deep Graph Embedding",
        "abstract": "  We propose a deep learning-based approach to the problem of premise\nselection: selecting mathematical statements relevant for proving a given\nconjecture. We represent a higher-order logic formula as a graph that is\ninvariant to variable renaming but still fully preserves syntactic and semantic\ninformation. We then embed the graph into a vector via a novel embedding method\nthat preserves the information of edge ordering. Our approach achieves\nstate-of-the-art results on the HolStep dataset, improving the classification\naccuracy from 83% to 90.3%.\n",
        "published": "2017",
        "authors": [
            "Mingzhe Wang",
            "Yihe Tang",
            "Jian Wang",
            "Jia Deng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.11355v1",
        "title": "Human-grounded Evaluations of Explanation Methods for Text\n  Classification",
        "abstract": "  Due to the black-box nature of deep learning models, methods for explaining\nthe models' results are crucial to gain trust from humans and support\ncollaboration between AIs and humans. In this paper, we consider several\nmodel-agnostic and model-specific explanation methods for CNNs for text\nclassification and conduct three human-grounded evaluations, focusing on\ndifferent purposes of explanations: (1) revealing model behavior, (2)\njustifying model predictions, and (3) helping humans investigate uncertain\npredictions. The results highlight dissimilar qualities of the various\nexplanation methods we consider and show the degree to which these methods\ncould serve for each purpose.\n",
        "published": "2019",
        "authors": [
            "Piyawat Lertvittayakumjorn",
            "Francesca Toni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.01771v2",
        "title": "Artificial Intelligence, Chaos, Prediction and Understanding in Science",
        "abstract": "  Machine learning and deep learning techniques are contributing much to the\nadvancement of science. Their powerful predictive capabilities appear in\nnumerous disciplines, including chaotic dynamics, but they miss understanding.\nThe main thesis here is that prediction and understanding are two very\ndifferent and important ideas that should guide us about the progress of\nscience. Furthermore, it is emphasized the important role played by that\nnonlinear dynamical systems for the process of understanding. The path of the\nfuture of science will be marked by a constructive dialogue between big data\nand big theory, without which we cannot understand.\n",
        "published": "2020",
        "authors": [
            "Miguel A. F. Sanjuan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.01641v1",
        "title": "Exploring Variational Deep Q Networks",
        "abstract": "  This study provides both analysis and a refined, research-ready\nimplementation of Tang and Kucukelbir's Variational Deep Q Network, a novel\napproach to maximising the efficiency of exploration in complex learning\nenvironments using Variational Bayesian Inference. Alongside reference\nimplementations of both Traditional and Double Deep Q Networks, a small novel\ncontribution is presented - the Double Variational Deep Q Network, which\nincorporates improvements to increase the stability and robustness of\ninference-based learning. Finally, an evaluation and discussion of the\neffectiveness of these approaches is discussed in the wider context of Bayesian\nDeep Learning.\n",
        "published": "2020",
        "authors": [
            "A. H. Bell-Thomas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.03701v1",
        "title": "Differentially Private Deep Learning with Direct Feedback Alignment",
        "abstract": "  Standard methods for differentially private training of deep neural networks\nreplace back-propagated mini-batch gradients with biased and noisy\napproximations to the gradient. These modifications to training often result in\na privacy-preserving model that is significantly less accurate than its\nnon-private counterpart. We hypothesize that alternative training algorithms\nmay be more amenable to differential privacy. Specifically, we examine the\nsuitability of direct feedback alignment (DFA). We propose the first\ndifferentially private method for training deep neural networks with DFA and\nshow that it achieves significant gains in accuracy (often by 10-20%) compared\nto backprop-based differentially private training on a variety of architectures\n(fully connected, convolutional) and datasets.\n",
        "published": "2020",
        "authors": [
            "Jaewoo Lee",
            "Daniel Kifer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.02397v1",
        "title": "A Comprehensive Study on Optimization Strategies for Gradient Descent In\n  Deep Learning",
        "abstract": "  One of the most important parts of Artificial Neural Networks is minimizing\nthe loss functions which tells us how good or bad our model is. To minimize\nthese losses we need to tune the weights and biases. Also to calculate the\nminimum value of a function we need gradient. And to update our weights we need\ngradient descent. But there are some problems with regular gradient descent ie.\nit is quite slow and not that accurate. This article aims to give an\nintroduction to optimization strategies to gradient descent. In addition, we\nshall also discuss the architecture of these algorithms and further\noptimization of Neural Networks in general\n",
        "published": "2021",
        "authors": [
            "Kaustubh Yadav"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.11075v3",
        "title": "Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged\n  Gradient Method for Stochastic Optimization",
        "abstract": "  We introduce MADGRAD, a novel optimization method in the family of AdaGrad\nadaptive gradient methods. MADGRAD shows excellent performance on deep learning\noptimization problems from multiple fields, including classification and\nimage-to-image tasks in vision, and recurrent and bidirectionally-masked models\nin natural language processing. For each of these tasks, MADGRAD matches or\noutperforms both SGD and ADAM in test set performance, even on problems for\nwhich adaptive methods normally perform poorly.\n",
        "published": "2021",
        "authors": [
            "Aaron Defazio",
            "Samy Jelassi"
        ]
    }
]