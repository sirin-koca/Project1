[
    {
        "id": "http://arxiv.org/abs/2008.00386v1",
        "title": "Bayesian Optimization for Selecting Efficient Machine Learning Models",
        "abstract": "  The performance of many machine learning models depends on their\nhyper-parameter settings. Bayesian Optimization has become a successful tool\nfor hyper-parameter optimization of machine learning algorithms, which aims to\nidentify optimal hyper-parameters during an iterative sequential process.\nHowever, most of the Bayesian Optimization algorithms are designed to select\nmodels for effectiveness only and ignore the important issue of model training\nefficiency. Given that both model effectiveness and training time are important\nfor real-world applications, models selected for effectiveness may not meet the\nstrict training time requirements necessary to deploy in a production\nenvironment. In this work, we present a unified Bayesian Optimization framework\nfor jointly optimizing models for both prediction effectiveness and training\nefficiency. We propose an objective that captures the tradeoff between these\ntwo metrics and demonstrate how we can jointly optimize them in a principled\nBayesian Optimization framework. Experiments on model selection for\nrecommendation tasks indicate models selected this way significantly improves\nmodel training efficiency while maintaining strong effectiveness as compared to\nstate-of-the-art Bayesian Optimization algorithms.\n",
        "published": "2020-08-02",
        "authors": [
            "Lidan Wang",
            "Franck Dernoncourt",
            "Trung Bui"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.02960v2",
        "title": "Sequence-to-Sequence Learning as Beam-Search Optimization",
        "abstract": "  Sequence-to-Sequence (seq2seq) modeling has rapidly become an important\ngeneral-purpose NLP tool that has proven effective for many text-generation and\nsequence-labeling tasks. Seq2seq builds on deep neural language modeling and\ninherits its remarkable accuracy in estimating local, next-word distributions.\nIn this work, we introduce a model and beam-search training scheme, based on\nthe work of Daume III and Marcu (2005), that extends seq2seq to learn global\nsequence scores. This structured approach avoids classical biases associated\nwith local training and unifies the training loss with the test-time usage,\nwhile preserving the proven model architecture of seq2seq and its efficient\ntraining approach. We show that our system outperforms a highly-optimized\nattention-based seq2seq system and other baselines on three different sequence\nto sequence tasks: word ordering, parsing, and machine translation.\n",
        "published": "2016-06-09",
        "authors": [
            "Sam Wiseman",
            "Alexander M. Rush"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1612.02695v1",
        "title": "Towards better decoding and language model integration in sequence to\n  sequence models",
        "abstract": "  The recently proposed Sequence-to-Sequence (seq2seq) framework advocates\nreplacing complex data processing pipelines, such as an entire automatic speech\nrecognition system, with a single neural network trained in an end-to-end\nfashion. In this contribution, we analyse an attention-based seq2seq speech\nrecognition system that directly transcribes recordings into characters. We\nobserve two shortcomings: overconfidence in its predictions and a tendency to\nproduce incomplete transcriptions when language models are used. We propose\npractical solutions to both problems achieving competitive speaker independent\nword error rates on the Wall Street Journal dataset: without separate language\nmodels we reach 10.6% WER, while together with a trigram language model, we\nreach 6.7% WER.\n",
        "published": "2016-12-08",
        "authors": [
            "Jan Chorowski",
            "Navdeep Jaitly"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.07443v1",
        "title": "Multi-view Sentence Representation Learning",
        "abstract": "  Multi-view learning can provide self-supervision when different views are\navailable of the same data. The distributional hypothesis provides another form\nof useful self-supervision from adjacent sentences which are plentiful in large\nunlabelled corpora. Motivated by the asymmetry in the two hemispheres of the\nhuman brain as well as the observation that different learning architectures\ntend to emphasise different aspects of sentence meaning, we create a unified\nmulti-view sentence representation learning framework, in which, one view\nencodes the input sentence with a Recurrent Neural Network (RNN), and the other\nview encodes it with a simple linear model, and the training objective is to\nmaximise the agreement specified by the adjacent context information between\ntwo views. We show that, after training, the vectors produced from our\nmulti-view training provide improved representations over the single-view\ntraining, and the combination of different views gives further representational\nimprovement and demonstrates solid transferability on standard downstream\ntasks.\n",
        "published": "2018-05-18",
        "authors": [
            "Shuai Tang",
            "Virginia R. de Sa"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.07475v3",
        "title": "Learning to Repair Software Vulnerabilities with Generative Adversarial\n  Networks",
        "abstract": "  Motivated by the problem of automated repair of software vulnerabilities, we\npropose an adversarial learning approach that maps from one discrete source\ndomain to another target domain without requiring paired labeled examples or\nsource and target domains to be bijections. We demonstrate that the proposed\nadversarial learning approach is an effective technique for repairing software\nvulnerabilities, performing close to seq2seq approaches that require labeled\npairs. The proposed Generative Adversarial Network approach is\napplication-agnostic in that it can be applied to other problems similar to\ncode repair, such as grammar correction or sentiment translation.\n",
        "published": "2018-05-18",
        "authors": [
            "Jacob Harer",
            "Onur Ozdemir",
            "Tomo Lazovich",
            "Christopher P. Reale",
            "Rebecca L. Russell",
            "Louis Y. Kim",
            "Peter Chin"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.12754v1",
        "title": "Recurrent Attention Unit",
        "abstract": "  Recurrent Neural Network (RNN) has been successfully applied in many sequence\nlearning problems. Such as handwriting recognition, image description, natural\nlanguage processing and video motion analysis. After years of development,\nresearchers have improved the internal structure of the RNN and introduced many\nvariants. Among others, Gated Recurrent Unit (GRU) is one of the most widely\nused RNN model. However, GRU lacks the capability of adaptively paying\nattention to certain regions or locations, so that it may cause information\nredundancy or loss during leaning. In this paper, we propose a RNN model,\ncalled Recurrent Attention Unit (RAU), which seamlessly integrates the\nattention mechanism into the interior of GRU by adding an attention gate. The\nattention gate can enhance GRU's ability to remember long-term memory and help\nmemory cells quickly discard unimportant content. RAU is capable of extracting\ninformation from the sequential data by adaptively selecting a sequence of\nregions or locations and pay more attention to the selected regions during\nlearning. Extensive experiments on image classification, sentiment\nclassification and language modeling show that RAU consistently outperforms GRU\nand other baseline methods.\n",
        "published": "2018-10-30",
        "authors": [
            "Guoqiang Zhong",
            "Guohua Yue",
            "Xiao Ling"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.02338v2",
        "title": "Consistency by Agreement in Zero-shot Neural Machine Translation",
        "abstract": "  Generalization and reliability of multilingual translation often highly\ndepend on the amount of available parallel data for each language pair of\ninterest. In this paper, we focus on zero-shot generalization---a challenging\nsetup that tests models on translation directions they have not been optimized\nfor at training time. To solve the problem, we (i) reformulate multilingual\ntranslation as probabilistic inference, (ii) define the notion of zero-shot\nconsistency and show why standard training often results in models unsuitable\nfor zero-shot tasks, and (iii) introduce a consistent agreement-based training\nmethod that encourages the model to produce equivalent translations of parallel\nsentences in auxiliary languages. We test our multilingual NMT models on\nmultiple public zero-shot translation benchmarks (IWSLT17, UN corpus, Europarl)\nand show that agreement-based learning often results in 2-3 BLEU zero-shot\nimprovement over strong baselines without any loss in performance on supervised\ntranslation directions.\n",
        "published": "2019-04-04",
        "authors": [
            "Maruan Al-Shedivat",
            "Ankur P. Parikh"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.11838v4",
        "title": "Copy mechanism and tailored training for character-based data-to-text\n  generation",
        "abstract": "  In the last few years, many different methods have been focusing on using\ndeep recurrent neural networks for natural language generation. The most widely\nused sequence-to-sequence neural methods are word-based: as such, they need a\npre-processing step called delexicalization (conversely, relexicalization) to\ndeal with uncommon or unknown words. These forms of processing, however, give\nrise to models that depend on the vocabulary used and are not completely\nneural.\n  In this work, we present an end-to-end sequence-to-sequence model with\nattention mechanism which reads and generates at a character level, no longer\nrequiring delexicalization, tokenization, nor even lowercasing. Moreover, since\ncharacters constitute the common \"building blocks\" of every text, it also\nallows a more general approach to text generation, enabling the possibility to\nexploit transfer learning for training. These skills are obtained thanks to two\nmajor features: (i) the possibility to alternate between the standard\ngeneration mechanism and a copy one, which allows to directly copy input facts\nto produce outputs, and (ii) the use of an original training pipeline that\nfurther improves the quality of the generated texts.\n  We also introduce a new dataset called E2E+, designed to highlight the\ncopying capabilities of character-based models, that is a modified version of\nthe well-known E2E dataset used in the E2E Challenge. We tested our model\naccording to five broadly accepted metrics (including the widely used BLEU),\nshowing that it yields competitive performance with respect to both\ncharacter-based and word-based approaches.\n",
        "published": "2019-04-26",
        "authors": [
            "Marco Roberti",
            "Giovanni Bonetta",
            "Rossella Cancelliere",
            "Patrick Gallinari"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.00021v2",
        "title": "Approximating Stacked and Bidirectional Recurrent Architectures with the\n  Delayed Recurrent Neural Network",
        "abstract": "  Recent work has shown that topological enhancements to recurrent neural\nnetworks (RNNs) can increase their expressiveness and representational\ncapacity. Two popular enhancements are stacked RNNs, which increases the\ncapacity for learning non-linear functions, and bidirectional processing, which\nexploits acausal information in a sequence. In this work, we explore the\ndelayed-RNN, which is a single-layer RNN that has a delay between the input and\noutput. We prove that a weight-constrained version of the delayed-RNN is\nequivalent to a stacked-RNN. We also show that the delay gives rise to partial\nacausality, much like bidirectional networks. Synthetic experiments confirm\nthat the delayed-RNN can mimic bidirectional networks, solving some acausal\ntasks similarly, and outperforming them in others. Moreover, we show similar\nperformance to bidirectional networks in a real-world natural language\nprocessing task. These results suggest that delayed-RNNs can approximate\ntopologies including stacked RNNs, bidirectional RNNs, and stacked\nbidirectional RNNs - but with equivalent or faster runtimes for the\ndelayed-RNNs.\n",
        "published": "2019-08-30",
        "authors": [
            "Javier S. Turek",
            "Shailee Jain",
            "Vy Vo",
            "Mihai Capota",
            "Alexander G. Huth",
            "Theodore L. Willke"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.10447v1",
        "title": "On Model Stability as a Function of Random Seed",
        "abstract": "  In this paper, we focus on quantifying model stability as a function of\nrandom seed by investigating the effects of the induced randomness on model\nperformance and the robustness of the model in general. We specifically perform\na controlled study on the effect of random seeds on the behaviour of attention,\ngradient-based and surrogate model based (LIME) interpretations. Our analysis\nsuggests that random seeds can adversely affect the consistency of models\nresulting in counterfactual interpretations. We propose a technique called\nAggressive Stochastic Weight Averaging (ASWA)and an extension called\nNorm-filtered Aggressive Stochastic Weight Averaging (NASWA) which improves the\nstability of models over random seeds. With our ASWA and NASWA based\noptimization, we are able to improve the robustness of the original model, on\naverage reducing the standard deviation of the model's performance by 72%.\n",
        "published": "2019-09-23",
        "authors": [
            "Pranava Madhyastha",
            "Rishabh Jain"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.04979v1",
        "title": "A Cordial Sync: Going Beyond Marginal Policies for Multi-Agent Embodied\n  Tasks",
        "abstract": "  Autonomous agents must learn to collaborate. It is not scalable to develop a\nnew centralized agent every time a task's difficulty outpaces a single agent's\nabilities. While multi-agent collaboration research has flourished in\ngridworld-like environments, relatively little work has considered visually\nrich domains. Addressing this, we introduce the novel task FurnMove in which\nagents work together to move a piece of furniture through a living room to a\ngoal. Unlike existing tasks, FurnMove requires agents to coordinate at every\ntimestep. We identify two challenges when training agents to complete FurnMove:\nexisting decentralized action sampling procedures do not permit expressive\njoint action policies and, in tasks requiring close coordination, the number of\nfailed actions dominates successful actions. To confront these challenges we\nintroduce SYNC-policies (synchronize your actions coherently) and CORDIAL\n(coordination loss). Using SYNC-policies and CORDIAL, our agents achieve a 58%\ncompletion rate on FurnMove, an impressive absolute gain of 25 percentage\npoints over competitive decentralized baselines. Our dataset, code, and\npretrained models are available at https://unnat.github.io/cordial-sync .\n",
        "published": "2020-07-09",
        "authors": [
            "Unnat Jain",
            "Luca Weihs",
            "Eric Kolve",
            "Ali Farhadi",
            "Svetlana Lazebnik",
            "Aniruddha Kembhavi",
            "Alexander Schwing"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.00931v2",
        "title": "GridToPix: Training Embodied Agents with Minimal Supervision",
        "abstract": "  While deep reinforcement learning (RL) promises freedom from hand-labeled\ndata, great successes, especially for Embodied AI, require significant work to\ncreate supervision via carefully shaped rewards. Indeed, without shaped\nrewards, i.e., with only terminal rewards, present-day Embodied AI results\ndegrade significantly across Embodied AI problems from single-agent\nHabitat-based PointGoal Navigation (SPL drops from 55 to 0) and two-agent\nAI2-THOR-based Furniture Moving (success drops from 58% to 1%) to three-agent\nGoogle Football-based 3 vs. 1 with Keeper (game score drops from 0.6 to 0.1).\nAs training from shaped rewards doesn't scale to more realistic tasks, the\ncommunity needs to improve the success of training with terminal rewards. For\nthis we propose GridToPix: 1) train agents with terminal rewards in gridworlds\nthat generically mirror Embodied AI environments, i.e., they are independent of\nthe task; 2) distill the learned policy into agents that reside in complex\nvisual worlds. Despite learning from only terminal rewards with identical\nmodels and RL algorithms, GridToPix significantly improves results across\ntasks: from PointGoal Navigation (SPL improves from 0 to 64) and Furniture\nMoving (success improves from 1% to 25%) to football gameplay (game score\nimproves from 0.1 to 0.6). GridToPix even helps to improve the results of\nshaped reward training.\n",
        "published": "2021-04-14",
        "authors": [
            "Unnat Jain",
            "Iou-Jen Liu",
            "Svetlana Lazebnik",
            "Aniruddha Kembhavi",
            "Luca Weihs",
            "Alexander Schwing"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.02067v2",
        "title": "Learning to Draw: Emergent Communication through Sketching",
        "abstract": "  Evidence that visual communication preceded written language and provided a\nbasis for it goes back to prehistory, in forms such as cave and rock paintings\ndepicting traces of our distant ancestors. Emergent communication research has\nsought to explore how agents can learn to communicate in order to\ncollaboratively solve tasks. Existing research has focused on language, with a\nlearned communication channel transmitting sequences of discrete tokens between\nthe agents. In this work, we explore a visual communication channel between\nagents that are allowed to draw with simple strokes. Our agents are\nparameterised by deep neural networks, and the drawing procedure is\ndifferentiable, allowing for end-to-end training. In the framework of a\nreferential communication game, we demonstrate that agents can not only\nsuccessfully learn to communicate by drawing, but with appropriate inductive\nbiases, can do so in a fashion that humans can interpret. We hope to encourage\nfuture research to consider visual communication as a more flexible and\ndirectly interpretable alternative of training collaborative agents.\n",
        "published": "2021-06-03",
        "authors": [
            "Daniela Mihai",
            "Jonathon Hare"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.00752v1",
        "title": "Flip Learning: Erase to Segment",
        "abstract": "  Nodule segmentation from breast ultrasound images is challenging yet\nessential for the diagnosis. Weakly-supervised segmentation (WSS) can help\nreduce time-consuming and cumbersome manual annotation. Unlike existing\nweakly-supervised approaches, in this study, we propose a novel and general WSS\nframework called Flip Learning, which only needs the box annotation.\nSpecifically, the target in the label box will be erased gradually to flip the\nclassification tag, and the erased region will be considered as the\nsegmentation result finally. Our contribution is three-fold. First, our\nproposed approach erases on superpixel level using a Multi-agent Reinforcement\nLearning framework to exploit the prior boundary knowledge and accelerate the\nlearning process. Second, we design two rewards: classification score and\nintensity distribution reward, to avoid under- and over-segmentation,\nrespectively. Third, we adopt a coarse-to-fine learning strategy to reduce the\nresidual errors and improve the segmentation performance. Extensively validated\non a large dataset, our proposed approach achieves competitive performance and\nshows great potential to narrow the gap between fully-supervised and\nweakly-supervised learning.\n",
        "published": "2021-08-02",
        "authors": [
            "Yuhao Huang",
            "Xin Yang",
            "Yuxin Zou",
            "Chaoyu Chen",
            "Jian Wang",
            "Haoran Dou",
            "Nishant Ravikumar",
            "Alejandro F Frangi",
            "Jianqiao Zhou",
            "Dong Ni"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.05769v1",
        "title": "Interpretation of Emergent Communication in Heterogeneous Collaborative\n  Embodied Agents",
        "abstract": "  Communication between embodied AI agents has received increasing attention in\nrecent years. Despite its use, it is still unclear whether the learned\ncommunication is interpretable and grounded in perception. To study the\ngrounding of emergent forms of communication, we first introduce the\ncollaborative multi-object navigation task CoMON. In this task, an oracle agent\nhas detailed environment information in the form of a map. It communicates with\na navigator agent that perceives the environment visually and is tasked to find\na sequence of goals. To succeed at the task, effective communication is\nessential. CoMON hence serves as a basis to study different communication\nmechanisms between heterogeneous agents, that is, agents with different\ncapabilities and roles. We study two common communication mechanisms and\nanalyze their communication patterns through an egocentric and spatial lens. We\nshow that the emergent communication can be grounded to the agent observations\nand the spatial structure of the 3D environment. Video summary:\nhttps://youtu.be/kLv2rxO9t0g\n",
        "published": "2021-10-12",
        "authors": [
            "Shivansh Patel",
            "Saim Wani",
            "Unnat Jain",
            "Alexander Schwing",
            "Svetlana Lazebnik",
            "Manolis Savva",
            "Angel X. Chang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.10553v2",
        "title": "MABe22: A Multi-Species Multi-Task Benchmark for Learned Representations\n  of Behavior",
        "abstract": "  We introduce MABe22, a large-scale, multi-agent video and trajectory\nbenchmark to assess the quality of learned behavior representations. This\ndataset is collected from a variety of biology experiments, and includes\ntriplets of interacting mice (4.7 million frames video+pose tracking data, 10\nmillion frames pose only), symbiotic beetle-ant interactions (10 million frames\nvideo data), and groups of interacting flies (4.4 million frames of pose\ntracking data). Accompanying these data, we introduce a panel of real-life\ndownstream analysis tasks to assess the quality of learned representations by\nevaluating how well they preserve information about the experimental conditions\n(e.g. strain, time of day, optogenetic stimulation) and animal behavior. We\ntest multiple state-of-the-art self-supervised video and trajectory\nrepresentation learning methods to demonstrate the use of our benchmark,\nrevealing that methods developed using human action datasets do not fully\ntranslate to animal datasets. We hope that our benchmark and dataset encourage\na broader exploration of behavior representation learning methods across\nspecies and settings.\n",
        "published": "2022-07-21",
        "authors": [
            "Jennifer J. Sun",
            "Markus Marks",
            "Andrew Ulmer",
            "Dipam Chakraborty",
            "Brian Geuther",
            "Edward Hayes",
            "Heng Jia",
            "Vivek Kumar",
            "Sebastian Oleszko",
            "Zachary Partridge",
            "Milan Peelman",
            "Alice Robie",
            "Catherine E. Schretter",
            "Keith Sheppard",
            "Chao Sun",
            "Param Uttarwar",
            "Julian M. Wagner",
            "Eric Werner",
            "Joseph Parker",
            "Pietro Perona",
            "Yisong Yue",
            "Kristin Branson",
            "Ann Kennedy"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.07891v1",
        "title": "Emergent Behaviors in Multi-Agent Target Acquisition",
        "abstract": "  Only limited studies and superficial evaluations are available on agents'\nbehaviors and roles within a Multi-Agent System (MAS). We simulate a MAS using\nReinforcement Learning (RL) in a pursuit-evasion (a.k.a predator-prey pursuit)\ngame, which shares task goals with target acquisition, and we create different\nadversarial scenarios by replacing RL-trained pursuers' policies with two\ndistinct (non-RL) analytical strategies. Using heatmaps of agents' positions\n(state-space variable) over time, we are able to categorize an RL-trained\nevader's behaviors. The novelty of our approach entails the creation of an\ninfluential feature set that reveals underlying data regularities, which allow\nus to classify an agent's behavior. This classification may aid in catching the\n(enemy) targets by enabling us to identify and predict their behaviors, and\nwhen extended to pursuers, this approach towards identifying teammates'\nbehavior may allow agents to coordinate more effectively.\n",
        "published": "2022-12-15",
        "authors": [
            "Piyush K. Sharma",
            "Erin Zaroukian",
            "Derrik E. Asher",
            "Bryson Howell"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.17773v1",
        "title": "Graph Convolutional Networks for Complex Traffic Scenario Classification",
        "abstract": "  A scenario-based testing approach can reduce the time required to obtain\nstatistically significant evidence of the safety of Automated Driving Systems\n(ADS). Identifying these scenarios in an automated manner is a challenging\ntask. Most methods on scenario classification do not work for complex scenarios\nwith diverse environments (highways, urban) and interaction with other traffic\nagents. This is mirrored in their approaches which model an individual vehicle\nin relation to its environment, but neglect the interaction between multiple\nvehicles (e.g. cut-ins, stationary lead vehicle). Furthermore, existing\ndatasets lack diversity and do not have per-frame annotations to accurately\nlearn the start and end time of a scenario. We propose a method for complex\ntraffic scenario classification that is able to model the interaction of a\nvehicle with the environment, as well as other agents. We use Graph\nConvolutional Networks to model spatial and temporal aspects of these\nscenarios. Expanding the nuScenes and Argoverse 2 driving datasets, we\nintroduce a scenario-labeled dataset, which covers different driving\nenvironments and is annotated per frame. Training our method on this dataset,\nwe present a promising baseline for future research on per-frame complex\nscenario classification.\n",
        "published": "2023-10-26",
        "authors": [
            "Tobias Hoek",
            "Holger Caesar",
            "Andreas Falkov\u00e9n",
            "Tommy Johansson"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1205.2382v3",
        "title": "Mesh Learning for Classifying Cognitive Processes",
        "abstract": "  A relatively recent advance in cognitive neuroscience has been multi-voxel\npattern analysis (MVPA), which enables researchers to decode brain states\nand/or the type of information represented in the brain during a cognitive\noperation. MVPA methods utilize machine learning algorithms to distinguish\namong types of information or cognitive states represented in the brain, based\non distributed patterns of neural activity. In the current investigation, we\npropose a new approach for representation of neural data for pattern analysis,\nnamely a Mesh Learning Model. In this approach, at each time instant, a star\nmesh is formed around each voxel, such that the voxel corresponding to the\ncenter node is surrounded by its p-nearest neighbors. The arc weights of each\nmesh are estimated from the voxel intensity values by least squares method. The\nestimated arc weights of all the meshes, called Mesh Arc Descriptors (MADs),\nare then used to train a classifier, such as Neural Networks, k-Nearest\nNeighbor, Na\\\"ive Bayes and Support Vector Machines. The proposed Mesh Model\nwas tested on neuroimaging data acquired via functional magnetic resonance\nimaging (fMRI) during a recognition memory experiment using categorized word\nlists, employing a previously established experimental paradigm (\\\"Oztekin &\nBadre, 2011). Results suggest that the proposed Mesh Learning approach can\nprovide an effective algorithm for pattern analysis of brain activity during\ncognitive processing.\n",
        "published": "2012-05-10",
        "authors": [
            "Mete Ozay",
            "Ilke \u00d6ztekin",
            "Uygar \u00d6ztekin",
            "Fatos T. Yarman Vural"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1808.02668v1",
        "title": "An Occam's Razor View on Learning Audiovisual Emotion Recognition with\n  Small Training Sets",
        "abstract": "  This paper presents a light-weight and accurate deep neural model for\naudiovisual emotion recognition. To design this model, the authors followed a\nphilosophy of simplicity, drastically limiting the number of parameters to\nlearn from the target datasets, always choosing the simplest earning methods:\ni) transfer learning and low-dimensional space embedding allows to reduce the\ndimensionality of the representations. ii) The isual temporal information is\nhandled by a simple score-per-frame selection process, averaged across time.\niii) A simple frame selection echanism is also proposed to weight the images of\na sequence. iv) The fusion of the different modalities is performed at\nprediction level (late usion). We also highlight the inherent challenges of the\nAFEW dataset and the difficulty of model selection with as few as 383\nvalidation equences. The proposed real-time emotion classifier achieved a\nstate-of-the-art accuracy of 60.64 % on the test set of AFEW, and ranked 4th at\nhe Emotion in the Wild 2018 challenge.\n",
        "published": "2018-08-08",
        "authors": [
            "Valentin Vielzeuf",
            "Corentin Kervadec",
            "St\u00e9phane Pateux",
            "Alexis Lechervy",
            "Fr\u00e9d\u00e9ric Jurie"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.00081v1",
        "title": "Synthesizing Deep Neural Network Architectures using Biological Synaptic\n  Strength Distributions",
        "abstract": "  In this work, we perform an exploratory study on synthesizing deep neural\nnetworks using biological synaptic strength distributions, and the potential\ninfluence of different distributions on modelling performance particularly for\nthe scenario associated with small data sets. Surprisingly, a CNN with\nconvolutional layer synaptic strengths drawn from biologically-inspired\ndistributions such as log-normal or correlated center-surround distributions\nperformed relatively well suggesting a possibility for designing deep neural\nnetwork architectures that do not require many data samples to learn, and can\nsidestep current training procedures while maintaining or boosting modelling\nperformance.\n",
        "published": "2017-07-01",
        "authors": [
            "A. H. Karimi",
            "M. J. Shafiee",
            "A. Ghodsi",
            "A. Wong"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1704.02081v1",
        "title": "Evolution in Groups: A deeper look at synaptic cluster driven evolution\n  of deep neural networks",
        "abstract": "  A promising paradigm for achieving highly efficient deep neural networks is\nthe idea of evolutionary deep intelligence, which mimics biological evolution\nprocesses to progressively synthesize more efficient networks. A crucial design\nfactor in evolutionary deep intelligence is the genetic encoding scheme used to\nsimulate heredity and determine the architectures of offspring networks. In\nthis study, we take a deeper look at the notion of synaptic cluster-driven\nevolution of deep neural networks which guides the evolution process towards\nthe formation of a highly sparse set of synaptic clusters in offspring\nnetworks. Utilizing a synaptic cluster-driven genetic encoding, the\nprobabilistic encoding of synaptic traits considers not only individual\nsynaptic properties but also inter-synaptic relationships within a deep neural\nnetwork. This process results in highly sparse offspring networks which are\nparticularly tailored for parallel computational devices such as GPUs and deep\nneural network accelerator chips. Comprehensive experimental results using four\nwell-known deep neural network architectures (LeNet-5, AlexNet, ResNet-56, and\nDetectNet) on two different tasks (object categorization and object detection)\ndemonstrate the efficiency of the proposed method. Cluster-driven genetic\nencoding scheme synthesizes networks that can achieve state-of-the-art\nperformance with significantly smaller number of synapses than that of the\noriginal ancestor network. ($\\sim$125-fold decrease in synapses for MNIST).\nFurthermore, the improved cluster efficiency in the generated offspring\nnetworks ($\\sim$9.71-fold decrease in clusters for MNIST and a $\\sim$8.16-fold\ndecrease in clusters for KITTI) is particularly useful for accelerated\nperformance on parallel computing hardware architectures such as those in GPUs\nand deep neural network accelerator chips.\n",
        "published": "2017-04-07",
        "authors": [
            "Mohammad Javad Shafiee",
            "Elnaz Barshan",
            "Alexander Wong"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.01022v2",
        "title": "A novel approach for multi-agent cooperative pursuit to capture grouped\n  evaders",
        "abstract": "  An approach of mobile multi-agent pursuit based on application of\nself-organizing feature map (SOFM) and along with that reinforcement learning\nbased on agent group role membership function (AGRMF) model is proposed. This\nmethod promotes dynamic organization of the pursuers' groups and also makes\npursuers' group evader according to their desire based on SOFM and AGRMF\ntechniques. This helps to overcome the shortcomings of the pursuers that they\ncannot fully reorganize when the goal is too independent in process of AGRMF\nmodels operation. Besides, we also discuss a new reward function. After the\nformation of the group, reinforcement learning is applied to get the optimal\nsolution for each agent. The results of each step in capturing process will\nfinally affect the AGR membership function to speed up the convergence of the\ncompetitive neural network. The experiments result shows that this approach is\nmore effective for the mobile agents to capture evaders.\n",
        "published": "2020-06-01",
        "authors": [
            "Muhammad Zuhair Qadir",
            "Songhao Piao",
            "Haiyang Jiang",
            "Mohammed El Habib Souidi"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.01811v1",
        "title": "Energy-based Potential Games for Joint Motion Forecasting and Control",
        "abstract": "  This work uses game theory as a mathematical framework to address interaction\nmodeling in multi-agent motion forecasting and control. Despite its\ninterpretability, applying game theory to real-world robotics, like automated\ndriving, faces challenges such as unknown game parameters. To tackle these, we\nestablish a connection between differential games, optimal control, and\nenergy-based models, demonstrating how existing approaches can be unified under\nour proposed Energy-based Potential Game formulation. Building upon this, we\nintroduce a new end-to-end learning application that combines neural networks\nfor game-parameter inference with a differentiable game-theoretic optimization\nlayer, acting as an inductive bias. The analysis provides empirical evidence\nthat the game-theoretic layer adds interpretability and improves the predictive\nperformance of various neural network backbones using two simulations and two\nreal-world driving datasets.\n",
        "published": "2023-12-04",
        "authors": [
            "Christopher Diehl",
            "Tobias Klosek",
            "Martin Kr\u00fcger",
            "Nils Murzyn",
            "Timo Osterburg",
            "Torsten Bertram"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.00776v1",
        "title": "Edge Computing based Human-Robot Cognitive Fusion: A Medical Case Study\n  in the Autism Spectrum Disorder Therapy",
        "abstract": "  In recent years, edge computing has served as a paradigm that enables many\nfuture technologies like AI, Robotics, IoT, and high-speed wireless sensor\nnetworks (like 5G) by connecting cloud computing facilities and services to the\nend users. Especially in medical and healthcare applications, it provides\nremote patient monitoring and increases voluminous multimedia. From the\nrobotics angle, robot-assisted therapy (RAT) is an active-assistive robotic\ntechnology in rehabilitation robotics, attracting many researchers to study and\nbenefit people with disability like autism spectrum disorder (ASD) children.\nHowever, the main challenge of RAT is that the model capable of detecting the\naffective states of ASD people exists and can recall individual preferences.\nMoreover, involving expert diagnosis and recommendations to guide robots in\nupdating the therapy approach to adapt to different statuses and scenarios is a\ncrucial part of the ASD therapy process. This paper proposes the architecture\nof edge cognitive computing by combining human experts and assisted robots\ncollaborating in the same framework to help ASD patients with long-term\nsupport. By integrating the real-time computing and analysis of a new cognitive\nrobotic model for ASD therapy, the proposed architecture can achieve a seamless\nremote diagnosis, round-the-clock symptom monitoring, emergency warning,\ntherapy alteration, and advanced assistance.\n",
        "published": "2024-01-01",
        "authors": [
            "Qin Yang"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1206.6230v2",
        "title": "Decentralized Data Fusion and Active Sensing with Mobile Sensors for\n  Modeling and Predicting Spatiotemporal Traffic Phenomena",
        "abstract": "  The problem of modeling and predicting spatiotemporal traffic phenomena over\nan urban road network is important to many traffic applications such as\ndetecting and forecasting congestion hotspots. This paper presents a\ndecentralized data fusion and active sensing (D2FAS) algorithm for mobile\nsensors to actively explore the road network to gather and assimilate the most\ninformative data for predicting the traffic phenomenon. We analyze the time and\ncommunication complexity of D2FAS and demonstrate that it can scale well with a\nlarge number of observations and sensors. We provide a theoretical guarantee on\nits predictive performance to be equivalent to that of a sophisticated\ncentralized sparse approximation for the Gaussian process (GP) model: The\ncomputation of such a sparse approximate GP model can thus be parallelized and\ndistributed among the mobile sensors (in a Google-like MapReduce paradigm),\nthereby achieving efficient and scalable prediction. We also theoretically\nguarantee its active sensing performance that improves under various practical\nenvironmental conditions. Empirical evaluation on real-world urban road network\ndata shows that our D2FAS algorithm is significantly more time-efficient and\nscalable than state-of-the-art centralized algorithms while achieving\ncomparable predictive performance.\n",
        "published": "2012-06-27",
        "authors": [
            "Jie Chen",
            "Kian Hsiang Low",
            "Colin Keng-Yan Tan",
            "Ali Oran",
            "Patrick Jaillet",
            "John M. Dolan",
            "Gaurav S. Sukhatme"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.11882v2",
        "title": "A Visual Communication Map for Multi-Agent Deep Reinforcement Learning",
        "abstract": "  Deep reinforcement learning has been applied successfully to solve various\nreal-world problems and the number of its applications in the multi-agent\nsettings has been increasing. Multi-agent learning distinctly poses significant\nchallenges in the effort to allocate a concealed communication medium. Agents\nreceive thorough knowledge from the medium to determine subsequent actions in a\ndistributed nature. Apparently, the goal is to leverage the cooperation of\nmultiple agents to achieve a designated objective efficiently. Recent studies\ntypically combine a specialized neural network with reinforcement learning to\nenable communication between agents. This approach, however, limits the number\nof agents or necessitates the homogeneity of the system. In this paper, we have\nproposed a more scalable approach that not only deals with a great number of\nagents but also enables collaboration between dissimilar functional agents and\ncompatibly combined with any deep reinforcement learning methods. Specifically,\nwe create a global communication map to represent the status of each agent in\nthe system visually. The visual map and the environmental state are fed to a\nshared-parameter network to train multiple agents concurrently. Finally, we\nselect the Asynchronous Advantage Actor-Critic (A3C) algorithm to demonstrate\nour proposed scheme, namely Visual communication map for Multi-agent A3C\n(VMA3C). Simulation results show that the use of visual communication map\nimproves the performance of A3C regarding learning speed, reward achievement,\nand robustness in multi-agent problems.\n",
        "published": "2020-02-27",
        "authors": [
            "Ngoc Duy Nguyen",
            "Thanh Thi Nguyen",
            "Doug Creighton",
            "Saeid Nahavandi"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.00165v2",
        "title": "FireCommander: An Interactive, Probabilistic Multi-agent Environment for\n  Heterogeneous Robot Teams",
        "abstract": "  The purpose of this tutorial is to help individuals use the\n\\underline{FireCommander} game environment for research applications. The\nFireCommander is an interactive, probabilistic joint perception-action\nreconnaissance environment in which a composite team of agents (e.g., robots)\ncooperate to fight dynamic, propagating firespots (e.g., targets). In\nFireCommander game, a team of agents must be tasked to optimally deal with a\nwildfire situation in an environment with propagating fire areas and some\nfacilities such as houses, hospitals, power stations, etc. The team of agents\ncan accomplish their mission by first sensing (e.g., estimating fire states),\ncommunicating the sensed fire-information among each other and then taking\naction to put the firespots out based on the sensed information (e.g., dropping\nwater on estimated fire locations). The FireCommander environment can be useful\nfor research topics spanning a wide range of applications from Reinforcement\nLearning (RL) and Learning from Demonstration (LfD), to Coordination,\nPsychology, Human-Robot Interaction (HRI) and Teaming. There are four important\nfacets of the FireCommander environment that overall, create a non-trivial\ngame: (1) Complex Objectives: Multi-objective Stochastic Environment,\n(2)Probabilistic Environment: Agents' actions result in probabilistic\nperformance, (3) Hidden Targets: Partially Observable Environment and, (4)\nUni-task Robots: Perception-only and Action-only agents. The FireCommander\nenvironment is first-of-its-kind in terms of including Perception-only and\nAction-only agents for coordination. It is a general multi-purpose game that\ncan be useful in a variety of combinatorial optimization problems and\nstochastic games, such as applications of Reinforcement Learning (RL), Learning\nfrom Demonstration (LfD) and Inverse RL (iRL).\n",
        "published": "2020-10-31",
        "authors": [
            "Esmaeil Seraj",
            "Xiyang Wu",
            "Matthew Gombolay"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.04679v4",
        "title": "Self-Adaptive Swarm System (SASS)",
        "abstract": "  Distributed artificial intelligence (DAI) studies artificial intelligence\nentities working together to reason, plan, solve problems, organize behaviors\nand strategies, make collective decisions and learn. This Ph.D. research\nproposes a principled Multi-Agent Systems (MAS) cooperation framework --\nSelf-Adaptive Swarm System (SASS) -- to bridge the fourth level automation gap\nbetween perception, communication, planning, execution, decision-making, and\nlearning.\n",
        "published": "2021-05-25",
        "authors": [
            "Qin Yang"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.00882v1",
        "title": "MACRPO: Multi-Agent Cooperative Recurrent Policy Optimization",
        "abstract": "  This work considers the problem of learning cooperative policies in\nmulti-agent settings with partially observable and non-stationary environments\nwithout a communication channel. We focus on improving information sharing\nbetween agents and propose a new multi-agent actor-critic method called\n\\textit{Multi-Agent Cooperative Recurrent Proximal Policy Optimization}\n(MACRPO). We propose two novel ways of integrating information across agents\nand time in MACRPO: First, we use a recurrent layer in critic's network\narchitecture and propose a new framework to use a meta-trajectory to train the\nrecurrent layer. This allows the network to learn the cooperation and dynamics\nof interactions between agents, and also handle partial observability. Second,\nwe propose a new advantage function that incorporates other agents' rewards and\nvalue functions. We evaluate our algorithm on three challenging multi-agent\nenvironments with continuous and discrete action spaces, Deepdrive-Zero,\nMulti-Walker, and Particle environment. We compare the results with several\nablations and state-of-the-art multi-agent algorithms such as QMIX and MADDPG\nand also single-agent methods with shared parameters between agents such as\nIMPALA and APEX. The results show superior performance against other\nalgorithms. The code is available online at\nhttps://github.com/kargarisaac/macrpo.\n",
        "published": "2021-09-02",
        "authors": [
            "Eshagh Kargar",
            "Ville Kyrki"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.06380v1",
        "title": "Near-Optimal Multi-Agent Learning for Safe Coverage Control",
        "abstract": "  In multi-agent coverage control problems, agents navigate their environment\nto reach locations that maximize the coverage of some density. In practice, the\ndensity is rarely known $\\textit{a priori}$, further complicating the original\nNP-hard problem. Moreover, in many applications, agents cannot visit arbitrary\nlocations due to $\\textit{a priori}$ unknown safety constraints. In this paper,\nwe aim to efficiently learn the density to approximately solve the coverage\nproblem while preserving the agents' safety. We first propose a conditionally\nlinear submodular coverage function that facilitates theoretical analysis.\nUtilizing this structure, we develop MacOpt, a novel algorithm that efficiently\ntrades off the exploration-exploitation dilemma due to partial observability,\nand show that it achieves sublinear regret. Next, we extend results on\nsingle-agent safe exploration to our multi-agent setting and propose SafeMac\nfor safe coverage and exploration. We analyze SafeMac and give first of its\nkind results: near optimal coverage in finite time while provably guaranteeing\nsafety. We extensively evaluate our algorithms on synthetic and real problems,\nincluding a bio-diversity monitoring task under safety constraints, where\nSafeMac outperforms competing methods.\n",
        "published": "2022-10-12",
        "authors": [
            "Manish Prajapat",
            "Matteo Turchetta",
            "Melanie N. Zeilinger",
            "Andreas Krause"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.01274v1",
        "title": "BRNES: Enabling Security and Privacy-aware Experience Sharing in\n  Multiagent Robotic and Autonomous Systems",
        "abstract": "  Although experience sharing (ES) accelerates multiagent reinforcement\nlearning (MARL) in an advisor-advisee framework, attempts to apply ES to\ndecentralized multiagent systems have so far relied on trusted environments and\noverlooked the possibility of adversarial manipulation and inference.\nNevertheless, in a real-world setting, some Byzantine attackers, disguised as\nadvisors, may provide false advice to the advisee and catastrophically degrade\nthe overall learning performance. Also, an inference attacker, disguised as an\nadvisee, may conduct several queries to infer the advisors' private information\nand make the entire ES process questionable in terms of privacy leakage. To\naddress and tackle these issues, we propose a novel MARL framework (BRNES) that\nheuristically selects a dynamic neighbor zone for each advisee at each learning\nstep and adopts a weighted experience aggregation technique to reduce Byzantine\nattack impact. Furthermore, to keep the agent's private information safe from\nadversarial inference attacks, we leverage the local differential privacy\n(LDP)-induced noise during the ES process. Our experiments show that our\nframework outperforms the state-of-the-art in terms of the steps to goal,\nobtained reward, and time to goal metrics. Particularly, our evaluation shows\nthat the proposed framework is 8.32x faster than the current non-private\nframeworks and 1.41x faster than the private frameworks in an adversarial\nsetting.\n",
        "published": "2023-08-02",
        "authors": [
            "Md Tamjid Hossain",
            "Hung Manh La",
            "Shahriar Badsha",
            "Anton Netchaev"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.16539v2",
        "title": "On a Connection between Differential Games, Optimal Control, and\n  Energy-based Models for Multi-Agent Interactions",
        "abstract": "  Game theory offers an interpretable mathematical framework for modeling\nmulti-agent interactions. However, its applicability in real-world robotics\napplications is hindered by several challenges, such as unknown agents'\npreferences and goals. To address these challenges, we show a connection\nbetween differential games, optimal control, and energy-based models and\ndemonstrate how existing approaches can be unified under our proposed\nEnergy-based Potential Game formulation. Building upon this formulation, this\nwork introduces a new end-to-end learning application that combines neural\nnetworks for game-parameter inference with a differentiable game-theoretic\noptimization layer, acting as an inductive bias. The experiments using\nsimulated mobile robot pedestrian interactions and real-world automated driving\ndata provide empirical evidence that the game-theoretic layer improves the\npredictive performance of various neural network backbones.\n",
        "published": "2023-08-31",
        "authors": [
            "Christopher Diehl",
            "Tobias Klosek",
            "Martin Kr\u00fcger",
            "Nils Murzyn",
            "Torsten Bertram"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.13189v1",
        "title": "Can Visual Context Improve Automatic Speech Recognition for an Embodied\n  Agent?",
        "abstract": "  The usage of automatic speech recognition (ASR) systems are becoming\nomnipresent ranging from personal assistant to chatbots, home, and industrial\nautomation systems, etc. Modern robots are also equipped with ASR capabilities\nfor interacting with humans as speech is the most natural interaction modality.\nHowever, ASR in robots faces additional challenges as compared to a personal\nassistant. Being an embodied agent, a robot must recognize the physical\nentities around it and therefore reliably recognize the speech containing the\ndescription of such entities. However, current ASR systems are often unable to\ndo so due to limitations in ASR training, such as generic datasets and\nopen-vocabulary modeling. Also, adverse conditions during inference, such as\nnoise, accented, and far-field speech makes the transcription inaccurate. In\nthis work, we present a method to incorporate a robot's visual information into\nan ASR system and improve the recognition of a spoken utterance containing a\nvisible entity. Specifically, we propose a new decoder biasing technique to\nincorporate the visual context while ensuring the ASR output does not degrade\nfor incorrect context. We achieve a 59% relative reduction in WER from an\nunmodified ASR system.\n",
        "published": "2022-10-21",
        "authors": [
            "Pradip Pramanick",
            "Chayan Sarkar"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.05976v1",
        "title": "An evolutionary model of personality traits related to cooperative\n  behavior using a large language model",
        "abstract": "  This paper aims to shed light on the evolutionary dynamics of diverse and\nsocial populations by introducing the rich expressiveness of generative models\ninto the trait expression of social agent-based evolutionary models.\nSpecifically, we focus on the evolution of personality traits in the context of\na game-theoretic relationship as a situation in which inter-individual\ninterests exert strong selection pressures. We construct an agent model in\nwhich linguistic descriptions of personality traits related to cooperative\nbehavior are used as genes. The deterministic strategies extracted from Large\nLanguage Model (LLM) that make behavioral decisions based on these personality\ntraits are used as behavioral traits. The population is evolved according to\nselection based on average payoff and mutation of genes by asking LLM to\nslightly modify the parent gene toward cooperative or selfish. Through\npreliminary experiments and analyses, we clarify that such a model can indeed\nexhibit the evolution of cooperative behavior based on the diverse and\nhigher-order representation of personality traits. We also observed the\nrepeated intrusion of cooperative and selfish personality traits through\nchanges in the expression of personality traits, and found that the emerging\nwords in the evolved gene well reflected the behavioral tendency of its\npersonality in terms of their semantics.\n",
        "published": "2023-10-03",
        "authors": [
            "Reiji Suzuki",
            "Takaya Arita"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.3714v2",
        "title": "Feature Weight Tuning for Recursive Neural Networks",
        "abstract": "  This paper addresses how a recursive neural network model can automatically\nleave out useless information and emphasize important evidence, in other words,\nto perform \"weight tuning\" for higher-level representation acquisition. We\npropose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural\nNetwork (BENN), which automatically control how much one specific unit\ncontributes to the higher-level representation. The proposed model can be\nviewed as incorporating a more powerful compositional function for embedding\nacquisition in recursive neural networks. Experimental results demonstrate the\nsignificant improvement over standard neural models.\n",
        "published": "2014-12-11",
        "authors": [
            "Jiwei Li"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1505.04771v2",
        "title": "DopeLearning: A Computational Approach to Rap Lyrics Generation",
        "abstract": "  Writing rap lyrics requires both creativity to construct a meaningful,\ninteresting story and lyrical skills to produce complex rhyme patterns, which\nform the cornerstone of good flow. We present a rap lyrics generation method\nthat captures both of these aspects. First, we develop a prediction model to\nidentify the next line of existing lyrics from a set of candidate next lines.\nThis model is based on two machine-learning techniques: the RankSVM algorithm\nand a deep neural network model with a novel structure. Results show that the\nprediction model can identify the true next line among 299 randomly selected\nlines with an accuracy of 17%, i.e., over 50 times more likely than by random.\nSecond, we employ the prediction model to combine lines from existing songs,\nproducing lyrics with rhyme and a meaning. An evaluation of the produced lyrics\nshows that in terms of quantitative rhyme density, the method outperforms the\nbest human rappers by 21%. The rap lyrics generator has been deployed as an\nonline tool called DeepBeat, and the performance of the tool has been assessed\nby analyzing its usage logs. This analysis shows that machine-learned rankings\ncorrelate with user preferences.\n",
        "published": "2015-05-18",
        "authors": [
            "Eric Malmi",
            "Pyry Takala",
            "Hannu Toivonen",
            "Tapani Raiko",
            "Aristides Gionis"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.00195v1",
        "title": "Recurrent Neural Networks with External Memory for Language\n  Understanding",
        "abstract": "  Recurrent Neural Networks (RNNs) have become increasingly popular for the\ntask of language understanding. In this task, a semantic tagger is deployed to\nassociate a semantic label to each word in an input sequence. The success of\nRNN may be attributed to its ability to memorize long-term dependence that\nrelates the current-time semantic label prediction to the observations many\ntime instances away. However, the memory capacity of simple RNNs is limited\nbecause of the gradient vanishing and exploding problem. We propose to use an\nexternal memory to improve memorization capability of RNNs. We conducted\nexperiments on the ATIS dataset, and observed that the proposed model was able\nto achieve the state-of-the-art results. We compare our proposed model with\nalternative models and report analysis results that may provide insights for\nfuture research.\n",
        "published": "2015-05-31",
        "authors": [
            "Baolin Peng",
            "Kaisheng Yao"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.06714v1",
        "title": "A Neural Network Approach to Context-Sensitive Generation of\n  Conversational Responses",
        "abstract": "  We present a novel response generation system that can be trained end to end\non large quantities of unstructured Twitter conversations. A neural network\narchitecture is used to address sparsity issues that arise when integrating\ncontextual information into classic statistical models, allowing the system to\ntake into account previous dialog utterances. Our dynamic-context generative\nmodels show consistent gains over both context-sensitive and\nnon-context-sensitive Machine Translation and Information Retrieval baselines.\n",
        "published": "2015-06-22",
        "authors": [
            "Alessandro Sordoni",
            "Michel Galley",
            "Michael Auli",
            "Chris Brockett",
            "Yangfeng Ji",
            "Margaret Mitchell",
            "Jian-Yun Nie",
            "Jianfeng Gao",
            "Bill Dolan"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.08909v3",
        "title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured\n  Multi-Turn Dialogue Systems",
        "abstract": "  This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost\n1 million multi-turn dialogues, with a total of over 7 million utterances and\n100 million words. This provides a unique resource for research into building\ndialogue managers based on neural language models that can make use of large\namounts of unlabeled data. The dataset has both the multi-turn property of\nconversations in the Dialog State Tracking Challenge datasets, and the\nunstructured nature of interactions from microblog services such as Twitter. We\nalso describe two neural learning architectures suitable for analyzing this\ndataset, and provide benchmark performance on the task of selecting the best\nnext response.\n",
        "published": "2015-06-30",
        "authors": [
            "Ryan Lowe",
            "Nissan Pow",
            "Iulian Serban",
            "Joelle Pineau"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1507.04808v3",
        "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical\n  Neural Network Models",
        "abstract": "  We investigate the task of building open domain, conversational dialogue\nsystems based on large dialogue corpora using generative models. Generative\nmodels produce system responses that are autonomously generated word-by-word,\nopening up the possibility for realistic, flexible interactions. In support of\nthis goal, we extend the recently proposed hierarchical recurrent\nencoder-decoder neural network to the dialogue domain, and demonstrate that\nthis model is competitive with state-of-the-art neural language models and\nback-off n-gram models. We investigate the limitations of this and similar\napproaches, and show how its performance can be improved by bootstrapping the\nlearning from a larger question-answer pair corpus and from pretrained word\nembeddings.\n",
        "published": "2015-07-17",
        "authors": [
            "Iulian V. Serban",
            "Alessandro Sordoni",
            "Yoshua Bengio",
            "Aaron Courville",
            "Joelle Pineau"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1508.04395v2",
        "title": "End-to-End Attention-based Large Vocabulary Speech Recognition",
        "abstract": "  Many of the current state-of-the-art Large Vocabulary Continuous Speech\nRecognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov\nModels (HMMs). Most of these systems contain separate components that deal with\nthe acoustic modelling, language modelling and sequence decoding. We\ninvestigate a more direct approach in which the HMM is replaced with a\nRecurrent Neural Network (RNN) that performs sequence prediction directly at\nthe character level. Alignment between the input features and the desired\ncharacter sequence is learned automatically by an attention mechanism built\ninto the RNN. For each predicted character, the attention mechanism scans the\ninput sequence and chooses relevant frames. We propose two methods to speed up\nthis operation: limiting the scan to a subset of most promising frames and\npooling over time the information contained in neighboring frames, thereby\nreducing source sequence length. Integrating an n-gram language model into the\ndecoding process yields recognition accuracies similar to other HMM-free\nRNN-based approaches.\n",
        "published": "2015-08-18",
        "authors": [
            "Dzmitry Bahdanau",
            "Jan Chorowski",
            "Dmitriy Serdyuk",
            "Philemon Brakel",
            "Yoshua Bengio"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1508.05508v1",
        "title": "Towards Neural Network-based Reasoning",
        "abstract": "  We propose Neural Reasoner, a framework for neural network-based reasoning\nover natural language sentences. Given a question, Neural Reasoner can infer\nover multiple supporting facts and find an answer to the question in specific\nforms. Neural Reasoner has 1) a specific interaction-pooling mechanism,\nallowing it to examine multiple facts, and 2) a deep architecture, allowing it\nto model the complicated logical relations in reasoning tasks. Assuming no\nparticular structure exists in the question and facts, Neural Reasoner is able\nto accommodate different types of reasoning and different forms of language\nexpressions. Despite the model complexity, Neural Reasoner can still be trained\neffectively in an end-to-end manner. Our empirical studies show that Neural\nReasoner can outperform existing neural reasoning systems with remarkable\nmargins on two difficult artificial tasks (Positional Reasoning and Path\nFinding) proposed in [8]. For example, it improves the accuracy on Path\nFinding(10K) from 33.4% [6] to over 98%.\n",
        "published": "2015-08-22",
        "authors": [
            "Baolin Peng",
            "Zhengdong Lu",
            "Hang Li",
            "Kam-Fai Wong"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1509.00838v2",
        "title": "What to talk about and how? Selective Generation using LSTMs with\n  Coarse-to-Fine Alignment",
        "abstract": "  We propose an end-to-end, domain-independent neural encoder-aligner-decoder\nmodel for selective generation, i.e., the joint task of content selection and\nsurface realization. Our model first encodes a full set of over-determined\ndatabase event records via an LSTM-based recurrent neural network, then\nutilizes a novel coarse-to-fine aligner to identify the small subset of salient\nrecords to talk about, and finally employs a decoder to generate free-form\ndescriptions of the aligned, selected records. Our model achieves the best\nselection and generation results reported to-date (with 59% relative\nimprovement in generation) on the benchmark WeatherGov dataset, despite using\nno specialized features or linguistic resources. Using an improved k-nearest\nneighbor beam filter helps further. We also perform a series of ablations and\nvisualizations to elucidate the contributions of our key model components.\nLastly, we evaluate the generalizability of our model on the RoboCup dataset,\nand get results that are competitive with or better than the state-of-the-art,\ndespite being severely data-starved.\n",
        "published": "2015-09-02",
        "authors": [
            "Hongyuan Mei",
            "Mohit Bansal",
            "Matthew R. Walter"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1509.06664v4",
        "title": "Reasoning about Entailment with Neural Attention",
        "abstract": "  While most approaches to automatically recognizing entailment relations have\nused classifiers employing hand engineered features derived from complex\nnatural language processing pipelines, in practice their performance has been\nonly slightly better than bag-of-word pair classifiers using only lexical\nsimilarity. The only attempt so far to build an end-to-end differentiable\nneural network for entailment failed to outperform such a simple similarity\nclassifier. In this paper, we propose a neural model that reads two sentences\nto determine entailment using long short-term memory units. We extend this\nmodel with a word-by-word neural attention mechanism that encourages reasoning\nover entailments of pairs of words and phrases. Furthermore, we present a\nqualitative analysis of attention weights produced by this model, demonstrating\nsuch reasoning capabilities. On a large entailment dataset this model\noutperforms the previous best neural model and a classifier with engineered\nfeatures by a substantial margin. It is the first generic end-to-end\ndifferentiable system that achieves state-of-the-art accuracy on a textual\nentailment dataset.\n",
        "published": "2015-09-22",
        "authors": [
            "Tim Rockt\u00e4schel",
            "Edward Grefenstette",
            "Karl Moritz Hermann",
            "Tom\u00e1\u0161 Ko\u010disk\u00fd",
            "Phil Blunsom"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.06127v4",
        "title": "Sentence Pair Scoring: Towards Unified Framework for Text Comprehension",
        "abstract": "  We review the task of Sentence Pair Scoring, popular in the literature in\nvarious forms - viewed as Answer Sentence Selection, Semantic Text Scoring,\nNext Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a\ncomponent of Memory Networks.\n  We argue that all such tasks are similar from the model perspective and\npropose new baselines by comparing the performance of common IR metrics and\npopular convolutional, recurrent and attention-based neural models across many\nSentence Pair Scoring tasks and datasets. We discuss the problem of evaluating\nrandomized models, propose a statistically grounded methodology, and attempt to\nimprove comparisons by releasing new datasets that are much harder than some of\nthe currently used well explored benchmarks. We introduce a unified open source\nsoftware framework with easily pluggable models and tasks, which enables us to\nexperiment with multi-task reusability of trained sentence model. We set a new\nstate-of-art in performance on the Ubuntu Dialogue dataset.\n",
        "published": "2016-03-19",
        "authors": [
            "Petr Baudi\u0161",
            "Jan Pichl",
            "Tom\u00e1\u0161 Vysko\u010dil",
            "Jan \u0160ediv\u00fd"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.06393v3",
        "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning",
        "abstract": "  We address an important problem in sequence-to-sequence (Seq2Seq) learning\nreferred to as copying, in which certain segments in the input sequence are\nselectively replicated in the output sequence. A similar phenomenon is\nobservable in human language communication. For example, humans tend to repeat\nentity names or even long phrases in conversation. The challenge with regard to\ncopying in Seq2Seq is that new machinery is needed to decide when to perform\nthe operation. In this paper, we incorporate copying into neural network-based\nSeq2Seq learning and propose a new model called CopyNet with encoder-decoder\nstructure. CopyNet can nicely integrate the regular way of word generation in\nthe decoder with the new copying mechanism which can choose sub-sequences in\nthe input sequence and put them at proper places in the output sequence. Our\nempirical study on both synthetic data sets and real world data sets\ndemonstrates the efficacy of CopyNet. For example, CopyNet can outperform\nregular RNN-based model with remarkable margins on text summarization tasks.\n",
        "published": "2016-03-21",
        "authors": [
            "Jiatao Gu",
            "Zhengdong Lu",
            "Hang Li",
            "Victor O. K. Li"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.06807v2",
        "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M\n  Factoid Question-Answer Corpus",
        "abstract": "  Over the past decade, large-scale supervised learning corpora have enabled\nmachine learning researchers to make substantial advances. However, to this\ndate, there are no large-scale question-answer corpora available. In this paper\nwe present the 30M Factoid Question-Answer Corpus, an enormous question answer\npair corpus produced by applying a novel neural network architecture on the\nknowledge base Freebase to transduce facts into natural language questions. The\nproduced question answer pairs are evaluated both by human evaluators and using\nautomatic evaluation metrics, including well-established machine translation\nand sentence similarity metrics. Across all evaluation criteria the\nquestion-generation model outperforms the competing template-based baseline.\nFurthermore, when presented to human evaluators, the generated questions appear\ncomparable in quality to real human-generated questions.\n",
        "published": "2016-03-22",
        "authors": [
            "Iulian Vlad Serban",
            "Alberto Garc\u00eda-Dur\u00e1n",
            "Caglar Gulcehre",
            "Sungjin Ahn",
            "Sarath Chandar",
            "Aaron Courville",
            "Yoshua Bengio"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.08023v2",
        "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of\n  Unsupervised Evaluation Metrics for Dialogue Response Generation",
        "abstract": "  We investigate evaluation metrics for dialogue response generation systems\nwhere supervised labels, such as task completion, are not available. Recent\nworks in response generation have adopted metrics from machine translation to\ncompare a model's generated response to a single target response. We show that\nthese metrics correlate very weakly with human judgements in the non-technical\nTwitter domain, and not at all in the technical Ubuntu domain. We provide\nquantitative and qualitative results highlighting specific weaknesses in\nexisting metrics, and provide recommendations for future development of better\nautomatic evaluation metrics for dialogue systems.\n",
        "published": "2016-03-25",
        "authors": [
            "Chia-Wei Liu",
            "Ryan Lowe",
            "Iulian V. Serban",
            "Michael Noseworthy",
            "Laurent Charlin",
            "Joelle Pineau"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1605.06069v3",
        "title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating\n  Dialogues",
        "abstract": "  Sequential data often possesses a hierarchical structure with complex\ndependencies between subsequences, such as found between the utterances in a\ndialogue. In an effort to model this kind of generative process, we propose a\nneural network-based generative architecture, with latent stochastic variables\nthat span a variable number of time steps. We apply the proposed model to the\ntask of dialogue response generation and compare it with recent neural network\narchitectures. We evaluate the model performance through automatic evaluation\nmetrics and by carrying out a human evaluation. The experiments demonstrate\nthat our model improves upon recently proposed models and that the latent\nvariables facilitate the generation of long outputs and maintain the context.\n",
        "published": "2016-05-19",
        "authors": [
            "Iulian Vlad Serban",
            "Alessandro Sordoni",
            "Ryan Lowe",
            "Laurent Charlin",
            "Joelle Pineau",
            "Aaron Courville",
            "Yoshua Bengio"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1702.04521v1",
        "title": "Frustratingly Short Attention Spans in Neural Language Modeling",
        "abstract": "  Neural language models predict the next token using a latent representation\nof the immediate token history. Recently, various methods for augmenting neural\nlanguage models with an attention mechanism over a differentiable memory have\nbeen proposed. For predicting the next token, these models query information\nfrom a memory of the recent history which can facilitate learning mid- and\nlong-range dependencies. However, conventional attention mechanisms used in\nmemory-augmented neural language models produce a single output vector per time\nstep. This vector is used both for predicting the next token as well as for the\nkey and value of a differentiable memory of a token history. In this paper, we\npropose a neural language model with a key-value attention mechanism that\noutputs separate representations for the key and value of a differentiable\nmemory, as well as for encoding the next-word distribution. This model\noutperforms existing memory-augmented neural language models on two corpora.\nYet, we found that our method mainly utilizes a memory of the five most recent\noutput representations. This led to the unexpected main finding that a much\nsimpler model based only on the concatenation of recent output representations\nfrom previous time steps is on par with more sophisticated memory-augmented\nneural language models.\n",
        "published": "2017-02-15",
        "authors": [
            "Micha\u0142 Daniluk",
            "Tim Rockt\u00e4schel",
            "Johannes Welbl",
            "Sebastian Riedel"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.03130v1",
        "title": "A Structured Self-attentive Sentence Embedding",
        "abstract": "  This paper proposes a new model for extracting an interpretable sentence\nembedding by introducing self-attention. Instead of using a vector, we use a\n2-D matrix to represent the embedding, with each row of the matrix attending on\na different part of the sentence. We also propose a self-attention mechanism\nand a special regularization term for the model. As a side effect, the\nembedding comes with an easy way of visualizing what specific parts of the\nsentence are encoded into the embedding. We evaluate our model on 3 different\ntasks: author profiling, sentiment classification, and textual entailment.\nResults show that our model yields a significant performance gain compared to\nother sentence embedding methods in all of the 3 tasks.\n",
        "published": "2017-03-09",
        "authors": [
            "Zhouhan Lin",
            "Minwei Feng",
            "Cicero Nogueira dos Santos",
            "Mo Yu",
            "Bing Xiang",
            "Bowen Zhou",
            "Yoshua Bengio"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1706.01331v3",
        "title": "Event Representations for Automated Story Generation with Deep Neural\n  Nets",
        "abstract": "  Automated story generation is the problem of automatically selecting a\nsequence of events, actions, or words that can be told as a story. We seek to\ndevelop a system that can generate stories by learning everything it needs to\nknow from textual story corpora. To date, recurrent neural networks that learn\nlanguage models at character, word, or sentence levels have had little success\ngenerating coherent stories. We explore the question of event representations\nthat provide a mid-level of abstraction between words and sentences in order to\nretain the semantic information of the original data while minimizing event\nsparsity. We present a technique for preprocessing textual story data into\nevent sequences. We then present a technique for automated story generation\nwhereby we decompose the problem into the generation of successive events\n(event2event) and the generation of natural language sentences from events\n(event2sentence). We give empirical results comparing different event\nrepresentations and their effects on event successor generation and the\ntranslation of events to natural language.\n",
        "published": "2017-06-05",
        "authors": [
            "Lara J. Martin",
            "Prithviraj Ammanabrolu",
            "Xinyu Wang",
            "William Hancock",
            "Shruti Singh",
            "Brent Harrison",
            "Mark O. Riedl"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1706.01450v1",
        "title": "A Joint Model for Question Answering and Question Generation",
        "abstract": "  We propose a generative machine comprehension model that learns jointly to\nask and answer questions based on documents. The proposed model uses a\nsequence-to-sequence framework that encodes the document and generates a\nquestion (answer) given an answer (question). Significant improvement in model\nperformance is observed empirically on the SQuAD corpus, confirming our\nhypothesis that the model benefits from jointly learning to perform both tasks.\nWe believe the joint model's novelty offers a new perspective on machine\ncomprehension beyond architectural engineering, and serves as a first step\ntowards autonomous information seeking.\n",
        "published": "2017-06-05",
        "authors": [
            "Tong Wang",
            "Xingdi Yuan",
            "Adam Trischler"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.06176v3",
        "title": "Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy\n  Learning",
        "abstract": "  Training a task-completion dialogue agent via reinforcement learning (RL) is\ncostly because it requires many interactions with real users. One common\nalternative is to use a user simulator. However, a user simulator usually lacks\nthe language complexity of human interlocutors and the biases in its design may\ntend to degrade the agent. To address these issues, we present Deep Dyna-Q,\nwhich to our knowledge is the first deep RL framework that integrates planning\nfor task-completion dialogue policy learning. We incorporate into the dialogue\nagent a model of the environment, referred to as the world model, to mimic real\nuser response and generate simulated experience. During dialogue policy\nlearning, the world model is constantly updated with real user experience to\napproach real user behavior, and in turn, the dialogue agent is optimized using\nboth real experience and simulated experience. The effectiveness of our\napproach is demonstrated on a movie-ticket booking task in both simulated and\nhuman-in-the-loop settings.\n",
        "published": "2018-01-18",
        "authors": [
            "Baolin Peng",
            "Xiujun Li",
            "Jianfeng Gao",
            "Jingjing Liu",
            "Kam-Fai Wong",
            "Shang-Yu Su"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.07089v2",
        "title": "Attentive Tensor Product Learning",
        "abstract": "  This paper proposes a new architecture - Attentive Tensor Product Learning\n(ATPL) - to represent grammatical structures in deep learning models. ATPL is a\nnew architecture to bridge this gap by exploiting Tensor Product\nRepresentations (TPR), a structured neural-symbolic model developed in\ncognitive science, aiming to integrate deep learning with explicit language\nstructures and rules. The key ideas of ATPL are: 1) unsupervised learning of\nrole-unbinding vectors of words via TPR-based deep neural network; 2) employing\nattention modules to compute TPR; and 3) integration of TPR with typical deep\nlearning architectures including Long Short-Term Memory (LSTM) and Feedforward\nNeural Network (FFNN). The novelty of our approach lies in its ability to\nextract the grammatical structure of a sentence by using role-unbinding\nvectors, which are obtained in an unsupervised manner. This ATPL approach is\napplied to 1) image captioning, 2) part of speech (POS) tagging, and 3)\nconstituency parsing of a sentence. Experimental results demonstrate the\neffectiveness of the proposed approach.\n",
        "published": "2018-02-20",
        "authors": [
            "Qiuyuan Huang",
            "Li Deng",
            "Dapeng Wu",
            "Chang Liu",
            "Xiaodong He"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.00135v1",
        "title": "Dirichlet Variational Autoencoder for Text Modeling",
        "abstract": "  We introduce an improved variational autoencoder (VAE) for text modeling with\ntopic information explicitly modeled as a Dirichlet latent variable. By\nproviding the proposed model topic awareness, it is more superior at\nreconstructing input texts. Furthermore, due to the inherent interactions\nbetween the newly introduced Dirichlet variable and the conventional\nmultivariate Gaussian variable, the model is less prone to KL divergence\nvanishing. We derive the variational lower bound for the new model and conduct\nexperiments on four different data sets. The results show that the proposed\nmodel is superior at text reconstruction across the latent space and\nclassifications on learned representations have higher test accuracies.\n",
        "published": "2018-10-31",
        "authors": [
            "Yijun Xiao",
            "Tiancheng Zhao",
            "William Yang Wang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.00147v1",
        "title": "DOLORES: Deep Contextualized Knowledge Graph Embeddings",
        "abstract": "  We introduce a new method DOLORES for learning knowledge graph embeddings\nthat effectively captures contextual cues and dependencies among entities and\nrelations. First, we note that short paths on knowledge graphs comprising of\nchains of entities and relations can encode valuable information regarding\ntheir contextual usage. We operationalize this notion by representing knowledge\ngraphs not as a collection of triples but as a collection of entity-relation\nchains, and learn embeddings for entities and relations using deep neural\nmodels that capture such contextual usage. In particular, our model is based on\nBi-Directional LSTMs and learn deep representations of entities and relations\nfrom constructed entity-relation chains. We show that these representations can\nvery easily be incorporated into existing models to significantly advance the\nstate of the art on several knowledge graph prediction tasks like link\nprediction, triple classification, and missing relation type prediction (in\nsome cases by at least 9.5%).\n",
        "published": "2018-10-31",
        "authors": [
            "Haoyu Wang",
            "Vivek Kulkarni",
            "William Yang Wang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.00196v2",
        "title": "Towards Explainable NLP: A Generative Explanation Framework for Text\n  Classification",
        "abstract": "  Building explainable systems is a critical problem in the field of Natural\nLanguage Processing (NLP), since most machine learning models provide no\nexplanations for the predictions. Existing approaches for explainable machine\nlearning systems tend to focus on interpreting the outputs or the connections\nbetween inputs and outputs. However, the fine-grained information is often\nignored, and the systems do not explicitly generate the human-readable\nexplanations. To better alleviate this problem, we propose a novel generative\nexplanation framework that learns to make classification decisions and generate\nfine-grained explanations at the same time. More specifically, we introduce the\nexplainable factor and the minimum risk training approach that learn to\ngenerate more reasonable explanations. We construct two new datasets that\ncontain summaries, rating scores, and fine-grained reasons. We conduct\nexperiments on both datasets, comparing with several strong neural network\nbaseline systems. Experimental results show that our method surpasses all\nbaselines on both datasets, and is able to generate concise explanations at the\nsame time.\n",
        "published": "2018-11-01",
        "authors": [
            "Hui Liu",
            "Qingyu Yin",
            "William Yang Wang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.00198v1",
        "title": "MOHONE: Modeling Higher Order Network Effects in KnowledgeGraphs via\n  Network Infused Embeddings",
        "abstract": "  Many knowledge graph embedding methods operate on triples and are therefore\nimplicitly limited by a very local view of the entire knowledge graph. We\npresent a new framework MOHONE to effectively model higher order network\neffects in knowledge-graphs, thus enabling one to capture varying degrees of\nnetwork connectivity (from the local to the global). Our framework is generic,\nexplicitly models the network scale, and captures two different aspects of\nsimilarity in networks: (a) shared local neighborhood and (b) structural\nrole-based similarity. First, we introduce methods that learn network\nrepresentations of entities in the knowledge graph capturing these varied\naspects of similarity. We then propose a fast, efficient method to incorporate\nthe information captured by these network representations into existing\nknowledge graph embeddings. We show that our method consistently and\nsignificantly improves the performance on link prediction of several different\nknowledge-graph embedding methods including TRANSE, TRANSD, DISTMULT, and\nCOMPLEX(by at least 4 points or 17% in some cases).\n",
        "published": "2018-11-01",
        "authors": [
            "Hao Yu",
            "Vivek Kulkarni",
            "William Wang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.01910v2",
        "title": "Evolutionary Data Measures: Understanding the Difficulty of Text\n  Classification Tasks",
        "abstract": "  Classification tasks are usually analysed and improved through new model\narchitectures or hyperparameter optimisation but the underlying properties of\ndatasets are discovered on an ad-hoc basis as errors occur. However,\nunderstanding the properties of the data is crucial in perfecting models. In\nthis paper we analyse exactly which characteristics of a dataset best determine\nhow difficult that dataset is for the task of text classification. We then\npropose an intuitive measure of difficulty for text classification datasets\nwhich is simple and fast to calculate. We show that this measure generalises to\nunseen data by comparing it to state-of-the-art datasets and results. This\nmeasure can be used to analyse the precise source of errors in a dataset and\nallows fast estimation of how difficult a dataset is to learn. We searched for\nthis measure by training 12 classical and neural network based models on 78\nreal-world datasets, then use a genetic algorithm to discover the best measure\nof difficulty. Our difficulty-calculating code ( https://github.com/Wluper/edm\n) and datasets ( http://data.wluper.com ) are publicly available.\n",
        "published": "2018-11-05",
        "authors": [
            "Edward Collins",
            "Nikolai Rozanov",
            "Bingbing Zhang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.07253v1",
        "title": "Quantifying Uncertainties in Natural Language Processing Tasks",
        "abstract": "  Reliable uncertainty quantification is a first step towards building\nexplainable, transparent, and accountable artificial intelligent systems.\nRecent progress in Bayesian deep learning has made such quantification\nrealizable. In this paper, we propose novel methods to study the benefits of\ncharacterizing model and data uncertainties for natural language processing\n(NLP) tasks. With empirical experiments on sentiment analysis, named entity\nrecognition, and language modeling using convolutional and recurrent neural\nnetwork models, we show that explicitly modeling uncertainties is not only\nnecessary to measure output confidence levels, but also useful at enhancing\nmodel performances in various NLP tasks.\n",
        "published": "2018-11-18",
        "authors": [
            "Yijun Xiao",
            "William Yang Wang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.07550v1",
        "title": "Switch-based Active Deep Dyna-Q: Efficient Adaptive Planning for\n  Task-Completion Dialogue Policy Learning",
        "abstract": "  Training task-completion dialogue agents with reinforcement learning usually\nrequires a large number of real user experiences. The Dyna-Q algorithm extends\nQ-learning by integrating a world model, and thus can effectively boost\ntraining efficiency using simulated experiences generated by the world model.\nThe effectiveness of Dyna-Q, however, depends on the quality of the world model\n- or implicitly, the pre-specified ratio of real vs. simulated experiences used\nfor Q-learning. To this end, we extend the recently proposed Deep Dyna-Q (DDQ)\nframework by integrating a switcher that automatically determines whether to\nuse a real or simulated experience for Q-learning. Furthermore, we explore the\nuse of active learning for improving sample efficiency, by encouraging the\nworld model to generate simulated experiences in the state-action space where\nthe agent has not (fully) explored. Our results show that by combining switcher\nand active learning, the new framework named as Switch-based Active Deep Dyna-Q\n(Switch-DDQ), leads to significant improvement over DDQ and Q-learning\nbaselines in both simulation and human evaluations.\n",
        "published": "2018-11-19",
        "authors": [
            "Yuexin Wu",
            "Xiujun Li",
            "Jingjing Liu",
            "Jianfeng Gao",
            "Yiming Yang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.06610v2",
        "title": "Hierarchical Attentional Hybrid Neural Networks for Document\n  Classification",
        "abstract": "  Document classification is a challenging task with important applications.\nThe deep learning approaches to the problem have gained much attention\nrecently. Despite the progress, the proposed models do not incorporate the\nknowledge of the document structure in the architecture efficiently and not\ntake into account the contexting importance of words and sentences. In this\npaper, we propose a new approach based on a combination of convolutional neural\nnetworks, gated recurrent units, and attention mechanisms for document\nclassification tasks. The main contribution of this work is the use of\nconvolution layers to extract more meaningful, generalizable and abstract\nfeatures by the hierarchical representation. The proposed method in this paper\nimproves the results of the current attention-based approaches for document\nclassification.\n",
        "published": "2019-01-20",
        "authors": [
            "Jader Abreu",
            "Luis Fred",
            "David Mac\u00eado",
            "Cleber Zanchettin"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.14870v4",
        "title": "Mind Your Inflections! Improving NLP for Non-Standard Englishes with\n  Base-Inflection Encoding",
        "abstract": "  Inflectional variation is a common feature of World Englishes such as\nColloquial Singapore English and African American Vernacular English. Although\ncomprehension by human readers is usually unimpaired by non-standard\ninflections, current NLP systems are not yet robust. We propose Base-Inflection\nEncoding (BITE), a method to tokenize English text by reducing inflected words\nto their base forms before reinjecting the grammatical information as special\nsymbols. Fine-tuning pretrained NLP models for downstream tasks using our\nencoding defends against inflectional adversaries while maintaining performance\non clean data. Models using BITE generalize better to dialects with\nnon-standard inflections without explicit training and translation models\nconverge faster when trained with BITE. Finally, we show that our encoding\nimproves the vocabulary efficiency of popular data-driven subword tokenizers.\nSince there has been no prior work on quantitatively evaluating vocabulary\nefficiency, we propose metrics to do so.\n",
        "published": "2020-04-30",
        "authors": [
            "Samson Tan",
            "Shafiq Joty",
            "Lav R. Varshney",
            "Min-Yen Kan"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1512.00965v2",
        "title": "Neural Enquirer: Learning to Query Tables with Natural Language",
        "abstract": "  We proposed Neural Enquirer as a neural network architecture to execute a\nnatural language (NL) query on a knowledge-base (KB) for answers. Basically,\nNeural Enquirer finds the distributed representation of a query and then\nexecutes it on knowledge-base tables to obtain the answer as one of the values\nin the tables. Unlike similar efforts in end-to-end training of semantic\nparsers, Neural Enquirer is fully \"neuralized\": it not only gives\ndistributional representation of the query and the knowledge-base, but also\nrealizes the execution of compositional queries as a series of differentiable\noperations, with intermediate results (consisting of annotations of the tables\nat different levels) saved on multiple layers of memory. Neural Enquirer can be\ntrained with gradient descent, with which not only the parameters of the\ncontrolling components and semantic parsing component, but also the embeddings\nof the tables and query words can be learned from scratch. The training can be\ndone in an end-to-end fashion, but it can take stronger guidance, e.g., the\nstep-by-step supervision for complicated queries, and benefit from it. Neural\nEnquirer is one step towards building neural network systems which seek to\nunderstand language by executing it on real-world. Our experiments show that\nNeural Enquirer can learn to execute fairly complicated NL queries on tables\nwith rich structures.\n",
        "published": "2015-12-03",
        "authors": [
            "Pengcheng Yin",
            "Zhengdong Lu",
            "Hang Li",
            "Ben Kao"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.05027v7",
        "title": "Learning Intrinsic Sparse Structures within Long Short-Term Memory",
        "abstract": "  Model compression is significant for the wide adoption of Recurrent Neural\nNetworks (RNNs) in both user devices possessing limited resources and business\nclusters requiring quick responses to large-scale service requests. This work\naims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the\nsizes of basic structures within LSTM units, including input updates, gates,\nhidden states, cell states and outputs. Independently reducing the sizes of\nbasic structures can result in inconsistent dimensions among them, and\nconsequently, end up with invalid LSTM units. To overcome the problem, we\npropose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS\nwill simultaneously decrease the sizes of all basic structures by one and\nthereby always maintain the dimension consistency. By learning ISS within LSTM\nunits, the obtained LSTMs remain regular while having much smaller basic\nstructures. Based on group Lasso regularization, our method achieves 10.59x\nspeedup without losing any perplexity of a language modeling of Penn TreeBank\ndataset. It is also successfully evaluated through a compact model with only\n2.69M weights for machine Question Answering of SQuAD dataset. Our approach is\nsuccessfully extended to non- LSTM RNNs, like Recurrent Highway Networks\n(RHNs). Our source code is publicly available at\nhttps://github.com/wenwei202/iss-rnns\n",
        "published": "2017-09-15",
        "authors": [
            "Wei Wen",
            "Yuxiong He",
            "Samyam Rajbhandari",
            "Minjia Zhang",
            "Wenhan Wang",
            "Fang Liu",
            "Bin Hu",
            "Yiran Chen",
            "Hai Li"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.06673v2",
        "title": "Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational\n  Compositional Operators for Analogy Detection",
        "abstract": "  Representing the semantic relations that exist between two given words (or\nentities) is an important first step in a wide-range of NLP applications such\nas analogical reasoning, knowledge base completion and relational information\nretrieval. A simple, yet surprisingly accurate method for representing a\nrelation between two words is to compute the vector offset (\\PairDiff) between\ntheir corresponding word embeddings. Despite the empirical success, it remains\nunclear as to whether \\PairDiff is the best operator for obtaining a relational\nrepresentation from word embeddings. We conduct a theoretical analysis of\ngeneralised bilinear operators that can be used to measure the $\\ell_{2}$\nrelational distance between two word-pairs. We show that, if the word\nembeddings are standardised and uncorrelated, such an operator will be\nindependent of bilinear terms, and can be simplified to a linear form, where\n\\PairDiff is a special case. For numerous word embedding types, we empirically\nverify the uncorrelation assumption, demonstrating the general applicability of\nour theoretical result. Moreover, we experimentally discover \\PairDiff from the\nbilinear relation composition operator on several benchmark analogy datasets.\n",
        "published": "2017-09-19",
        "authors": [
            "Huda Hakami",
            "Danushka Bollegala",
            "Hayashi Kohei"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.08853v6",
        "title": "Object-oriented Neural Programming (OONP) for Document Understanding",
        "abstract": "  We propose Object-oriented Neural Programming (OONP), a framework for\nsemantically parsing documents in specific domains. Basically, OONP reads a\ndocument and parses it into a predesigned object-oriented data structure\n(referred to as ontology in this paper) that reflects the domain-specific\nsemantics of the document. An OONP parser models semantic parsing as a decision\nprocess: a neural net-based Reader sequentially goes through the document, and\nduring the process it builds and updates an intermediate ontology to summarize\nits partial understanding of the text it covers. OONP supports a rich family of\noperations (both symbolic and differentiable) for composing the ontology, and a\nbig variety of forms (both symbolic and differentiable) for representing the\nstate and the document. An OONP parser can be trained with supervision of\ndifferent forms and strength, including supervised learning (SL) ,\nreinforcement learning (RL) and hybrid of the two. Our experiments on both\nsynthetic and real-world document parsing tasks have shown that OONP can learn\nto handle fairly complicated ontology with training data of modest sizes.\n",
        "published": "2017-09-26",
        "authors": [
            "Zhengdong Lu",
            "Xianggen Liu",
            "Haotian Cui",
            "Yukun Yan",
            "Daqi Zheng"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.10204v2",
        "title": "A Neural Comprehensive Ranker (NCR) for Open-Domain Question Answering",
        "abstract": "  This paper proposes a novel neural machine reading model for open-domain\nquestion answering at scale. Existing machine comprehension models typically\nassume that a short piece of relevant text containing answers is already\nidentified and given to the models, from which the models are designed to\nextract answers. This assumption, however, is not realistic for building a\nlarge-scale open-domain question answering system which requires both deep text\nunderstanding and identifying relevant text from corpus simultaneously.\n  In this paper, we introduce Neural Comprehensive Ranker (NCR) that integrates\nboth passage ranking and answer extraction in one single framework. A Q&A\nsystem based on this framework allows users to issue an open-domain question\nwithout needing to provide a piece of text that must contain the answer.\nExperiments show that the unified NCR model is able to outperform the\nstates-of-the-art in both retrieval of relevant text and answer extraction.\n",
        "published": "2017-09-29",
        "authors": [
            "Bin Bi",
            "Hao Ma"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.10331v1",
        "title": "Deep Reinforcement Learning for Chatbots Using Clustered Actions and\n  Human-Likeness Rewards",
        "abstract": "  Training chatbots using the reinforcement learning paradigm is challenging\ndue to high-dimensional states, infinite action spaces and the difficulty in\nspecifying the reward function. We address such problems using clustered\nactions instead of infinite actions, and a simple but promising reward function\nbased on human-likeness scores derived from human-human dialogue data. We train\nDeep Reinforcement Learning (DRL) agents using chitchat data in raw\ntext---without any manual annotations. Experimental results using different\nsplits of training data report the following. First, that our agents learn\nreasonable policies in the environments they get familiarised with, but their\nperformance drops substantially when they are exposed to a test set of unseen\ndialogues. Second, that the choice of sentence embedding size between 100 and\n300 dimensions is not significantly different on test data. Third, that our\nproposed human-likeness rewards are reasonable for training chatbots as long as\nthey use lengthy dialogue histories of >=10 sentences.\n",
        "published": "2019-08-27",
        "authors": [
            "Heriberto Cuay\u00e1huitl",
            "Donghyeon Lee",
            "Seonghan Ryu",
            "Sungja Choi",
            "Inchul Hwang",
            "Jihie Kim"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1604.04378v1",
        "title": "Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN",
        "abstract": "  Semantic matching, which aims to determine the matching degree between two\ntexts, is a fundamental problem for many NLP applications. Recently, deep\nlearning approach has been applied to this problem and significant improvements\nhave been achieved. In this paper, we propose to view the generation of the\nglobal interaction between two texts as a recursive process: i.e. the\ninteraction of two texts at each position is a composition of the interactions\nbetween their prefixes as well as the word level interaction at the current\nposition. Based on this idea, we propose a novel deep architecture, namely\nMatch-SRNN, to model the recursive matching structure. Firstly, a tensor is\nconstructed to capture the word level interactions. Then a spatial RNN is\napplied to integrate the local interactions recursively, with importance\ndetermined by four types of gates. Finally, the matching score is calculated\nbased on the global interaction. We show that, after degenerated to the exact\nmatching scenario, Match-SRNN can approximate the dynamic programming process\nof longest common subsequence. Thus, there exists a clear interpretation for\nMatch-SRNN. Our experiments on two semantic matching tasks showed the\neffectiveness of Match-SRNN, and its ability of visualizing the learned\nmatching structure.\n",
        "published": "2016-04-15",
        "authors": [
            "Shengxian Wan",
            "Yanyan Lan",
            "Jun Xu",
            "Jiafeng Guo",
            "Liang Pang",
            "Xueqi Cheng"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1604.06635v1",
        "title": "Bridging LSTM Architecture and the Neural Dynamics during Reading",
        "abstract": "  Recently, the long short-term memory neural network (LSTM) has attracted wide\ninterest due to its success in many tasks. LSTM architecture consists of a\nmemory cell and three gates, which looks similar to the neuronal networks in\nthe brain. However, there still lacks the evidence of the cognitive\nplausibility of LSTM architecture as well as its working mechanism. In this\npaper, we study the cognitive plausibility of LSTM by aligning its internal\narchitecture with the brain activity observed via fMRI when the subjects read a\nstory. Experiment results show that the artificial memory vector in LSTM can\naccurately predict the observed sequential brain activities, indicating the\ncorrelation between LSTM architecture and the cognitive process of story\nreading.\n",
        "published": "2016-04-22",
        "authors": [
            "Peng Qian",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.07773v1",
        "title": "NUIG-Shubhanker@Dravidian-CodeMix-FIRE2020: Sentiment Analysis of\n  Code-Mixed Dravidian text using XLNet",
        "abstract": "  Social media has penetrated into multilingual societies, however most of them\nuse English to be a preferred language for communication. So it looks natural\nfor them to mix their cultural language with English during conversations\nresulting in abundance of multilingual data, call this code-mixed data,\navailable in todays' world.Downstream NLP tasks using such data is challenging\ndue to the semantic nature of it being spread across multiple languages.One\nsuch Natural Language Processing task is sentiment analysis, for this we use an\nauto-regressive XLNet model to perform sentiment analysis on code-mixed\nTamil-English and Malayalam-English datasets.\n",
        "published": "2020-10-15",
        "authors": [
            "Shubhanker Banerjee",
            "Arun Jayapal",
            "Sajeetha Thavareesan"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.12681v1",
        "title": "Robust Document Representations using Latent Topics and Metadata",
        "abstract": "  Task specific fine-tuning of a pre-trained neural language model using a\ncustom softmax output layer is the de facto approach of late when dealing with\ndocument classification problems. This technique is not adequate when labeled\nexamples are not available at training time and when the metadata artifacts in\na document must be exploited. We address these challenges by generating\ndocument representations that capture both text and metadata artifacts in a\ntask agnostic manner. Instead of traditional auto-regressive or auto-encoding\nbased training, our novel self-supervised approach learns a soft-partition of\nthe input space when generating text embeddings. Specifically, we employ a\npre-learned topic model distribution as surrogate labels and construct a loss\nfunction based on KL divergence. Our solution also incorporates metadata\nexplicitly rather than just augmenting them with text. The generated document\nembeddings exhibit compositional characteristics and are directly used by\ndownstream classification tasks to create decision boundaries from a small\nnumber of labeled examples, thereby eschewing complicated recognition methods.\nWe demonstrate through extensive evaluation that our proposed cross-model\nfusion solution outperforms several competitive baselines on multiple datasets.\n",
        "published": "2020-10-23",
        "authors": [
            "Natraj Raman",
            "Armineh Nourbakhsh",
            "Sameena Shah",
            "Manuela Veloso"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.09688v2",
        "title": "Stereotype and Skew: Quantifying Gender Bias in Pre-trained and\n  Fine-tuned Language Models",
        "abstract": "  This paper proposes two intuitive metrics, skew and stereotype, that quantify\nand analyse the gender bias present in contextual language models when tackling\nthe WinoBias pronoun resolution task. We find evidence that gender stereotype\ncorrelates approximately negatively with gender skew in out-of-the-box models,\nsuggesting that there is a trade-off between these two forms of bias. We\ninvestigate two methods to mitigate bias. The first approach is an online\nmethod which is effective at removing skew at the expense of stereotype. The\nsecond, inspired by previous work on ELMo, involves the fine-tuning of BERT\nusing an augmented gender-balanced dataset. We show that this reduces both skew\nand stereotype relative to its unaugmented fine-tuned counterpart. However, we\nfind that existing gender bias benchmarks do not fully probe professional bias\nas pronoun resolution may be obfuscated by cross-correlations from other\nmanifestations of gender prejudice. Our code is available online, at\nhttps://github.com/12kleingordon34/NLP_masters_project.\n",
        "published": "2021-01-24",
        "authors": [
            "Daniel de Vassimon Manela",
            "David Errington",
            "Thomas Fisher",
            "Boris van Breugel",
            "Pasquale Minervini"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.10447v2",
        "title": "Transformer Quality in Linear Time",
        "abstract": "  We revisit the design choices in Transformers, and propose methods to address\ntheir weaknesses in handling long sequences. First, we propose a simple layer\nnamed gated attention unit, which allows the use of a weaker single-head\nattention with minimal quality loss. We then propose a linear approximation\nmethod complementary to this new layer, which is accelerator-friendly and\nhighly competitive in quality. The resulting model, named FLASH, matches the\nperplexity of improved Transformers over both short (512) and long (8K) context\nlengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and\n12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on\nC4 for masked language modeling.\n",
        "published": "2022-02-21",
        "authors": [
            "Weizhe Hua",
            "Zihang Dai",
            "Hanxiao Liu",
            "Quoc V. Le"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.03864v2",
        "title": "Neural Associative Memory for Dual-Sequence Modeling",
        "abstract": "  Many important NLP problems can be posed as dual-sequence or\nsequence-to-sequence modeling tasks. Recent advances in building end-to-end\nneural architectures have been highly successful in solving such tasks. In this\nwork we propose a new architecture for dual-sequence modeling that is based on\nassociative memory. We derive AM-RNNs, a recurrent associative memory (AM)\nwhich augments generic recurrent neural networks (RNN). This architecture is\nextended to the Dual AM-RNN which operates on two AMs at once. Our models\nachieve very competitive results on textual entailment. A qualitative analysis\ndemonstrates that long range dependencies between source and target-sequence\ncan be bridged effectively using Dual AM-RNNs. However, an initial experiment\non auto-encoding reveals that these benefits are not exploited by the system\nwhen learning to solve sequence-to-sequence tasks which indicates that\nadditional supervision or regularization is needed.\n",
        "published": "2016-06-13",
        "authors": [
            "Dirk Weissenborn"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1607.02467v2",
        "title": "Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior\n  Knowledge",
        "abstract": "  We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural\nNetworks that replaces the softmax output layer by a log-linear output layer,\nof which the softmax is a special case. This conceptually simple move has two\nmain advantages. First, it allows the learner to combat training data sparsity\nby allowing it to model words (or more generally, output symbols) as complex\ncombinations of attributes without requiring that each combination is directly\nobserved in the training data (as the softmax does). Second, it permits the\ninclusion of flexible prior knowledge in the form of a priori specified modular\nfeatures, where the neural network component learns to dynamically control the\nweights of a log-linear distribution exploiting these features.\n  We conduct experiments in the domain of language modelling of French, that\nexploit morphological prior knowledge and show an important decrease in\nperplexity relative to a baseline RNN.\n  We provide other motivating iillustrations, and finally argue that the\nlog-linear and the neural-network components contribute complementary strengths\nto the LL-RNN: the LL aspect allows the model to incorporate rich prior\nknowledge, while the NN aspect, according to the \"representation learning\"\nparadigm, allows the model to discover novel combination of characteristics.\n",
        "published": "2016-07-08",
        "authors": [
            "Marc Dymetman",
            "Chunyang Xiao"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1610.00956v1",
        "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension",
        "abstract": "  There is a practically unlimited amount of natural language data available.\nStill, recent work in text comprehension has focused on datasets which are\nsmall relative to current computing possibilities. This article is making a\ncase for the community to move to larger data and as a step in that direction\nit is proposing the BookTest, a new dataset similar to the popular Children's\nBook Test (CBT), however more than 60 times larger. We show that training on\nthe new data improves the accuracy of our Attention-Sum Reader model on the\noriginal CBT test data by a much larger margin than many recent attempts to\nimprove the model architecture. On one version of the dataset our ensemble even\nexceeds the human baseline provided by Facebook. We then show in our own human\nstudy that there is still space for further improvement.\n",
        "published": "2016-10-04",
        "authors": [
            "Ondrej Bajgar",
            "Rudolf Kadlec",
            "Jan Kleindienst"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1610.01891v1",
        "title": "A New Data Representation Based on Training Data Characteristics to\n  Extract Drug Named-Entity in Medical Text",
        "abstract": "  One essential task in information extraction from the medical corpus is drug\nname recognition. Compared with text sources come from other domains, the\nmedical text is special and has unique characteristics. In addition, the\nmedical text mining poses more challenges, e.g., more unstructured text, the\nfast growing of new terms addition, a wide range of name variation for the same\ndrug. The mining is even more challenging due to the lack of labeled dataset\nsources and external knowledge, as well as multiple token representations for a\nsingle drug name that is more common in the real application setting. Although\nmany approaches have been proposed to overwhelm the task, some problems\nremained with poor F-score performance (less than 0.75). This paper presents a\nnew treatment in data representation techniques to overcome some of those\nchallenges. We propose three data representation techniques based on the\ncharacteristics of word distribution and word similarities as a result of word\nembedding training. The first technique is evaluated with the standard NN\nmodel, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two\ndeep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked\nDenoising Encoders). The third technique represents the sentence as a sequence\nthat is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term\nMemory). In extracting the drug name entities, the third technique gives the\nbest F-score performance compared to the state of the art, with its average\nF-score being 0.8645.\n",
        "published": "2016-10-06",
        "authors": [
            "Sadikin Mujiono",
            "Mohamad Ivan Fanany",
            "Chan Basaruddin"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1611.01576v2",
        "title": "Quasi-Recurrent Neural Networks",
        "abstract": "  Recurrent neural networks are a powerful tool for modeling sequential data,\nbut the dependence of each timestep's computation on the previous timestep's\noutput limits parallelism and makes RNNs unwieldy for very long sequences. We\nintroduce quasi-recurrent neural networks (QRNNs), an approach to neural\nsequence modeling that alternates convolutional layers, which apply in parallel\nacross timesteps, and a minimalist recurrent pooling function that applies in\nparallel across channels. Despite lacking trainable recurrent layers, stacked\nQRNNs have better predictive accuracy than stacked LSTMs of the same hidden\nsize. Due to their increased parallelism, they are up to 16 times faster at\ntrain and test time. Experiments on language modeling, sentiment\nclassification, and character-level neural machine translation demonstrate\nthese advantages and underline the viability of QRNNs as a basic building block\nfor a variety of sequence tasks.\n",
        "published": "2016-11-05",
        "authors": [
            "James Bradbury",
            "Stephen Merity",
            "Caiming Xiong",
            "Richard Socher"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1611.09434v2",
        "title": "Input Switched Affine Networks: An RNN Architecture Designed for\n  Interpretability",
        "abstract": "  There exist many problem domains where the interpretability of neural network\nmodels is essential for deployment. Here we introduce a recurrent architecture\ncomposed of input-switched affine transformations - in other words an RNN\nwithout any explicit nonlinearities, but with input-dependent recurrent\nweights. This simple form allows the RNN to be analyzed via straightforward\nlinear methods: we can exactly characterize the linear contribution of each\ninput to the model predictions; we can use a change-of-basis to disentangle\ninput, output, and computational hidden unit subspaces; we can fully\nreverse-engineer the architecture's solution to a simple task. Despite this\nease of interpretation, the input switched affine network achieves reasonable\nperformance on a text modeling tasks, and allows greater computational\nefficiency than networks with standard nonlinearities.\n",
        "published": "2016-11-28",
        "authors": [
            "Jakob N. Foerster",
            "Justin Gilmer",
            "Jan Chorowski",
            "Jascha Sohl-Dickstein",
            "David Sussillo"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1612.00377v4",
        "title": "Piecewise Latent Variables for Neural Variational Text Processing",
        "abstract": "  Advances in neural variational inference have facilitated the learning of\npowerful directed graphical models with continuous latent variables, such as\nvariational autoencoders. The hope is that such models will learn to represent\nrich, multi-modal latent factors in real-world data, such as natural language\ntext. However, current models often assume simplistic priors on the latent\nvariables - such as the uni-modal Gaussian distribution - which are incapable\nof representing complex latent factors efficiently. To overcome this\nrestriction, we propose the simple, but highly flexible, piecewise constant\ndistribution. This distribution has the capacity to represent an exponential\nnumber of modes of a latent target distribution, while remaining mathematically\ntractable. Our results demonstrate that incorporating this new latent\ndistribution into different models yields substantial improvements in natural\nlanguage processing tasks such as document modeling and natural language\ngeneration for dialogue.\n",
        "published": "2016-12-01",
        "authors": [
            "Iulian V. Serban",
            "Alexander G. Ororbia II",
            "Joelle Pineau",
            "Aaron Courville"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1704.08092v1",
        "title": "A Recurrent Neural Model with Attention for the Recognition of Chinese\n  Implicit Discourse Relations",
        "abstract": "  We introduce an attention-based Bi-LSTM for Chinese implicit discourse\nrelations and demonstrate that modeling argument pairs as a joint sequence can\noutperform word order-agnostic approaches. Our model benefits from a partial\nsampling scheme and is conceptually simple, yet achieves state-of-the-art\nperformance on the Chinese Discourse Treebank. We also visualize its attention\nactivity to illustrate the model's ability to selectively focus on the relevant\nparts of an input sequence.\n",
        "published": "2017-04-26",
        "authors": [
            "Samuel R\u00f6nnqvist",
            "Niko Schenk",
            "Christian Chiarcos"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.00641v1",
        "title": "Improving speech recognition by revising gated recurrent units",
        "abstract": "  Speech recognition is largely taking advantage of deep learning, showing that\nsubstantial benefits can be obtained by modern Recurrent Neural Networks\n(RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which\ntypically reach state-of-the-art performance in many tasks thanks to their\nability to learn long-term dependencies and robustness to vanishing gradients.\nNevertheless, LSTMs have a rather complex design with three multiplicative\ngates, that might impair their efficient implementation. An attempt to simplify\nLSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just\ntwo multiplicative gates.\n  This paper builds on these efforts by further revising GRUs and proposing a\nsimplified architecture potentially more suitable for speech recognition. The\ncontribution of this work is two-fold. First, we suggest to remove the reset\ngate in the GRU design, resulting in a more efficient single-gate architecture.\nSecond, we propose to replace tanh with ReLU activations in the state update\nequations. Results show that, in our implementation, the revised architecture\nreduces the per-epoch training time with more than 30% and consistently\nimproves recognition performance across different tasks, input features, and\nnoisy conditions when compared to a standard GRU.\n",
        "published": "2017-09-29",
        "authors": [
            "Mirco Ravanelli",
            "Philemon Brakel",
            "Maurizio Omologo",
            "Yoshua Bengio"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.02341v1",
        "title": "Compositional Obverter Communication Learning From Raw Visual Input",
        "abstract": "  One of the distinguishing aspects of human language is its compositionality,\nwhich allows us to describe complex environments with limited vocabulary.\nPreviously, it has been shown that neural network agents can learn to\ncommunicate in a highly structured, possibly compositional language based on\ndisentangled input (e.g. hand- engineered features). Humans, however, do not\nlearn to communicate based on well-summarized features. In this work, we train\nneural agents to simultaneously develop visual perception from raw image\npixels, and learn to communicate with a sequence of discrete symbols. The\nagents play an image description game where the image contains factors such as\ncolors and shapes. We train the agents using the obverter technique where an\nagent introspects to generate messages that maximize its own understanding.\nThrough qualitative analysis, visualization and a zero-shot test, we show that\nthe agents can develop, out of raw image pixels, a language with compositional\nproperties, given a proper pressure from the environment.\n",
        "published": "2018-04-06",
        "authors": [
            "Edward Choi",
            "Angeliki Lazaridou",
            "Nando de Freitas"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1805.12352v2",
        "title": "DialogWAE: Multimodal Response Generation with Conditional Wasserstein\n  Auto-Encoder",
        "abstract": "  Variational autoencoders~(VAEs) have shown a promise in data-driven\nconversation modeling. However, most VAE conversation models match the\napproximate posterior distribution over the latent variables to a simple prior\nsuch as standard normal distribution, thereby restricting the generated\nresponses to a relatively simple (e.g., unimodal) scope. In this paper, we\npropose DialogWAE, a conditional Wasserstein autoencoder~(WAE) specially\ndesigned for dialogue modeling. Unlike VAEs that impose a simple distribution\nover the latent variables, DialogWAE models the distribution of data by\ntraining a GAN within the latent variable space. Specifically, our model\nsamples from the prior and posterior distributions over the latent variables by\ntransforming context-dependent random noise using neural networks and minimizes\nthe Wasserstein distance between the two distributions. We further develop a\nGaussian mixture prior network to enrich the latent space. Experiments on two\npopular datasets show that DialogWAE outperforms the state-of-the-art\napproaches in generating more coherent, informative and diverse responses.\n",
        "published": "2018-05-31",
        "authors": [
            "Xiaodong Gu",
            "Kyunghyun Cho",
            "Jung-Woo Ha",
            "Sunghun Kim"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.10802v1",
        "title": "Tackling Sequence to Sequence Mapping Problems with Neural Networks",
        "abstract": "  In Natural Language Processing (NLP), it is important to detect the\nrelationship between two sequences or to generate a sequence of tokens given\nanother observed sequence. We call the type of problems on modelling sequence\npairs as sequence to sequence (seq2seq) mapping problems. A lot of research has\nbeen devoted to finding ways of tackling these problems, with traditional\napproaches relying on a combination of hand-crafted features, alignment models,\nsegmentation heuristics, and external linguistic resources. Although great\nprogress has been made, these traditional approaches suffer from various\ndrawbacks, such as complicated pipeline, laborious feature engineering, and the\ndifficulty for domain adaptation. Recently, neural networks emerged as a\npromising solution to many problems in NLP, speech recognition, and computer\nvision. Neural models are powerful because they can be trained end to end,\ngeneralise well to unseen examples, and the same framework can be easily\nadapted to a new domain.\n  The aim of this thesis is to advance the state-of-the-art in seq2seq mapping\nproblems with neural networks. We explore solutions from three major aspects:\ninvestigating neural models for representing sequences, modelling interactions\nbetween sequences, and using unpaired data to boost the performance of neural\nmodels. For each aspect, we propose novel models and evaluate their efficacy on\nvarious tasks of seq2seq mapping.\n",
        "published": "2018-10-25",
        "authors": [
            "Lei Yu"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.05606v1",
        "title": "Multi-lingual Dialogue Act Recognition with Deep Learning Methods",
        "abstract": "  This paper deals with multi-lingual dialogue act (DA) recognition. The\nproposed approaches are based on deep neural networks and use word2vec\nembeddings for word representation. Two multi-lingual models are proposed for\nthis task. The first approach uses one general model trained on the embeddings\nfrom all available languages. The second method trains the model on a single\npivot language and a linear transformation method is used to project other\nlanguages onto the pivot language. The popular convolutional neural network and\nLSTM architectures with different set-ups are used as classifiers. To the best\nof our knowledge this is the first attempt at multi-lingual DA recognition\nusing neural networks. The multi-lingual models are validated experimentally on\ntwo languages from the Verbmobil corpus.\n",
        "published": "2019-04-11",
        "authors": [
            "Ji\u0159\u00ed Mart\u00ednek",
            "Pavel Kr\u00e1l",
            "Ladislav Lenc",
            "Christophe Cerisara"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.00499v1",
        "title": "Budgeted Policy Learning for Task-Oriented Dialogue Systems",
        "abstract": "  This paper presents a new approach that extends Deep Dyna-Q (DDQ) by\nincorporating a Budget-Conscious Scheduling (BCS) to best utilize a fixed,\nsmall amount of user interactions (budget) for learning task-oriented dialogue\nagents. BCS consists of (1) a Poisson-based global scheduler to allocate budget\nover different stages of training; (2) a controller to decide at each training\nstep whether the agent is trained using real or simulated experiences; (3) a\nuser goal sampling module to generate the experiences that are most effective\nfor policy learning. Experiments on a movie-ticket booking task with simulated\nand real users show that our approach leads to significant improvements in\nsuccess rate over the state-of-the-art baselines given the fixed budget.\n",
        "published": "2019-06-02",
        "authors": [
            "Zhirui Zhang",
            "Xiujun Li",
            "Jianfeng Gao",
            "Enhong Chen"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.00141v2",
        "title": "Deep Reinforcement Learning with Distributional Semantic Rewards for\n  Abstractive Summarization",
        "abstract": "  Deep reinforcement learning (RL) has been a commonly-used strategy for the\nabstractive summarization task to address both the exposure bias and\nnon-differentiable task issues. However, the conventional reward Rouge-L simply\nlooks for exact n-grams matches between candidates and annotated references,\nwhich inevitably makes the generated sentences repetitive and incoherent. In\nthis paper, instead of Rouge-L, we explore the practicability of utilizing the\ndistributional semantics to measure the matching degrees. With distributional\nsemantics, sentence-level evaluation can be obtained, and semantically-correct\nphrases can also be generated without being limited to the surface form of the\nreference sentences. Human judgments on Gigaword and CNN/Daily Mail datasets\nshow that our proposed distributional semantics reward (DSR) has distinct\nsuperiority in capturing the lexical and compositional diversity of natural\nlanguage.\n",
        "published": "2019-08-31",
        "authors": [
            "Siyao Li",
            "Deren Lei",
            "Pengda Qin",
            "William Yang Wang"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.03722v1",
        "title": "Template Controllable keywords-to-text Generation",
        "abstract": "  This paper proposes a novel neural model for the understudied task of\ngenerating text from keywords. The model takes as input a set of un-ordered\nkeywords, and part-of-speech (POS) based template instructions. This makes it\nideal for surface realization in any NLG setup. The framework is based on the\nencode-attend-decode paradigm, where keywords and templates are encoded first,\nand the decoder judiciously attends over the contexts derived from the encoded\nkeywords and templates to generate the sentences. Training exploits weak\nsupervision, as the model trains on a large amount of labeled data with\nkeywords and POS based templates prepared through completely automatic means.\nQualitative and quantitative performance analyses on publicly available\ntest-data in various domains reveal our system's superiority over baselines,\nbuilt using state-of-the-art neural machine translation and controllable\ntransfer techniques. Our approach is indifferent to the order of input\nkeywords.\n",
        "published": "2020-11-07",
        "authors": [
            "Abhijit Mishra",
            "Md Faisal Mahbub Chowdhury",
            "Sagar Manohar",
            "Dan Gutfreund",
            "Karthik Sankaranarayanan"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.10208v1",
        "title": "Collaborative Storytelling with Large-scale Neural Language Models",
        "abstract": "  Storytelling plays a central role in human socializing and entertainment.\nHowever, much of the research on automatic storytelling generation assumes that\nstories will be generated by an agent without any human interaction. In this\npaper, we introduce the task of collaborative storytelling, where an artificial\nintelligence agent and a person collaborate to create a unique story by taking\nturns adding to it. We present a collaborative storytelling system which works\nwith a human storyteller to create a story by generating new utterances based\non the story so far. We constructed the storytelling system by tuning a\npublicly-available large scale language model on a dataset of writing prompts\nand their accompanying fictional works. We identify generating sufficiently\nhuman-like utterances to be an important technical issue and propose a\nsample-and-rank approach to improve utterance quality. Quantitative evaluation\nshows that our approach outperforms a baseline, and we present qualitative\nevaluation of our system's capabilities.\n",
        "published": "2020-11-20",
        "authors": [
            "Eric Nichols",
            "Leo Gao",
            "Randy Gomez"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.09823v1",
        "title": "Continual Lifelong Learning in Natural Language Processing: A Survey",
        "abstract": "  Continual learning (CL) aims to enable information systems to learn from a\ncontinuous data stream across time. However, it is difficult for existing deep\nlearning architectures to learn a new task without largely forgetting\npreviously acquired knowledge. Furthermore, CL is particularly challenging for\nlanguage learning, as natural language is ambiguous: it is discrete,\ncompositional, and its meaning is context-dependent. In this work, we look at\nthe problem of CL through the lens of various NLP tasks. Our survey discusses\nmajor challenges in CL and current methods applied in neural network models. We\nalso provide a critical review of the existing CL evaluation methods and\ndatasets in NLP. Finally, we present our outlook on future research directions.\n",
        "published": "2020-12-17",
        "authors": [
            "Magdalena Biesialska",
            "Katarzyna Biesialska",
            "Marta R. Costa-juss\u00e0"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.02810v2",
        "title": "Controlling Hallucinations at Word Level in Data-to-Text Generation",
        "abstract": "  Data-to-Text Generation (DTG) is a subfield of Natural Language Generation\naiming at transcribing structured data in natural language descriptions. The\nfield has been recently boosted by the use of neural-based generators which\nexhibit on one side great syntactic skills without the need of hand-crafted\npipelines; on the other side, the quality of the generated text reflects the\nquality of the training data, which in realistic settings only offer\nimperfectly aligned structure-text pairs. Consequently, state-of-art neural\nmodels include misleading statements - usually called hallucinations - in their\noutputs. The control of this phenomenon is today a major challenge for DTG, and\nis the problem addressed in the paper.\n  Previous work deal with this issue at the instance level: using an alignment\nscore for each table-reference pair. In contrast, we propose a finer-grained\napproach, arguing that hallucinations should rather be treated at the word\nlevel. Specifically, we propose a Multi-Branch Decoder which is able to\nleverage word-level labels to learn the relevant parts of each training\ninstance. These labels are obtained following a simple and efficient scoring\nprocedure based on co-occurrence analysis and dependency parsing. Extensive\nevaluations, via automated metrics and human judgment on the standard WikiBio\nbenchmark, show the accuracy of our alignment labels and the effectiveness of\nthe proposed Multi-Branch Decoder. Our model is able to reduce and control\nhallucinations, while keeping fluency and coherence in generated texts. Further\nexperiments on a degraded version of ToTTo show that our model could be\nsuccessfully used on very noisy settings.\n",
        "published": "2021-02-04",
        "authors": [
            "Cl\u00e9ment Rebuffel",
            "Marco Roberti",
            "Laure Soulier",
            "Geoffrey Scoutheeten",
            "Rossella Cancelliere",
            "Patrick Gallinari"
        ],
        "categories": [
            null,
            null,
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.03108v1",
        "title": "An empirical analysis of phrase-based and neural machine translation",
        "abstract": "  Two popular types of machine translation (MT) are phrase-based and neural\nmachine translation systems. Both of these types of systems are composed of\nmultiple complex models or layers. Each of these models and layers learns\ndifferent linguistic aspects of the source language. However, for some of these\nmodels and layers, it is not clear which linguistic phenomena are learned or\nhow this information is learned. For phrase-based MT systems, it is often clear\nwhat information is learned by each model, and the question is rather how this\ninformation is learned, especially for its phrase reordering model. For neural\nmachine translation systems, the situation is even more complex, since for many\ncases it is not exactly clear what information is learned and how it is\nlearned.\n  To shed light on what linguistic phenomena are captured by MT systems, we\nanalyze the behavior of important models in both phrase-based and neural MT\nsystems. We consider phrase reordering models from phrase-based MT systems to\ninvestigate which words from inside of a phrase have the biggest impact on\ndefining the phrase reordering behavior. Additionally, to contribute to the\ninterpretability of neural MT systems we study the behavior of the attention\nmodel, which is a key component in neural MT systems and the closest model in\nfunctionality to phrase reordering models in phrase-based systems. The\nattention model together with the encoder hidden state representations form the\nmain components to encode source side linguistic information in neural MT. To\nthis end, we also analyze the information captured in the encoder hidden state\nrepresentations of a neural MT system. We investigate the extent to which\nsyntactic and lexical-semantic information from the source side is captured by\nhidden state representations of different neural MT architectures.\n",
        "published": "2021-03-04",
        "authors": [
            "Hamidreza Ghader"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2108.08708v3",
        "title": "Czech News Dataset for Semantic Textual Similarity",
        "abstract": "  This paper describes a novel dataset consisting of sentences with semantic\nsimilarity annotations. The data originate from the journalistic domain in the\nCzech language. We describe the process of collecting and annotating the data\nin detail. The dataset contains 138,556 human annotations divided into train\nand test sets. In total, 485 journalism students participated in the creation\nprocess. To increase the reliability of the test set, we compute the annotation\nas an average of 9 individual annotations. We evaluate the quality of the\ndataset by measuring inter and intra annotation annotators' agreements. Beside\nagreement numbers, we provide detailed statistics of the collected dataset. We\nconclude our paper with a baseline experiment of building a system for\npredicting the semantic similarity of sentences. Due to the massive number of\ntraining annotations (116 956), the model can perform significantly better than\nan average annotator (0,92 versus 0,86 of Person's correlation coefficients).\n",
        "published": "2021-08-19",
        "authors": [
            "Jakub Sido",
            "Michal Sej\u00e1k",
            "Ond\u0159ej Pra\u017e\u00e1k",
            "Miloslav Konop\u00edk",
            "V\u00e1clav Moravec"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.08668v2",
        "title": "Primer: Searching for Efficient Transformers for Language Modeling",
        "abstract": "  Large Transformer models have been central to recent advances in natural\nlanguage processing. The training and inference costs of these models, however,\nhave grown rapidly and become prohibitively expensive. Here we aim to reduce\nthe costs of Transformers by searching for a more efficient variant. Compared\nto previous approaches, our search is performed at a lower level, over the\nprimitives that define a Transformer TensorFlow program. We identify an\narchitecture, named Primer, that has a smaller training cost than the original\nTransformer and other variants for auto-regressive language modeling. Primer's\nimprovements can be mostly attributed to two simple modifications: squaring\nReLU activations and adding a depthwise convolution layer after each Q, K, and\nV projection in self-attention.\n  Experiments show Primer's gains over Transformer increase as compute scale\ngrows and follow a power law with respect to quality at optimal model sizes. We\nalso verify empirically that Primer can be dropped into different codebases to\nsignificantly speed up training without additional tuning. For example, at a\n500M parameter size, Primer improves the original T5 architecture on C4\nauto-regressive language modeling, reducing the training cost by 4X.\nFurthermore, the reduced training cost means Primer needs much less compute to\nreach a target one-shot performance. For instance, in a 1.9B parameter\nconfiguration similar to GPT-3 XL, Primer uses 1/3 of the training compute to\nachieve the same one-shot performance as Transformer. We open source our models\nand several comparisons in T5 to help with reproducibility.\n",
        "published": "2021-09-17",
        "authors": [
            "David R. So",
            "Wojciech Ma\u0144ke",
            "Hanxiao Liu",
            "Zihang Dai",
            "Noam Shazeer",
            "Quoc V. Le"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.00998v1",
        "title": "Simple Recurrent Neural Networks is all we need for clinical events\n  predictions using EHR data",
        "abstract": "  Recently, there is great interest to investigate the application of deep\nlearning models for the prediction of clinical events using electronic health\nrecords (EHR) data. In EHR data, a patient's history is often represented as a\nsequence of visits, and each visit contains multiple events. As a result, deep\nlearning models developed for sequence modeling, like recurrent neural networks\n(RNNs) are common architecture for EHR-based clinical events predictive models.\nWhile a large variety of RNN models were proposed in the literature, it is\nunclear if complex architecture innovations will offer superior predictive\nperformance. In order to move this field forward, a rigorous evaluation of\nvarious methods is needed. In this study, we conducted a thorough benchmark of\nRNN architectures in modeling EHR data. We used two prediction tasks: the risk\nfor developing heart failure and the risk of early readmission for inpatient\nhospitalization. We found that simple gated RNN models, including GRUs and\nLSTMs, often offer competitive results when properly tuned with Bayesian\nOptimization, which is in line with similar to findings in the natural language\nprocessing (NLP) domain. For reproducibility, Our codebase is shared at\nhttps://github.com/ZhiGroup/pytorch_ehr.\n",
        "published": "2021-10-03",
        "authors": [
            "Laila Rasmy",
            "Jie Zhu",
            "Zhiheng Li",
            "Xin Hao",
            "Hong Thoai Tran",
            "Yujia Zhou",
            "Firat Tiryaki",
            "Yang Xiang",
            "Hua Xu",
            "Degui Zhi"
        ],
        "categories": [
            null,
            null,
            null,
            null
        ]
    }
]