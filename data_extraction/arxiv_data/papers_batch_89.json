[
    {
        "id": "http://arxiv.org/abs/2206.06489v1",
        "title": "BEHAVIOR in Habitat 2.0: Simulator-Independent Logical Task Description\n  for Benchmarking Embodied AI Agents",
        "abstract": "  Robots excel in performing repetitive and precision-sensitive tasks in\ncontrolled environments such as warehouses and factories, but have not been yet\nextended to embodied AI agents providing assistance in household tasks.\nInspired by the catalyzing effect that benchmarks have played in the AI fields\nsuch as computer vision and natural language processing, the community is\nlooking for new benchmarks for embodied AI. Prior work in embodied AI benchmark\ndefines tasks using a different formalism, often specific to one environment,\nsimulator or domain, making it hard to develop general and comparable\nsolutions. In this work, we bring a subset of BEHAVIOR activities into Habitat\n2.0 to benefit from its fast simulation speed, as a first step towards\ndemonstrating the ease of adapting activities defined in the logic space into\ndifferent simulators.\n",
        "published": "2022",
        "authors": [
            "Ziang Liu",
            "Roberto Mart\u00edn-Mart\u00edn",
            "Fei Xia",
            "Jiajun Wu",
            "Li Fei-Fei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.06994v1",
        "title": "ProcTHOR: Large-Scale Embodied AI Using Procedural Generation",
        "abstract": "  Massive datasets and high-capacity models have driven many recent\nadvancements in computer vision and natural language understanding. This work\npresents a platform to enable similar success stories in Embodied AI. We\npropose ProcTHOR, a framework for procedural generation of Embodied AI\nenvironments. ProcTHOR enables us to sample arbitrarily large datasets of\ndiverse, interactive, customizable, and performant virtual environments to\ntrain and evaluate embodied agents across navigation, interaction, and\nmanipulation tasks. We demonstrate the power and potential of ProcTHOR via a\nsample of 10,000 generated houses and a simple neural model. Models trained\nusing only RGB images on ProcTHOR, with no explicit mapping and no human task\nsupervision produce state-of-the-art results across 6 embodied AI benchmarks\nfor navigation, rearrangement, and arm manipulation, including the presently\nrunning Habitat 2022, AI2-THOR Rearrangement 2022, and RoboTHOR challenges. We\nalso demonstrate strong 0-shot results on these benchmarks, via pre-training on\nProcTHOR with no fine-tuning on the downstream benchmark, often beating\nprevious state-of-the-art systems that access the downstream training data.\n",
        "published": "2022",
        "authors": [
            "Matt Deitke",
            "Eli VanderBilt",
            "Alvaro Herrasti",
            "Luca Weihs",
            "Jordi Salvador",
            "Kiana Ehsani",
            "Winson Han",
            "Eric Kolve",
            "Ali Farhadi",
            "Aniruddha Kembhavi",
            "Roozbeh Mottaghi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.08927v1",
        "title": "Cross-task Attention Mechanism for Dense Multi-task Learning",
        "abstract": "  Multi-task learning has recently become a promising solution for a\ncomprehensive understanding of complex scenes. Not only being memory-efficient,\nmulti-task models with an appropriate design can favor exchange of\ncomplementary signals across tasks. In this work, we jointly address 2D\nsemantic segmentation, and two geometry-related tasks, namely dense depth,\nsurface normal estimation as well as edge estimation showing their benefit on\nindoor and outdoor datasets. We propose a novel multi-task learning\narchitecture that exploits pair-wise cross-task exchange through\ncorrelation-guided attention and self-attention to enhance the average\nrepresentation learning for all tasks. We conduct extensive experiments\nconsidering three multi-task setups, showing the benefit of our proposal in\ncomparison to competitive baselines in both synthetic and real benchmarks. We\nalso extend our method to the novel multi-task unsupervised domain adaptation\nsetting. Our code is available at https://github.com/cv-rits/DenseMTL.\n",
        "published": "2022",
        "authors": [
            "Ivan Lopes",
            "Tuan-Hung Vu",
            "Raoul de Charette"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.09474v2",
        "title": "3D Object Detection for Autonomous Driving: A Comprehensive Survey",
        "abstract": "  Autonomous driving, in recent years, has been receiving increasing attention\nfor its potential to relieve drivers' burdens and improve the safety of\ndriving. In modern autonomous driving pipelines, the perception system is an\nindispensable component, aiming to accurately estimate the status of\nsurrounding environments and provide reliable observations for prediction and\nplanning. 3D object detection, which intelligently predicts the locations,\nsizes, and categories of the critical 3D objects near an autonomous vehicle, is\nan important part of a perception system. This paper reviews the advances in 3D\nobject detection for autonomous driving. First, we introduce the background of\n3D object detection and discuss the challenges in this task. Second, we conduct\na comprehensive survey of the progress in 3D object detection from the aspects\nof models and sensory inputs, including LiDAR-based, camera-based, and\nmulti-modal detection approaches. We also provide an in-depth analysis of the\npotentials and challenges in each category of methods. Additionally, we\nsystematically investigate the applications of 3D object detection in driving\nsystems. Finally, we conduct a performance analysis of the 3D object detection\napproaches, and we further summarize the research trends over the years and\nprospect the future directions of this area.\n",
        "published": "2022",
        "authors": [
            "Jiageng Mao",
            "Shaoshuai Shi",
            "Xiaogang Wang",
            "Hongsheng Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.09509v2",
        "title": "Hybrid Facial Expression Recognition (FER2013) Model for Real-Time\n  Emotion Classification and Prediction",
        "abstract": "  Facial Expression Recognition is a vital research topic in most fields\nranging from artificial intelligence and gaming to Human-Computer Interaction\n(HCI) and Psychology. This paper proposes a hybrid model for Facial Expression\nrecognition, which comprises a Deep Convolutional Neural Network (DCNN) and\nHaar Cascade deep learning architectures. The objective is to classify\nreal-time and digital facial images into one of the seven facial emotion\ncategories considered. The DCNN employed in this research has more\nconvolutional layers, ReLU Activation functions, and multiple kernels to\nenhance filtering depth and facial feature extraction. In addition, a haar\ncascade model was also mutually used to detect facial features in real-time\nimages and video frames. Grayscale images from the Kaggle repository (FER-2013)\nand then exploited Graphics Processing Unit (GPU) computation to expedite the\ntraining and validation process. Pre-processing and data augmentation\ntechniques are applied to improve training efficiency and classification\nperformance. The experimental results show a significantly improved\nclassification performance compared to state-of-the-art (SoTA) experiments and\nresearch. Also, compared to other conventional models, this paper validates\nthat the proposed architecture is superior in classification performance with\nan improvement of up to 6%, totaling up to 70% accuracy, and with less\nexecution time of 2098.8s.\n",
        "published": "2022",
        "authors": [
            "Ozioma Collins Oguine",
            "Kanyifeechukwu Jane Oguine",
            "Hashim Ibrahim Bisallah",
            "Daniel Ofuani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.11610v2",
        "title": "1st Place Solutions for RxR-Habitat Vision-and-Language Navigation\n  Competition (CVPR 2022)",
        "abstract": "  This report presents the methods of the winning entry of the RxR-Habitat\nCompetition in CVPR 2022. The competition addresses the problem of\nVision-and-Language Navigation in Continuous Environments (VLN-CE), which\nrequires an agent to follow step-by-step natural language instructions to reach\na target. We present a modular plan-and-control approach for the task. Our\nmodel consists of three modules: the candidate waypoints predictor (CWP), the\nhistory enhanced planner and the tryout controller. In each decision loop, CWP\nfirst predicts a set of candidate waypoints based on depth observations from\nmultiple views. It can reduce the complexity of the action space and facilitate\nplanning. Then, a history-enhanced planner is adopted to select one of the\ncandidate waypoints as the subgoal. The planner additionally encodes historical\nmemory to track the navigation progress, which is especially effective for\nlong-horizon navigation. Finally, we propose a non-parametric heuristic\ncontroller named tryout to execute low-level actions to reach the planned\nsubgoal. It is based on the trial-and-error mechanism which can help the agent\nto avoid obstacles and escape from getting stuck. All three modules work\nhierarchically until the agent stops. We further take several recent advances\nof Vision-and-Language Navigation (VLN) to improve the performance such as\npretraining based on large-scale synthetic in-domain dataset, environment-level\ndata augmentation and snapshot model ensemble. Our model won the RxR-Habitat\nCompetition 2022, with 48% and 90% relative improvements over existing methods\non NDTW and SR metrics respectively.\n",
        "published": "2022",
        "authors": [
            "Dong An",
            "Zun Wang",
            "Yangguang Li",
            "Yi Wang",
            "Yicong Hong",
            "Yan Huang",
            "Liang Wang",
            "Jing Shao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.13294v2",
        "title": "LaRa: Latents and Rays for Multi-Camera Bird's-Eye-View Semantic\n  Segmentation",
        "abstract": "  Recent works in autonomous driving have widely adopted the bird's-eye-view\n(BEV) semantic map as an intermediate representation of the world. Online\nprediction of these BEV maps involves non-trivial operations such as\nmulti-camera data extraction as well as fusion and projection into a common\ntopview grid. This is usually done with error-prone geometric operations (e.g.,\nhomography or back-projection from monocular depth estimation) or expensive\ndirect dense mapping between image pixels and pixels in BEV (e.g., with MLP or\nattention). In this work, we present 'LaRa', an efficient encoder-decoder,\ntransformer-based model for vehicle semantic segmentation from multiple\ncameras. Our approach uses a system of cross-attention to aggregate information\nover multiple sensors into a compact, yet rich, collection of latent\nrepresentations. These latent representations, after being processed by a\nseries of self-attention blocks, are then reprojected with a second\ncross-attention in the BEV space. We demonstrate that our model outperforms the\nbest previous works using transformers on nuScenes. The code and trained models\nare available at https://github.com/valeoai/LaRa\n",
        "published": "2022",
        "authors": [
            "Florent Bartoccioni",
            "\u00c9loi Zablocki",
            "Andrei Bursuc",
            "Patrick P\u00e9rez",
            "Matthieu Cord",
            "Karteek Alahari"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.14116v3",
        "title": "SSL-Lanes: Self-Supervised Learning for Motion Forecasting in Autonomous\n  Driving",
        "abstract": "  Self-supervised learning (SSL) is an emerging technique that has been\nsuccessfully employed to train convolutional neural networks (CNNs) and graph\nneural networks (GNNs) for more transferable, generalizable, and robust\nrepresentation learning. However its potential in motion forecasting for\nautonomous driving has rarely been explored. In this study, we report the first\nsystematic exploration and assessment of incorporating self-supervision into\nmotion forecasting. We first propose to investigate four novel self-supervised\nlearning tasks for motion forecasting with theoretical rationale and\nquantitative and qualitative comparisons on the challenging large-scale\nArgoverse dataset. Secondly, we point out that our auxiliary SSL-based learning\nsetup not only outperforms forecasting methods which use transformers,\ncomplicated fusion mechanisms and sophisticated online dense goal candidate\noptimization algorithms in terms of performance accuracy, but also has low\ninference time and architectural complexity. Lastly, we conduct several\nexperiments to understand why SSL improves motion forecasting. Code is\nopen-sourced at \\url{https://github.com/AutoVision-cloud/SSL-Lanes}.\n",
        "published": "2022",
        "authors": [
            "Prarthana Bhattacharyya",
            "Chengjie Huang",
            "Krzysztof Czarnecki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.15170v1",
        "title": "LiDAR-as-Camera for End-to-End Driving",
        "abstract": "  The core task of any autonomous driving system is to transform sensory inputs\ninto driving commands. In end-to-end driving, this is achieved via a neural\nnetwork, with one or multiple cameras as the most commonly used input and\nlow-level driving command, e.g. steering angle, as output. However,\ndepth-sensing has been shown in simulation to make the end-to-end driving task\neasier. On a real car, combining depth and visual information can be\nchallenging, due to the difficulty of obtaining good spatial and temporal\nalignment of the sensors. To alleviate alignment problems, Ouster LiDARs can\noutput surround-view LiDAR-images with depth, intensity, and ambient radiation\nchannels. These measurements originate from the same sensor, rendering them\nperfectly aligned in time and space. We demonstrate that such LiDAR-images are\nsufficient for the real-car road-following task and perform at least equally to\ncamera-based models in the tested conditions, with the difference increasing\nwhen needing to generalize to new weather conditions. In the second direction\nof study, we reveal that the temporal smoothness of off-policy prediction\nsequences correlates equally well with actual on-policy driving ability as the\ncommonly used mean absolute error.\n",
        "published": "2022",
        "authors": [
            "Ardi Tampuu",
            "Romet Aidla",
            "Jan Are van Gent",
            "Tambet Matiisen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.09934v5",
        "title": "DeepIPC: Deeply Integrated Perception and Control for an Autonomous\n  Vehicle in Real Environments",
        "abstract": "  We propose DeepIPC, an end-to-end autonomous driving model that handles both\nperception and control tasks in driving a vehicle. The model consists of two\nmain parts, perception and controller modules. The perception module takes an\nRGBD image to perform semantic segmentation and bird's eye view (BEV) semantic\nmapping along with providing their encoded features. Meanwhile, the controller\nmodule processes these features with the measurement of GNSS locations and\nangular speed to estimate waypoints that come with latent features. Then, two\ndifferent agents are used to translate waypoints and latent features into a set\nof navigational controls to drive the vehicle. The model is evaluated by\npredicting driving records and performing automated driving under various\nconditions in real environments. The experimental results show that DeepIPC\nachieves the best drivability and multi-task performance even with fewer\nparameters compared to the other models. Codes will be published at\nhttps://github.com/oskarnatan/DeepIPC.\n",
        "published": "2022",
        "authors": [
            "Oskar Natan",
            "Jun Miura"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.11234v1",
        "title": "A System-driven Automatic Ground Truth Generation Method for DL\n  Inner-City Driving Corridor Detectors",
        "abstract": "  Data-driven perception approaches are well-established in automated driving\nsystems. In many fields even super-human performance is reached. Unlike\nprediction and planning approaches, mainly supervised learning algorithms are\nused for the perception domain. Therefore, a major remaining challenge is the\nefficient generation of ground truth data. As perception modules are positioned\nclose to the sensor, they typically run on raw sensor data of high bandwidth.\nDue to that, the generation of ground truth labels typically causes a\nsignificant manual effort, which leads to high costs for the labelling itself\nand the necessary quality control. In this contribution, we propose an\nautomatic labeling approach for semantic segmentation of the drivable ego\ncorridor that reduces the manual effort by a factor of 150 and more. The\nproposed holistic approach could be used in an automated data loop, allowing a\ncontinuous improvement of the depending perception modules.\n",
        "published": "2022",
        "authors": [
            "Jona Ruthardt",
            "Thomas Michalke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.01708v2",
        "title": "Autonomous Agriculture Robot for Smart Farming",
        "abstract": "  This project aims to develop and demonstrate a ground robot with intelligence\ncapable of conducting semi-autonomous farm operations for different low-heights\nvegetable crops referred as Agriculture Application Robot(AAR). AAR is a\nlightweight, solar-electric powered robot that uses intelligent perception for\nconducting detection and classification of plants and their characteristics.\nThe system also has a robotic arm for the autonomous weed cutting process. The\nrobot can deliver fertilizer spraying, insecticide, herbicide, and other fluids\nto the targets such as crops, weeds, and other pests. Besides, it provides\ninformation for future research into higher-level tasks such as yield\nestimation, crop, and soil health monitoring. We present the design of robot\nand the associated experiments which show the promising results in real world\nenvironments.\n",
        "published": "2022",
        "authors": [
            "Vinay Ummadi",
            "Aravind Gundlapalle",
            "Althaf Shaik",
            "Shaik Mohammad Rafi B"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.04530v1",
        "title": "VectorFlow: Combining Images and Vectors for Traffic Occupancy and Flow\n  Prediction",
        "abstract": "  Predicting future behaviors of road agents is a key task in autonomous\ndriving. While existing models have demonstrated great success in predicting\nmarginal agent future behaviors, it remains a challenge to efficiently predict\nconsistent joint behaviors of multiple agents. Recently, the occupancy flow\nfields representation was proposed to represent joint future states of road\nagents through a combination of occupancy grid and flow, which supports\nefficient and consistent joint predictions. In this work, we propose a novel\noccupancy flow fields predictor to produce accurate occupancy and flow\npredictions, by combining the power of an image encoder that learns features\nfrom a rasterized traffic image and a vector encoder that captures information\nof continuous agent trajectories and map states. The two encoded features are\nfused by multiple attention modules before generating final predictions. Our\nsimple but effective model ranks 3rd place on the Waymo Open Dataset Occupancy\nand Flow Prediction Challenge, and achieves the best performance in the\noccluded occupancy and flow prediction task.\n",
        "published": "2022",
        "authors": [
            "Xin Huang",
            "Xiaoyu Tian",
            "Junru Gu",
            "Qiao Sun",
            "Hang Zhao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.06233v1",
        "title": "Dynamic Sensor Matching based on Geomagnetic Inertial Navigation",
        "abstract": "  Optical sensors can capture dynamic environments and derive depth information\nin near real-time. The quality of these digital reconstructions is determined\nby factors like illumination, surface and texture conditions, sensing speed and\nother sensor characteristics as well as the sensor-object relations.\nImprovements can be obtained by using dynamically collected data from multiple\nsensors. However, matching the data from multiple sensors requires a shared\nworld coordinate system. We present a concept for transferring multi-sensor\ndata into a commonly referenced world coordinate system: the earth's magnetic\nfield. The steady presence of our planetary magnetic field provides a reliable\nworld coordinate system, which can serve as a reference for a position-defined\nreconstruction of dynamic environments. Our approach is evaluated using\nmagnetic field sensors of the ZED 2 stereo camera from Stereolabs, which\nprovides orientation relative to the North Pole similar to a compass. With the\nhelp of inertial measurement unit informations, each camera's position data can\nbe transferred into the unified world coordinate system. Our evaluation reveals\nthe level of quality possible using the earth magnetic field and allows a basis\nfor dynamic and real-time-based applications of optical multi-sensors for\nenvironment detection.\n",
        "published": "2022",
        "authors": [
            "Simone M\u00fcller",
            "Dieter Kranzlm\u00fcller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.06494v1",
        "title": "Occlusion-Robust Multi-Sensory Posture Estimation in Physical\n  Human-Robot Interaction",
        "abstract": "  3D posture estimation is important in analyzing and improving ergonomics in\nphysical human-robot interaction and reducing the risk of musculoskeletal\ndisorders. Vision-based posture estimation approaches are prone to sensor and\nmodel errors, as well as occlusion, while posture estimation solely from the\ninteracting robot's trajectory suffers from ambiguous solutions. To benefit\nfrom the advantages of both approaches and improve upon their drawbacks, we\nintroduce a low-cost, non-intrusive, and occlusion-robust multi-sensory 3D\npostural estimation algorithm in physical human-robot interaction. We use 2D\npostures from OpenPose over a single camera, and the trajectory of the\ninteracting robot while the human performs a task. We model the problem as a\npartially-observable dynamical system and we infer the 3D posture via a\nparticle filter. We present our work in teleoperation, but it can be\ngeneralized to other applications of physical human-robot interaction. We show\nthat our multi-sensory system resolves human kinematic redundancy better than\nposture estimation solely using OpenPose or posture estimation solely using the\nrobot's trajectory. This will increase the accuracy of estimated postures\ncompared to the gold-standard motion capture postures. Moreover, our approach\nalso performs better than other single sensory methods when postural assessment\nusing RULA assessment tool.\n",
        "published": "2022",
        "authors": [
            "Amir Yazdani",
            "Roya Sabbagh Novin",
            "Andrew Merryweather",
            "Tucker Hermans"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.14646v1",
        "title": "An Empirical Study and Analysis of Learning Generalizable Manipulation\n  Skill in the SAPIEN Simulator",
        "abstract": "  This paper provides a brief overview of our submission to the no interaction\ntrack of SAPIEN ManiSkill Challenge 2021. Our approach follows an end-to-end\npipeline which mainly consists of two steps: we first extract the point cloud\nfeatures of multiple objects; then we adopt these features to predict the\naction score of the robot simulators through a deep and wide transformer-based\nnetwork. More specially, %to give guidance for future work, to open up avenues\nfor exploitation of learning manipulation skill, we present an empirical study\nthat includes a bag of tricks and abortive attempts. Finally, our method\nachieves a promising ranking on the leaderboard. All code of our solution is\navailable at https://github.com/liu666666/bigfish\\_codes.\n",
        "published": "2022",
        "authors": [
            "Kun Liu",
            "Huiyuan Fu",
            "Zheng Zhang",
            "Huanpu Yin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.01357v1",
        "title": "DualCam: A Novel Benchmark Dataset for Fine-grained Real-time Traffic\n  Light Detection",
        "abstract": "  Traffic light detection is essential for self-driving cars to navigate safely\nin urban areas. Publicly available traffic light datasets are inadequate for\nthe development of algorithms for detecting distant traffic lights that provide\nimportant navigation information. We introduce a novel benchmark traffic light\ndataset captured using a synchronized pair of narrow-angle and wide-angle\ncameras covering urban and semi-urban roads. We provide 1032 images for\ntraining and 813 synchronized image pairs for testing. Additionally, we provide\nsynchronized video pairs for qualitative analysis. The dataset includes images\nof resolution 1920$\\times$1080 covering 10 different classes. Furthermore, we\npropose a post-processing algorithm for combining outputs from the two cameras.\nResults show that our technique can strike a balance between speed and\naccuracy, compared to the conventional approach of using a single camera frame.\n",
        "published": "2022",
        "authors": [
            "Harindu Jayarathne",
            "Tharindu Samarakoon",
            "Hasara Koralege",
            "Asitha Divisekara",
            "Ranga Rodrigo",
            "Peshala Jayasekara"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.01962v6",
        "title": "Adversarial Detection: Attacking Object Detection in Real Time",
        "abstract": "  Intelligent robots rely on object detection models to perceive the\nenvironment. Following advances in deep learning security it has been revealed\nthat object detection models are vulnerable to adversarial attacks. However,\nprior research primarily focuses on attacking static images or offline videos.\nTherefore, it is still unclear if such attacks could jeopardize real-world\nrobotic applications in dynamic environments. This paper bridges this gap by\npresenting the first real-time online attack against object detection models.\nWe devise three attacks that fabricate bounding boxes for nonexistent objects\nat desired locations. The attacks achieve a success rate of about 90% within\nabout 20 iterations. The demo video is available at\nhttps://youtu.be/zJZ1aNlXsMU.\n",
        "published": "2022",
        "authors": [
            "Han Wu",
            "Syed Yunas",
            "Sareh Rowlands",
            "Wenjie Ruan",
            "Johan Wahlstrom"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.04278v2",
        "title": "Deep learning-based Crop Row Detection for Infield Navigation of\n  Agri-Robots",
        "abstract": "  Autonomous navigation in agricultural environments is challenged by varying\nfield conditions that arise in arable fields. State-of-the-art solutions for\nautonomous navigation in such environments require expensive hardware such as\nRTK-GNSS. This paper presents a robust crop row detection algorithm that\nwithstands such field variations using inexpensive cameras. Existing datasets\nfor crop row detection does not represent all the possible field variations. A\ndataset of sugar beet images was created representing 11 field variations\ncomprised of multiple grow stages, light levels, varying weed densities, curved\ncrop rows and discontinuous crop rows. The proposed pipeline segments the crop\nrows using a deep learning-based method and employs the predicted segmentation\nmask for extraction of the central crop using a novel central crop row\nselection algorithm. The novel crop row detection algorithm was tested for crop\nrow detection performance and the capability of visual servoing along a crop\nrow. The visual servoing-based navigation was tested on a realistic simulation\nscenario with the real ground and plant textures. Our algorithm demonstrated\nrobust vision-based crop row detection in challenging field conditions\noutperforming the baseline.\n",
        "published": "2022",
        "authors": [
            "Rajitha de Silva",
            "Grzegorz Cielniak",
            "Gang Wang",
            "Junfeng Gao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.05432v1",
        "title": "Self-supervised Wide Baseline Visual Servoing via 3D Equivariance",
        "abstract": "  One of the challenging input settings for visual servoing is when the initial\nand goal camera views are far apart. Such settings are difficult because the\nwide baseline can cause drastic changes in object appearance and cause\nocclusions. This paper presents a novel self-supervised visual servoing method\nfor wide baseline images which does not require 3D ground truth supervision.\nExisting approaches that regress absolute camera pose with respect to an object\nrequire 3D ground truth data of the object in the forms of 3D bounding boxes or\nmeshes. We learn a coherent visual representation by leveraging a geometric\nproperty called 3D equivariance-the representation is transformed in a\npredictable way as a function of 3D transformation. To ensure that the\nfeature-space is faithful to the underlying geodesic space, a geodesic\npreserving constraint is applied in conjunction with the equivariance. We\ndesign a Siamese network that can effectively enforce these two geometric\nproperties without requiring 3D supervision. With the learned model, the\nrelative transformation can be inferred simply by following the gradient in the\nlearned space and used as feedback for closed-loop visual servoing. Our method\nis evaluated on objects from the YCB dataset, showing meaningful outperformance\non a visual servoing task, or object alignment task with respect to\nstate-of-the-art approaches that use 3D supervision. Ours yields more than 35%\naverage distance error reduction and more than 90% success rate with 3cm error\ntolerance.\n",
        "published": "2022",
        "authors": [
            "Jinwook Huh",
            "Jungseok Hong",
            "Suveer Garg",
            "Hyun Soo Park",
            "Volkan Isler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.06918v1",
        "title": "NanoFlowNet: Real-time Dense Optical Flow on a Nano Quadcopter",
        "abstract": "  Nano quadcopters are small, agile, and cheap platforms that are well suited\nfor deployment in narrow, cluttered environments. Due to their limited payload,\nthese vehicles are highly constrained in processing power, rendering\nconventional vision-based methods for safe and autonomous navigation\nincompatible. Recent machine learning developments promise high-performance\nperception at low latency, while dedicated edge computing hardware has the\npotential to augment the processing capabilities of these limited devices. In\nthis work, we present NanoFlowNet, a lightweight convolutional neural network\nfor real-time dense optical flow estimation on edge computing hardware. We draw\ninspiration from recent advances in semantic segmentation for the design of\nthis network. Additionally, we guide the learning of optical flow using motion\nboundary ground truth data, which improves performance with no impact on\nlatency. Validation results on the MPI-Sintel dataset show the high performance\nof the proposed network given its constrained architecture. Additionally, we\nsuccessfully demonstrate the capabilities of NanoFlowNet by deploying it on the\nultra-low power GAP8 microprocessor and by applying it to vision-based obstacle\navoidance on board a Bitcraze Crazyflie, a 34 g nano quadcopter.\n",
        "published": "2022",
        "authors": [
            "Rik J. Bouwmeester",
            "Federico Paredes-Vall\u00e9s",
            "Guido C. H. E. de Croon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.09786v1",
        "title": "An Outlier Exposure Approach to Improve Visual Anomaly Detection\n  Performance for Mobile Robots",
        "abstract": "  We consider the problem of building visual anomaly detection systems for\nmobile robots. Standard anomaly detection models are trained using large\ndatasets composed only of non-anomalous data. However, in robotics\napplications, it is often the case that (potentially very few) examples of\nanomalies are available. We tackle the problem of exploiting these data to\nimprove the performance of a Real-NVP anomaly detection model, by minimizing,\njointly with the Real-NVP loss, an auxiliary outlier exposure margin loss. We\nperform quantitative experiments on a novel dataset (which we publish as\nsupplementary material) designed for anomaly detection in an indoor patrolling\nscenario. On a disjoint test set, our approach outperforms alternatives and\nshows that exposing even a small number of anomalous frames yields significant\nperformance improvements.\n",
        "published": "2022",
        "authors": [
            "Dario Mantegazza",
            "Alessandro Giusti",
            "Luca Maria Gambardella",
            "J\u00e9r\u00f4me Guzzi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.09874v2",
        "title": "Open-vocabulary Queryable Scene Representations for Real World Planning",
        "abstract": "  Large language models (LLMs) have unlocked new capabilities of task planning\nfrom human instructions. However, prior attempts to apply LLMs to real-world\nrobotic tasks are limited by the lack of grounding in the surrounding scene. In\nthis paper, we develop NLMap, an open-vocabulary and queryable scene\nrepresentation to address this problem. NLMap serves as a framework to gather\nand integrate contextual information into LLM planners, allowing them to see\nand query available objects in the scene before generating a\ncontext-conditioned plan. NLMap first establishes a natural language queryable\nscene representation with Visual Language models (VLMs). An LLM based object\nproposal module parses instructions and proposes involved objects to query the\nscene representation for object availability and location. An LLM planner then\nplans with such information about the scene. NLMap allows robots to operate\nwithout a fixed list of objects nor executable options, enabling real robot\noperation unachievable by previous methods. Project website:\nhttps://nlmap-saycan.github.io\n",
        "published": "2022",
        "authors": [
            "Boyuan Chen",
            "Fei Xia",
            "Brian Ichter",
            "Kanishka Rao",
            "Keerthana Gopalakrishnan",
            "Michael S. Ryoo",
            "Austin Stone",
            "Daniel Kappler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.12674v1",
        "title": "Exploring Attention GAN for Vehicle Motion Prediction",
        "abstract": "  The design of a safe and reliable Autonomous Driving stack (ADS) is one of\nthe most challenging tasks of our era. These ADS are expected to be driven in\nhighly dynamic environments with full autonomy, and a reliability greater than\nhuman beings. In that sense, to efficiently and safely navigate through\narbitrarily complex traffic scenarios, ADS must have the ability to forecast\nthe future trajectories of surrounding actors. Current state-of-the-art models\nare typically based on Recurrent, Graph and Convolutional networks, achieving\nnoticeable results in the context of vehicle prediction. In this paper we\nexplore the influence of attention in generative models for motion prediction,\nconsidering both physical and social context to compute the most plausible\ntrajectories. We first encode the past trajectories using a LSTM network, which\nserves as input to a Multi-Head Self-Attention module that computes the social\ncontext. On the other hand, we formulate a weighted interpolation to calculate\nthe velocity and orientation in the last observation frame in order to\ncalculate acceptable target points, extracted from the driveable of the HDMap\ninformation, which represents our physical context. Finally, the input of our\ngenerator is a white noise vector sampled from a multivariate normal\ndistribution while the social and physical context are its conditions, in order\nto predict plausible trajectories. We validate our method using the Argoverse\nMotion Forecasting Benchmark 1.1, achieving competitive unimodal results.\n",
        "published": "2022",
        "authors": [
            "Carlos G\u00f3mez-Hu\u00e9lamo",
            "Marcos V. Conde",
            "Miguel Ortiz",
            "Santiago Montiel",
            "Rafael Barea",
            "Luis M. Bergasa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.13036v2",
        "title": "MonoGraspNet: 6-DoF Grasping with a Single RGB Image",
        "abstract": "  6-DoF robotic grasping is a long-lasting but unsolved problem. Recent methods\nutilize strong 3D networks to extract geometric grasping representations from\ndepth sensors, demonstrating superior accuracy on common objects but perform\nunsatisfactorily on photometrically challenging objects, e.g., objects in\ntransparent or reflective materials. The bottleneck lies in that the surface of\nthese objects can not reflect back accurate depth due to the absorption or\nrefraction of light. In this paper, in contrast to exploiting the inaccurate\ndepth data, we propose the first RGB-only 6-DoF grasping pipeline called\nMonoGraspNet that utilizes stable 2D features to simultaneously handle\narbitrary object grasping and overcome the problems induced by photometrically\nchallenging objects. MonoGraspNet leverages keypoint heatmap and normal map to\nrecover the 6-DoF grasping poses represented by our novel representation\nparameterized with 2D keypoints with corresponding depth, grasping direction,\ngrasping width, and angle. Extensive experiments in real scenes demonstrate\nthat our method can achieve competitive results in grasping common objects and\nsurpass the depth-based competitor by a large margin in grasping\nphotometrically challenging objects. To further stimulate robotic manipulation\nresearch, we additionally annotate and open-source a multi-view and multi-scene\nreal-world grasping dataset, containing 120 objects of mixed photometric\ncomplexity with 20M accurate grasping labels.\n",
        "published": "2022",
        "authors": [
            "Guangyao Zhai",
            "Dianye Huang",
            "Shun-Cheng Wu",
            "Hyunjun Jung",
            "Yan Di",
            "Fabian Manhardt",
            "Federico Tombari",
            "Nassir Navab",
            "Benjamin Busam"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.15370v1",
        "title": "Automatic Context-Driven Inference of Engagement in HMI: A Survey",
        "abstract": "  An integral part of seamless human-human communication is engagement, the\nprocess by which two or more participants establish, maintain, and end their\nperceived connection. Therefore, to develop successful human-centered\nhuman-machine interaction applications, automatic engagement inference is one\nof the tasks required to achieve engaging interactions between humans and\nmachines, and to make machines attuned to their users, hence enhancing user\nsatisfaction and technology acceptance. Several factors contribute to\nengagement state inference, which include the interaction context and\ninteractants' behaviours and identity. Indeed, engagement is a multi-faceted\nand multi-modal construct that requires high accuracy in the analysis and\ninterpretation of contextual, verbal and non-verbal cues. Thus, the development\nof an automated and intelligent system that accomplishes this task has been\nproven to be challenging so far. This paper presents a comprehensive survey on\nprevious work in engagement inference for human-machine interaction, entailing\ninterdisciplinary definition, engagement components and factors, publicly\navailable datasets, ground truth assessment, and most commonly used features\nand methods, serving as a guide for the development of future human-machine\ninteraction interfaces with reliable context-aware engagement inference\ncapability. An in-depth review across embodied and disembodied interaction\nmodes, and an emphasis on the interaction context of which engagement\nperception modules are integrated sets apart the presented survey from existing\nsurveys.\n",
        "published": "2022",
        "authors": [
            "Hanan Salam",
            "Oya Celiktutan",
            "Hatice Gunes",
            "Mohamed Chetouani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.07729v2",
        "title": "Model-Based Imitation Learning for Urban Driving",
        "abstract": "  An accurate model of the environment and the dynamic agents acting in it\noffers great potential for improving motion planning. We present MILE: a\nModel-based Imitation LEarning approach to jointly learn a model of the world\nand a policy for autonomous driving. Our method leverages 3D geometry as an\ninductive bias and learns a highly compact latent space directly from\nhigh-resolution videos of expert demonstrations. Our model is trained on an\noffline corpus of urban driving data, without any online interaction with the\nenvironment. MILE improves upon prior state-of-the-art by 31% in driving score\non the CARLA simulator when deployed in a completely new town and new weather\nconditions. Our model can predict diverse and plausible states and actions,\nthat can be interpretably decoded to bird's-eye view semantic segmentation.\nFurther, we demonstrate that it can execute complex driving manoeuvres from\nplans entirely predicted in imagination. Our approach is the first camera-only\nmethod that models static scene, dynamic scene, and ego-behaviour in an urban\ndriving environment. The code and model weights are available at\nhttps://github.com/wayveai/mile.\n",
        "published": "2022",
        "authors": [
            "Anthony Hu",
            "Gianluca Corrado",
            "Nicolas Griffiths",
            "Zak Murez",
            "Corina Gurau",
            "Hudson Yeo",
            "Alex Kendall",
            "Roberto Cipolla",
            "Jamie Shotton"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.12521v1",
        "title": "H-SAUR: Hypothesize, Simulate, Act, Update, and Repeat for Understanding\n  Object Articulations from Interactions",
        "abstract": "  The world is filled with articulated objects that are difficult to determine\nhow to use from vision alone, e.g., a door might open inwards or outwards.\nHumans handle these objects with strategic trial-and-error: first pushing a\ndoor then pulling if that doesn't work. We enable these capabilities in\nautonomous agents by proposing \"Hypothesize, Simulate, Act, Update, and Repeat\"\n(H-SAUR), a probabilistic generative framework that simultaneously generates a\ndistribution of hypotheses about how objects articulate given input\nobservations, captures certainty over hypotheses over time, and infer plausible\nactions for exploration and goal-conditioned manipulation. We compare our model\nwith existing work in manipulating objects after a handful of exploration\nactions, on the PartNet-Mobility dataset. We further propose a novel\nPuzzleBoxes benchmark that contains locked boxes that require multiple steps to\nsolve. We show that the proposed model significantly outperforms the current\nstate-of-the-art articulated object manipulation framework, despite using zero\ntraining data. We further improve the test-time efficiency of H-SAUR by\nintegrating a learned prior from learning-based vision models.\n",
        "published": "2022",
        "authors": [
            "Kei Ota",
            "Hsiao-Yu Tung",
            "Kevin A. Smith",
            "Anoop Cherian",
            "Tim K. Marks",
            "Alan Sullivan",
            "Asako Kanezaki",
            "Joshua B. Tenenbaum"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.01600v1",
        "title": "nerf2nerf: Pairwise Registration of Neural Radiance Fields",
        "abstract": "  We introduce a technique for pairwise registration of neural fields that\nextends classical optimization-based local registration (i.e. ICP) to operate\non Neural Radiance Fields (NeRF) -- neural 3D scene representations trained\nfrom collections of calibrated images. NeRF does not decompose illumination and\ncolor, so to make registration invariant to illumination, we introduce the\nconcept of a ''surface field'' -- a field distilled from a pre-trained NeRF\nmodel that measures the likelihood of a point being on the surface of an\nobject. We then cast nerf2nerf registration as a robust optimization that\niteratively seeks a rigid transformation that aligns the surface fields of the\ntwo scenes. We evaluate the effectiveness of our technique by introducing a\ndataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative\nevaluations and comparisons to classical registration techniques, while our\nreal scenes demonstrate the validity of our technique in real-world scenarios.\nAdditional results available at: https://nerf2nerf.github.io\n",
        "published": "2022",
        "authors": [
            "Lily Goli",
            "Daniel Rebain",
            "Sara Sabour",
            "Animesh Garg",
            "Andrea Tagliasacchi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.03367v1",
        "title": "Semantic-Aware Environment Perception for Mobile Human-Robot Interaction",
        "abstract": "  Current technological advances open up new opportunities for bringing\nhuman-machine interaction to a new level of human-centered cooperation. In this\ncontext, a key issue is the semantic understanding of the environment in order\nto enable mobile robots more complex interactions and a facilitated\ncommunication with humans. Prerequisites are the vision-based registration of\nsemantic objects and humans, where the latter are further analyzed for\npotential interaction partners. Despite significant research achievements, the\nreliable and fast registration of semantic information still remains a\nchallenging task for mobile robots in real-world scenarios. In this paper, we\npresent a vision-based system for mobile assistive robots to enable a\nsemantic-aware environment perception without additional a-priori knowledge. We\ndeploy our system on a mobile humanoid robot that enables us to test our\nmethods in real-world applications.\n",
        "published": "2022",
        "authors": [
            "Thorsten Hempel",
            "Marc-Andr\u00e9 Fiedler",
            "Aly Khalifa",
            "Ayoub Al-Hamadi",
            "Laslo Dinges"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.04718v2",
        "title": "On the Application of Efficient Neural Mapping to Real-Time Indoor\n  Localisation for Unmanned Ground Vehicles",
        "abstract": "  Global localisation from visual data is a challenging problem applicable to\nmany robotics domains. Prior works have shown that neural networks can be\ntrained to map images of an environment to absolute camera pose within that\nenvironment, learning an implicit neural mapping in the process. In this work\nwe evaluate the applicability of such an approach to real-world robotics\nscenarios, demonstrating that by constraining the problem to 2-dimensions and\nsignificantly increasing the quantity of training data, a compact model capable\nof real-time inference on embedded platforms can be used to achieve\nlocalisation accuracy of several centimetres. We deploy our trained model\nonboard a UGV platform, demonstrating its effectiveness in a waypoint\nnavigation task, wherein it is able to localise with a mean accuracy of 9cm at\na rate of 6fps running on the UGV onboard CPU, 35fps on an embedded GPU, or\n220fps on a desktop GPU. Along with this work we will release a novel\nlocalisation dataset comprising simulated and real environments, each with\ntraining samples numbering in the tens of thousands.\n",
        "published": "2022",
        "authors": [
            "Christopher J. Holder",
            "Muhammad Shafique"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.02497v1",
        "title": "PEANUT: Predicting and Navigating to Unseen Targets",
        "abstract": "  Efficient ObjectGoal navigation (ObjectNav) in novel environments requires an\nunderstanding of the spatial and semantic regularities in environment layouts.\nIn this work, we present a straightforward method for learning these\nregularities by predicting the locations of unobserved objects from incomplete\nsemantic maps. Our method differs from previous prediction-based navigation\nmethods, such as frontier potential prediction or egocentric map completion, by\ndirectly predicting unseen targets while leveraging the global context from all\npreviously explored areas. Our prediction model is lightweight and can be\ntrained in a supervised manner using a relatively small amount of passively\ncollected data. Once trained, the model can be incorporated into a modular\npipeline for ObjectNav without the need for any reinforcement learning. We\nvalidate the effectiveness of our method on the HM3D and MP3D ObjectNav\ndatasets. We find that it achieves the state-of-the-art on both datasets,\ndespite not using any additional data for training.\n",
        "published": "2022",
        "authors": [
            "Albert J. Zhai",
            "Shenlong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.04819v1",
        "title": "Phone2Proc: Bringing Robust Robots Into Our Chaotic World",
        "abstract": "  Training embodied agents in simulation has become mainstream for the embodied\nAI community. However, these agents often struggle when deployed in the\nphysical world due to their inability to generalize to real-world environments.\nIn this paper, we present Phone2Proc, a method that uses a 10-minute phone scan\nand conditional procedural generation to create a distribution of training\nscenes that are semantically similar to the target environment. The generated\nscenes are conditioned on the wall layout and arrangement of large objects from\nthe scan, while also sampling lighting, clutter, surface textures, and\ninstances of smaller objects with randomized placement and materials.\nLeveraging just a simple RGB camera, training with Phone2Proc shows massive\nimprovements from 34.7% to 70.7% success rate in sim-to-real ObjectNav\nperformance across a test suite of over 200 trials in diverse real-world\nenvironments, including homes, offices, and RoboTHOR. Furthermore, Phone2Proc's\ndiverse distribution of generated scenes makes agents remarkably robust to\nchanges in the real world, such as human movement, object rearrangement,\nlighting changes, or clutter.\n",
        "published": "2022",
        "authors": [
            "Matt Deitke",
            "Rose Hendrix",
            "Luca Weihs",
            "Ali Farhadi",
            "Kiana Ehsani",
            "Aniruddha Kembhavi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.01350v1",
        "title": "LunarNav: Crater-based Localization for Long-range Autonomous Lunar\n  Rover Navigation",
        "abstract": "  The Artemis program requires robotic and crewed lunar rovers for resource\nprospecting and exploitation, construction and maintenance of facilities, and\nhuman exploration. These rovers must support navigation for 10s of kilometers\n(km) from base camps. A lunar science rover mission concept - Endurance-A, has\nbeen recommended by the new Decadal Survey as the highest priority medium-class\nmission of the Lunar Discovery and Exploration Program, and would be required\nto traverse approximately 2000 km in the South Pole-Aitkin (SPA) Basin, with\nindividual drives of several kilometers between stops for downlink. These rover\nmission scenarios require functionality that provides onboard, autonomous,\nglobal position knowledge ( aka absolute localization). However, planetary\nrovers have no onboard global localization capability to date; they have only\nused relative localization, by integrating combinations of wheel odometry,\nvisual odometry, and inertial measurements during each drive to track position\nrelative to the start of each drive. In this work, we summarize recent\ndevelopments from the LunarNav project, where we have developed algorithms and\nsoftware to enable lunar rovers to estimate their global position and heading\non the Moon with a goal performance of position error less than 5 meters (m)\nand heading error less than 3-degree, 3-sigma, in sunlit areas. This will be\nachieved autonomously onboard by detecting craters in the vicinity of the rover\nand matching them to a database of known craters mapped from orbit. The overall\ntechnical framework consists of three main elements: 1) crater detection, 2)\ncrater matching, and 3) state estimation. In previous work, we developed crater\ndetection algorithms for three different sensing modalities. Our results\nsuggest that rover localization with an error less than 5 m is highly probable\nduring daytime operations.\n",
        "published": "2023",
        "authors": [
            "Shreyansh Daftry",
            "Zhanlin Chen",
            "Yang Cheng",
            "Scott Tepsuporn",
            "Brian Coltin",
            "Ussama Naam",
            "Lanssie Mingyue Ma",
            "Shehryar Khattak",
            "Matthew Deans",
            "Larry Matthies"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.04352v1",
        "title": "Graph based Environment Representation for Vision-and-Language\n  Navigation in Continuous Environments",
        "abstract": "  Vision-and-Language Navigation in Continuous Environments (VLN-CE) is a\nnavigation task that requires an agent to follow a language instruction in a\nrealistic environment. The understanding of environments is a crucial part of\nthe VLN-CE task, but existing methods are relatively simple and direct in\nunderstanding the environment, without delving into the relationship between\nlanguage instructions and visual environments. Therefore, we propose a new\nenvironment representation in order to solve the above problems. First, we\npropose an Environment Representation Graph (ERG) through object detection to\nexpress the environment in semantic level. This operation enhances the\nrelationship between language and environment. Then, the relational\nrepresentations of object-object, object-agent in ERG are learned through GCN,\nso as to obtain a continuous expression about ERG. Sequentially, we combine the\nERG expression with object label embeddings to obtain the environment\nrepresentation. Finally, a new cross-modal attention navigation framework is\nproposed, incorporating our environment representation and a special loss\nfunction dedicated to training ERG. Experimental result shows that our method\nachieves satisfactory performance in terms of success rate on VLN-CE tasks.\nFurther analysis explains that our method attains better cross-modal matching\nand strong generalization ability.\n",
        "published": "2023",
        "authors": [
            "Ting Wang",
            "Zongkai Wu",
            "Feiyu Yao",
            "Donglin Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.04454v1",
        "title": "Allo-centric Occupancy Grid Prediction for Urban Traffic Scene Using\n  Video Prediction Networks",
        "abstract": "  Prediction of dynamic environment is crucial to safe navigation of an\nautonomous vehicle. Urban traffic scenes are particularly challenging to\nforecast due to complex interactions between various dynamic agents, such as\nvehicles and vulnerable road users. Previous approaches have used egocentric\noccupancy grid maps to represent and predict dynamic environments. However,\nthese predictions suffer from blurriness, loss of scene structure at turns, and\nvanishing of agents over longer prediction horizon. In this work, we propose a\nnovel framework to make long-term predictions by representing the traffic scene\nin a fixed frame, referred as allo-centric occupancy grid. This allows for the\nstatic scene to remain fixed and to represent motion of the ego-vehicle on the\ngrid like other agents'. We study the allo-centric grid prediction with\ndifferent video prediction networks and validate the approach on the real-world\nNuscenes dataset. The results demonstrate that the allo-centric grid\nrepresentation significantly improves scene prediction, in comparison to the\nconventional ego-centric grid approach.\n",
        "published": "2023",
        "authors": [
            "Rabbia Asghar",
            "Lukas Rummelhard",
            "Anne Spalanzani",
            "Christian Laugier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.11522v1",
        "title": "A Comparison of Tiny-nerf versus Spatial Representations for 3d\n  Reconstruction",
        "abstract": "  Neural rendering has emerged as a powerful paradigm for synthesizing images,\noffering many benefits over classical rendering by using neural networks to\nreconstruct surfaces, represent shapes, and synthesize novel views, either for\nobjects or scenes. In this neural rendering, the environment is encoded into a\nneural network. We believe that these new representations can be used to codify\nthe scene for a mobile robot. Therefore, in this work, we perform a comparison\nbetween a trending neural rendering, called tiny-NeRF, and other volume\nrepresentations that are commonly used as maps in robotics, such as voxel maps,\npoint clouds, and triangular meshes. The target is to know the advantages and\ndisadvantages of neural representations in the robotics context. The comparison\nis made in terms of spatial complexity and processing time to obtain a model.\nExperiments show that tiny-NeRF requires three times less memory space compared\nto other representations. In terms of processing time, tiny-NeRF takes about\nsix times more to compute the model.\n",
        "published": "2023",
        "authors": [
            "Saulo Abraham Gante",
            "Juan Irving Vasquez",
            "Marco Antonio Valencia",
            "Mauricio Olgu\u00edn Carbajal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.12614v1",
        "title": "RREx-BoT: Remote Referring Expressions with a Bag of Tricks",
        "abstract": "  Household robots operate in the same space for years. Such robots\nincrementally build dynamic maps that can be used for tasks requiring remote\nobject localization. However, benchmarks in robot learning often test\ngeneralization through inference on tasks in unobserved environments. In an\nobserved environment, locating an object is reduced to choosing from among all\nobject proposals in the environment, which may number in the 100,000s. Armed\nwith this intuition, using only a generic vision-language scoring model with\nminor modifications for 3d encoding and operating in an embodied environment,\nwe demonstrate an absolute performance gain of 9.84% on remote object grounding\nabove state of the art models for REVERIE and of 5.04% on FAO. When allowed to\npre-explore an environment, we also exceed the previous state of the art\npre-exploration method on REVERIE. Additionally, we demonstrate our model on a\nreal-world TurtleBot platform, highlighting the simplicity and usefulness of\nthe approach. Our analysis outlines a \"bag of tricks\" essential for\naccomplishing this task, from utilizing 3d coordinates and context, to\ngeneralizing vision-language models to large 3d search spaces.\n",
        "published": "2023",
        "authors": [
            "Gunnar A. Sigurdsson",
            "Jesse Thomason",
            "Gaurav S. Sukhatme",
            "Robinson Piramuthu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.01295v1",
        "title": "Ditto in the House: Building Articulation Models of Indoor Scenes\n  through Interactive Perception",
        "abstract": "  Virtualizing the physical world into virtual models has been a critical\ntechnique for robot navigation and planning in the real world. To foster\nmanipulation with articulated objects in everyday life, this work explores\nbuilding articulation models of indoor scenes through a robot's purposeful\ninteractions in these scenes. Prior work on articulation reasoning primarily\nfocuses on siloed objects of limited categories. To extend to room-scale\nenvironments, the robot has to efficiently and effectively explore a\nlarge-scale 3D space, locate articulated objects, and infer their\narticulations. We introduce an interactive perception approach to this task.\nOur approach, named Ditto in the House, discovers possible articulated objects\nthrough affordance prediction, interacts with these objects to produce\narticulated motions, and infers the articulation properties from the visual\nobservations before and after each interaction. It tightly couples affordance\nprediction and articulation inference to improve both tasks. We demonstrate the\neffectiveness of our approach in both simulation and real-world scenes. Code\nand additional results are available at\nhttps://ut-austin-rpl.github.io/HouseDitto/\n",
        "published": "2023",
        "authors": [
            "Cheng-Chun Hsu",
            "Zhenyu Jiang",
            "Yuke Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.04486v1",
        "title": "A General Mobile Manipulator Automation Framework for Flexible\n  Manufacturing in Hostile Industrial Environments",
        "abstract": "  To enable a mobile manipulator to perform human tasks from a single teaching\ndemonstration is vital to flexible manufacturing. We call our proposed method\nMMPA (Mobile Manipulator Process Automation with One-shot Teaching). Currently,\nthere is no effective and robust MMPA framework which is not influenced by\nharsh industrial environments and the mobile base's parking precision. The\nproposed MMPA framework consists of two stages: collecting data (mobile base's\nlocation, environment information, end-effector's path) in the teaching stage\nfor robot learning; letting the end-effector repeat the nearly same path as the\nreference path in the world frame to reproduce the work in the automation\nstage. More specifically, in the automation stage, the robot navigates to the\nspecified location without the need of a precise parking. Then, based on\ncolored point cloud registration, the proposed IPE (Iterative Pose Estimation\nby Eye & Hand) algorithm could estimate the accurate 6D relative parking pose\nof the robot arm base without the need of any marker. Finally, the robot could\nlearn the error compensation from the parking pose's bias to modify the\nend-effector's path to make it repeat a nearly same path in the world\ncoordinate system as recorded in the teaching stage. Hundreds of trials have\nbeen conducted with a real mobile manipulator to show the superior robustness\nof the system and the accuracy of the process automation regardless of the\nharsh industrial conditions and parking precision. For the released code,\nplease contact marketing@amigaga.com\n",
        "published": "2023",
        "authors": [
            "Can Pu",
            "Chuanyu Yang",
            "Jinnian Pu",
            "Robert B. Fisher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.06414v1",
        "title": "LAPTNet-FPN: Multi-scale LiDAR-aided Projective Transform Network for\n  Real Time Semantic Grid Prediction",
        "abstract": "  Semantic grids can be useful representations of the scene around an\nautonomous system. By having information about the layout of the space around\nitself, a robot can leverage this type of representation for crucial tasks such\nas navigation or tracking. By fusing information from multiple sensors,\nrobustness can be increased and the computational load for the task can be\nlowered, achieving real time performance. Our multi-scale LiDAR-Aided\nPerspective Transform network uses information available in point clouds to\nguide the projection of image features to a top-view representation, resulting\nin a relative improvement in the state of the art for semantic grid generation\nfor human (+8.67%) and movable object (+49.07%) classes in the nuScenes\ndataset, as well as achieving results close to the state of the art for the\nvehicle, drivable area and walkway classes, while performing inference at 25\nFPS.\n",
        "published": "2023",
        "authors": [
            "Manuel Alejandro Diaz-Zapata",
            "David Sierra Gonz\u00e1lez",
            "\u00d6zg\u00fcr Erkent",
            "Jilles Dibangoye",
            "Christian Laugier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.07241v3",
        "title": "ConceptFusion: Open-set Multimodal 3D Mapping",
        "abstract": "  Building 3D maps of the environment is central to robot navigation, planning,\nand interaction with objects in a scene. Most existing approaches that\nintegrate semantic concepts with 3D maps largely remain confined to the\nclosed-set setting: they can only reason about a finite set of concepts,\npre-defined at training time. Further, these maps can only be queried using\nclass labels, or in recent work, using text prompts.\n  We address both these issues with ConceptFusion, a scene representation that\nis (1) fundamentally open-set, enabling reasoning beyond a closed set of\nconcepts and (ii) inherently multimodal, enabling a diverse range of possible\nqueries to the 3D map, from language, to images, to audio, to 3D geometry, all\nworking in concert. ConceptFusion leverages the open-set capabilities of\ntoday's foundation models pre-trained on internet-scale data to reason about\nconcepts across modalities such as natural language, images, and audio. We\ndemonstrate that pixel-aligned open-set features can be fused into 3D maps via\ntraditional SLAM and multi-view fusion approaches. This enables effective\nzero-shot spatial reasoning, not needing any additional training or finetuning,\nand retains long-tailed concepts better than supervised approaches,\noutperforming them by more than 40% margin on 3D IoU. We extensively evaluate\nConceptFusion on a number of real-world datasets, simulated home environments,\na real-world tabletop manipulation task, and an autonomous driving platform. We\nshowcase new avenues for blending foundation models with 3D open-set multimodal\nmapping.\n  For more information, visit our project page https://concept-fusion.github.io\nor watch our 5-minute explainer video\nhttps://www.youtube.com/watch?v=rkXgws8fiDs\n",
        "published": "2023",
        "authors": [
            "Krishna Murthy Jatavallabhula",
            "Alihusein Kuwajerwala",
            "Qiao Gu",
            "Mohd Omama",
            "Tao Chen",
            "Alaa Maalouf",
            "Shuang Li",
            "Ganesh Iyer",
            "Soroush Saryazdi",
            "Nikhil Keetha",
            "Ayush Tewari",
            "Joshua B. Tenenbaum",
            "Celso Miguel de Melo",
            "Madhava Krishna",
            "Liam Paull",
            "Florian Shkurti",
            "Antonio Torralba"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2302.10445v1",
        "title": "Graph-Transporter: A Graph-based Learning Method for Goal-Conditioned\n  Deformable Object Rearranging Task",
        "abstract": "  Rearranging deformable objects is a long-standing challenge in robotic\nmanipulation for the high dimensionality of configuration space and the complex\ndynamics of deformable objects. We present a novel framework,\nGraph-Transporter, for goal-conditioned deformable object rearranging tasks. To\ntackle the challenge of complex configuration space and dynamics, we represent\nthe configuration space of a deformable object with a graph structure and the\ngraph features are encoded by a graph convolution network. Our framework adopts\nan architecture based on Fully Convolutional Network (FCN) to output pixel-wise\npick-and-place actions from only visual input. Extensive experiments have been\nconducted to validate the effectiveness of the graph representation of\ndeformable object configuration. The experimental results also demonstrate that\nour framework is effective and general in handling goal-conditioned deformable\nobject rearranging tasks.\n",
        "published": "2023",
        "authors": [
            "Yuhong Deng",
            "Chongkun Xia",
            "Xueqian Wang",
            "Lipeng Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.02401v5",
        "title": "Open-Vocabulary Affordance Detection in 3D Point Clouds",
        "abstract": "  Affordance detection is a challenging problem with a wide variety of robotic\napplications. Traditional affordance detection methods are limited to a\npredefined set of affordance labels, hence potentially restricting the\nadaptability of intelligent robots in complex and dynamic environments. In this\npaper, we present the Open-Vocabulary Affordance Detection (OpenAD) method,\nwhich is capable of detecting an unbounded number of affordances in 3D point\nclouds. By simultaneously learning the affordance text and the point feature,\nOpenAD successfully exploits the semantic relationships between affordances.\nTherefore, our proposed method enables zero-shot detection and can be able to\ndetect previously unseen affordances without a single annotation example.\nIntensive experimental results show that OpenAD works effectively on a wide\nrange of affordance detection setups and outperforms other baselines by a large\nmargin. Additionally, we demonstrate the practicality of the proposed OpenAD in\nreal-world robotic applications with a fast inference speed (~100ms). Our\nproject is available at https://openad2023.github.io.\n",
        "published": "2023",
        "authors": [
            "Toan Nguyen",
            "Minh Nhat Vu",
            "An Vuong",
            "Dzung Nguyen",
            "Thieu Vo",
            "Ngan Le",
            "Anh Nguyen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.03178v2",
        "title": "A System for Generalized 3D Multi-Object Search",
        "abstract": "  Searching for objects is a fundamental skill for robots. As such, we expect\nobject search to eventually become an off-the-shelf capability for robots,\nsimilar to e.g., object detection and SLAM. In contrast, however, no system for\n3D object search exists that generalizes across real robots and environments.\nIn this paper, building upon a recent theoretical framework that exploited the\noctree structure for representing belief in 3D, we present GenMOS (Generalized\nMulti-Object Search), the first general-purpose system for multi-object search\n(MOS) in a 3D region that is robot-independent and environment-agnostic. GenMOS\ntakes as input point cloud observations of the local region, object detection\nresults, and localization of the robot's view pose, and outputs a 6D viewpoint\nto move to through online planning. In particular, GenMOS uses point cloud\nobservations in three ways: (1) to simulate occlusion; (2) to inform occupancy\nand initialize octree belief; and (3) to sample a belief-dependent graph of\nview positions that avoid obstacles. We evaluate our system both in simulation\nand on two real robot platforms. Our system enables, for example, a Boston\nDynamics Spot robot to find a toy cat hidden underneath a couch in under one\nminute. We further integrate 3D local search with 2D global search to handle\nlarger areas, demonstrating the resulting system in a 25m$^2$ lobby area.\n",
        "published": "2023",
        "authors": [
            "Kaiyu Zheng",
            "Anirudha Paul",
            "Stefanie Tellex"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.03508v1",
        "title": "Memory Maps for Video Object Detection and Tracking on UAVs",
        "abstract": "  This paper introduces a novel approach to video object detection detection\nand tracking on Unmanned Aerial Vehicles (UAVs). By incorporating metadata, the\nproposed approach creates a memory map of object locations in actual world\ncoordinates, providing a more robust and interpretable representation of object\nlocations in both, image space and the real world. We use this representation\nto boost confidences, resulting in improved performance for several temporal\ncomputer vision tasks, such as video object detection, short and long-term\nsingle and multi-object tracking, and video anomaly detection. These findings\nconfirm the benefits of metadata in enhancing the capabilities of UAVs in the\nfield of temporal computer vision and pave the way for further advancements in\nthis area.\n",
        "published": "2023",
        "authors": [
            "Benjamin Kiefer",
            "Yitong Quan",
            "Andreas Zell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.03943v2",
        "title": "CUREE: A Curious Underwater Robot for Ecosystem Exploration",
        "abstract": "  The current approach to exploring and monitoring complex underwater\necosystems, such as coral reefs, is to conduct surveys using diver-held or\nstatic cameras, or deploying sensor buoys. These approaches often fail to\ncapture the full variation and complexity of interactions between different\nreef organisms and their habitat. The CUREE platform presented in this paper\nprovides a unique set of capabilities in the form of robot behaviors and\nperception algorithms to enable scientists to explore different aspects of an\necosystem. Examples of these capabilities include low-altitude visual surveys,\nsoundscape surveys, habitat characterization, and animal following. We\ndemonstrate these capabilities by describing two field deployments on coral\nreefs in the US Virgin Islands. In the first deployment, we show that CUREE can\nidentify the preferred habitat type of snapping shrimp in a reef through a\ncombination of a visual survey, habitat characterization, and a soundscape\nsurvey. In the second deployment, we demonstrate CUREE's ability to follow\narbitrary animals by separately following a barracuda and stingray for several\nminutes each in midwater and benthic environments, respectively.\n",
        "published": "2023",
        "authors": [
            "Yogesh Girdhar",
            "Nathan McGuire",
            "Levi Cai",
            "Stewart Jamieson",
            "Seth McCammon",
            "Brian Claus",
            "John E. San Soucie",
            "Jessica E. Todd",
            "T. Aran Mooney"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.04077v1",
        "title": "Meta-Explore: Exploratory Hierarchical Vision-and-Language Navigation\n  Using Scene Object Spectrum Grounding",
        "abstract": "  The main challenge in vision-and-language navigation (VLN) is how to\nunderstand natural-language instructions in an unseen environment. The main\nlimitation of conventional VLN algorithms is that if an action is mistaken, the\nagent fails to follow the instructions or explores unnecessary regions, leading\nthe agent to an irrecoverable path. To tackle this problem, we propose\nMeta-Explore, a hierarchical navigation method deploying an exploitation policy\nto correct misled recent actions. We show that an exploitation policy, which\nmoves the agent toward a well-chosen local goal among unvisited but observable\nstates, outperforms a method which moves the agent to a previously visited\nstate. We also highlight the demand for imagining regretful explorations with\nsemantically meaningful clues. The key to our approach is understanding the\nobject placements around the agent in spectral-domain. Specifically, we present\na novel visual representation, called scene object spectrum (SOS), which\nperforms category-wise 2D Fourier transform of detected objects. Combining\nexploitation policy and SOS features, the agent can correct its path by\nchoosing a promising local goal. We evaluate our method in three VLN\nbenchmarks: R2R, SOON, and REVERIE. Meta-Explore outperforms other baselines\nand shows significant generalization performance. In addition, local goal\nsearch using the proposed spectral-domain SOS features significantly improves\nthe success rate by 17.1% and SPL by 20.6% for the SOON benchmark.\n",
        "published": "2023",
        "authors": [
            "Minyoung Hwang",
            "Jaeyeon Jeong",
            "Minsoo Kim",
            "Yoonseon Oh",
            "Songhwai Oh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.04302v1",
        "title": "Camera-Radar Perception for Autonomous Vehicles and ADAS: Concepts,\n  Datasets and Metrics",
        "abstract": "  One of the main paths towards the reduction of traffic accidents is the\nincrease in vehicle safety through driver assistance systems or even systems\nwith a complete level of autonomy. In these types of systems, tasks such as\nobstacle detection and segmentation, especially the Deep Learning-based ones,\nplay a fundamental role in scene understanding for correct and safe navigation.\nBesides that, the wide variety of sensors in vehicles nowadays provides a rich\nset of alternatives for improvement in the robustness of perception in\nchallenging situations, such as navigation under lighting and weather adverse\nconditions. Despite the current focus given to the subject, the literature\nlacks studies on radar-based and radar-camera fusion-based perception. Hence,\nthis work aims to carry out a study on the current scenario of camera and\nradar-based perception for ADAS and autonomous vehicles. Concepts and\ncharacteristics related to both sensors, as well as to their fusion, are\npresented. Additionally, we give an overview of the Deep Learning-based\ndetection and segmentation tasks, and the main datasets, metrics, challenges,\nand open questions in vehicle perception.\n",
        "published": "2023",
        "authors": [
            "Felipe Manfio Barbosa",
            "Fernando Santos Os\u00f3rio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.08105v2",
        "title": "Image Guidance for Robot-Assisted Ankle Fracture Repair",
        "abstract": "  This project concerns developing and validating an image guidance framework\nfor application to a robotic-assisted fibular reduction in ankle fracture\nsurgery. The aim is to produce and demonstrate proper functioning of software\nfor automatic determination of directions for fibular repositioning with the\nultimate goal of application to a robotic reduction procedure that can reduce\nthe time and complexity of the procedure as well as provide the benefits of\nreduced error in ideal final fibular position, improved syndesmosis restoration\nand reduced incidence of post-traumatic osteoarthritis. The focus of this\nproduct will be developing and testing the image guidance software, from the\ninput of preoperative images through the steps of automated segmentation and\nregistration until the output of a final transformation that can be used as\ninstructions to a robot on how to reposition the fibula, but will not involve\ndeveloping or implementing the hardware of the robot itself.\n",
        "published": "2023",
        "authors": [
            "Asef Islam",
            "Anthony Wu",
            "Jay Mandavilli",
            "Wojtek Zbijewski",
            "Jeff Siewerdsen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.13182v1",
        "title": "CMG-Net: An End-to-End Contact-Based Multi-Finger Dexterous Grasping\n  Network",
        "abstract": "  In this paper, we propose a novel representation for grasping using contacts\nbetween multi-finger robotic hands and objects to be manipulated. This\nrepresentation significantly reduces the prediction dimensions and accelerates\nthe learning process. We present an effective end-to-end network, CMG-Net, for\ngrasping unknown objects in a cluttered environment by efficiently predicting\nmulti-finger grasp poses and hand configurations from a single-shot point\ncloud. Moreover, we create a synthetic grasp dataset that consists of five\nthousand cluttered scenes, 80 object categories, and 20 million annotations. We\nperform a comprehensive empirical study and demonstrate the effectiveness of\nour grasping representation and CMG-Net. Our work significantly outperforms the\nstate-of-the-art for three-finger robotic hands. We also demonstrate that the\nmodel trained using synthetic data performs very well for real robots.\n",
        "published": "2023",
        "authors": [
            "Mingze Wei",
            "Yaomin Huang",
            "Zhiyuan Xu",
            "Ning Liu",
            "Zhengping Che",
            "Xinyu Zhang",
            "Chaomin Shen",
            "Feifei Feng",
            "Chun Shan",
            "Jian Tang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.14158v1",
        "title": "BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown\n  Objects",
        "abstract": "  We present a near real-time method for 6-DoF tracking of an unknown object\nfrom a monocular RGBD video sequence, while simultaneously performing neural 3D\nreconstruction of the object. Our method works for arbitrary rigid objects,\neven when visual texture is largely absent. The object is assumed to be\nsegmented in the first frame only. No additional information is required, and\nno assumption is made about the interaction agent. Key to our method is a\nNeural Object Field that is learned concurrently with a pose graph optimization\nprocess in order to robustly accumulate information into a consistent 3D\nrepresentation capturing both geometry and appearance. A dynamic pool of posed\nmemory frames is automatically maintained to facilitate communication between\nthese threads. Our approach handles challenging sequences with large pose\nchanges, partial and full occlusion, untextured surfaces, and specular\nhighlights. We show results on HO3D, YCBInEOAT, and BEHAVE datasets,\ndemonstrating that our method significantly outperforms existing approaches.\nProject page: https://bundlesdf.github.io\n",
        "published": "2023",
        "authors": [
            "Bowen Wen",
            "Jonathan Tremblay",
            "Valts Blukis",
            "Stephen Tyree",
            "Thomas Muller",
            "Alex Evans",
            "Dieter Fox",
            "Jan Kautz",
            "Stan Birchfield"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.17386v1",
        "title": "Complementary Random Masking for RGB-Thermal Semantic Segmentation",
        "abstract": "  RGB-thermal semantic segmentation is one potential solution to achieve\nreliable semantic scene understanding in adverse weather and lighting\nconditions. However, the previous studies mostly focus on designing a\nmulti-modal fusion module without consideration of the nature of multi-modality\ninputs. Therefore, the networks easily become over-reliant on a single\nmodality, making it difficult to learn complementary and meaningful\nrepresentations for each modality. This paper proposes 1) a complementary\nrandom masking strategy of RGB-T images and 2) self-distillation loss between\nclean and masked input modalities. The proposed masking strategy prevents\nover-reliance on a single modality. It also improves the accuracy and\nrobustness of the neural network by forcing the network to segment and classify\nobjects even when one modality is partially available. Also, the proposed\nself-distillation loss encourages the network to extract complementary and\nmeaningful representations from a single modality or complementary masked\nmodalities. Based on the proposed method, we achieve state-of-the-art\nperformance over three RGB-T semantic segmentation benchmarks. Our source code\nis available at https://github.com/UkcheolShin/CRM_RGBTSeg.\n",
        "published": "2023",
        "authors": [
            "Ukcheol Shin",
            "Kyunghyun Lee",
            "In So Kweon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.17619v1",
        "title": "Gaze-based Attention Recognition for Human-Robot Collaboration",
        "abstract": "  Attention (and distraction) recognition is a key factor in improving\nhuman-robot collaboration. We present an assembly scenario where a human\noperator and a cobot collaborate equally to piece together a gearbox. The setup\nprovides multiple opportunities for the cobot to adapt its behavior depending\non the operator's attention, which can improve the collaboration experience and\nreduce psychological strain. As a first step, we recognize the areas in the\nworkspace that the human operator is paying attention to, and consequently,\ndetect when the operator is distracted. We propose a novel deep-learning\napproach to develop an attention recognition model. First, we train a\nconvolutional neural network to estimate the gaze direction using a publicly\navailable image dataset. Then, we use transfer learning with a small dataset to\nmap the gaze direction onto pre-defined areas of interest. Models trained using\nthis approach performed very well in leave-one-subject-out evaluation on the\nsmall dataset. We performed an additional validation of our models using the\nvideo snippets collected from participants working as an operator in the\npresented assembly scenario. Although the recall for the Distracted class was\nlower in this case, the models performed well in recognizing the areas the\noperator paid attention to. To the best of our knowledge, this is the first\nwork that validated an attention recognition model using data from a setting\nthat mimics industrial human-robot collaboration. Our findings highlight the\nneed for validation of attention recognition solutions in such full-fledged,\nnon-guided scenarios.\n",
        "published": "2023",
        "authors": [
            "Pooja Prajod",
            "Matteo Lavit Nicora",
            "Matteo Malosio",
            "Elisabeth Andr\u00e9"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2303.18101v3",
        "title": "INoD: Injected Noise Discriminator for Self-Supervised Representation\n  Learning in Agricultural Fields",
        "abstract": "  Perception datasets for agriculture are limited both in quantity and\ndiversity which hinders effective training of supervised learning approaches.\nSelf-supervised learning techniques alleviate this problem, however, existing\nmethods are not optimized for dense prediction tasks in agriculture domains\nwhich results in degraded performance. In this work, we address this limitation\nwith our proposed Injected Noise Discriminator (INoD) which exploits principles\nof feature replacement and dataset discrimination for self-supervised\nrepresentation learning. INoD interleaves feature maps from two disjoint\ndatasets during their convolutional encoding and predicts the dataset\naffiliation of the resultant feature map as a pretext task. Our approach\nenables the network to learn unequivocal representations of objects seen in one\ndataset while observing them in conjunction with similar features from the\ndisjoint dataset. This allows the network to reason about higher-level\nsemantics of the entailed objects, thus improving its performance on various\ndownstream tasks. Additionally, we introduce the novel Fraunhofer Potato 2022\ndataset consisting of over 16,800 images for object detection in potato fields.\nExtensive evaluations of our proposed INoD pretraining strategy for the tasks\nof object detection, semantic segmentation, and instance segmentation on the\nSugar Beets 2016 and our potato dataset demonstrate that it achieves\nstate-of-the-art performance.\n",
        "published": "2023",
        "authors": [
            "Julia Hindel",
            "Nikhil Gosala",
            "Kevin Bregler",
            "Abhinav Valada"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.00670v3",
        "title": "CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception",
        "abstract": "  Autonomous driving requires an accurate and fast 3D perception system that\nincludes 3D object detection, tracking, and segmentation. Although recent\nlow-cost camera-based approaches have shown promising results, they are\nsusceptible to poor illumination or bad weather conditions and have a large\nlocalization error. Hence, fusing camera with low-cost radar, which provides\nprecise long-range measurement and operates reliably in all environments, is\npromising but has not yet been thoroughly investigated. In this paper, we\npropose Camera Radar Net (CRN), a novel camera-radar fusion framework that\ngenerates a semantically rich and spatially accurate bird's-eye-view (BEV)\nfeature map for various tasks. To overcome the lack of spatial information in\nan image, we transform perspective view image features to BEV with the help of\nsparse but accurate radar points. We further aggregate image and radar feature\nmaps in BEV using multi-modal deformable attention designed to tackle the\nspatial misalignment between inputs. CRN with real-time setting operates at 20\nFPS while achieving comparable performance to LiDAR detectors on nuScenes, and\neven outperforms at a far distance on 100m setting. Moreover, CRN with offline\nsetting yields 62.4% NDS, 57.5% mAP on nuScenes test set and ranks first among\nall camera and camera-radar 3D object detectors.\n",
        "published": "2023",
        "authors": [
            "Youngseok Kim",
            "Juyeb Shin",
            "Sanmin Kim",
            "In-Jae Lee",
            "Jun Won Choi",
            "Dongsuk Kum"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.01201v1",
        "title": "Neural Volumetric Memory for Visual Locomotion Control",
        "abstract": "  Legged robots have the potential to expand the reach of autonomy beyond paved\nroads. In this work, we consider the difficult problem of locomotion on\nchallenging terrains using a single forward-facing depth camera. Due to the\npartial observability of the problem, the robot has to rely on past\nobservations to infer the terrain currently beneath it. To solve this problem,\nwe follow the paradigm in computer vision that explicitly models the 3D\ngeometry of the scene and propose Neural Volumetric Memory (NVM), a geometric\nmemory architecture that explicitly accounts for the SE(3) equivariance of the\n3D world. NVM aggregates feature volumes from multiple camera views by first\nbringing them back to the ego-centric frame of the robot. We test the learned\nvisual-locomotion policy on a physical robot and show that our approach, which\nexplicitly introduces geometric priors during training, offers superior\nperformance than more na\\\"ive methods. We also include ablation studies and\nshow that the representations stored in the neural volumetric memory capture\nsufficient geometric information to reconstruct the scene. Our project page\nwith videos is https://rchalyang.github.io/NVM .\n",
        "published": "2023",
        "authors": [
            "Ruihan Yang",
            "Ge Yang",
            "Xiaolong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.03623v1",
        "title": "RSPT: Reconstruct Surroundings and Predict Trajectories for\n  Generalizable Active Object Tracking",
        "abstract": "  Active Object Tracking (AOT) aims to maintain a specific relation between the\ntracker and object(s) by autonomously controlling the motion system of a\ntracker given observations. AOT has wide-ranging applications, such as in\nmobile robots and autonomous driving. However, building a generalizable active\ntracker that works robustly across different scenarios remains a challenge,\nespecially in unstructured environments with cluttered obstacles and diverse\nlayouts. We argue that constructing a state representation capable of modeling\nthe geometry structure of the surroundings and the dynamics of the target is\ncrucial for achieving this goal. To address this challenge, we present RSPT, a\nframework that forms a structure-aware motion representation by Reconstructing\nthe Surroundings and Predicting the target Trajectory. Additionally, we enhance\nthe generalization of the policy network by training in an asymmetric dueling\nmechanism. We evaluate RSPT on various simulated scenarios and show that it\noutperforms existing methods in unseen environments, particularly those with\ncomplex obstacles and layouts. We also demonstrate the successful transfer of\nRSPT to real-world settings. Project Website:\nhttps://sites.google.com/view/aot-rspt.\n",
        "published": "2023",
        "authors": [
            "Fangwei Zhong",
            "Xiao Bi",
            "Yudi Zhang",
            "Wei Zhang",
            "Yizhou Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.10410v2",
        "title": "Radar-Camera Fusion for Object Detection and Semantic Segmentation in\n  Autonomous Driving: A Comprehensive Review",
        "abstract": "  Driven by deep learning techniques, perception technology in autonomous\ndriving has developed rapidly in recent years, enabling vehicles to accurately\ndetect and interpret surrounding environment for safe and efficient navigation.\nTo achieve accurate and robust perception capabilities, autonomous vehicles are\noften equipped with multiple sensors, making sensor fusion a crucial part of\nthe perception system. Among these fused sensors, radars and cameras enable a\ncomplementary and cost-effective perception of the surrounding environment\nregardless of lighting and weather conditions. This review aims to provide a\ncomprehensive guideline for radar-camera fusion, particularly concentrating on\nperception tasks related to object detection and semantic segmentation.Based on\nthe principles of the radar and camera sensors, we delve into the data\nprocessing process and representations, followed by an in-depth analysis and\nsummary of radar-camera fusion datasets. In the review of methodologies in\nradar-camera fusion, we address interrogative questions, including \"why to\nfuse\", \"what to fuse\", \"where to fuse\", \"when to fuse\", and \"how to fuse\",\nsubsequently discussing various challenges and potential research directions\nwithin this domain. To ease the retrieval and comparison of datasets and fusion\nmethods, we also provide an interactive website:\nhttps://radar-camera-fusion.github.io.\n",
        "published": "2023",
        "authors": [
            "Shanliang Yao",
            "Runwei Guan",
            "Xiaoyu Huang",
            "Zhuoxiao Li",
            "Xiangyu Sha",
            "Yong Yue",
            "Eng Gee Lim",
            "Hyungjoon Seo",
            "Ka Lok Man",
            "Xiaohui Zhu",
            "Yutao Yue"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.11193v1",
        "title": "Combining Vision and Tactile Sensation for Video Prediction",
        "abstract": "  In this paper, we explore the impact of adding tactile sensation to video\nprediction models for physical robot interactions. Predicting the impact of\nrobotic actions on the environment is a fundamental challenge in robotics.\nCurrent methods leverage visual and robot action data to generate video\npredictions over a given time period, which can then be used to adjust robot\nactions. However, humans rely on both visual and tactile feedback to develop\nand maintain a mental model of their physical surroundings. In this paper, we\ninvestigate the impact of integrating tactile feedback into video prediction\nmodels for physical robot interactions. We propose three multi-modal\nintegration approaches and compare the performance of these tactile-enhanced\nvideo prediction models. Additionally, we introduce two new datasets of robot\npushing that use a magnetic-based tactile sensor for unsupervised learning. The\nfirst dataset contains visually identical objects with different physical\nproperties, while the second dataset mimics existing robot-pushing datasets of\nhousehold object clusters. Our results demonstrate that incorporating tactile\nfeedback into video prediction models improves scene prediction accuracy and\nenhances the agent's perception of physical interactions and understanding of\ncause-effect relationships during physical robot interactions.\n",
        "published": "2023",
        "authors": [
            "Willow Mandil",
            "Amir Ghalamzan-E"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.11821v1",
        "title": "Interruption-Aware Cooperative Perception for V2X Communication-Aided\n  Autonomous Driving",
        "abstract": "  Cooperative perception enabled by V2X Communication technologies can\nsignificantly improve the perception performance of autonomous vehicles beyond\nthe limited perception ability of the individual vehicles, therefore, improving\nthe safety and efficiency of autonomous driving in intelligent transportation\nsystems. However, in order to fully reap the benefits of cooperative perception\nin practice, the impacts of imperfect V2X communication, i.e., communication\nerrors and disruptions, need to be understood and effective remedies need to be\ndeveloped to alleviate their adverse impacts. Motivated by this need, we\npropose a novel INterruption-aware robust COoperative Perception (V2X-INCOP)\nsolution for V2X communication-aided autonomous driving, which leverages\nhistorical information to recover missing information due to interruption. To\nachieve comprehensive recovery, we design a communication adaptive multi-scale\nspatial-temporal prediction model to extract multi-scale spatial-temporal\nfeatures based on V2X communication conditions and capture the most significant\ninformation for the prediction of the missing information. To further improve\nrecovery performance, we adopt a knowledge distillation framework to give\ndirect supervision to the prediction model and a curriculum learning strategy\nto stabilize the training of the model. Our experiments on three public\ncooperative perception datasets demonstrate that our proposed method is\neffective in alleviating the impacts of communication interruption on\ncooperative perception.\n",
        "published": "2023",
        "authors": [
            "Shunli Ren",
            "Zixing Lei",
            "Zi Wang",
            "Mehrdad Dianati",
            "Yafei Wang",
            "Siheng Chen",
            "Wenjun Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.12289v1",
        "title": "Moving Forward by Moving Backward: Embedding Action Impact over Action\n  Semantics",
        "abstract": "  A common assumption when training embodied agents is that the impact of\ntaking an action is stable; for instance, executing the \"move ahead\" action\nwill always move the agent forward by a fixed distance, perhaps with some small\namount of actuator-induced noise. This assumption is limiting; an agent may\nencounter settings that dramatically alter the impact of actions: a move ahead\naction on a wet floor may send the agent twice as far as it expects and using\nthe same action with a broken wheel might transform the expected translation\ninto a rotation. Instead of relying that the impact of an action stably\nreflects its pre-defined semantic meaning, we propose to model the impact of\nactions on-the-fly using latent embeddings. By combining these latent action\nembeddings with a novel, transformer-based, policy head, we design an Action\nAdaptive Policy (AAP). We evaluate our AAP on two challenging visual navigation\ntasks in the AI2-THOR and Habitat environments and show that our AAP is highly\nperformant even when faced, at inference-time with missing actions and,\npreviously unseen, perturbed action space. Moreover, we observe significant\nimprovement in robustness against these actions when evaluating in real-world\nscenarios.\n",
        "published": "2023",
        "authors": [
            "Kuo-Hao Zeng",
            "Luca Weihs",
            "Roozbeh Mottaghi",
            "Ali Farhadi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.02085v1",
        "title": "A Systematic Study on Object Recognition Using Millimeter-wave Radar",
        "abstract": "  Due to its light and weather-independent sensing, millimeter-wave (MMW) radar\nis essential in smart environments. Intelligent vehicle systems and\nindustry-grade MMW radars have integrated such capabilities. Industry-grade MMW\nradars are expensive and hard to get for community-purpose smart environment\napplications. However, commercially available MMW radars have hidden\nunderpinning challenges that need to be investigated for tasks like recognizing\nobjects and activities, real-time person tracking, object localization, etc.\nImage and video data are straightforward to gather, understand, and annotate\nfor such jobs. Image and video data are light and weather-dependent,\nsusceptible to the occlusion effect, and present privacy problems. To eliminate\ndependence and ensure privacy, commercial MMW radars should be tested. MMW\nradar's practicality and performance in varied operating settings must be\naddressed before promoting it. To address the problems, we collected a dataset\nusing Texas Instruments' Automotive mmWave Radar (AWR2944) and reported the\nbest experimental settings for object recognition performance using different\ndeep learning algorithms. Our extensive data gathering technique allows us to\nsystematically explore and identify object identification task problems under\ncross-ambience conditions. We investigated several solutions and published\ndetailed experimental data.\n",
        "published": "2023",
        "authors": [
            "Maloy Kumar Devnath",
            "Avijoy Chakma",
            "Mohammad Saeid Anwar",
            "Emon Dey",
            "Zahid Hasan",
            "Marc Conn",
            "Biplab Pal",
            "Nirmalya Roy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.06121v2",
        "title": "Transformer-based model for monocular visual odometry: a video\n  understanding approach",
        "abstract": "  Estimating the camera's pose given images of a single camera is a traditional\ntask in mobile robots and autonomous vehicles. This problem is called monocular\nvisual odometry and it often relies on geometric approaches that require\nconsiderable engineering effort for a specific scenario. Deep learning methods\nhave shown to be generalizable after proper training and a large amount of\navailable data. Transformer-based architectures have dominated the\nstate-of-the-art in natural language processing and computer vision tasks, such\nas image and video understanding. In this work, we deal with the monocular\nvisual odometry as a video understanding task to estimate the 6-DoF camera's\npose. We contribute by presenting the TSformer-VO model based on\nspatio-temporal self-attention mechanisms to extract features from clips and\nestimate the motions in an end-to-end manner. Our approach achieved competitive\nstate-of-the-art performance compared with geometry-based and deep\nlearning-based methods on the KITTI visual odometry dataset, outperforming the\nDeepVO implementation highly accepted in the visual odometry community.\n",
        "published": "2023",
        "authors": [
            "Andr\u00e9 O. Fran\u00e7ani",
            "Marcos R. O. A. Maximo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.11566v2",
        "title": "StereoVAE: A lightweight stereo matching system through embedded GPUs",
        "abstract": "  We present a lightweight system for stereo matching through embedded GPUs. It\nbreaks the trade-off between accuracy and processing speed in stereo matching,\nenabling our embedded system to further improve the matching accuracy while\nensuring real-time processing. The main idea of our method is to construct a\ntiny neural network based on variational auto-encoder (VAE) to upsample and\nrefinement a small size of coarse disparity map, which is first generated by a\ntraditional matching method. The proposed hybrid structure cannot only bring\nthe advantage of traditional methods in terms of computational complexity, but\nalso ensure the matching accuracy under the impact of neural network. Extensive\nexperiments on the KITTI 2015 benchmark demonstrate that our tiny system\nexhibits high robustness in improving the accuracy of the coarse disparity maps\ngenerated by different algorithms, while also running in real-time on embedded\nGPUs.\n",
        "published": "2023",
        "authors": [
            "Qiong Chang",
            "Xiang Li",
            "Xin Xu",
            "Xin Liu",
            "Yun Li",
            "Miyazaki Jun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.11772v2",
        "title": "Neural Foundations of Mental Simulation: Future Prediction of Latent\n  Representations on Dynamic Scenes",
        "abstract": "  Humans and animals have a rich and flexible understanding of the physical\nworld, which enables them to infer the underlying dynamical trajectories of\nobjects and events, plausible future states, and use that to plan and\nanticipate the consequences of actions. However, the neural mechanisms\nunderlying these computations are unclear. We combine a goal-driven modeling\napproach with dense neurophysiological data and high-throughput human\nbehavioral readouts to directly impinge on this question. Specifically, we\nconstruct and evaluate several classes of sensory-cognitive networks to predict\nthe future state of rich, ethologically-relevant environments, ranging from\nself-supervised end-to-end models with pixel-wise or object-centric objectives,\nto models that future predict in the latent space of purely static image-based\nor dynamic video-based pretrained foundation models. We find strong\ndifferentiation across these model classes in their ability to predict neural\nand behavioral data both within and across diverse environments. In particular,\nwe find that neural responses are currently best predicted by models trained to\npredict the future state of their environment in the latent space of pretrained\nfoundation models optimized for dynamic scenes in a self-supervised manner.\nNotably, models that future predict in the latent space of video foundation\nmodels that are optimized to support a diverse range of sensorimotor tasks,\nreasonably match both human behavioral error patterns and neural dynamics\nacross all environmental scenarios that we were able to test. Overall, these\nfindings suggest that the neural mechanisms and behaviors of primate mental\nsimulation are thus far most consistent with being optimized to future predict\non dynamic, reusable visual representations that are useful for Embodied AI\nmore generally.\n",
        "published": "2023",
        "authors": [
            "Aran Nayebi",
            "Rishi Rajalingham",
            "Mehrdad Jazayeri",
            "Guangyu Robert Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.12409v2",
        "title": "Deep Radar Inverse Sensor Models for Dynamic Occupancy Grid Maps",
        "abstract": "  To implement autonomous driving, one essential step is to model the vehicle\nenvironment based on the sensor inputs. Radars, with their well-known\nadvantages, became a popular option to infer the occupancy state of grid cells\nsurrounding the vehicle. To tackle data sparsity and noise of radar detections,\nwe propose a deep learning-based Inverse Sensor Model (ISM) to learn the\nmapping from sparse radar detections to polar measurement grids. Improved\nlidar-based measurement grids are used as reference. The learned radar\nmeasurement grids, combined with radar Doppler velocity measurements, are\nfurther used to generate a Dynamic Grid Map (DGM). Experiments in real-world\nhighway scenarios show that our approach outperforms the hand-crafted geometric\nISMs. In comparison to state-of-the-art deep learning methods, our approach is\nthe first one to learn a single-frame measurement grid in the polar scheme from\nradars with a limited Field Of View (FOV). The learning framework makes the\nlearned ISM independent of the radar mounting. This enables us to flexibly use\none or more radar sensors without network retraining and without requirements\non 360{\\deg} sensor coverage.\n",
        "published": "2023",
        "authors": [
            "Zihang Wei",
            "Rujiao Yan",
            "Matthias Schreier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.13019v1",
        "title": "Robots in the Garden: Artificial Intelligence and Adaptive Landscapes",
        "abstract": "  This paper introduces ELUA, the Ecological Laboratory for Urban Agriculture,\na collaboration among landscape architects, architects and computer scientists\nwho specialize in artificial intelligence, robotics and computer vision. ELUA\nhas two gantry robots, one indoors and the other outside on the rooftop of a\n6-story campus building. Each robot can seed, water, weed, and prune in its\ngarden. To support responsive landscape research, ELUA also includes sensor\narrays, an AI-powered camera, and an extensive network infrastructure. This\nproject demonstrates a way to integrate artificial intelligence into an\nevolving urban ecosystem, and encourages landscape architects to develop an\nadaptive design framework where design becomes a long-term engagement with the\nenvironment.\n",
        "published": "2023",
        "authors": [
            "Zihao Zhang",
            "Susan L. Epstein",
            "Casey Breen",
            "Sophia Xia",
            "Zhigang Zhu",
            "Christian Volkmann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.08814v1",
        "title": "A Self-Supervised Miniature One-Shot Texture Segmentation (MOSTS) Model\n  for Real-Time Robot Navigation and Embedded Applications",
        "abstract": "  Determining the drivable area, or free space segmentation, is critical for\nmobile robots to navigate indoor environments safely. However, the lack of\ncoherent markings and structures (e.g., lanes, curbs, etc.) in indoor spaces\nplaces the burden of traversability estimation heavily on the mobile robot.\nThis paper explores the use of a self-supervised one-shot texture segmentation\nframework and an RGB-D camera to achieve robust drivable area segmentation.\nWith a fast inference speed and compact size, the developed model, MOSTS is\nideal for real-time robot navigation and various embedded applications. A\nbenchmark study was conducted to compare MOSTS's performance with existing\none-shot texture segmentation models to evaluate its performance. Additionally,\na validation dataset was built to assess MOSTS's ability to perform texture\nsegmentation in the wild, where it effectively identified small low-lying\nobjects that were previously undetectable by depth measurements. Further, the\nstudy also compared MOSTS's performance with two State-Of-The-Art (SOTA) indoor\nsemantic segmentation models, both quantitatively and qualitatively. The\nresults showed that MOSTS offers comparable accuracy with up to eight times\nfaster inference speed in indoor drivable area segmentation.\n",
        "published": "2023",
        "authors": [
            "Yu Chen",
            "Chirag Rastogi",
            "Zheyu Zhou",
            "William R. Norris"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.10014v1",
        "title": "Coaching a Teachable Student",
        "abstract": "  We propose a novel knowledge distillation framework for effectively teaching\na sensorimotor student agent to drive from the supervision of a privileged\nteacher agent. Current distillation for sensorimotor agents methods tend to\nresult in suboptimal learned driving behavior by the student, which we\nhypothesize is due to inherent differences between the input, modeling\ncapacity, and optimization processes of the two agents. We develop a novel\ndistillation scheme that can address these limitations and close the gap\nbetween the sensorimotor agent and its privileged teacher. Our key insight is\nto design a student which learns to align their input features with the\nteacher's privileged Bird's Eye View (BEV) space. The student then can benefit\nfrom direct supervision by the teacher over the internal representation\nlearning. To scaffold the difficult sensorimotor learning task, the student\nmodel is optimized via a student-paced coaching mechanism with various\nauxiliary supervision. We further propose a high-capacity imitation learned\nprivileged agent that surpasses prior privileged agents in CARLA and ensures\nthe student learns safe driving behavior. Our proposed sensorimotor agent\nresults in a robust image-based behavior cloning agent in CARLA, improving over\ncurrent models by over 20.6% in driving score without requiring LiDAR,\nhistorical observations, ensemble of models, on-policy data aggregation or\nreinforcement learning.\n",
        "published": "2023",
        "authors": [
            "Jimuyang Zhang",
            "Zanming Huang",
            "Eshed Ohn-Bar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.10142v1",
        "title": "Enhancing Visual Domain Adaptation with Source Preparation",
        "abstract": "  Robotic Perception in diverse domains such as low-light scenarios, where new\nmodalities like thermal imaging and specialized night-vision sensors are\nincreasingly employed, remains a challenge. Largely, this is due to the limited\navailability of labeled data. Existing Domain Adaptation (DA) techniques, while\npromising to leverage labels from existing well-lit RGB images, fail to\nconsider the characteristics of the source domain itself. We holistically\naccount for this factor by proposing Source Preparation (SP), a method to\nmitigate source domain biases. Our Almost Unsupervised Domain Adaptation (AUDA)\nframework, a label-efficient semi-supervised approach for robotic scenarios --\nemploys Source Preparation (SP), Unsupervised Domain Adaptation (UDA) and\nSupervised Alignment (SA) from limited labeled data. We introduce\nCityIntensified, a novel dataset comprising temporally aligned image pairs\ncaptured from a high-sensitivity camera and an intensifier camera for semantic\nsegmentation and object detection in low-light settings. We demonstrate the\neffectiveness of our method in semantic segmentation, with experiments showing\nthat SP enhances UDA across a range of visual domains, with improvements up to\n40.64% in mIoU over baseline, while making target models more robust to\nreal-world shifts within the target domain. We show that AUDA is a\nlabel-efficient framework for effective DA, significantly improving target\ndomain performance with only tens of labeled samples from the target domain.\n",
        "published": "2023",
        "authors": [
            "Anirudha Ramesh",
            "Anurag Ghosh",
            "Christoph Mertz",
            "Jeff Schneider"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.11565v2",
        "title": "HomeRobot: Open-Vocabulary Mobile Manipulation",
        "abstract": "  HomeRobot (noun): An affordable compliant robot that navigates homes and\nmanipulates a wide range of objects in order to complete everyday tasks.\nOpen-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object\nin any unseen environment, and placing it in a commanded location. This is a\nfoundational challenge for robots to be useful assistants in human\nenvironments, because it involves tackling sub-problems from across robotics:\nperception, language understanding, navigation, and manipulation are all\nessential to OVMM. In addition, integration of the solutions to these\nsub-problems poses its own substantial challenges. To drive research in this\narea, we introduce the HomeRobot OVMM benchmark, where an agent navigates\nhousehold environments to grasp novel objects and place them on target\nreceptacles. HomeRobot has two components: a simulation component, which uses a\nlarge and diverse curated object set in new, high-quality multi-room home\nenvironments; and a real-world component, providing a software stack for the\nlow-cost Hello Robot Stretch to encourage replication of real-world experiments\nacross labs. We implement both reinforcement learning and heuristic\n(model-based) baselines and show evidence of sim-to-real transfer. Our\nbaselines achieve a 20% success rate in the real world; our experiments\nidentify ways future research work improve performance. See videos on our\nwebsite: https://ovmm.github.io/.\n",
        "published": "2023",
        "authors": [
            "Sriram Yenamandra",
            "Arun Ramachandran",
            "Karmesh Yadav",
            "Austin Wang",
            "Mukul Khanna",
            "Theophile Gervet",
            "Tsung-Yen Yang",
            "Vidhi Jain",
            "Alexander William Clegg",
            "John Turner",
            "Zsolt Kira",
            "Manolis Savva",
            "Angel Chang",
            "Devendra Singh Chaplot",
            "Dhruv Batra",
            "Roozbeh Mottaghi",
            "Yonatan Bisk",
            "Chris Paxton"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.16928v1",
        "title": "One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape\n  Optimization",
        "abstract": "  Single image 3D reconstruction is an important but challenging task that\nrequires extensive knowledge of our natural world. Many existing methods solve\nthis problem by optimizing a neural radiance field under the guidance of 2D\ndiffusion models but suffer from lengthy optimization time, 3D inconsistency\nresults, and poor geometry. In this work, we propose a novel method that takes\na single image of any object as input and generates a full 360-degree 3D\ntextured mesh in a single feed-forward pass. Given a single image, we first use\na view-conditioned 2D diffusion model, Zero123, to generate multi-view images\nfor the input view, and then aim to lift them up to 3D space. Since traditional\nreconstruction methods struggle with inconsistent multi-view predictions, we\nbuild our 3D reconstruction module upon an SDF-based generalizable neural\nsurface reconstruction method and propose several critical training strategies\nto enable the reconstruction of 360-degree meshes. Without costly\noptimizations, our method reconstructs 3D shapes in significantly less time\nthan existing methods. Moreover, our method favors better geometry, generates\nmore 3D consistent results, and adheres more closely to the input image. We\nevaluate our approach on both synthetic data and in-the-wild images and\ndemonstrate its superiority in terms of both mesh quality and runtime. In\naddition, our approach can seamlessly support the text-to-3D task by\nintegrating with off-the-shelf text-to-image diffusion models.\n",
        "published": "2023",
        "authors": [
            "Minghua Liu",
            "Chao Xu",
            "Haian Jin",
            "Linghao Chen",
            "Mukund Varma T",
            "Zexiang Xu",
            "Hao Su"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.17602v2",
        "title": "S.T.A.R.-Track: Latent Motion Models for End-to-End 3D Object Tracking\n  with Adaptive Spatio-Temporal Appearance Representations",
        "abstract": "  Following the tracking-by-attention paradigm, this paper introduces an\nobject-centric, transformer-based framework for tracking in 3D. Traditional\nmodel-based tracking approaches incorporate the geometric effect of object- and\nego motion between frames with a geometric motion model. Inspired by this, we\npropose S.T.A.R.-Track, which uses a novel latent motion model (LMM) to\nadditionally adjust object queries to account for changes in viewing direction\nand lighting conditions directly in the latent space, while still modeling the\ngeometric motion explicitly. Combined with a novel learnable track embedding\nthat aids in modeling the existence probability of tracks, this results in a\ngeneric tracking framework that can be integrated with any query-based\ndetector. Extensive experiments on the nuScenes benchmark demonstrate the\nbenefits of our approach, showing \\ac{sota} performance for DETR3D-based\ntrackers while drastically reducing the number of identity switches of tracks\nat the same time.\n",
        "published": "2023",
        "authors": [
            "Simon Doll",
            "Niklas Hanselmann",
            "Lukas Schneider",
            "Richard Schulz",
            "Markus Enzweiler",
            "Hendrik P. A. Lensch"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.01848v1",
        "title": "Embodied Task Planning with Large Language Models",
        "abstract": "  Equipping embodied agents with commonsense is important for robots to\nsuccessfully complete complex human instructions in general environments.\nRecent large language models (LLM) can embed rich semantic knowledge for agents\nin plan generation of complex tasks, while they lack the information about the\nrealistic world and usually yield infeasible action sequences. In this paper,\nwe propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning\nwith physical scene constraint, where the agent generates executable plans\naccording to the existed objects in the scene by aligning LLMs with the visual\nperception models. Specifically, we first construct a multimodal dataset\ncontaining triplets of indoor scenes, instructions and action plans, where we\nprovide the designed prompts and the list of existing objects in the scene for\nGPT-3.5 to generate a large number of instructions and corresponding planned\nactions. The generated data is leveraged for grounded plan tuning of\npre-trained LLMs. During inference, we discover the objects in the scene by\nextending open-vocabulary object detectors to multi-view RGB images collected\nin different achievable locations. Experimental results show that the generated\nplan from our TaPA framework can achieve higher success rate than LLaVA and\nGPT-3.5 by a sizable margin, which indicates the practicality of embodied task\nplanning in general and complex environments.\n",
        "published": "2023",
        "authors": [
            "Zhenyu Wu",
            "Ziwei Wang",
            "Xiuwei Xu",
            "Jiwen Lu",
            "Haibin Yan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.05913v1",
        "title": "Close-up View synthesis by Interpolating Optical Flow",
        "abstract": "  The virtual viewpoint is perceived as a new technique in virtual navigation,\nas yet not supported due to the lack of depth information and obscure camera\nparameters. In this paper, a method for achieving close-up virtual view is\nproposed and it only uses optical flow to build parallax effects to realize\npseudo 3D projection without using depth sensor. We develop a bidirectional\noptical flow method to obtain any virtual viewpoint by proportional\ninterpolation of optical flow. Moreover, with the ingenious application of the\noptical-flow-value, we achieve clear and visual-fidelity magnified results\nthrough lens stretching in any corner, which overcomes the visual distortion\nand image blur through viewpoint magnification and transition in Google Street\nView system.\n",
        "published": "2023",
        "authors": [
            "Xinyi Bai",
            "Ze Wang",
            "Lu Yang",
            "Hong Cheng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.06647v2",
        "title": "DeepIPCv2: LiDAR-powered Robust Environmental Perception and\n  Navigational Control for Autonomous Vehicle",
        "abstract": "  We present DeepIPCv2, an autonomous driving model that perceives the\nenvironment using a LiDAR sensor for more robust drivability, especially when\ndriving under poor illumination conditions where everything is not clearly\nvisible. DeepIPCv2 takes a set of LiDAR point clouds as the main perception\ninput. Since point clouds are not affected by illumination changes, they can\nprovide a clear observation of the surroundings no matter what the condition\nis. This results in a better scene understanding and stable features provided\nby the perception module to support the controller module in estimating\nnavigational control properly. To evaluate its performance, we conduct several\ntests by deploying the model to predict a set of driving records and perform\nreal automated driving under three different conditions. We also conduct\nablation and comparative studies with some recent models to justify its\nperformance. Based on the experimental results, DeepIPCv2 shows a robust\nperformance by achieving the best drivability in all driving scenarios.\nFurthermore, we will upload the codes to\nhttps://github.com/oskarnatan/DeepIPCv2.\n",
        "published": "2023",
        "authors": [
            "Oskar Natan",
            "Jun Miura"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.07469v1",
        "title": "Interactive Spatiotemporal Token Attention Network for Skeleton-based\n  General Interactive Action Recognition",
        "abstract": "  Recognizing interactive action plays an important role in human-robot\ninteraction and collaboration. Previous methods use late fusion and\nco-attention mechanism to capture interactive relations, which have limited\nlearning capability or inefficiency to adapt to more interacting entities. With\nassumption that priors of each entity are already known, they also lack\nevaluations on a more general setting addressing the diversity of subjects. To\naddress these problems, we propose an Interactive Spatiotemporal Token\nAttention Network (ISTA-Net), which simultaneously model spatial, temporal, and\ninteractive relations. Specifically, our network contains a tokenizer to\npartition Interactive Spatiotemporal Tokens (ISTs), which is a unified way to\nrepresent motions of multiple diverse entities. By extending the entity\ndimension, ISTs provide better interactive representations. To jointly learn\nalong three dimensions in ISTs, multi-head self-attention blocks integrated\nwith 3D convolutions are designed to capture inter-token correlations. When\nmodeling correlations, a strict entity ordering is usually irrelevant for\nrecognizing interactive actions. To this end, Entity Rearrangement is proposed\nto eliminate the orderliness in ISTs for interchangeable entities. Extensive\nexperiments on four datasets verify the effectiveness of ISTA-Net by\noutperforming state-of-the-art methods. Our code is publicly available at\nhttps://github.com/Necolizer/ISTA-Net\n",
        "published": "2023",
        "authors": [
            "Yuhang Wen",
            "Zixuan Tang",
            "Yunsheng Pang",
            "Beichen Ding",
            "Mengyuan Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.09827v1",
        "title": "Online Continual Learning for Robust Indoor Object Recognition",
        "abstract": "  Vision systems mounted on home robots need to interact with unseen classes in\nchanging environments. Robots have limited computational resources, labelled\ndata and storage capability. These requirements pose some unique challenges:\nmodels should adapt without forgetting past knowledge in a data- and\nparameter-efficient way. We characterize the problem as few-shot (FS) online\ncontinual learning (OCL), where robotic agents learn from a non-repeated stream\nof few-shot data updating only a few model parameters. Additionally, such\nmodels experience variable conditions at test time, where objects may appear in\ndifferent poses (e.g., horizontal or vertical) and environments (e.g., day or\nnight). To improve robustness of CL agents, we propose RobOCLe, which; 1)\nconstructs an enriched feature space computing high order statistical moments\nfrom the embedded features of samples; and 2) computes similarity between high\norder statistics of the samples on the enriched feature space, and predicts\ntheir class labels. We evaluate robustness of CL models to train/test\naugmentations in various cases. We show that different moments allow RobOCLe to\ncapture different properties of deformations, providing higher robustness with\nno decrease of inference speed.\n",
        "published": "2023",
        "authors": [
            "Umberto Michieli",
            "Mete Ozay"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.10713v1",
        "title": "Kick Back & Relax: Learning to Reconstruct the World by Watching SlowTV",
        "abstract": "  Self-supervised monocular depth estimation (SS-MDE) has the potential to\nscale to vast quantities of data. Unfortunately, existing approaches limit\nthemselves to the automotive domain, resulting in models incapable of\ngeneralizing to complex environments such as natural or indoor settings.\n  To address this, we propose a large-scale SlowTV dataset curated from\nYouTube, containing an order of magnitude more data than existing automotive\ndatasets. SlowTV contains 1.7M images from a rich diversity of environments,\nsuch as worldwide seasonal hiking, scenic driving and scuba diving. Using this\ndataset, we train an SS-MDE model that provides zero-shot generalization to a\nlarge collection of indoor/outdoor datasets. The resulting model outperforms\nall existing SSL approaches and closes the gap on supervised SoTA, despite\nusing a more efficient architecture.\n  We additionally introduce a collection of best-practices to further maximize\nperformance and zero-shot generalization. This includes 1) aspect ratio\naugmentation, 2) camera intrinsic estimation, 3) support frame randomization\nand 4) flexible motion estimation. Code is available at\nhttps://github.com/jspenmar/slowtv_monodepth.\n",
        "published": "2023",
        "authors": [
            "Jaime Spencer",
            "Chris Russell",
            "Simon Hadfield",
            "Richard Bowden"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.12837v1",
        "title": "EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge: Mixed\n  Sequences Prediction",
        "abstract": "  This report presents the technical details of our approach for the\nEPIC-Kitchens-100 Unsupervised Domain Adaptation (UDA) Challenge in Action\nRecognition. Our approach is based on the idea that the order in which actions\nare performed is similar between the source and target domains. Based on this,\nwe generate a modified sequence by randomly combining actions from the source\nand target domains. As only unlabelled target data are available under the UDA\nsetting, we use a standard pseudo-labeling strategy for extracting action\nlabels for the target. We then ask the network to predict the resulting action\nsequence. This allows to integrate information from both domains during\ntraining and to achieve better transfer results on target. Additionally, to\nbetter incorporate sequence information, we use a language model to filter\nunlikely sequences. Lastly, we employed a co-occurrence matrix to eliminate\nunseen combinations of verbs and nouns. Our submission, labeled as 'sshayan',\ncan be found on the leaderboard, where it currently holds the 2nd position for\n'verb' and the 4th position for both 'noun' and 'action'.\n",
        "published": "2023",
        "authors": [
            "Amirshayan Nasirimajd",
            "Simone Alberto Peirone",
            "Chiara Plizzari",
            "Barbara Caputo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.15989v2",
        "title": "Freespace Optical Flow Modeling for Automated Driving",
        "abstract": "  Optical flow and disparity are two informative visual features for autonomous\ndriving perception. They have been used for a variety of applications, such as\nobstacle and lane detection. The concept of \"U-V-Disparity\" has been widely\nexplored in the literature, while its counterpart in optical flow has received\nrelatively little attention. Traditional motion analysis algorithms estimate\noptical flow by matching correspondences between two successive video frames,\nwhich limits the full utilization of environmental information and geometric\nconstraints. Therefore, we propose a novel strategy to model optical flow in\nthe collision-free space (also referred to as drivable area or simply\nfreespace) for intelligent vehicles, with the full utilization of geometry\ninformation in a 3D driving environment. We provide explicit representations of\noptical flow and deduce the quadratic relationship between the optical flow\ncomponent and the vertical coordinate. Through extensive experiments on several\npublic datasets, we demonstrate the high accuracy and robustness of our model.\nAdditionally, our proposed freespace optical flow model boasts a diverse array\nof applications within the realm of automated driving, providing a geometric\nconstraint in freespace detection, vehicle localization, and more. We have made\nour source code publicly available at https://mias.group/FSOF.\n",
        "published": "2023",
        "authors": [
            "Yi Feng",
            "Ruge Zhang",
            "Jiayuan Du",
            "Qijun Chen",
            "Rui Fan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.00801v1",
        "title": "Artificial Eye for the Blind",
        "abstract": "  The main backbone of our Artificial Eye model is the Raspberry pi3 which is\nconnected to the webcam ,ultrasonic proximity sensor, speaker and we also run\nall our software models i.e object detection, Optical Character recognition,\ngoogle text to speech conversion and the Mycroft voice assistance model. At\nfirst the ultrasonic proximity sensor will be measuring the distance between\nitself and any obstacle in front of it .When the Proximity sensor detects any\nobstacle in front within its specified range, the blind person will hear an\naudio prompt about an obstacle in his way at a certain distance. At this time\nthe Webcam will capture an image in front of it and the Object detection model\nand the Optical Character Recognition model will begin to run on the Raspberry\npi. The imat of the blind person. The text and the object detected are conveyed\nto the blind pege captured is first sent through the Tesseract OCR module to\ndetect any texts in the image and then through the Object detection model to\ndetect the objects in fronrson by converting the texts to speech by using the\ngTTS module. Along with the above mentioned process going on there will be an\nactive MYCROFT voice assistant model which can be used to interact with the\nblind person. The blind person can ask about the weather , daily news , any\ninformation on the internet ,etc\n",
        "published": "2023",
        "authors": [
            "Abhinav Benagi",
            "Dhanyatha Narayan",
            "Charith Rage",
            "A Sushmitha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.01006v4",
        "title": "FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of\n  Autonomous Driving",
        "abstract": "  Building a multi-modality multi-task neural network toward accurate and\nrobust performance is a de-facto standard in perception task of autonomous\ndriving. However, leveraging such data from multiple sensors to jointly\noptimize the prediction and planning tasks remains largely unexplored. In this\npaper, we present FusionAD, to the best of our knowledge, the first unified\nframework that fuse the information from two most critical sensors, camera and\nLiDAR, goes beyond perception task. Concretely, we first build a transformer\nbased multi-modality fusion network to effectively produce fusion based\nfeatures. In constrast to camera-based end-to-end method UniAD, we then\nestablish a fusion aided modality-aware prediction and status-aware planning\nmodules, dubbed FMSPnP that take advantages of multi-modality features. We\nconduct extensive experiments on commonly used benchmark nuScenes dataset, our\nFusionAD achieves state-of-the-art performance and surpassing baselines on\naverage 15% on perception tasks like detection and tracking, 10% on occupancy\nprediction accuracy, reducing prediction error from 0.708 to 0.389 in ADE score\nand reduces the collision rate from 0.31% to only 0.12%.\n",
        "published": "2023",
        "authors": [
            "Tengju Ye",
            "Wei Jing",
            "Chunyong Hu",
            "Shikun Huang",
            "Lingping Gao",
            "Fangzhen Li",
            "Jingke Wang",
            "Ke Guo",
            "Wencong Xiao",
            "Weibo Mao",
            "Hang Zheng",
            "Kun Li",
            "Junbo Chen",
            "Kaicheng Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.02239v1",
        "title": "DTF-Net: Category-Level Pose Estimation and Shape Reconstruction via\n  Deformable Template Field",
        "abstract": "  Estimating 6D poses and reconstructing 3D shapes of objects in open-world\nscenes from RGB-depth image pairs is challenging. Many existing methods rely on\nlearning geometric features that correspond to specific templates while\ndisregarding shape variations and pose differences among objects in the same\ncategory. As a result, these methods underperform when handling unseen object\ninstances in complex environments. In contrast, other approaches aim to achieve\ncategory-level estimation and reconstruction by leveraging normalized geometric\nstructure priors, but the static prior-based reconstruction struggles with\nsubstantial intra-class variations. To solve these problems, we propose the\nDTF-Net, a novel framework for pose estimation and shape reconstruction based\non implicit neural fields of object categories. In DTF-Net, we design a\ndeformable template field to represent the general category-wise shape latent\nfeatures and intra-category geometric deformation features. The field\nestablishes continuous shape correspondences, deforming the category template\ninto arbitrary observed instances to accomplish shape reconstruction. We\nintroduce a pose regression module that shares the deformation features and\ntemplate codes from the fields to estimate the accurate 6D pose of each object\nin the scene. We integrate a multi-modal representation extraction module to\nextract object features and semantic masks, enabling end-to-end inference.\nMoreover, during training, we implement a shape-invariant training strategy and\na viewpoint sampling method to further enhance the model's capability to\nextract object pose features. Extensive experiments on the REAL275 and CAMERA25\ndatasets demonstrate the superiority of DTF-Net in both synthetic and real\nscenes. Furthermore, we show that DTF-Net effectively supports grasping tasks\nwith a real robot arm.\n",
        "published": "2023",
        "authors": [
            "Haowen Wang",
            "Zhipeng Fan",
            "Zhen Zhao",
            "Zhengping Che",
            "Zhiyuan Xu",
            "Dong Liu",
            "Feifei Feng",
            "Yakun Huang",
            "Xiuquan Qiao",
            "Jian Tang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.03718v2",
        "title": "SEM-GAT: Explainable Semantic Pose Estimation using Learned Graph\n  Attention",
        "abstract": "  This paper proposes a Graph Neural Network(GNN)-based method for exploiting\nsemantics and local geometry to guide the identification of reliable pointcloud\nregistration candidates. Semantic and morphological features of the environment\nserve as key reference points for registration, enabling accurate lidar-based\npose estimation. Our novel lightweight static graph structure informs our\nattention-based node aggregation network by identifying semantic-instance\nrelationships, acting as an inductive bias to significantly reduce the\ncomputational burden of pointcloud registration. By connecting candidate nodes\nand exploiting cross-graph attention, we identify confidence scores for all\npotential registration correspondences and estimate the displacement between\npointcloud scans. Our pipeline enables introspective analysis of the model's\nperformance by correlating it with the individual contributions of local\nstructures in the environment, providing valuable insights into the system's\nbehaviour. We test our method on the KITTI odometry dataset, achieving\ncompetitive accuracy compared to benchmark methods and a higher track\nsmoothness while relying on significantly fewer network parameters.\n",
        "published": "2023",
        "authors": [
            "Efimia Panagiotaki",
            "Daniele De Martini",
            "Georgi Pramatarov",
            "Matthew Gadd",
            "Lars Kunze"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.06735v1",
        "title": "AerialVLN: Vision-and-Language Navigation for UAVs",
        "abstract": "  Recently emerged Vision-and-Language Navigation (VLN) tasks have drawn\nsignificant attention in both computer vision and natural language processing\ncommunities. Existing VLN tasks are built for agents that navigate on the\nground, either indoors or outdoors. However, many tasks require intelligent\nagents to carry out in the sky, such as UAV-based goods delivery,\ntraffic/security patrol, and scenery tour, to name a few. Navigating in the sky\nis more complicated than on the ground because agents need to consider the\nflying height and more complex spatial relationship reasoning. To fill this gap\nand facilitate research in this field, we propose a new task named AerialVLN,\nwhich is UAV-based and towards outdoor environments. We develop a 3D simulator\nrendered by near-realistic pictures of 25 city-level scenarios. Our simulator\nsupports continuous navigation, environment extension and configuration. We\nalso proposed an extended baseline model based on the widely-used\ncross-modal-alignment (CMA) navigation methods. We find that there is still a\nsignificant gap between the baseline model and human performance, which\nsuggests AerialVLN is a new challenging task. Dataset and code is available at\nhttps://github.com/AirVLN/AirVLN.\n",
        "published": "2023",
        "authors": [
            "Shubo Liu",
            "Hongsheng Zhang",
            "Yuankai Qi",
            "Peng Wang",
            "Yaning Zhang",
            "Qi Wu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.07948v2",
        "title": "Leveraging Symmetries in Pick and Place",
        "abstract": "  Robotic pick and place tasks are symmetric under translations and rotations\nof both the object to be picked and the desired place pose. For example, if the\npick object is rotated or translated, then the optimal pick action should also\nrotate or translate. The same is true for the place pose; if the desired place\npose changes, then the place action should also transform accordingly. A\nrecently proposed pick and place framework known as Transporter Net captures\nsome of these symmetries, but not all. This paper analytically studies the\nsymmetries present in planar robotic pick and place and proposes a method of\nincorporating equivariant neural models into Transporter Net in a way that\ncaptures all symmetries. The new model, which we call Equivariant Transporter\nNet, is equivariant to both pick and place symmetries and can immediately\ngeneralize pick and place knowledge to different pick and place poses. We\nevaluate the new model empirically and show that it is much more sample\nefficient than the non-symmetric version, resulting in a system that can\nimitate demonstrated pick and place behavior using very few human\ndemonstrations on a variety of imitation learning tasks.\n",
        "published": "2023",
        "authors": [
            "Haojie Huang",
            "Dian Wang",
            "Arsh Tangri",
            "Robin Walters",
            "Robert Platt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.08746v2",
        "title": "SurgicalSAM: Efficient Class Promptable Surgical Instrument Segmentation",
        "abstract": "  The Segment Anything Model (SAM) is a powerful foundation model that has\nrevolutionised image segmentation. To apply SAM to surgical instrument\nsegmentation, a common approach is to locate precise points or boxes of\ninstruments and then use them as prompts for SAM in a zero-shot manner.\nHowever, we observe two problems with this naive pipeline: (1) the domain gap\nbetween natural objects and surgical instruments leads to inferior\ngeneralisation of SAM; and (2) SAM relies on precise point or box locations for\naccurate segmentation, requiring either extensive manual guidance or a\nwell-performing specialist detector for prompt preparation, which leads to a\ncomplex multi-stage pipeline. To address these problems, we introduce\nSurgicalSAM, a novel end-to-end efficient-tuning approach for SAM to\neffectively integrate surgical-specific information with SAM's pre-trained\nknowledge for improved generalisation. Specifically, we propose a lightweight\nprototype-based class prompt encoder for tuning, which directly generates\nprompt embeddings from class prototypes and eliminates the use of explicit\nprompts for improved robustness and a simpler pipeline. In addition, to address\nthe low inter-class variance among surgical instrument categories, we propose\ncontrastive prototype learning, further enhancing the discrimination of the\nclass prototypes for more accurate class prompting. The results of extensive\nexperiments on both EndoVis2018 and EndoVis2017 datasets demonstrate that\nSurgicalSAM achieves state-of-the-art performance while only requiring a small\nnumber of tunable parameters. The source code is available at\nhttps://github.com/wenxi-yue/SurgicalSAM.\n",
        "published": "2023",
        "authors": [
            "Wenxi Yue",
            "Jing Zhang",
            "Kun Hu",
            "Yong Xia",
            "Jiebo Luo",
            "Zhiyong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.09678v2",
        "title": "PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Robust 3D\n  Human Pose Estimation",
        "abstract": "  Existing 3D human pose estimators face challenges in adapting to new datasets\ndue to the lack of 2D-3D pose pairs in training sets. To overcome this issue,\nwe propose \\textit{Multi-Hypothesis \\textbf{P}ose \\textbf{Syn}thesis\n\\textbf{D}omain \\textbf{A}daptation} (\\textbf{PoSynDA}) framework to bridge\nthis data disparity gap in target domain. Typically, PoSynDA uses a\ndiffusion-inspired structure to simulate 3D pose distribution in the target\ndomain. By incorporating a multi-hypothesis network, PoSynDA generates diverse\npose hypotheses and aligns them with the target domain. To do this, it first\nutilizes target-specific source augmentation to obtain the target domain\ndistribution data from the source domain by decoupling the scale and position\nparameters. The process is then further refined through the teacher-student\nparadigm and low-rank adaptation. With extensive comparison of benchmarks such\nas Human3.6M and MPI-INF-3DHP, PoSynDA demonstrates competitive performance,\neven comparable to the target-trained MixSTE model\\cite{zhang2022mixste}. This\nwork paves the way for the practical application of 3D human pose estimation in\nunseen domains. The code is available at https://github.com/hbing-l/PoSynDA.\n",
        "published": "2023",
        "authors": [
            "Hanbing Liu",
            "Jun-Yan He",
            "Zhi-Qi Cheng",
            "Wangmeng Xiang",
            "Qize Yang",
            "Wenhao Chai",
            "Gaoang Wang",
            "Xu Bao",
            "Bin Luo",
            "Yifeng Geng",
            "Xuansong Xie"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.15357v1",
        "title": "Ego-Motion Estimation and Dynamic Motion Separation from 3D Point Clouds\n  for Accumulating Data and Improving 3D Object Detection",
        "abstract": "  New 3+1D high-resolution radar sensors are gaining importance for 3D object\ndetection in the automotive domain due to their relative affordability and\nimproved detection compared to classic low-resolution radar sensors. One\nlimitation of high-resolution radar sensors, compared to lidar sensors, is the\nsparsity of the generated point cloud. This sparsity could be partially\novercome by accumulating radar point clouds of subsequent time steps. This\ncontribution analyzes limitations of accumulating radar point clouds on the\nView-of-Delft dataset. By employing different ego-motion estimation approaches,\nthe dataset's inherent constraints, and possible solutions are analyzed.\nAdditionally, a learning-based instance motion estimation approach is deployed\nto investigate the influence of dynamic motion on the accumulated point cloud\nfor object detection. Experiments document an improved object detection\nperformance by applying an ego-motion estimation and dynamic motion correction\napproach.\n",
        "published": "2023",
        "authors": [
            "Patrick Palmer",
            "Martin Krueger",
            "Richard Altendorfer",
            "Torsten Bertram"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.02561v2",
        "title": "Physically Grounded Vision-Language Models for Robotic Manipulation",
        "abstract": "  Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this\nphysically-grounded VLM in an interactive framework with a large language\nmodel-based robotic planner, and show improved planning performance on tasks\nthat require reasoning about physical object concepts, compared to baselines\nthat do not leverage physically-grounded VLMs. We additionally illustrate the\nbenefits of our physically-grounded VLM on a real robot, where it improves task\nsuccess rates. We release our dataset and provide further details and\nvisualizations of our results at https://iliad.stanford.edu/pg-vlm/.\n",
        "published": "2023",
        "authors": [
            "Jensen Gao",
            "Bidipta Sarkar",
            "Fei Xia",
            "Ted Xiao",
            "Jiajun Wu",
            "Brian Ichter",
            "Anirudha Majumdar",
            "Dorsa Sadigh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.07510v4",
        "title": "Learning Environment-Aware Affordance for 3D Articulated Object\n  Manipulation under Occlusions",
        "abstract": "  Perceiving and manipulating 3D articulated objects in diverse environments is\nessential for home-assistant robots. Recent studies have shown that point-level\naffordance provides actionable priors for downstream manipulation tasks.\nHowever, existing works primarily focus on single-object scenarios with\nhomogeneous agents, overlooking the realistic constraints imposed by the\nenvironment and the agent's morphology, e.g., occlusions and physical\nlimitations. In this paper, we propose an environment-aware affordance\nframework that incorporates both object-level actionable priors and environment\nconstraints. Unlike object-centric affordance approaches, learning\nenvironment-aware affordance faces the challenge of combinatorial explosion due\nto the complexity of various occlusions, characterized by their quantities,\ngeometries, positions and poses. To address this and enhance data efficiency,\nwe introduce a novel contrastive affordance learning framework capable of\ntraining on scenes containing a single occluder and generalizing to scenes with\ncomplex occluder combinations. Experiments demonstrate the effectiveness of our\nproposed approach in learning affordance considering environment constraints.\nProject page at https://chengkaiacademycity.github.io/EnvAwareAfford/\n",
        "published": "2023",
        "authors": [
            "Kai Cheng",
            "Ruihai Wu",
            "Yan Shen",
            "Chuanruo Ning",
            "Guanqi Zhan",
            "Hao Dong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.09118v1",
        "title": "Uncertainty-aware 3D Object-Level Mapping with Deep Shape Priors",
        "abstract": "  3D object-level mapping is a fundamental problem in robotics, which is\nespecially challenging when object CAD models are unavailable during inference.\nIn this work, we propose a framework that can reconstruct high-quality\nobject-level maps for unknown objects. Our approach takes multiple RGB-D images\nas input and outputs dense 3D shapes and 9-DoF poses (including 3 scale\nparameters) for detected objects. The core idea of our approach is to leverage\na learnt generative model for shape categories as a prior and to formulate a\nprobabilistic, uncertainty-aware optimization framework for 3D reconstruction.\nWe derive a probabilistic formulation that propagates shape and pose\nuncertainty through two novel loss functions. Unlike current state-of-the-art\napproaches, we explicitly model the uncertainty of the object shapes and poses\nduring our optimization, resulting in a high-quality object-level mapping\nsystem. Moreover, the resulting shape and pose uncertainties, which we\ndemonstrate can accurately reflect the true errors of our object maps, can also\nbe useful for downstream robotics tasks such as active vision. We perform\nextensive evaluations on indoor and outdoor real-world datasets, achieving\nachieves substantial improvements over state-of-the-art methods. Our code will\nbe available at https://github.com/TRAILab/UncertainShapePose.\n",
        "published": "2023",
        "authors": [
            "Ziwei Liao",
            "Jun Yang",
            "Jingxing Qian",
            "Angela P. Schoellig",
            "Steven L. Waslander"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.09676v1",
        "title": "Conditioning Latent-Space Clusters for Real-World Anomaly Classification",
        "abstract": "  Anomalies in the domain of autonomous driving are a major hindrance to the\nlarge-scale deployment of autonomous vehicles. In this work, we focus on\nhigh-resolution camera data from urban scenes that include anomalies of various\ntypes and sizes. Based on a Variational Autoencoder, we condition its latent\nspace to classify samples as either normal data or anomalies. In order to\nemphasize especially small anomalies, we perform experiments where we provide\nthe VAE with a discrepancy map as an additional input, evaluating its impact on\nthe detection performance. Our method separates normal data and anomalies into\nisolated clusters while still reconstructing high-quality images, leading to\nmeaningful latent representations.\n",
        "published": "2023",
        "authors": [
            "Daniel Bogdoll",
            "Svetlana Pavlitska",
            "Simon Klaus",
            "J. Marius Z\u00f6llner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.13393v2",
        "title": "AgriSORT: A Simple Online Real-time Tracking-by-Detection framework for\n  robotics in precision agriculture",
        "abstract": "  The problem of multi-object tracking (MOT) consists in detecting and tracking\nall the objects in a video sequence while keeping a unique identifier for each\nobject. It is a challenging and fundamental problem for robotics. In precision\nagriculture the challenge of achieving a satisfactory solution is amplified by\nextreme camera motion, sudden illumination changes, and strong occlusions. Most\nmodern trackers rely on the appearance of objects rather than motion for\nassociation, which can be ineffective when most targets are static objects with\nthe same appearance, as in the agricultural case. To this end, on the trail of\nSORT [5], we propose AgriSORT, a simple, online, real-time\ntracking-by-detection pipeline for precision agriculture based only on motion\ninformation that allows for accurate and fast propagation of tracks between\nframes. The main focuses of AgriSORT are efficiency, flexibility, minimal\ndependencies, and ease of deployment on robotic platforms. We test the proposed\npipeline on a novel MOT benchmark specifically tailored for the agricultural\ncontext, based on video sequences taken in a table grape vineyard, particularly\nchallenging due to strong self-similarity and density of the instances. Both\nthe code and the dataset are available for future comparisons.\n",
        "published": "2023",
        "authors": [
            "Leonardo Saraceni",
            "Ionut M. Motoi",
            "Daniele Nardi",
            "Thomas A. Ciarfuglia"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.13744v2",
        "title": "A Systematic Literature Review of Computer Vision Applications in\n  Robotized Wire Harness Assembly",
        "abstract": "  This article presents a systematic literature review on computer vision\napplications that have been proposed for robotized wire harness assembly,\nderives challenges from existing studies, and identifies opportunities for\nfuture research to promote a more practical robotized assembly of wire\nharnesses.\n",
        "published": "2023",
        "authors": [
            "Hao Wang",
            "Omkar Salunkhe",
            "Walter Quadrini",
            "Bj\u00f6rn Johansson",
            "Dan L\u00e4mkull",
            "Fredrik Ore",
            "M\u00e9lanie Despeisse",
            "Luca Fumagalli",
            "Johan Stahre"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.13746v1",
        "title": "Deep Learning-Based Connector Detection for Robotized Assembly of\n  Automotive Wire Harnesses",
        "abstract": "  The shift towards electrification and autonomous driving in the automotive\nindustry results in more and more automotive wire harnesses being installed in\nmodern automobiles, which stresses the great significance of guaranteeing the\nquality of automotive wire harness assembly. The mating of connectors is\nessential in the final assembly of automotive wire harnesses due to the\nimportance of connectors on wire harness connection and signal transmission.\nHowever, the current manual operation of mating connectors leads to severe\nproblems regarding assembly quality and ergonomics, where the robotized\nassembly has been considered, and different vision-based solutions have been\nproposed to facilitate a better perception of the robot control system on\nconnectors. Nonetheless, there has been a lack of deep learning-based solutions\nfor detecting automotive wire harness connectors in previous literature. This\npaper presents a deep learning-based connector detection for robotized\nautomotive wire harness assembly. A dataset of twenty automotive wire harness\nconnectors was created to train and evaluate a two-stage and a one-stage object\ndetection model, respectively. The experiment results indicate the\neffectiveness of deep learning-based connector detection for automotive wire\nharness assembly but are limited by the design of the exteriors of connectors.\n",
        "published": "2023",
        "authors": [
            "Hao Wang",
            "Bj\u00f6rn Johansson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.13893v1",
        "title": "Scene Informer: Anchor-based Occlusion Inference and Trajectory\n  Prediction in Partially Observable Environments",
        "abstract": "  Navigating complex and dynamic environments requires autonomous vehicles\n(AVs) to reason about both visible and occluded regions. This involves\npredicting the future motion of observed agents, inferring occluded ones, and\nmodeling their interactions based on vectorized scene representations of the\npartially observable environment. However, prior work on occlusion inference\nand trajectory prediction have developed in isolation, with the former based on\nsimplified rasterized methods and the latter assuming full environment\nobservability. We introduce the Scene Informer, a unified approach for\npredicting both observed agent trajectories and inferring occlusions in a\npartially observable setting. It uses a transformer to aggregate various input\nmodalities and facilitate selective queries on occlusions that might intersect\nwith the AV's planned path. The framework estimates occupancy probabilities and\nlikely trajectories for occlusions, as well as forecast motion for observed\nagents. We explore common observability assumptions in both domains and their\nperformance impact. Our approach outperforms existing methods in both occupancy\nprediction and trajectory prediction in partially observable setting on the\nWaymo Open Motion Dataset.\n",
        "published": "2023",
        "authors": [
            "Bernard Lange",
            "Jiachen Li",
            "Mykel J. Kochenderfer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.14660v2",
        "title": "CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud\n  Registration",
        "abstract": "  Image-to-point cloud (I2P) registration is a fundamental task in the field of\nautonomous vehicles and transportation systems for cross-modality data fusion\nand localization. Existing I2P registration methods estimate correspondences at\nthe point/pixel level, often overlooking global alignment. However, I2P\nmatching can easily converge to a local optimum when performed without\nhigh-level guidance from global constraints. To address this issue, this paper\nintroduces CoFiI2P, a novel I2P registration network that extracts\ncorrespondences in a coarse-to-fine manner to achieve the globally optimal\nsolution. First, the image and point cloud data are processed through a Siamese\nencoder-decoder network for hierarchical feature extraction. Second, a\ncoarse-to-fine matching module is designed to leverage these features and\nestablish robust feature correspondences. Specifically, In the coarse matching\nphase, a novel I2P transformer module is employed to capture both homogeneous\nand heterogeneous global information from the image and point cloud data. This\nenables the estimation of coarse super-point/super-pixel matching pairs with\ndiscriminative descriptors. In the fine matching module, point/pixel pairs are\nestablished with the guidance of super-point/super-pixel correspondences.\nFinally, based on matching pairs, the transform matrix is estimated with the\nEPnP-RANSAC algorithm. Extensive experiments conducted on the KITTI dataset\ndemonstrate that CoFiI2P achieves impressive results, with a relative rotation\nerror (RRE) of 1.14 degrees and a relative translation error (RTE) of 0.29\nmeters. These results represent a significant improvement of 84\\% in RRE and\n89\\% in RTE compared to the current state-of-the-art (SOTA) method. Qualitative\nresults are available at https://youtu.be/ovbedasXuZE. The source code will be\npublicly released at https://github.com/kang-1-2-3/CoFiI2P.\n",
        "published": "2023",
        "authors": [
            "Shuhao Kang",
            "Youqi Liao",
            "Jianping Li",
            "Fuxun Liang",
            "Yuhao Li",
            "Fangning Li",
            "Zhen Dong",
            "Bisheng Yang"
        ]
    }
]