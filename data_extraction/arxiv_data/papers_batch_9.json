[
    {
        "id": "http://arxiv.org/abs/1606.02407v1",
        "title": "Structured Convolution Matrices for Energy-efficient Deep learning",
        "abstract": "  We derive a relationship between network representation in energy-efficient\nneuromorphic architectures and block Toplitz convolutional matrices. Inspired\nby this connection, we develop deep convolutional networks using a family of\nstructured convolutional matrices and achieve state-of-the-art trade-off\nbetween energy efficiency and classification accuracy for well-known image\nrecognition tasks. We also put forward a novel method to train binary\nconvolutional networks by utilising an existing connection between\nnoisy-rectified linear units and binary activations.\n",
        "published": "2016",
        "authors": [
            "Rathinakumar Appuswamy",
            "Tapan Nayak",
            "John Arthur",
            "Steven Esser",
            "Paul Merolla",
            "Jeffrey Mckinstry",
            "Timothy Melano",
            "Myron Flickner",
            "Dharmendra Modha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.04442v2",
        "title": "DeepMath - Deep Sequence Models for Premise Selection",
        "abstract": "  We study the effectiveness of neural sequence models for premise selection in\nautomated theorem proving, one of the main bottlenecks in the formalization of\nmathematics. We propose a two stage approach for this task that yields good\nresults for the premise selection task on the Mizar corpus while avoiding the\nhand-engineered features of existing state-of-the-art models. To our knowledge,\nthis is the first time deep learning has been applied to theorem proving on a\nlarge scale.\n",
        "published": "2016",
        "authors": [
            "Alex A. Alemi",
            "Francois Chollet",
            "Niklas Een",
            "Geoffrey Irving",
            "Christian Szegedy",
            "Josef Urban"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1611.06791v1",
        "title": "Generalized Dropout",
        "abstract": "  Deep Neural Networks often require good regularizers to generalize well.\nDropout is one such regularizer that is widely used among Deep Learning\npractitioners. Recent work has shown that Dropout can also be viewed as\nperforming Approximate Bayesian Inference over the network parameters. In this\nwork, we generalize this notion and introduce a rich family of regularizers\nwhich we call Generalized Dropout. One set of methods in this family, called\nDropout++, is a version of Dropout with trainable parameters. Classical Dropout\nemerges as a special case of this method. Another member of this family selects\nthe width of neural network layers. Experiments show that these methods help in\nimproving generalization performance over Dropout.\n",
        "published": "2016",
        "authors": [
            "Suraj Srinivas",
            "R. Venkatesh Babu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.02229v1",
        "title": "Italian Event Detection Goes Deep Learning",
        "abstract": "  This paper reports on a set of experiments with different word embeddings to\ninitialize a state-of-the-art Bi-LSTM-CRF network for event detection and\nclassification in Italian, following the EVENTI evaluation exercise. The net-\nwork obtains a new state-of-the-art result by improving the F1 score for\ndetection of 1.3 points, and of 6.5 points for classification, by using a\nsingle step approach. The results also provide further evidence that embeddings\nhave a major impact on the performance of such architectures.\n",
        "published": "2018",
        "authors": [
            "Tommaso Caselli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.11649v1",
        "title": "Fabrik: An Online Collaborative Neural Network Editor",
        "abstract": "  We present Fabrik, an online neural network editor that provides tools to\nvisualize, edit, and share neural networks from within a browser. Fabrik\nprovides a simple and intuitive GUI to import neural networks written in\npopular deep learning frameworks such as Caffe, Keras, and TensorFlow, and\nallows users to interact with, build, and edit models via simple drag and drop.\nFabrik is designed to be framework agnostic and support high interoperability,\nand can be used to export models back to any supported framework. Finally, it\nprovides powerful collaborative features to enable users to iterate over model\ndesign remotely and at scale.\n",
        "published": "2018",
        "authors": [
            "Utsav Garg",
            "Viraj Prabhu",
            "Deshraj Yadav",
            "Ram Ramrakhya",
            "Harsh Agrawal",
            "Dhruv Batra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.01638v1",
        "title": "A Strong Baseline for Domain Adaptation and Generalization in Medical\n  Imaging",
        "abstract": "  This work provides a strong baseline for the problem of multi-source\nmulti-target domain adaptation and generalization in medical imaging. Using a\ndiverse collection of ten chest X-ray datasets, we empirically demonstrate the\nbenefits of training medical imaging deep learning models on varied patient\npopulations for generalization to out-of-sample domains.\n",
        "published": "2019",
        "authors": [
            "Li Yao",
            "Jordan Prosky",
            "Ben Covington",
            "Kevin Lyman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.01188v1",
        "title": "RandomForestMLP: An Ensemble-Based Multi-Layer Perceptron Against Curse\n  of Dimensionality",
        "abstract": "  We present a novel and practical deep learning pipeline termed\nRandomForestMLP. This core trainable classification engine consists of a\nconvolutional neural network backbone followed by an ensemble-based multi-layer\nperceptrons core for the classification task. It is designed in the context of\nself and semi-supervised learning tasks to avoid overfitting while training on\nvery small datasets. The paper details the architecture of the RandomForestMLP\nand present different strategies for neural network decision aggregation. Then,\nit assesses its robustness to overfitting when trained on realistic image\ndatasets and compares its classification performance with existing regular\nclassifiers.\n",
        "published": "2020",
        "authors": [
            "Mohamed Mejri",
            "Aymen Mejri"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.01839v2",
        "title": "Distributed Training and Optimization Of Neural Networks",
        "abstract": "  Deep learning models are yielding increasingly better performances thanks to\nmultiple factors. To be successful, model may have large number of parameters\nor complex architectures and be trained on large dataset. This leads to large\nrequirements on computing resource and turn around time, even more so when\nhyper-parameter optimization is done (e.g search over model architectures).\nWhile this is a challenge that goes beyond particle physics, we review the\nvarious ways to do the necessary computations in parallel, and put it in the\ncontext of high energy physics.\n",
        "published": "2020",
        "authors": [
            "Jean-Roch Vlimant",
            "Junqi Yin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.06311v1",
        "title": "Differentiable Histogram with Hard-Binning",
        "abstract": "  The simplicity and expressiveness of a histogram render it a useful feature\nin different contexts including deep learning. Although the process of\ncomputing a histogram is non-differentiable, researchers have proposed\ndifferentiable approximations, which have some limitations. A differentiable\nhistogram that directly approximates the hard-binning operation in conventional\nhistograms is proposed. It combines the strength of existing differentiable\nhistograms and overcomes their individual challenges. In comparison to a\nhistogram computed using Numpy, the proposed histogram has an absolute\napproximation error of 0.000158.\n",
        "published": "2020",
        "authors": [
            "Ibrahim Yusuf",
            "George Igwegbe",
            "Oluwafemi Azeez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.06452v1",
        "title": "A New Neural Network Architecture Invariant to the Action of Symmetry\n  Subgroups",
        "abstract": "  We propose a computationally efficient $G$-invariant neural network that\napproximates functions invariant to the action of a given permutation subgroup\n$G \\leq S_n$ of the symmetric group on input data. The key element of the\nproposed network architecture is a new $G$-invariant transformation module,\nwhich produces a $G$-invariant latent representation of the input data.\nTheoretical considerations are supported by numerical experiments, which\ndemonstrate the effectiveness and strong generalization properties of the\nproposed method in comparison to other $G$-invariant neural networks.\n",
        "published": "2020",
        "authors": [
            "Piotr Kicki",
            "Mete Ozay",
            "Piotr Skrzypczy\u0144ski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.06898v1",
        "title": "Revisiting \"Qualitatively Characterizing Neural Network Optimization\n  Problems\"",
        "abstract": "  We revisit and extend the experiments of Goodfellow et al. (2014), who showed\nthat - for then state-of-the-art networks - \"the objective function has a\nsimple, approximately convex shape\" along the linear path between\ninitialization and the trained weights. We do not find this to be the case for\nmodern networks on CIFAR-10 and ImageNet. Instead, although loss is roughly\nmonotonically non-increasing along this path, it remains high until close to\nthe optimum. In addition, training quickly becomes linearly separated from the\noptimum by loss barriers. We conclude that, although Goodfellow et al.'s\nfindings describe the \"relatively easy to optimize\" MNIST setting, behavior is\nqualitatively different in modern settings.\n",
        "published": "2020",
        "authors": [
            "Jonathan Frankle"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.07119v2",
        "title": "Demystifying Deep Neural Networks Through Interpretation: A Survey",
        "abstract": "  Modern deep learning algorithms tend to optimize an objective metric, such as\nminimize a cross entropy loss on a training dataset, to be able to learn. The\nproblem is that the single metric is an incomplete description of the real\nworld tasks. The single metric cannot explain why the algorithm learn. When an\nerroneous happens, the lack of interpretability causes a hardness of\nunderstanding and fixing the error. Recently, there are works done to tackle\nthe problem of interpretability to provide insights into neural networks\nbehavior and thought process. The works are important to identify potential\nbias and to ensure algorithm fairness as well as expected performance.\n",
        "published": "2020",
        "authors": [
            "Giang Dao",
            "Minwoo Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.10076v1",
        "title": "Semantics and explanation: why counterfactual explanations produce\n  adversarial examples in deep neural networks",
        "abstract": "  Recent papers in explainable AI have made a compelling case for\ncounterfactual modes of explanation. While counterfactual explanations appear\nto be extremely effective in some instances, they are formally equivalent to\nadversarial examples. This presents an apparent paradox for explainability\nresearchers: if these two procedures are formally equivalent, what accounts for\nthe explanatory divide apparent between counterfactual explanations and\nadversarial examples? We resolve this paradox by placing emphasis back on the\nsemantics of counterfactual expressions. Producing satisfactory explanations\nfor deep learning systems will require that we find ways to interpret the\nsemantics of hidden layer representations in deep neural networks.\n",
        "published": "2020",
        "authors": [
            "Kieran Browne",
            "Ben Swift"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.10257v1",
        "title": "Domain Generalization using Ensemble Learning",
        "abstract": "  Domain generalization is a sub-field of transfer learning that aims at\nbridging the gap between two different domains in the absence of any knowledge\nabout the target domain. Our approach tackles the problem of a model's weak\ngeneralization when it is trained on a single source domain. From this\nperspective, we build an ensemble model on top of base deep learning models\ntrained on a single source to enhance the generalization of their collective\nprediction. The results achieved thus far have demonstrated promising\nimprovements of the ensemble over any of its base learners.\n",
        "published": "2021",
        "authors": [
            "Yusuf Mesbah",
            "Youssef Youssry Ibrahim",
            "Adil Mehood Khan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.11943v1",
        "title": "BERT: A Review of Applications in Natural Language Processing and\n  Understanding",
        "abstract": "  In this review, we describe the application of one of the most popular deep\nlearning-based language models - BERT. The paper describes the mechanism of\noperation of this model, the main areas of its application to the tasks of text\nanalytics, comparisons with similar models in each task, as well as a\ndescription of some proprietary models. In preparing this review, the data of\nseveral dozen original scientific articles published over the past few years,\nwhich attracted the most attention in the scientific community, were\nsystematized. This survey will be useful to all students and researchers who\nwant to get acquainted with the latest advances in the field of natural\nlanguage text analysis.\n",
        "published": "2021",
        "authors": [
            "M. V. Koroteev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2103.12169v1",
        "title": "A Pilot Study For Fragment Identification Using 2D NMR and Deep Learning",
        "abstract": "  This paper presents a method to identify substructures in NMR spectra of\nmixtures, specifically 2D spectra, using a bespoke image-based Convolutional\nNeural Network application. This is done using HSQC and HMBC spectra separately\nand in combination. The application can reliably detect substructures in pure\ncompounds, using a simple network. It can work for mixtures when trained on\npure compounds only. HMBC data and the combination of HMBC and HSQC show better\nresults than HSQC alone.\n",
        "published": "2021",
        "authors": [
            "Stefan Kuhn",
            "Eda Tumer",
            "Simon Colreavy-Donnelly",
            "Ricardo Moreira Borges"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.03418v1",
        "title": "Quantum Enhanced Filter: QFilter",
        "abstract": "  Convolutional Neural Networks (CNN) are used mainly to treat problems with\nmany images characteristic of Deep Learning. In this work, we propose a hybrid\nimage classification model to take advantage of quantum and classical\ncomputing. The method will use the potential that convolutional networks have\nshown in artificial intelligence by replacing classical filters with\nvariational quantum filters. Similarly, this work will compare with other\nclassification methods and the system's execution on different servers. The\nalgorithm's quantum feasibility is modelled and tested on Amazon Braket\nNotebook instances and experimented on the Pennylane's philosophy and\nframework.\n",
        "published": "2021",
        "authors": [
            "Parfait Atchade-Adelomou",
            "Guillermo Alonso-Linaje"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.07705v2",
        "title": "How to Train BERT with an Academic Budget",
        "abstract": "  While large language models a la BERT are used ubiquitously in NLP,\npretraining them is considered a luxury that only a few well-funded industry\nlabs can afford. How can one train such models with a more modest budget? We\npresent a recipe for pretraining a masked language model in 24 hours using a\nsingle low-end deep learning server. We demonstrate that through a combination\nof software optimizations, design choices, and hyperparameter tuning, it is\npossible to produce models that are competitive with BERT-base on GLUE tasks at\na fraction of the original pretraining cost.\n",
        "published": "2021",
        "authors": [
            "Peter Izsak",
            "Moshe Berchansky",
            "Omer Levy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2104.10215v1",
        "title": "Evaluating the Impact of a Hierarchical Discourse Representation on\n  Entity Coreference Resolution Performance",
        "abstract": "  Recent work on entity coreference resolution (CR) follows current trends in\nDeep Learning applied to embeddings and relatively simple task-related\nfeatures. SOTA models do not make use of hierarchical representations of\ndiscourse structure. In this work, we leverage automatically constructed\ndiscourse parse trees within a neural approach and demonstrate a significant\nimprovement on two benchmark entity coreference-resolution datasets. We explore\nhow the impact varies depending upon the type of mention.\n",
        "published": "2021",
        "authors": [
            "Sopan Khosla",
            "James Fiacco",
            "Carolyn Rose"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.00274v1",
        "title": "Analysis of classifiers robust to noisy labels",
        "abstract": "  We explore contemporary robust classification algorithms for overcoming\nclass-dependant labelling noise: Forward, Importance Re-weighting and\nT-revision. The classifiers are trained and evaluated on class-conditional\nrandom label noise data while the final test data is clean. We demonstrate\nmethods for estimating the transition matrix in order to obtain better\nclassifier performance when working with noisy data. We apply deep learning to\nthree data-sets and derive an end-to-end analysis with unknown noise on the\nCIFAR data-set from scratch. The effectiveness and robustness of the\nclassifiers are analysed, and we compare and contrast the results of each\nexperiment are using top-1 accuracy as our criterion.\n",
        "published": "2021",
        "authors": [
            "Alex D\u00edaz",
            "Damian Steele"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.08446v2",
        "title": "Bridge Networks: Relating Inputs through Vector-Symbolic Manipulations",
        "abstract": "  Despite rapid progress, current deep learning methods face a number of\ncritical challenges. These include high energy consumption, catastrophic\nforgetting, dependance on global losses, and an inability to reason\nsymbolically. By combining concepts from information bottleneck theory and\nvector-symbolic architectures, we propose and implement a novel information\nprocessing architecture, the 'Bridge network.' We show this architecture\nprovides unique advantages which can address the problem of global losses and\ncatastrophic forgetting. Furthermore, we argue that it provides a further basis\nfor increasing energy efficiency of execution and the ability to reason\nsymbolically.\n",
        "published": "2021",
        "authors": [
            "Wilkie Olin-Ammentorp",
            "Maxim Bazhenov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.13911v1",
        "title": "Predictive Control Using Learned State Space Models via Rolling Horizon\n  Evolution",
        "abstract": "  A large part of the interest in model-based reinforcement learning derives\nfrom the potential utility to acquire a forward model capable of strategic long\nterm decision making. Assuming that an agent succeeds in learning a useful\npredictive model, it still requires a mechanism to harness it to generate and\nselect among competing simulated plans. In this paper, we explore this theme\ncombining evolutionary algorithmic planning techniques with models learned via\ndeep learning and variational inference. We demonstrate the approach with an\nagent that reliably performs online planning in a set of visual navigation\ntasks.\n",
        "published": "2021",
        "authors": [
            "Alvaro Ovalle",
            "Simon M. Lucas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.03383v1",
        "title": "DeepZensols: Deep Natural Language Processing Framework",
        "abstract": "  Reproducing results in publications by distributing publicly available source\ncode is becoming ever more popular. Given the difficulty of reproducing machine\nlearning (ML) experiments, there have been significant efforts in reducing the\nvariance of these results. As in any science, the ability to consistently\nreproduce results effectively strengthens the underlying hypothesis of the\nwork, and thus, should be regarded as important as the novel aspect of the\nresearch itself. The contribution of this work is a framework that is able to\nreproduce consistent results and provides a means of easily creating, training,\nand evaluating natural language processing (NLP) deep learning (DL) models.\n",
        "published": "2021",
        "authors": [
            "Paul Landes",
            "Barbara Di Eugenio",
            "Cornelia Caragea"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.08291v1",
        "title": "Natlog: a Lightweight Logic Programming Language with a Neuro-symbolic\n  Touch",
        "abstract": "  We introduce Natlog, a lightweight Logic Programming language, sharing\nProlog's unification-driven execution model, but with a simplified syntax and\nsemantics. Our proof-of-concept Natlog implementation is tightly embedded in\nthe Python-based deep-learning ecosystem with focus on content-driven indexing\nof ground term datasets. As an overriding of our symbolic indexing algorithm,\nthe same function can be delegated to a neural network, serving ground facts to\nNatlog's resolution engine. Our open-source implementation is available as a\nPython package at https://pypi.org/project/natlog/ .\n",
        "published": "2021",
        "authors": [
            "Paul Tarau"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.08996v1",
        "title": "Dynamic and Systematic Survey of Deep Learning Approaches for Driving\n  Behavior Analysis",
        "abstract": "  Improper driving results in fatalities, damages, increased energy\nconsumptions, and depreciation of the vehicles. Analyzing driving behaviour\ncould lead to optimize and avoid mentioned issues. By identifying the type of\ndriving and mapping them to the consequences of that type of driving, we can\nget a model to prevent them. In this regard, we try to create a dynamic survey\npaper to review and present driving behaviour survey data for future\nresearchers in our research. By analyzing 58 articles, we attempt to classify\nstandard methods and provide a framework for future articles to be examined and\nstudied in different dashboards and updated about trends.\n",
        "published": "2021",
        "authors": [
            "Farid Talebloo",
            "Emad A. Mohammed",
            "Behrouz H. Far"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2109.10100v1",
        "title": "A Novel Structured Natural Gradient Descent for Deep Learning",
        "abstract": "  Natural gradient descent (NGD) provided deep insights and powerful tools to\ndeep neural networks. However the computation of Fisher information matrix\nbecomes more and more difficult as the network structure turns large and\ncomplex. This paper proposes a new optimization method whose main idea is to\naccurately replace the natural gradient optimization by reconstructing the\nnetwork. More specifically, we reconstruct the structure of the deep neural\nnetwork, and optimize the new network using traditional gradient descent (GD).\nThe reconstructed network achieves the effect of the optimization way with\nnatural gradient descent. Experimental results show that our optimization\nmethod can accelerate the convergence of deep network models and achieve better\nperformance than GD while sharing its computational simplicity.\n",
        "published": "2021",
        "authors": [
            "Weihua Liu",
            "Xiabi Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.00625v1",
        "title": "Accelerate Distributed Stochastic Descent for Nonconvex Optimization\n  with Momentum",
        "abstract": "  Momentum method has been used extensively in optimizers for deep learning.\nRecent studies show that distributed training through K-step averaging has many\nnice properties. We propose a momentum method for such model averaging\napproaches. At each individual learner level traditional stochastic gradient is\napplied. At the meta-level (global learner level), one momentum term is applied\nand we call it block momentum. We analyze the convergence and scaling\nproperties of such momentum methods. Our experimental results show that block\nmomentum not only accelerates training, but also achieves better results.\n",
        "published": "2021",
        "authors": [
            "Guojing Cong",
            "Tianyi Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.02279v1",
        "title": "Turing approximations, toric isometric embeddings & manifold\n  convolutions",
        "abstract": "  Convolutions are fundamental elements in deep learning architectures. Here,\nwe present a theoretical framework for combining extrinsic and intrinsic\napproaches to manifold convolution through isometric embeddings into tori. In\nthis way, we define a convolution operator for a manifold of arbitrary topology\nand dimension. We also explain geometric and topological conditions that make\nsome local definitions of convolutions which rely on translating filters along\ngeodesic paths on a manifold, computationally intractable. A result of Alan\nTuring from 1938 underscores the need for such a toric isometric embedding\napproach to achieve a global definition of convolution on computable, finite\nmetric space approximations to a smooth manifold.\n",
        "published": "2021",
        "authors": [
            "P. Su\u00e1rez-Serrato"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.06510v1",
        "title": "The Dawn of Quantum Natural Language Processing",
        "abstract": "  In this paper, we discuss the initial attempts at boosting understanding\nhuman language based on deep-learning models with quantum computing. We\nsuccessfully train a quantum-enhanced Long Short-Term Memory network to perform\nthe parts-of-speech tagging task via numerical simulations. Moreover, a\nquantum-enhanced Transformer is proposed to perform the sentiment analysis\nbased on the existing dataset.\n",
        "published": "2021",
        "authors": [
            "Riccardo Di Sipio",
            "Jia-Hong Huang",
            "Samuel Yen-Chi Chen",
            "Stefano Mangini",
            "Marcel Worring"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.08037v1",
        "title": "Tensor-to-Image: Image-to-Image Translation with Vision Transformers",
        "abstract": "  Transformers gain huge attention since they are first introduced and have a\nwide range of applications. Transformers start to take over all areas of deep\nlearning and the Vision transformers paper also proved that they can be used\nfor computer vision tasks. In this paper, we utilized a vision\ntransformer-based custom-designed model, tensor-to-image, for the image to\nimage translation. With the help of self-attention, our model was able to\ngeneralize and apply to different problems without a single modification.\n",
        "published": "2021",
        "authors": [
            "Yi\u011fit G\u00fcnd\u00fc\u00e7"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.04273v1",
        "title": "Mimic: An adaptive algorithm for multivariate time series classification",
        "abstract": "  Time series data are valuable but are often inscrutable. Gaining trust in\ntime series classifiers for finance, healthcare, and other critical\napplications may rely on creating interpretable models. Researchers have\npreviously been forced to decide between interpretable methods that lack\npredictive power and deep learning methods that lack transparency. In this\npaper, we propose a novel Mimic algorithm that retains the predictive accuracy\nof the strongest classifiers while introducing interpretability. Mimic mirrors\nthe learning method of an existing multivariate time series classifier while\nsimultaneously producing a visual representation that enhances user\nunderstanding of the learned model. Experiments on 26 time series datasets\nsupport Mimic's ability to imitate a variety of time series classifiers\nvisually and accurately.\n",
        "published": "2021",
        "authors": [
            "Yuhui Wang",
            "Diane J. Cook"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.04313v1",
        "title": "A Relational Model for One-Shot Classification",
        "abstract": "  We show that a deep learning model with built-in relational inductive bias\ncan bring benefits to sample-efficient learning, without relying on extensive\ndata augmentation. The proposed one-shot classification model performs\nrelational matching of a pair of inputs in the form of local and pairwise\nattention. Our approach solves perfectly the one-shot image classification\nOmniglot challenge. Our model exceeds human level accuracy, as well as the\nprevious state of the art, with no data augmentation.\n",
        "published": "2021",
        "authors": [
            "Arturs Polis",
            "Alexander Ilin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.05498v2",
        "title": "Attention Approximates Sparse Distributed Memory",
        "abstract": "  While Attention has come to be an important mechanism in deep learning, there\nremains limited intuition for why it works so well. Here, we show that\nTransformer Attention can be closely related under certain data conditions to\nKanerva's Sparse Distributed Memory (SDM), a biologically plausible associative\nmemory model. We confirm that these conditions are satisfied in pre-trained\nGPT2 Transformer models. We discuss the implications of the Attention-SDM map\nand provide new computational and biological interpretations of Attention.\n",
        "published": "2021",
        "authors": [
            "Trenton Bricken",
            "Cengiz Pehlevan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2201.09395v1",
        "title": "MISeval: a Metric Library for Medical Image Segmentation Evaluation",
        "abstract": "  Correct performance assessment is crucial for evaluating modern artificial\nintelligence algorithms in medicine like deep-learning based medical image\nsegmentation models. However, there is no universal metric library in Python\nfor standardized and reproducible evaluation. Thus, we propose our open-source\npublicly available Python package MISeval: a metric library for Medical Image\nSegmentation Evaluation. The implemented metrics can be intuitively used and\neasily integrated into any performance assessment pipeline. The package\nutilizes modern CI/CD strategies to ensure functionality and stability. MISeval\nis available from PyPI (miseval) and GitHub:\nhttps://github.com/frankkramer-lab/miseval.\n",
        "published": "2022",
        "authors": [
            "Dominik M\u00fcller",
            "Dennis Hartmann",
            "Philip Meyer",
            "Florian Auer",
            "I\u00f1aki Soto-Rey",
            "Frank Kramer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.00360v1",
        "title": "Extracting Rules from Neural Networks with Partial Interpretations",
        "abstract": "  We investigate the problem of extracting rules, expressed in Horn logic, from\nneural network models. Our work is based on the exact learning model, in which\na learner interacts with a teacher (the neural network model) via queries in\norder to learn an abstract target concept, which in our case is a set of Horn\nrules. We consider partial interpretations to formulate the queries. These can\nbe understood as a representation of the world where part of the knowledge\nregarding the truthiness of propositions is unknown. We employ Angluin s\nalgorithm for learning Horn rules via queries and evaluate our strategy\nempirically.\n",
        "published": "2022",
        "authors": [
            "Cosimo Persia",
            "Ana Ozaki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.01620v1",
        "title": "Towards Deep Industrial Transfer Learning: Clustering for Transfer Case\n  Selection",
        "abstract": "  Industrial transfer learning increases the adaptability of deep learning\nalgorithms towards heterogenous and dynamic industrial use cases without high\nmanual efforts. The appropriate selection of what to transfer can vastly\nimprove a transfer's results. In this paper, a transfer case selection based\nupon clustering is presented. Founded on a survey of clustering algorithms, the\nBIRCH algorithm is selected for this purpose. It is evaluated on an industrial\ntime series dataset from a discrete manufacturing scenario. Results underline\nthe approaches' applicability caused by its results' reproducibility and\npractical indifference to sequence, size and dimensionality of (sub-)datasets\nto be clustered sequentially.\n",
        "published": "2022",
        "authors": [
            "Benjamin Maschler",
            "Tim Knodel",
            "Michael Weyrich"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.03418v1",
        "title": "Continual Inference: A Library for Efficient Online Inference with Deep\n  Neural Networks in PyTorch",
        "abstract": "  We present Continual Inference, a Python library for implementing Continual\nInference Networks (CINs) in PyTorch, a class of Neural Networks designed\nspecifically for efficient inference in both online and batch processing\nscenarios. We offer a comprehensive introduction and guide to CINs and their\nimplementation in practice, and provide best-practices and code examples for\ncomposing complex modules for modern Deep Learning. Continual Inference is\nreadily downloadable via the Python Package Index and at\n\\url{www.github.com/lukashedegaard/continual-inference}.\n",
        "published": "2022",
        "authors": [
            "Lukas Hedegaard",
            "Alexandros Iosifidis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.05806v1",
        "title": "Variational Heteroscedastic Volatility Model",
        "abstract": "  We propose Variational Heteroscedastic Volatility Model (VHVM) -- an\nend-to-end neural network architecture capable of modelling heteroscedastic\nbehaviour in multivariate financial time series. VHVM leverages recent advances\nin several areas of deep learning, namely sequential modelling and\nrepresentation learning, to model complex temporal dynamics between different\nasset returns. At its core, VHVM consists of a variational autoencoder to\ncapture relationships between assets, and a recurrent neural network to model\nthe time-evolution of these dependencies. The outputs of VHVM are time-varying\nconditional volatilities in the form of covariance matrices. We demonstrate the\neffectiveness of VHVM against existing methods such as Generalised\nAutoRegressive Conditional Heteroscedasticity (GARCH) and Stochastic Volatility\n(SV) models on a wide range of multivariate foreign currency (FX) datasets.\n",
        "published": "2022",
        "authors": [
            "Zexuan Yin",
            "Paolo Barucca"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.12665v1",
        "title": "Relational Abstractions for Generalized Reinforcement Learning on\n  Symbolic Problems",
        "abstract": "  Reinforcement learning in problems with symbolic state spaces is challenging\ndue to the need for reasoning over long horizons. This paper presents a new\napproach that utilizes relational abstractions in conjunction with deep\nlearning to learn a generalizable Q-function for such problems. The learned\nQ-function can be efficiently transferred to related problems that have\ndifferent object names and object quantities, and thus, entirely different\nstate spaces. We show that the learned generalized Q-function can be utilized\nfor zero-shot transfer to related problems without an explicit, hand-coded\ncurriculum. Empirical evaluations on a range of problems show that our method\nfacilitates efficient zero-shot transfer of learned knowledge to much larger\nproblem instances containing many objects.\n",
        "published": "2022",
        "authors": [
            "Rushang Karia",
            "Siddharth Srivastava"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.00523v1",
        "title": "Deep Learning with Logical Constraints",
        "abstract": "  In recent years, there has been an increasing interest in exploiting\nlogically specified background knowledge in order to obtain neural models (i)\nwith a better performance, (ii) able to learn from less data, and/or (iii)\nguaranteed to be compliant with the background knowledge itself, e.g., for\nsafety-critical applications. In this survey, we retrace such works and\ncategorize them based on (i) the logical language that they use to express the\nbackground knowledge and (ii) the goals that they achieve.\n",
        "published": "2022",
        "authors": [
            "Eleonora Giunchiglia",
            "Mihaela Catalina Stoian",
            "Thomas Lukasiewicz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.02343v1",
        "title": "Convolutional and Residual Networks Provably Contain Lottery Tickets",
        "abstract": "  The Lottery Ticket Hypothesis continues to have a profound practical impact\non the quest for small scale deep neural networks that solve modern deep\nlearning tasks at competitive performance. These lottery tickets are identified\nby pruning large randomly initialized neural networks with architectures that\nare as diverse as their applications. Yet, theoretical insights that attest\ntheir existence have been mostly focused on deep fully-connected feed forward\nnetworks with ReLU activation functions. We prove that also modern\narchitectures consisting of convolutional and residual layers that can be\nequipped with almost arbitrary activation functions can contain lottery tickets\nwith high probability.\n",
        "published": "2022",
        "authors": [
            "Rebekka Burkholz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.02572v1",
        "title": "Automated visual inspection of silicon detectors in CMS experiment",
        "abstract": "  In the CMS experiment at CERN, Geneva, a large number of HGCAL sensor modules\nare fabricated in advanced laboratories around the world. Each sensor module\ncontains about 700 checkpoints for visual inspection thus making it almost\nimpossible to carry out such inspection manually. As artificial intelligence is\nmore and more widely used in manufacturing, traditional detection technologies\nare gradually being intelligent. In order to more accurately evaluate the\ncheckpoints, we propose to use deep learning-based object detection techniques\nto detect manufacturing defects in testing large numbers of modules\nautomatically.\n",
        "published": "2022",
        "authors": [
            "Dr. Nupur Giri",
            "Dr. Shashi Dugad",
            "Amit Chhabria",
            "Rashmi Manwani",
            "Priyanka Asrani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.07655v1",
        "title": "Classification of EEG Motor Imagery Using Deep Learning for\n  Brain-Computer Interface Systems",
        "abstract": "  A trained T1 class Convolutional Neural Network (CNN) model will be used to\nexamine its ability to successfully identify motor imagery when fed\npre-processed electroencephalography (EEG) data. In theory, and if the model\nhas been trained accurately, it should be able to identify a class and label it\naccordingly. The CNN model will then be restored and used to try and identify\nthe same class of motor imagery data using much smaller sampled data in an\nattempt to simulate live data.\n",
        "published": "2022",
        "authors": [
            "Alessandro Gallo",
            "Manh Duong Phung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.09075v1",
        "title": "Comment on Transferability and Input Transformation with Additive Noise",
        "abstract": "  Adversarial attacks have verified the existence of the vulnerability of\nneural networks. By adding small perturbations to a benign example, adversarial\nattacks successfully generate adversarial examples that lead misclassification\nof deep learning models. More importantly, an adversarial example generated\nfrom a specific model can also deceive other models without modification. We\ncall this phenomenon ``transferability\". Here, we analyze the relationship\nbetween transferability and input transformation with additive noise by\nmathematically proving that the modified optimization can produce more\ntransferable adversarial examples.\n",
        "published": "2022",
        "authors": [
            "Hoki Kim",
            "Jinseong Park",
            "Jaewook Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.02108v2",
        "title": "AI-based Malware and Ransomware Detection Models",
        "abstract": "  Cybercrime is one of the major digital threats of this century. In\nparticular, ransomware attacks have significantly increased, resulting in\nglobal damage costs of tens of billion dollars. In this paper, we train and\ntest different Machine Learning and Deep Learning models for malware detection,\nmalware classification and ransomware detection. We introduce a novel and\nflexible solution that combines two optimized models for malware and ransomware\ndetection. Our results demonstrate some improvements both in terms of detection\nperformances and flexibility. In particular, our combined models pave the way\nfor easier future enhancements using specialized and thus interchangeable\ndetection modules.\n",
        "published": "2022",
        "authors": [
            "Benjamin Marais",
            "Tony Quertier",
            "St\u00e9phane Morucci"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.07769v1",
        "title": "Anomalous behaviour in loss-gradient based interpretability methods",
        "abstract": "  Loss-gradients are used to interpret the decision making process of deep\nlearning models. In this work, we evaluate loss-gradient based attribution\nmethods by occluding parts of the input and comparing the performance of the\noccluded input to the original input. We observe that the occluded input has\nbetter performance than the original across the test dataset under certain\nconditions. Similar behaviour is observed in sound and image recognition tasks.\nWe explore different loss-gradient attribution methods, occlusion levels and\nreplacement values to explain the phenomenon of performance improvement under\nocclusion.\n",
        "published": "2022",
        "authors": [
            "Vinod Subramanian",
            "Siddharth Gururani",
            "Emmanouil Benetos",
            "Mark Sandler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2207.09145v1",
        "title": "GAFX: A General Audio Feature eXtractor",
        "abstract": "  Most machine learning models for audio tasks are dealing with a handcrafted\nfeature, the spectrogram. However, it is still unknown whether the spectrogram\ncould be replaced with deep learning based features. In this paper, we answer\nthis question by comparing the different learnable neural networks extracting\nfeatures with a successful spectrogram model and proposed a General Audio\nFeature eXtractor (GAFX) based on a dual U-Net (GAFX-U), ResNet (GAFX-R), and\nAttention (GAFX-A) modules. We design experiments to evaluate this model on the\nmusic genre classification task on the GTZAN dataset and perform a detailed\nablation study of different configurations of our framework and our model\nGAFX-U, following the Audio Spectrogram Transformer (AST) classifier achieves\ncompetitive performance.\n",
        "published": "2022",
        "authors": [
            "Zhaoyang Bu",
            "Hanhaodi Zhang",
            "Xiaohu Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.04892v1",
        "title": "Intrinsically Motivated Learning of Causal World Models",
        "abstract": "  Despite the recent progress in deep learning and reinforcement learning,\ntransfer and generalization of skills learned on specific tasks is very limited\ncompared to human (or animal) intelligence. The lifelong, incremental building\nof common sense knowledge might be a necessary component on the way to achieve\nmore general intelligence. A promising direction is to build world models\ncapturing the true physical mechanisms hidden behind the sensorimotor\ninteraction with the environment. Here we explore the idea that inferring the\ncausal structure of the environment could benefit from well-chosen actions as\nmeans to collect relevant interventional data.\n",
        "published": "2022",
        "authors": [
            "Louis Annabi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2208.06957v2",
        "title": "Syntax-driven Data Augmentation for Named Entity Recognition",
        "abstract": "  In low resource settings, data augmentation strategies are commonly leveraged\nto improve performance. Numerous approaches have attempted document-level\naugmentation (e.g., text classification), but few studies have explored\ntoken-level augmentation. Performed naively, data augmentation can produce\nsemantically incongruent and ungrammatical examples. In this work, we compare\nsimple masked language model replacement and an augmentation method using\nconstituency tree mutations to improve the performance of named entity\nrecognition in low-resource settings with the aim of preserving linguistic\ncohesion of the augmented sentences.\n",
        "published": "2022",
        "authors": [
            "Arie Pratama Sutiono",
            "Gus Hahn-Powell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.03580v1",
        "title": "Conformal Methods for Quantifying Uncertainty in Spatiotemporal Data: A\n  Survey",
        "abstract": "  Machine learning methods are increasingly widely used in high-risk settings\nsuch as healthcare, transportation, and finance. In these settings, it is\nimportant that a model produces calibrated uncertainty to reflect its own\nconfidence and avoid failures. In this paper we survey recent works on\nuncertainty quantification (UQ) for deep learning, in particular\ndistribution-free Conformal Prediction method for its mathematical properties\nand wide applicability. We will cover the theoretical guarantees of conformal\nmethods, introduce techniques that improve calibration and efficiency for UQ in\nthe context of spatiotemporal data, and discuss the role of UQ in the context\nof safe decision making.\n",
        "published": "2022",
        "authors": [
            "Sophia Sun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2209.05709v1",
        "title": "Generalization Bounds for Deep Transfer Learning Using Majority\n  Predictor Accuracy",
        "abstract": "  We analyze new generalization bounds for deep learning models trained by\ntransfer learning from a source to a target task. Our bounds utilize a quantity\ncalled the majority predictor accuracy, which can be computed efficiently from\ndata. We show that our theory is useful in practice since it implies that the\nmajority predictor accuracy can be used as a transferability measure, a fact\nthat is also validated by our experiments.\n",
        "published": "2022",
        "authors": [
            "Cuong N. Nguyen",
            "Lam Si Tung Ho",
            "Vu Dinh",
            "Tal Hassner",
            "Cuong V. Nguyen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.01806v1",
        "title": "Low-Light Image Restoration Based on Retina Model using Neural Networks",
        "abstract": "  We report the possibility of using a simple neural network for effortless\nrestoration of low-light images inspired by the retina model, which mimics the\nneurophysiological principles and dynamics of various types of optical neurons.\nThe proposed neural network model saves the cost of computational overhead in\ncontrast with traditional signal-processing models, and generates results\ncomparable with complicated deep learning models from the subjective perceptual\nperspective. This work shows that to directly simulate the functionalities of\nretinal neurons using neural networks not only avoids the manually seeking for\nthe optimal parameters, but also paves the way to build corresponding\nartificial versions for certain neurobiological organizations.\n",
        "published": "2022",
        "authors": [
            "Yurui Ming",
            "Yuanyuan Liang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.15387v2",
        "title": "AIREPAIR: A Repair Platform for Neural Networks",
        "abstract": "  We present AIREPAIR, a platform for repairing neural networks. It features\nthe integration of existing network repair tools. Based on AIREPAIR, one can\nrun different repair methods on the same model, thus enabling the fair\ncomparison of different repair techniques. We evaluate AIREPAIR with three\nstate-of-the-art repair tools on popular deep-learning datasets and models. Our\nevaluation confirms the utility of AIREPAIR, by comparing and analyzing the\nresults from different repair techniques. A demonstration is available at\nhttps://youtu.be/UkKw5neeWhw.\n",
        "published": "2022",
        "authors": [
            "Xidan Song",
            "Youcheng Sun",
            "Mustafa A. Mustafa",
            "Lucas Cordeiro"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.04030v1",
        "title": "Analysis of Deep Learning Architectures and Efficacy of Detecting Forest\n  Fires",
        "abstract": "  The aim of this research is to review the state of computer vision as applied\nto combatting forest fires. My motivation to research this topic comes from the\nurgency with which new participants and stakeholders require guidance in this\nfield. One of these new stakeholder groups are practitioners of machine\nlearning that lack domain expertise. Introducing these new entrants to domain\nspecific datasets and methods is critical to supporting this aim as general\ncomputer vision datasets are insufficient to support specialized research\ninitiatives. The overarching aim of the research is to introduce datasets and\nmethods to make them more accessible to the community.\n",
        "published": "2022",
        "authors": [
            "Ryan Marinelli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.07126v1",
        "title": "Explainability of Text Processing and Retrieval Methods: A Critical\n  Survey",
        "abstract": "  Deep Learning and Machine Learning based models have become extremely popular\nin text processing and information retrieval. However, the non-linear\nstructures present inside the networks make these models largely inscrutable. A\nsignificant body of research has focused on increasing the transparency of\nthese models. This article provides a broad overview of research on the\nexplainability and interpretability of natural language processing and\ninformation retrieval methods. More specifically, we survey approaches that\nhave been applied to explain word embeddings, sequence modeling, attention\nmodules, transformers, BERT, and document ranking. The concluding section\nsuggests some possible directions for future research on this topic.\n",
        "published": "2022",
        "authors": [
            "Sourav Saha",
            "Debapriyo Majumdar",
            "Mandar Mitra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.12067v1",
        "title": "Enhancing the prediction of disease outcomes using electronic health\n  records and pretrained deep learning models",
        "abstract": "  Question: Can an encoder-decoder architecture pretrained on a large dataset\nof longitudinal electronic health records improves patient outcome predictions?\nFindings: In this prognostic study of 6.8 million patients, our denoising\nsequence-to-sequence prediction model of multiple outcomes outperformed\nstate-of-the-art models scuh pretrained BERT on a broad range of patient\noutcomes, including intentional self-harm and pancreatic cancer. Meaning: Deep\nbidirectional and autoregressive representation improves patient outcome\nprediction.\n",
        "published": "2022",
        "authors": [
            "Zhichao Yang",
            "Weisong Liu",
            "Dan Berlowitz",
            "Hong Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2301.09567v1",
        "title": "Rig Inversion by Training a Differentiable Rig Function",
        "abstract": "  Rig inversion is the problem of creating a method that can find the rig\nparameter vector that best approximates a given input mesh. In this paper we\npropose to solve this problem by first obtaining a differentiable rig function\nby training a multi layer perceptron to approximate the rig function. This\ndifferentiable rig function can then be used to train a deep learning model of\nrig inversion.\n",
        "published": "2023",
        "authors": [
            "Mathieu Marquis Bolduc",
            "Hau Nghiep Phan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.08852v1",
        "title": "Python Tool for Visualizing Variability of Pareto Fronts over Multiple\n  Runs",
        "abstract": "  Hyperparameter optimization is crucial to achieving high performance in deep\nlearning. On top of the performance, other criteria such as inference time or\nmemory requirement often need to be optimized due to some practical reasons.\nThis motivates research on multi-objective optimization (MOO). However, Pareto\nfronts of MOO methods are often shown without considering the variability\ncaused by random seeds and this makes the performance stability evaluation\ndifficult. Although there is a concept named empirical attainment surface to\nenable the visualization with uncertainty over multiple runs, there is no major\nPython package for empirical attainment surface. We, therefore, develop a\nPython package for this purpose and describe the usage. The package is\navailable at https://github.com/nabenabe0928/empirical-attainment-func.\n",
        "published": "2023",
        "authors": [
            "Shuhei Watanabe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.08609v1",
        "title": "TomoSAM: a 3D Slicer extension using SAM for tomography segmentation",
        "abstract": "  TomoSAM has been developed to integrate the cutting-edge Segment Anything\nModel (SAM) into 3D Slicer, a highly capable software platform used for 3D\nimage processing and visualization. SAM is a promptable deep learning model\nthat is able to identify objects and create image masks in a zero-shot manner,\nbased only on a few user clicks. The synergy between these tools aids in the\nsegmentation of complex 3D datasets from tomography or other imaging\ntechniques, which would otherwise require a laborious manual segmentation\nprocess. The source code associated with this article can be found at\nhttps://github.com/fsemerar/SlicerTomoSAM\n",
        "published": "2023",
        "authors": [
            "Federico Semeraro",
            "Alexandre Quintart",
            "Sergio Fraile Izquierdo",
            "Joseph C. Ferguson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.13905v1",
        "title": "Spatio-temporal Storytelling? Leveraging Generative Models for Semantic\n  Trajectory Analysis",
        "abstract": "  In this paper, we lay out a vision for analysing semantic trajectory traces\nand generating synthetic semantic trajectory data (SSTs) using generative\nlanguage model. Leveraging the advancements in deep learning, as evident by\nprogress in the field of natural language processing (NLP), computer vision,\netc. we intend to create intelligent models that can study the semantic\ntrajectories in various contexts, predicting future trends, increasing machine\nunderstanding of the movement of animals, humans, goods, etc. enhancing\nhuman-computer interactions, and contributing to an array of applications\nranging from urban-planning to personalized recommendation engines and business\nstrategy.\n",
        "published": "2023",
        "authors": [
            "Shreya Ghosh",
            "Saptarshi Sengupta",
            "Prasenjit Mitra"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2307.14109v1",
        "title": "GraphRNN Revisited: An Ablation Study and Extensions for Directed\n  Acyclic Graphs",
        "abstract": "  GraphRNN is a deep learning-based architecture proposed by You et al. for\nlearning generative models for graphs. We replicate the results of You et al.\nusing a reproduced implementation of the GraphRNN architecture and evaluate\nthis against baseline models using new metrics. Through an ablation study, we\nfind that the BFS traversal suggested by You et al. to collapse representations\nof isomorphic graphs contributes significantly to model performance.\nAdditionally, we extend GraphRNN to generate directed acyclic graphs by\nreplacing the BFS traversal with a topological sort. We demonstrate that this\nmethod improves significantly over a directed-multiclass variant of GraphRNN on\na real-world dataset.\n",
        "published": "2023",
        "authors": [
            "Taniya Das",
            "Mark Koch",
            "Maya Ravichandran",
            "Nikhil Khatri"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.03337v1",
        "title": "Solving Falkner-Skan type equations via Legendre and Chebyshev Neural\n  Blocks",
        "abstract": "  In this paper, a new deep-learning architecture for solving the non-linear\nFalkner-Skan equation is proposed. Using Legendre and Chebyshev neural blocks,\nthis approach shows how orthogonal polynomials can be used in neural networks\nto increase the approximation capability of artificial neural networks. In\naddition, utilizing the mathematical properties of these functions, we overcome\nthe computational complexity of the backpropagation algorithm by using the\noperational matrices of the derivative. The efficiency of the proposed method\nis carried out by simulating various configurations of the Falkner-Skan\nequation.\n",
        "published": "2023",
        "authors": [
            "Alireza Afzal Aghaei",
            "Kourosh Parand",
            "Ali Nikkhah",
            "Shakila Jaberi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.04469v1",
        "title": "Correlating Medi-Claim Service by Deep Learning Neural Networks",
        "abstract": "  Medical insurance claims are of organized crimes related to patients,\nphysicians, diagnostic centers, and insurance providers, forming a chain\nreaction that must be monitored constantly. These kinds of frauds affect the\nfinancial growth of both insured people and health insurance companies. The\nConvolution Neural Network architecture is used to detect fraudulent claims\nthrough a correlation study of regression models, which helps to detect money\nlaundering on different claims given by different providers. Supervised and\nunsupervised classifiers are used to detect fraud and non-fraud claims.\n",
        "published": "2023",
        "authors": [
            "Jayanthi Vajiram",
            "Negha Senthil",
            "Nean Adhith. P"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.10740v1",
        "title": "We Don't Need No Adam, All We Need Is EVE: On The Variance of Dual\n  Learning Rate And Beyond",
        "abstract": "  In the rapidly advancing field of deep learning, optimising deep neural\nnetworks is paramount. This paper introduces a novel method, Enhanced Velocity\nEstimation (EVE), which innovatively applies different learning rates to\ndistinct components of the gradients. By bifurcating the learning rate, EVE\nenables more nuanced control and faster convergence, addressing the challenges\nassociated with traditional single learning rate approaches. Utilising a\nmomentum term that adapts to the learning landscape, the method achieves a more\nefficient navigation of the complex loss surface, resulting in enhanced\nperformance and stability. Extensive experiments demonstrate that EVE\nsignificantly outperforms existing optimisation techniques across various\nbenchmark datasets and architectures.\n",
        "published": "2023",
        "authors": [
            "Afshin Khadangi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.12918v1",
        "title": "Evaluating the Vulnerabilities in ML systems in terms of adversarial\n  attacks",
        "abstract": "  There have been recent adversarial attacks that are difficult to find. These\nnew adversarial attacks methods may pose challenges to current deep learning\ncyber defense systems and could influence the future defense of cyberattacks.\nThe authors focus on this domain in this research paper. They explore the\nconsequences of vulnerabilities in AI systems. This includes discussing how\nthey might arise, differences between randomized and adversarial examples and\nalso potential ethical implications of vulnerabilities. Moreover, it is\nimportant to train the AI systems appropriately when they are in testing phase\nand getting them ready for broader use.\n",
        "published": "2023",
        "authors": [
            "John Harshith",
            "Mantej Singh Gill",
            "Madhan Jothimani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.15899v1",
        "title": "Beyond Traditional Neural Networks: Toward adding Reasoning and Learning\n  Capabilities through Computational Logic Techniques",
        "abstract": "  Deep Learning (DL) models have become popular for solving complex problems,\nbut they have limitations such as the need for high-quality training data, lack\nof transparency, and robustness issues. Neuro-Symbolic AI has emerged as a\npromising approach combining the strengths of neural networks and symbolic\nreasoning. Symbolic knowledge injection (SKI) techniques are a popular method\nto incorporate symbolic knowledge into sub-symbolic systems. This work proposes\nsolutions to improve the knowledge injection process and integrate elements of\nML and logic into multi-agent systems (MAS).\n",
        "published": "2023",
        "authors": [
            "Andrea Rafanelli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.08171v1",
        "title": "Unveiling Invariances via Neural Network Pruning",
        "abstract": "  Invariance describes transformations that do not alter data's underlying\nsemantics. Neural networks that preserve natural invariance capture good\ninductive biases and achieve superior performance. Hence, modern networks are\nhandcrafted to handle well-known invariances (ex. translations). We propose a\nframework to learn novel network architectures that capture data-dependent\ninvariances via pruning. Our learned architectures consistently outperform\ndense neural networks on both vision and tabular datasets in both efficiency\nand effectiveness. We demonstrate our framework on multiple deep learning\nmodels across 3 vision and 40 tabular datasets.\n",
        "published": "2023",
        "authors": [
            "Derek Xu",
            "Yizhou Sun",
            "Wei Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.15139v1",
        "title": "PINF: Continuous Normalizing Flows for Physics-Constrained Deep Learning",
        "abstract": "  The normalization constraint on probability density poses a significant\nchallenge for solving the Fokker-Planck equation. Normalizing Flow, an\ninvertible generative model leverages the change of variables formula to ensure\nprobability density conservation and enable the learning of complex data\ndistributions. In this paper, we introduce Physics-Informed Normalizing Flows\n(PINF), a novel extension of continuous normalizing flows, incorporating\ndiffusion through the method of characteristics. Our method, which is mesh-free\nand causality-free, can efficiently solve high dimensional time-dependent and\nsteady-state Fokker-Planck equations.\n",
        "published": "2023",
        "authors": [
            "Feng Liu",
            "Faguo Wu",
            "Xiao Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.02840v1",
        "title": "Saturn: Efficient Multi-Large-Model Deep Learning",
        "abstract": "  In this paper, we propose Saturn, a new data system to improve the efficiency\nof multi-large-model training (e.g., during model selection/hyperparameter\noptimization). We first identify three key interconnected systems challenges\nfor users building large models in this setting -- parallelism technique\nselection, distribution of GPUs over jobs, and scheduling. We then formalize\nthese as a joint problem, and build a new system architecture to tackle these\nchallenges simultaneously. Our evaluations show that our joint-optimization\napproach yields 39-49% lower model selection runtimes than typical current DL\npractice.\n",
        "published": "2023",
        "authors": [
            "Kabir Nagrecha",
            "Arun Kumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.15924v1",
        "title": "Diagnosis driven Anomaly Detection for CPS",
        "abstract": "  In Cyber-Physical Systems (CPS) research, anomaly detection (detecting\nabnormal behavior) and diagnosis (identifying the underlying root cause) are\noften treated as distinct, isolated tasks. However, diagnosis algorithms\nrequire symptoms, i.e. temporally and spatially isolated anomalies, as input.\nThus, anomaly detection and diagnosis must be developed together to provide a\nholistic solution for diagnosis in CPS. We therefore propose a method for\nutilizing deep learning-based anomaly detection to generate inputs for\nConsistency-Based Diagnosis (CBD). We evaluate our approach on a simulated and\na real-world CPS dataset, where our model demonstrates strong performance\nrelative to other state-of-the-art models.\n",
        "published": "2023",
        "authors": [
            "Henrik S. Steude",
            "Lukas Moddemann",
            "Alexander Diedrich",
            "Jonas Ehrhardt",
            "Oliver Niggemann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.06522v1",
        "title": "Label Smoothing for Enhanced Text Sentiment Classification",
        "abstract": "  Label smoothing is a widely used technique in various domains, such as image\nclassification and speech recognition, known for effectively combating model\noverfitting. However, there is few research on its application to text\nsentiment classification. To fill in the gap, this study investigates the\nimplementation of label smoothing for sentiment classification by utilizing\ndifferent levels of smoothing. The primary objective is to enhance sentiment\nclassification accuracy by transforming discrete labels into smoothed label\ndistributions. Through extensive experiments, we demonstrate the superior\nperformance of label smoothing in text sentiment classification tasks across\neight diverse datasets and deep learning architectures: TextCNN, BERT, and\nRoBERTa, under two learning schemes: training from scratch and fine-tuning.\n",
        "published": "2023",
        "authors": [
            "Yijie Gao",
            "Shijing Si"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.11561v1",
        "title": "COPD-FlowNet: Elevating Non-invasive COPD Diagnosis with CFD Simulations",
        "abstract": "  Chronic Obstructive Pulmonary Disorder (COPD) is a prevalent respiratory\ndisease that significantly impacts the quality of life of affected individuals.\nThis paper presents COPDFlowNet, a novel deep-learning framework that leverages\na custom Generative Adversarial Network (GAN) to generate synthetic\nComputational Fluid Dynamics (CFD) velocity flow field images specific to the\ntrachea of COPD patients. These synthetic images serve as a valuable resource\nfor data augmentation and model training. Additionally, COPDFlowNet\nincorporates a custom Convolutional Neural Network (CNN) architecture to\npredict the location of the obstruction site.\n",
        "published": "2023",
        "authors": [
            "Aryan Tyagi",
            "Aryaman Rao",
            "Shubhanshu Rao",
            "Raj Kumar Singh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.04799v3",
        "title": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning",
        "abstract": "  There is an increasing need to bring machine learning to a wide diversity of\nhardware devices. Current frameworks rely on vendor-specific operator libraries\nand optimize for a narrow range of server-class GPUs. Deploying workloads to\nnew platforms -- such as mobile phones, embedded devices, and accelerators\n(e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a\ncompiler that exposes graph-level and operator-level optimizations to provide\nperformance portability to deep learning workloads across diverse hardware\nback-ends. TVM solves optimization challenges specific to deep learning, such\nas high-level operator fusion, mapping to arbitrary hardware primitives, and\nmemory latency hiding. It also automates optimization of low-level programs to\nhardware characteristics by employing a novel, learning-based cost modeling\nmethod for rapid exploration of code optimizations. Experimental results show\nthat TVM delivers performance across hardware back-ends that are competitive\nwith state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and\nserver-class GPUs. We also demonstrate TVM's ability to target new accelerator\nback-ends, such as the FPGA-based generic deep learning accelerator. The system\nis open sourced and in production use inside several major companies.\n",
        "published": "2018",
        "authors": [
            "Tianqi Chen",
            "Thierry Moreau",
            "Ziheng Jiang",
            "Lianmin Zheng",
            "Eddie Yan",
            "Meghan Cowan",
            "Haichen Shen",
            "Leyuan Wang",
            "Yuwei Hu",
            "Luis Ceze",
            "Carlos Guestrin",
            "Arvind Krishnamurthy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.02942v3",
        "title": "SupportNet: solving catastrophic forgetting in class incremental\n  learning with support data",
        "abstract": "  A plain well-trained deep learning model often does not have the ability to\nlearn new knowledge without forgetting the previously learned knowledge, which\nis known as catastrophic forgetting. Here we propose a novel method,\nSupportNet, to efficiently and effectively solve the catastrophic forgetting\nproblem in the class incremental learning scenario. SupportNet combines the\nstrength of deep learning and support vector machine (SVM), where SVM is used\nto identify the support data from the old data, which are fed to the deep\nlearning model together with the new data for further training so that the\nmodel can review the essential information of the old data when learning the\nnew information. Two powerful consolidation regularizers are applied to\nstabilize the learned representation and ensure the robustness of the learned\nmodel. We validate our method with comprehensive experiments on various tasks,\nwhich show that SupportNet drastically outperforms the state-of-the-art\nincremental learning methods and even reaches similar performance as the deep\nlearning model trained from scratch on both old and new data. Our program is\naccessible at: https://github.com/lykaust15/SupportNet\n",
        "published": "2018",
        "authors": [
            "Yu Li",
            "Zhongxiao Li",
            "Lizhong Ding",
            "Yijie Pan",
            "Chao Huang",
            "Yuhui Hu",
            "Wei Chen",
            "Xin Gao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.09060v1",
        "title": "Deep Confidence: A Computationally Efficient Framework for Calculating\n  Reliable Errors for Deep Neural Networks",
        "abstract": "  Deep learning architectures have proved versatile in a number of drug\ndiscovery applications, including the modelling of in vitro compound activity.\nWhile controlling for prediction confidence is essential to increase the trust,\ninterpretability and usefulness of virtual screening models in drug discovery,\ntechniques to estimate the reliability of the predictions generated with deep\nlearning networks remain largely underexplored. Here, we present Deep\nConfidence, a framework to compute valid and efficient confidence intervals for\nindividual predictions using the deep learning technique Snapshot Ensembling\nand conformal prediction. Specifically, Deep Confidence generates an ensemble\nof deep neural networks by recording the network parameters throughout the\nlocal minima visited during the optimization phase of a single neural network.\nThis approach serves to derive a set of base learners (i.e., snapshots) with\ncomparable predictive power on average, that will however generate slightly\ndifferent predictions for a given instance. The variability across base\nlearners and the validation residuals are in turn harnessed to compute\nconfidence intervals using the conformal prediction framework. Using a set of\n24 diverse IC50 data sets from ChEMBL 23, we show that Snapshot Ensembles\nperform on par with Random Forest (RF) and ensembles of independently trained\ndeep neural networks. In addition, we find that the confidence regions\npredicted using the Deep Confidence framework span a narrower set of values.\nOverall, Deep Confidence represents a highly versatile error prediction\nframework that can be applied to any deep learning-based application at no\nextra computational cost.\n",
        "published": "2018",
        "authors": [
            "Isidro Cortes-Ciriano",
            "Andreas Bender"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.06988v2",
        "title": "Deep learning generates custom-made logistic regression models for\n  explaining how breast cancer subtypes are classified",
        "abstract": "  Differentiating the intrinsic subtypes of breast cancer is crucial for\ndeciding the best treatment strategy. Deep learning can predict the subtypes\nfrom genetic information more accurately than conventional statistical methods,\nbut to date, deep learning has not been directly utilized to examine which\ngenes are associated with which subtypes. To clarify the mechanisms embedded in\nthe intrinsic subtypes, we developed an explainable deep learning model called\na point-wise linear (PWL) model that generates a custom-made logistic\nregression for each patient. Logistic regression, which is familiar to both\nphysicians and medical informatics researchers, allows us to analyze the\nimportance of the feature variables, and the PWL model harnesses these\npractical abilities of logistic regression. In this study, we show that\nanalyzing breast cancer subtypes is clinically beneficial for patients and one\nof the best ways to validate the capability of the PWL model. First, we trained\nthe PWL model with RNA-seq data to predict PAM50 intrinsic subtypes and applied\nit to the 41/50 genes of PAM50 through the subtype prediction task. Second, we\ndeveloped a deep enrichment analysis method to reveal the relationships between\nthe PAM50 subtypes and the copy numbers of breast cancer. Our findings showed\nthat the PWL model utilized genes relevant to the cell cycle-related pathways.\nThese preliminary successes in breast cancer subtype analysis demonstrate the\npotential of our analysis strategy to clarify the mechanisms underlying breast\ncancer and improve overall clinical outcomes.\n",
        "published": "2020",
        "authors": [
            "Takuma Shibahara",
            "Chisa Wada",
            "Yasuho Yamashita",
            "Kazuhiro Fujita",
            "Masamichi Sato",
            "Junichi Kuwata",
            "Atsushi Okamoto",
            "Yoshimasa Ono"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.01478v1",
        "title": "What do AI algorithms actually learn? - On false structures in deep\n  learning",
        "abstract": "  There are two big unsolved mathematical questions in artificial intelligence\n(AI): (1) Why is deep learning so successful in classification problems and (2)\nwhy are neural nets based on deep learning at the same time universally\nunstable, where the instabilities make the networks vulnerable to adversarial\nattacks. We present a solution to these questions that can be summed up in two\nwords; false structures. Indeed, deep learning does not learn the original\nstructures that humans use when recognising images (cats have whiskers, paws,\nfur, pointy ears, etc), but rather different false structures that correlate\nwith the original structure and hence yield the success. However, the false\nstructure, unlike the original structure, is unstable. The false structure is\nsimpler than the original structure, hence easier to learn with less data and\nthe numerical algorithm used in the training will more easily converge to the\nneural network that captures the false structure. We formally define the\nconcept of false structures and formulate the solution as a conjecture. Given\nthat trained neural networks always are computed with approximations, this\nconjecture can only be established through a combination of theoretical and\ncomputational results similar to how one establishes a postulate in theoretical\nphysics (e.g. the speed of light is constant). Establishing the conjecture\nfully will require a vast research program characterising the false structures.\nWe provide the foundations for such a program establishing the existence of the\nfalse structures in practice. Finally, we discuss the far reaching consequences\nthe existence of the false structures has on state-of-the-art AI and Smale's\n18th problem.\n",
        "published": "2019",
        "authors": [
            "Laura Thesing",
            "Vegard Antun",
            "Anders C. Hansen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.02637v1",
        "title": "Big Data Analytics Applying the Fusion Approach of Multicriteria\n  Decision Making with Deep Learning Algorithms",
        "abstract": "  Data is evolving with the rapid progress of population and communication for\nvarious types of devices such as networks, cloud computing, Internet of Things\n(IoT), actuators, and sensors. The increment of data and communication content\ngoes with the equivalence of velocity, speed, size, and value to provide the\nuseful and meaningful knowledge that helps to solve the future challenging\ntasks and latest issues. Besides, multicriteria based decision making is one of\nthe key issues to solve for various issues related to the alternative effects\nin big data analysis. It tends to find a solution based on the latest machine\nlearning techniques that include algorithms like decision making and deep\nlearning mechanism based on multicriteria in providing insights to big data. On\nthe other hand, the derivations are made for it to go with the approximations\nto increase the duality of runtime and improve the entire system's potentiality\nand efficacy. In essence, several fields, including business, agriculture,\ninformation technology, and computer science, use deep learning and\nmulticriteria-based decision-making problems. This paper aims to provide\nvarious applications that involve the concepts of deep learning techniques and\nexploiting the multicriteria approaches for issues that are facing in big data\nanalytics by proposing new studies with the fusion approaches of data-driven\ntechniques.\n",
        "published": "2021",
        "authors": [
            "Swarajya Lakshmi V Papineni",
            "Snigdha Yarlagadda",
            "Harita Akkineni",
            "A. Mallikarjuna Reddy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.10745v1",
        "title": "Classification of Quasars, Galaxies, and Stars in the Mapping of the\n  Universe Multi-modal Deep Learning",
        "abstract": "  In this paper, the fourth version the Sloan Digital Sky Survey (SDSS-4), Data\nRelease 16 dataset was used to classify the SDSS dataset into galaxies, stars,\nand quasars using machine learning and deep learning architectures. We\nefficiently utilize both image and metadata in tabular format to build a novel\nmulti-modal architecture and achieve state-of-the-art results. In addition, our\nexperiments on transfer learning using Imagenet weights on five different\narchitectures (Resnet-50, DenseNet-121 VGG-16, Xception, and EfficientNet)\nreveal that freezing all layers and adding a final trainable layer may not be\nan optimal solution for transfer learning. It is hypothesized that higher the\nnumber of trainable layers, higher will be the training time and accuracy of\npredictions. It is also hypothesized that any subsequent increase in the number\nof training layers towards the base layers will not increase in accuracy as the\npre trained lower layers only help in low level feature extraction which would\nbe quite similar in all the datasets. Hence the ideal level of trainable layers\nneeds to be identified for each model in respect to the number of parameters.\nFor the tabular data, we compared classical machine learning algorithms\n(Logistic Regression, Random Forest, Decision Trees, Adaboost, LightGBM etc.,)\nwith artificial neural networks. Our works shed new light on transfer learning\nand multi-modal deep learning architectures. The multi-modal architecture not\nonly resulted in higher metrics (accuracy, precision, recall, F1 score) than\nmodels using only image data or tabular data. Furthermore, multi-modal\narchitecture achieved the best metrics in lesser training epochs and improved\nthe metrics on all classes.\n",
        "published": "2022",
        "authors": [
            "Sabeesh Ethiraj",
            "Bharath Kumar Bolla"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2211.00534v3",
        "title": "Deep Learning for Global Wildfire Forecasting",
        "abstract": "  Climate change is expected to aggravate wildfire activity through the\nexacerbation of fire weather. Improving our capabilities to anticipate\nwildfires on a global scale is of uttermost importance for mitigating their\nnegative effects. In this work, we create a global fire dataset and demonstrate\na prototype for predicting the presence of global burned areas on a\nsub-seasonal scale with the use of segmentation deep learning models.\nParticularly, we present an open-access global analysis-ready datacube, which\ncontains a variety of variables related to the seasonal and sub-seasonal fire\ndrivers (climate, vegetation, oceanic indices, human-related variables), as\nwell as the historical burned areas and wildfire emissions for 2001-2021. We\ntrain a deep learning model, which treats global wildfire forecasting as an\nimage segmentation task and skillfully predicts the presence of burned areas 8,\n16, 32 and 64 days ahead of time. Our work motivates the use of deep learning\nfor global burned area forecasting and paves the way towards improved\nanticipation of global wildfire patterns.\n",
        "published": "2022",
        "authors": [
            "Ioannis Prapas",
            "Akanksha Ahuja",
            "Spyros Kondylatos",
            "Ilektra Karasante",
            "Eleanna Panagiotou",
            "Lazaro Alonso",
            "Charalampos Davalas",
            "Dimitrios Michail",
            "Nuno Carvalhais",
            "Ioannis Papoutsis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.00966v1",
        "title": "A Hybrid Deep Learning Anomaly Detection Framework for Intrusion\n  Detection",
        "abstract": "  Cyber intrusion attacks that compromise the users' critical and sensitive\ndata are escalating in volume and intensity, especially with the growing\nconnections between our daily life and the Internet. The large volume and high\ncomplexity of such intrusion attacks have impeded the effectiveness of most\ntraditional defence techniques. While at the same time, the remarkable\nperformance of the machine learning methods, especially deep learning, in\ncomputer vision, had garnered research interests from the cyber security\ncommunity to further enhance and automate intrusion detections. However, the\nexpensive data labeling and limitation of anomalous data make it challenging to\ntrain an intrusion detector in a fully supervised manner. Therefore, intrusion\ndetection based on unsupervised anomaly detection is an important feature too.\nIn this paper, we propose a three-stage deep learning anomaly detection based\nnetwork intrusion attack detection framework. The framework comprises an\nintegration of unsupervised (K-means clustering), semi-supervised (GANomaly)\nand supervised learning (CNN) algorithms. We then evaluated and showed the\nperformance of our implemented framework on three benchmark datasets: NSL-KDD,\nCIC-IDS2018, and TON_IoT.\n",
        "published": "2022",
        "authors": [
            "Rahul Kale",
            "Zhi Lu",
            "Kar Wai Fok",
            "Vrizlynn L. L. Thing"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2305.03063v2",
        "title": "Neuro-symbolic model for cantilever beams damage detection",
        "abstract": "  In the last decade, damage detection approaches swiftly changed from advanced\nsignal processing methods to machine learning and especially deep learning\nmodels, to accurately and non-intrusively estimate the state of the beam\nstructures. But as the deep learning models reached their peak performances,\nalso their limitations in applicability and vulnerabilities were observed. One\nof the most important reason for the lack of trustworthiness in operational\nconditions is the absence of intrinsic explainability of the deep learning\nsystem, due to the encoding of the knowledge in tensor values and without the\ninclusion of logical constraints. In this paper, we propose a neuro-symbolic\nmodel for the detection of damages in cantilever beams based on a novel\ncognitive architecture in which we join the processing power of convolutional\nnetworks with the interactive control offered by queries realized through the\ninclusion of real logic directly into the model. The hybrid discriminative\nmodel is introduced under the name Logic Convolutional Neural Regressor and it\nis tested on a dataset of values of the relative natural frequency shifts of\ncantilever beams derived from an original mathematical relation. While the\nobtained results preserve all the predictive capabilities of deep learning\nmodels, the usage of three distances as predicates for satisfiability, makes\nthe system more trustworthy and scalable for practical applications. Extensive\nnumerical and laboratory experiments were performed, and they all demonstrated\nthe superiority of the hybrid approach, which can open a new path for solving\nthe damage detection problem.\n",
        "published": "2023",
        "authors": [
            "Darian Onchis",
            "Gilbert-Rainer Gillich",
            "Eduard Hogea",
            "Cristian Tufisi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.01593v1",
        "title": "Deep Learning Overloaded Vehicle Identification for Long Span Bridges\n  Based on Structural Health Monitoring Data",
        "abstract": "  Overloaded vehicles bring great harm to transportation infrastructures. BWIM\n(bridge weigh-in-motion) method for overloaded vehicle identification is\ngetting more popular because it can be implemented without interruption to the\ntraffic. However, its application is still limited because its effectiveness\nlargely depends on professional knowledge and extra information, and is\nsusceptible to occurrence of multiple vehicles. In this paper, a deep learning\nbased overloaded vehicle identification approach (DOVI) is proposed, with the\npurpose of overloaded vehicle identification for long-span bridges by the use\nof structural health monitoring data. The proposed DOVI model uses temporal\nconvolutional architectures to extract the spatial and temporal features of the\ninput sequence data, thus provides an end-to-end overloaded vehicle\nidentification solution which neither needs the influence line nor needs to\nobtain velocity and wheelbase information in advance and can be applied under\nthe occurrence of multiple vehicles. Model evaluations are conducted on a\nsimply supported beam and a long-span cable-stayed bridge under random traffic\nflow. Results demonstrate that the proposed deep-learning overloaded vehicle\nidentification approach has better effectiveness and robustness, compared with\nother machine learning and deep learning approaches.\n",
        "published": "2023",
        "authors": [
            "Yuqin Li",
            "Jun Liu",
            "Shengliang Zhong",
            "Licheng Zhou",
            "Shoubin Dong",
            "Zejia Liu",
            "Liqun Tang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.07899v2",
        "title": "Morphological Profiling for Drug Discovery in the Era of Deep Learning",
        "abstract": "  Morphological profiling is a valuable tool in phenotypic drug discovery. The\nadvent of high-throughput automated imaging has enabled the capturing of a wide\nrange of morphological features of cells or organisms in response to\nperturbations at the single-cell resolution. Concurrently, significant advances\nin machine learning and deep learning, especially in computer vision, have led\nto substantial improvements in analyzing large-scale high-content images at\nhigh-throughput. These efforts have facilitated understanding of compound\nmechanism-of-action (MOA), drug repurposing, characterization of cell\nmorphodynamics under perturbation, and ultimately contributing to the\ndevelopment of novel therapeutics. In this review, we provide a comprehensive\noverview of the recent advances in the field of morphological profiling. We\nsummarize the image profiling analysis workflow, survey a broad spectrum of\nanalysis strategies encompassing feature engineering- and deep learning-based\napproaches, and introduce publicly available benchmark datasets. We place a\nparticular emphasis on the application of deep learning in this pipeline,\ncovering cell segmentation, image representation learning, and multimodal\nlearning. Additionally, we illuminate the application of morphological\nprofiling in phenotypic drug discovery and highlight potential challenges and\nopportunities in this field.\n",
        "published": "2023",
        "authors": [
            "Qiaosi Tang",
            "Ranjala Ratnayake",
            "Gustavo Seabra",
            "Zhe Jiang",
            "Ruogu Fang",
            "Lina Cui",
            "Yousong Ding",
            "Tamer Kahveci",
            "Jiang Bian",
            "Chenglong Li",
            "Hendrik Luesch",
            "Yanjun Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2012.04115v2",
        "title": "Generalization bounds for deep learning",
        "abstract": "  Generalization in deep learning has been the topic of much recent theoretical\nand empirical research. Here we introduce desiderata for techniques that\npredict generalization errors for deep learning models in supervised learning.\nSuch predictions should 1) scale correctly with data complexity; 2) scale\ncorrectly with training set size; 3) capture differences between architectures;\n4) capture differences between optimization algorithms; 5) be quantitatively\nnot too far from the true error (in particular, be non-vacuous); 6) be\nefficiently computable; and 7) be rigorous. We focus on generalization error\nupper bounds, and introduce a categorisation of bounds depending on assumptions\non the algorithm and data. We review a wide range of existing approaches, from\nclassical VC dimension to recent PAC-Bayesian bounds, commenting on how well\nthey perform against the desiderata.\n  We next use a function-based picture to derive a marginal-likelihood\nPAC-Bayesian bound. This bound is, by one definition, optimal up to a\nmultiplicative constant in the asymptotic limit of large training sets, as long\nas the learning curve follows a power law, which is typically found in practice\nfor deep learning problems. Extensive empirical analysis demonstrates that our\nmarginal-likelihood PAC-Bayes bound fulfills desiderata 1-3 and 5. The results\nfor 6 and 7 are promising, but not yet fully conclusive, while only desideratum\n4 is currently beyond the scope of our bound. Finally, we comment on why this\nfunction-based bound performs significantly better than current parameter-based\nPAC-Bayes bounds.\n",
        "published": "2020",
        "authors": [
            "Guillermo Valle-P\u00e9rez",
            "Ard A. Louis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2204.03503v1",
        "title": "Survey on Automated Short Answer Grading with Deep Learning: from Word\n  Embeddings to Transformers",
        "abstract": "  Automated short answer grading (ASAG) has gained attention in education as a\nmeans to scale educational tasks to the growing number of students. Recent\nprogress in Natural Language Processing and Machine Learning has largely\ninfluenced the field of ASAG, of which we survey the recent research\nadvancements. We complement previous surveys by providing a comprehensive\nanalysis of recently published methods that deploy deep learning approaches. In\nparticular, we focus our analysis on the transition from hand engineered\nfeatures to representation learning approaches, which learn representative\nfeatures for the task at hand automatically from large corpora of data. We\nstructure our analysis of deep learning methods along three categories: word\nembeddings, sequential models, and attention-based methods. Deep learning\nimpacted ASAG differently than other fields of NLP, as we noticed that the\nlearned representations alone do not contribute to achieve the best results,\nbut they rather show to work in a complementary way with hand-engineered\nfeatures. The best performance are indeed achieved by methods that combine the\ncarefully hand-engineered features with the power of the semantic descriptions\nprovided by the latest models, like transformers architectures. We identify\nchallenges and provide an outlook on research direction that can be addressed\nin the future\n",
        "published": "2022",
        "authors": [
            "Stefan Haller",
            "Adina Aldea",
            "Christin Seifert",
            "Nicola Strisciuglio"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.13392v1",
        "title": "Remote Sensing Image Classification using Transfer Learning and\n  Attention Based Deep Neural Network",
        "abstract": "  The task of remote sensing image scene classification (RSISC), which aims at\nclassifying remote sensing images into groups of semantic categories based on\ntheir contents, has taken the important role in a wide range of applications\nsuch as urban planning, natural hazards detection, environment\nmonitoring,vegetation mapping, or geospatial object detection. During the past\nyears, research community focusing on RSISC task has shown significant effort\nto publish diverse datasets as well as propose different approaches to deal\nwith the RSISC challenges. Recently, almost proposed RSISC systems base on deep\nlearning models which prove powerful and outperform traditional approaches\nusing image processing and machine learning. In this paper, we also leverage\nthe power of deep learning technology, evaluate a variety of deep neural\nnetwork architectures, indicate main factors affecting the performance of a\nRSISC system. Given the comprehensive analysis, we propose a deep learning\nbased framework for RSISC, which makes use of the transfer learning technique\nand multihead attention scheme. The proposed deep learning framework is\nevaluated on the benchmark NWPU-RESISC45 dataset and achieves the best\nclassification accuracy of 94.7% which shows competitive to the\nstate-of-the-art systems and potential for real-life applications.\n",
        "published": "2022",
        "authors": [
            "Lam Pham",
            "Khoa Tran",
            "Dat Ngo",
            "Jasmin Lampert",
            "Alexander Schindler"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.13197v1",
        "title": "Knowledge Tracing with Sequential Key-Value Memory Networks",
        "abstract": "  Can machines trace human knowledge like humans? Knowledge tracing (KT) is a\nfundamental task in a wide range of applications in education, such as massive\nopen online courses (MOOCs), intelligent tutoring systems, educational games,\nand learning management systems. It models dynamics in a student's knowledge\nstates in relation to different learning concepts through their interactions\nwith learning activities. Recently, several attempts have been made to use deep\nlearning models for tackling the KT problem. Although these deep learning\nmodels have shown promising results, they have limitations: either lack the\nability to go deeper to trace how specific concepts in a knowledge state are\nmastered by a student, or fail to capture long-term dependencies in an exercise\nsequence. In this paper, we address these limitations by proposing a novel deep\nlearning model for knowledge tracing, namely Sequential Key-Value Memory\nNetworks (SKVMN). This model unifies the strengths of recurrent modelling\ncapacity and memory capacity of the existing deep learning KT models for\nmodelling student learning. We have extensively evaluated our proposed model on\nfive benchmark datasets. The experimental results show that (1) SKVMN\noutperforms the state-of-the-art KT models on all datasets, (2) SKVMN can\nbetter discover the correlation between latent concepts and questions, and (3)\nSKVMN can trace the knowledge state of students dynamics, and a leverage\nsequential dependencies in an exercise sequence for improved predication\naccuracy.\n",
        "published": "2019",
        "authors": [
            "Ghodai Abdelrahman",
            "Qing Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.04766v1",
        "title": "A Heuristically Self-Organised Linguistic Attribute Deep Learning in\n  Edge Computing For IoT Intelligence",
        "abstract": "  With the development of Internet of Things (IoT), IoT intelligence becomes\nemerging technology. \"Curse of Dimensionality\" is the barrier of data fusion in\nedge devices for the success of IoT intelligence. A Linguistic Attribute\nHierarchy (LAH), embedded with Linguistic Decision Trees (LDTs), can represent\na new attribute deep learning. In contrast to the conventional deep learning,\nan LAH could overcome the shortcoming of missing interpretation by providing\ntransparent information propagation through the rules, produced by LDTs in the\nLAH. Similar to the conventional deep learning, the computing complexity of\noptimising LAHs blocks the applications of LAHs. In this paper, we propose a\nheuristic approach to constructing an LAH, embedded with LDTs for decision\nmaking or classification by utilising the distance correlations between\nattributes and between attributes and the goal variable. The set of attributes\nis divided to some attribute clusters, and then they are heuristically\norganised to form a linguistic attribute hierarchy. The proposed approach was\nvalidated with some benchmark decision making or classification problems from\nthe UCI machine learning repository. The experimental results show that the\nproposed self-organisation algorithm can construct an effective and efficient\nlinguistic attribute hierarchy. Such a self-organised linguistic attribute\nhierarchy embedded with LDTs can not only efficiently tackle \"curse of\ndimensionality\" in a single LDT for data fusion with massive attributes, but\nalso achieve better or comparable performance on decision making or\nclassification, compared to the single LDT for the problem to be solved. The\nself-organisation algorithm is much efficient than the Genetic Algorithm in\nWrapper for the optimisation of LAHs. This makes it feasible to embed the\nself-organisation algorithm in edge devices for IoT intelligence.\n",
        "published": "2020",
        "authors": [
            "Hongmei He",
            "Zhenhuan Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.12149v1",
        "title": "SATNet: Bridging deep learning and logical reasoning using a\n  differentiable satisfiability solver",
        "abstract": "  Integrating logical reasoning within deep learning architectures has been a\nmajor goal of modern AI systems. In this paper, we propose a new direction\ntoward this goal by introducing a differentiable (smoothed) maximum\nsatisfiability (MAXSAT) solver that can be integrated into the loop of larger\ndeep learning systems. Our (approximate) solver is based upon a fast coordinate\ndescent approach to solving the semidefinite program (SDP) associated with the\nMAXSAT problem. We show how to analytically differentiate through the solution\nto this SDP and efficiently solve the associated backward pass. We demonstrate\nthat by integrating this solver into end-to-end learning systems, we can learn\nthe logical structure of challenging problems in a minimally supervised\nfashion. In particular, we show that we can learn the parity function using\nsingle-bit supervision (a traditionally hard task for deep networks) and learn\nhow to play 9x9 Sudoku solely from examples. We also solve a \"visual Sudok\"\nproblem that maps images of Sudoku puzzles to their associated logical\nsolutions by combining our MAXSAT solver with a traditional convolutional\narchitecture. Our approach thus shows promise in integrating logical structures\nwithin deep learning.\n",
        "published": "2019",
        "authors": [
            "Po-Wei Wang",
            "Priya L. Donti",
            "Bryan Wilder",
            "Zico Kolter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.03230v2",
        "title": "Hierarchical Predictive Coding Models in a Deep-Learning Framework",
        "abstract": "  Bayesian predictive coding is a putative neuromorphic method for acquiring\nhigher-level neural representations to account for sensory input. Although\noriginating in the neuroscience community, there are also efforts in the\nmachine learning community to study these models. This paper reviews some of\nthe more well known models. Our review analyzes module connectivity and\npatterns of information transfer, seeking to find general principles used\nacross the models. We also survey some recent attempts to cast these models\nwithin a deep learning framework. A defining feature of Bayesian predictive\ncoding is that it uses top-down, reconstructive mechanisms to predict incoming\nsensory inputs or their lower-level representations. Discrepancies between the\npredicted and the actual inputs, known as prediction errors, then give rise to\nfuture learning that refines and improves the predictive accuracy of learned\nhigher-level representations. Predictive coding models intended to describe\ncomputations in the neocortex emerged prior to the development of deep learning\nand used a communication structure between modules that we name the Rao-Ballard\nprotocol. This protocol was derived from a Bayesian generative model with some\nrather strong statistical assumptions. The RB protocol provides a rubric to\nassess the fidelity of deep learning models that claim to implement predictive\ncoding.\n",
        "published": "2020",
        "authors": [
            "Matin Hosseini",
            "Anthony Maida"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.03002v1",
        "title": "Zero Training Overhead Portfolios for Learning to Solve Combinatorial\n  Problems",
        "abstract": "  There has been an increasing interest in harnessing deep learning to tackle\ncombinatorial optimization (CO) problems in recent years. Typical CO deep\nlearning approaches leverage the problem structure in the model architecture.\nNevertheless, the model selection is still mainly based on the conventional\nmachine learning setting. Due to the discrete nature of CO problems, a single\nmodel is unlikely to learn the problem entirely. We introduce ZTop, which\nstands for Zero Training Overhead Portfolio, a simple yet effective model\nselection and ensemble mechanism for learning to solve combinatorial problems.\nZTop is inspired by algorithm portfolios, a popular CO ensembling strategy,\nparticularly restart portfolios, which periodically restart a randomized CO\nalgorithm, de facto exploring the search space with different heuristics. We\nhave observed that well-trained models acquired in the same training\ntrajectory, with similar top validation performance, perform well on very\ndifferent validation instances. Following this observation, ZTop ensembles a\nset of well-trained models, each providing a unique heuristic with zero\ntraining overhead, and applies them, sequentially or in parallel, to solve the\ntest instances. We show how ZTopping, i.e., using a ZTop ensemble strategy with\na given deep learning approach, can significantly improve the performance of\nthe current state-of-the-art deep learning approaches on three prototypical CO\ndomains, the hardest unique-solution Sudoku instances, challenging routing\nproblems, and the graph maximum cut problem, as well as on multi-label\nclassification, a machine learning task with a large combinatorial label space.\n",
        "published": "2021",
        "authors": [
            "Yiwei Bai",
            "Wenting Zhao",
            "Carla P. Gomes"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2206.10034v2",
        "title": "Deep Learning Models on CPUs: A Methodology for Efficient Training",
        "abstract": "  GPUs have been favored for training deep learning models due to their highly\nparallelized architecture. As a result, most studies on training optimization\nfocus on GPUs. There is often a trade-off, however, between cost and efficiency\nwhen deciding on how to choose the proper hardware for training. In particular,\nCPU servers can be beneficial if training on CPUs was more efficient, as they\nincur fewer hardware update costs and better utilizing existing infrastructure.\nThis paper makes several contributions to research on training deep learning\nmodels using CPUs. First, it presents a method for optimizing the training of\ndeep learning models on Intel CPUs and a toolkit called ProfileDNN, which we\ndeveloped to improve performance profiling. Second, we describe a generic\ntraining optimization method that guides our workflow and explores several case\nstudies where we identified performance issues and then optimized the Intel\nExtension for PyTorch, resulting in an overall 2x training performance increase\nfor the RetinaNet-ResNext50 model. Third, we show how to leverage the\nvisualization capabilities of ProfileDNN, which enabled us to pinpoint\nbottlenecks and create a custom focal loss kernel that was two times faster\nthan the official reference PyTorch implementation.\n",
        "published": "2022",
        "authors": [
            "Quchen Fu",
            "Ramesh Chukka",
            "Keith Achorn",
            "Thomas Atta-fosu",
            "Deepak R. Canchi",
            "Zhongwei Teng",
            "Jules White",
            "Douglas C. Schmidt"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2210.11049v2",
        "title": "How Does a Deep Learning Model Architecture Impact Its Privacy? A\n  Comprehensive Study of Privacy Attacks on CNNs and Transformers",
        "abstract": "  As a booming research area in the past decade, deep learning technologies\nhave been driven by big data collected and processed on an unprecedented scale.\nHowever, privacy concerns arise due to the potential leakage of sensitive\ninformation from the training data. Recent research has revealed that deep\nlearning models are vulnerable to various privacy attacks, including membership\ninference attacks, attribute inference attacks, and gradient inversion attacks.\nNotably, the efficacy of these attacks varies from model to model. In this\npaper, we answer a fundamental question: Does model architecture affect model\nprivacy? By investigating representative model architectures from CNNs to\nTransformers, we demonstrate that Transformers generally exhibit higher\nvulnerability to privacy attacks compared to CNNs. Additionally, We identify\nthe micro design of activation layers, stem layers, and LN layers, as major\nfactors contributing to the resilience of CNNs against privacy attacks, while\nthe presence of attention modules is another main factor that exacerbates the\nprivacy vulnerability of Transformers. Our discovery reveals valuable insights\nfor deep learning models to defend against privacy attacks and inspires the\nresearch community to develop privacy-friendly model architectures.\n",
        "published": "2022",
        "authors": [
            "Guangsheng Zhang",
            "Bo Liu",
            "Huan Tian",
            "Tianqing Zhu",
            "Ming Ding",
            "Wanlei Zhou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2304.09406v1",
        "title": "How to Do Things with Deep Learning Code",
        "abstract": "  The premise of this article is that a basic understanding of the composition\nand functioning of large language models is critically urgent. To that end, we\nextract a representational map of OpenAI's GPT-2 with what we articulate as two\nclasses of deep learning code, that which pertains to the model and that which\nunderwrites applications built around the model. We then verify this map\nthrough case studies of two popular GPT-2 applications: the text adventure\ngame, AI Dungeon, and the language art project, This Word Does Not Exist. Such\nan exercise allows us to test the potential of Critical Code Studies when the\nobject of study is deep learning code and to demonstrate the validity of code\nas an analytical focus for researchers in the subfields of Critical Artificial\nIntelligence and Critical Machine Learning Studies. More broadly, however, our\nwork draws attention to the means by which ordinary users might interact with,\nand even direct, the behavior of deep learning systems, and by extension works\ntoward demystifying some of the auratic mystery of \"AI.\" What is at stake is\nthe possibility of achieving an informed sociotechnical consensus about the\nresponsible applications of large language models, as well as a more expansive\nsense of their creative capabilities-indeed, understanding how and where\nengagement occurs allows all of us to become more active participants in the\ndevelopment of machine learning systems.\n",
        "published": "2023",
        "authors": [
            "Minh Hua",
            "Rita Raley"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2308.07052v1",
        "title": "Diagnosis of Scalp Disorders using Machine Learning and Deep Learning\n  Approach -- A Review",
        "abstract": "  The morbidity of scalp diseases is minuscule compared to other diseases, but\nthe impact on the patient's life is enormous. It is common for people to\nexperience scalp problems that include Dandruff, Psoriasis, Tinea-Capitis,\nAlopecia and Atopic-Dermatitis. In accordance with WHO research, approximately\n70% of adults have problems with their scalp. It has been demonstrated in\ndescriptive research that hair quality is impaired by impaired scalp, but these\nimpacts are reversible with early diagnosis and treatment. Deep Learning\nadvances have demonstrated the effectiveness of CNN paired with FCN in\ndiagnosing scalp and skin disorders. In one proposed Deep-Learning-based scalp\ninspection and diagnosis system, an imaging microscope and a trained model are\ncombined with an app that classifies scalp disorders accurately with an average\nprecision of 97.41%- 99.09%. Another research dealt with classifying the\nPsoriasis using the CNN with an accuracy of 82.9%. As part of another study, an\nML based algorithm was also employed. It accurately classified the healthy\nscalp and alopecia areata with 91.4% and 88.9% accuracy with SVM and KNN\nalgorithms. Using deep learning models to diagnose scalp related diseases has\nimproved due to advancements i computation capabilities and computer vision,\nbut there remains a wide horizon for further improvements.\n",
        "published": "2023",
        "authors": [
            "Hrishabh Tiwari",
            "Jatin Moolchandani",
            "Shamla Mantri"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.12632v2",
        "title": "Towards a Deep Learning-based Online Quality Prediction System for\n  Welding Processes",
        "abstract": "  The digitization of manufacturing processes enables promising applications\nfor machine learning-assisted quality assurance. A widely used manufacturing\nprocess that can strongly benefit from data-driven solutions is gas metal arc\nwelding (GMAW). The welding process is characterized by complex cause-effect\nrelationships between material properties, process conditions and weld quality.\nIn non-laboratory environments with frequently changing process parameters,\naccurate determination of weld quality by destructive testing is economically\nunfeasible. Deep learning offers the potential to identify the relationships in\navailable process data and predict the weld quality from process observations.\nIn this paper, we present a concept for a deep learning based predictive\nquality system in GMAW. At its core, the concept involves a pipeline consisting\nof four major phases: collection and management of multi-sensor data (e.g.\ncurrent and voltage), real-time processing and feature engineering of the time\nseries data by means of autoencoders, training and deployment of suitable\nrecurrent deep learning models for quality predictions, and model evolutions\nunder changing process conditions using continual learning. The concept\nprovides the foundation for future research activities in which we will realize\nan online predictive quality system for running production.\n",
        "published": "2023",
        "authors": [
            "Yannik Hahn",
            "Robert Maack",
            "Guido Buchholz",
            "Marion Purrio",
            "Matthias Angerhausen",
            "Hasan Tercan",
            "Tobias Meisen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.00288v1",
        "title": "Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit",
        "abstract": "  Code intelligence leverages machine learning techniques to extract knowledge\nfrom extensive code corpora, with the aim of developing intelligent tools to\nimprove the quality and productivity of computer programming. Currently, there\nis already a thriving research community focusing on code intelligence, with\nefforts ranging from software engineering, machine learning, data mining,\nnatural language processing, and programming languages. In this paper, we\nconduct a comprehensive literature review on deep learning for code\nintelligence, from the aspects of code representation learning, deep learning\ntechniques, and application tasks. We also benchmark several state-of-the-art\nneural models for code intelligence, and provide an open-source toolkit\ntailored for the rapid prototyping of deep-learning-based code intelligence\nmodels. In particular, we inspect the existing code intelligence models under\nthe basis of code representation learning, and provide a comprehensive overview\nto enhance comprehension of the present state of code intelligence.\nFurthermore, we publicly release the source code and data resources to provide\nthe community with a ready-to-use benchmark, which can facilitate the\nevaluation and comparison of existing and future code intelligence models\n(https://xcodemind.github.io). At last, we also point out several challenging\nand promising directions for future research.\n",
        "published": "2023",
        "authors": [
            "Yao Wan",
            "Yang He",
            "Zhangqian Bi",
            "Jianguo Zhang",
            "Hongyu Zhang",
            "Yulei Sui",
            "Guandong Xu",
            "Hai Jin",
            "Philip S. Yu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.09537v3",
        "title": "The Prominence of Artificial Intelligence in COVID-19",
        "abstract": "  In December 2019, a novel virus called COVID-19 had caused an enormous number\nof causalities to date. The battle with the novel Coronavirus is baffling and\nhorrifying after the Spanish Flu 2019. While the front-line doctors and medical\nresearchers have made significant progress in controlling the spread of the\nhighly contiguous virus, technology has also proved its significance in the\nbattle. Moreover, Artificial Intelligence has been adopted in many medical\napplications to diagnose many diseases, even baffling experienced doctors.\nTherefore, this survey paper explores the methodologies proposed that can aid\ndoctors and researchers in early and inexpensive methods of diagnosis of the\ndisease. Most developing countries have difficulties carrying out tests using\nthe conventional manner, but a significant way can be adopted with Machine and\nDeep Learning. On the other hand, the access to different types of medical\nimages has motivated the researchers. As a result, a mammoth number of\ntechniques are proposed. This paper first details the background knowledge of\nthe conventional methods in the Artificial Intelligence domain. Following that,\nwe gather the commonly used datasets and their use cases to date. In addition,\nwe also show the percentage of researchers adopting Machine Learning over Deep\nLearning. Thus we provide a thorough analysis of this scenario. Lastly, in the\nresearch challenges, we elaborate on the problems faced in COVID-19 research,\nand we address the issues with our understanding to build a bright and healthy\nenvironment.\n",
        "published": "2021",
        "authors": [
            "MD Abdullah Al Nasim",
            "Aditi Dhali",
            "Faria Afrin",
            "Noshin Tasnim Zaman",
            "Nazmul Karimm",
            "Md Mahim Anjum Haque"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.09786v3",
        "title": "AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks",
        "abstract": "  New types of machine learning hardware in development and entering the market\nhold the promise of revolutionizing deep learning in a manner as profound as\nGPUs. However, existing software frameworks and training algorithms for deep\nlearning have yet to evolve to fully leverage the capability of the new wave of\nsilicon. We already see the limitations of existing algorithms for models that\nexploit structured input via complex and instance-dependent control flow, which\nprohibits minibatching. We present an asynchronous model-parallel (AMP)\ntraining algorithm that is specifically motivated by training on networks of\ninterconnected devices. Through an implementation on multi-core CPUs, we show\nthat AMP training converges to the same accuracy as conventional synchronous\ntraining algorithms in a similar number of epochs, but utilizes the available\nhardware more efficiently even for small minibatch sizes, resulting in\nsignificantly shorter overall training times. Our framework opens the door for\nscaling up a new class of deep learning models that cannot be efficiently\ntrained today.\n",
        "published": "2017",
        "authors": [
            "Alexander L. Gaunt",
            "Matthew A. Johnson",
            "Maik Riechert",
            "Daniel Tarlow",
            "Ryota Tomioka",
            "Dimitrios Vytiniotis",
            "Sam Webster"
        ]
    }
]