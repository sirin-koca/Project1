[
    {
        "id": "http://arxiv.org/abs/2309.14793v2",
        "title": "Semantic Map Learning of Traffic Light to Lane Assignment based on\n  Motion Data",
        "abstract": "  Understanding which traffic light controls which lane is crucial to navigate\nintersections safely. Autonomous vehicles commonly rely on High Definition (HD)\nmaps that contain information about the assignment of traffic lights to lanes.\nThe manual provisioning of this information is tedious, expensive, and not\nscalable. To remedy these issues, our novel approach derives the assignments\nfrom traffic light states and the corresponding motion patterns of vehicle\ntraffic. This works in an automated way and independently of the geometric\narrangement. We show the effectiveness of basic statistical approaches for this\ntask by implementing and evaluating a pattern-based contribution method. In\naddition, our novel rejection method includes accompanying safety\nconsiderations by leveraging statistical hypothesis testing. Finally, we\npropose a dataset transformation to re-purpose available motion prediction\ndatasets for semantic map learning. Our publicly available API for the Lyft\nLevel 5 dataset enables researchers to develop and evaluate their own\napproaches.\n",
        "published": "2023",
        "authors": [
            "Thomas Monninger",
            "Andreas Weber",
            "Steffen Staab"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.15110v1",
        "title": "Doduo: Learning Dense Visual Correspondence from Unsupervised\n  Semantic-Aware Flow",
        "abstract": "  Dense visual correspondence plays a vital role in robotic perception. This\nwork focuses on establishing the dense correspondence between a pair of images\nthat captures dynamic scenes undergoing substantial transformations. We\nintroduce Doduo to learn general dense visual correspondence from in-the-wild\nimages and videos without ground truth supervision. Given a pair of images, it\nestimates the dense flow field encoding the displacement of each pixel in one\nimage to its corresponding pixel in the other image. Doduo uses flow-based\nwarping to acquire supervisory signals for the training. Incorporating semantic\npriors with self-supervised flow training, Doduo produces accurate dense\ncorrespondence robust to the dynamic changes of the scenes. Trained on an\nin-the-wild video dataset, Doduo illustrates superior performance on\npoint-level correspondence estimation over existing self-supervised\ncorrespondence learning baselines. We also apply Doduo to articulation\nestimation and zero-shot goal-conditioned manipulation, underlining its\npractical applications in robotics. Code and additional visualizations are\navailable at https://ut-austin-rpl.github.io/Doduo\n",
        "published": "2023",
        "authors": [
            "Zhenyu Jiang",
            "Hanwen Jiang",
            "Yuke Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.16772v3",
        "title": "XVO: Generalized Visual Odometry via Cross-Modal Self-Training",
        "abstract": "  We propose XVO, a semi-supervised learning method for training generalized\nmonocular Visual Odometry (VO) models with robust off-the-self operation across\ndiverse datasets and settings. In contrast to standard monocular VO approaches\nwhich often study a known calibration within a single dataset, XVO efficiently\nlearns to recover relative pose with real-world scale from visual scene\nsemantics, i.e., without relying on any known camera parameters. We optimize\nthe motion estimation model via self-training from large amounts of\nunconstrained and heterogeneous dash camera videos available on YouTube. Our\nkey contribution is twofold. First, we empirically demonstrate the benefits of\nsemi-supervised training for learning a general-purpose direct VO regression\nnetwork. Second, we demonstrate multi-modal supervision, including\nsegmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate\ngeneralized representations for the VO task. Specifically, we find audio\nprediction task to significantly enhance the semi-supervised learning process\nwhile alleviating noisy pseudo-labels, particularly in highly dynamic and\nout-of-domain video data. Our proposed teacher network achieves\nstate-of-the-art performance on the commonly used KITTI benchmark despite no\nmulti-frame optimization or knowledge of camera parameters. Combined with the\nproposed semi-supervised step, XVO demonstrates off-the-shelf knowledge\ntransfer across diverse conditions on KITTI, nuScenes, and Argoverse without\nfine-tuning.\n",
        "published": "2023",
        "authors": [
            "Lei Lai",
            "Zhongkai Shangguan",
            "Jimuyang Zhang",
            "Eshed Ohn-Bar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.16940v2",
        "title": "Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow",
        "abstract": "  Collaborative perception can substantially boost each agent's perception\nability by facilitating communication among multiple agents. However, temporal\nasynchrony among agents is inevitable in the real world due to communication\ndelays, interruptions, and clock misalignments. This issue causes information\nmismatch during multi-agent fusion, seriously shaking the foundation of\ncollaboration. To address this issue, we propose CoBEVFlow, an\nasynchrony-robust collaborative perception system based on bird's eye view\n(BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align\nasynchronous collaboration messages sent by multiple agents. To model the\nmotion in a scene, we propose BEV flow, which is a collection of the motion\nvector corresponding to each spatial location. Based on BEV flow, asynchronous\nperceptual features can be reassigned to appropriate positions, mitigating the\nimpact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle\nasynchronous collaboration messages sent at irregular, continuous time stamps\nwithout discretization; and (ii) with BEV flow, CoBEVFlow only transports the\noriginal perceptual features, instead of generating new perceptual features,\navoiding additional noises. To validate CoBEVFlow's efficacy, we create\nIRregular V2V(IRV2V), the first synthetic collaborative perception dataset with\nvarious temporal asynchronies that simulate different real-world scenarios.\nExtensive experiments conducted on both IRV2V and the real-world dataset\nDAIR-V2X show that CoBEVFlow consistently outperforms other baselines and is\nrobust in extremely asynchronous settings. The code is available at\nhttps://github.com/MediaBrain-SJTU/CoBEVFlow.\n",
        "published": "2023",
        "authors": [
            "Sizhe Wei",
            "Yuxi Wei",
            "Yue Hu",
            "Yifan Lu",
            "Yiqi Zhong",
            "Siheng Chen",
            "Ya Zhang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2309.17080v1",
        "title": "GAIA-1: A Generative World Model for Autonomous Driving",
        "abstract": "  Autonomous driving promises transformative improvements to transportation,\nbut building systems capable of safely navigating the unstructured complexity\nof real-world scenarios remains challenging. A critical problem lies in\neffectively predicting the various potential outcomes that may emerge in\nresponse to the vehicle's actions as the world evolves.\n  To address this challenge, we introduce GAIA-1 ('Generative AI for\nAutonomy'), a generative world model that leverages video, text, and action\ninputs to generate realistic driving scenarios while offering fine-grained\ncontrol over ego-vehicle behavior and scene features. Our approach casts world\nmodeling as an unsupervised sequence modeling problem by mapping the inputs to\ndiscrete tokens, and predicting the next token in the sequence. Emerging\nproperties from our model include learning high-level structures and scene\ndynamics, contextual awareness, generalization, and understanding of geometry.\nThe power of GAIA-1's learned representation that captures expectations of\nfuture events, combined with its ability to generate realistic samples,\nprovides new possibilities for innovation in the field of autonomy, enabling\nenhanced and accelerated training of autonomous driving technology.\n",
        "published": "2023",
        "authors": [
            "Anthony Hu",
            "Lloyd Russell",
            "Hudson Yeo",
            "Zak Murez",
            "George Fedoseev",
            "Alex Kendall",
            "Jamie Shotton",
            "Gianluca Corrado"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.05717v1",
        "title": "STOPNet: Multiview-based 6-DoF Suction Detection for Transparent Objects\n  on Production Lines",
        "abstract": "  In this work, we present STOPNet, a framework for 6-DoF object suction\ndetection on production lines, with a focus on but not limited to transparent\nobjects, which is an important and challenging problem in robotic systems and\nmodern industry. Current methods requiring depth input fail on transparent\nobjects due to depth cameras' deficiency in sensing their geometry, while we\nproposed a novel framework to reconstruct the scene on the production line\ndepending only on RGB input, based on multiview stereo. Compared to existing\nworks, our method not only reconstructs the whole 3D scene in order to obtain\nhigh-quality 6-DoF suction poses in real time but also generalizes to novel\nenvironments, novel arrangements and novel objects, including challenging\ntransparent objects, both in simulation and the real world. Extensive\nexperiments in simulation and the real world show that our method significantly\nsurpasses the baselines and has better generalizability, which caters to\npractical industrial needs.\n",
        "published": "2023",
        "authors": [
            "Yuxuan Kuang",
            "Qin Han",
            "Danshi Li",
            "Qiyu Dai",
            "Lian Ding",
            "Dong Sun",
            "Hanlin Zhao",
            "He Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.06603v1",
        "title": "V2X-AHD:Vehicle-to-Everything Cooperation Perception via Asymmetric\n  Heterogenous Distillation Network",
        "abstract": "  Object detection is the central issue of intelligent traffic systems, and\nrecent advancements in single-vehicle lidar-based 3D detection indicate that it\ncan provide accurate position information for intelligent agents to make\ndecisions and plan. Compared with single-vehicle perception, multi-view\nvehicle-road cooperation perception has fundamental advantages, such as the\nelimination of blind spots and a broader range of perception, and has become a\nresearch hotspot. However, the current perception of cooperation focuses on\nimproving the complexity of fusion while ignoring the fundamental problems\ncaused by the absence of single-view outlines. We propose a multi-view\nvehicle-road cooperation perception system, vehicle-to-everything cooperative\nperception (V2X-AHD), in order to enhance the identification capability,\nparticularly for predicting the vehicle's shape. At first, we propose an\nasymmetric heterogeneous distillation network fed with different training data\nto improve the accuracy of contour recognition, with multi-view teacher\nfeatures transferring to single-view student features. While the point cloud\ndata are sparse, we propose Spara Pillar, a spare convolutional-based plug-in\nfeature extraction backbone, to reduce the number of parameters and improve and\nenhance feature extraction capabilities. Moreover, we leverage the multi-head\nself-attention (MSA) to fuse the single-view feature, and the lightweight\ndesign makes the fusion feature a smooth expression. The results of applying\nour algorithm to the massive open dataset V2Xset demonstrate that our method\nachieves the state-of-the-art result. The V2X-AHD can effectively improve the\naccuracy of 3D object detection and reduce the number of network parameters,\naccording to this study, which serves as a benchmark for cooperative\nperception. The code for this article is available at\nhttps://github.com/feeling0414-lab/V2X-AHD.\n",
        "published": "2023",
        "authors": [
            "Caizhen He",
            "Hai Wang",
            "Long Chen",
            "Tong Luo",
            "Yingfeng Cai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.12007v1",
        "title": "KI-PMF: Knowledge Integrated Plausible Motion Forecasting",
        "abstract": "  Accurately forecasting the motion of traffic actors is crucial for the\ndeployment of autonomous vehicles at a large scale. Current trajectory\nforecasting approaches primarily concentrate on optimizing a loss function with\na specific metric, which can result in predictions that do not adhere to\nphysical laws or violate external constraints. Our objective is to incorporate\nexplicit knowledge priors that allow a network to forecast future trajectories\nin compliance with both the kinematic constraints of a vehicle and the geometry\nof the driving environment. To achieve this, we introduce a non-parametric\npruning layer and attention layers to integrate the defined knowledge priors.\nOur proposed method is designed to ensure reachability guarantees for traffic\nactors in both complex and dynamic situations. By conditioning the network to\nfollow physical laws, we can obtain accurate and safe predictions, essential\nfor maintaining autonomous vehicles' safety and efficiency in real-world\nsettings.In summary, this paper presents concepts that prevent off-road\npredictions for safe and reliable motion forecasting by incorporating knowledge\npriors into the training process.\n",
        "published": "2023",
        "authors": [
            "Abhishek Vivekanandan",
            "Ahmed Abouelazm",
            "Philip Sch\u00f6rner",
            "J. Marius Z\u00f6llner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.14702v2",
        "title": "BM2CP: Efficient Collaborative Perception with LiDAR-Camera Modalities",
        "abstract": "  Collaborative perception enables agents to share complementary perceptual\ninformation with nearby agents. This would improve the perception performance\nand alleviate the issues of single-view perception, such as occlusion and\nsparsity. Most existing approaches mainly focus on single modality (especially\nLiDAR), and not fully exploit the superiority of multi-modal perception. We\npropose a collaborative perception paradigm, BM2CP, which employs LiDAR and\ncamera to achieve efficient multi-modal perception. It utilizes LiDAR-guided\nmodal fusion, cooperative depth generation and modality-guided intermediate\nfusion to acquire deep interactions among modalities of different agents,\nMoreover, it is capable to cope with the special case where one of the sensors,\nsame or different type, of any agent is missing. Extensive experiments validate\nthat our approach outperforms the state-of-the-art methods with 50X lower\ncommunication volumes in both simulated and real-world autonomous driving\nscenarios. Our code is available at https://github.com/byzhaoAI/BM2CP.\n",
        "published": "2023",
        "authors": [
            "Binyu Zhao",
            "Wei Zhang",
            "Zhaonian Zou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.18966v1",
        "title": "Spacecraft Autonomous Decision-Planning for Collision Avoidance: a\n  Reinforcement Learning Approach",
        "abstract": "  The space environment around the Earth is becoming increasingly populated by\nboth active spacecraft and space debris. To avoid potential collision events,\nsignificant improvements in Space Situational Awareness (SSA) activities and\nCollision Avoidance (CA) technologies are allowing the tracking and maneuvering\nof spacecraft with increasing accuracy and reliability. However, these\nprocedures still largely involve a high level of human intervention to make the\nnecessary decisions. For an increasingly complex space environment, this\ndecision-making strategy is not likely to be sustainable. Therefore, it is\nimportant to successfully introduce higher levels of automation for key Space\nTraffic Management (STM) processes to ensure the level of reliability needed\nfor navigating a large number of spacecraft. These processes range from\ncollision risk detection to the identification of the appropriate action to\ntake and the execution of avoidance maneuvers. This work proposes an\nimplementation of autonomous CA decision-making capabilities on spacecraft\nbased on Reinforcement Learning (RL) techniques. A novel methodology based on a\nPartially Observable Markov Decision Process (POMDP) framework is developed to\ntrain the Artificial Intelligence (AI) system on board the spacecraft,\nconsidering epistemic and aleatory uncertainties. The proposed framework\nconsiders imperfect monitoring information about the status of the debris in\norbit and allows the AI system to effectively learn stochastic policies to\nperform accurate Collision Avoidance Maneuvers (CAMs). The objective is to\nsuccessfully delegate the decision-making process for autonomously implementing\na CAM to the spacecraft without human intervention. This approach would allow\nfor a faster response in the decision-making process and for highly\ndecentralized operations.\n",
        "published": "2023",
        "authors": [
            "Nicolas Bourriez",
            "Adrien Loizeau",
            "Adam F. Abdin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2310.19372v1",
        "title": "RGB-X Object Detection via Scene-Specific Fusion Modules",
        "abstract": "  Multimodal deep sensor fusion has the potential to enable autonomous vehicles\nto visually understand their surrounding environments in all weather\nconditions. However, existing deep sensor fusion methods usually employ\nconvoluted architectures with intermingled multimodal features, requiring large\ncoregistered multimodal datasets for training. In this work, we present an\nefficient and modular RGB-X fusion network that can leverage and fuse\npretrained single-modal models via scene-specific fusion modules, thereby\nenabling joint input-adaptive network architectures to be created using small,\ncoregistered multimodal datasets. Our experiments demonstrate the superiority\nof our method compared to existing works on RGB-thermal and RGB-gated datasets,\nperforming fusion using only a small amount of additional parameters. Our code\nis available at https://github.com/dsriaditya999/RGBXFusion.\n",
        "published": "2023",
        "authors": [
            "Sri Aditya Deevi",
            "Connor Lee",
            "Lu Gan",
            "Sushruth Nagesh",
            "Gaurav Pandey",
            "Soon-Jo Chung"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.00926v1",
        "title": "M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place",
        "abstract": "  With the advent of large language models and large-scale robotic datasets,\nthere has been tremendous progress in high-level decision-making for object\nmanipulation. These generic models are able to interpret complex tasks using\nlanguage commands, but they often have difficulties generalizing to\nout-of-distribution objects due to the inability of low-level action\nprimitives. In contrast, existing task-specific models excel in low-level\nmanipulation of unknown objects, but only work for a single type of action. To\nbridge this gap, we present M2T2, a single model that supplies different types\nof low-level actions that work robustly on arbitrary objects in cluttered\nscenes. M2T2 is a transformer model which reasons about contact points and\npredicts valid gripper poses for different action modes given a raw point cloud\nof the scene. Trained on a large-scale synthetic dataset with 128K scenes, M2T2\nachieves zero-shot sim2real transfer on the real robot, outperforming the\nbaseline system with state-of-the-art task-specific models by about 19% in\noverall performance and 37.5% in challenging scenes where the object needs to\nbe re-oriented for collision-free placement. M2T2 also achieves\nstate-of-the-art results on a subset of language conditioned tasks in RLBench.\nVideos of robot experiments on unseen objects in both real world and simulation\nare available on our project website https://m2-t2.github.io.\n",
        "published": "2023",
        "authors": [
            "Wentao Yuan",
            "Adithyavairavan Murali",
            "Arsalan Mousavian",
            "Dieter Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.01057v2",
        "title": "Ultra-Efficient On-Device Object Detection on AI-Integrated Smart\n  Glasses with TinyissimoYOLO",
        "abstract": "  Smart glasses are rapidly gaining advanced functionality thanks to\ncutting-edge computing technologies, accelerated hardware architectures, and\ntiny AI algorithms. Integrating AI into smart glasses featuring a small form\nfactor and limited battery capacity is still challenging when targeting\nfull-day usage for a satisfactory user experience. This paper illustrates the\ndesign and implementation of tiny machine-learning algorithms exploiting novel\nlow-power processors to enable prolonged continuous operation in smart glasses.\nWe explore the energy- and latency-efficient of smart glasses in the case of\nreal-time object detection. To this goal, we designed a smart glasses prototype\nas a research platform featuring two microcontrollers, including a novel\nmilliwatt-power RISC-V parallel processor with a hardware accelerator for\nvisual AI, and a Bluetooth low-power module for communication. The smart\nglasses integrate power cycling mechanisms, including image and audio sensing\ninterfaces. Furthermore, we developed a family of novel tiny deep-learning\nmodels based on YOLO with sub-million parameters customized for\nmicrocontroller-based inference dubbed TinyissimoYOLO v1.3, v5, and v8, aiming\nat benchmarking object detection with smart glasses for energy and latency.\nEvaluations on the prototype of the smart glasses demonstrate TinyissimoYOLO's\n17ms inference latency and 1.59mJ energy consumption per inference while\nensuring acceptable detection accuracy. Further evaluation reveals an\nend-to-end latency from image capturing to the algorithm's prediction of 56ms\nor equivalently 18 fps, with a total power consumption of 62.9mW, equivalent to\na 9.3 hours of continuous run time on a 154mAh battery. These results\noutperform MCUNet (TinyNAS+TinyEngine), which runs a simpler task (image\nclassification) at just 7.3 fps per second.\n",
        "published": "2023",
        "authors": [
            "Julian Moosmann",
            "Pietro Bonazzi",
            "Yawei Li",
            "Sizhen Bian",
            "Philipp Mayer",
            "Luca Benini",
            "Michele Magno"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.02305v1",
        "title": "OSM vs HD Maps: Map Representations for Trajectory Prediction",
        "abstract": "  While High Definition (HD) Maps have long been favored for their precise\ndepictions of static road elements, their accessibility constraints and\nsusceptibility to rapid environmental changes impede the widespread deployment\nof autonomous driving, especially in the motion forecasting task. In this\ncontext, we propose to leverage OpenStreetMap (OSM) as a promising alternative\nto HD Maps for long-term motion forecasting. The contributions of this work are\nthreefold: firstly, we extend the application of OSM to long-horizon\nforecasting, doubling the forecasting horizon compared to previous studies.\nSecondly, through an expanded receptive field and the integration of\nintersection priors, our OSM-based approach exhibits competitive performance,\nnarrowing the gap with HD Map-based models. Lastly, we conduct an exhaustive\ncontext-aware analysis, providing deeper insights in motion forecasting across\ndiverse scenarios as well as conducting class-aware comparisons. This research\nnot only advances long-term motion forecasting with coarse map representations\nbut additionally offers a potential scalable solution within the domain of\nautonomous driving.\n",
        "published": "2023",
        "authors": [
            "Jing-Yan Liao",
            "Parth Doshi",
            "Zihan Zhang",
            "David Paz",
            "Henrik Christensen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.02337v1",
        "title": "STOW: Discrete-Frame Segmentation and Tracking of Unseen Objects for\n  Warehouse Picking Robots",
        "abstract": "  Segmentation and tracking of unseen object instances in discrete frames pose\na significant challenge in dynamic industrial robotic contexts, such as\ndistribution warehouses. Here, robots must handle object rearrangement,\nincluding shifting, removal, and partial occlusion by new items, and track\nthese items after substantial temporal gaps. The task is further complicated\nwhen robots encounter objects not learned in their training sets, which\nrequires the ability to segment and track previously unseen items. Considering\nthat continuous observation is often inaccessible in such settings, our task\ninvolves working with a discrete set of frames separated by indefinite periods\nduring which substantial changes to the scene may occur. This task also\ntranslates to domestic robotic applications, such as rearrangement of objects\non a table. To address these demanding challenges, we introduce new synthetic\nand real-world datasets that replicate these industrial and household\nscenarios. We also propose a novel paradigm for joint segmentation and tracking\nin discrete frames along with a transformer module that facilitates efficient\ninter-frame communication. The experiments we conduct show that our approach\nsignificantly outperforms recent methods. For additional results and videos,\nplease visit \\href{https://sites.google.com/view/stow-corl23}{website}. Code\nand dataset will be released.\n",
        "published": "2023",
        "authors": [
            "Yi Li",
            "Muru Zhang",
            "Markus Grotz",
            "Kaichun Mo",
            "Dieter Fox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.11722v1",
        "title": "Sparse4D v3: Advancing End-to-End 3D Detection and Tracking",
        "abstract": "  In autonomous driving perception systems, 3D detection and tracking are the\ntwo fundamental tasks. This paper delves deeper into this field, building upon\nthe Sparse4D framework. We introduce two auxiliary training tasks (Temporal\nInstance Denoising and Quality Estimation) and propose decoupled attention to\nmake structural improvements, leading to significant enhancements in detection\nperformance. Additionally, we extend the detector into a tracker using a\nstraightforward approach that assigns instance ID during inference, further\nhighlighting the advantages of query-based algorithms. Extensive experiments\nconducted on the nuScenes benchmark validate the effectiveness of the proposed\nimprovements. With ResNet50 as the backbone, we witnessed enhancements of\n3.0\\%, 2.2\\%, and 7.6\\% in mAP, NDS, and AMOTA, achieving 46.9\\%, 56.1\\%, and\n49.0\\%, respectively. Our best model achieved 71.9\\% NDS and 67.7\\% AMOTA on\nthe nuScenes test set. Code will be released at\n\\url{https://github.com/linxuewu/Sparse4D}.\n",
        "published": "2023",
        "authors": [
            "Xuewu Lin",
            "Zixiang Pei",
            "Tianwei Lin",
            "Lichao Huang",
            "Zhizhong Su"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.12151v1",
        "title": "Teaching Robots to Build Simulations of Themselves",
        "abstract": "  Simulation enables robots to plan and estimate the outcomes of prospective\nactions without the need to physically execute them. We introduce a\nself-supervised learning framework to enable robots model and predict their\nmorphology, kinematics and motor control using only brief raw video data,\neliminating the need for extensive real-world data collection and kinematic\npriors. By observing their own movements, akin to humans watching their\nreflection in a mirror, robots learn an ability to simulate themselves and\npredict their spatial motion for various tasks. Our results demonstrate that\nthis self-learned simulation not only enables accurate motion planning but also\nallows the robot to detect abnormalities and recover from damage.\n",
        "published": "2023",
        "authors": [
            "Yuhang Hu",
            "Jiong Lin",
            "Hod Lipson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2311.14786v1",
        "title": "GPT-4V Takes the Wheel: Evaluating Promise and Challenges for Pedestrian\n  Behavior Prediction",
        "abstract": "  Existing pedestrian behavior prediction methods rely primarily on deep neural\nnetworks that utilize features extracted from video frame sequences. Although\nthese vision-based models have shown promising results, they face limitations\nin effectively capturing and utilizing the dynamic spatio-temporal interactions\nbetween the target pedestrian and its surrounding traffic elements, crucial for\naccurate reasoning. Additionally, training these models requires manually\nannotating domain-specific datasets, a process that is expensive,\ntime-consuming, and difficult to generalize to new environments and scenarios.\nThe recent emergence of Large Multimodal Models (LMMs) offers potential\nsolutions to these limitations due to their superior visual understanding and\ncausal reasoning capabilities, which can be harnessed through semi-supervised\ntraining. GPT-4V(ision), the latest iteration of the state-of-the-art\nLarge-Language Model GPTs, now incorporates vision input capabilities. This\nreport provides a comprehensive evaluation of the potential of GPT-4V for\npedestrian behavior prediction in autonomous driving using publicly available\ndatasets: JAAD, PIE, and WiDEVIEW. Quantitative and qualitative evaluations\ndemonstrate GPT-4V(ision)'s promise in zero-shot pedestrian behavior prediction\nand driving scene understanding ability for autonomous driving. However, it\nstill falls short of the state-of-the-art traditional domain-specific models.\nChallenges include difficulties in handling small pedestrians and vehicles in\nmotion. These limitations highlight the need for further research and\ndevelopment in this area.\n",
        "published": "2023",
        "authors": [
            "Jia Huang",
            "Peng Jiang",
            "Alvika Gautam",
            "Srikanth Saripalli"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.02126v1",
        "title": "SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM",
        "abstract": "  Dense simultaneous localization and mapping (SLAM) is pivotal for embodied\nscene understanding. Recent work has shown that 3D Gaussians enable\nhigh-quality reconstruction and real-time rendering of scenes using multiple\nposed cameras. In this light, we show for the first time that representing a\nscene by 3D Gaussians can enable dense SLAM using a single unposed monocular\nRGB-D camera. Our method, SplaTAM, addresses the limitations of prior radiance\nfield-based representations, including fast rendering and optimization, the\nability to determine if areas have been previously mapped, and structured map\nexpansion by adding more Gaussians. We employ an online tracking and mapping\npipeline while tailoring it to specifically use an underlying Gaussian\nrepresentation and silhouette-guided optimization via differentiable rendering.\nExtensive experiments show that SplaTAM achieves up to 2X state-of-the-art\nperformance in camera pose estimation, map construction, and novel-view\nsynthesis, demonstrating its superiority over existing approaches, while\nallowing real-time rendering of a high-resolution dense 3D map.\n",
        "published": "2023",
        "authors": [
            "Nikhil Keetha",
            "Jay Karhade",
            "Krishna Murthy Jatavallabhula",
            "Gengshan Yang",
            "Sebastian Scherer",
            "Deva Ramanan",
            "Jonathon Luiten"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.04316v3",
        "title": "Towards Knowledge-driven Autonomous Driving",
        "abstract": "  This paper explores the emerging knowledge-driven autonomous driving\ntechnologies. Our investigation highlights the limitations of current\nautonomous driving systems, in particular their sensitivity to data bias,\ndifficulty in handling long-tail scenarios, and lack of interpretability.\nConversely, knowledge-driven methods with the abilities of cognition,\ngeneralization and life-long learning emerge as a promising way to overcome\nthese challenges. This paper delves into the essence of knowledge-driven\nautonomous driving and examines its core components: dataset \\& benchmark,\nenvironment, and driver agent. By leveraging large language models, world\nmodels, neural rendering, and other advanced artificial intelligence\ntechniques, these components collectively contribute to a more holistic,\nadaptive, and intelligent autonomous driving system. The paper systematically\norganizes and reviews previous research efforts in this area, and provides\ninsights and guidance for future research and practical applications of\nautonomous driving. We will continually share the latest updates on\ncutting-edge developments in knowledge-driven autonomous driving along with the\nrelevant valuable open-source resources at:\n\\url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.\n",
        "published": "2023",
        "authors": [
            "Xin Li",
            "Yeqi Bai",
            "Pinlong Cai",
            "Licheng Wen",
            "Daocheng Fu",
            "Bo Zhang",
            "Xuemeng Yang",
            "Xinyu Cai",
            "Tao Ma",
            "Jianfei Guo",
            "Xing Gao",
            "Min Dou",
            "Yikang Li",
            "Botian Shi",
            "Yong Liu",
            "Liang He",
            "Yu Qiao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.06330v1",
        "title": "Navigating Open Set Scenarios for Skeleton-based Action Recognition",
        "abstract": "  In real-world scenarios, human actions often fall outside the distribution of\ntraining data, making it crucial for models to recognize known actions and\nreject unknown ones. However, using pure skeleton data in such open-set\nconditions poses challenges due to the lack of visual background cues and the\ndistinct sparse structure of body pose sequences. In this paper, we tackle the\nunexplored Open-Set Skeleton-based Action Recognition (OS-SAR) task and\nformalize the benchmark on three skeleton-based datasets. We assess the\nperformance of seven established open-set approaches on our task and identify\ntheir limits and critical generalization issues when dealing with skeleton\ninformation. To address these challenges, we propose a distance-based\ncross-modality ensemble method that leverages the cross-modal alignment of\nskeleton joints, bones, and velocities to achieve superior open-set recognition\nperformance. We refer to the key idea as CrossMax - an approach that utilizes a\nnovel cross-modality mean max discrepancy suppression mechanism to align latent\nspaces during training and a cross-modality distance-based logits refinement\nmethod during testing. CrossMax outperforms existing approaches and\nconsistently yields state-of-the-art results across all datasets and backbones.\nThe benchmark, code, and models will be released at\nhttps://github.com/KPeng9510/OS-SAR.\n",
        "published": "2023",
        "authors": [
            "Kunyu Peng",
            "Cheng Yin",
            "Junwei Zheng",
            "Ruiping Liu",
            "David Schneider",
            "Jiaming Zhang",
            "Kailun Yang",
            "M. Saquib Sarfraz",
            "Rainer Stiefelhagen",
            "Alina Roitberg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.09676v1",
        "title": "nuScenes Knowledge Graph -- A comprehensive semantic representation of\n  traffic scenes for trajectory prediction",
        "abstract": "  Trajectory prediction in traffic scenes involves accurately forecasting the\nbehaviour of surrounding vehicles. To achieve this objective it is crucial to\nconsider contextual information, including the driving path of vehicles, road\ntopology, lane dividers, and traffic rules. Although studies demonstrated the\npotential of leveraging heterogeneous context for improving trajectory\nprediction, state-of-the-art deep learning approaches still rely on a limited\nsubset of this information. This is mainly due to the limited availability of\ncomprehensive representations. This paper presents an approach that utilizes\nknowledge graphs to model the diverse entities and their semantic connections\nwithin traffic scenes. Further, we present nuScenes Knowledge Graph (nSKG), a\nknowledge graph for the nuScenes dataset, that models explicitly all scene\nparticipants and road elements, as well as their semantic and spatial\nrelationships. To facilitate the usage of the nSKG via graph neural networks\nfor trajectory prediction, we provide the data in a format, ready-to-use by the\nPyG library. All artefacts can be found here:\nhttps://github.com/boschresearch/nuScenes_Knowledge_Graph\n",
        "published": "2023",
        "authors": [
            "Leon Mlodzian",
            "Zhigang Sun",
            "Hendrik Berkemeyer",
            "Sebastian Monka",
            "Zixu Wang",
            "Stefan Dietze",
            "Lavdim Halilaj",
            "Juergen Luettin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.14115v1",
        "title": "LingoQA: Video Question Answering for Autonomous Driving",
        "abstract": "  Autonomous driving has long faced a challenge with public acceptance due to\nthe lack of explainability in the decision-making process. Video\nquestion-answering (QA) in natural language provides the opportunity for\nbridging this gap. Nonetheless, evaluating the performance of Video QA models\nhas proved particularly tough due to the absence of comprehensive benchmarks.\nTo fill this gap, we introduce LingoQA, a benchmark specifically for autonomous\ndriving Video QA. The LingoQA trainable metric demonstrates a 0.95 Spearman\ncorrelation coefficient with human evaluations. We introduce a Video QA dataset\nof central London consisting of 419k samples that we release with the paper. We\nestablish a baseline vision-language model and run extensive ablation studies\nto understand its performance.\n",
        "published": "2023",
        "authors": [
            "Ana-Maria Marcu",
            "Long Chen",
            "Jan H\u00fcnermann",
            "Alice Karnsund",
            "Benoit Hanotte",
            "Prajwal Chidananda",
            "Saurabh Nair",
            "Vijay Badrinarayanan",
            "Alex Kendall",
            "Jamie Shotton",
            "Oleg Sinavski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.14481v1",
        "title": "Part to Whole: Collaborative Prompting for Surgical Instrument\n  Segmentation",
        "abstract": "  Foundation models like the Segment Anything Model (SAM) have demonstrated\npromise in generic object segmentation. However, directly applying SAM to\nsurgical instrument segmentation presents key challenges. First, SAM relies on\nper-frame point-or-box prompts which complicate surgeon-computer interaction.\nAlso, SAM yields suboptimal performance on segmenting surgical instruments,\nowing to insufficient surgical data in its pre-training as well as the complex\nstructure and fine-grained details of various surgical instruments. To address\nthese challenges, in this paper, we investigate text promptable surgical\ninstrument segmentation and propose SP-SAM (SurgicalPart-SAM), a novel\nefficient-tuning approach that integrates surgical instrument structure\nknowledge with the generic segmentation knowledge of SAM. Specifically, we\nachieve this by proposing (1) collaborative prompts in the text form \"[part\nname] of [instrument category name]\" that decompose instruments into\nfine-grained parts; (2) a Cross-Modal Prompt Encoder that encodes text prompts\njointly with visual embeddings into discriminative part-level representations;\nand (3) a Part-to-Whole Selective Fusion and a Hierarchical Decoding strategy\nthat selectively assemble the part-level representations into a whole for\naccurate instrument segmentation. Built upon them, SP-SAM acquires a better\ncapability to comprehend surgical instrument structures and distinguish between\nvarious categories. Extensive experiments on both the EndoVis2018 and\nEndoVis2017 datasets demonstrate SP-SAM's state-of-the-art performance with\nminimal tunable parameters. Code is at\nhttps://github.com/wenxi-yue/SurgicalPart-SAM.\n",
        "published": "2023",
        "authors": [
            "Wenxi Yue",
            "Jing Zhang",
            "Kun Hu",
            "Qiuxia Wu",
            "Zongyuan Ge",
            "Yong Xia",
            "Jiebo Luo",
            "Zhiyong Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2312.16170v1",
        "title": "EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards\n  Embodied AI",
        "abstract": "  In the realm of computer vision and robotics, embodied agents are expected to\nexplore their environment and carry out human instructions. This necessitates\nthe ability to fully understand 3D scenes given their first-person observations\nand contextualize them into language for interaction. However, traditional\nresearch focuses more on scene-level input and output setups from a global\nview. To address the gap, we introduce EmbodiedScan, a multi-modal, ego-centric\n3D perception dataset and benchmark for holistic 3D scene understanding. It\nencompasses over 5k scans encapsulating 1M ego-centric RGB-D views, 1M language\nprompts, 160k 3D-oriented boxes spanning over 760 categories, some of which\npartially align with LVIS, and dense semantic occupancy with 80 common\ncategories. Building upon this database, we introduce a baseline framework\nnamed Embodied Perceptron. It is capable of processing an arbitrary number of\nmulti-modal inputs and demonstrates remarkable 3D perception capabilities, both\nwithin the two series of benchmarks we set up, i.e., fundamental 3D perception\ntasks and language-grounded tasks, and in the wild. Codes, datasets, and\nbenchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.\n",
        "published": "2023",
        "authors": [
            "Tai Wang",
            "Xiaohan Mao",
            "Chenming Zhu",
            "Runsen Xu",
            "Ruiyuan Lyu",
            "Peisen Li",
            "Xiao Chen",
            "Wenwei Zhang",
            "Kai Chen",
            "Tianfan Xue",
            "Xihui Liu",
            "Cewu Lu",
            "Dahua Lin",
            "Jiangmiao Pang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.07314v1",
        "title": "MapGPT: Map-Guided Prompting for Unified Vision-and-Language Navigation",
        "abstract": "  Embodied agents equipped with GPT as their brain have exhibited extraordinary\nthinking and decision-making abilities across various tasks. However, existing\nzero-shot agents for vision-and-language navigation (VLN) only prompt the GPT\nto handle excessive environmental information and select potential locations\nwithin localized environments, without constructing an effective\n''global-view'' (e.g., a commonly-used map) for the agent to understand the\noverall environment. In this work, we present a novel map-guided GPT-based\npath-planning agent, dubbed MapGPT, for the zero-shot VLN task. Specifically,\nwe convert a topological map constructed online into prompts to encourage\nmap-guided global exploration, and require the agent to explicitly output and\nupdate multi-step path planning to avoid getting stuck in local exploration.\nExtensive experiments demonstrate that our MapGPT is effective, achieving\nimpressive performance on both the R2R and REVERIE datasets (38.8% and 28.4%\nsuccess rate, respectively) and showcasing the newly emerged global thinking\nand path planning capabilities of the GPT model. Unlike previous VLN agents,\nwhich require separate parameters fine-tuning or specific prompt design to\naccommodate various instruction styles across different datasets, our MapGPT is\nmore unified as it can adapt to different instruction styles seamlessly, which\nis the first of its kind in this field.\n",
        "published": "2024",
        "authors": [
            "Jiaqi Chen",
            "Bingqian Lin",
            "Ran Xu",
            "Zhenhua Chai",
            "Xiaodan Liang",
            "Kwan-Yee K. Wong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.01443v1",
        "title": "A semi-supervised fuzzy GrowCut algorithm to segment and classify\n  regions of interest of mammographic images",
        "abstract": "  According to the World Health Organization, breast cancer is the most common\nform of cancer in women. It is the second leading cause of death among women\nround the world, becoming the most fatal form of cancer. Mammographic image\nsegmentation is a fundamental task to support image analysis and diagnosis,\ntaking into account shape analysis of mammary lesions and their borders.\nHowever, mammogram segmentation is a very hard process, once it is highly\ndependent on the types of mammary tissues. In this work we present a new\nsemi-supervised segmentation algorithm based on the modification of the GrowCut\nalgorithm to perform automatic mammographic image segmentation once a region of\ninterest is selected by a specialist. In our proposal, we used fuzzy Gaussian\nmembership functions to modify the evolution rule of the original GrowCut\nalgorithm, in order to estimate the uncertainty of a pixel being object or\nbackground. The main impact of the proposed method is the significant reduction\nof expert effort in the initialization of seed points of GrowCut to perform\naccurate segmentation, once it removes the need of selection of background\nseeds. We also constructed an automatic point selection process based on the\nsimulated annealing optimization method, avoiding the need of human\nintervention. The proposed approach was qualitatively compared with other\nstate-of-the-art segmentation techniques, considering the shape of segmented\nregions. In order to validate our proposal, we built an image classifier using\na classical multilayer perceptron. We used Zernike moments to extract segmented\nimage features. This analysis employed 685 mammograms from IRMA breast cancer\ndatabase, using fat and fibroid tissues. Results show that the proposed\ntechnique could achieve a classification rate of 91.28\\% for fat tissues,\nevidencing the feasibility of our approach.\n",
        "published": "2017",
        "authors": [
            "Filipe Rolim Cordeiro",
            "Wellington Pinheiro dos Santos",
            "Abel Guilhermino da Silva Filho"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.13229v1",
        "title": "An evolutionary perspective on the design of neuromorphic shape filters",
        "abstract": "  A substantial amount of time and energy has been invested to develop machine\nvision using connectionist (neural network) principles. Most of that work has\nbeen inspired by theories advanced by neuroscientists and behaviorists for how\ncortical systems store stimulus information. Those theories call for\ninformation flow through connections among several neuron populations, with the\ninitial connections being random (or at least non-functional). Then the\nstrength or location of connections are modified through training trials to\nachieve an effective output, such as the ability to identify an object. Those\ntheories ignored the fact that animals that have no cortex, e.g., fish, can\ndemonstrate visual skills that outpace the best neural network models. Neural\ncircuits that allow for immediate effective vision and quick learning have been\npreprogrammed by hundreds of millions of years of evolution and the visual\nskills are available shortly after hatching. Cortical systems may be providing\nadvanced image processing, but most likely are using design principles that had\nbeen proven effective in simpler systems. The present article provides a brief\noverview of retinal and cortical mechanisms for registering shape information,\nwith the hope that it might contribute to the design of shape-encoding circuits\nthat more closely match the mechanisms of biological vision.\n",
        "published": "2020",
        "authors": [
            "Ernest Greene"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.08378v1",
        "title": "BrainSlug: Transparent Acceleration of Deep Learning Through Depth-First\n  Parallelism",
        "abstract": "  Neural network frameworks such as PyTorch and TensorFlow are the workhorses\nof numerous machine learning applications ranging from object recognition to\nmachine translation. While these frameworks are versatile and straightforward\nto use, the training of and inference in deep neural networks is resource\n(energy, compute, and memory) intensive. In contrast to recent works focusing\non algorithmic enhancements, we introduce BrainSlug, a framework that\ntransparently accelerates neural network workloads by changing the default\nlayer-by-layer processing to a depth-first approach, reducing the amount of\ndata required by the computations and thus improving the performance of the\navailable hardware caches. BrainSlug achieves performance improvements of up to\n41.1% on CPUs and 35.7% on GPUs. These optimizations come at zero cost to the\nuser as they do not require hardware changes and only need tiny adjustments to\nthe software.\n",
        "published": "2018",
        "authors": [
            "Nicolas Weber",
            "Florian Schmidt",
            "Mathias Niepert",
            "Felipe Huici"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2110.15719v2",
        "title": "Generational Frameshifts in Technology: Computer Science and\n  Neurosurgery, The VR Use Case",
        "abstract": "  We are at a unique moment in history where there is a confluence of\ntechnologies which will synergistically come together to transform the practice\nof neurosurgery. These technological transformations will be all-encompassing,\nincluding improved tools and methods for intraoperative performance of\nneurosurgery, scalable solutions for asynchronous neurosurgical training and\nsimulation, as well as broad aggregation of operative data allowing fundamental\nchanges in quality assessment, billing, outcome measures, and dissemination of\nsurgical best practices. The ability to perform surgery more safely and more\nefficiently while capturing the operative details and parsing each component of\nthe operation will open an entirely new epoch advancing our field and all\nsurgical specialties. The digitization of all components within the operating\nroom will allow us to leverage the various fields within computer and\ncomputational science to obtain new insights that will improve care and\ndelivery of the highest quality neurosurgery regardless of location. The\ndemocratization of neurosurgery is at hand and will be driven by our\ndevelopment, extraction, and adoption of these tools of the modern world.\nVirtual reality provides a good example of how consumer-facing technologies are\nfinding a clear role in industry and medicine and serves as a notable example\nof the confluence of various computer science technologies creating a novel\nparadigm for scaling human ability and interactions. The authors describe the\ntechnology ecosystem that has come and highlight a myriad of computational and\ndata sciences that will be necessary to enable the operating room of the near\nfuture.\n",
        "published": "2021",
        "authors": [
            "Samuel R. Browd",
            "Maya Sharma",
            "Chetan Sharma"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2212.13285v1",
        "title": "On the Level Sets and Invariance of Neural Tuning Landscapes",
        "abstract": "  Visual representations can be defined as the activations of neuronal\npopulations in response to images. The activation of a neuron as a function\nover all image space has been described as a \"tuning landscape\". As a function\nover a high-dimensional space, what is the structure of this landscape? In this\nstudy, we characterize tuning landscapes through the lens of level sets and\nMorse theory. A recent study measured the in vivo two-dimensional tuning maps\nof neurons in different brain regions. Here, we developed a statistically\nreliable signature for these maps based on the change of topology in level\nsets. We found this topological signature changed progressively throughout the\ncortical hierarchy, with similar trends found for units in convolutional neural\nnetworks (CNNs). Further, we analyzed the geometry of level sets on the tuning\nlandscapes of CNN units. We advanced the hypothesis that higher-order units can\nbe locally regarded as isotropic radial basis functions, but not globally. This\nshows the power of level sets as a conceptual tool to understand neuronal\nactivations over image space.\n",
        "published": "2022",
        "authors": [
            "Binxu Wang",
            "Carlos R. Ponce"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.05849v1",
        "title": "XNORBIN: A 95 TOp/s/W Hardware Accelerator for Binary Convolutional\n  Neural Networks",
        "abstract": "  Deploying state-of-the-art CNNs requires power-hungry processors and off-chip\nmemory. This precludes the implementation of CNNs in low-power embedded\nsystems. Recent research shows CNNs sustain extreme quantization, binarizing\ntheir weights and intermediate feature maps, thereby saving 8-32\\x memory and\ncollapsing energy-intensive sum-of-products into XNOR-and-popcount operations.\n  We present XNORBIN, an accelerator for binary CNNs with computation tightly\ncoupled to memory for aggressive data reuse. Implemented in UMC 65nm technology\nXNORBIN achieves an energy efficiency of 95 TOp/s/W and an area efficiency of\n2.0 TOp/s/MGE at 0.8 V.\n",
        "published": "2018",
        "authors": [
            "Andrawes Al Bahou",
            "Geethan Karunaratne",
            "Renzo Andri",
            "Lukas Cavigelli",
            "Luca Benini"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1712.01694v1",
        "title": "Fuzzy-Based Dialectical Non-Supervised Image Classification and\n  Clustering",
        "abstract": "  The materialist dialectical method is a philosophical investigative method to\nanalyze aspects of reality. These aspects are viewed as complex processes\ncomposed by basic units named poles, which interact with each other. Dialectics\nhas experienced considerable progress in the 19th century, with Hegel's\ndialectics and, in the 20th century, with the works of Marx, Engels, and\nGramsci, in Philosophy and Economics. The movement of poles through their\ncontradictions is viewed as a dynamic process with intertwined phases of\nevolution and revolutionary crisis. In order to build a computational process\nbased on dialectics, the interaction between poles can be modeled using fuzzy\nmembership functions. Based on this assumption, we introduce the Objective\nDialectical Classifier (ODC), a non-supervised map for classification based on\nmaterialist dialectics and designed as an extension of fuzzy c-means\nclassifier. As a case study, we used ODC to classify 181 magnetic resonance\nsynthetic multispectral images composed by proton density, $T_1$- and\n$T_2$-weighted synthetic brain images. Comparing ODC to k-means, fuzzy c-means,\nand Kohonen's self-organized maps, concerning with image fidelity indexes as\nestimatives of quantization distortion, we proved that ODC can reach almost the\nsame quantization performance as optimal non-supervised classifiers like\nKohonen's self-organized maps.\n",
        "published": "2017",
        "authors": [
            "Wellington Pinheiro dos Santos",
            "Francisco Marcos de Assis",
            "Ricardo Emmanuel de Souza",
            "Priscilla B. Mendes",
            "Henrique S. S. Monteiro",
            "Havana Diogo Alves"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1611.03130v1",
        "title": "Computationally Efficient Target Classification in Multispectral Image\n  Data with Deep Neural Networks",
        "abstract": "  Detecting and classifying targets in video streams from surveillance cameras\nis a cumbersome, error-prone and expensive task. Often, the incurred costs are\nprohibitive for real-time monitoring. This leads to data being stored locally\nor transmitted to a central storage site for post-incident examination. The\nrequired communication links and archiving of the video data are still\nexpensive and this setup excludes preemptive actions to respond to imminent\nthreats. An effective way to overcome these limitations is to build a smart\ncamera that transmits alerts when relevant video sequences are detected. Deep\nneural networks (DNNs) have come to outperform humans in visual classifications\ntasks. The concept of DNNs and Convolutional Networks (ConvNets) can easily be\nextended to make use of higher-dimensional input data such as multispectral\ndata. We explore this opportunity in terms of achievable accuracy and required\ncomputational effort. To analyze the precision of DNNs for scene labeling in an\nurban surveillance scenario we have created a dataset with 8 classes obtained\nin a field experiment. We combine an RGB camera with a 25-channel VIS-NIR\nsnapshot sensor to assess the potential of multispectral image data for target\nclassification. We evaluate several new DNNs, showing that the spectral\ninformation fused together with the RGB frames can be used to improve the\naccuracy of the system or to achieve similar accuracy with a 3x smaller\ncomputation effort. We achieve a very high per-pixel accuracy of 99.1%. Even\nfor scarcely occurring, but particularly interesting classes, such as cars, 75%\nof the pixels are labeled correctly with errors occurring only around the\nborder of the objects. This high accuracy was obtained with a training set of\nonly 30 labeled images, paving the way for fast adaptation to various\napplication scenarios.\n",
        "published": "2016",
        "authors": [
            "Lukas Cavigelli",
            "Dominic Bernath",
            "Michele Magno",
            "Luca Benini"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1704.00616v2",
        "title": "Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance\n  for Action Classification and Detection",
        "abstract": "  General human action recognition requires understanding of various visual\ncues. In this paper, we propose a network architecture that computes and\nintegrates the most important visual cues for action recognition: pose, motion,\nand the raw images. For the integration, we introduce a Markov chain model\nwhich adds cues successively. The resulting approach is efficient and\napplicable to action classification as well as to spatial and temporal action\nlocalization. The two contributions clearly improve the performance over\nrespective baselines. The overall approach achieves state-of-the-art action\nclassification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover,\nit yields state-of-the-art spatio-temporal action localization results on\nUCF101 and J-HMDB.\n",
        "published": "2017",
        "authors": [
            "Mohammadreza Zolfaghari",
            "Gabriel L. Oliveira",
            "Nima Sedaghat",
            "Thomas Brox"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1207.2491v1",
        "title": "A Spectral Learning Approach to Range-Only SLAM",
        "abstract": "  We present a novel spectral learning algorithm for simultaneous localization\nand mapping (SLAM) from range data with known correspondences. This algorithm\nis an instance of a general spectral system identification framework, from\nwhich it inherits several desirable properties, including statistical\nconsistency and no local optima. Compared with popular batch optimization or\nmultiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral\napproach offers guaranteed low computational requirements and good tracking\nperformance. Compared with popular extended Kalman filter (EKF) or extended\ninformation filter (EIF) approaches, and many MHT ones, our approach does not\nneed to linearize a transition or measurement model; such linearizations can\ncause severe errors in EKFs and EIFs, and to a lesser extent MHT, particularly\nfor the highly non-Gaussian posteriors encountered in range-only SLAM. We\nprovide a theoretical analysis of our method, including finite-sample error\nbounds. Finally, we demonstrate on a real-world robotic SLAM problem that our\nalgorithm is not only theoretically justified, but works well in practice: in a\ncomparison of multiple methods, the lowest errors come from a combination of\nour algorithm with batch optimization, but our method alone produces nearly as\ngood a result at far lower computational cost.\n",
        "published": "2012",
        "authors": [
            "Byron Boots",
            "Geoffrey J. Gordon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1305.6568v1",
        "title": "Reinforcement Learning for the Soccer Dribbling Task",
        "abstract": "  We propose a reinforcement learning solution to the \\emph{soccer dribbling\ntask}, a scenario in which a soccer agent has to go from the beginning to the\nend of a region keeping possession of the ball, as an adversary attempts to\ngain possession. While the adversary uses a stationary policy, the dribbler\nlearns the best action to take at each decision point. After defining\nmeaningful variables to represent the state space, and high-level macro-actions\nto incorporate domain knowledge, we describe our application of the\nreinforcement learning algorithm \\emph{Sarsa} with CMAC for function\napproximation. Our experiments show that, after the training period, the\ndribbler is able to accomplish its task against a strong adversary around 58%\nof the time.\n",
        "published": "2013",
        "authors": [
            "Arthur Carvalho",
            "Renato Oliveira"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1502.02251v3",
        "title": "From Pixels to Torques: Policy Learning with Deep Dynamical Models",
        "abstract": "  Data-efficient learning in continuous state-action spaces using very\nhigh-dimensional observations remains a key challenge in developing fully\nautonomous systems. In this paper, we consider one instance of this challenge,\nthe pixels to torques problem, where an agent must learn a closed-loop control\npolicy from pixel information only. We introduce a data-efficient, model-based\nreinforcement learning algorithm that learns such a closed-loop policy directly\nfrom pixel information. The key ingredient is a deep dynamical model that uses\ndeep auto-encoders to learn a low-dimensional embedding of images jointly with\na predictive model in this low-dimensional feature space. Joint learning\nensures that not only static but also dynamic properties of the data are\naccounted for. This is crucial for long-term predictions, which lie at the core\nof the adaptive model predictive control strategy that we use for closed-loop\ncontrol. Compared to state-of-the-art reinforcement learning methods for\ncontinuous states and actions, our approach learns quickly, scales to\nhigh-dimensional state spaces and is an important step toward fully autonomous\nlearning from pixels to torques.\n",
        "published": "2015",
        "authors": [
            "Niklas Wahlstr\u00f6m",
            "Thomas B. Sch\u00f6n",
            "Marc Peter Deisenroth"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1502.02860v2",
        "title": "Gaussian Processes for Data-Efficient Learning in Robotics and Control",
        "abstract": "  Autonomous learning has been a promising direction in control and robotics\nfor more than a decade since data-driven learning allows to reduce the amount\nof engineering knowledge, which is otherwise required. However, autonomous\nreinforcement learning (RL) approaches typically require many interactions with\nthe system to learn controllers, which is a practical limitation in real\nsystems, such as robots, where many interactions can be impractical and time\nconsuming. To address this problem, current learning approaches typically\nrequire task-specific knowledge in form of expert demonstrations, realistic\nsimulators, pre-shaped policies, or specific knowledge about the underlying\ndynamics. In this article, we follow a different approach and speed up learning\nby extracting more information from data. In particular, we learn a\nprobabilistic, non-parametric Gaussian process transition model of the system.\nBy explicitly incorporating model uncertainty into long-term planning and\ncontroller learning our approach reduces the effects of model errors, a key\nproblem in model-based learning. Compared to state-of-the art RL our\nmodel-based policy search method achieves an unprecedented speed of learning.\nWe demonstrate its applicability to autonomous learning in real robot and\ncontrol tasks.\n",
        "published": "2015",
        "authors": [
            "Marc Peter Deisenroth",
            "Dieter Fox",
            "Carl Edward Rasmussen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1601.04549v1",
        "title": "Incremental Semiparametric Inverse Dynamics Learning",
        "abstract": "  This paper presents a novel approach for incremental semiparametric inverse\ndynamics learning. In particular, we consider the mixture of two approaches:\nParametric modeling based on rigid body dynamics equations and nonparametric\nmodeling based on incremental kernel methods, with no prior information on the\nmechanical properties of the system. This yields to an incremental\nsemiparametric approach, leveraging the advantages of both the parametric and\nnonparametric models. We validate the proposed technique learning the dynamics\nof one arm of the iCub humanoid robot.\n",
        "published": "2016",
        "authors": [
            "Raffaello Camoriano",
            "Silvio Traversaro",
            "Lorenzo Rosasco",
            "Giorgio Metta",
            "Francesco Nori"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.02899v1",
        "title": "Model-Based Policy Search for Automatic Tuning of Multivariate PID\n  Controllers",
        "abstract": "  PID control architectures are widely used in industrial applications. Despite\ntheir low number of open parameters, tuning multiple, coupled PID controllers\ncan become tedious in practice. In this paper, we extend PILCO, a model-based\npolicy search framework, to automatically tune multivariate PID controllers\npurely based on data observed on an otherwise unknown system. The system's\nstate is extended appropriately to frame the PID policy as a static state\nfeedback policy. This renders PID tuning possible as the solution of a finite\nhorizon optimal control problem without further a priori knowledge. The\nframework is applied to the task of balancing an inverted pendulum on a seven\ndegree-of-freedom robotic arm, thereby demonstrating its capabilities of fast\nand data-efficient policy learning, even on complex real world problems.\n",
        "published": "2017",
        "authors": [
            "Andreas Doerr",
            "Duy Nguyen-Tuong",
            "Alonso Marco",
            "Stefan Schaal",
            "Sebastian Trimpe"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.01932v3",
        "title": "End-to-End Learning of Semantic Grasping",
        "abstract": "  We consider the task of semantic robotic grasping, in which a robot picks up\nan object of a user-specified class using only monocular images. Inspired by\nthe two-stream hypothesis of visual reasoning, we present a semantic grasping\nframework that learns object detection, classification, and grasp planning in\nan end-to-end fashion. A \"ventral stream\" recognizes object class while a\n\"dorsal stream\" simultaneously interprets the geometric relationships necessary\nto execute successful grasps. We leverage the autonomous data collection\ncapabilities of robots to obtain a large self-supervised dataset for training\nthe dorsal stream, and use semi-supervised label propagation to train the\nventral stream with only a modest amount of human supervision. We\nexperimentally show that our approach improves upon grasping systems whose\ncomponents are not learned end-to-end, including a baseline method that uses\nbounding box detection. Furthermore, we show that jointly training our model\nwith auxiliary data consisting of non-semantic grasping data, as well as\nsemantically labeled images without grasp actions, has the potential to\nsubstantially improve semantic grasping performance.\n",
        "published": "2017",
        "authors": [
            "Eric Jang",
            "Sudheendra Vijayanarasimhan",
            "Peter Pastor",
            "Julian Ibarz",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.02796v2",
        "title": "Semi-Supervised Haptic Material Recognition for Robots using Generative\n  Adversarial Networks",
        "abstract": "  Material recognition enables robots to incorporate knowledge of material\nproperties into their interactions with everyday objects. For example, material\nrecognition opens up opportunities for clearer communication with a robot, such\nas \"bring me the metal coffee mug\", and recognizing plastic versus metal is\ncrucial when using a microwave or oven. However, collecting labeled training\ndata with a robot is often more difficult than unlabeled data. We present a\nsemi-supervised learning approach for material recognition that uses generative\nadversarial networks (GANs) with haptic features such as force, temperature,\nand vibration. Our approach achieves state-of-the-art results and enables a\nrobot to estimate the material class of household objects with ~90% accuracy\nwhen 92% of the training data are unlabeled. We explore how well this approach\ncan recognize the material of new objects and we discuss challenges facing\ngeneralization. To motivate learning from unlabeled training data, we also\ncompare results against several common supervised learning classifiers. In\naddition, we have released the dataset used for this work which consists of\ntime-series haptic measurements from a robot that conducted thousands of\ninteractions with 72 household objects.\n",
        "published": "2017",
        "authors": [
            "Zackory Erickson",
            "Sonia Chernova",
            "Charles C. Kemp"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.04683v1",
        "title": "Tensor Decompositions for Modeling Inverse Dynamics",
        "abstract": "  Modeling inverse dynamics is crucial for accurate feedforward robot control.\nThe model computes the necessary joint torques, to perform a desired movement.\nThe highly non-linear inverse function of the dynamical system can be\napproximated using regression techniques. We propose as regression method a\ntensor decomposition model that exploits the inherent three-way interaction of\npositions x velocities x accelerations. Most work in tensor factorization has\naddressed the decomposition of dense tensors. In this paper, we build upon the\ndecomposition of sparse tensors, with only small amounts of nonzero entries.\nThe decomposition of sparse tensors has successfully been used in relational\nlearning, e.g., the modeling of large knowledge graphs. Recently, the approach\nhas been extended to multi-class classification with discrete input variables.\nRepresenting the data in high dimensional sparse tensors enables the\napproximation of complex highly non-linear functions. In this paper we show how\nthe decomposition of sparse tensors can be applied to regression problems.\nFurthermore, we extend the method to continuous inputs, by learning a mapping\nfrom the continuous inputs to the latent representations of the tensor\ndecomposition, using basis functions. We evaluate our proposed model on a\ndataset with trajectories from a seven degrees of freedom SARCOS robot arm. Our\nexperimental results show superior performance of the proposed functional\ntensor model, compared to challenging state-of-the art methods.\n",
        "published": "2017",
        "authors": [
            "Stephan Baier",
            "Volker Tresp"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.06179v1",
        "title": "Learning to Race through Coordinate Descent Bayesian Optimisation",
        "abstract": "  In the automation of many kinds of processes, the observable outcome can\noften be described as the combined effect of an entire sequence of actions, or\ncontrols, applied throughout its execution. In these cases, strategies to\noptimise control policies for individual stages of the process might not be\napplicable, and instead the whole policy might have to be optimised at once. On\nthe other hand, the cost to evaluate the policy's performance might also be\nhigh, being desirable that a solution can be found with as few interactions as\npossible with the real system. We consider the problem of optimising control\npolicies to allow a robot to complete a given race track within a minimum\namount of time. We assume that the robot has no prior information about the\ntrack or its own dynamical model, just an initial valid driving example.\nLocalisation is only applied to monitor the robot and to provide an indication\nof its position along the track's centre axis. We propose a method for finding\na policy that minimises the time per lap while keeping the vehicle on the track\nusing a Bayesian optimisation (BO) approach over a reproducing kernel Hilbert\nspace. We apply an algorithm to search more efficiently over high-dimensional\npolicy-parameter spaces with BO, by iterating over each dimension individually,\nin a sequential coordinate descent-like scheme. Experiments demonstrate the\nperformance of the algorithm against other methods in a simulated car racing\nenvironment.\n",
        "published": "2018",
        "authors": [
            "Rafael Oliveira",
            "Fernando H. M. Rocha",
            "Lionel Ott",
            "Vitor Guizilini",
            "Fabio Ramos",
            "Valdir Grassi Jr"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.08678v2",
        "title": "Verifying Controllers Against Adversarial Examples with Bayesian\n  Optimization",
        "abstract": "  Recent successes in reinforcement learning have lead to the development of\ncomplex controllers for real-world robots. As these robots are deployed in\nsafety-critical applications and interact with humans, it becomes critical to\nensure safety in order to avoid causing harm. A first step in this direction is\nto test the controllers in simulation. To be able to do this, we need to\ncapture what we mean by safety and then efficiently search the space of all\nbehaviors to see if they are safe. In this paper, we present an active-testing\nframework based on Bayesian Optimization. We specify safety constraints using\nlogic and exploit structure in the problem in order to test the system for\nadversarial counter examples that violate the safety specifications. These\nspecifications are defined as complex boolean combinations of smooth functions\non the trajectories and, unlike reward functions in reinforcement learning, are\nexpressive and impose hard constraints on the system. In our framework, we\nexploit regularity assumptions on individual functions in form of a Gaussian\nProcess (GP) prior. We combine these into a coherent optimization framework\nusing problem structure. The resulting algorithm is able to provably verify\ncomplex safety specifications or alternatively find counter examples.\nExperimental results show that the proposed method is able to find adversarial\nexamples quickly.\n",
        "published": "2018",
        "authors": [
            "Shromona Ghosh",
            "Felix Berkenkamp",
            "Gireeja Ranade",
            "Shaz Qadeer",
            "Ashish Kapoor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.10264v2",
        "title": "Deep Reinforcement Learning for Vision-Based Robotic Grasping: A\n  Simulated Comparative Evaluation of Off-Policy Methods",
        "abstract": "  In this paper, we explore deep reinforcement learning algorithms for\nvision-based robotic grasping. Model-free deep reinforcement learning (RL) has\nbeen successfully applied to a range of challenging environments, but the\nproliferation of algorithms makes it difficult to discern which particular\napproach would be best suited for a rich, diverse task like grasping. To answer\nthis question, we propose a simulated benchmark for robotic grasping that\nemphasizes off-policy learning and generalization to unseen objects. Off-policy\nlearning enables utilization of grasping data over a wide variety of objects,\nand diversity is important to enable the method to generalize to new objects\nthat were not seen during training. We evaluate the benchmark tasks against a\nvariety of Q-function estimation methods, a method previously proposed for\nrobotic grasping with deep neural network models, and a novel approach based on\na combination of Monte Carlo return estimation and an off-policy correction.\nOur results indicate that several simple methods provide a surprisingly strong\ncompetitor to popular algorithms such as double Q-learning, and our analysis of\nstability sheds light on the relative tradeoffs between the algorithms.\n",
        "published": "2018",
        "authors": [
            "Deirdre Quillen",
            "Eric Jang",
            "Ofir Nachum",
            "Chelsea Finn",
            "Julian Ibarz",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.10567v1",
        "title": "Learning by Playing - Solving Sparse Reward Tasks from Scratch",
        "abstract": "  We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in\nthe context of Reinforcement Learning (RL). SAC-X enables learning of complex\nbehaviors - from scratch - in the presence of multiple sparse reward signals.\nTo this end, the agent is equipped with a set of general auxiliary tasks, that\nit attempts to learn simultaneously via off-policy RL. The key idea behind our\nmethod is that active (learned) scheduling and execution of auxiliary policies\nallows the agent to efficiently explore its environment - enabling it to excel\nat sparse reward RL. Our experiments in several challenging robotic\nmanipulation settings demonstrate the power of our approach.\n",
        "published": "2018",
        "authors": [
            "Martin Riedmiller",
            "Roland Hafner",
            "Thomas Lampe",
            "Michael Neunert",
            "Jonas Degrave",
            "Tom Van de Wiele",
            "Volodymyr Mnih",
            "Nicolas Heess",
            "Jost Tobias Springenberg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.11347v6",
        "title": "Learning to Adapt in Dynamic, Real-World Environments Through\n  Meta-Reinforcement Learning",
        "abstract": "  Although reinforcement learning methods can achieve impressive results in\nsimulation, the real world presents two major challenges: generating samples is\nexceedingly expensive, and unexpected perturbations or unseen situations cause\nproficient but specialized policies to fail at test time. Given that it is\nimpractical to train separate policies to accommodate all situations the agent\nmay see in the real world, this work proposes to learn how to quickly and\neffectively adapt online to new tasks. To enable sample-efficient learning, we\nconsider learning online adaptation in the context of model-based reinforcement\nlearning. Our approach uses meta-learning to train a dynamics model prior such\nthat, when combined with recent data, this prior can be rapidly adapted to the\nlocal context. Our experiments demonstrate online adaptation for continuous\ncontrol tasks on both simulated and real-world agents. We first show simulated\nagents adapting their behavior online to novel terrains, crippled body parts,\nand highly-dynamic environments. We also illustrate the importance of\nincorporating online adaptation into autonomous agents that operate in the real\nworld by applying our method to a real dynamic legged millirobot. We\ndemonstrate the agent's learned ability to quickly adapt online to a missing\nleg, adjust to novel terrains and slopes, account for miscalibration or errors\nin pose estimation, and compensate for pulling payloads.\n",
        "published": "2018",
        "authors": [
            "Anusha Nagabandi",
            "Ignasi Clavera",
            "Simin Liu",
            "Ronald S. Fearing",
            "Pieter Abbeel",
            "Sergey Levine",
            "Chelsea Finn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.06063v3",
        "title": "Probabilistic Trajectory Segmentation by Means of Hierarchical Dirichlet\n  Process Switching Linear Dynamical Systems",
        "abstract": "  Using movement primitive libraries is an effective means to enable robots to\nsolve more complex tasks. In order to build these movement libraries, current\nalgorithms require a prior segmentation of the demonstration trajectories. A\npromising approach is to model the trajectory as being generated by a set of\nSwitching Linear Dynamical Systems and inferring a meaningful segmentation by\ninspecting the transition points characterized by the switching dynamics. With\nrespect to the learning, a nonparametric Bayesian approach is employed\nutilizing a Gibbs sampler.\n",
        "published": "2018",
        "authors": [
            "Maximilian Sieb",
            "Matthias Schultheis",
            "Sebastian Szelag",
            "Rudolf Lioutikov",
            "Jan Peters"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.10166v2",
        "title": "Modular meta-learning",
        "abstract": "  Many prediction problems, such as those that arise in the context of\nrobotics, have a simplifying underlying structure that, if known, could\naccelerate learning. In this paper, we present a strategy for learning a set of\nneural network modules that can be combined in different ways. We train\ndifferent modular structures on a set of related tasks and generalize to new\ntasks by composing the learned modules in new ways. By reusing modules to\ngeneralize we achieve combinatorial generalization, akin to the \"infinite use\nof finite means\" displayed in language. Finally, we show this improves\nperformance in two robotics-related problems.\n",
        "published": "2018",
        "authors": [
            "Ferran Alet",
            "Tom\u00e1s Lozano-P\u00e9rez",
            "Leslie P. Kaelbling"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.11244v3",
        "title": "One-Shot Learning of Multi-Step Tasks from Observation via Activity\n  Localization in Auxiliary Video",
        "abstract": "  Due to burdensome data requirements, learning from demonstration often falls\nshort of its promise to allow users to quickly and naturally program robots.\nDemonstrations are inherently ambiguous and incomplete, making correct\ngeneralization to unseen situations difficult without a large number of\ndemonstrations in varying conditions. By contrast, humans are often able to\nlearn complex tasks from a single demonstration (typically observations without\naction labels) by leveraging context learned over a lifetime. Inspired by this\ncapability, our goal is to enable robots to perform one-shot learning of\nmulti-step tasks from observation by leveraging auxiliary video data as\ncontext. Our primary contribution is a novel system that achieves this goal by:\n(1) using a single user-segmented demonstration to define the primitive actions\nthat comprise a task, (2) localizing additional examples of these actions in\nunsegmented auxiliary videos via a metalearning-based approach, (3) using these\nadditional examples to learn a reward function for each action, and (4)\nperforming reinforcement learning on top of the inferred reward functions to\nlearn action policies that can be combined to accomplish the task. We\nempirically demonstrate that a robot can learn multi-step tasks more\neffectively when provided auxiliary video, and that performance greatly\nimproves when localizing individual actions, compared to learning from\nunsegmented videos.\n",
        "published": "2018",
        "authors": [
            "Wonjoon Goo",
            "Scott Niekum"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.02128v4",
        "title": "Adaptive Path-Integral Autoencoder: Representation Learning and Planning\n  for Dynamical Systems",
        "abstract": "  We present a representation learning algorithm that learns a low-dimensional\nlatent dynamical system from high-dimensional \\textit{sequential} raw data,\ne.g., video. The framework builds upon recent advances in amortized inference\nmethods that use both an inference network and a refinement procedure to output\nsamples from a variational distribution given an observation sequence, and\ntakes advantage of the duality between control and inference to approximately\nsolve the intractable inference problem using the path integral control\napproach. The learned dynamical model can be used to predict and plan the\nfuture states; we also present the efficient planning method that exploits the\nlearned low-dimensional latent dynamics. Numerical experiments show that the\nproposed path-integral control based variational inference method leads to\ntighter lower bounds in statistical model learning of sequential data. The\nsupplementary video: https://youtu.be/xCp35crUoLQ\n",
        "published": "2018",
        "authors": [
            "Jung-Su Ha",
            "Young-Jin Park",
            "Hyeok-Joo Chae",
            "Soon-Seo Park",
            "Han-Lim Choi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.04109v1",
        "title": "Modeling and Soft-fault Diagnosis of Underwater Thrusters with Recurrent\n  Neural Networks",
        "abstract": "  Noncritical soft-faults and model deviations are a challenge for Fault\nDetection and Diagnosis (FDD) of resident Autonomous Underwater Vehicles\n(AUVs). Such systems may have a faster performance degradation due to the\npermanent exposure to the marine environment, and constant monitoring of\ncomponent conditions is required to ensure their reliability. This works\npresents an evaluation of Recurrent Neural Networks (RNNs) for a data-driven\nfault detection and diagnosis scheme for underwater thrusters with empirical\ndata. The nominal behavior of the thruster was modeled using the measured\ncontrol input, voltage, rotational speed and current signals. We evaluated the\nperformance of fault classification using all the measured signals compared to\nusing the computed residuals from the nominal model as features.\n",
        "published": "2018",
        "authors": [
            "Samy Nascimento",
            "Matias Valdenegro-Toro"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.00145v3",
        "title": "Scalable End-to-End Autonomous Vehicle Testing via Rare-event Simulation",
        "abstract": "  While recent developments in autonomous vehicle (AV) technology highlight\nsubstantial progress, we lack tools for rigorous and scalable testing.\nReal-world testing, the $\\textit{de facto}$ evaluation environment, places the\npublic in danger, and, due to the rare nature of accidents, will require\nbillions of miles in order to statistically validate performance claims. We\nimplement a simulation framework that can test an entire modern autonomous\ndriving system, including, in particular, systems that employ deep-learning\nperception and control algorithms. Using adaptive importance-sampling methods\nto accelerate rare-event probability evaluation, we estimate the probability of\nan accident under a base distribution governing standard traffic behavior. We\ndemonstrate our framework on a highway scenario, accelerating system evaluation\nby $2$-$20$ times over naive Monte Carlo sampling methods and $10$-$300\n\\mathsf{P}$ times (where $\\mathsf{P}$ is the number of processors) over\nreal-world testing.\n",
        "published": "2018",
        "authors": [
            "Matthew O'Kelly",
            "Aman Sinha",
            "Hongseok Namkoong",
            "John Duchi",
            "Russ Tedrake"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.04539v1",
        "title": "Adversarial Learning-Based On-Line Anomaly Monitoring for Assured\n  Autonomy",
        "abstract": "  The paper proposes an on-line monitoring framework for continuous real-time\nsafety/security in learning-based control systems (specifically application to\na unmanned ground vehicle). We monitor validity of mappings from sensor inputs\nto actuator commands, controller-focused anomaly detection (CFAM), and from\nactuator commands to sensor inputs, system-focused anomaly detection (SFAM).\nCFAM is an image conditioned energy based generative adversarial network\n(EBGAN) in which the energy based discriminator distinguishes between proper\nand anomalous actuator commands. SFAM is based on an action condition video\nprediction framework to detect anomalies between predicted and observed\ntemporal evolution of sensor data. We demonstrate the effectiveness of the\napproach on our autonomous ground vehicle for indoor environments and on\nUdacity dataset for outdoor environments.\n",
        "published": "2018",
        "authors": [
            "Naman Patel",
            "Apoorva Nandini Saridena",
            "Anna Choromanska",
            "Prashanth Krishnamurthy",
            "Farshad Khorrami"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.01291v2",
        "title": "On the Utility of Model Learning in HRI",
        "abstract": "  Fundamental to robotics is the debate between model-based and model-free\nlearning: should the robot build an explicit model of the world, or learn a\npolicy directly? In the context of HRI, part of the world to be modeled is the\nhuman. One option is for the robot to treat the human as a black box and learn\na policy for how they act directly. But it can also model the human as an\nagent, and rely on a \"theory of mind\" to guide or bias the learning (grey box).\nWe contribute a characterization of the performance of these methods for an\nautonomous driving task under the optimistic case of having an ideal theory of\nmind, as well as under different scenarios in which the assumptions behind the\nrobot's theory of mind for the human are wrong, as they inevitably will be in\npractice.\n",
        "published": "2019",
        "authors": [
            "Gokul Swamy",
            "Jens Schulz",
            "Rohan Choudhury",
            "Dylan Hadfield-Menell",
            "Anca Dragan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.01536v3",
        "title": "Exploring applications of deep reinforcement learning for real-world\n  autonomous driving systems",
        "abstract": "  Deep Reinforcement Learning (DRL) has become increasingly powerful in recent\nyears, with notable achievements such as Deepmind's AlphaGo. It has been\nsuccessfully deployed in commercial vehicles like Mobileye's path planning\nsystem. However, a vast majority of work on DRL is focused on toy examples in\ncontrolled synthetic car simulator environments such as TORCS and CARLA. In\ngeneral, DRL is still at its infancy in terms of usability in real-world\napplications. Our goal in this paper is to encourage real-world deployment of\nDRL in various autonomous driving (AD) applications. We first provide an\noverview of the tasks in autonomous driving systems, reinforcement learning\nalgorithms and applications of DRL to AD systems. We then discuss the\nchallenges which must be addressed to enable further progress towards\nreal-world deployment.\n",
        "published": "2019",
        "authors": [
            "Victor Talpaert",
            "Ibrahim Sobh",
            "B Ravi Kiran",
            "Patrick Mannion",
            "Senthil Yogamani",
            "Ahmad El-Sallab",
            "Patrick Perez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.06181v1",
        "title": "TactileGCN: A Graph Convolutional Network for Predicting Grasp Stability\n  with Tactile Sensors",
        "abstract": "  Tactile sensors provide useful contact data during the interaction with an\nobject which can be used to accurately learn to determine the stability of a\ngrasp. Most of the works in the literature represented tactile readings as\nplain feature vectors or matrix-like tactile images, using them to train\nmachine learning models. In this work, we explore an alternative way of\nexploiting tactile information to predict grasp stability by leveraging\ngraph-like representations of tactile data, which preserve the actual spatial\narrangement of the sensor's taxels and their locality. In experimentation, we\ntrained a Graph Neural Network to binary classify grasps as stable or slippery\nones. To train such network and prove its predictive capabilities for the\nproblem at hand, we captured a novel dataset of approximately 5000\nthree-fingered grasps across 41 objects for training and 1000 grasps with 10\nunknown objects for testing. Our experiments prove that this novel approach can\nbe effectively used to predict grasp stability.\n",
        "published": "2019",
        "authors": [
            "Alberto Garcia-Garcia",
            "Brayan Stiven Zapata-Impata",
            "Sergio Orts-Escolano",
            "Pablo Gil",
            "Jose Garcia-Rodriguez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.07186v4",
        "title": "Towards Learning to Imitate from a Single Video Demonstration",
        "abstract": "  Agents that can learn to imitate given video observation -- \\emph{without\ndirect access to state or action information} are more applicable to learning\nin the natural world. However, formulating a reinforcement learning (RL) agent\nthat facilitates this goal remains a significant challenge. We approach this\nchallenge using contrastive training to learn a reward function comparing an\nagent's behaviour with a single demonstration. We use a Siamese recurrent\nneural network architecture to learn rewards in space and time between motion\nclips while training an RL policy to minimize this distance. Through\nexperimentation, we also find that the inclusion of multi-task data and\nadditional image encoding losses improve the temporal consistency of the\nlearned rewards and, as a result, significantly improves policy learning. We\ndemonstrate our approach on simulated humanoid, dog, and raptor agents in 2D\nand a quadruped and a humanoid in 3D. We show that our method outperforms\ncurrent state-of-the-art techniques in these environments and can learn to\nimitate from a single video demonstration.\n",
        "published": "2019",
        "authors": [
            "Glen Berseth",
            "Florian Golemo",
            "Christopher Pal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.08651v3",
        "title": "Decoupling feature extraction from policy learning: assessing benefits\n  of state representation learning in goal based robotics",
        "abstract": "  Scaling end-to-end reinforcement learning to control real robots from vision\npresents a series of challenges, in particular in terms of sample efficiency.\nAgainst end-to-end learning, state representation learning can help learn a\ncompact, efficient and relevant representation of states that speeds up policy\nlearning, reducing the number of samples needed, and that is easier to\ninterpret. We evaluate several state representation learning methods on goal\nbased robotics tasks and propose a new unsupervised model that stacks\nrepresentations and combines strengths of several of these approaches. This\nmethod encodes all the relevant features, performs on par or better than\nend-to-end learning with better sample efficiency, and is robust to\nhyper-parameters change.\n",
        "published": "2019",
        "authors": [
            "Antonin Raffin",
            "Ashley Hill",
            "Ren\u00e9 Traor\u00e9",
            "Timoth\u00e9e Lesort",
            "Natalia D\u00edaz-Rodr\u00edguez",
            "David Filliat"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.08652v1",
        "title": "Learning agile and dynamic motor skills for legged robots",
        "abstract": "  Legged robots pose one of the greatest challenges in robotics. Dynamic and\nagile maneuvers of animals cannot be imitated by existing methods that are\ncrafted by humans. A compelling alternative is reinforcement learning, which\nrequires minimal craftsmanship and promotes the natural evolution of a control\npolicy. However, so far, reinforcement learning research for legged robots is\nmainly limited to simulation, and only few and comparably simple examples have\nbeen deployed on real systems. The primary reason is that training with real\nrobots, particularly with dynamically balancing systems, is complicated and\nexpensive. In the present work, we introduce a method for training a neural\nnetwork policy in simulation and transferring it to a state-of-the-art legged\nsystem, thereby leveraging fast, automated, and cost-effective data generation\nschemes. The approach is applied to the ANYmal robot, a sophisticated\nmedium-dog-sized quadrupedal system. Using policies trained in simulation, the\nquadrupedal machine achieves locomotion skills that go beyond what had been\nachieved with prior methods: ANYmal is capable of precisely and\nenergy-efficiently following high-level body velocity commands, running faster\nthan before, and recovering from falling even in complex configurations.\n",
        "published": "2019",
        "authors": [
            "Jemin Hwangbo",
            "Joonho Lee",
            "Alexey Dosovitskiy",
            "Dario Bellicoso",
            "Vassilios Tsounis",
            "Vladlen Koltun",
            "Marco Hutter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.11212v2",
        "title": "A Data Driven Method of Optimizing Feedforward Compensator for\n  Autonomous Vehicle",
        "abstract": "  A reliable controller is critical and essential for the execution of safe and\nsmooth maneuvers of an autonomous vehicle.The controller must be robust to\nexternal disturbances, such as road surface, weather, and wind conditions, and\nso on.It also needs to deal with the internal parametric variations of vehicle\nsub-systems, including power-train efficiency, measurement errors, time\ndelay,so on.Moreover, as in most production vehicles, the low-control commands\nfor the engine, brake, and steering systems are delivered through separate\nelectronic control units.These aforementioned factors introduce opaque and\nineffectiveness issues in controller performance.In this paper, we design a\nfeed-forward compensate process via a data-driven method to model and further\noptimize the controller performance.We apply the principal component analysis\nto the extraction of most influential features.Subsequently,we adopt a time\ndelay neural network and include the accuracy of the predicted error in a\nfuture time horizon.Utilizing the predicted error,we then design a feed-forward\ncompensate process to improve the control performance.Finally,we demonstrate\nthe effectiveness of the proposed feed-forward compensate process in simulation\nscenarios.\n",
        "published": "2019",
        "authors": [
            "Tianyu Shi",
            "Pin Wang",
            "Ching-Yao Chan",
            "Chonghao Zou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.01670v2",
        "title": "Interactively shaping robot behaviour with unlabeled human instructions",
        "abstract": "  In this paper, we propose a framework that enables a human teacher to shape a\nrobot behaviour by interactively providing it with unlabeled instructions. We\nground the meaning of instruction signals in the task-learning process, and use\nthem simultaneously for guiding the latter. We implement our framework as a\nmodular architecture, named TICS (Task-Instruction-Contingency-Shaping) that\ncombines different information sources: a predefined reward function, human\nevaluative feedback and unlabeled instructions. This approach provides a novel\nperspective for robotic task learning that lies between Reinforcement Learning\nand Supervised Learning paradigms. We evaluate our framework both in simulation\nand with a real robot. The experimental results demonstrate the effectiveness\nof our framework in accelerating the task-learning process and in reducing the\nnumber of required teaching signals.\n",
        "published": "2019",
        "authors": [
            "Anis Najar",
            "Olivier Sigaud",
            "Mohamed Chetouani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.02441v1",
        "title": "Artificial Intelligence for Prosthetics - challenge solutions",
        "abstract": "  In the NeurIPS 2018 Artificial Intelligence for Prosthetics challenge,\nparticipants were tasked with building a controller for a musculoskeletal model\nwith a goal of matching a given time-varying velocity vector. Top participants\nwere invited to describe their algorithms. In this work, we describe the\nchallenge and present thirteen solutions that used deep reinforcement learning\napproaches. Many solutions use similar relaxations and heuristics, such as\nreward shaping, frame skipping, discretization of the action space, symmetry,\nand policy blending. However, each team implemented different modifications of\nthe known algorithms by, for example, dividing the task into subtasks, learning\nlow-level control, or by incorporating expert knowledge and using imitation\nlearning.\n",
        "published": "2019",
        "authors": [
            "\u0141ukasz Kidzi\u0144ski",
            "Carmichael Ong",
            "Sharada Prasanna Mohanty",
            "Jennifer Hicks",
            "Sean F. Carroll",
            "Bo Zhou",
            "Hongsheng Zeng",
            "Fan Wang",
            "Rongzhong Lian",
            "Hao Tian",
            "Wojciech Ja\u015bkowski",
            "Garrett Andersen",
            "Odd Rune Lykkeb\u00f8",
            "Nihat Engin Toklu",
            "Pranav Shyam",
            "Rupesh Kumar Srivastava",
            "Sergey Kolesnikov",
            "Oleksii Hrinchuk",
            "Anton Pechenko",
            "Mattias Ljungstr\u00f6m",
            "Zhen Wang",
            "Xu Hu",
            "Zehong Hu",
            "Minghui Qiu",
            "Jun Huang",
            "Aleksei Shpilman",
            "Ivan Sosin",
            "Oleg Svidchenko",
            "Aleksandra Malysheva",
            "Daniel Kudenko",
            "Lance Rane",
            "Aditya Bhatt",
            "Zhengfei Wang",
            "Penghui Qi",
            "Zeyang Yu",
            "Peng Peng",
            "Quan Yuan",
            "Wenxin Li",
            "Yunsheng Tian",
            "Ruihan Yang",
            "Pingchuan Ma",
            "Shauharda Khadka",
            "Somdeb Majumdar",
            "Zach Dwiel",
            "Yinyin Liu",
            "Evren Tumer",
            "Jeremy Watson",
            "Marcel Salath\u00e9",
            "Sergey Levine",
            "Scott Delp"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.03701v1",
        "title": "Generalization through Simulation: Integrating Simulated and Real Data\n  into Deep Reinforcement Learning for Vision-Based Autonomous Flight",
        "abstract": "  Deep reinforcement learning provides a promising approach for vision-based\ncontrol of real-world robots. However, the generalization of such models\ndepends critically on the quantity and variety of data available for training.\nThis data can be difficult to obtain for some types of robotic systems, such as\nfragile, small-scale quadrotors. Simulated rendering and physics can provide\nfor much larger datasets, but such data is inherently of lower quality: many of\nthe phenomena that make the real-world autonomous flight problem challenging,\nsuch as complex physics and air currents, are modeled poorly or not at all, and\nthe systematic differences between simulation and the real world are typically\nimpossible to eliminate. In this work, we investigate how data from both\nsimulation and the real world can be combined in a hybrid deep reinforcement\nlearning algorithm. Our method uses real-world data to learn about the dynamics\nof the system, and simulated data to learn a generalizable perception system\nthat can enable the robot to avoid collisions using only a monocular camera. We\ndemonstrate our approach on a real-world nano aerial vehicle collision\navoidance task, showing that with only an hour of real-world data, the\nquadrotor can avoid collisions in new environments with various lighting\nconditions and geometry. Code, instructions for building the aerial vehicles,\nand videos of the experiments can be found at github.com/gkahn13/GtS\n",
        "published": "2019",
        "authors": [
            "Katie Kang",
            "Suneel Belkhale",
            "Gregory Kahn",
            "Pieter Abbeel",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.04706v2",
        "title": "Simultaneously Learning Vision and Feature-based Control Policies for\n  Real-world Ball-in-a-Cup",
        "abstract": "  We present a method for fast training of vision based control policies on\nreal robots. The key idea behind our method is to perform multi-task\nReinforcement Learning with auxiliary tasks that differ not only in the reward\nto be optimized but also in the state-space in which they operate. In\nparticular, we allow auxiliary task policies to utilize task features that are\navailable only at training-time. This allows for fast learning of auxiliary\npolicies, which subsequently generate good data for training the main,\nvision-based control policies. This method can be seen as an extension of the\nScheduled Auxiliary Control (SAC-X) framework. We demonstrate the efficacy of\nour method by using both a simulated and real-world Ball-in-a-Cup game\ncontrolled by a robot arm. In simulation, our approach leads to significant\nlearning speed-ups when compared to standard SAC-X. On the real robot we show\nthat the task can be learned from-scratch, i.e., with no transfer from\nsimulation and no imitation learning. Videos of our learned policies running on\nthe real robot can be found at\nhttps://sites.google.com/view/rss-2019-sawyer-bic/.\n",
        "published": "2019",
        "authors": [
            "Devin Schwab",
            "Tobias Springenberg",
            "Murilo F. Martins",
            "Thomas Lampe",
            "Michael Neunert",
            "Abbas Abdolmaleki",
            "Tim Hertweck",
            "Roland Hafner",
            "Francesco Nori",
            "Martin Riedmiller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.07908v1",
        "title": "Bayesian optimisation under uncertain inputs",
        "abstract": "  Bayesian optimisation (BO) has been a successful approach to optimise\nfunctions which are expensive to evaluate and whose observations are noisy.\nClassical BO algorithms, however, do not account for errors about the location\nwhere observations are taken, which is a common issue in problems with physical\ncomponents. In these cases, the estimation of the actual query location is also\nsubject to uncertainty. In this context, we propose an upper confidence bound\n(UCB) algorithm for BO problems where both the outcome of a query and the true\nquery location are uncertain. The algorithm employs a Gaussian process model\nthat takes probability distributions as inputs. Theoretical results are\nprovided for both the proposed algorithm and a conventional UCB approach within\nthe uncertain-inputs setting. Finally, we evaluate each method's performance\nexperimentally, comparing them to other input noise aware BO approaches on\nsimulated scenarios involving synthetic and real data.\n",
        "published": "2019",
        "authors": [
            "Rafael Oliveira",
            "Lionel Ott",
            "Fabio Ramos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.09068v1",
        "title": "A Driving Intention Prediction Method Based on Hidden Markov Model for\n  Autonomous Driving",
        "abstract": "  In a mixed-traffic scenario where both autonomous vehicles and human-driving\nvehicles exist, a timely prediction of driving intentions of nearby\nhuman-driving vehicles is essential for the safe and efficient driving of an\nautonomous vehicle. In this paper, a driving intention prediction method based\non Hidden Markov Model (HMM) is proposed for autonomous vehicles. HMMs\nrepresenting different driving intentions are trained and tested with field\ncollected data from a flyover. When training the models, either discrete or\ncontinuous characterization of the mobility features of vehicles is applied.\nExperimental results show that the HMMs trained with the continuous\ncharacterization of mobility features can give a higher prediction accuracy\nwhen they are used for predicting driving intentions. Moreover, when the\nsurrounding traffic of the vehicle is taken into account, the performances of\nthe proposed prediction method are further improved.\n",
        "published": "2019",
        "authors": [
            "Shiwen Liu",
            "Kan Zheng",
            "Long Zhao",
            "Pingzhi Fan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.01738v2",
        "title": "State Representation Learning from Demonstration",
        "abstract": "  Robots could learn their own state and world representation from perception\nand experience without supervision. This desirable goal is the main focus of\nour field of interest, state representation learning (SRL). Indeed, a compact\nrepresentation of such a state is beneficial to help robots grasp onto their\nenvironment for interacting. The properties of this representation have a\nstrong impact on the adaptive capability of the agent. In this article we\npresent an approach based on imitation learning. The idea is to train several\npolicies that share the same representation to reproduce various\ndemonstrations. To do so, we use a multi-head neural network with a shared\nstate representation feeding a task-specific agent. If the demonstrations are\ndiverse, the trained representation will eventually contain the information\nnecessary for all tasks, while discarding irrelevant information. As such, it\nwill potentially become a compact state representation useful for new tasks. We\ncall this approach SRLfD (State Representation Learning from Demonstration).\nOur experiments confirm that when a controller takes SRLfD-based\nrepresentations as input, it can achieve better performance than with other\nrepresentation strategies and promote more efficient reinforcement learning\n(RL) than with an end-to-end RL strategy.\n",
        "published": "2019",
        "authors": [
            "Astrid Merckling",
            "Alexandre Coninx",
            "Loic Cressot",
            "St\u00e9phane Doncieux",
            "Nicolas Perrin-Gilbert"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.02787v3",
        "title": "Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping",
        "abstract": "  The distributional perspective on reinforcement learning (RL) has given rise\nto a series of successful Q-learning algorithms, resulting in state-of-the-art\nperformance in arcade game environments. However, it has not yet been analyzed\nhow these findings from a discrete setting translate to complex practical\napplications characterized by noisy, high dimensional and continuous\nstate-action spaces. In this work, we propose Quantile QT-Opt (Q2-Opt), a\ndistributional variant of the recently introduced distributed Q-learning\nalgorithm for continuous domains, and examine its behaviour in a series of\nsimulated and real vision-based robotic grasping tasks. The absence of an actor\nin Q2-Opt allows us to directly draw a parallel to the previous discrete\nexperiments in the literature without the additional complexities induced by an\nactor-critic architecture. We demonstrate that Q2-Opt achieves a superior\nvision-based object grasping success rate, while also being more sample\nefficient. The distributional formulation also allows us to experiment with\nvarious risk distortion metrics that give us an indication of how robots can\nconcretely manage risk in practice using a Deep RL control policy. As an\nadditional contribution, we perform batch RL experiments in our virtual\nenvironment and compare them with the latest findings from discrete settings.\nSurprisingly, we find that the previous batch RL findings from the literature\nobtained on arcade game environments do not generalise to our setup.\n",
        "published": "2019",
        "authors": [
            "Cristian Bodnar",
            "Adrian Li",
            "Karol Hausman",
            "Peter Pastor",
            "Mrinal Kalakrishnan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.02910v2",
        "title": "Scaled Autonomy: Enabling Human Operators to Control Robot Fleets",
        "abstract": "  Autonomous robots often encounter challenging situations where their control\npolicies fail and an expert human operator must briefly intervene, e.g.,\nthrough teleoperation. In settings where multiple robots act in separate\nenvironments, a single human operator can manage a fleet of robots by\nidentifying and teleoperating one robot at any given time. The key challenge is\nthat users have limited attention: as the number of robots increases, users\nlose the ability to decide which robot requires teleoperation the most. Our\ngoal is to automate this decision, thereby enabling users to supervise more\nrobots than their attention would normally allow for. Our insight is that we\ncan model the user's choice of which robot to control as an approximately\noptimal decision that maximizes the user's utility function. We learn a model\nof the user's preferences from observations of the user's choices in easy\nsettings with a few robots, and use it in challenging settings with more robots\nto automatically identify which robot the user would most likely choose to\ncontrol, if they were able to evaluate the states of all robots at all times.\nWe run simulation experiments and a user study with twelve participants that\nshow our method can be used to assist users in performing a simulated\nnavigation task. We also run a hardware demonstration that illustrates how our\nmethod can be applied to a real-world mobile robot navigation task.\n",
        "published": "2019",
        "authors": [
            "Gokul Swamy",
            "Siddharth Reddy",
            "Sergey Levine",
            "Anca D. Dragan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.03358v1",
        "title": "Deep Value Model Predictive Control",
        "abstract": "  In this paper, we introduce an actor-critic algorithm called Deep Value Model\nPredictive Control (DMPC), which combines model-based trajectory optimization\nwith value function estimation. The DMPC actor is a Model Predictive Control\n(MPC) optimizer with an objective function defined in terms of a value function\nestimated by the critic. We show that our MPC actor is an importance sampler,\nwhich minimizes an upper bound of the cross-entropy to the state distribution\nof the optimal sampling policy. In our experiments with a Ballbot system, we\nshow that our algorithm can work with sparse and binary reward signals to\nefficiently solve obstacle avoidance and target reaching tasks. Compared to\nprevious work, we show that including the value function in the running cost of\nthe trajectory optimizer speeds up the convergence. We also discuss the\nnecessary strategies to robustify the algorithm in practice.\n",
        "published": "2019",
        "authors": [
            "Farbod Farshidian",
            "David Hoeller",
            "Marco Hutter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.03620v1",
        "title": "Receding Horizon Curiosity",
        "abstract": "  Sample-efficient exploration is crucial not only for discovering rewarding\nexperiences but also for adapting to environment changes in a task-agnostic\nfashion. A principled treatment of the problem of optimal input synthesis for\nsystem identification is provided within the framework of sequential Bayesian\nexperimental design. In this paper, we present an effective\ntrajectory-optimization-based approximate solution of this otherwise\nintractable problem that models optimal exploration in an unknown Markov\ndecision process (MDP). By interleaving episodic exploration with Bayesian\nnonlinear system identification, our algorithm takes advantage of the inductive\nbias to explore in a directed manner, without assuming prior knowledge of the\nMDP. Empirical evaluations indicate a clear advantage of the proposed algorithm\nin terms of the rate of convergence and the final model fidelity when compared\nto intrinsic-motivation-based algorithms employing exploration bonuses such as\nprediction error and information gain. Moreover, our method maintains a\ncomputational advantage over a recent model-based active exploration (MAX)\nalgorithm, by focusing on the information gain along trajectories instead of\nseeking a global exploration policy. A reference implementation of our\nalgorithm and the conducted experiments is publicly available.\n",
        "published": "2019",
        "authors": [
            "Matthias Schultheis",
            "Boris Belousov",
            "Hany Abdulsamad",
            "Jan Peters"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.03644v2",
        "title": "Stochastic triangular mesh mapping: A terrain mapping technique for\n  autonomous mobile robots",
        "abstract": "  For mobile robots to operate autonomously in general environments, perception\nis required in the form of a dense metric map. For this purpose, we present the\nstochastic triangular mesh (STM) mapping technique: a 2.5-D representation of\nthe surface of the environment using a continuous mesh of triangular surface\nelements, where each surface element models the mean plane and roughness of the\nunderlying surface. In contrast to existing mapping techniques, a STM map\nmodels the structure of the environment by ensuring a continuous model, while\nalso being able to be incrementally updated with linear computational cost in\nthe number of measurements. We reduce the effect of uncertainty in the robot\npose (position and orientation) by using landmark-relative submaps. The\nuncertainty in the measurements and robot pose are accounted for by the use of\nBayesian inference techniques during the map update. We demonstrate that a STM\nmap can be used with sensors that generate point measurements, such as light\ndetection and ranging (LiDAR) sensors and stereo cameras. We show that a STM\nmap is a more accurate model than the only comparable online surface mapping\ntechnique$\\unicode{x2014}$a standard elevation map$\\unicode{x2014}$and we also\nprovide qualitative results on practical datasets.\n",
        "published": "2019",
        "authors": [
            "Clint D. Lombard",
            "Corn\u00e9 E. van Daalen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.03732v1",
        "title": "Ctrl-Z: Recovering from Instability in Reinforcement Learning",
        "abstract": "  When learning behavior, training data is often generated by the learner\nitself; this can result in unstable training dynamics, and this problem has\nparticularly important applications in safety-sensitive real-world control\ntasks such as robotics. In this work, we propose a principled and\nmodel-agnostic approach to mitigate the issue of unstable learning dynamics by\nmaintaining a history of a reinforcement learning agent over the course of\ntraining, and reverting to the parameters of a previous agent whenever\nperformance significantly decreases. We develop techniques for evaluating this\nperformance through statistical hypothesis testing of continued improvement,\nand evaluate them on a standard suite of challenging benchmark tasks involving\ncontinuous control of simulated robots. We show improvements over\nstate-of-the-art reinforcement learning algorithms in performance and\nrobustness to hyperparameters, outperforming DDPG in 5 out of 6 evaluation\nenvironments and showing no decrease in performance with TD3, which is known to\nbe relatively stable. In this way, our approach takes an important step towards\nincreasing data efficiency and stability in training for real-world robotic\napplications.\n",
        "published": "2019",
        "authors": [
            "Vibhavari Dasagi",
            "Jake Bruce",
            "Thierry Peynot",
            "J\u00fcrgen Leitner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.05527v1",
        "title": "Regularizing Model-Based Planning with Energy-Based Models",
        "abstract": "  Model-based reinforcement learning could enable sample-efficient learning by\nquickly acquiring rich knowledge about the world and using it to improve\nbehaviour without additional data. Learned dynamics models can be directly used\nfor planning actions but this has been challenging because of inaccuracies in\nthe learned models. In this paper, we focus on planning with learned dynamics\nmodels and propose to regularize it using energy estimates of state transitions\nin the environment. We visually demonstrate the effectiveness of the proposed\nmethod and show that off-policy training of an energy estimator can be\neffectively used to regularize planning with pre-trained dynamics models.\nFurther, we demonstrate that the proposed method enables sample-efficient\nlearning to achieve competitive performance in challenging continuous control\ntasks such as Half-cheetah and Ant in just a few minutes of experience.\n",
        "published": "2019",
        "authors": [
            "Rinu Boney",
            "Juho Kannala",
            "Alexander Ilin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.06031v1",
        "title": "Imitating by generating: deep generative models for imitation of\n  interactive tasks",
        "abstract": "  To coordinate actions with an interaction partner requires a constant\nexchange of sensorimotor signals. Humans acquire these skills in infancy and\nearly childhood mostly by imitation learning and active engagement with a\nskilled partner. They require the ability to predict and adapt to one's partner\nduring an interaction. In this work we want to explore these ideas in a\nhuman-robot interaction setting in which a robot is required to learn\ninteractive tasks from a combination of observational and kinesthetic learning.\nTo this end, we propose a deep learning framework consisting of a number of\ncomponents for (1) human and robot motion embedding, (2) motion prediction of\nthe human partner and (3) generation of robot joint trajectories matching the\nhuman motion. To test these ideas, we collect human-human interaction data and\nhuman-robot interaction data of four interactive tasks \"hand-shake\",\n\"hand-wave\", \"parachute fist-bump\" and \"rocket fist-bump\". We demonstrate\nexperimentally the importance of predictive and adaptive components as well as\nlow-level abstractions to successfully learn to imitate human behavior in\ninteractive social tasks.\n",
        "published": "2019",
        "authors": [
            "Judith B\u00fctepage",
            "Ali Ghadirzadeh",
            "\u00d6zge \u00d6ztimur Karadag",
            "M\u00e5rten Bj\u00f6rkman",
            "Danica Kragic"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.07224v1",
        "title": "Teacher algorithms for curriculum learning of Deep RL in continuously\n  parameterized environments",
        "abstract": "  We consider the problem of how a teacher algorithm can enable an unknown Deep\nReinforcement Learning (DRL) student to become good at a skill over a wide\nrange of diverse environments. To do so, we study how a teacher algorithm can\nlearn to generate a learning curriculum, whereby it sequentially samples\nparameters controlling a stochastic procedural generation of environments.\nBecause it does not initially know the capacities of its student, a key\nchallenge for the teacher is to discover which environments are easy, difficult\nor unlearnable, and in what order to propose them to maximize the efficiency of\nlearning over the learnable ones. To achieve this, this problem is transformed\ninto a surrogate continuous bandit problem where the teacher samples\nenvironments in order to maximize absolute learning progress of its student. We\npresent a new algorithm modeling absolute learning progress with Gaussian\nmixture models (ALP-GMM). We also adapt existing algorithms and provide a\ncomplete study in the context of DRL. Using parameterized variants of the\nBipedalWalker environment, we study their efficiency to personalize a learning\ncurriculum for different learners (embodiments), their robustness to the ratio\nof learnable/unlearnable environments, and their scalability to non-linear and\nhigh-dimensional parameter spaces. Videos and code are available at\nhttps://github.com/flowersteam/teachDeepRL.\n",
        "published": "2019",
        "authors": [
            "R\u00e9my Portelas",
            "C\u00e9dric Colas",
            "Katja Hofmann",
            "Pierre-Yves Oudeyer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.07459v1",
        "title": "Creativity in Robot Manipulation with Deep Reinforcement Learning",
        "abstract": "  Deep Reinforcement Learning (DRL) has emerged as a powerful control technique\nin robotic science. In contrast to control theory, DRL is more robust in the\nthorough exploration of the environment. This capability of DRL generates more\nhuman-like behaviour and intelligence when applied to the robots. To explore\nthis capability, we designed challenging manipulation tasks to observe robots\nstrategy to handle complex scenarios. We observed that robots not only perform\ntasks successfully, but also transpire a creative and non intuitive solution.\nWe also observed robot's persistence in tasks that are close to success and its\nstriking ability in discerning to continue or give up.\n",
        "published": "2019",
        "authors": [
            "Juan Carlos Vargas",
            "Malhar Bhoite",
            "Amir Barati Farimani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.07487v2",
        "title": "Embodiment dictates learnability in neural controllers",
        "abstract": "  Catastrophic forgetting continues to severely restrict the learnability of\ncontrollers suitable for multiple task environments. Efforts to combat\ncatastrophic forgetting reported in the literature to date have focused on how\ncontrol systems can be updated more rapidly, hastening their adjustment from\ngood initial settings to new environments, or more circumspectly, suppressing\ntheir ability to overfit to any one environment. When using robots, the\nenvironment includes the robot's own body, its shape and material properties,\nand how its actuators and sensors are distributed along its mechanical\nstructure. Here we demonstrate for the first time how one such design decision\n(sensor placement) can alter the landscape of the loss function itself, either\nexpanding or shrinking the weight manifolds containing suitable controllers for\neach individual task, thus increasing or decreasing their probability of\noverlap across tasks, and thus reducing or inducing the potential for\ncatastrophic forgetting.\n",
        "published": "2019",
        "authors": [
            "Joshua Powers",
            "Ryan Grindle",
            "Sam Kriegman",
            "Lapo Frati",
            "Nick Cheney",
            "Josh Bongard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.07772v4",
        "title": "Teaching Vehicles to Anticipate: A Systematic Study on Probabilistic\n  Behavior Prediction Using Large Data Sets",
        "abstract": "  By observing their environment as well as other traffic participants, humans\nare enabled to drive road vehicles safely. Vehicle passengers, however,\nperceive a notable difference between non-experienced and experienced drivers.\nIn particular, they may get the impression that the latter ones anticipate what\nwill happen in the next few moments and consider these foresights in their\ndriving behavior. To make the driving style of automated vehicles comparable to\nthe one of human drivers with respect to comfort and perceived safety, the\naforementioned anticipation skills need to become a built-in feature of\nself-driving vehicles. This article provides a systematic comparison of methods\nand strategies to generate this intention for self-driving cars using machine\nlearning techniques. To implement and test these algorithms we use a large data\nset collected over more than 30000 km of highway driving and containing\napproximately 40000 real-world driving situations. We further show that it is\npossible to classify driving maneuvers upcoming within the next 5 s with an\nArea Under the ROC Curve (AUC) above 0.92 for all defined maneuver classes.\nThis enables us to predict the lateral position with a prediction horizon of 5\ns with a median lateral error of less than 0.21 m.\n",
        "published": "2019",
        "authors": [
            "Florian Wirthm\u00fcller",
            "Julian Schlechtriemen",
            "Jochen Hipp",
            "Manfred Reichert"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.08264v2",
        "title": "Learning Compositional Koopman Operators for Model-Based Control",
        "abstract": "  Finding an embedding space for a linear approximation of a nonlinear\ndynamical system enables efficient system identification and control synthesis.\nThe Koopman operator theory lays the foundation for identifying the\nnonlinear-to-linear coordinate transformations with data-driven methods.\nRecently, researchers have proposed to use deep neural networks as a more\nexpressive class of basis functions for calculating the Koopman operators.\nThese approaches, however, assume a fixed dimensional state space; they are\ntherefore not applicable to scenarios with a variable number of objects. In\nthis paper, we propose to learn compositional Koopman operators, using graph\nneural networks to encode the state into object-centric embeddings and using a\nblock-wise linear transition matrix to regularize the shared structure across\nobjects. The learned dynamics can quickly adapt to new environments of unknown\nphysical parameters and produce control signals to achieve a specified goal.\nOur experiments on manipulating ropes and controlling soft robots show that the\nproposed method has better efficiency and generalization ability than existing\nbaselines.\n",
        "published": "2019",
        "authors": [
            "Yunzhu Li",
            "Hao He",
            "Jiajun Wu",
            "Dina Katabi",
            "Antonio Torralba"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.10754v1",
        "title": "Learning Q-network for Active Information Acquisition",
        "abstract": "  In this paper, we propose a novel Reinforcement Learning approach for solving\nthe Active Information Acquisition problem, which requires an agent to choose a\nsequence of actions in order to acquire information about a process of interest\nusing on-board sensors. The classic challenges in the information acquisition\nproblem are the dependence of a planning algorithm on known models and the\ndifficulty of computing information-theoretic cost functions over arbitrary\ndistributions. In contrast, the proposed framework of reinforcement learning\ndoes not require any knowledge on models and alleviates the problems during an\nextended training stage. It results in policies that are efficient to execute\nonline and applicable for real-time control of robotic systems. Furthermore,\nthe state-of-the-art planning methods are typically restricted to short\nhorizons, which may become problematic with local minima. Reinforcement\nlearning naturally handles the issue of planning horizon in information\nproblems as it maximizes a discounted sum of rewards over a long finite or\ninfinite time horizon. We discuss the potential benefits of the proposed\nframework and compare the performance of the novel algorithm to an existing\ninformation acquisition method for multi-target tracking scenarios.\n",
        "published": "2019",
        "authors": [
            "Heejin Jeong",
            "Brent Schlotfeldt",
            "Hamed Hassani",
            "Manfred Morari",
            "Daniel D. Lee",
            "George J. Pappas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.11956v1",
        "title": "Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and\n  Reinforcement Learning",
        "abstract": "  We present relay policy learning, a method for imitation and reinforcement\nlearning that can solve multi-stage, long-horizon robotic tasks. This general\nand universally-applicable, two-phase approach consists of an imitation\nlearning stage that produces goal-conditioned hierarchical policies, and a\nreinforcement learning phase that finetunes these policies for task\nperformance. Our method, while not necessarily perfect at imitation learning,\nis very amenable to further improvement via environment interaction, allowing\nit to scale to challenging long-horizon tasks. We simplify the long-horizon\npolicy learning problem by using a novel data-relabeling algorithm for learning\ngoal-conditioned hierarchical policies, where the low-level only acts for a\nfixed number of steps, regardless of the goal achieved. While we rely on\ndemonstration data to bootstrap policy learning, we do not assume access to\ndemonstrations of every specific tasks that is being solved, and instead\nleverage unstructured and unsegmented demonstrations of semantically meaningful\nbehaviors that are not only less burdensome to provide, but also can greatly\nfacilitate further improvement using reinforcement learning. We demonstrate the\neffectiveness of our method on a number of multi-stage, long-horizon\nmanipulation tasks in a challenging kitchen simulation environment. Videos are\navailable at https://relay-policy-learning.github.io/\n",
        "published": "2019",
        "authors": [
            "Abhishek Gupta",
            "Vikash Kumar",
            "Corey Lynch",
            "Sergey Levine",
            "Karol Hausman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.02860v2",
        "title": "Weakly-Supervised Reinforcement Learning for Controllable Behavior",
        "abstract": "  Reinforcement learning (RL) is a powerful framework for learning to take\nactions to solve tasks. However, in many settings, an agent must winnow down\nthe inconceivably large space of all possible tasks to the single task that it\nis currently being asked to solve. Can we instead constrain the space of tasks\nto those that are semantically meaningful? In this work, we introduce a\nframework for using weak supervision to automatically disentangle this\nsemantically meaningful subspace of tasks from the enormous space of\nnonsensical \"chaff\" tasks. We show that this learned subspace enables efficient\nexploration and provides a representation that captures distance between\nstates. On a variety of challenging, vision-based continuous control problems,\nour approach leads to substantial performance gains, particularly as the\ncomplexity of the environment grows.\n",
        "published": "2020",
        "authors": [
            "Lisa Lee",
            "Benjamin Eysenbach",
            "Ruslan Salakhutdinov",
            "Shixiang Shane Gu",
            "Chelsea Finn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.04650v2",
        "title": "State-Only Imitation Learning for Dexterous Manipulation",
        "abstract": "  Modern model-free reinforcement learning methods have recently demonstrated\nimpressive results on a number of problems. However, complex domains like\ndexterous manipulation remain a challenge due to the high sample complexity. To\naddress this, current approaches employ expert demonstrations in the form of\nstate-action pairs, which are difficult to obtain for real-world settings such\nas learning from videos. In this paper, we move toward a more realistic setting\nand explore state-only imitation learning. To tackle this setting, we train an\ninverse dynamics model and use it to predict actions for state-only\ndemonstrations. The inverse dynamics model and the policy are trained jointly.\nOur method performs on par with state-action approaches and considerably\noutperforms RL alone. By not relying on expert actions, we are able to learn\nfrom demonstrations with different dynamics, morphologies, and objects. Videos\navailable at https://people.eecs.berkeley.edu/~ilija/soil .\n",
        "published": "2020",
        "authors": [
            "Ilija Radosavovic",
            "Xiaolong Wang",
            "Lerrel Pinto",
            "Jitendra Malik"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.06247v2",
        "title": "Improving Movement Predictions of Traffic Actors in Bird's-Eye View\n  Models using GANs and Differentiable Trajectory Rasterization",
        "abstract": "  One of the most critical pieces of the self-driving puzzle is the task of\npredicting future movement of surrounding traffic actors, which allows the\nautonomous vehicle to safely and effectively plan its future route in a complex\nworld. Recently, a number of algorithms have been proposed to address this\nimportant problem, spurred by a growing interest of researchers from both\nindustry and academia. Methods based on top-down scene rasterization on one\nside and Generative Adversarial Networks (GANs) on the other have shown to be\nparticularly successful, obtaining state-of-the-art accuracies on the task of\ntraffic movement prediction. In this paper we build upon these two directions\nand propose a raster-based conditional GAN architecture, powered by a novel\ndifferentiable rasterizer module at the input of the conditional discriminator\nthat maps generated trajectories into the raster space in a differentiable\nmanner. This simplifies the task for the discriminator as trajectories that are\nnot scene-compliant are easier to discern, and allows the gradients to flow\nback forcing the generator to output better, more realistic trajectories. We\nevaluated the proposed method on a large-scale, real-world data set, showing\nthat it outperforms state-of-the-art GAN-based baselines.\n",
        "published": "2020",
        "authors": [
            "Eason Wang",
            "Henggang Cui",
            "Sai Yalamanchi",
            "Mohana Moorthy",
            "Fang-Chieh Chou",
            "Nemanja Djuric"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.06531v2",
        "title": "Adversarial Evaluation of Autonomous Vehicles in Lane-Change Scenarios",
        "abstract": "  Autonomous vehicles must be comprehensively evaluated before deployed in\ncities and highways. However, most existing evaluation approaches for\nautonomous vehicles are static and lack adaptability, so they are usually\ninefficient in generating challenging scenarios for tested vehicles. In this\npaper, we propose an adaptive evaluation framework to efficiently evaluate\nautonomous vehicles in adversarial environments generated by deep reinforcement\nlearning. Considering the multimodal nature of dangerous scenarios, we use\nensemble models to represent different local optimums for diversity. We then\nutilize a nonparametric Bayesian method to cluster the adversarial policies.\nThe proposed method is validated in a typical lane-change scenario that\ninvolves frequent interactions between the ego vehicle and the surrounding\nvehicles. Results show that the adversarial scenarios generated by our method\nsignificantly degrade the performance of the tested vehicles. We also\nillustrate different patterns of generated adversarial environments, which can\nbe used to infer the weaknesses of the tested vehicles.\n",
        "published": "2020",
        "authors": [
            "Baiming Chen",
            "Xiang Chen",
            "Wu Qiong",
            "Liang Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.06805v2",
        "title": "Interpretable Safety Validation for Autonomous Vehicles",
        "abstract": "  An open problem for autonomous driving is how to validate the safety of an\nautonomous vehicle in simulation. Automated testing procedures can find\nfailures of an autonomous system but these failures may be difficult to\ninterpret due to their high dimensionality and may be so unlikely as to not be\nimportant. This work describes an approach for finding interpretable failures\nof an autonomous system. The failures are described by signal temporal logic\nexpressions that can be understood by a human, and are optimized to produce\nfailures that have high likelihood. Our methodology is demonstrated for the\nsafety validation of an autonomous vehicle in the context of an unprotected\nleft turn and a crosswalk with a pedestrian. Compared to a baseline importance\nsampling approach, our methodology finds more failures with higher likelihood\nwhile retaining interpretability.\n",
        "published": "2020",
        "authors": [
            "Anthony Corso",
            "Mykel J. Kochenderfer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.11910v3",
        "title": "A Relation Spectrum Inheriting Taylor Series: Muscle Synergy and\n  Coupling for Hand",
        "abstract": "  There are two famous function decomposition methods in math: Taylor Series\nand Fourier Series. Fourier series developed into Fourier spectrum, which was\napplied to signal decomposition\\analysis. However, because the Taylor series\nwhose function without a definite functional expression cannot be solved,\nTaylor Series has rarely been used in engineering. Here, we developed Taylor\nseries by our Dendrite Net, constructed a relation spectrum, and applied it to\nmodel or system decomposition\\analysis. Specific engineering: the knowledge of\nthe intuitive link between muscle activity and the finger movement is vital for\nthe design of commercial prosthetic hands that do not need user pre-training.\nHowever, this link has yet to be understood due to the complexity of human\nhand. In this study, the relation spectrum was applied to analyze the\nmuscle-finger system. One single muscle actuates multiple fingers, or multiple\nmuscles actuate one single finger simultaneously. Thus, the research was in\nmuscle synergy and muscle coupling for hand. This paper has two main\ncontributions. (1) The findings of hand contribute to designing prosthetic\nhands. (2) The relation spectrum makes the online model human-readable, which\nunifies online performance and offline results. Code (novel tool for most\nfields) is available at https://github.com/liugang1234567/Gang-neuron.\n",
        "published": "2020",
        "authors": [
            "Gang Liu",
            "Jing Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.11938v1",
        "title": "Towards Differentiable Resampling",
        "abstract": "  Resampling is a key component of sample-based recursive state estimation in\nparticle filters. Recent work explores differentiable particle filters for\nend-to-end learning. However, resampling remains a challenge in these works, as\nit is inherently non-differentiable. We address this challenge by replacing\ntraditional resampling with a learned neural network resampler. We present a\nnovel network architecture, the particle transformer, and train it for particle\nresampling using a likelihood-based loss function over sets of particles.\nIncorporated into a differentiable particle filter, our model can be end-to-end\noptimized jointly with the other particle filter components via gradient\ndescent. Our results show that our learned resampler outperforms traditional\nresampling techniques on synthetic data and in a simulated robot localization\ntask.\n",
        "published": "2020",
        "authors": [
            "Michael Zhu",
            "Kevin Murphy",
            "Rico Jonschkowski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.12570v1",
        "title": "The Ingredients of Real-World Robotic Reinforcement Learning",
        "abstract": "  The success of reinforcement learning for real world robotics has been, in\nmany cases limited to instrumented laboratory scenarios, often requiring\narduous human effort and oversight to enable continuous learning. In this work,\nwe discuss the elements that are needed for a robotic learning system that can\ncontinually and autonomously improve with data collected in the real world. We\npropose a particular instantiation of such a system, using dexterous\nmanipulation as our case study. Subsequently, we investigate a number of\nchallenges that come up when learning without instrumentation. In such\nsettings, learning must be feasible without manually designed resets, using\nonly on-board perception, and without hand-engineered reward functions. We\npropose simple and scalable solutions to these challenges, and then demonstrate\nthe efficacy of our proposed system on a set of dexterous robotic manipulation\ntasks, providing an in-depth analysis of the challenges associated with this\nlearning paradigm. We demonstrate that our complete system can learn without\nany human intervention, acquiring a variety of vision-based skills with a\nreal-world three-fingered hand. Results and videos can be found at\nhttps://sites.google.com/view/realworld-rl/\n",
        "published": "2020",
        "authors": [
            "Henry Zhu",
            "Justin Yu",
            "Abhishek Gupta",
            "Dhruv Shah",
            "Kristian Hartikainen",
            "Avi Singh",
            "Vikash Kumar",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.14567v1",
        "title": "Plan-Space State Embeddings for Improved Reinforcement Learning",
        "abstract": "  Robot control problems are often structured with a policy function that maps\nstate values into control values, but in many dynamic problems the observed\nstate can have a difficult to characterize relationship with useful policy\nactions. In this paper we present a new method for learning state embeddings\nfrom plans or other forms of demonstrations such that the embedding space has a\nspecified geometric relationship with the demonstrations. We present a novel\nvariational framework for learning these embeddings that attempts to optimize\ntrajectory linearity in the learned embedding space. We show how these\nembedding spaces can then be used as an augmentation to the robot state in\nreinforcement learning problems. We use kinodynamic planning to generate\ntraining trajectories for some example environments, and then train embedding\nspaces for these environments. We show empirically that observing a system in\nthe learned embedding space improves the performance of policy gradient\nreinforcement learning algorithms, particularly by reducing the variance\nbetween training runs. Our technique is limited to environments where\ndemonstration data is available, but places no limits on how that data is\ncollected. Our embedding technique provides a way to transfer domain knowledge\nfrom existing technologies such as planning and control algorithms, into more\nflexible policy learning algorithms, by creating an abstract representation of\nthe robot state with meaningful geometry.\n",
        "published": "2020",
        "authors": [
            "Max Pflueger",
            "Gaurav S. Sukhatme"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1404.5165v2",
        "title": "GP-Localize: Persistent Mobile Robot Localization using Online Sparse\n  Gaussian Process Observation Model",
        "abstract": "  Central to robot exploration and mapping is the task of persistent\nlocalization in environmental fields characterized by spatially correlated\nmeasurements. This paper presents a Gaussian process localization (GP-Localize)\nalgorithm that, in contrast to existing works, can exploit the spatially\ncorrelated field measurements taken during a robot's exploration (instead of\nrelying on prior training data) for efficiently and scalably learning the GP\nobservation model online through our proposed novel online sparse GP. As a\nresult, GP-Localize is capable of achieving constant time and memory (i.e.,\nindependent of the size of the data) per filtering step, which demonstrates the\npractical feasibility of using GPs for persistent robot localization and\nautonomy. Empirical evaluation via simulated experiments with real-world\ndatasets and a real robot experiment shows that GP-Localize outperforms\nexisting GP localization algorithms.\n",
        "published": "2014",
        "authors": [
            "Nuo Xu",
            "Kian Hsiang Low",
            "Jie Chen",
            "Keng Kiat Lim",
            "Etkin Baris Ozgul"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1512.08065v4",
        "title": "Inverse Reinforcement Learning via Deep Gaussian Process",
        "abstract": "  We propose a new approach to inverse reinforcement learning (IRL) based on\nthe deep Gaussian process (deep GP) model, which is capable of learning\ncomplicated reward structures with few demonstrations. Our model stacks\nmultiple latent GP layers to learn abstract representations of the state\nfeature space, which is linked to the demonstrations through the Maximum\nEntropy learning framework. Incorporating the IRL engine into the nonlinear\nlatent structure renders existing deep GP inference approaches intractable. To\ntackle this, we develop a non-standard variational approximation framework\nwhich extends previous inference schemes. This allows for approximate Bayesian\ntreatment of the feature space and guards against overfitting. Carrying out\nrepresentation and inverse reinforcement learning simultaneously within our\nmodel outperforms state-of-the-art approaches, as we demonstrate with\nexperiments on standard benchmarks (\"object world\",\"highway driving\") and a new\nbenchmark (\"binary world\").\n",
        "published": "2015",
        "authors": [
            "Ming Jin",
            "Andreas Damianou",
            "Pieter Abbeel",
            "Costas Spanos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.08120v3",
        "title": "GP-SUM. Gaussian Processes Filtering of non-Gaussian Beliefs",
        "abstract": "  This work studies the problem of stochastic dynamic filtering and state\npropagation with complex beliefs. The main contribution is GP-SUM, a filtering\nalgorithm tailored to dynamic systems and observation models expressed as\nGaussian Processes (GP), and to states represented as a weighted sum of\nGaussians. The key attribute of GP-SUM is that it does not rely on\nlinearizations of the dynamic or observation models, or on unimodal Gaussian\napproximations of the belief, hence enables tracking complex state\ndistributions. The algorithm can be seen as a combination of a sampling-based\nfilter with a probabilistic Bayes filter. On the one hand, GP-SUM operates by\nsampling the state distribution and propagating each sample through the dynamic\nsystem and observation models. On the other hand, it achieves effective\nsampling and accurate probabilistic propagation by relying on the GP form of\nthe system, and the sum-of-Gaussian form of the belief. We show that GP-SUM\noutperforms several GP-Bayes and Particle Filters on a standard benchmark. We\nalso demonstrate its use in a pushing task, predicting with experimental\naccuracy the naturally occurring non-Gaussian distributions.\n",
        "published": "2017",
        "authors": [
            "Maria Bauza",
            "Alberto Rodriguez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.01657v2",
        "title": "Dynamics-Aware Unsupervised Discovery of Skills",
        "abstract": "  Conventionally, model-based reinforcement learning (MBRL) aims to learn a\nglobal model for the dynamics of the environment. A good model can potentially\nenable planning algorithms to generate a large variety of behaviors and solve\ndiverse tasks. However, learning an accurate model for complex dynamical\nsystems is difficult, and even then, the model might not generalize well\noutside the distribution of states on which it was trained. In this work, we\ncombine model-based learning with model-free learning of primitives that make\nmodel-based planning easy. To that end, we aim to answer the question: how can\nwe discover skills whose outcomes are easy to predict? We propose an\nunsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS),\nwhich simultaneously discovers predictable behaviors and learns their dynamics.\nOur method can leverage continuous skill spaces, theoretically, allowing us to\nlearn infinitely many behaviors even for high-dimensional state-spaces. We\ndemonstrate that zero-shot planning in the learned latent space significantly\noutperforms standard MBRL and model-free goal-conditioned RL, can handle\nsparse-reward tasks, and substantially improves over prior hierarchical RL\nmethods for unsupervised skill discovery.\n",
        "published": "2019",
        "authors": [
            "Archit Sharma",
            "Shixiang Gu",
            "Sergey Levine",
            "Vikash Kumar",
            "Karol Hausman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.02426v1",
        "title": "Multimodal Uncertainty Reduction for Intention Recognition in\n  Human-Robot Interaction",
        "abstract": "  Assistive robots can potentially improve the quality of life and personal\nindependence of elderly people by supporting everyday life activities. To\nguarantee a safe and intuitive interaction between human and robot, human\nintentions need to be recognized automatically. As humans communicate their\nintentions multimodally, the use of multiple modalities for intention\nrecognition may not just increase the robustness against failure of individual\nmodalities but especially reduce the uncertainty about the intention to be\npredicted. This is desirable as particularly in direct interaction between\nrobots and potentially vulnerable humans a minimal uncertainty about the\nsituation as well as knowledge about this actual uncertainty is necessary.\nThus, in contrast to existing methods, in this work a new approach for\nmultimodal intention recognition is introduced that focuses on uncertainty\nreduction through classifier fusion. For the four considered modalities speech,\ngestures, gaze directions and scene objects individual intention classifiers\nare trained, all of which output a probability distribution over all possible\nintentions. By combining these output distributions using the Bayesian method\nIndependent Opinion Pool the uncertainty about the intention to be recognized\ncan be decreased. The approach is evaluated in a collaborative human-robot\ninteraction task with a 7-DoF robot arm. The results show that fused\nclassifiers which combine multiple modalities outperform the respective\nindividual base classifiers with respect to increased accuracy, robustness, and\nreduced uncertainty.\n",
        "published": "2019",
        "authors": [
            "Susanne Trick",
            "Dorothea Koert",
            "Jan Peters",
            "Constantin Rothkopf"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.03116v1",
        "title": "Intrinsic Motivation Driven Intuitive Physics Learning using Deep\n  Reinforcement Learning with Intrinsic Reward Normalization",
        "abstract": "  At an early age, human infants are able to learn and build a model of the\nworld very quickly by constantly observing and interacting with objects around\nthem. One of the most fundamental intuitions human infants acquire is intuitive\nphysics. Human infants learn and develop these models, which later serve as\nprior knowledge for further learning. Inspired by such behaviors exhibited by\nhuman infants, we introduce a graphical physics network integrated with deep\nreinforcement learning. Specifically, we introduce an intrinsic reward\nnormalization method that allows our agent to efficiently choose actions that\ncan improve its intuitive physics model the most.\n  Using a 3D physics engine, we show that our graphical physics network is able\nto infer object's positions and velocities very effectively, and our deep\nreinforcement learning network encourages an agent to improve its model by\nmaking it continuously interact with objects only using intrinsic motivation.\nWe experiment our model in both stationary and non-stationary state problems\nand show benefits of our approach in terms of the number of different actions\nthe agent performs and the accuracy of agent's intuition model.\n  Videos are at https://www.youtube.com/watch?v=pDbByp91r3M&t=2s\n",
        "published": "2019",
        "authors": [
            "JaeWon Choi",
            "Sung-eui Yoon"
        ]
    }
]