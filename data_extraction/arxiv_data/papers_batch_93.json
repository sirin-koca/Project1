[
    {
        "id": "http://arxiv.org/abs/1706.04702v1",
        "title": "Deep learning-based numerical methods for high-dimensional parabolic\n  partial differential equations and backward stochastic differential equations",
        "abstract": "  We propose a new algorithm for solving parabolic partial differential\nequations (PDEs) and backward stochastic differential equations (BSDEs) in high\ndimension, by making an analogy between the BSDE and reinforcement learning\nwith the gradient of the solution playing the role of the policy function, and\nthe loss function given by the error between the prescribed terminal condition\nand the solution of the BSDE. The policy function is then approximated by a\nneural network, as is done in deep reinforcement learning. Numerical results\nusing TensorFlow illustrate the efficiency and accuracy of the proposed\nalgorithms for several 100-dimensional nonlinear PDEs from physics and finance\nsuch as the Allen-Cahn equation, the Hamilton-Jacobi-Bellman equation, and a\nnonlinear pricing model for financial derivatives.\n",
        "published": "2017",
        "authors": [
            "Weinan E",
            "Jiequn Han",
            "Arnulf Jentzen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.06673v3",
        "title": "Neon2: Finding Local Minima via First-Order Oracles",
        "abstract": "  We propose a reduction for non-convex optimization that can (1) turn an\nstationary-point finding algorithm into an local-minimum finding one, and (2)\nreplace the Hessian-vector product computations with only gradient\ncomputations. It works both in the stochastic and the deterministic settings,\nwithout hurting the algorithm's performance.\n  As applications, our reduction turns Natasha2 into a first-order method\nwithout hurting its performance. It also converts SGD, GD, SCSG, and SVRG into\nalgorithms finding approximate local minima, outperforming some best known\nresults.\n",
        "published": "2017",
        "authors": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.04443v1",
        "title": "On Characterizing the Capacity of Neural Networks using Algebraic\n  Topology",
        "abstract": "  The learnability of different neural architectures can be characterized\ndirectly by computable measures of data complexity. In this paper, we reframe\nthe problem of architecture selection as understanding how data determines the\nmost expressive and generalizable architectures suited to that data, beyond\ninductive bias. After suggesting algebraic topology as a measure for data\ncomplexity, we show that the power of a network to express the topological\ncomplexity of a dataset in its decision region is a strictly limiting factor in\nits ability to generalize. We then provide the first empirical characterization\nof the topological capacity of neural networks. Our empirical analysis shows\nthat at every level of dataset complexity, neural networks exhibit topological\nphase transitions. This observation allowed us to connect existing theory to\nempirically driven conjectures on the choice of architectures for\nfully-connected neural networks.\n",
        "published": "2018",
        "authors": [
            "William H. Guss",
            "Ruslan Salakhutdinov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.08203v1",
        "title": "Residual Networks: Lyapunov Stability and Convex Decomposition",
        "abstract": "  While training error of most deep neural networks degrades as the depth of\nthe network increases, residual networks appear to be an exception. We show\nthat the main reason for this is the Lyapunov stability of the gradient descent\nalgorithm: for an arbitrarily chosen step size, the equilibria of the gradient\ndescent are most likely to remain stable for the parametrization of residual\nnetworks. We then present an architecture with a pair of residual networks to\napproximate a large class of functions by decomposing them into a convex and a\nconcave part. Some parameters of this model are shown to change little during\ntraining, and this imperfect optimization prevents overfitting the data and\nleads to solutions with small Lipschitz constants, while providing clues about\nthe generalization of other deep networks.\n",
        "published": "2018",
        "authors": [
            "Kamil Nar",
            "Shankar Sastry"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.02957v2",
        "title": "A Deep Neural Network Surrogate for High-Dimensional Random Partial\n  Differential Equations",
        "abstract": "  Developing efficient numerical algorithms for the solution of high\ndimensional random Partial Differential Equations (PDEs) has been a challenging\ntask due to the well-known curse of dimensionality. We present a new solution\nframework for these problems based on a deep learning approach. Specifically,\nthe random PDE is approximated by a feed-forward fully-connected deep residual\nnetwork, with either strong or weak enforcement of initial and boundary\nconstraints. The framework is mesh-free, and can handle irregular computational\ndomains. Parameters of the approximating deep neural network are determined\niteratively using variants of the Stochastic Gradient Descent (SGD) algorithm.\nThe satisfactory accuracy of the proposed frameworks is numerically\ndemonstrated on diffusion and heat conduction problems, in comparison with the\nconverged Monte Carlo-based finite element results.\n",
        "published": "2018",
        "authors": [
            "Mohammad Amin Nabian",
            "Hadi Meidani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.05387v1",
        "title": "Parameter Learning and Change Detection Using a Particle Filter With\n  Accelerated Adaptation",
        "abstract": "  This paper presents the construction of a particle filter, which incorporates\nelements inspired by genetic algorithms, in order to achieve accelerated\nadaptation of the estimated posterior distribution to changes in model\nparameters. Specifically, the filter is designed for the situation where the\nsubsequent data in online sequential filtering does not match the model\nposterior filtered based on data up to a current point in time. The examples\nconsidered encompass parameter regime shifts and stochastic volatility. The\nfilter adapts to regime shifts extremely rapidly and delivers a clear heuristic\nfor distinguishing between regime shifts and stochastic volatility, even though\nthe model dynamics assumed by the filter exhibit neither of those features.\n",
        "published": "2018",
        "authors": [
            "Karol Gellert",
            "Erik Schl\u00f6gl"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.05876v1",
        "title": "Financial Risk and Returns Prediction with Modular Networked Learning",
        "abstract": "  An artificial agent for financial risk and returns' prediction is built with\na modular cognitive system comprised of interconnected recurrent neural\nnetworks, such that the agent learns to predict the financial returns, and\nlearns to predict the squared deviation around these predicted returns. These\ntwo expectations are used to build a volatility-sensitive interval prediction\nfor financial returns, which is evaluated on three major financial indices and\nshown to be able to predict financial returns with higher than 80% success rate\nin interval prediction in both training and testing, raising into question the\nEfficient Market Hypothesis. The agent is introduced as an example of a class\nof artificial intelligent systems that are equipped with a Modular Networked\nLearning cognitive system, defined as an integrated networked system of machine\nlearning modules, where each module constitutes a functional unit that is\ntrained for a given specific task that solves a subproblem of a complex main\nproblem expressed as a network of linked subproblems. In the case of neural\nnetworks, these systems function as a form of an \"artificial brain\", where each\nmodule is like a specialized brain region comprised of a neural network with a\nspecific architecture.\n",
        "published": "2018",
        "authors": [
            "Carlos Pedro Gon\u00e7alves"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.07741v2",
        "title": "A large-scale evaluation framework for EEG deep learning architectures",
        "abstract": "  EEG is the most common signal source for noninvasive BCI applications. For\nsuch applications, the EEG signal needs to be decoded and translated into\nappropriate actions. A recently emerging EEG decoding approach is deep learning\nwith Convolutional or Recurrent Neural Networks (CNNs, RNNs) with many\ndifferent architectures already published. Here we present a novel framework\nfor the large-scale evaluation of different deep-learning architectures on\ndifferent EEG datasets. This framework comprises (i) a collection of EEG\ndatasets currently including 100 examples (recording sessions) from six\ndifferent classification problems, (ii) a collection of different EEG decoding\nalgorithms, and (iii) a wrapper linking the decoders to the data as well as\nhandling structured documentation of all settings and (hyper-) parameters and\nstatistics, designed to ensure transparency and reproducibility. As an\napplications example we used our framework by comparing three publicly\navailable CNN architectures: the Braindecode Deep4 ConvNet, Braindecode Shallow\nConvNet, and two versions of EEGNet. We also show how our framework can be used\nto study similarities and differences in the performance of different decoding\nmethods across tasks. We argue that the deep learning EEG framework as\ndescribed here could help to tap the full potential of deep learning for BCI\napplications.\n",
        "published": "2018",
        "authors": [
            "Felix A. Heilmeyer",
            "Robin T. Schirrmeister",
            "Lukas D. J. Fiederer",
            "Martin V\u00f6lker",
            "Joos Behncke",
            "Tonio Ball"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.04098v1",
        "title": "A Recurrent Neural Network Survival Model: Predicting Web User Return\n  Time",
        "abstract": "  The size of a website's active user base directly affects its value. Thus, it\nis important to monitor and influence a user's likelihood to return to a site.\nEssential to this is predicting when a user will return. Current state of the\nart approaches to solve this problem come in two flavors: (1) Recurrent Neural\nNetwork (RNN) based solutions and (2) survival analysis methods. We observe\nthat both techniques are severely limited when applied to this problem.\nSurvival models can only incorporate aggregate representations of users instead\nof automatically learning a representation directly from a raw time series of\nuser actions. RNNs can automatically learn features, but can not be directly\ntrained with examples of non-returning users who have no target value for their\nreturn time. We develop a novel RNN survival model that removes the limitations\nof the state of the art methods. We demonstrate that this model can\nsuccessfully be applied to return time prediction on a large e-commerce dataset\nwith a superior ability to discriminate between returning and non-returning\nusers than either method applied in isolation.\n",
        "published": "2018",
        "authors": [
            "Georg L. Grob",
            "\u00c2ngelo Cardoso",
            "C. H. Bryan Liu",
            "Duncan A. Little",
            "Benjamin Paul Chamberlain"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.03962v5",
        "title": "A Convergence Theory for Deep Learning via Over-Parameterization",
        "abstract": "  Deep neural networks (DNNs) have demonstrated dominating performance in many\nfields; since AlexNet, networks used in practice are going wider and deeper. On\nthe theoretical side, a long line of works has been focusing on training neural\nnetworks with one hidden layer. The theory of multi-layer networks remains\nlargely unsettled.\n  In this work, we prove why stochastic gradient descent (SGD) can find\n$\\textit{global minima}$ on the training objective of DNNs in\n$\\textit{polynomial time}$. We only make two assumptions: the inputs are\nnon-degenerate and the network is over-parameterized. The latter means the\nnetwork width is sufficiently large: $\\textit{polynomial}$ in $L$, the number\nof layers and in $n$, the number of samples.\n  Our key technique is to derive that, in a sufficiently large neighborhood of\nthe random initialization, the optimization landscape is almost-convex and\nsemi-smooth even with ReLU activations. This implies an equivalence between\nover-parameterized neural networks and neural tangent kernel (NTK) in the\nfinite (and polynomial) width setting.\n  As concrete examples, starting from randomly initialized weights, we prove\nthat SGD can attain 100% training accuracy in classification tasks, or minimize\nregression loss in linear convergence speed, with running time polynomial in\n$n,L$. Our theory applies to the widely-used but non-smooth ReLU activation,\nand to any smooth and possibly non-convex loss functions. In terms of network\narchitectures, our theory at least applies to fully-connected neural networks,\nconvolutional neural networks (CNN), and residual neural networks (ResNet).\n",
        "published": "2018",
        "authors": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li",
            "Zhao Song"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.04918v6",
        "title": "Learning and Generalization in Overparameterized Neural Networks, Going\n  Beyond Two Layers",
        "abstract": "  The fundamental learning theory behind neural networks remains largely open.\nWhat classes of functions can neural networks actually learn? Why doesn't the\ntrained network overfit when it is overparameterized?\n  In this work, we prove that overparameterized neural networks can learn some\nnotable concept classes, including two and three-layer networks with fewer\nparameters and smooth activations. Moreover, the learning can be simply done by\nSGD (stochastic gradient descent) or its variants in polynomial time using\npolynomially many samples. The sample complexity can also be almost independent\nof the number of parameters in the network.\n  On the technique side, our analysis goes beyond the so-called NTK (neural\ntangent kernel) linearization of neural networks in prior works. We establish a\nnew notion of quadratic approximation of the neural network (that can be viewed\nas a second-order variant of NTK), and connect it to the SGD theory of escaping\nsaddle points.\n",
        "published": "2018",
        "authors": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li",
            "Yingyu Liang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.05537v1",
        "title": "Data Driven Governing Equations Approximation Using Deep Neural Networks",
        "abstract": "  We present a numerical framework for approximating unknown governing\nequations using observation data and deep neural networks (DNN). In particular,\nwe propose to use residual network (ResNet) as the basic building block for\nequation approximation. We demonstrate that the ResNet block can be considered\nas a one-step method that is exact in temporal integration. We then present two\nmulti-step methods, recurrent ResNet (RT-ResNet) method and recursive ReNet\n(RS-ResNet) method. The RT-ResNet is a multi-step method on uniform time steps,\nwhereas the RS-ResNet is an adaptive multi-step method using variable time\nsteps. All three methods presented here are based on integral form of the\nunderlying dynamical system. As a result, they do not require time derivative\ndata for equation recovery and can cope with relatively coarsely distributed\ntrajectory data. Several numerical examples are presented to demonstrate the\nperformance of the methods.\n",
        "published": "2018",
        "authors": [
            "Tong Qin",
            "Kailiang Wu",
            "Dongbin Xiu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.12809v1",
        "title": "Computing Vertex Centrality Measures in Massive Real Networks with a\n  Neural Learning Model",
        "abstract": "  Vertex centrality measures are a multi-purpose analysis tool, commonly used\nin many application environments to retrieve information and unveil knowledge\nfrom the graphs and network structural properties. However, the algorithms of\nsuch metrics are expensive in terms of computational resources when running\nreal-time applications or massive real world networks. Thus, approximation\ntechniques have been developed and used to compute the measures in such\nscenarios. In this paper, we demonstrate and analyze the use of neural network\nlearning algorithms to tackle such task and compare their performance in terms\nof solution quality and computation time with other techniques from the\nliterature. Our work offers several contributions. We highlight both the pros\nand cons of approximating centralities though neural learning. By empirical\nmeans and statistics, we then show that the regression model generated with a\nfeedforward neural networks trained by the Levenberg-Marquardt algorithm is not\nonly the best option considering computational resources, but also achieves the\nbest solution quality for relevant applications and large-scale networks.\nKeywords: Vertex Centrality Measures, Neural Networks, Complex Network Models,\nMachine Learning, Regression Model\n",
        "published": "2018",
        "authors": [
            "Felipe Grando",
            "Luis C. Lamb"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.07298v2",
        "title": "Online Estimation of Multiple Dynamic Graphs in Pattern Sequences",
        "abstract": "  Sequences of correlated binary patterns can represent many time-series data\nincluding text, movies, and biological signals. These patterns may be described\nby weighted combinations of a few dominant structures that underpin specific\ninteractions among the binary elements. To extract the dominant correlation\nstructures and their contributions to generating data in a time-dependent\nmanner, we model the dynamics of binary patterns using the state-space model of\nan Ising-type network that is composed of multiple undirected graphs. We\nprovide a sequential Bayes algorithm to estimate the dynamics of weights on the\ngraphs while gaining the graph structures online. This model can uncover\noverlapping graphs underlying the data better than a traditional orthogonal\ndecomposition method, and outperforms an original time-dependent Ising model.\nWe assess the performance of the method by simulated data, and demonstrate that\nspontaneous activity of cultured hippocampal neurons is represented by dynamics\nof multiple graphs.\n",
        "published": "2019",
        "authors": [
            "Jimmy Gaudreault",
            "Arunabh Saxena",
            "Hideaki Shimazaki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.10860v4",
        "title": "Learning Context-Dependent Choice Functions",
        "abstract": "  Choice functions accept a set of alternatives as input and produce a\npreferred subset of these alternatives as output. We study the problem of\nlearning such functions under conditions of context-dependence of preferences,\nwhich means that the preference in favor of a certain choice alternative may\ndepend on what other options are also available. In spite of its practical\nrelevance, this kind of context-dependence has received little attention in\npreference learning so far. We propose a suitable model based on\ncontext-dependent (latent) utility functions, thereby reducing the problem to\nthe task of learning such utility functions. Practically, this comes with a\nnumber of challenges. For example, the set of alternatives provided as input to\na choice function can be of any size, and the output of the function should not\ndepend on the order in which the alternatives are presented. To meet these\nrequirements, we propose two general approaches based on two representations of\ncontext-dependent utility functions, as well as instantiations in the form of\nappropriate end-to-end trainable neural network architectures. Moreover, to\ndemonstrate the performance of both networks, we present extensive empirical\nevaluations on both synthetic and real-world datasets.\n",
        "published": "2019",
        "authors": [
            "Karlson Pfannschmidt",
            "Pritha Gupta",
            "Bj\u00f6rn Haddenhorst",
            "Eyke H\u00fcllermeier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.01028v2",
        "title": "Can SGD Learn Recurrent Neural Networks with Provable Generalization?",
        "abstract": "  Recurrent Neural Networks (RNNs) are among the most popular models in\nsequential data analysis. Yet, in the foundational PAC learning language, what\nconcept class can it learn? Moreover, how can the same recurrent unit\nsimultaneously learn functions from different input tokens to different output\ntokens, without affecting each other? Existing generalization bounds for RNN\nscale exponentially with the input length, significantly limiting their\npractical implications.\n  In this paper, we show using the vanilla stochastic gradient descent (SGD),\nRNN can actually learn some notable concept class efficiently, meaning that\nboth time and sample complexity scale polynomially in the input length (or\nalmost polynomially, depending on the concept). This concept class at least\nincludes functions where each output token is generated from inputs of earlier\ntokens using a smooth two-layer neural network.\n",
        "published": "2019",
        "authors": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.06948v2",
        "title": "Data-Driven Deep Learning of Partial Differential Equations in Modal\n  Space",
        "abstract": "  We present a framework for recovering/approximating unknown time-dependent\npartial differential equation (PDE) using its solution data. Instead of\nidentifying the terms in the underlying PDE, we seek to approximate the\nevolution operator of the underlying PDE numerically. The evolution operator of\nthe PDE, defined in infinite-dimensional space, maps the solution from a\ncurrent time to a future time and completely characterizes the solution\nevolution of the underlying unknown PDE. Our recovery strategy relies on\napproximation of the evolution operator in a properly defined modal space,\ni.e., generalized Fourier space, in order to reduce the problem to finite\ndimensions. The finite dimensional approximation is then accomplished by\ntraining a deep neural network structure, which is based on residual network\n(ResNet), using the given data. Error analysis is provided to illustrate the\npredictive accuracy of the proposed method. A set of examples of different\ntypes of PDEs, including inviscid Burgers' equation that develops discontinuity\nin its solution, are presented to demonstrate the effectiveness of the proposed\nmethod.\n",
        "published": "2019",
        "authors": [
            "Kailiang Wu",
            "Dongbin Xiu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.09599v4",
        "title": "On the space-time expressivity of ResNets",
        "abstract": "  Residual networks (ResNets) are a deep learning architecture that\nsubstantially improved the state of the art performance in certain supervised\nlearning tasks. Since then, they have received continuously growing attention.\nResNets have a recursive structure $x_{k+1} = x_k + R_k(x_k)$ where $R_k$ is a\nneural network called a residual block. This structure can be seen as the Euler\ndiscretisation of an associated ordinary differential equation (ODE) which is\ncalled a neural ODE. Recently, ResNets were proposed as the space-time\napproximation of ODEs which are not of this neural type. To elaborate this\nconnection we show that by increasing the number of residual blocks as well as\ntheir expressivity the solution of an arbitrary ODE can be approximated in\nspace and time simultaneously by deep ReLU ResNets. Further, we derive\nestimates on the complexity of the residual blocks required to obtain a\nprescribed accuracy under certain regularity assumptions.\n",
        "published": "2019",
        "authors": [
            "Johannes M\u00fcller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.12620v3",
        "title": "AeGAN: Time-Frequency Speech Denoising via Generative Adversarial\n  Networks",
        "abstract": "  Automatic speech recognition (ASR) systems are of vital importance nowadays\nin commonplace tasks such as speech-to-text processing and language\ntranslation. This created the need for an ASR system that can operate in\nrealistic crowded environments. Thus, speech enhancement is a valuable building\nblock in ASR systems and other applications such as hearing aids, smartphones\nand teleconferencing systems. In this paper, a generative adversarial network\n(GAN) based framework is investigated for the task of speech enhancement, more\nspecifically speech denoising of audio tracks. A new architecture based on\nCasNet generator and an additional feature-based loss are incorporated to get\nrealistically denoised speech phonetics. Finally, the proposed framework is\nshown to outperform other learning and traditional model-based speech\nenhancement approaches.\n",
        "published": "2019",
        "authors": [
            "Sherif Abdulatif",
            "Karim Armanious",
            "Karim Guirguis",
            "Jayasankar T. Sajeev",
            "Bin Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.01190v3",
        "title": "Predicting the outputs of finite deep neural networks trained with noisy\n  gradients",
        "abstract": "  A recent line of works studied wide deep neural networks (DNNs) by\napproximating them as Gaussian Processes (GPs). A DNN trained with gradient\nflow was shown to map to a GP governed by the Neural Tangent Kernel (NTK),\nwhereas earlier works showed that a DNN with an i.i.d. prior over its weights\nmaps to the so-called Neural Network Gaussian Process (NNGP). Here we consider\na DNN training protocol, involving noise, weight decay and finite width, whose\noutcome corresponds to a certain non-Gaussian stochastic process. An analytical\nframework is then introduced to analyze this non-Gaussian process, whose\ndeviation from a GP is controlled by the finite width. Our contribution is\nthree-fold: (i) In the infinite width limit, we establish a correspondence\nbetween DNNs trained with noisy gradients and the NNGP, not the NTK. (ii) We\nprovide a general analytical form for the finite width correction (FWC) for\nDNNs with arbitrary activation functions and depth and use it to predict the\noutputs of empirical finite networks with high accuracy. Analyzing the FWC\nbehavior as a function of $n$, the training set size, we find that it is\nnegligible for both the very small $n$ regime, and, surprisingly, for the large\n$n$ regime (where the GP error scales as $O(1/n)$). (iii) We flesh out\nalgebraically how these FWCs can improve the performance of finite\nconvolutional neural networks (CNNs) relative to their GP counterparts on image\nclassification tasks.\n",
        "published": "2020",
        "authors": [
            "Gadi Naveh",
            "Oded Ben-David",
            "Haim Sompolinsky",
            "Zohar Ringel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.01902v2",
        "title": "Rational neural networks",
        "abstract": "  We consider neural networks with rational activation functions. The choice of\nthe nonlinear activation function in deep learning architectures is crucial and\nheavily impacts the performance of a neural network. We establish optimal\nbounds in terms of network complexity and prove that rational neural networks\napproximate smooth functions more efficiently than ReLU networks with\nexponentially smaller depth. The flexibility and smoothness of rational\nactivation functions make them an attractive alternative to ReLU, as we\ndemonstrate with numerical experiments.\n",
        "published": "2020",
        "authors": [
            "Nicolas Boull\u00e9",
            "Yuji Nakatsukasa",
            "Alex Townsend"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.11120v1",
        "title": "Software-Level Accuracy Using Stochastic Computing With\n  Charge-Trap-Flash Based Weight Matrix",
        "abstract": "  The in-memory computing paradigm with emerging memory devices has been\nrecently shown to be a promising way to accelerate deep learning. Resistive\nprocessing unit (RPU) has been proposed to enable the vector-vector outer\nproduct in a crossbar array using a stochastic train of identical pulses to\nenable one-shot weight update, promising intense speed-up in matrix\nmultiplication operations, which form the bulk of training neural networks.\nHowever, the performance of the system suffers if the device does not satisfy\nthe condition of linear conductance change over around 1,000 conductance\nlevels. This is a challenge for nanoscale memories. Recently, Charge Trap Flash\n(CTF) memory was shown to have a large number of levels before saturation, but\nvariable non-linearity. In this paper, we explore the trade-off between the\nrange of conductance change and linearity. We show, through simulations, that\nat an optimum choice of the range, our system performs nearly as well as the\nmodels trained using exact floating point operations, with less than 1%\nreduction in the performance. Our system reaches an accuracy of 97.9% on MNIST\ndataset, 89.1% and 70.5% accuracy on CIFAR-10 and CIFAR-100 datasets (using\npre-extracted features). We also show its use in reinforcement learning, where\nit is used for value function approximation in Q-Learning, and learns to\ncomplete an episode the mountain car control problem in around 146 steps.\nBenchmarked to state-of-the-art, the CTF based RPU shows best in class\nperformance to enable software equivalent performance.\n",
        "published": "2020",
        "authors": [
            "Varun Bhatt",
            "Shalini Shrivastava",
            "Tanmay Chavan",
            "Udayan Ganguly"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.06051v4",
        "title": "SparkNet: Training Deep Networks in Spark",
        "abstract": "  Training deep networks is a time-consuming process, with networks for object\nrecognition often requiring multiple days to train. For this reason, leveraging\nthe resources of a cluster to speed up training is an important area of work.\nHowever, widely-popular batch-processing computational frameworks like\nMapReduce and Spark were not designed to support the asynchronous and\ncommunication-intensive workloads of existing distributed deep learning\nsystems. We introduce SparkNet, a framework for training deep networks in\nSpark. Our implementation includes a convenient interface for reading data from\nSpark RDDs, a Scala interface to the Caffe deep learning framework, and a\nlightweight multi-dimensional tensor library. Using a simple parallelization\nscheme for stochastic gradient descent, SparkNet scales well with the cluster\nsize and tolerates very high-latency communication. Furthermore, it is easy to\ndeploy and use with no parameter tuning, and it is compatible with existing\nCaffe models. We quantify the dependence of the speedup obtained by SparkNet on\nthe number of machines, the communication frequency, and the cluster's\ncommunication overhead, and we benchmark our system's performance on the\nImageNet dataset.\n",
        "published": "2015",
        "authors": [
            "Philipp Moritz",
            "Robert Nishihara",
            "Ion Stoica",
            "Michael I. Jordan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.05963v1",
        "title": "Machine learning approximation algorithms for high-dimensional fully\n  nonlinear partial differential equations and second-order backward stochastic\n  differential equations",
        "abstract": "  High-dimensional partial differential equations (PDE) appear in a number of\nmodels from the financial industry, such as in derivative pricing models,\ncredit valuation adjustment (CVA) models, or portfolio optimization models. The\nPDEs in such applications are high-dimensional as the dimension corresponds to\nthe number of financial assets in a portfolio. Moreover, such PDEs are often\nfully nonlinear due to the need to incorporate certain nonlinear phenomena in\nthe model such as default risks, transaction costs, volatility uncertainty\n(Knightian uncertainty), or trading constraints in the model. Such\nhigh-dimensional fully nonlinear PDEs are exceedingly difficult to solve as the\ncomputational effort for standard approximation methods grows exponentially\nwith the dimension. In this work we propose a new method for solving\nhigh-dimensional fully nonlinear second-order PDEs. Our method can in\nparticular be used to sample from high-dimensional nonlinear expectations. The\nmethod is based on (i) a connection between fully nonlinear second-order PDEs\nand second-order backward stochastic differential equations (2BSDEs), (ii) a\nmerged formulation of the PDE and the 2BSDE problem, (iii) a temporal forward\ndiscretization of the 2BSDE and a spatial approximation via deep neural nets,\nand (iv) a stochastic gradient descent-type optimization procedure. Numerical\nresults obtained using ${\\rm T{\\small ENSOR}F{\\small LOW}}$ in ${\\rm P{\\small\nYTHON}}$ illustrate the efficiency and the accuracy of the method in the cases\nof a $100$-dimensional Black-Scholes-Barenblatt equation, a $100$-dimensional\nHamilton-Jacobi-Bellman equation, and a nonlinear expectation of a $ 100\n$-dimensional $ G $-Brownian motion.\n",
        "published": "2017",
        "authors": [
            "Christian Beck",
            "Weinan E",
            "Arnulf Jentzen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.01515v1",
        "title": "Electroencephalogram (EEG) for Delineating Objective Measure of Autism\n  Spectrum Disorder (ASD) (Extended Version)",
        "abstract": "  Autism Spectrum Disorder (ASD) is a developmental disorder that often impairs\na child's normal development of the brain. According to CDC, it is estimated\nthat 1 in 6 children in the US suffer from development disorders, and 1 in 68\nchildren in the US suffer from ASD. This condition has a negative impact on a\nperson's ability to hear, socialize and communicate. Overall, ASD has a broad\nrange of symptoms and severity; hence the term spectrum is used. One of the\nmain contributors to ASD is known to be genetics. Up to date, no suitable cure\nfor ASD has been found. Early diagnosis is crucial for the long-term treatment\nof ASD, but this is challenging due to the lack of a proper objective measures.\nSubjective measures often take more time, resources, and have false positives\nor false negatives. There is a need for efficient objective measures that can\nhelp in diagnosing this disease early as possible with less effort.\n  EEG measures the electric signals of the brain via electrodes placed on\nvarious places on the scalp. These signals can be used to study complex\nneuropsychiatric issues. Studies have shown that EEG has the potential to be\nused as a biomarker for various neurological conditions including ASD. This\nchapter will outline the usage of EEG measurement for the classification of ASD\nusing machine learning algorithms.\n",
        "published": "2019",
        "authors": [
            "Yasith Jayawardana",
            "Mark Jaime",
            "Sashi Thapaliya",
            "Sampath Jayarathna"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.11129v1",
        "title": "Semisupervised Adversarial Neural Networks for Cyber Security Transfer\n  Learning",
        "abstract": "  On the path to establishing a global cybersecurity framework where each\nenterprise shares information about malicious behavior, an important question\narises. How can a machine learning representation characterizing a cyber attack\non one network be used to detect similar attacks on other enterprise networks\nif each networks has wildly different distributions of benign and malicious\ntraffic? We address this issue by comparing the results of naively transferring\na model across network domains and using CORrelation ALignment, to our novel\nadversarial Siamese neural network. Our proposed model learns attack\nrepresentations that are more invariant to each network's particularities via\nan adversarial approach. It uses a simple ranking loss that prioritizes the\nlabeling of the most egregious malicious events correctly over average\naccuracy. This is appropriate for driving an alert triage workflow wherein an\nanalyst only has time to inspect the top few events ranked highest by the\nmodel. In terms of accuracy, the other approaches fail completely to detect any\nmalicious events when models were trained on one dataset are evaluated on\nanother for the first 100 events. While, the method presented here retrieves\nsizable proportions of malicious events, at the expense of some training\ninstabilities due in adversarial modeling. We evaluate these approaches using 2\npublicly available networking datasets, and suggest areas for future research.\n",
        "published": "2019",
        "authors": [
            "Casey Kneale",
            "Kolia Sadeghi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.05744v1",
        "title": "Heuristic Dynamic Programming for Adaptive Virtual Synchronous\n  Generators",
        "abstract": "  In this paper a neural network heuristic dynamic programing (HDP) is used for\noptimal control of the virtual inertia based control of grid connected three\nphase inverters. It is shown that the conventional virtual inertia controllers\nare not suited for non inductive grids. A neural network based controller is\nproposed to adapt to any impedance angle. Applying an adaptive dynamic\nprogramming controller instead of a supervised controlled method enables the\nsystem to adjust itself to different conditions. The proposed HDP consists of\ntwo subnetworks, critic network and action network. These networks can be\ntrained during the same training cycle to decrease the training time. The\nsimulation results confirm that the proposed neural network HDP controller\nperforms better than the traditional direct fed voltage and reactive power\ncontrollers in virtual inertia control schemes.\n",
        "published": "2019",
        "authors": [
            "Sepehr Saadatmand",
            "Mohammad Saleh Sanjarinia",
            "Pourya Shamsi",
            "Mehdi Ferdowsi",
            "Donald C. Wunsch"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1712.10158v1",
        "title": "Non-linear motor control by local learning in spiking neural networks",
        "abstract": "  Learning weights in a spiking neural network with hidden neurons, using\nlocal, stable and online rules, to control non-linear body dynamics is an open\nproblem. Here, we employ a supervised scheme, Feedback-based Online Local\nLearning Of Weights (FOLLOW), to train a network of heterogeneous spiking\nneurons with hidden layers, to control a two-link arm so as to reproduce a\ndesired state trajectory. The network first learns an inverse model of the\nnon-linear dynamics, i.e. from state trajectory as input to the network, it\nlearns to infer the continuous-time command that produced the trajectory.\nConnection weights are adjusted via a local plasticity rule that involves\npre-synaptic firing and post-synaptic feedback of the error in the inferred\ncommand. We choose a network architecture, termed differential feedforward,\nthat gives the lowest test error from different feedforward and recurrent\narchitectures. The learned inverse model is then used to generate a\ncontinuous-time motor command to control the arm, given a desired trajectory.\n",
        "published": "2017",
        "authors": [
            "Aditya Gilra",
            "Wulfram Gerstner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.08655v3",
        "title": "Surrogate-assisted Bayesian inversion for landscape and basin evolution\n  models",
        "abstract": "  The complex and computationally expensive nature of landscape evolution\nmodels pose significant challenges in the inference and optimisation of unknown\nparameters. Bayesian inference provides a methodology for estimation and\nuncertainty quantification of unknown model parameters. In our previous work,\nwe developed parallel tempering Bayeslands as a framework for parameter\nestimation and uncertainty quantification for the Badlands landscape evolution\nmodel. Parallel tempering Bayeslands features high-performance computing with\ndozens of processing cores running in parallel to enhance computational\nefficiency. Although we use parallel computing, the procedure remains\ncomputationally challenging since thousands of samples need to be drawn and\nevaluated. \\textcolor{black}{In large-scale landscape and basin evolution\nproblems, a single model evaluation can take from several minutes to hours, and\nin some instances, even days. Surrogate-assisted optimisation has been used for\nseveral computationally expensive engineering problems which motivate its use\nin optimisation and inference of complex geoscientific models.} The use of\nsurrogate models can speed up parallel tempering Bayeslands by developing\ncomputationally inexpensive models to mimic expensive ones. In this paper, we\napply surrogate-assisted parallel tempering where that surrogate mimics a\nlandscape evolution model by estimating the likelihood function from the model.\n\\textcolor{black}{We employ a neural network-based surrogate model that learns\nfrom the history of samples generated. } The entire framework is developed in a\nparallel computing infrastructure to take advantage of parallelism. The results\nshow that the proposed methodology is effective in lowering the overall\ncomputational cost significantly while retaining the quality of solutions.\n",
        "published": "2018",
        "authors": [
            "Rohitash Chandra",
            "Danial Azam",
            "Arpit Kapoor",
            "R. Dietmar M\u00fcller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.03759v1",
        "title": "Machine Learning Based Prediction and Classification of Computational\n  Jobs in Cloud Computing Centers",
        "abstract": "  With the rapid growth of the data volume and the fast increasing of the\ncomputational model complexity in the scenario of cloud computing, it becomes\nan important topic that how to handle users' requests by scheduling\ncomputational jobs and assigning the resources in data center.\n  In order to have a better perception of the computing jobs and their requests\nof resources, we analyze its characteristics and focus on the prediction and\nclassification of the computing jobs with some machine learning approaches.\nSpecifically, we apply LSTM neural network to predict the arrival of the jobs\nand the aggregated requests for computing resources. Then we evaluate it on\nGoogle Cluster dataset and it shows that the accuracy has been improved\ncompared to the current existing methods. Additionally, to have a better\nunderstanding of the computing jobs, we use an unsupervised hierarchical\nclustering algorithm, BIRCH, to make classification and get some\ninterpretability of our results in the computing centers.\n",
        "published": "2019",
        "authors": [
            "Zheqi Zhu",
            "Pingyi Fan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.09688v2",
        "title": "Symbolic Regression Methods for Reinforcement Learning",
        "abstract": "  Reinforcement learning algorithms can solve dynamic decision-making and\noptimal control problems. With continuous-valued state and input variables,\nreinforcement learning algorithms must rely on function approximators to\nrepresent the value function and policy mappings. Commonly used numerical\napproximators, such as neural networks or basis function expansions, have two\nmain drawbacks: they are black-box models offering little insight into the\nmappings learned, and they require extensive trial and error tuning of their\nhyper-parameters. In this paper, we propose a new approach to constructing\nsmooth value functions in the form of analytic expressions by using symbolic\nregression. We introduce three off-line methods for finding value functions\nbased on a state-transition model: symbolic value iteration, symbolic policy\niteration, and a direct solution of the Bellman equation. The methods are\nillustrated on four nonlinear control problems: velocity control under\nfriction, one-link and two-link pendulum swing-up, and magnetic manipulation.\nThe results show that the value functions yield well-performing policies and\nare compact, mathematically tractable, and easy to plug into other algorithms.\nThis makes them potentially suitable for further analysis of the closed-loop\nsystem. A comparison with an alternative approach using neural networks shows\nthat our method outperforms the neural network-based one.\n",
        "published": "2019",
        "authors": [
            "Ji\u0159\u00ed Kubal\u00edk",
            "Erik Derner",
            "Jan \u017degklitz",
            "Robert Babu\u0161ka"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.07242v1",
        "title": "More Data Can Hurt for Linear Regression: Sample-wise Double Descent",
        "abstract": "  In this expository note we describe a surprising phenomenon in\noverparameterized linear regression, where the dimension exceeds the number of\nsamples: there is a regime where the test risk of the estimator found by\ngradient descent increases with additional samples. In other words, more data\nactually hurts the estimator. This behavior is implicit in a recent line of\ntheoretical works analyzing \"double-descent\" phenomenon in linear models. In\nthis note, we isolate and understand this behavior in an extremely simple\nsetting: linear regression with isotropic Gaussian covariates. In particular,\nthis occurs due to an unconventional type of bias-variance tradeoff in the\noverparameterized regime: the bias decreases with more samples, but variance\nincreases.\n",
        "published": "2019",
        "authors": [
            "Preetum Nakkiran"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.10703v2",
        "title": "Variational Recurrent Models for Solving Partially Observable Control\n  Tasks",
        "abstract": "  In partially observable (PO) environments, deep reinforcement learning (RL)\nagents often suffer from unsatisfactory performance, since two problems need to\nbe tackled together: how to extract information from the raw observations to\nsolve the task, and how to improve the policy. In this study, we propose an RL\nalgorithm for solving PO tasks. Our method comprises two parts: a variational\nrecurrent model (VRM) for modeling the environment, and an RL controller that\nhas access to both the environment and the VRM. The proposed algorithm was\ntested in two types of PO robotic control tasks, those in which either\ncoordinates or velocities were not observable and those that require long-term\nmemorization. Our experiments show that the proposed algorithm achieved better\ndata efficiency and/or learned more optimal policy than other alternative\napproaches in tasks in which unobserved states cannot be inferred from raw\nobservations in a simple manner.\n",
        "published": "2019",
        "authors": [
            "Dongqi Han",
            "Kenji Doya",
            "Jun Tani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.04413v6",
        "title": "Backward Feature Correction: How Deep Learning Performs Deep\n  (Hierarchical) Learning",
        "abstract": "  Deep learning is also known as hierarchical learning, where the learner\n_learns_ to represent a complicated target function by decomposing it into a\nsequence of simpler functions to reduce sample and time complexity. This paper\nformally analyzes how multi-layer neural networks can perform such hierarchical\nlearning _efficiently_ and _automatically_ by SGD on the training objective.\n  On the conceptual side, we present a theoretical characterizations of how\ncertain types of deep (i.e. super-constant layer) neural networks can still be\nsample and time efficiently trained on some hierarchical tasks, when no\nexisting algorithm (including layerwise training, kernel method, etc) is known\nto be efficient. We establish a new principle called \"backward feature\ncorrection\", where the errors in the lower-level features can be automatically\ncorrected when training together with the higher-level layers. We believe this\nis a key behind how deep learning is performing deep (hierarchical) learning,\nas opposed to layerwise learning or simulating some non-hierarchical method.\n  On the technical side, we show for every input dimension $d > 0$, there is a\nconcept class of degree $\\omega(1)$ multi-variate polynomials so that, using\n$\\omega(1)$-layer neural networks as learners, SGD can learn any function from\nthis class in $\\mathsf{poly}(d)$ time to any $\\frac{1}{\\mathsf{poly}(d)}$\nerror, through learning to represent it as a composition of $\\omega(1)$ layers\nof quadratic functions using \"backward feature correction.\" In contrast, we do\nnot know any other simpler algorithm (including layerwise training, applying\nkernel method sequentially, training a two-layer network, etc) that can learn\nthis concept class in $\\mathsf{poly}(d)$ time even to any $d^{-0.01}$ error. As\na side result, we prove $d^{\\omega(1)}$ lower bounds for several\nnon-hierarchical learners, including any kernel methods.\n",
        "published": "2020",
        "authors": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.06938v2",
        "title": "Memory capacity of neural networks with threshold and ReLU activations",
        "abstract": "  Overwhelming theoretical and empirical evidence shows that mildly\noverparametrized neural networks -- those with more connections than the size\nof the training data -- are often able to memorize the training data with\n$100\\%$ accuracy. This was rigorously proved for networks with sigmoid\nactivation functions and, very recently, for ReLU activations. Addressing a\n1988 open question of Baum, we prove that this phenomenon holds for general\nmultilayered perceptrons, i.e. neural networks with threshold activation\nfunctions, or with any mix of threshold and ReLU activations. Our construction\nis probabilistic and exploits sparsity.\n",
        "published": "2020",
        "authors": [
            "Roman Vershynin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.08290v2",
        "title": "Transformer-based Online CTC/attention End-to-End Speech Recognition\n  Architecture",
        "abstract": "  Recently, Transformer has gained success in automatic speech recognition\n(ASR) field. However, it is challenging to deploy a Transformer-based\nend-to-end (E2E) model for online speech recognition. In this paper, we propose\nthe Transformer-based online CTC/attention E2E ASR architecture, which contains\nthe chunk self-attention encoder (chunk-SAE) and the monotonic truncated\nattention (MTA) based self-attention decoder (SAD). Firstly, the chunk-SAE\nsplits the speech into isolated chunks. To reduce the computational cost and\nimprove the performance, we propose the state reuse chunk-SAE. Sencondly, the\nMTA based SAD truncates the speech features monotonically and performs\nattention on the truncated features. To support the online recognition, we\nintegrate the state reuse chunk-SAE and the MTA based SAD into online\nCTC/attention architecture. We evaluate the proposed online models on the HKUST\nMandarin ASR benchmark and achieve a 23.66% character error rate (CER) with a\n320 ms latency. Our online model yields as little as 0.19% absolute CER\ndegradation compared with the offline baseline, and achieves significant\nimprovement over our prior work on Long Short-Term Memory (LSTM) based online\nE2E models.\n",
        "published": "2020",
        "authors": [
            "Haoran Miao",
            "Gaofeng Cheng",
            "Changfeng Gao",
            "Pengyuan Zhang",
            "Yonghong Yan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.01897v2",
        "title": "Optimal Regularization Can Mitigate Double Descent",
        "abstract": "  Recent empirical and theoretical studies have shown that many learning\nalgorithms -- from linear regression to neural networks -- can have test\nperformance that is non-monotonic in quantities such the sample size and model\nsize. This striking phenomenon, often referred to as \"double descent\", has\nraised questions of if we need to re-think our current understanding of\ngeneralization. In this work, we study whether the double-descent phenomenon\ncan be avoided by using optimal regularization. Theoretically, we prove that\nfor certain linear regression models with isotropic data distribution,\noptimally-tuned $\\ell_2$ regularization achieves monotonic test performance as\nwe grow either the sample size or the model size. We also demonstrate\nempirically that optimally-tuned $\\ell_2$ regularization can mitigate double\ndescent for more general models, including neural networks. Our results suggest\nthat it may also be informative to study the test risk scalings of various\nalgorithms in the context of appropriately tuned regularization.\n",
        "published": "2020",
        "authors": [
            "Preetum Nakkiran",
            "Prayaag Venkat",
            "Sham Kakade",
            "Tengyu Ma"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.02436v1",
        "title": "Talking-Heads Attention",
        "abstract": "  We introduce \"talking-heads attention\" - a variation on multi-head attention\nwhich includes linearprojections across the attention-heads dimension,\nimmediately before and after the softmax operation.While inserting only a small\nnumber of additional parameters and a moderate amount of additionalcomputation,\ntalking-heads attention leads to better perplexities on masked language\nmodeling tasks, aswell as better quality when transfer-learning to language\ncomprehension and question answering tasks.\n",
        "published": "2020",
        "authors": [
            "Noam Shazeer",
            "Zhenzhong Lan",
            "Youlong Cheng",
            "Nan Ding",
            "Le Hou"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.10523v1",
        "title": "Neural Networks and Polynomial Regression. Demystifying the\n  Overparametrization Phenomena",
        "abstract": "  In the context of neural network models, overparametrization refers to the\nphenomena whereby these models appear to generalize well on the unseen data,\neven though the number of parameters significantly exceeds the sample sizes,\nand the model perfectly fits the in-training data. A conventional explanation\nof this phenomena is based on self-regularization properties of algorithms used\nto train the data. In this paper we prove a series of results which provide a\nsomewhat diverging explanation. Adopting a teacher/student model where the\nteacher network is used to generate the predictions and student network is\ntrained on the observed labeled data, and then tested on out-of-sample data, we\nshow that any student network interpolating the data generated by a teacher\nnetwork generalizes well, provided that the sample size is at least an explicit\nquantity controlled by data dimension and approximation guarantee alone,\nregardless of the number of internal nodes of either teacher or student\nnetwork.\n  Our claim is based on approximating both teacher and student networks by\npolynomial (tensor) regression models with degree depending on the desired\naccuracy and network depth only. Such a parametrization notably does not depend\non the number of internal nodes. Thus a message implied by our results is that\nparametrizing wide neural networks by the number of hidden nodes is misleading,\nand a more fitting measure of parametrization complexity is the number of\nregression coefficients associated with tensorized data. In particular, this\nsomewhat reconciles the generalization ability of neural networks with more\nclassical statistical notions of data complexity and generalization bounds. Our\nempirical results on MNIST and Fashion-MNIST datasets indeed confirm that\ntensorized regression achieves a good out-of-sample performance, even when the\ndegree of the tensor is at most two.\n",
        "published": "2020",
        "authors": [
            "Matt Emschwiller",
            "David Gamarnik",
            "Eren C. K\u0131z\u0131lda\u011f",
            "Ilias Zadik"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.11640v1",
        "title": "Transfer between long-term and short-term memory using Conceptors",
        "abstract": "  We introduce a recurrent neural network model of working memory combining\nshort-term and long-term components. e short-term component is modelled using a\ngated reservoir model that is trained to hold a value from an input stream when\na gate signal is on. e long-term component is modelled using conceptors in\norder to store inner temporal patterns (that corresponds to values). We combine\nthese two components to obtain a model where information can go from long-term\nmemory to short-term memory and vice-versa and we show how standard operations\non conceptors allow to combine long-term memories and describe their effect on\nshort-term memory.\n",
        "published": "2020",
        "authors": [
            "Anthony Strock",
            "Nicolas Rougier",
            "Xavier Hinaut"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.02341v3",
        "title": "Non-Euclidean Universal Approximation",
        "abstract": "  Modifications to a neural network's input and output layers are often\nrequired to accommodate the specificities of most practical learning tasks.\nHowever, the impact of such changes on architecture's approximation\ncapabilities is largely not understood. We present general conditions\ndescribing feature and readout maps that preserve an architecture's ability to\napproximate any continuous functions uniformly on compacts. As an application,\nwe show that if an architecture is capable of universal approximation, then\nmodifying its final layer to produce binary values creates a new architecture\ncapable of deterministically approximating any classifier. In particular, we\nobtain guarantees for deep CNNs and deep feed-forward networks. Our results\nalso have consequences within the scope of geometric deep learning.\nSpecifically, when the input and output spaces are Cartan-Hadamard manifolds,\nwe obtain geometrically meaningful feature and readout maps satisfying our\ncriteria. Consequently, commonly used non-Euclidean regression models between\nspaces of symmetric positive definite matrices are extended to universal DNNs.\nThe same result allows us to show that the hyperbolic feed-forward networks,\nused for hierarchical learning, are universal. Our result is also used to show\nthat the common practice of randomizing all but the last two layers of a DNN\nproduces a universal family of functions with probability one. We also provide\nconditions on a DNN's first (resp. last) few layer's connections and activation\nfunction which guarantee that these layers can have a width equal to the input\n(resp. output) space's dimension while not negatively affecting the\narchitecture's approximation capabilities.\n",
        "published": "2020",
        "authors": [
            "Anastasis Kratsios",
            "Eugene Bilokopytov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.04791v2",
        "title": "Complexity for deep neural networks and other characteristics of deep\n  feature representations",
        "abstract": "  We define a notion of complexity, which quantifies the nonlinearity of the\ncomputation of a neural network, as well as a complementary measure of the\neffective dimension of feature representations. We investigate these\nobservables both for trained networks for various datasets as well as explore\ntheir dynamics during training, uncovering in particular power law scaling.\nThese observables can be understood in a dual way as uncovering hidden internal\nstructure of the datasets themselves as a function of scale or depth. The\nentropic character of the proposed notion of complexity should allow to\ntransfer modes of analysis from neuroscience and statistical physics to the\ndomain of artificial neural networks. The introduced observables can be applied\nwithout any change to the analysis of biological neuronal systems.\n",
        "published": "2020",
        "authors": [
            "Romuald A. Janik",
            "Przemek Witaszczyk"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.06494v2",
        "title": "Anti-Transfer Learning for Task Invariance in Convolutional Neural\n  Networks for Speech Processing",
        "abstract": "  We introduce the novel concept of anti-transfer learning for speech\nprocessing with convolutional neural networks. While transfer learning assumes\nthat the learning process for a target task will benefit from re-using\nrepresentations learned for another task, anti-transfer avoids the learning of\nrepresentations that have been learned for an orthogonal task, i.e., one that\nis not relevant and potentially misleading for the target task, such as speaker\nidentity for speech recognition or speech content for emotion recognition. In\nanti-transfer learning, we penalize similarity between activations of a network\nbeing trained and another one previously trained on an orthogonal task, which\nyields more suitable representations. This leads to better generalization and\nprovides a degree of control over correlations that are spurious or\nundesirable, e.g. to avoid social bias. We have implemented anti-transfer for\nconvolutional neural networks in different configurations with several\nsimilarity metrics and aggregation functions, which we evaluate and analyze\nwith several speech and audio tasks and settings, using six datasets. We show\nthat anti-transfer actually leads to the intended invariance to the orthogonal\ntask and to more appropriate features for the target task at hand.\nAnti-transfer learning consistently improves classification accuracy in all\ntest cases. While anti-transfer creates computation and memory cost at training\ntime, there is relatively little computation cost when using pre-trained models\nfor orthogonal tasks. Anti-transfer is widely applicable and particularly\nuseful where a specific invariance is desirable or where trained models are\navailable and labeled data for orthogonal tasks are difficult to obtain.\n",
        "published": "2020",
        "authors": [
            "Eric Guizzo",
            "Tillman Weyde",
            "Giacomo Tarroni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.06762v5",
        "title": "Ansor: Generating High-Performance Tensor Programs for Deep Learning",
        "abstract": "  High-performance tensor programs are crucial to guarantee efficient execution\nof deep neural networks. However, obtaining performant tensor programs for\ndifferent operators on various hardware platforms is notoriously challenging.\nCurrently, deep learning systems rely on vendor-provided kernel libraries or\nvarious search strategies to get performant tensor programs. These approaches\neither require significant engineering effort to develop platform-specific\noptimization code or fall short of finding high-performance programs due to\nrestricted search space and ineffective exploration strategy.\n  We present Ansor, a tensor program generation framework for deep learning\napplications. Compared with existing search strategies, Ansor explores many\nmore optimization combinations by sampling programs from a hierarchical\nrepresentation of the search space. Ansor then fine-tunes the sampled programs\nwith evolutionary search and a learned cost model to identify the best\nprograms. Ansor can find high-performance programs that are outside the search\nspace of existing state-of-the-art approaches. In addition, Ansor utilizes a\ntask scheduler to simultaneously optimize multiple subgraphs in deep neural\nnetworks. We show that Ansor improves the execution performance of deep neural\nnetworks relative to the state-of-the-art on the Intel CPU, ARM CPU, and NVIDIA\nGPU by up to $3.8\\times$, $2.6\\times$, and $1.7\\times$, respectively.\n",
        "published": "2020",
        "authors": [
            "Lianmin Zheng",
            "Chengfan Jia",
            "Minmin Sun",
            "Zhao Wu",
            "Cody Hao Yu",
            "Ameer Haj-Ali",
            "Yida Wang",
            "Jun Yang",
            "Danyang Zhuo",
            "Koushik Sen",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.07239v3",
        "title": "Surrogate gradients for analog neuromorphic computing",
        "abstract": "  To rapidly process temporal information at a low metabolic cost, biological\nneurons integrate inputs as an analog sum but communicate with spikes, binary\nevents in time. Analog neuromorphic hardware uses the same principles to\nemulate spiking neural networks with exceptional energy-efficiency. However,\ninstantiating high-performing spiking networks on such hardware remains a\nsignificant challenge due to device mismatch and the lack of efficient training\nalgorithms. Here, we introduce a general in-the-loop learning framework based\non surrogate gradients that resolves these issues. Using the BrainScaleS-2\nneuromorphic system, we show that learning self-corrects for device mismatch\nresulting in competitive spiking network performance on both vision and speech\nbenchmarks. Our networks display sparse spiking activity with, on average, far\nless than one spike per hidden neuron and input, perform inference at rates of\nup to 85 k frames/second, and consume less than 200 mW. In summary, our work\nsets several new benchmarks for low-energy spiking network processing on analog\nneuromorphic hardware and paves the way for future on-chip learning algorithms.\n",
        "published": "2020",
        "authors": [
            "Benjamin Cramer",
            "Sebastian Billaudelle",
            "Simeon Kanya",
            "Aron Leibfried",
            "Andreas Gr\u00fcbl",
            "Vitali Karasenko",
            "Christian Pehle",
            "Korbinian Schreiber",
            "Yannik Stradmann",
            "Johannes Weis",
            "Johannes Schemmel",
            "Friedemann Zenke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.11469v2",
        "title": "Coupling-based Invertible Neural Networks Are Universal Diffeomorphism\n  Approximators",
        "abstract": "  Invertible neural networks based on coupling flows (CF-INNs) have various\nmachine learning applications such as image synthesis and representation\nlearning. However, their desirable characteristics such as analytic\ninvertibility come at the cost of restricting the functional forms. This poses\na question on their representation power: are CF-INNs universal approximators\nfor invertible functions? Without a universality, there could be a well-behaved\ninvertible transformation that the CF-INN can never approximate, hence it would\nrender the model class unreliable. We answer this question by showing a\nconvenient criterion: a CF-INN is universal if its layers contain affine\ncoupling and invertible linear functions as special cases. As its corollary, we\ncan affirmatively resolve a previously unsolved problem: whether normalizing\nflow models based on affine coupling can be universal distributional\napproximators. In the course of proving the universality, we prove a general\ntheorem to show the equivalence of the universality for certain diffeomorphism\nclasses, a theoretical insight that is of interest by itself.\n",
        "published": "2020",
        "authors": [
            "Takeshi Teshima",
            "Isao Ishikawa",
            "Koichi Tojo",
            "Kenta Oono",
            "Masahiro Ikeda",
            "Masashi Sugiyama"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.14560v2",
        "title": "Learning compositional functions via multiplicative weight updates",
        "abstract": "  Compositionality is a basic structural feature of both biological and\nartificial neural networks. Learning compositional functions via gradient\ndescent incurs well known problems like vanishing and exploding gradients,\nmaking careful learning rate tuning essential for real-world applications. This\npaper proves that multiplicative weight updates satisfy a descent lemma\ntailored to compositional functions. Based on this lemma, we derive Madam -- a\nmultiplicative version of the Adam optimiser -- and show that it can train\nstate of the art neural network architectures without learning rate tuning. We\nfurther show that Madam is easily adapted to train natively compressed neural\nnetworks by representing their weights in a logarithmic number system. We\nconclude by drawing connections between multiplicative weight updates and\nrecent findings about synapses in biology.\n",
        "published": "2020",
        "authors": [
            "Jeremy Bernstein",
            "Jiawei Zhao",
            "Markus Meister",
            "Ming-Yu Liu",
            "Anima Anandkumar",
            "Yisong Yue"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.15167v2",
        "title": "Deep Involutive Generative Models for Neural MCMC",
        "abstract": "  We introduce deep involutive generative models, a new architecture for deep\ngenerative modeling, and use them to define Involutive Neural MCMC, a new\napproach to fast neural MCMC. An involutive generative model represents a\nprobability kernel $G(\\phi \\mapsto \\phi')$ as an involutive (i.e.,\nself-inverting) deterministic function $f(\\phi, \\pi)$ on an enlarged state\nspace containing auxiliary variables $\\pi$. We show how to make these models\nvolume preserving, and how to use deep volume-preserving involutive generative\nmodels to make valid Metropolis-Hastings updates based on an auxiliary variable\nscheme with an easy-to-calculate acceptance ratio. We prove that deep\ninvolutive generative models and their volume-preserving special case are\nuniversal approximators for probability kernels. This result implies that with\nenough network capacity and training time, they can be used to learn\narbitrarily complex MCMC updates. We define a loss function and optimization\nalgorithm for training parameters given simulated data. We also provide initial\nexperiments showing that Involutive Neural MCMC can efficiently explore\nmulti-modal distributions that are intractable for Hybrid Monte Carlo, and can\nconverge faster than A-NICE-MC, a recently introduced neural MCMC technique.\n",
        "published": "2020",
        "authors": [
            "Span Spanbauer",
            "Cameron Freer",
            "Vikash Mansinghka"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.15218v3",
        "title": "Traditional and accelerated gradient descent for neural architecture\n  search",
        "abstract": "  In this paper we introduce two algorithms for neural architecture search\n(NASGD and NASAGD) following the theoretical work by two of the authors [5]\nwhich used the geometric structure of optimal transport to introduce the\nconceptual basis for new notions of traditional and accelerated gradient\ndescent algorithms for the optimization of a function on a semi-discrete space.\nOur algorithms, which use the network morphism framework introduced in [2] as a\nbaseline, can analyze forty times as many architectures as the hill climbing\nmethods [2, 14] while using the same computational resources and time and\nachieving comparable levels of accuracy. For example, using NASGD on CIFAR-10,\nour method designs and trains networks with an error rate of 4.06 in only 12\nhours on a single GPU.\n",
        "published": "2020",
        "authors": [
            "Nicolas Garcia Trillos",
            "Felix Morales",
            "Javier Morales"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.02047v2",
        "title": "Relationship between manifold smoothness and adversarial vulnerability\n  in deep learning with local errors",
        "abstract": "  Artificial neural networks can achieve impressive performances, and even\noutperform humans in some specific tasks. Nevertheless, unlike biological\nbrains, the artificial neural networks suffer from tiny perturbations in\nsensory input, under various kinds of adversarial attacks. It is therefore\nnecessary to study the origin of the adversarial vulnerability. Here, we\nestablish a fundamental relationship between geometry of hidden representations\n(manifold perspective) and the generalization capability of the deep networks.\nFor this purpose, we choose a deep neural network trained by local errors, and\nthen analyze emergent properties of trained networks through the manifold\ndimensionality, manifold smoothness, and the generalization capability. To\nexplore effects of adversarial examples, we consider independent Gaussian noise\nattacks and fast-gradient-sign-method (FGSM) attacks. Our study reveals that a\nhigh generalization accuracy requires a relatively fast power-law decay of the\neigen-spectrum of hidden representations. Under Gaussian attacks, the\nrelationship between generalization accuracy and power-law exponent is\nmonotonic, while a non-monotonic behavior is observed for FGSM attacks. Our\nempirical study provides a route towards a final mechanistic interpretation of\nadversarial vulnerability under adversarial attacks.\n",
        "published": "2020",
        "authors": [
            "Zijian Jiang",
            "Jianwen Zhou",
            "Haiping Huang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2007.12371v2",
        "title": "Dopant Network Processing Units: Towards Efficient Neural-network\n  Emulators with High-capacity Nanoelectronic Nodes",
        "abstract": "  The rapidly growing computational demands of deep neural networks require\nnovel hardware designs. Recently, tunable nanoelectronic devices were developed\nbased on hopping electrons through a network of dopant atoms in silicon. These\n\"Dopant Network Processing Units\" (DNPUs) are highly energy-efficient and have\npotentially very high throughput. By adapting the control voltages applied to\nits terminals, a single DNPU can solve a variety of linearly non-separable\nclassification problems. However, using a single device has limitations due to\nthe implicit single-node architecture. This paper presents a promising novel\napproach to neural information processing by introducing DNPUs as high-capacity\nneurons and moving from a single to a multi-neuron framework. By implementing\nand testing a small multi-DNPU classifier in hardware, we show that\nfeed-forward DNPU networks improve the performance of a single DNPU from 77% to\n94% test accuracy on a binary classification task with concentric classes on a\nplane. Furthermore, motivated by the integration of DNPUs with memristor\narrays, we study the potential of using DNPUs in combination with linear\nlayers. We show by simulation that a single-layer MNIST classifier with only 10\nDNPUs achieves over 96% test accuracy. Our results pave the road towards\nhardware neural-network emulators that offer atomic-scale information\nprocessing with low latency and energy consumption.\n",
        "published": "2020",
        "authors": [
            "Hans-Christian Ruiz-Euler",
            "Unai Alegre-Ibarra",
            "Bram van de Ven",
            "Hajo Broersma",
            "Peter A. Bobbert",
            "Wilfred G. van der Wiel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2008.04470v1",
        "title": "PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings,\n  Semi-Supervised Conversational Data, and Biased Loss",
        "abstract": "  Neural network applications generally benefit from larger-sized models, but\nfor current speech enhancement models, larger scale networks often suffer from\ndecreased robustness to the variety of real-world use cases beyond what is\nencountered in training data. We introduce several innovations that lead to\nbetter large neural networks for speech enhancement. The novel PoCoNet\narchitecture is a convolutional neural network that, with the use of\nfrequency-positional embeddings, is able to more efficiently build\nfrequency-dependent features in the early layers. A semi-supervised method\nhelps increase the amount of conversational training data by pre-enhancing\nnoisy datasets, improving performance on real recordings. A new loss function\nbiased towards preserving speech quality helps the optimization better match\nhuman perceptual opinions on speech quality. Ablation experiments and objective\nand human opinion metrics show the benefits of the proposed improvements.\n",
        "published": "2020",
        "authors": [
            "Umut Isik",
            "Ritwik Giri",
            "Neerad Phansalkar",
            "Jean-Marc Valin",
            "Karim Helwani",
            "Arvindh Krishnaswamy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.01851v8",
        "title": "On the Universality of the Double Descent Peak in Ridgeless Regression",
        "abstract": "  We prove a non-asymptotic distribution-independent lower bound for the\nexpected mean squared generalization error caused by label noise in ridgeless\nlinear regression. Our lower bound generalizes a similar known result to the\noverparameterized (interpolating) regime. In contrast to most previous works,\nour analysis applies to a broad class of input distributions with almost surely\nfull-rank feature matrices, which allows us to cover various types of\ndeterministic or random feature maps. Our lower bound is asymptotically sharp\nand implies that in the presence of label noise, ridgeless linear regression\ndoes not perform well around the interpolation threshold for any of these\nfeature maps. We analyze the imposed assumptions in detail and provide a theory\nfor analytic (random) feature maps. Using this theory, we can show that our\nassumptions are satisfied for input distributions with a (Lebesgue) density and\nfeature maps given by random deep neural networks with analytic activation\nfunctions like sigmoid, tanh, softplus or GELU. As further examples, we show\nthat feature maps from random Fourier features and polynomial kernels also\nsatisfy our assumptions. We complement our theory with further experimental and\nanalytic results.\n",
        "published": "2020",
        "authors": [
            "David Holzm\u00fcller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2010.06845v1",
        "title": "Extended Koopman Models",
        "abstract": "  We introduce two novel generalizations of the Koopman operator method of\nnonlinear dynamic modeling. Each of these generalizations leads to greatly\nimproved predictive performance without sacrificing a unique trait of Koopman\nmethods: the potential for fast, globally optimal control of nonlinear,\nnonconvex systems. The first generalization, Convex Koopman Models, uses convex\nrather than linear dynamics in the lifted space. The second, Extended Koopman\nModels, additionally introduces an invertible transformation of the control\nsignal which contributes to the lifted convex dynamics. We describe a deep\nlearning architecture for parameterizing these classes of models, and show\nexperimentally that each significantly outperforms traditional Koopman models\nin trajectory prediction for two nonlinear, nonconvex dynamic systems.\n",
        "published": "2020",
        "authors": [
            "Span Spanbauer",
            "Ian Hunter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2101.07833v1",
        "title": "Implicit Bias of Linear RNNs",
        "abstract": "  Contemporary wisdom based on empirical studies suggests that standard\nrecurrent neural networks (RNNs) do not perform well on tasks requiring\nlong-term memory. However, precise reasoning for this behavior is still\nunknown. This paper provides a rigorous explanation of this property in the\nspecial case of linear RNNs. Although this work is limited to linear RNNs, even\nthese systems have traditionally been difficult to analyze due to their\nnon-linear parameterization. Using recently-developed kernel regime analysis,\nour main result shows that linear RNNs learned from random initializations are\nfunctionally equivalent to a certain weighted 1D-convolutional network.\nImportantly, the weightings in the equivalent model cause an implicit bias to\nelements with smaller time lags in the convolution and hence, shorter memory.\nThe degree of this bias depends on the variance of the transition kernel matrix\nat initialization and is related to the classic exploding and vanishing\ngradients problem. The theory is validated in both synthetic and real data\nexperiments.\n",
        "published": "2021",
        "authors": [
            "Melikasadat Emami",
            "Mojtaba Sahraee-Ardakan",
            "Parthe Pandit",
            "Sundeep Rangan",
            "Alyson K. Fletcher"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.07743v5",
        "title": "Universal Regular Conditional Distributions",
        "abstract": "  We introduce a deep learning model that can universally approximate regular\nconditional distributions (RCDs). The proposed model operates in three phases:\nfirst, it linearizes inputs from a given metric space $\\mathcal{X}$ to\n$\\mathbb{R}^d$ via a feature map, then a deep feedforward neural network\nprocesses these linearized features, and then the network's outputs are then\ntransformed to the $1$-Wasserstein space $\\mathcal{P}_1(\\mathbb{R}^D)$ via a\nprobabilistic extension of the attention mechanism of Bahdanau et al.\\ (2014).\nOur model, called the \\textit{probabilistic transformer (PT)}, can approximate\nany continuous function from $\\mathbb{R}^d $ to $\\mathcal{P}_1(\\mathbb{R}^D)$\nuniformly on compact sets, quantitatively. We identify two ways in which the PT\navoids the curse of dimensionality when approximating\n$\\mathcal{P}_1(\\mathbb{R}^D)$-valued functions. The first strategy builds\nfunctions in $C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$ which can be\nefficiently approximated by a PT, uniformly on any given compact subset of\n$\\mathbb{R}^d$. In the second approach, given any function $f$ in\n$C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$, we build compact subsets of\n$\\mathbb{R}^d$ whereon $f$ can be efficiently approximated by a PT.\n",
        "published": "2021",
        "authors": [
            "Anastasis Kratsios"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.08675v3",
        "title": "The Computational Complexity of ReLU Network Training Parameterized by\n  Data Dimensionality",
        "abstract": "  Understanding the computational complexity of training simple neural networks\nwith rectified linear units (ReLUs) has recently been a subject of intensive\nresearch. Closing gaps and complementing results from the literature, we\npresent several results on the parameterized complexity of training two-layer\nReLU networks with respect to various loss functions. After a brief discussion\nof other parameters, we focus on analyzing the influence of the dimension $d$\nof the training data on the computational complexity. We provide running time\nlower bounds in terms of W[1]-hardness for parameter $d$ and prove that known\nbrute-force strategies are essentially optimal (assuming the Exponential Time\nHypothesis). In comparison with previous work, our results hold for a broad(er)\nrange of loss functions, including $\\ell^p$-loss for all $p\\in[0,\\infty]$. In\nparticular, we extend a known polynomial-time algorithm for constant $d$ and\nconvex loss functions to a more general class of loss functions, matching our\nrunning time lower bounds also in these cases.\n",
        "published": "2021",
        "authors": [
            "Vincent Froese",
            "Christoph Hertrich",
            "Rolf Niedermeier"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.12245v2",
        "title": "Scaling Properties of Deep Residual Networks",
        "abstract": "  Residual networks (ResNets) have displayed impressive results in pattern\nrecognition and, recently, have garnered considerable theoretical interest due\nto a perceived link with neural ordinary differential equations (neural ODEs).\nThis link relies on the convergence of network weights to a smooth function as\nthe number of layers increases. We investigate the properties of weights\ntrained by stochastic gradient descent and their scaling with network depth\nthrough detailed numerical experiments. We observe the existence of scaling\nregimes markedly different from those assumed in neural ODE literature.\nDepending on certain features of the network architecture, such as the\nsmoothness of the activation function, one may obtain an alternative ODE limit,\na stochastic differential equation or neither of these. These findings cast\ndoubts on the validity of the neural ODE model as an adequate asymptotic\ndescription of deep ResNets and point to an alternative class of differential\nequations as a better description of the deep network limit.\n",
        "published": "2021",
        "authors": [
            "Alain-Sam Cohen",
            "Rama Cont",
            "Alain Rossier",
            "Renyuan Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2105.14835v4",
        "title": "Towards Lower Bounds on the Depth of ReLU Neural Networks",
        "abstract": "  We contribute to a better understanding of the class of functions that can be\nrepresented by a neural network with ReLU activations and a given architecture.\nUsing techniques from mixed-integer optimization, polyhedral theory, and\ntropical geometry, we provide a mathematical counterbalance to the universal\napproximation theorems which suggest that a single hidden layer is sufficient\nfor learning any function. In particular, we investigate whether the class of\nexactly representable functions strictly increases by adding more layers (with\nno restrictions on size). As a by-product of our investigations, we settle an\nold conjecture about piecewise linear functions by Wang and Sun (2005) in the\naffirmative. We also present upper bounds on the sizes of neural networks\nrequired to represent functions with logarithmic depth.\n",
        "published": "2021",
        "authors": [
            "Christoph Hertrich",
            "Amitabh Basu",
            "Marco Di Summa",
            "Martin Skutella"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2107.06446v1",
        "title": "Hierarchical Associative Memory",
        "abstract": "  Dense Associative Memories or Modern Hopfield Networks have many appealing\nproperties of associative memory. They can do pattern completion, store a large\nnumber of memories, and can be described using a recurrent neural network with\na degree of biological plausibility and rich feedback between the neurons. At\nthe same time, up until now all the models of this class have had only one\nhidden layer, and have only been formulated with densely connected network\narchitectures, two aspects that hinder their machine learning applications.\nThis paper tackles this gap and describes a fully recurrent model of\nassociative memory with an arbitrary large number of layers, some of which can\nbe locally connected (convolutional), and a corresponding energy function that\ndecreases on the dynamical trajectory of the neurons' activations. The memories\nof the full network are dynamically \"assembled\" using primitives encoded in the\nsynaptic weights of the lower layers, with the \"assembling rules\" encoded in\nthe synaptic weights of the higher layers. In addition to the bottom-up\npropagation of information, typical of commonly used feedforward neural\nnetworks, the model described has rich top-down feedback from higher layers\nthat help the lower-layer neurons to decide on their response to the input\nstimuli.\n",
        "published": "2021",
        "authors": [
            "Dmitry Krotov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2202.12823v2",
        "title": "Gen\u00e9Live! Generating Rhythm Actions in Love Live!",
        "abstract": "  This article presents our generative model for rhythm action games together\nwith applications in business operations. Rhythm action games are video games\nin which the player is challenged to issue commands at the right timings during\na music session. The timings are rendered in the chart, which consists of\nvisual symbols, called notes, flying through the screen. We introduce our deep\ngenerative model, Gen\\'eLive!, which outperforms the state-of-the-art model by\ntaking into account musical structures through beats and temporal scales.\nThanks to its favorable performance, Gen\\'eLive! was put into operation at KLab\nInc., a Japan-based video game developer, and reduced the business cost of\nchart generation by as much as half. The application target included the\nphenomenal \"Love Live!,\" which has more than 10 million users across Asia and\nbeyond, and is one of the few rhythm action franchises that has led the online\nera of the genre. In this article, we evaluate the generative performance of\nGen\\'eLive! using production datasets at KLab as well as open datasets for\nreproducibility, while the model continues to operate in their business. Our\ncode and the model, tuned and trained using a supercomputer, are publicly\navailable.\n",
        "published": "2022",
        "authors": [
            "Atsushi Takada",
            "Daichi Yamazaki",
            "Likun Liu",
            "Yudai Yoshida",
            "Nyamkhuu Ganbat",
            "Takayuki Shimotomai",
            "Taiga Yamamoto",
            "Daisuke Sakurai",
            "Naoki Hamada"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1503.05743v1",
        "title": "Implementation of a Practical Distributed Calculation System with\n  Browsers and JavaScript, and Application to Distributed Deep Learning",
        "abstract": "  Deep learning can achieve outstanding results in various fields. However, it\nrequires so significant computational power that graphics processing units\n(GPUs) and/or numerous computers are often required for the practical\napplication. We have developed a new distributed calculation framework called\n\"Sashimi\" that allows any computer to be used as a distribution node only by\naccessing a website. We have also developed a new JavaScript neural network\nframework called \"Sukiyaki\" that uses general purpose GPUs with web browsers.\nSukiyaki performs 30 times faster than a conventional JavaScript library for\ndeep convolutional neural networks (deep CNNs) learning. The combination of\nSashimi and Sukiyaki, as well as new distribution algorithms, demonstrates the\ndistributed deep learning of deep CNNs only with web browsers on various\ndevices. The libraries that comprise the proposed methods are available under\nMIT license at http://mil-tokyo.github.io/.\n",
        "published": "2015",
        "authors": [
            "Ken Miura",
            "Tatsuya Harada"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1606.01164v2",
        "title": "Dense Associative Memory for Pattern Recognition",
        "abstract": "  A model of associative memory is studied, which stores and reliably retrieves\nmany more patterns than the number of neurons in the network. We propose a\nsimple duality between this dense associative memory and neural networks\ncommonly used in deep learning. On the associative memory side of this duality,\na family of models that smoothly interpolates between two limiting cases can be\nconstructed. One limit is referred to as the feature-matching mode of pattern\nrecognition, and the other one as the prototype regime. On the deep learning\nside of the duality, this family corresponds to feedforward neural networks\nwith one hidden layer and various activation functions, which transmit the\nactivities of the visible neurons to the hidden layer. This family of\nactivation functions includes logistics, rectified linear units, and rectified\npolynomials of higher degrees. The proposed duality makes it possible to apply\nenergy-based intuition from associative memory to analyze computational\nproperties of neural networks with unusual activation functions - the higher\nrectified polynomials which until now have not been used in deep learning. The\nutility of the dense memories is illustrated for two test cases: the logical\ngate XOR and the recognition of handwritten digits from the MNIST data set.\n",
        "published": "2016",
        "authors": [
            "Dmitry Krotov",
            "John J Hopfield"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1611.01046v3",
        "title": "Learning to Pivot with Adversarial Networks",
        "abstract": "  Several techniques for domain adaptation have been proposed to account for\ndifferences in the distribution of the data used for training and testing. The\nmajority of this work focuses on a binary domain label. Similar problems occur\nin a scientific context where there may be a continuous family of plausible\ndata generation processes associated to the presence of systematic\nuncertainties. Robust inference is possible if it is based on a pivot -- a\nquantity whose distribution does not depend on the unknown values of the\nnuisance parameters that parametrize this family of data generation processes.\nIn this work, we introduce and derive theoretical results for a training\nprocedure based on adversarial networks for enforcing the pivotal property (or,\nequivalently, fairness with respect to continuous attributes) on a predictive\nmodel. The method includes a hyperparameter to control the trade-off between\naccuracy and robustness. We demonstrate the effectiveness of this approach with\na toy example and examples from particle physics.\n",
        "published": "2016",
        "authors": [
            "Gilles Louppe",
            "Michael Kagan",
            "Kyle Cranmer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1612.02130v2",
        "title": "Predictive Business Process Monitoring with LSTM Neural Networks",
        "abstract": "  Predictive business process monitoring methods exploit logs of completed\ncases of a process in order to make predictions about running cases thereof.\nExisting methods in this space are tailor-made for specific prediction tasks.\nMoreover, their relative accuracy is highly sensitive to the dataset at hand,\nthus requiring users to engage in trial-and-error and tuning when applying them\nin a specific setting. This paper investigates Long Short-Term Memory (LSTM)\nneural networks as an approach to build consistently accurate models for a wide\nrange of predictive process monitoring tasks. First, we show that LSTMs\noutperform existing techniques to predict the next event of a running case and\nits timestamp. Next, we show how to use models for predicting the next task in\norder to predict the full continuation of a running case. Finally, we apply the\nsame approach to predict the remaining time, and show that this approach\noutperforms existing tailor-made methods.\n",
        "published": "2016",
        "authors": [
            "Niek Tax",
            "Ilya Verenich",
            "Marcello La Rosa",
            "Marlon Dumas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1708.08694v4",
        "title": "Natasha 2: Faster Non-Convex Optimization Than SGD",
        "abstract": "  We design a stochastic algorithm to train any smooth neural network to\n$\\varepsilon$-approximate local minima, using $O(\\varepsilon^{-3.25})$\nbackpropagations. The best result was essentially $O(\\varepsilon^{-4})$ by SGD.\n  More broadly, it finds $\\varepsilon$-approximate local minima of any smooth\nnonconvex function in rate $O(\\varepsilon^{-3.25})$, with only oracle access to\nstochastic gradients.\n",
        "published": "2017",
        "authors": [
            "Zeyuan Allen-Zhu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1710.08961v3",
        "title": "Fast and Scalable Distributed Deep Convolutional Autoencoder for fMRI\n  Big Data Analytics",
        "abstract": "  In recent years, analyzing task-based fMRI (tfMRI) data has become an\nessential tool for understanding brain function and networks. However, due to\nthe sheer size of tfMRI data, its intrinsic complex structure, and lack of\nground truth of underlying neural activities, modeling tfMRI data is hard and\nchallenging. Previously proposed data-modeling methods including Independent\nComponent Analysis (ICA) and Sparse Dictionary Learning only provided a weakly\nestablished model based on blind source separation under the strong assumption\nthat original fMRI signals could be linearly decomposed into time series\ncomponents with corresponding spatial maps. Meanwhile, analyzing and learning a\nlarge amount of tfMRI data from a variety of subjects has been shown to be very\ndemanding but yet challenging even with technological advances in computational\nhardware. Given the Convolutional Neural Network (CNN), a robust method for\nlearning high-level abstractions from low-level data such as tfMRI time series,\nin this work we propose a fast and scalable novel framework for distributed\ndeep Convolutional Autoencoder model. This model aims to both learn the complex\nhierarchical structure of the tfMRI data and to leverage the processing power\nof multiple GPUs in a distributed fashion. To implement such a model, we have\ncreated an enhanced processing pipeline on the top of Apache Spark and\nTensorflow library, leveraging from a very large cluster of GPU machines.\nExperimental data from applying the model on the Human Connectome Project (HCP)\nshow that the proposed model is efficient and scalable toward tfMRI big data\nanalytics, thus enabling data-driven extraction of hierarchical neuroscientific\ninformation from massive fMRI big data in the future.\n",
        "published": "2017",
        "authors": [
            "Milad Makkie",
            "Heng Huang",
            "Yu Zhao",
            "Athanasios V. Vasilakos",
            "Tianming Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1804.04806v1",
        "title": "\u03bc-cuDNN: Accelerating Deep Learning Frameworks with Micro-Batching",
        "abstract": "  NVIDIA cuDNN is a low-level library that provides GPU kernels frequently used\nin deep learning. Specifically, cuDNN implements several equivalent convolution\nalgorithms, whose performance and memory footprint may vary considerably,\ndepending on the layer dimensions. When an algorithm is automatically selected\nby cuDNN, the decision is performed on a per-layer basis, and thus it often\nresorts to slower algorithms that fit the workspace size constraints. We\npresent {\\mu}-cuDNN, a transparent wrapper library for cuDNN, which divides\nlayers' mini-batch computation into several micro-batches. Based on Dynamic\nProgramming and Integer Linear Programming, {\\mu}-cuDNN enables faster\nalgorithms by decreasing the workspace requirements. At the same time,\n{\\mu}-cuDNN keeps the computational semantics unchanged, so that it decouples\nstatistical efficiency from the hardware efficiency safely. We demonstrate the\neffectiveness of {\\mu}-cuDNN over two frameworks, Caffe and TensorFlow,\nachieving speedups of 1.63x for AlexNet and 1.21x for ResNet-18 on P100-SXM2\nGPU. These results indicate that using micro-batches can seamlessly increase\nthe performance of deep learning, while maintaining the same memory footprint.\n",
        "published": "2018",
        "authors": [
            "Yosuke Oyama",
            "Tal Ben-Nun",
            "Torsten Hoefler",
            "Satoshi Matsuoka"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.07791v1",
        "title": "MaaSim: A Liveability Simulation for Improving the Quality of Life in\n  Cities",
        "abstract": "  Urbanism is no longer planned on paper thanks to powerful models and 3D\nsimulation platforms. However, current work is not open to the public and lacks\nan optimisation agent that could help in decision making. This paper describes\nthe creation of an open-source simulation based on an existing Dutch\nliveability score with a built-in AI module. Features are selected using\nfeature engineering and Random Forests. Then, a modified scoring function is\nbuilt based on the former liveability classes. The score is predicted using\nRandom Forest for regression and achieved a recall of 0.83 with 10-fold\ncross-validation. Afterwards, Exploratory Factor Analysis is applied to select\nthe actions present in the model. The resulting indicators are divided into 5\ngroups, and 12 actions are generated. The performance of four optimisation\nalgorithms is compared, namely NSGA-II, PAES, SPEA2 and eps-MOEA, on three\nestablished criteria of quality: cardinality, the spread of the solutions,\nspacing, and the resulting score and number of turns. Although all four\nalgorithms show different strengths, eps-MOEA is selected to be the most\nsuitable for this problem. Ultimately, the simulation incorporates the model\nand the selected AI module in a GUI written in the Kivy framework for Python.\nTests performed on users show positive responses and encourage further\ninitiatives towards joining technology and public applications.\n",
        "published": "2018",
        "authors": [
            "Dominika Woszczyk",
            "Gerasimos Spanakis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.08923v1",
        "title": "CNNPred: CNN-based stock market prediction using several data sources",
        "abstract": "  Feature extraction from financial data is one of the most important problems\nin market prediction domain for which many approaches have been suggested.\nAmong other modern tools, convolutional neural networks (CNN) have recently\nbeen applied for automatic feature selection and market prediction. However, in\nexperiments reported so far, less attention has been paid to the correlation\namong different markets as a possible source of information for extracting\nfeatures. In this paper, we suggest a CNN-based framework with specially\ndesigned CNNs, that can be applied on a collection of data from a variety of\nsources, including different markets, in order to extract features for\npredicting the future of those markets. The suggested framework has been\napplied for predicting the next day's direction of movement for the indices of\nS&P 500, NASDAQ, DJI, NYSE, and RUSSELL markets based on various sets of\ninitial features. The evaluations show a significant improvement in\nprediction's performance compared to the state of the art baseline algorithms.\n",
        "published": "2018",
        "authors": [
            "Ehsan Hoseinzade",
            "Saman Haratizadeh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.11760v1",
        "title": "Machine Learning in Network Centrality Measures: Tutorial and Outlook",
        "abstract": "  Complex networks are ubiquitous to several Computer Science domains.\nCentrality measures are an important analysis mechanism to uncover vital\nelements of complex networks. However, these metrics have high computational\ncosts and requirements that hinder their applications in large real-world\nnetworks. In this tutorial, we explain how the use of neural network learning\nalgorithms can render the application of the metrics in complex networks of\narbitrary size. Moreover, the tutorial describes how to identify the best\nconfiguration for neural network training and learning such for tasks, besides\npresenting an easy way to generate and acquire training data. We do so by means\nof a general methodology, using complex network models adaptable to any\napplication. We show that a regression model generated by the neural network\nsuccessfully approximates the metric values and therefore are a robust,\neffective alternative in real-world applications. The methodology and proposed\nmachine learning model use only a fraction of time with respect to other\napproximation algorithms, which is crucial in complex network applications.\n",
        "published": "2018",
        "authors": [
            "Felipe Grando",
            "Lisando Z. Granville",
            "Luis C. Lamb"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1810.12065v4",
        "title": "On the Convergence Rate of Training Recurrent Neural Networks",
        "abstract": "  How can local-search methods such as stochastic gradient descent (SGD) avoid\nbad local minima in training multi-layer neural networks? Why can they fit\nrandom labels even given non-convex and non-smooth architectures? Most existing\ntheory only covers networks with one hidden layer, so can we go deeper?\n  In this paper, we focus on recurrent neural networks (RNNs) which are\nmulti-layer networks widely used in natural language processing. They are\nharder to analyze than feedforward neural networks, because the $\\textit{same}$\nrecurrent unit is repeatedly applied across the entire time horizon of length\n$L$, which is analogous to feedforward networks of depth $L$. We show when the\nnumber of neurons is sufficiently large, meaning polynomial in the training\ndata size and in $L$, then SGD is capable of minimizing the regression loss in\nthe linear convergence rate. This gives theoretical evidence of how RNNs can\nmemorize data.\n  More importantly, in this paper we build general toolkits to analyze\nmulti-layer networks with ReLU activations. For instance, we prove why ReLU\nactivations can prevent exponential gradient explosion or vanishing, and build\na perturbation theory to analyze first-order approximation of multi-layer\nnetworks.\n",
        "published": "2018",
        "authors": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li",
            "Zhao Song"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1904.03491v1",
        "title": "A Compendium on Network and Host based Intrusion Detection Systems",
        "abstract": "  The techniques of deep learning have become the state of the art methodology\nfor executing complicated tasks from various domains of computer vision,\nnatural language processing, and several other areas. Due to its rapid\ndevelopment and promising benchmarks in those fields, researchers started\nexperimenting with this technique to perform in the area of, especially in\nintrusion detection related tasks. Deep learning is a subset and a natural\nextension of classical Machine learning and an evolved model of neural\nnetworks. This paper contemplates and discusses all the methodologies related\nto the leading edge Deep learning and Neural network models purposing to the\narena of Intrusion Detection Systems.\n",
        "published": "2019",
        "authors": [
            "Rahul-Vigneswaran K",
            "Prabaharan Poornachandran",
            "Soman KP"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.10224v1",
        "title": "Semi-Supervised Classification on Non-Sparse Graphs Using Low-Rank Graph\n  Convolutional Networks",
        "abstract": "  Graph Convolutional Networks (GCNs) have proven to be successful tools for\nsemi-supervised learning on graph-based datasets. For sparse graphs, linear and\npolynomial filter functions have yielded impressive results. For large\nnon-sparse graphs, however, network training and evaluation becomes\nprohibitively expensive. By introducing low-rank filters, we gain significant\nruntime acceleration and simultaneously improved accuracy. We further propose\nan architecture change mimicking techniques from Model Order Reduction in what\nwe call a reduced-order GCN. Moreover, we present how our method can also be\napplied to hypergraph datasets and how hypergraph convolution can be\nimplemented efficiently.\n",
        "published": "2019",
        "authors": [
            "Dominik Alfke",
            "Martin Stoll"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.10337v3",
        "title": "What Can ResNet Learn Efficiently, Going Beyond Kernels?",
        "abstract": "  How can neural networks such as ResNet efficiently learn CIFAR-10 with test\naccuracy more than 96%, while other methods, especially kernel methods, fall\nrelatively behind? Can we more provide theoretical justifications for this gap?\n  Recently, there is an influential line of work relating neural networks to\nkernels in the over-parameterized regime, proving they can learn certain\nconcept class that is also learnable by kernels with similar test error. Yet,\ncan neural networks provably learn some concept class BETTER than kernels?\n  We answer this positively in the distribution-free setting. We prove neural\nnetworks can efficiently learn a notable class of functions, including those\ndefined by three-layer residual networks with smooth activations, without any\ndistributional assumption. At the same time, we prove there are simple\nfunctions in this class such that with the same number of training examples,\nthe test error obtained by neural networks can be MUCH SMALLER than ANY kernel\nmethod, including neural tangent kernels (NTK).\n  The main intuition is that multi-layer neural networks can implicitly perform\nhierarchical learning using different layers, which reduces the sample\ncomplexity comparing to \"one-shot\" learning algorithms such as kernel methods.\nIn a follow-up work [2], this theory of hierarchical learning is further\nstrengthened to incorporate the \"backward feature correction\" process when\ntraining deep networks.\n  In the end, we also prove a computation complexity advantage of ResNet with\nrespect to other learning methods including linear regression over arbitrary\nfeature mappings.\n",
        "published": "2019",
        "authors": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.12601v1",
        "title": "A Novel Chaos Theory Inspired Neuronal Architecture",
        "abstract": "  The practical success of widely used machine learning (ML) and deep learning\n(DL) algorithms in Artificial Intelligence (AI) community owes to availability\nof large datasets for training and huge computational resources. Despite the\nenormous practical success of AI, these algorithms are only loosely inspired\nfrom the biological brain and do not mimic any of the fundamental properties of\nneurons in the brain, one such property being the chaotic firing of biological\nneurons. This motivates us to develop a novel neuronal architecture where the\nindividual neurons are intrinsically chaotic in nature. By making use of the\ntopological transitivity property of chaos, our neuronal network is able to\nperform classification tasks with very less number of training samples. For the\nMNIST dataset, with as low as $0.1 \\%$ of the total training data, our method\noutperforms ML and matches DL in classification accuracy for up to $7$ training\nsamples/class. For the Iris dataset, our accuracy is comparable with ML\nalgorithms, and even with just two training samples/class, we report an\naccuracy as high as $95.8 \\%$. This work highlights the effectiveness of chaos\nand its properties for learning and paves the way for chaos-inspired neuronal\narchitectures by closely mimicking the chaotic nature of neurons in the brain.\n",
        "published": "2019",
        "authors": [
            "Harikrishnan N B",
            "Nithin Nagaraj"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1905.12921v3",
        "title": "Quantifying the Alignment of Graph and Features in Deep Learning",
        "abstract": "  We show that the classification performance of graph convolutional networks\n(GCNs) is related to the alignment between features, graph, and ground truth,\nwhich we quantify using a subspace alignment measure (SAM) corresponding to the\nFrobenius norm of the matrix of pairwise chordal distances between three\nsubspaces associated with features, graph, and ground truth. The proposed\nmeasure is based on the principal angles between subspaces and has both\nspectral and geometrical interpretations. We showcase the relationship between\nthe SAM and the classification performance through the study of limiting cases\nof GCNs and systematic randomizations of both features and graph structure\napplied to a constructive example and several examples of citation networks of\ndifferent origins. The analysis also reveals the relative importance of the\ngraph and features for classification purposes.\n",
        "published": "2019",
        "authors": [
            "Yifan Qian",
            "Paul Expert",
            "Tom Rieu",
            "Pietro Panzarasa",
            "Mauricio Barahona"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.05301v4",
        "title": "Learning Curves for Deep Neural Networks: A Gaussian Field Theory\n  Perspective",
        "abstract": "  In the past decade, deep neural networks (DNNs) came to the fore as the\nleading machine learning algorithms for a variety of tasks. Their raise was\nfounded on market needs and engineering craftsmanship, the latter based more on\ntrial and error than on theory. While still far behind the application\nforefront, the theoretical study of DNNs has recently made important\nadvancements in analyzing the highly over-parameterized regime where some exact\nresults have been obtained. Leveraging these ideas and adopting a more\nphysics-like approach, here we construct a versatile field-theory formalism for\nsupervised deep learning, involving renormalization group, Feynman diagrams and\nreplicas. In particular we show that our approach leads to highly accurate\npredictions of learning curves of truly deep DNNs trained on polynomial\nregression tasks and that these predictions can be used for efficient\nhyper-parameter optimization. In addition, they explain how DNNs generalize\nwell despite being highly over-parameterized, this due to an entropic bias to\nsimple functions which, for the case of fully-connected DNNs with data sampled\non the hypersphere, are low order polynomials in the input vector. Being a\ncomplex interacting system of artificial neurons, we believe that such tools\nand methodologies borrowed from condensed matter physics would prove essential\nfor obtaining an accurate quantitative understanding of deep learning.\n",
        "published": "2019",
        "authors": [
            "Omry Cohen",
            "Or Malka",
            "Zohar Ringel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.08167v1",
        "title": "PABO: Pseudo Agent-Based Multi-Objective Bayesian Hyperparameter\n  Optimization for Efficient Neural Accelerator Design",
        "abstract": "  The ever increasing computational cost of Deep Neural Networks (DNN) and the\ndemand for energy efficient hardware for DNN acceleration has made accuracy and\nhardware cost co-optimization for DNNs tremendously important, especially for\nedge devices. Owing to the large parameter space and cost of evaluating each\nparameter in the search space, manually tuning of DNN hyperparameters is\nimpractical. Automatic joint DNN and hardware hyperparameter optimization is\nindispensable for such problems. Bayesian optimization-based approaches have\nshown promising results for hyperparameter optimization of DNNs. However, most\nof these techniques have been developed without considering the underlying\nhardware, thereby leading to inefficient designs. Further, the few works that\nperform joint optimization are not generalizable and mainly focus on CMOS-based\narchitectures. In this work, we present a novel pseudo agent-based\nmulti-objective hyperparameter optimization (PABO) for maximizing the DNN\nperformance while obtaining low hardware cost. Compared to the existing\nmethods, our work poses a theoretically different approach for joint\noptimization of accuracy and hardware cost and focuses on memristive\ncrossbar-based accelerators. PABO uses a supervisor agent to establish\nconnections between the posterior Gaussian distribution models of network\naccuracy and hardware cost requirements. The agent reduces the mathematical\ncomplexity of the co-optimization problem by removing unnecessary computations\nand updates of acquisition functions, thereby achieving significant speed-ups\nfor the optimization procedure. PABO outputs a Pareto frontier that underscores\nthe trade-offs between designing high-accuracy and hardware efficiency. Our\nresults demonstrate a superior performance compared to the state-of-the-art\nmethods both in terms of accuracy and computational speed (~100x speed up).\n",
        "published": "2019",
        "authors": [
            "Maryam Parsa",
            "Aayush Ankit",
            "Amirkoushyar Ziabari",
            "Kaushik Roy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1906.11122v1",
        "title": "Physics-Informed Echo State Networks for Chaotic Systems Forecasting",
        "abstract": "  We propose a physics-informed Echo State Network (ESN) to predict the\nevolution of chaotic systems. Compared to conventional ESNs, the\nphysics-informed ESNs are trained to solve supervised learning tasks while\nensuring that their predictions do not violate physical laws. This is achieved\nby introducing an additional loss function during the training of the ESNs,\nwhich penalizes non-physical predictions without the need of any additional\ntraining data. This approach is demonstrated on a chaotic Lorenz system, where\nthe physics-informed ESNs improve the predictability horizon by about two\nLyapunov times as compared to conventional ESNs. The proposed framework shows\nthe potential of using machine learning combined with prior physical knowledge\nto improve the time-accurate prediction of chaotic dynamical systems.\n",
        "published": "2019",
        "authors": [
            "Nguyen Anh Khoa Doan",
            "Wolfgang Polifke",
            "Luca Magri"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.01730v2",
        "title": "Deep Convolutional Networks in System Identification",
        "abstract": "  Recent developments within deep learning are relevant for nonlinear system\nidentification problems. In this paper, we establish connections between the\ndeep learning and the system identification communities. It has recently been\nshown that convolutional architectures are at least as capable as recurrent\narchitectures when it comes to sequence modeling tasks. Inspired by these\nresults we explore the explicit relationships between the recently proposed\ntemporal convolutional network (TCN) and two classic system identification\nmodel structures; Volterra series and block-oriented models. We end the paper\nwith an experimental study where we provide results on two real-world problems,\nthe well-known Silverbox dataset and a newer dataset originating from ground\nvibration experiments on an F-16 fighter aircraft.\n",
        "published": "2019",
        "authors": [
            "Carl Andersson",
            "Ant\u00f4nio H. Ribeiro",
            "Koen Tiels",
            "Niklas Wahlstr\u00f6m",
            "Thomas B. Sch\u00f6n"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.02702v1",
        "title": "Port-Hamiltonian Approach to Neural Network Training",
        "abstract": "  Neural networks are discrete entities: subdivided into discrete layers and\nparametrized by weights which are iteratively optimized via difference\nequations. Recent work proposes networks with layer outputs which are no longer\nquantized but are solutions of an ordinary differential equation (ODE);\nhowever, these networks are still optimized via discrete methods (e.g. gradient\ndescent). In this paper, we explore a different direction: namely, we propose a\nnovel framework for learning in which the parameters themselves are solutions\nof ODEs. By viewing the optimization process as the evolution of a\nport-Hamiltonian system, we can ensure convergence to a minimum of the\nobjective function. Numerical experiments have been performed to show the\nvalidity and effectiveness of the proposed methods.\n",
        "published": "2019",
        "authors": [
            "Stefano Massaroli",
            "Michael Poli",
            "Federico Califano",
            "Angela Faragasso",
            "Jinkyoo Park",
            "Atsushi Yamashita",
            "Hajime Asama"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1909.05176v2",
        "title": "Optimal Machine Intelligence at the Edge of Chaos",
        "abstract": "  It has long been suggested that the biological brain operates at some\ncritical point between two different phases, possibly order and chaos. Despite\nmany indirect empirical evidence from the brain and analytical indication on\nsimple neural networks, the foundation of this hypothesis on generic non-linear\nsystems remains unclear. Here we develop a general theory that reveals the\nexact edge of chaos is the boundary between the chaotic phase and the\n(pseudo)periodic phase arising from Neimark-Sacker bifurcation. This edge is\nanalytically determined by the asymptotic Jacobian norm values of the\nnon-linear operator and influenced by the dimensionality of the system. The\noptimality at the edge of chaos is associated with the highest information\ntransfer between input and output at this point similar to that of the logistic\nmap. As empirical validations, our experiments on the various deep learning\nmodels in computer vision demonstrate the optimality of the models near the\nedge of chaos, and we observe that the state-of-art training algorithms push\nthe models towards such edge as they become more accurate. We further\nestablishes the theoretical understanding of deep learning model generalization\nthrough asymptotic stability.\n",
        "published": "2019",
        "authors": [
            "Ling Feng",
            "Lin Zhang",
            "Choy Heng Lai"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1911.07662v2",
        "title": "Variational mean-field theory for training restricted Boltzmann machines\n  with binary synapses",
        "abstract": "  Unsupervised learning requiring only raw data is not only a fundamental\nfunction of the cerebral cortex, but also a foundation for a next generation of\nartificial neural networks. However, a unified theoretical framework to treat\nsensory inputs, synapses and neural activity together is still lacking. The\ncomputational obstacle originates from the discrete nature of synapses, and\ncomplex interactions among these three essential elements of learning. Here, we\npropose a variational mean-field theory in which the distribution of synaptic\nweights is considered. The unsupervised learning can then be decomposed into\ntwo intertwined steps: a maximization step is carried out as a gradient ascent\nof the lower-bound on the data log-likelihood, in which the synaptic weight\ndistribution is determined by updating variational parameters, and an\nexpectation step is carried out as a message passing procedure on an equivalent\nor dual neural network whose parameter is specified by the variational\nparameters of the weight distribution. Therefore, our framework provides\ninsights on how data (or sensory inputs), synapses and neural activities\ninteract with each other to achieve the goal of extracting statistical\nregularities in sensory inputs. This variational framework is verified in\nrestricted Boltzmann machines with planted synaptic weights and\nhandwritten-digits learning.\n",
        "published": "2019",
        "authors": [
            "Haiping Huang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.03432v3",
        "title": "On the distance between two neural networks and the stability of\n  learning",
        "abstract": "  This paper relates parameter distance to gradient breakdown for a broad class\nof nonlinear compositional functions. The analysis leads to a new distance\nfunction called deep relative trust and a descent lemma for neural networks.\nSince the resulting learning rule seems to require little to no learning rate\ntuning, it may unlock a simpler workflow for training deeper and more complex\nneural networks. The Python code used in this paper is here:\nhttps://github.com/jxbz/fromage.\n",
        "published": "2020",
        "authors": [
            "Jeremy Bernstein",
            "Arash Vahdat",
            "Yisong Yue",
            "Ming-Yu Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2002.09286v1",
        "title": "Efficient Trainable Front-Ends for Neural Speech Enhancement",
        "abstract": "  Many neural speech enhancement and source separation systems operate in the\ntime-frequency domain. Such models often benefit from making their Short-Time\nFourier Transform (STFT) front-ends trainable. In current literature, these are\nimplemented as large Discrete Fourier Transform matrices; which are\nprohibitively inefficient for low-compute systems. We present an efficient,\ntrainable front-end based on the butterfly mechanism to compute the Fast\nFourier Transform, and show its accuracy and efficiency benefits for\nlow-compute neural speech enhancement models. We also explore the effects of\nmaking the STFT window trainable.\n",
        "published": "2020",
        "authors": [
            "Jonah Casebeer",
            "Umut Isik",
            "Shrikant Venkataramani",
            "Arvindh Krishnaswamy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.02503v1",
        "title": "Information-Theoretic Bounds on the Generalization Error and Privacy\n  Leakage in Federated Learning",
        "abstract": "  Machine learning algorithms operating on mobile networks can be characterized\ninto three different categories. First is the classical situation in which the\nend-user devices send their data to a central server where this data is used to\ntrain a model. Second is the distributed setting in which each device trains\nits own model and send its model parameters to a central server where these\nmodel parameters are aggregated to create one final model. Third is the\nfederated learning setting in which, at any given time $t$, a certain number of\nactive end users train with their own local data along with feedback provided\nby the central server and then send their newly estimated model parameters to\nthe central server. The server, then, aggregates these new parameters, updates\nits own model, and feeds the updated parameters back to all the end users,\ncontinuing this process until it converges.\n  The main objective of this work is to provide an information-theoretic\nframework for all of the aforementioned learning paradigms. Moreover, using the\nprovided framework, we develop upper and lower bounds on the generalization\nerror together with bounds on the privacy leakage in the classical, distributed\nand federated learning settings.\n  Keywords: Federated Learning, Distributed Learning, Machine Learning, Model\nAggregation.\n",
        "published": "2020",
        "authors": [
            "Semih Yagli",
            "Alex Dytso",
            "H. Vincent Poor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.05053v1",
        "title": "Low-Rank Nonlinear Decoding of $\u03bc$-ECoG from the Primary Auditory\n  Cortex",
        "abstract": "  This paper considers the problem of neural decoding from parallel neural\nmeasurements systems such as micro-electrocorticography ($\\mu$-ECoG). In\nsystems with large numbers of array elements at very high sampling rates, the\ndimension of the raw measurement data may be large. Learning neural decoders\nfor this high-dimensional data can be challenging, particularly when the number\nof training samples is limited. To address this challenge, this work presents a\nnovel neural network decoder with a low-rank structure in the first hidden\nlayer. The low-rank constraints dramatically reduce the number of parameters in\nthe decoder while still enabling a rich class of nonlinear decoder maps. The\nlow-rank decoder is illustrated on $\\mu$-ECoG data from the primary auditory\ncortex (A1) of awake rats. This decoding problem is particularly challenging\ndue to the complexity of neural responses in the auditory cortex and the\npresence of confounding signals in awake animals. It is shown that the proposed\nlow-rank decoder significantly outperforms models using standard dimensionality\nreduction techniques such as principal component analysis (PCA).\n",
        "published": "2020",
        "authors": [
            "Melikasadat Emami",
            "Mojtaba Sahraee-Ardakan",
            "Parthe Pandit",
            "Alyson K. Fletcher",
            "Sundeep Rangan",
            "Michael Trumpis",
            "Brinnae Bent",
            "Chia-Han Chiang",
            "Jonathan Viventi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2005.14105v2",
        "title": "Provably Good Solutions to the Knapsack Problem via Neural Networks of\n  Bounded Size",
        "abstract": "  The development of a satisfying and rigorous mathematical understanding of\nthe performance of neural networks is a major challenge in artificial\nintelligence. Against this background, we study the expressive power of neural\nnetworks through the example of the classical NP-hard Knapsack Problem. Our\nmain contribution is a class of recurrent neural networks (RNNs) with rectified\nlinear units that are iteratively applied to each item of a Knapsack instance\nand thereby compute optimal or provably good solution values. We show that an\nRNN of depth four and width depending quadratically on the profit of an optimum\nKnapsack solution is sufficient to find optimum Knapsack solutions. We also\nprove the following tradeoff between the size of an RNN and the quality of the\ncomputed Knapsack solution: for Knapsack instances consisting of $n$ items, an\nRNN of depth five and width $w$ computes a solution of value at least\n$1-\\mathcal{O}(n^2/\\sqrt{w})$ times the optimum solution value. Our results\nbuild upon a classical dynamic programming formulation of the Knapsack Problem\nas well as a careful rounding of profit values that are also at the core of the\nwell-known fully polynomial-time approximation scheme for the Knapsack Problem.\nA carefully conducted computational study qualitatively supports our\ntheoretical size bounds. Finally, we point out that our results can be\ngeneralized to many other combinatorial optimization problems that admit\ndynamic programming solution methods, such as various Shortest Path Problems,\nthe Longest Common Subsequence Problem, and the Traveling Salesperson Problem.\n",
        "published": "2020",
        "authors": [
            "Christoph Hertrich",
            "Martin Skutella"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.00845v1",
        "title": "Online system identification in a Duffing oscillator by free energy\n  minimisation",
        "abstract": "  Online system identification is the estimation of parameters of a dynamical\nsystem, such as mass or friction coefficients, for each measurement of the\ninput and output signals. Here, the nonlinear stochastic differential equation\nof a Duffing oscillator is cast to a generative model and dynamical parameters\nare inferred using variational message passing on a factor graph of the model.\nThe approach is validated with an experiment on data from an electronic\nimplementation of a Duffing oscillator. The proposed inference procedure\nperforms as well as offline prediction error minimisation in a state-of-the-art\nnonlinear model.\n",
        "published": "2020",
        "authors": [
            "Wouter M Kouw"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.08092v2",
        "title": "Distributional Generalization: A New Kind of Generalization",
        "abstract": "  We introduce a new notion of generalization -- Distributional Generalization\n-- which roughly states that outputs of a classifier at train and test time are\nclose *as distributions*, as opposed to close in just their average error. For\nexample, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then\na ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as\ncats on the *test set* as well, while leaving other classes unaffected. This\nbehavior is not captured by classical generalization, which would only consider\nthe average error and not the distribution of errors over the input domain. Our\nformal conjectures, which are much more general than this example, characterize\nthe form of distributional generalization that can be expected in terms of\nproblem parameters: model architecture, training procedure, number of samples,\nand data distribution. We give empirical evidence for these conjectures across\na variety of domains in machine learning, including neural networks, kernel\nmachines, and decision trees. Our results thus advance our empirical\nunderstanding of interpolating classifiers.\n",
        "published": "2020",
        "authors": [
            "Preetum Nakkiran",
            "Yamini Bansal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.09521v2",
        "title": "Towards Interpretable-AI Policies Induction using Evolutionary Nonlinear\n  Decision Trees for Discrete Action Systems",
        "abstract": "  Black-box AI induction methods such as deep reinforcement learning (DRL) are\nincreasingly being used to find optimal policies for a given control task.\nAlthough policies represented using a black-box AI are capable of efficiently\nexecuting the underlying control task and achieving optimal closed-loop\nperformance, the developed control rules are often complex and neither\ninterpretable nor explainable. In this paper, we use a recently proposed\nnonlinear decision-tree (NLDT) approach to find a hierarchical set of control\nrules in an attempt to maximize the open-loop performance for approximating and\nexplaining the pre-trained black-box DRL (oracle) agent using the labelled\nstate-action dataset. Recent advances in nonlinear optimization approaches\nusing evolutionary computation facilitates finding a hierarchical set of\nnonlinear control rules as a function of state variables using a\ncomputationally fast bilevel optimization procedure at each node of the\nproposed NLDT. Additionally, we propose a re-optimization procedure for\nenhancing closed-loop performance of an already derived NLDT. We evaluate our\nproposed methodologies (open and closed-loop NLDTs) on different control\nproblems having multiple discrete actions. In all these problems our proposed\napproach is able to find relatively simple and interpretable rules involving\none to four non-linear terms per rule, while simultaneously achieving on par\nclosed-loop performance when compared to a trained black-box DRL agent. A\npost-processing approach for simplifying the NLDT is also suggested. The\nobtained results are inspiring as they suggest the replacement of complicated\nblack-box DRL policies involving thousands of parameters (making them\nnon-interpretable) with relatively simple interpretable policies. Results are\nencouraging and motivating to pursue further applications of proposed approach\nin solving more complex control tasks.\n",
        "published": "2020",
        "authors": [
            "Yashesh Dhebar",
            "Kalyanmoy Deb",
            "Subramanya Nageshrao",
            "Ling Zhu",
            "Dimitar Filev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2009.14702v2",
        "title": "Some Remarks on Replicated Simulated Annealing",
        "abstract": "  Recently authors have introduced the idea of training discrete weights neural\nnetworks using a mix between classical simulated annealing and a replica ansatz\nknown from the statistical physics literature. Among other points, they claim\ntheir method is able to find robust configurations. In this paper, we analyze\nthis so-called \"replicated simulated annealing\" algorithm. In particular, we\nexplicit criteria to guarantee its convergence, and study when it successfully\nsamples from configurations. We also perform experiments using synthetic and\nreal data bases.\n",
        "published": "2020",
        "authors": [
            "Vincent Gripon",
            "Matthias L\u00f6we",
            "Franck Vermet"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2011.12413v2",
        "title": "Wide-band butterfly network: stable and efficient inversion via\n  multi-frequency neural networks",
        "abstract": "  We introduce an end-to-end deep learning architecture called the wide-band\nbutterfly network (WideBNet) for approximating the inverse scattering map from\nwide-band scattering data. This architecture incorporates tools from\ncomputational harmonic analysis, such as the butterfly factorization, and\ntraditional multi-scale methods, such as the Cooley-Tukey FFT algorithm, to\ndrastically reduce the number of trainable parameters to match the inherent\ncomplexity of the problem. As a result WideBNet is efficient: it requires fewer\ntraining points than off-the-shelf architectures, and has stable training\ndynamics, thus it can rely on standard weight initialization strategies. The\narchitecture automatically adapts to the dimensions of the data with only a few\nhyper-parameters that the user must specify. WideBNet is able to produce images\nthat are competitive with optimization-based approaches, but at a fraction of\nthe cost, and we also demonstrate numerically that it learns to super-resolve\nscatterers in the full aperture scattering setup.\n",
        "published": "2020",
        "authors": [
            "Matthew Li",
            "Laurent Demanet",
            "Leonardo Zepeda-N\u00fa\u00f1ez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.06635v4",
        "title": "ReLU Neural Networks of Polynomial Size for Exact Maximum Flow\n  Computation",
        "abstract": "  This paper studies the expressive power of artificial neural networks with\nrectified linear units. In order to study them as a model of real-valued\ncomputation, we introduce the concept of Max-Affine Arithmetic Programs and\nshow equivalence between them and neural networks concerning natural complexity\nmeasures. We then use this result to show that two fundamental combinatorial\noptimization problems can be solved with polynomial-size neural networks.\nFirst, we show that for any undirected graph with $n$ nodes, there is a neural\nnetwork (with fixed weights and biases) of size $\\mathcal{O}(n^3)$ that takes\nthe edge weights as input and computes the value of a minimum spanning tree of\nthe graph. Second, we show that for any directed graph with $n$ nodes and $m$\narcs, there is a neural network of size $\\mathcal{O}(m^2n^2)$ that takes the\narc capacities as input and computes a maximum flow. Our results imply that\nthese two problems can be solved with strongly polynomial time algorithms that\nsolely uses affine transformations and maxima computations, but no\ncomparison-based branchings.\n",
        "published": "2021",
        "authors": [
            "Christoph Hertrich",
            "Leon Sering"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2102.09544v3",
        "title": "Combinatorial optimization and reasoning with graph neural networks",
        "abstract": "  Combinatorial optimization is a well-established area in operations research\nand computer science. Until recently, its methods have focused on solving\nproblem instances in isolation, ignoring that they often stem from related data\ndistributions in practice. However, recent years have seen a surge of interest\nin using machine learning, especially graph neural networks (GNNs), as a key\nbuilding block for combinatorial tasks, either directly as solvers or by\nenhancing exact solvers. The inductive bias of GNNs effectively encodes\ncombinatorial and relational input due to their invariance to permutations and\nawareness of input sparsity. This paper presents a conceptual review of recent\nkey advancements in this emerging field, aiming at optimization and machine\nlearning researchers.\n",
        "published": "2021",
        "authors": [
            "Quentin Cappart",
            "Didier Ch\u00e9telat",
            "Elias Khalil",
            "Andrea Lodi",
            "Christopher Morris",
            "Petar Veli\u010dkovi\u0107"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.02619v2",
        "title": "Forward Super-Resolution: How Can GANs Learn Hierarchical Generative\n  Models for Real-World Distributions",
        "abstract": "  Generative adversarial networks (GANs) are among the most successful models\nfor learning high-complexity, real-world distributions. However, in theory, due\nto the highly non-convex, non-concave landscape of the minmax training\nobjective, GAN remains one of the least understood deep learning models. In\nthis work, we formally study how GANs can efficiently learn certain\nhierarchically generated distributions that are close to the distribution of\nreal-life images. We prove that when a distribution has a structure that we\nrefer to as Forward Super-Resolution, then simply training generative\nadversarial networks using stochastic gradient descent ascent (SGDA) can learn\nthis distribution efficiently, both in sample and time complexities. We also\nprovide empirical evidence that our assumption \"forward super-resolution\" is\nvery natural in practice, and the underlying learning mechanisms that we study\nin this paper (to allow us efficiently train GAN via SGDA in theory) simulates\nthe actual learning process of GANs on real-world problems.\n",
        "published": "2021",
        "authors": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2106.16225v2",
        "title": "Analytic Insights into Structure and Rank of Neural Network Hessian Maps",
        "abstract": "  The Hessian of a neural network captures parameter interactions through\nsecond-order derivatives of the loss. It is a fundamental object of study,\nclosely tied to various problems in deep learning, including model design,\noptimization, and generalization. Most prior work has been empirical, typically\nfocusing on low-rank approximations and heuristics that are blind to the\nnetwork structure. In contrast, we develop theoretical tools to analyze the\nrange of the Hessian map, providing us with a precise understanding of its rank\ndeficiency as well as the structural reasons behind it. This yields exact\nformulas and tight upper bounds for the Hessian rank of deep linear networks,\nallowing for an elegant interpretation in terms of rank deficiency. Moreover,\nwe demonstrate that our bounds remain faithful as an estimate of the numerical\nHessian rank, for a larger class of models such as rectified and hyperbolic\ntangent networks. Further, we also investigate the implications of model\narchitecture (e.g.~width, depth, bias) on the rank deficiency. Overall, our\nwork provides novel insights into the source and extent of redundancy in\noverparameterized networks.\n",
        "published": "2021",
        "authors": [
            "Sidak Pal Singh",
            "Gregor Bachmann",
            "Thomas Hofmann"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2111.05496v2",
        "title": "ResNEsts and DenseNEsts: Block-based DNN Models with Improved\n  Representation Guarantees",
        "abstract": "  Models recently used in the literature proving residual networks (ResNets)\nare better than linear predictors are actually different from standard ResNets\nthat have been widely used in computer vision. In addition to the assumptions\nsuch as scalar-valued output or single residual block, these models have no\nnonlinearities at the final residual representation that feeds into the final\naffine layer. To codify such a difference in nonlinearities and reveal a linear\nestimation property, we define ResNEsts, i.e., Residual Nonlinear Estimators,\nby simply dropping nonlinearities at the last residual representation from\nstandard ResNets. We show that wide ResNEsts with bottleneck blocks can always\nguarantee a very desirable training property that standard ResNets aim to\nachieve, i.e., adding more blocks does not decrease performance given the same\nset of basis elements. To prove that, we first recognize ResNEsts are basis\nfunction models that are limited by a coupling problem in basis learning and\nlinear prediction. Then, to decouple prediction weights from basis learning, we\nconstruct a special architecture termed augmented ResNEst (A-ResNEst) that\nalways guarantees no worse performance with the addition of a block. As a\nresult, such an A-ResNEst establishes empirical risk lower bounds for a ResNEst\nusing corresponding bases. Our results demonstrate ResNEsts indeed have a\nproblem of diminishing feature reuse; however, it can be avoided by\nsufficiently expanding or widening the input space, leading to the\nabove-mentioned desirable property. Inspired by the DenseNets that have been\nshown to outperform ResNets, we also propose a corresponding new model called\nDensely connected Nonlinear Estimator (DenseNEst). We show that any DenseNEst\ncan be represented as a wide ResNEst with bottleneck blocks. Unlike ResNEsts,\nDenseNEsts exhibit the desirable property without any special architectural\nre-design.\n",
        "published": "2021",
        "authors": [
            "Kuan-Lin Chen",
            "Ching-Hua Lee",
            "Harinath Garudadri",
            "Bhaskar D. Rao"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2203.16462v4",
        "title": "Convergence of gradient descent for deep neural networks",
        "abstract": "  This article presents a criterion for convergence of gradient descent to a\nglobal minimum, which is then used to show that gradient descent with proper\ninitialization converges to a global minimum when training any feedforward\nneural network with smooth and strictly increasing activation functions,\nprovided that the input dimension is greater than or equal to the number of\ndata points. The main difference with prior work is that the width of the\nnetwork can be a fixed number instead of growing as some multiple or power of\nthe number of data points.\n",
        "published": "2022",
        "authors": [
            "Sourav Chatterjee"
        ]
    }
]