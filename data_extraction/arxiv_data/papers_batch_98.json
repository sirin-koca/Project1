[
    {
        "id": "http://arxiv.org/abs/2401.01481v1",
        "title": "Optimizing UAV-UGV Coalition Operations: A Hybrid Clustering and\n  Multi-Agent Reinforcement Learning Approach for Path Planning in Obstructed\n  Environment",
        "abstract": "  One of the most critical applications undertaken by coalitions of Unmanned\nAerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is reaching\npredefined targets by following the most time-efficient routes while avoiding\ncollisions. Unfortunately, UAVs are hampered by limited battery life, and UGVs\nface challenges in reachability due to obstacles and elevation variations.\nExisting literature primarily focuses on one-to-one coalitions, which\nconstrains the efficiency of reaching targets. In this work, we introduce a\nnovel approach for a UAV-UGV coalition with a variable number of vehicles,\nemploying a modified mean-shift clustering algorithm to segment targets into\nmultiple zones. Each vehicle utilizes Multi-agent Deep Deterministic Policy\nGradient (MADDPG) and Multi-agent Proximal Policy Optimization (MAPPO), two\nadvanced reinforcement learning algorithms, to form an effective coalition for\nnavigating obstructed environments without collisions. This approach of\nassigning targets to various circular zones, based on density and range,\nsignificantly reduces the time required to reach these targets. Moreover,\nintroducing variability in the number of UAVs and UGVs in a coalition enhances\ntask efficiency by enabling simultaneous multi-target engagement. The results\nof our experimental evaluation demonstrate that our proposed method\nsubstantially surpasses current state-of-the-art techniques, nearly doubling\nefficiency in terms of target navigation time and task completion rate.\n",
        "published": "2024",
        "authors": [
            "Shamyo Brotee",
            "Farhan Kabir",
            "Md. Abdur Razzaque",
            "Palash Roy",
            "Md. Mamun-Or-Rashid",
            "Md. Rafiul Hassan",
            "Mohammad Mehedi Hassan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2401.01657v2",
        "title": "Distributed Pose-graph Optimization with Multi-level Partitioning for\n  Collaborative SLAM",
        "abstract": "  The back-end module of Distributed Collaborative Simultaneous Localization\nand Mapping (DCSLAM) requires solving a nonlinear Pose Graph Optimization (PGO)\nunder a distributed setting, also known as SE(d)-synchronization. Most existing\ndistributed graph optimization algorithms employ a simple sequential\npartitioning scheme, which may result in unbalanced subgraph dimensions due to\nthe different geographic locations of each robot, and hence imposes extra\ncommunication load. Moreover, the performance of current Riemannian\noptimization algorithms can be further accelerated. In this letter, we propose\na novel distributed pose graph optimization algorithm combining multi-level\npartitioning with an accelerated Riemannian optimization method. Firstly, we\nemploy the multi-level graph partitioning algorithm to preprocess the naive\npose graph to formulate a balanced optimization problem. In addition, inspired\nby the accelerated coordinate descent method, we devise an Improved Riemannian\nBlock Coordinate Descent (IRBCD) algorithm and the critical point obtained is\nglobally optimal. Finally, we evaluate the effects of four common graph\npartitioning approaches on the correlation of the inter-subgraphs, and discover\nthat the Highest scheme has the best partitioning performance. Also, we\nimplement simulations to quantitatively demonstrate that our proposed algorithm\noutperforms the state-of-the-art distributed pose graph optimization protocols.\n",
        "published": "2024",
        "authors": [
            "Cunhao Li",
            "Peng Yi",
            "Guanghui Guo",
            "Yiguang Hong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2205.13229v1",
        "title": "Symbiotic Child Emotional Support with Social Robots and Temporal\n  Knowledge Graphs",
        "abstract": "  In current youth-care programs, children with needs (mental health, family\nissues, learning disabilities, and autism) receive support from youth and\nfamily experts as one-to-one assistance at schools or hospitals. Occasionally,\nsocial robots have featured in such settings as support roles in a one-to-one\ninteraction with the child. In this paper, we suggest the development of a\nsymbiotic framework for real-time Emotional Support (ES) with social robots\nKnowledge Graphs (KG). By augmenting a domain-specific corpus from the\nliterature on ES for children (between the age of 8 and 12) and providing\nscenario-driven context including the history of events, we suggest developing\nan experimental knowledge-aware ES framework. The framework both guides the\nsocial robot in providing ES statements to the child and assists the expert in\ntracking and interpreting the child's emotional state and related events over\ntime.\n",
        "published": "2022",
        "authors": [
            "Isabella Saccardi",
            "Duygu Sezen Islakoglu",
            "Anouk Neerincx",
            "Federica Lucia Vinella"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2306.05741v1",
        "title": "Challenges and Opportunities for the Design of Smart Speakers",
        "abstract": "  Advances in voice technology and voice user interfaces (VUIs) -- such as\nAlexa, Siri, and Google Home -- have opened up the potential for many new types\nof interaction. However, despite the potential of these devices reflected by\nthe growing market and body of VUI research, there is a lingering sense that\nthe technology is still underused. In this paper, we conducted a systematic\nliterature review of 35 papers to identify and synthesize 127 VUI design\nguidelines into five themes. Additionally, we conducted semi-structured\ninterviews with 15 smart speaker users to understand their use and non-use of\nthe technology. From the interviews, we distill four design challenges that\ncontribute the most to non-use. Based on their (non-)use, we identify four\nopportunity spaces for designers to explore such as focusing on information\nsupport while multitasking (cooking, driving, childcare, etc), incorporating\nusers' mental models for smart speakers, and integrating calm design\nprinciples.\n",
        "published": "2023",
        "authors": [
            "Tao Long",
            "Lydia B. Chilton"
        ]
    },
    {
        "id": "http://arxiv.org/abs/0903.4930v1",
        "title": "Time manipulation technique for speeding up reinforcement learning in\n  simulations",
        "abstract": "  A technique for speeding up reinforcement learning algorithms by using time\nmanipulation is proposed. It is applicable to failure-avoidance control\nproblems running in a computer simulation. Turning the time of the simulation\nbackwards on failure events is shown to speed up the learning by 260% and\nimprove the state space exploration by 12% on the cart-pole balancing task,\ncompared to the conventional Q-learning and Actor-Critic algorithms.\n",
        "published": "2009",
        "authors": [
            "Petar Kormushev",
            "Kohei Nomoto",
            "Fangyan Dong",
            "Kaoru Hirota"
        ]
    },
    {
        "id": "http://arxiv.org/abs/0904.0545v2",
        "title": "Time Hopping technique for faster reinforcement learning in simulations",
        "abstract": "  This preprint has been withdrawn by the author for revision\n",
        "published": "2009",
        "authors": [
            "Petar Kormushev",
            "Kohei Nomoto",
            "Fangyan Dong",
            "Kaoru Hirota"
        ]
    },
    {
        "id": "http://arxiv.org/abs/0904.0546v1",
        "title": "Eligibility Propagation to Speed up Time Hopping for Reinforcement\n  Learning",
        "abstract": "  A mechanism called Eligibility Propagation is proposed to speed up the Time\nHopping technique used for faster Reinforcement Learning in simulations.\nEligibility Propagation provides for Time Hopping similar abilities to what\neligibility traces provide for conventional Reinforcement Learning. It\npropagates values from one state to all of its temporal predecessors using a\nstate transitions graph. Experiments on a simulated biped crawling robot\nconfirm that Eligibility Propagation accelerates the learning process more than\n3 times.\n",
        "published": "2009",
        "authors": [
            "Petar Kormushev",
            "Kohei Nomoto",
            "Fangyan Dong",
            "Kaoru Hirota"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1002.1480v1",
        "title": "A Minimum Relative Entropy Controller for Undiscounted Markov Decision\n  Processes",
        "abstract": "  Adaptive control problems are notoriously difficult to solve even in the\npresence of plant-specific controllers. One way to by-pass the intractable\ncomputation of the optimal policy is to restate the adaptive control as the\nminimization of the relative entropy of a controller that ignores the true\nplant dynamics from an informed controller. The solution is given by the\nBayesian control rule-a set of equations characterizing a stochastic adaptive\ncontroller for the class of possible plant dynamics. Here, the Bayesian control\nrule is applied to derive BCR-MDP, a controller to solve undiscounted Markov\ndecision processes with finite state and action spaces and unknown dynamics. In\nparticular, we derive a non-parametric conjugate prior distribution over the\npolicy space that encapsulates the agent's whole relevant history and we\npresent a Gibbs sampler to draw random policies from this distribution.\nPreliminary results show that BCR-MDP successfully avoids sub-optimal limit\ncycles due to its built-in mechanism to balance exploration versus\nexploitation.\n",
        "published": "2010",
        "authors": [
            "Pedro A. Ortega",
            "Daniel A. Braun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1112.1937v1",
        "title": "Bootstrapping Intrinsically Motivated Learning with Human Demonstrations",
        "abstract": "  This paper studies the coupling of internally guided learning and social\ninteraction, and more specifically the improvement owing to demonstrations of\nthe learning by intrinsic motivation. We present Socially Guided Intrinsic\nMotivation by Demonstration (SGIM-D), an algorithm for learning in continuous,\nunbounded and non-preset environments. After introducing social learning and\nintrinsic motivation, we describe the design of our algorithm, before showing\nthrough a fishing experiment that SGIM-D efficiently combines the advantages of\nsocial learning and intrinsic motivation to gain a wide repertoire while being\nspecialised in specific subspaces.\n",
        "published": "2011",
        "authors": [
            "Sao Mai Nguyen",
            "Adrien Baranes",
            "Pierre-Yves Oudeyer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1202.2112v1",
        "title": "Predicting Contextual Sequences via Submodular Function Maximization",
        "abstract": "  Sequence optimization, where the items in a list are ordered to maximize some\nreward has many applications such as web advertisement placement, search, and\ncontrol libraries in robotics. Previous work in sequence optimization produces\na static ordering that does not take any features of the item or context of the\nproblem into account. In this work, we propose a general approach to order the\nitems within the sequence based on the context (e.g., perceptual information,\nenvironment description, and goals). We take a simple, efficient,\nreduction-based approach where the choice and order of the items is established\nby repeatedly learning simple classifiers or regressors for each \"slot\" in the\nsequence. Our approach leverages recent work on submodular function\nmaximization to provide a formal regret reduction from submodular sequence\noptimization to simple cost-sensitive prediction. We apply our contextual\nsequence prediction algorithm to optimize control libraries and demonstrate\nresults on two robotics problems: manipulator trajectory prediction and mobile\nrobot path planning.\n",
        "published": "2012",
        "authors": [
            "Debadeepta Dey",
            "Tian Yu Liu",
            "Martial Hebert",
            "J. Andrew Bagnell"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1302.0386v1",
        "title": "Fast Damage Recovery in Robotics with the T-Resilience Algorithm",
        "abstract": "  Damage recovery is critical for autonomous robots that need to operate for a\nlong time without assistance. Most current methods are complex and costly\nbecause they require anticipating each potential damage in order to have a\ncontingency plan ready. As an alternative, we introduce the T-resilience\nalgorithm, a new algorithm that allows robots to quickly and autonomously\ndiscover compensatory behaviors in unanticipated situations. This algorithm\nequips the robot with a self-model and discovers new behaviors by learning to\navoid those that perform differently in the self-model and in reality. Our\nalgorithm thus does not identify the damaged parts but it implicitly searches\nfor efficient behaviors that do not use them. We evaluate the T-Resilience\nalgorithm on a hexapod robot that needs to adapt to leg removal, broken legs\nand motor failures; we compare it to stochastic local search, policy gradient\nand the self-modeling algorithm proposed by Bongard et al. The behavior of the\nrobot is assessed on-board thanks to a RGB-D sensor and a SLAM algorithm. Using\nonly 25 tests on the robot and an overall running time of 20 minutes,\nT-Resilience consistently leads to substantially better results than the other\napproaches.\n",
        "published": "2013",
        "authors": [
            "Sylvain Koos",
            "Antoine Cully",
            "Jean-Baptiste Mouret"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1306.1520v1",
        "title": "Policy Search: Any Local Optimum Enjoys a Global Performance Guarantee",
        "abstract": "  Local Policy Search is a popular reinforcement learning approach for handling\nlarge state spaces. Formally, it searches locally in a paramet erized policy\nspace in order to maximize the associated value function averaged over some\npredefined distribution. It is probably commonly b elieved that the best one\ncan hope in general from such an approach is to get a local optimum of this\ncriterion. In this article, we show th e following surprising result:\n\\emph{any} (approximate) \\emph{local optimum} enjoys a \\emph{global performance\nguarantee}. We compare this g uarantee with the one that is satisfied by Direct\nPolicy Iteration, an approximate dynamic programming algorithm that does some\nform of Poli cy Search: if the approximation error of Local Policy Search may\ngenerally be bigger (because local search requires to consider a space of s\ntochastic policies), we argue that the concentrability coefficient that appears\nin the performance bound is much nicer. Finally, we discuss several practical\nand theoretical consequences of our analysis.\n",
        "published": "2013",
        "authors": [
            "Bruno Scherrer",
            "Matthieu Geist"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1306.5707v2",
        "title": "Synthesizing Manipulation Sequences for Under-Specified Tasks using\n  Unrolled Markov Random Fields",
        "abstract": "  Many tasks in human environments require performing a sequence of navigation\nand manipulation steps involving objects. In unstructured human environments,\nthe location and configuration of the objects involved often change in\nunpredictable ways. This requires a high-level planning strategy that is robust\nand flexible in an uncertain environment. We propose a novel dynamic planning\nstrategy, which can be trained from a set of example sequences. High level\ntasks are expressed as a sequence of primitive actions or controllers (with\nappropriate parameters). Our score function, based on Markov Random Field\n(MRF), captures the relations between environment, controllers, and their\narguments. By expressing the environment using sets of attributes, the approach\ngeneralizes well to unseen scenarios. We train the parameters of our MRF using\na maximum margin learning method. We provide a detailed empirical validation of\nour overall framework demonstrating successful plan strategies for a variety of\ntasks.\n",
        "published": "2013",
        "authors": [
            "Jaeyong Sung",
            "Bart Selman",
            "Ashutosh Saxena"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1309.0671v1",
        "title": "BayesOpt: A Library for Bayesian optimization with Robotics Applications",
        "abstract": "  The purpose of this paper is twofold. On one side, we present a general\nframework for Bayesian optimization and we compare it with some related fields\nin active learning and Bayesian numerical analysis. On the other hand, Bayesian\noptimization and related problems (bandits, sequential experimental design) are\nhighly dependent on the surrogate model that is selected. However, there is no\nclear standard in the literature. Thus, we present a fast and flexible toolbox\nthat allows to test and combine different models and criteria with little\neffort. It includes most of the state-of-the-art contributions, algorithms and\nmodels. Its speed also removes part of the stigma that Bayesian optimization\nmethods are only good for \"expensive functions\". The software is free and it\ncan be used in many operating systems and computer languages.\n",
        "published": "2013",
        "authors": [
            "Ruben Martinez-Cantin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1309.4714v1",
        "title": "Temporal-Difference Learning to Assist Human Decision Making during the\n  Control of an Artificial Limb",
        "abstract": "  In this work we explore the use of reinforcement learning (RL) to help with\nhuman decision making, combining state-of-the-art RL algorithms with an\napplication to prosthetics. Managing human-machine interaction is a problem of\nconsiderable scope, and the simplification of human-robot interfaces is\nespecially important in the domains of biomedical technology and rehabilitation\nmedicine. For example, amputees who control artificial limbs are often required\nto quickly switch between a number of control actions or modes of operation in\norder to operate their devices. We suggest that by learning to anticipate\n(predict) a user's behaviour, artificial limbs could take on an active role in\na human's control decisions so as to reduce the burden on their users.\nRecently, we showed that RL in the form of general value functions (GVFs) could\nbe used to accurately detect a user's control intent prior to their explicit\ncontrol choices. In the present work, we explore the use of temporal-difference\nlearning and GVFs to predict when users will switch their control influence\nbetween the different motor functions of a robot arm. Experiments were\nperformed using a multi-function robot arm that was controlled by muscle\nsignals from a user's body (similar to conventional artificial limb control).\nOur approach was able to acquire and maintain forecasts about a user's\nswitching decisions in real time. It also provides an intuitive and reward-free\nway for users to correct or reinforce the decisions made by the machine\nlearning system. We expect that when a system is certain enough about its\npredictions, it can begin to take over switching decisions from the user to\nstreamline control and potentially decrease the time and effort needed to\ncomplete tasks. This preliminary study therefore suggests a way to naturally\nintegrate human- and machine-based decision making systems.\n",
        "published": "2013",
        "authors": [
            "Ann L. Edwards",
            "Alexandra Kearney",
            "Michael Rory Dawson",
            "Richard S. Sutton",
            "Patrick M. Pilarski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1405.6341v1",
        "title": "Efficient Model Learning for Human-Robot Collaborative Tasks",
        "abstract": "  We present a framework for learning human user models from joint-action\ndemonstrations that enables the robot to compute a robust policy for a\ncollaborative task with a human. The learning takes place completely\nautomatically, without any human intervention. First, we describe the\nclustering of demonstrated action sequences into different human types using an\nunsupervised learning algorithm. These demonstrated sequences are also used by\nthe robot to learn a reward function that is representative for each type,\nthrough the employment of an inverse reinforcement learning algorithm. The\nlearned model is then used as part of a Mixed Observability Markov Decision\nProcess formulation, wherein the human type is a partially observable variable.\nWith this framework, we can infer, either offline or online, the human type of\na new user that was not included in the training set, and can compute a policy\nfor the robot that will be aligned to the preference of this new user and will\nbe robust to deviations of the human actions from prior demonstrations. Finally\nwe validate the approach using data collected in human subject experiments, and\nconduct proof-of-concept demonstrations in which a person performs a\ncollaborative task with a small industrial robot.\n",
        "published": "2014",
        "authors": [
            "Stefanos Nikolaidis",
            "Keren Gu",
            "Ramya Ramakrishnan",
            "Julie Shah"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1408.1913v1",
        "title": "Using Learned Predictions as Feedback to Improve Control and\n  Communication with an Artificial Limb: Preliminary Findings",
        "abstract": "  Many people suffer from the loss of a limb. Learning to get by without an arm\nor hand can be very challenging, and existing prostheses do not yet fulfil the\nneeds of individuals with amputations. One promising solution is to provide\ngreater communication between a prosthesis and its user. Towards this end, we\npresent a simple machine learning interface to supplement the control of a\nrobotic limb with feedback to the user about what the limb will be experiencing\nin the near future. A real-time prediction learner was implemented to predict\nimpact-related electrical load experienced by a robot limb; the learning\nsystem's predictions were then communicated to the device's user to aid in\ntheir interactions with a workspace. We tested this system with five\nable-bodied subjects. Each subject manipulated the robot arm while receiving\ndifferent forms of vibrotactile feedback regarding the arm's contact with its\nworkspace. Our trials showed that communicable predictions could be learned\nquickly during human control of the robot arm. Using these predictions as a\nbasis for feedback led to a statistically significant improvement in task\nperformance when compared to purely reactive feedback from the device. Our\nstudy therefore contributes initial evidence that prediction learning and\nmachine intelligence can benefit not just control, but also feedback from an\nartificial limb. We expect that a greater level of acceptance and ownership can\nbe achieved if the prosthesis itself takes an active role in transmitting\nlearned knowledge about its state and its situation of use.\n",
        "published": "2014",
        "authors": [
            "Adam S. R. Parker",
            "Ann L. Edwards",
            "Patrick M. Pilarski"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1411.3895v1",
        "title": "Learning Fuzzy Controllers in Mobile Robotics with Embedded\n  Preprocessing",
        "abstract": "  The automatic design of controllers for mobile robots usually requires two\nstages. In the first stage,sensorial data are preprocessed or transformed into\nhigh level and meaningful values of variables whichare usually defined from\nexpert knowledge. In the second stage, a machine learning technique is applied\ntoobtain a controller that maps these high level variables to the control\ncommands that are actually sent tothe robot. This paper describes an algorithm\nthat is able to embed the preprocessing stage into the learningstage in order\nto get controllers directly starting from sensorial raw data with no expert\nknowledgeinvolved. Due to the high dimensionality of the sensorial data, this\napproach uses Quantified Fuzzy Rules(QFRs), that are able to transform\nlow-level input variables into high-level input variables, reducingthe\ndimensionality through summarization. The proposed learning algorithm, called\nIterative QuantifiedFuzzy Rule Learning (IQFRL), is based on genetic\nprogramming. IQFRL is able to learn rules with differentstructures, and can\nmanage linguistic variables with multiple granularities. The algorithm has been\ntestedwith the implementation of the wall-following behavior both in several\nrealistic simulated environmentswith different complexity and on a Pioneer 3-AT\nrobot in two real environments. Results have beencompared with several\nwell-known learning algorithms combined with different data\npreprocessingtechniques, showing that IQFRL exhibits a better and statistically\nsignificant performance. Moreover,three real world applications for which IQFRL\nplays a central role are also presented: path and objecttracking with static\nand moving obstacles avoidance.\n",
        "published": "2014",
        "authors": [
            "I. Rodr\u00edguez-Fdez",
            "M. Mucientes",
            "A. Bugar\u00edn"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1412.6451v1",
        "title": "Grounding Hierarchical Reinforcement Learning Models for Knowledge\n  Transfer",
        "abstract": "  Methods of deep machine learning enable to to reuse low-level representations\nefficiently for generating more abstract high-level representations.\nOriginally, deep learning has been applied passively (e.g., for classification\npurposes). Recently, it has been extended to estimate the value of actions for\nautonomous agents within the framework of reinforcement learning (RL). Explicit\nmodels of the environment can be learned to augment such a value function.\nAlthough \"flat\" connectionist methods have already been used for model-based\nRL, up to now, only model-free variants of RL have been equipped with methods\nfrom deep learning. We propose a variant of deep model-based RL that enables an\nagent to learn arbitrarily abstract hierarchical representations of its\nenvironment. In this paper, we present research on how such hierarchical\nrepresentations can be grounded in sensorimotor interaction between an agent\nand its environment.\n",
        "published": "2014",
        "authors": [
            "Mark Wernsdorfer",
            "Ute Schmid"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1502.06132v1",
        "title": "Universal Memory Architectures for Autonomous Machines",
        "abstract": "  We propose a self-organizing memory architecture for perceptual experience,\ncapable of supporting autonomous learning and goal-directed problem solving in\nthe absence of any prior information about the agent's environment. The\narchitecture is simple enough to ensure (1) a quadratic bound (in the number of\navailable sensors) on space requirements, and (2) a quadratic bound on the\ntime-complexity of the update-execute cycle. At the same time, it is\nsufficiently complex to provide the agent with an internal representation which\nis (3) minimal among all representations of its class which account for every\nsensory equivalence class subject to the agent's belief state; (4) capable, in\nprinciple, of recovering the homotopy type of the system's state space; (5)\nlearnable with arbitrary precision through a random application of the\navailable actions. The provable properties of an effectively trained memory\nstructure exploit a duality between weak poc sets -- a symbolic (discrete)\nrepresentation of subset nesting relations -- and non-positively curved cubical\ncomplexes, whose rich convexity theory underlies the planning cycle of the\nproposed architecture.\n",
        "published": "2015",
        "authors": [
            "Dan P. Guralnik",
            "Daniel E. Koditschek"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1506.02312v1",
        "title": "A Framework for Constrained and Adaptive Behavior-Based Agents",
        "abstract": "  Behavior Trees are commonly used to model agents for robotics and games,\nwhere constrained behaviors must be designed by human experts in order to\nguarantee that these agents will execute a specific chain of actions given a\nspecific set of perceptions. In such application areas, learning is a desirable\nfeature to provide agents with the ability to adapt and improve interactions\nwith humans and environment, but often discarded due to its unreliability. In\nthis paper, we propose a framework that uses Reinforcement Learning nodes as\npart of Behavior Trees to address the problem of adding learning capabilities\nin constrained agents. We show how this framework relates to Options in\nHierarchical Reinforcement Learning, ensuring convergence of nested learning\nnodes, and we empirically show that the learning nodes do not affect the\nexecution of other nodes in the tree.\n",
        "published": "2015",
        "authors": [
            "Renato de Pontes Pereira",
            "Paulo Martins Engel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1507.02347v1",
        "title": "Achieving Synergy in Cognitive Behavior of Humanoids via Deep Learning\n  of Dynamic Visuo-Motor-Attentional Coordination",
        "abstract": "  The current study examines how adequate coordination among different\ncognitive processes including visual recognition, attention switching, action\npreparation and generation can be developed via learning of robots by\nintroducing a novel model, the Visuo-Motor Deep Dynamic Neural Network (VMDNN).\nThe proposed model is built on coupling of a dynamic vision network, a motor\ngeneration network, and a higher level network allocated on top of these two.\nThe simulation experiments using the iCub simulator were conducted for\ncognitive tasks including visual object manipulation responding to human\ngestures. The results showed that synergetic coordination can be developed via\niterative learning through the whole network when spatio-temporal hierarchy and\ntemporal one can be self-organized in the visual pathway and in the motor\npathway, respectively, such that the higher level can manipulate them with\nabstraction.\n",
        "published": "2015",
        "authors": [
            "Jungsik Hwang",
            "Minju Jung",
            "Naveen Madapana",
            "Jinhyung Kim",
            "Minkyu Choi",
            "Jun Tani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1601.00741v1",
        "title": "Learning Preferences for Manipulation Tasks from Online Coactive\n  Feedback",
        "abstract": "  We consider the problem of learning preferences over trajectories for mobile\nmanipulators such as personal robots and assembly line robots. The preferences\nwe learn are more intricate than simple geometric constraints on trajectories;\nthey are rather governed by the surrounding context of various objects and\nhuman interactions in the environment. We propose a coactive online learning\nframework for teaching preferences in contextually rich environments. The key\nnovelty of our approach lies in the type of feedback expected from the user:\nthe human user does not need to demonstrate optimal trajectories as training\ndata, but merely needs to iteratively provide trajectories that slightly\nimprove over the trajectory currently proposed by the system. We argue that\nthis coactive preference feedback can be more easily elicited than\ndemonstrations of optimal trajectories. Nevertheless, theoretical regret bounds\nof our algorithm match the asymptotic rates of optimal trajectory algorithms.\n  We implement our algorithm on two high degree-of-freedom robots, PR2 and\nBaxter, and present three intuitive mechanisms for providing such incremental\nfeedback. In our experimental evaluation we consider two context rich settings\n-- household chores and grocery store checkout -- and show that users are able\nto train the robot with just a few feedbacks (taking only a few\nminutes).\\footnote{Parts of this work has been published at NIPS and ISRR\nconferences~\\citep{Jain13,Jain13b}. This journal submission presents a\nconsistent full paper, and also includes the proof of regret bounds, more\ndetails of the robotic system, and a thorough related work.}\n",
        "published": "2016",
        "authors": [
            "Ashesh Jain",
            "Shikhar Sharma",
            "Thorsten Joachims",
            "Ashutosh Saxena"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1601.02705v1",
        "title": "Robobarista: Learning to Manipulate Novel Objects via Deep Multimodal\n  Embedding",
        "abstract": "  There is a large variety of objects and appliances in human environments,\nsuch as stoves, coffee dispensers, juice extractors, and so on. It is\nchallenging for a roboticist to program a robot for each of these object types\nand for each of their instantiations. In this work, we present a novel approach\nto manipulation planning based on the idea that many household objects share\nsimilarly-operated object parts. We formulate the manipulation planning as a\nstructured prediction problem and learn to transfer manipulation strategy\nacross different objects by embedding point-cloud, natural language, and\nmanipulation trajectory data into a shared embedding space using a deep neural\nnetwork. In order to learn semantically meaningful spaces throughout our\nnetwork, we introduce a method for pre-training its lower layers for multimodal\nfeature embedding and a method for fine-tuning this embedding space using a\nloss-based margin. In order to collect a large number of manipulation\ndemonstrations for different objects, we develop a new crowd-sourcing platform\ncalled Robobarista. We test our model on our dataset consisting of 116 objects\nand appliances with 249 parts along with 250 language instructions, for which\nthere are 1225 crowd-sourced manipulation demonstrations. We further show that\nour robot with our model can even prepare a cup of a latte with appliances it\nhas never seen before.\n",
        "published": "2016",
        "authors": [
            "Jaeyong Sung",
            "Seok Hyun Jin",
            "Ian Lenz",
            "Ashutosh Saxena"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.00448v3",
        "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy\n  Optimization",
        "abstract": "  Reinforcement learning can acquire complex behaviors from high-level\nspecifications. However, defining a cost function that can be optimized\neffectively and encodes the correct task is challenging in practice. We explore\nhow inverse optimal control (IOC) can be used to learn behaviors from\ndemonstrations, with applications to torque control of high-dimensional robotic\nsystems. Our method addresses two key challenges in inverse optimal control:\nfirst, the need for informative features and effective regularization to impose\nstructure on the cost, and second, the difficulty of learning the cost function\nunder unknown dynamics for high-dimensional continuous systems. To address the\nformer challenge, we present an algorithm capable of learning arbitrary\nnonlinear cost functions, such as neural networks, without meticulous feature\nengineering. To address the latter challenge, we formulate an efficient\nsample-based approximation for MaxEnt IOC. We evaluate our method on a series\nof simulated tasks and real-world robotic manipulation problems, demonstrating\nsubstantial improvement over prior methods both in terms of task complexity and\nsample efficiency.\n",
        "published": "2016",
        "authors": [
            "Chelsea Finn",
            "Sergey Levine",
            "Pieter Abbeel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.00748v1",
        "title": "Continuous Deep Q-Learning with Model-based Acceleration",
        "abstract": "  Model-free reinforcement learning has been successfully applied to a range of\nchallenging problems, and has recently been extended to handle large neural\nnetwork policies and value functions. However, the sample complexity of\nmodel-free algorithms, particularly when using high-dimensional function\napproximators, tends to limit their applicability to physical systems. In this\npaper, we explore algorithms and representations to reduce the sample\ncomplexity of deep reinforcement learning for continuous control tasks. We\npropose two complementary techniques for improving the efficiency of such\nalgorithms. First, we derive a continuous variant of the Q-learning algorithm,\nwhich we call normalized adantage functions (NAF), as an alternative to the\nmore commonly used policy gradient and actor-critic methods. NAF representation\nallows us to apply Q-learning with experience replay to continuous tasks, and\nsubstantially improves performance on a set of simulated robotic control tasks.\nTo further improve the efficiency of our approach, we explore the use of\nlearned models for accelerating model-free reinforcement learning. We show that\niteratively refitted local linear models are especially effective for this, and\ndemonstrate substantially faster learning on domains where such models are\napplicable.\n",
        "published": "2016",
        "authors": [
            "Shixiang Gu",
            "Timothy Lillicrap",
            "Ilya Sutskever",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.02038v1",
        "title": "Unscented Bayesian Optimization for Safe Robot Grasping",
        "abstract": "  We address the robot grasp optimization problem of unknown objects\nconsidering uncertainty in the input space. Grasping unknown objects can be\nachieved by using a trial and error exploration strategy. Bayesian optimization\nis a sample efficient optimization algorithm that is especially suitable for\nthis setups as it actively reduces the number of trials for learning about the\nfunction to optimize. In fact, this active object exploration is the same\nstrategy that infants do to learn optimal grasps. One problem that arises while\nlearning grasping policies is that some configurations of grasp parameters may\nbe very sensitive to error in the relative pose between the object and robot\nend-effector. We call these configurations unsafe because small errors during\ngrasp execution may turn good grasps into bad grasps. Therefore, to reduce the\nrisk of grasp failure, grasps should be planned in safe areas. We propose a new\nalgorithm, Unscented Bayesian optimization that is able to perform sample\nefficient optimization while taking into consideration input noise to find safe\noptima. The contribution of Unscented Bayesian optimization is twofold as if\nprovides a new decision process that drives exploration to safe regions and a\nnew selection procedure that chooses the optimal in terms of its safety without\nextra analysis or computational cost. Both contributions are rooted on the\nstrong theory behind the unscented transformation, a popular nonlinear\napproximation method. We show its advantages with respect to the classical\nBayesian optimization both in synthetic problems and in realistic robot grasp\nsimulations. The results highlights that our method achieves optimal and robust\ngrasping policies after few trials while the selected grasps remain in safe\nregions.\n",
        "published": "2016",
        "authors": [
            "Jos\u00e9 Nogueira",
            "Ruben Martinez-Cantin",
            "Alexandre Bernardino",
            "Lorenzo Jamone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1603.03833v4",
        "title": "From virtual demonstration to real-world manipulation using LSTM and MDN",
        "abstract": "  Robots assisting the disabled or elderly must perform complex manipulation\ntasks and must adapt to the home environment and preferences of their user.\nLearning from demonstration is a promising choice, that would allow the\nnon-technical user to teach the robot different tasks. However, collecting\ndemonstrations in the home environment of a disabled user is time consuming,\ndisruptive to the comfort of the user, and presents safety challenges. It would\nbe desirable to perform the demonstrations in a virtual environment. In this\npaper we describe a solution to the challenging problem of behavior transfer\nfrom virtual demonstration to a physical robot. The virtual demonstrations are\nused to train a deep neural network based controller, which is using a Long\nShort Term Memory (LSTM) recurrent neural network to generate trajectories. The\ntraining process uses a Mixture Density Network (MDN) to calculate an error\nsignal suitable for the multimodal nature of demonstrations. The controller\nlearned in the virtual environment is transferred to a physical robot (a\nRethink Robotics Baxter). An off-the-shelf vision component is used to\nsubstitute for geometric knowledge available in the simulation and an inverse\nkinematics module is used to allow the Baxter to enact the trajectory. Our\nexperimental studies validate the three contributions of the paper: (1) the\ncontroller learned from virtual demonstrations can be used to successfully\nperform the manipulation tasks on a physical robot, (2) the LSTM+MDN\narchitectural choice outperforms other choices, such as the use of feedforward\nnetworks and mean-squared error based training signals and (3) allowing\nimperfect demonstrations in the training set also allows the controller to\nlearn how to correct its manipulation mistakes.\n",
        "published": "2016",
        "authors": [
            "Rouhollah Rahmatizadeh",
            "Pooya Abolghasemi",
            "Aman Behal",
            "Ladislau B\u00f6l\u00f6ni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1605.06450v1",
        "title": "Query-Efficient Imitation Learning for End-to-End Autonomous Driving",
        "abstract": "  One way to approach end-to-end autonomous driving is to learn a policy\nfunction that maps from a sensory input, such as an image frame from a\nfront-facing camera, to a driving action, by imitating an expert driver, or a\nreference policy. This can be done by supervised learning, where a policy\nfunction is tuned to minimize the difference between the predicted and\nground-truth actions. A policy function trained in this way however is known to\nsuffer from unexpected behaviours due to the mismatch between the states\nreachable by the reference policy and trained policy functions. More advanced\nalgorithms for imitation learning, such as DAgger, addresses this issue by\niteratively collecting training examples from both reference and trained\npolicies. These algorithms often requires a large number of queries to a\nreference policy, which is undesirable as the reference policy is often\nexpensive. In this paper, we propose an extension of the DAgger, called\nSafeDAgger, that is query-efficient and more suitable for end-to-end autonomous\ndriving. We evaluate the proposed SafeDAgger in a car racing simulator and show\nthat it indeed requires less queries to a reference policy. We observe a\nsignificant speed up in convergence, which we conjecture to be due to the\neffect of automated curriculum learning.\n",
        "published": "2016",
        "authors": [
            "Jiakai Zhang",
            "Kyunghyun Cho"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1608.00359v1",
        "title": "Discovering Latent States for Model Learning: Applying Sensorimotor\n  Contingencies Theory and Predictive Processing to Model Context",
        "abstract": "  Autonomous robots need to be able to adapt to unforeseen situations and to\nacquire new skills through trial and error. Reinforcement learning in principle\noffers a suitable methodological framework for this kind of autonomous\nlearning. However current computational reinforcement learning agents mostly\nlearn each individual skill entirely from scratch. How can we enable artificial\nagents, such as robots, to acquire some form of generic knowledge, which they\ncould leverage for the learning of new skills? This paper argues that, like the\nbrain, the cognitive system of artificial agents has to develop a world model\nto support adaptive behavior and learning. Inspiration is taken from two recent\ndevelopments in the cognitive science literature: predictive processing\ntheories of cognition, and the sensorimotor contingencies theory of perception.\nBased on these, a hypothesis is formulated about what the content of\ninformation might be that is encoded in an internal world model, and how an\nagent could autonomously acquire it. A computational model is described to\nformalize this hypothesis, and is evaluated in a series of simulation\nexperiments.\n",
        "published": "2016",
        "authors": [
            "Nikolas J. Hemion"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1608.00627v1",
        "title": "Learning Transferable Policies for Monocular Reactive MAV Control",
        "abstract": "  The ability to transfer knowledge gained in previous tasks into new contexts\nis one of the most important mechanisms of human learning. Despite this,\nadapting autonomous behavior to be reused in partially similar settings is\nstill an open problem in current robotics research. In this paper, we take a\nsmall step in this direction and propose a generic framework for learning\ntransferable motion policies. Our goal is to solve a learning problem in a\ntarget domain by utilizing the training data in a different but related source\ndomain. We present this in the context of an autonomous MAV flight using\nmonocular reactive control, and demonstrate the efficacy of our proposed\napproach through extensive real-world flight experiments in outdoor cluttered\nenvironments.\n",
        "published": "2016",
        "authors": [
            "Shreyansh Daftry",
            "J. Andrew Bagnell",
            "Martial Hebert"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1608.00737v1",
        "title": "Context Discovery for Model Learning in Partially Observable\n  Environments",
        "abstract": "  The ability to learn a model is essential for the success of autonomous\nagents. Unfortunately, learning a model is difficult in partially observable\nenvironments, where latent environmental factors influence what the agent\nobserves. In the absence of a supervisory training signal, autonomous agents\ntherefore require a mechanism to autonomously discover these environmental\nfactors, or sensorimotor contexts.\n  This paper presents a method to discover sensorimotor contexts in partially\nobservable environments, by constructing a hierarchical transition model. The\nmethod is evaluated in a simulation experiment, in which a robot learns that\ndifferent rooms are characterized by different objects that are found in them.\n",
        "published": "2016",
        "authors": [
            "Nikolas J. Hemion"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1609.09001v2",
        "title": "Learning from the Hindsight Plan -- Episodic MPC Improvement",
        "abstract": "  Model predictive control (MPC) is a popular control method that has proved\neffective for robotics, among other fields. MPC performs re-planning at every\ntime step. Re-planning is done with a limited horizon per computational and\nreal-time constraints and often also for robustness to potential model errors.\nHowever, the limited horizon leads to suboptimal performance. In this work, we\nconsider the iterative learning setting, where the same task can be repeated\nseveral times, and propose a policy improvement scheme for MPC. The main idea\nis that between executions we can, offline, run MPC with a longer horizon,\nresulting in a hindsight plan. To bring the next real-world execution closer to\nthe hindsight plan, our approach learns to re-shape the original cost function\nwith the goal of satisfying the following property: short horizon planning (as\nrealistic during real executions) with respect to the shaped cost should result\nin mimicking the hindsight plan. This effectively consolidates long-term\nreasoning into the short-horizon planning. We empirically evaluate our approach\nin contact-rich manipulation tasks both in simulated and real environments,\nsuch as peg insertion by a real PR2 robot.\n",
        "published": "2016",
        "authors": [
            "Aviv Tamar",
            "Garrett Thomas",
            "Tianhao Zhang",
            "Sergey Levine",
            "Pieter Abbeel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1701.01497v1",
        "title": "Learning local trajectories for high precision robotic tasks :\n  application to KUKA LBR iiwa Cartesian positioning",
        "abstract": "  To ease the development of robot learning in industry, two conditions need to\nbe fulfilled. Manipulators must be able to learn high accuracy and precision\ntasks while being safe for workers in the factory. In this paper, we extend\npreviously submitted work which consists in rapid learning of local high\naccuracy behaviors. By exploration and regression, linear and quadratic models\nare learnt for respectively the dynamics and cost function. Iterative Linear\nQuadratic Gaussian Regulator combined with cost quadratic regression can\nconverge rapidly in the final stages towards high accuracy behavior as the cost\nfunction is modelled quite precisely. In this paper, both a different cost\nfunction and a second order improvement method are implemented within this\nframework. We also propose an analysis of the algorithm parameters through\nsimulation for a positioning task. Finally, an experimental validation on a\nKUKA LBR iiwa robot is carried out. This collaborative robot manipulator can be\neasily programmed into safety mode, which makes it qualified for the second\nindustry constraint stated above.\n",
        "published": "2017",
        "authors": [
            "Joris Guerin",
            "Olivier Gibaru",
            "Eric Nyiri",
            "Stephane Thiery"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1702.06329v1",
        "title": "Towards a Common Implementation of Reinforcement Learning for Multiple\n  Robotic Tasks",
        "abstract": "  Mobile robots are increasingly being employed for performing complex tasks in\ndynamic environments. Reinforcement learning (RL) methods are recognized to be\npromising for specifying such tasks in a relatively simple manner. However, the\nstrong dependency between the learning method and the task to learn is a\nwell-known problem that restricts practical implementations of RL in robotics,\noften requiring major modifications of parameters and adding other techniques\nfor each particular task. In this paper we present a practical core\nimplementation of RL which enables the learning process for multiple robotic\ntasks with minimal per-task tuning or none. Based on value iteration methods,\nthis implementation includes a novel approach for action selection, called\nQ-biased softmax regression (QBIASSR), which avoids poor performance of the\nlearning process when the robot reaches new unexplored states. Our approach\ntakes advantage of the structure of the state space by attending the physical\nvariables involved (e.g., distances to obstacles, X,Y,{\\theta} pose, etc.),\nthus experienced sets of states may favor the decision-making process of\nunexplored or rarely-explored states. This improvement has a relevant role in\nreducing the tuning of the algorithm for particular tasks. Experiments with\nreal and simulated robots, performed with the software framework also\nintroduced here, show that our implementation is effectively able to learn\ndifferent robotic tasks without tuning the learning method. Results also\nsuggest that the combination of true online SARSA({\\lambda}) with QBIASSR can\noutperform the existing RL core algorithms in low-dimensional robotic tasks.\n",
        "published": "2017",
        "authors": [
            "Angel Mart\u00ednez-Tenor",
            "Juan Antonio Fern\u00e1ndez-Madrigal",
            "Ana Cruz-Mart\u00edn",
            "Javier Gonz\u00e1lez-Jim\u00e9nez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.00420v4",
        "title": "Virtual-to-real Deep Reinforcement Learning: Continuous Control of\n  Mobile Robots for Mapless Navigation",
        "abstract": "  We present a learning-based mapless motion planner by taking the sparse\n10-dimensional range findings and the target position with respect to the\nmobile robot coordinate frame as input and the continuous steering commands as\noutput. Traditional motion planners for mobile ground robots with a laser range\nsensor mostly depend on the obstacle map of the navigation environment where\nboth the highly precise laser sensor and the obstacle map building work of the\nenvironment are indispensable. We show that, through an asynchronous deep\nreinforcement learning method, a mapless motion planner can be trained\nend-to-end without any manually designed features and prior demonstrations. The\ntrained planner can be directly applied in unseen virtual and real\nenvironments. The experiments show that the proposed mapless motion planner can\nnavigate the nonholonomic mobile robot to the desired targets without colliding\nwith any obstacles.\n",
        "published": "2017",
        "authors": [
            "Lei Tai",
            "Giuseppe Paolo",
            "Ming Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.01946v3",
        "title": "Metric Learning for Generalizing Spatial Relations to New Objects",
        "abstract": "  Human-centered environments are rich with a wide variety of spatial relations\nbetween everyday objects. For autonomous robots to operate effectively in such\nenvironments, they should be able to reason about these relations and\ngeneralize them to objects with different shapes and sizes. For example, having\nlearned to place a toy inside a basket, a robot should be able to generalize\nthis concept using a spoon and a cup. This requires a robot to have the\nflexibility to learn arbitrary relations in a lifelong manner, making it\nchallenging for an expert to pre-program it with sufficient knowledge to do so\nbeforehand. In this paper, we address the problem of learning spatial relations\nby introducing a novel method from the perspective of distance metric learning.\nOur approach enables a robot to reason about the similarity between pairwise\nspatial relations, thereby enabling it to use its previous knowledge when\npresented with a new relation to imitate. We show how this makes it possible to\nlearn arbitrary spatial relations from non-expert users using a small number of\nexamples and in an interactive manner. Our extensive evaluation with real-world\ndata demonstrates the effectiveness of our method in reasoning about a\ncontinuous spectrum of spatial relations and generalizing them to new objects.\n",
        "published": "2017",
        "authors": [
            "Oier Mees",
            "Nichola Abdo",
            "Mladen Mazuran",
            "Wolfram Burgard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.02660v2",
        "title": "Towards Generalization and Simplicity in Continuous Control",
        "abstract": "  This work shows that policies with simple linear and RBF parameterizations\ncan be trained to solve a variety of continuous control tasks, including the\nOpenAI gym benchmarks. The performance of these trained policies are\ncompetitive with state of the art results, obtained with more elaborate\nparameterizations such as fully connected neural networks. Furthermore,\nexisting training and testing scenarios are shown to be very limited and prone\nto over-fitting, thus giving rise to only trajectory-centric policies. Training\nwith a diverse initial state distribution is shown to produce more global\npolicies with better generalization. This allows for interactive control\nscenarios where the system recovers from large on-line perturbations; as shown\nin the supplementary video.\n",
        "published": "2017",
        "authors": [
            "Aravind Rajeswaran",
            "Kendall Lowrey",
            "Emanuel Todorov",
            "Sham Kakade"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.02905v2",
        "title": "Learning a Unified Control Policy for Safe Falling",
        "abstract": "  Being able to fall safely is a necessary motor skill for humanoids performing\nhighly dynamic tasks, such as running and jumping. We propose a new method to\nlearn a policy that minimizes the maximal impulse during the fall. The\noptimization solves for both a discrete contact planning problem and a\ncontinuous optimal control problem. Once trained, the policy can compute the\noptimal next contacting body part (e.g. left foot, right foot, or hands),\ncontact location and timing, and the required joint actuation. We represent the\npolicy as a mixture of actor-critic neural network, which consists of n control\npolicies and the corresponding value functions. Each pair of actor-critic is\nassociated with one of the n possible contacting body parts. During execution,\nthe policy corresponding to the highest value function will be executed while\nthe associated body part will be the next contact with the ground. With this\nmixture of actor-critic architecture, the discrete contact sequence planning is\nsolved through the selection of the best critics while the continuous control\nproblem is solved by the optimization of actors. We show that our policy can\nachieve comparable, sometimes even higher, rewards than a recursive search of\nthe action space using dynamic programming, while enjoying 50 to 400 times of\nspeed gain during online execution.\n",
        "published": "2017",
        "authors": [
            "Visak CV Kumar",
            "Sehoon Ha",
            "C Karen Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.07822v1",
        "title": "Information-theoretic Model Identification and Policy Search using\n  Physics Engines with Application to Robotic Manipulation",
        "abstract": "  We consider the problem of a robot learning the mechanical properties of\nobjects through physical interaction with the object, and introduce a\npractical, data-efficient approach for identifying the motion models of these\nobjects. The proposed method utilizes a physics engine, where the robot seeks\nto identify the inertial and friction parameters of the object by simulating\nits motion under different values of the parameters and identifying those that\nresult in a simulation which matches the observed real motions. The problem is\nsolved in a Bayesian optimization framework. The same framework is used for\nboth identifying the model of an object online and searching for a policy that\nwould minimize a given cost function according to the identified model.\nExperimental results both in simulation and using a real robot indicate that\nthe proposed method outperforms state-of-the-art model-free reinforcement\nlearning approaches.\n",
        "published": "2017",
        "authors": [
            "Shaojun Zhu",
            "Andrew Kimmel",
            "Abdeslam Boularias"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1703.11000v2",
        "title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration",
        "abstract": "  Visual servoing involves choosing actions that move a robot in response to\nobservations from a camera, in order to reach a goal configuration in the\nworld. Standard visual servoing approaches typically rely on manually designed\nfeatures and analytical dynamics models, which limits their generalization\ncapability and often requires extensive application-specific feature and model\nengineering. In this work, we study how learned visual features, learned\npredictive dynamics models, and reinforcement learning can be combined to learn\nvisual servoing mechanisms. We focus on target following, with the goal of\ndesigning algorithms that can learn a visual servo using low amounts of data of\nthe target in question, to enable quick adaptation to new targets. Our approach\nis based on servoing the camera in the space of learned visual features, rather\nthan image pixels or manually-designed keypoints. We demonstrate that standard\ndeep features, in our case taken from a model trained for object\nclassification, can be used together with a bilinear predictive model to learn\nan effective visual servo that is robust to visual variation, changes in\nviewing angle and appearance, and occlusions. A key component of our approach\nis to use a sample-efficient fitted Q-iteration algorithm to learn which\nfeatures are best suited for the task at hand. We show that we can learn an\neffective visual servo on a complex synthetic car following benchmark using\njust 20 training trajectory samples for reinforcement learning. We demonstrate\nsubstantial improvement over a conventional approach based on image pixels or\nhand-designed keypoints, and we show an improvement in sample-efficiency of\nmore than two orders of magnitude over standard model-free deep reinforcement\nlearning algorithms. Videos are available at\nhttp://rll.berkeley.edu/visual_servoing .\n",
        "published": "2017",
        "authors": [
            "Alex X. Lee",
            "Sergey Levine",
            "Pieter Abbeel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.05394v1",
        "title": "Probabilistically Safe Policy Transfer",
        "abstract": "  Although learning-based methods have great potential for robotics, one\nconcern is that a robot that updates its parameters might cause large amounts\nof damage before it learns the optimal policy. We formalize the idea of safe\nlearning in a probabilistic sense by defining an optimization problem: we\ndesire to maximize the expected return while keeping the expected damage below\na given safety limit. We study this optimization for the case of a robot\nmanipulator with safety-based torque limits. We would like to ensure that the\ndamage constraint is maintained at every step of the optimization and not just\nat convergence. To achieve this aim, we introduce a novel method which predicts\nhow modifying the torque limit, as well as how updating the policy parameters,\nmight affect the robot's safety. We show through a number of experiments that\nour approach allows the robot to improve its performance while ensuring that\nthe expected damage constraint is not violated during the learning process.\n",
        "published": "2017",
        "authors": [
            "David Held",
            "Zoe McCarthy",
            "Michael Zhang",
            "Fred Shentu",
            "Pieter Abbeel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.06243v1",
        "title": "Learning to Represent Haptic Feedback for Partially-Observable Tasks",
        "abstract": "  The sense of touch, being the earliest sensory system to develop in a human\nbody [1], plays a critical part of our daily interaction with the environment.\nIn order to successfully complete a task, many manipulation interactions\nrequire incorporating haptic feedback. However, manually designing a feedback\nmechanism can be extremely challenging. In this work, we consider manipulation\ntasks that need to incorporate tactile sensor feedback in order to modify a\nprovided nominal plan. To incorporate partial observation, we present a new\nframework that models the task as a partially observable Markov decision\nprocess (POMDP) and learns an appropriate representation of haptic feedback\nwhich can serve as the state for a POMDP model. The model, that is parametrized\nby deep recurrent neural networks, utilizes variational Bayes methods to\noptimize the approximate posterior. Finally, we build on deep Q-learning to be\nable to select the optimal action in each state without access to a simulator.\nWe test our model on a PR2 robot for multiple tasks of turning a knob until it\nclicks.\n",
        "published": "2017",
        "authors": [
            "Jaeyong Sung",
            "J. Kenneth Salisbury",
            "Ashutosh Saxena"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.06366v5",
        "title": "Automatic Goal Generation for Reinforcement Learning Agents",
        "abstract": "  Reinforcement learning is a powerful technique to train an agent to perform a\ntask. However, an agent that is trained using reinforcement learning is only\ncapable of achieving the single task that is specified via its reward function.\nSuch an approach does not scale well to settings in which an agent needs to\nperform a diverse set of tasks, such as navigating to varying positions in a\nroom or moving objects to varying locations. Instead, we propose a method that\nallows an agent to automatically discover the range of tasks that it is capable\nof performing. We use a generator network to propose tasks for the agent to try\nto achieve, specified as goal states. The generator network is optimized using\nadversarial training to produce tasks that are always at the appropriate level\nof difficulty for the agent. Our method thus automatically produces a\ncurriculum of tasks for the agent to learn. We show that, by using this\nframework, an agent can efficiently and automatically learn to perform a wide\nset of tasks without requiring any prior knowledge of its environment. Our\nmethod can also learn to achieve tasks with sparse rewards, which traditionally\npose significant challenges.\n",
        "published": "2017",
        "authors": [
            "Carlos Florensa",
            "David Held",
            "Xinyang Geng",
            "Pieter Abbeel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1705.10422v2",
        "title": "Learning End-to-end Multimodal Sensor Policies for Autonomous Navigation",
        "abstract": "  Multisensory polices are known to enhance both state estimation and target\ntracking. However, in the space of end-to-end sensorimotor control, this\nmulti-sensor outlook has received limited attention. Moreover, systematic ways\nto make policies robust to partial sensor failure are not well explored. In\nthis work, we propose a specific customization of Dropout, called\n\\textit{Sensor Dropout}, to improve multisensory policy robustness and handle\npartial failure in the sensor-set. We also introduce an additional auxiliary\nloss on the policy network in order to reduce variance in the band of potential\nmulti- and uni-sensory policies to reduce jerks during policy switching\ntriggered by an abrupt sensor failure or deactivation/activation. Finally,\nthrough the visualization of gradients, we show that the learned policies are\nconditioned on the same latent states representation despite having diverse\nobservations spaces - a hallmark of true sensor-fusion. Simulation results of\nthe multisensory policy, as visualized in TORCS racing game, can be seen here:\nhttps://youtu.be/QAK2lcXjNZc.\n",
        "published": "2017",
        "authors": [
            "Guan-Horng Liu",
            "Avinash Siravuru",
            "Sai Prabhakar",
            "Manuela Veloso",
            "George Kantor"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1706.00387v1",
        "title": "Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient\n  Estimation for Deep Reinforcement Learning",
        "abstract": "  Off-policy model-free deep reinforcement learning methods using previously\ncollected data can improve sample efficiency over on-policy policy gradient\ntechniques. On the other hand, on-policy algorithms are often more stable and\neasier to use. This paper examines, both theoretically and empirically,\napproaches to merging on- and off-policy updates for deep reinforcement\nlearning. Theoretical results show that off-policy updates with a value\nfunction estimator can be interpolated with on-policy policy gradient updates\nwhilst still satisfying performance bounds. Our analysis uses control variate\nmethods to produce a family of policy gradient algorithms, with several\nrecently proposed algorithms being special cases of this family. We then\nprovide an empirical comparison of these techniques with the remaining\nalgorithmic details fixed, and show how different mixing of off-policy gradient\nestimates with on-policy samples contribute to improvements in empirical\nperformance. The final algorithm provides a generalization and unification of\nexisting deep policy gradient techniques, has theoretical guarantees on the\nbias introduced by off-policy updates, and improves on the state-of-the-art\nmodel-free deep RL methods on a number of OpenAI Gym continuous control\nbenchmarks.\n",
        "published": "2017",
        "authors": [
            "Shixiang Gu",
            "Timothy Lillicrap",
            "Zoubin Ghahramani",
            "Richard E. Turner",
            "Bernhard Sch\u00f6lkopf",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1706.02423v1",
        "title": "Seamless Integration and Coordination of Cognitive Skills in Humanoid\n  Robots: A Deep Learning Approach",
        "abstract": "  This study investigates how adequate coordination among the different\ncognitive processes of a humanoid robot can be developed through end-to-end\nlearning of direct perception of visuomotor stream. We propose a deep dynamic\nneural network model built on a dynamic vision network, a motor generation\nnetwork, and a higher-level network. The proposed model was designed to process\nand to integrate direct perception of dynamic visuomotor patterns in a\nhierarchical model characterized by different spatial and temporal constraints\nimposed on each level. We conducted synthetic robotic experiments in which a\nrobot learned to read human's intention through observing the gestures and then\nto generate the corresponding goal-directed actions. Results verify that the\nproposed model is able to learn the tutored skills and to generalize them to\nnovel situations. The model showed synergic coordination of perception, action\nand decision making, and it integrated and coordinated a set of cognitive\nskills including visual perception, intention reading, attention switching,\nworking memory, action preparation and execution in a seamless manner. Analysis\nreveals that coherent internal representations emerged at each level of the\nhierarchy. Higher-level representation reflecting actional intention developed\nby means of continuous integration of the lower-level visuo-proprioceptive\nstream.\n",
        "published": "2017",
        "authors": [
            "Jungsik Hwang",
            "Jun Tani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1706.02444v1",
        "title": "Predictive Coding-based Deep Dynamic Neural Network for Visuomotor\n  Learning",
        "abstract": "  This study presents a dynamic neural network model based on the predictive\ncoding framework for perceiving and predicting the dynamic visuo-proprioceptive\npatterns. In our previous study [1], we have shown that the deep dynamic neural\nnetwork model was able to coordinate visual perception and action generation in\na seamless manner. In the current study, we extended the previous model under\nthe predictive coding framework to endow the model with a capability of\nperceiving and predicting dynamic visuo-proprioceptive patterns as well as a\ncapability of inferring intention behind the perceived visuomotor information\nthrough minimizing prediction error. A set of synthetic experiments were\nconducted in which a robot learned to imitate the gestures of another robot in\na simulation environment. The experimental results showed that with given\nintention states, the model was able to mentally simulate the possible incoming\ndynamic visuo-proprioceptive patterns in a top-down process without the inputs\nfrom the external environment. Moreover, the results highlighted the role of\nminimizing prediction error in inferring underlying intention of the perceived\nvisuo-proprioceptive patterns, supporting the predictive coding account of the\nmirror neuron systems. The results also revealed that minimizing prediction\nerror in one modality induced the recall of the corresponding representation of\nanother modality acquired during the consolidative learning of raw-level\nvisuo-proprioceptive patterns.\n",
        "published": "2017",
        "authors": [
            "Jungsik Hwang",
            "Jinhyung Kim",
            "Ahmadreza Ahmadi",
            "Minkyu Choi",
            "Jun Tani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1706.09520v7",
        "title": "Neural SLAM: Learning to Explore with External Memory",
        "abstract": "  We present an approach for agents to learn representations of a global map\nfrom sensor data, to aid their exploration in new environments. To achieve\nthis, we embed procedures mimicking that of traditional Simultaneous\nLocalization and Mapping (SLAM) into the soft attention based addressing of\nexternal memory architectures, in which the external memory acts as an internal\nrepresentation of the environment. This structure encourages the evolution of\nSLAM-like behaviors inside a completely differentiable deep neural network. We\nshow that this approach can help reinforcement learning agents to successfully\nexplore new environments where long-term memory is essential. We validate our\napproach in both challenging grid-world environments and preliminary Gazebo\nexperiments. A video of our experiments can be found at: https://goo.gl/G2Vu5y.\n",
        "published": "2017",
        "authors": [
            "Jingwei Zhang",
            "Lei Tai",
            "Ming Liu",
            "Joschka Boedecker",
            "Wolfram Burgard"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.02920v2",
        "title": "Vision-Based Multi-Task Manipulation for Inexpensive Robots Using\n  End-To-End Learning from Demonstration",
        "abstract": "  We propose a technique for multi-task learning from demonstration that trains\nthe controller of a low-cost robotic arm to accomplish several complex picking\nand placing tasks, as well as non-prehensile manipulation. The controller is a\nrecurrent neural network using raw images as input and generating robot arm\ntrajectories, with the parameters shared across the tasks. The controller also\ncombines VAE-GAN-based reconstruction with autoregressive multimodal action\nprediction. Our results demonstrate that it is possible to learn complex\nmanipulation tasks, such as picking up a towel, wiping an object, and\ndepositing the towel to its previous position, entirely from raw images with\ndirect behavior cloning. We show that weight sharing and reconstruction-based\nregularization substantially improve generalization and robustness, and\ntraining on multiple tasks simultaneously increases the success rate on all\ntasks.\n",
        "published": "2017",
        "authors": [
            "Rouhollah Rahmatizadeh",
            "Pooya Abolghasemi",
            "Ladislau B\u00f6l\u00f6ni",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.03034v1",
        "title": "Learning Heuristic Search via Imitation",
        "abstract": "  Robotic motion planning problems are typically solved by constructing a\nsearch tree of valid maneuvers from a start to a goal configuration. Limited\nonboard computation and real-time planning constraints impose a limit on how\nlarge this search tree can grow. Heuristics play a crucial role in such\nsituations by guiding the search towards potentially good directions and\nconsequently minimizing search effort. Moreover, it must infer such directions\nin an efficient manner using only the information uncovered by the search up\nuntil that time. However, state of the art methods do not address the problem\nof computing a heuristic that explicitly minimizes search effort. In this\npaper, we do so by training a heuristic policy that maps the partial\ninformation from the search to decide which node of the search tree to expand.\nUnfortunately, naively training such policies leads to slow convergence and\npoor local minima. We present SaIL, an efficient algorithm that trains\nheuristic policies by imitating \"clairvoyant oracles\" - oracles that have full\ninformation about the world and demonstrate decisions that minimize search\neffort. We leverage the fact that such oracles can be efficiently computed\nusing dynamic programming and derive performance guarantees for the learnt\nheuristic. We validate the approach on a spectrum of environments which show\nthat SaIL consistently outperforms state of the art algorithms. Our approach\npaves the way forward for learning heuristics that demonstrate an anytime\nnature - finding feasible solutions quickly and incrementally refining it over\ntime.\n",
        "published": "2017",
        "authors": [
            "Mohak Bhardwaj",
            "Sanjiban Choudhury",
            "Sebastian Scherer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1707.06354v2",
        "title": "Pragmatic-Pedagogic Value Alignment",
        "abstract": "  As intelligent systems gain autonomy and capability, it becomes vital to\nensure that their objectives match those of their human users; this is known as\nthe value-alignment problem. In robotics, value alignment is key to the design\nof collaborative robots that can integrate into human workflows, successfully\ninferring and adapting to their users' objectives as they go. We argue that a\nmeaningful solution to value alignment must combine multi-agent decision theory\nwith rich mathematical models of human cognition, enabling robots to tap into\npeople's natural collaborative capabilities. We present a solution to the\ncooperative inverse reinforcement learning (CIRL) dynamic game based on\nwell-established cognitive models of decision making and theory of mind. The\nsolution captures a key reciprocity relation: the human will not plan her\nactions in isolation, but rather reason pedagogically about how the robot might\nlearn from them; the robot, in turn, can anticipate this and interpret the\nhuman's actions pragmatically. To our knowledge, this work constitutes the\nfirst formal analysis of value alignment grounded in empirically validated\ncognitive models.\n",
        "published": "2017",
        "authors": [
            "Jaime F. Fisac",
            "Monica A. Gates",
            "Jessica B. Hamrick",
            "Chang Liu",
            "Dylan Hadfield-Menell",
            "Malayandi Palaniappan",
            "Dhruv Malik",
            "S. Shankar Sastry",
            "Thomas L. Griffiths",
            "Anca D. Dragan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.01391v1",
        "title": "Guiding the search in continuous state-action spaces by learning an\n  action sampling distribution from off-target samples",
        "abstract": "  In robotics, it is essential to be able to plan efficiently in\nhigh-dimensional continuous state-action spaces for long horizons. For such\ncomplex planning problems, unguided uniform sampling of actions until a path to\na goal is found is hopelessly inefficient, and gradient-based approaches often\nfall short when the optimization manifold of a given problem is not smooth. In\nthis paper we present an approach that guides the search of a state-space\nplanner, such as A*, by learning an action-sampling distribution that can\ngeneralize across different instances of a planning problem. The motivation is\nthat, unlike typical learning approaches for planning for continuous action\nspace that estimate a policy, an estimated action sampler is more robust to\nerror since it has a planner to fall back on. We use a Generative Adversarial\nNetwork (GAN), and address an important issue: search experience consists of a\nrelatively large number of actions that are not on a solution path and a\nrelatively small number of actions that actually are on a solution path. We\nintroduce a new technique, based on an importance-ratio estimation method, for\nusing samples from a non-target distribution to make GAN learning more\ndata-efficient. We provide theoretical guarantees and empirical evaluation in\nthree challenging continuous robot planning problems to illustrate the\neffectiveness of our algorithm.\n",
        "published": "2017",
        "authors": [
            "Beomjoon Kim",
            "Leslie Pack Kaelbling",
            "Tomas Lozano-Perez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.01703v1",
        "title": "RoboCupSimData: A RoboCup soccer research dataset",
        "abstract": "  RoboCup is an international scientific robot competition in which teams of\nmultiple robots compete against each other. Its different leagues provide many\nsources of robotics data, that can be used for further analysis and application\nof machine learning. This paper describes a large dataset from games of some of\nthe top teams (from 2016 and 2017) in RoboCup Soccer Simulation League (2D),\nwhere teams of 11 robots (agents) compete against each other. Overall, we used\n10 different teams to play each other, resulting in 45 unique pairings. For\neach pairing, we ran 25 matches (of 10mins), leading to 1125 matches or more\nthan 180 hours of game play. The generated CSV files are 17GB of data (zipped),\nor 229GB (unzipped). The dataset is unique in the sense that it contains both\nthe ground truth data (global, complete, noise-free information of all objects\non the field), as well as the noisy, local and incomplete percepts of each\nrobot. These data are made available as CSV files, as well as in the original\nsoccer simulator formats.\n",
        "published": "2017",
        "authors": [
            "Olivia Michael",
            "Oliver Obst",
            "Falk Schmidsberger",
            "Frieder Stolzenburg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.10055v2",
        "title": "Risk-sensitive Inverse Reinforcement Learning via Semi- and\n  Non-Parametric Methods",
        "abstract": "  The literature on Inverse Reinforcement Learning (IRL) typically assumes that\nhumans take actions in order to minimize the expected value of a cost function,\ni.e., that humans are risk neutral. Yet, in practice, humans are often far from\nbeing risk neutral. To fill this gap, the objective of this paper is to devise\na framework for risk-sensitive IRL in order to explicitly account for a human's\nrisk sensitivity. To this end, we propose a flexible class of models based on\ncoherent risk measures, which allow us to capture an entire spectrum of risk\npreferences from risk-neutral to worst-case. We propose efficient\nnon-parametric algorithms based on linear programming and semi-parametric\nalgorithms based on maximum likelihood for inferring a human's underlying risk\nmeasure and cost function for a rich class of static and dynamic\ndecision-making settings. The resulting approach is demonstrated on a simulated\ndriving game with ten human participants. Our method is able to infer and mimic\na wide range of qualitatively different driving styles from highly risk-averse\nto risk-neutral in a data-efficient manner. Moreover, comparisons of the\nRisk-Sensitive (RS) IRL approach with a risk-neutral model show that the RS-IRL\nframework more accurately captures observed participant behavior both\nqualitatively and quantitatively, especially in scenarios where catastrophic\noutcomes such as collisions can occur.\n",
        "published": "2017",
        "authors": [
            "Sumeet Singh",
            "Jonathan Lacotte",
            "Anirudha Majumdar",
            "Marco Pavone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1711.10137v2",
        "title": "One-Shot Reinforcement Learning for Robot Navigation with Interactive\n  Replay",
        "abstract": "  Recently, model-free reinforcement learning algorithms have been shown to\nsolve challenging problems by learning from extensive interaction with the\nenvironment. A significant issue with transferring this success to the robotics\ndomain is that interaction with the real world is costly, but training on\nlimited experience is prone to overfitting. We present a method for learning to\nnavigate, to a fixed goal and in a known environment, on a mobile robot. The\nrobot leverages an interactive world model built from a single traversal of the\nenvironment, a pre-trained visual feature encoder, and stochastic environmental\naugmentation, to demonstrate successful zero-shot transfer under real-world\nenvironmental variations without fine-tuning.\n",
        "published": "2017",
        "authors": [
            "Jake Bruce",
            "Niko Suenderhauf",
            "Piotr Mirowski",
            "Raia Hadsell",
            "Michael Milford"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1801.08214v1",
        "title": "Active Neural Localization",
        "abstract": "  Localization is the problem of estimating the location of an autonomous agent\nfrom an observation and a map of the environment. Traditional methods of\nlocalization, which filter the belief based on the observations, are\nsub-optimal in the number of steps required, as they do not decide the actions\ntaken by the agent. We propose \"Active Neural Localizer\", a fully\ndifferentiable neural network that learns to localize accurately and\nefficiently. The proposed model incorporates ideas of traditional\nfiltering-based localization methods, by using a structured belief of the state\nwith multiplicative interactions to propagate belief, and combines it with a\npolicy model to localize accurately while minimizing the number of steps\nrequired for localization. Active Neural Localizer is trained end-to-end with\nreinforcement learning. We use a variety of simulation environments for our\nexperiments which include random 2D mazes, random mazes in the Doom game engine\nand a photo-realistic environment in the Unreal game engine. The results on the\n2D environments show the effectiveness of the learned policy in an idealistic\nsetting while results on the 3D environments demonstrate the model's capability\nof learning the policy and perceptual model jointly from raw-pixel based RGB\nobservations. We also show that a model trained on random textures in the Doom\nenvironment generalizes well to a photo-realistic office space environment in\nthe Unreal engine.\n",
        "published": "2018",
        "authors": [
            "Devendra Singh Chaplot",
            "Emilio Parisotto",
            "Ruslan Salakhutdinov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.08311v1",
        "title": "Structured Control Nets for Deep Reinforcement Learning",
        "abstract": "  In recent years, Deep Reinforcement Learning has made impressive advances in\nsolving several important benchmark problems for sequential decision making.\nMany control applications use a generic multilayer perceptron (MLP) for\nnon-vision parts of the policy network. In this work, we propose a new neural\nnetwork architecture for the policy network representation that is simple yet\neffective. The proposed Structured Control Net (SCN) splits the generic MLP\ninto two separate sub-modules: a nonlinear control module and a linear control\nmodule. Intuitively, the nonlinear control is for forward-looking and global\ncontrol, while the linear control stabilizes the local dynamics around the\nresidual of global control. We hypothesize that this will bring together the\nbenefits of both linear and nonlinear policies: improve training sample\nefficiency, final episodic reward, and generalization of learned policy, while\nrequiring a smaller network and being generally applicable to different\ntraining methods. We validated our hypothesis with competitive results on\nsimulations from OpenAI MuJoCo, Roboschool, Atari, and a custom 2D urban\ndriving environment, with various ablation and generalization tests, trained\nwith multiple black-box and policy gradient training methods. The proposed\narchitecture has the potential to improve upon broader control tasks by\nincorporating problem specific priors into the architecture. As a case study,\nwe demonstrate much improved performance for locomotion tasks by emulating the\nbiological central pattern generators (CPGs) as the nonlinear part of the\narchitecture.\n",
        "published": "2018",
        "authors": [
            "Mario Srouji",
            "Jian Zhang",
            "Ruslan Salakhutdinov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.09464v2",
        "title": "Multi-Goal Reinforcement Learning: Challenging Robotics Environments and\n  Request for Research",
        "abstract": "  The purpose of this technical report is two-fold. First of all, it introduces\na suite of challenging continuous control tasks (integrated with OpenAI Gym)\nbased on currently existing robotics hardware. The tasks include pushing,\nsliding and pick & place with a Fetch robotic arm as well as in-hand object\nmanipulation with a Shadow Dexterous Hand. All tasks have sparse binary rewards\nand follow a Multi-Goal Reinforcement Learning (RL) framework in which an agent\nis told what to do using an additional input.\n  The second part of the paper presents a set of concrete research ideas for\nimproving RL algorithms, most of which are related to Multi-Goal RL and\nHindsight Experience Replay.\n",
        "published": "2018",
        "authors": [
            "Matthias Plappert",
            "Marcin Andrychowicz",
            "Alex Ray",
            "Bob McGrew",
            "Bowen Baker",
            "Glenn Powell",
            "Jonas Schneider",
            "Josh Tobin",
            "Maciek Chociej",
            "Peter Welinder",
            "Vikash Kumar",
            "Wojciech Zaremba"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.09564v2",
        "title": "Reinforcement and Imitation Learning for Diverse Visuomotor Skills",
        "abstract": "  We propose a model-free deep reinforcement learning method that leverages a\nsmall amount of demonstration data to assist a reinforcement learning agent. We\napply this approach to robotic manipulation tasks and train end-to-end\nvisuomotor policies that map directly from RGB camera inputs to joint\nvelocities. We demonstrate that our approach can solve a wide variety of\nvisuomotor tasks, for which engineering a scripted controller would be\nlaborious. In experiments, our reinforcement and imitation agent achieves\nsignificantly better performances than agents trained with reinforcement\nlearning or imitation learning alone. We also illustrate that these policies,\ntrained with large visual and dynamics variations, can achieve preliminary\nsuccesses in zero-shot sim2real transfer. A brief visual description of this\nwork can be viewed in https://youtu.be/EDl8SQUNjj0\n",
        "published": "2018",
        "authors": [
            "Yuke Zhu",
            "Ziyu Wang",
            "Josh Merel",
            "Andrei Rusu",
            "Tom Erez",
            "Serkan Cabi",
            "Saran Tunyasuvunakool",
            "J\u00e1nos Kram\u00e1r",
            "Raia Hadsell",
            "Nando de Freitas",
            "Nicolas Heess"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1802.10592v2",
        "title": "Model-Ensemble Trust-Region Policy Optimization",
        "abstract": "  Model-free reinforcement learning (RL) methods are succeeding in a growing\nnumber of tasks, aided by recent advances in deep learning. However, they tend\nto suffer from high sample complexity, which hinders their use in real-world\ndomains. Alternatively, model-based reinforcement learning promises to reduce\nsample complexity, but tends to require careful tuning and to date have\nsucceeded mainly in restrictive domains where simple models are sufficient for\nlearning. In this paper, we analyze the behavior of vanilla model-based\nreinforcement learning methods when deep neural networks are used to learn both\nthe model and the policy, and show that the learned policy tends to exploit\nregions where insufficient data is available for the model to be learned,\ncausing instability in training. To overcome this issue, we propose to use an\nensemble of models to maintain the model uncertainty and regularize the\nlearning process. We further show that the use of likelihood ratio derivatives\nyields much more stable learning than backpropagation through time. Altogether,\nour approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO)\nsignificantly reduces the sample complexity compared to model-free deep RL\nmethods on challenging continuous control benchmark tasks.\n",
        "published": "2018",
        "authors": [
            "Thanard Kurutach",
            "Ignasi Clavera",
            "Yan Duan",
            "Aviv Tamar",
            "Pieter Abbeel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.05752v1",
        "title": "Rearrangement with Nonprehensile Manipulation Using Deep Reinforcement\n  Learning",
        "abstract": "  Rearranging objects on a tabletop surface by means of nonprehensile\nmanipulation is a task which requires skillful interaction with the physical\nworld. Usually, this is achieved by precisely modeling physical properties of\nthe objects, robot, and the environment for explicit planning. In contrast, as\nexplicitly modeling the physical environment is not always feasible and\ninvolves various uncertainties, we learn a nonprehensile rearrangement strategy\nwith deep reinforcement learning based on only visual feedback. For this, we\nmodel the task with rewards and train a deep Q-network. Our potential\nfield-based heuristic exploration strategy reduces the amount of collisions\nwhich lead to suboptimal outcomes and we actively balance the training set to\navoid bias towards poor examples. Our training process leads to quicker\nlearning and better performance on the task as compared to uniform exploration\nand standard experience replay. We demonstrate empirical evidence from\nsimulation that our method leads to a success rate of 85%, show that our system\ncan cope with sudden changes of the environment, and compare our performance\nwith human level performance.\n",
        "published": "2018",
        "authors": [
            "Weihao Yuan",
            "Johannes A. Stork",
            "Danica Kragic",
            "Michael Y. Wang",
            "Kaiyu Hang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.08287v3",
        "title": "Learning-based Model Predictive Control for Safe Exploration",
        "abstract": "  Learning-based methods have been successful in solving complex control tasks\nwithout significant prior knowledge about the system. However, these methods\ntypically do not provide any safety guarantees, which prevents their use in\nsafety-critical, real-world applications. In this paper, we present a\nlearning-based model predictive control scheme that can provide provable\nhigh-probability safety guarantees. To this end, we exploit regularity\nassumptions on the dynamics in terms of a Gaussian process prior to construct\nprovably accurate confidence intervals on predicted trajectories. Unlike\nprevious approaches, we do not assume that model uncertainties are independent.\nBased on these predictions, we guarantee that trajectories satisfy safety\nconstraints. Moreover, we use a terminal set constraint to recursively\nguarantee the existence of safe control actions at every iteration. In our\nexperiments, we show that the resulting algorithm can be used to safely and\nefficiently explore and learn about dynamic systems.\n",
        "published": "2018",
        "authors": [
            "Torsten Koller",
            "Felix Berkenkamp",
            "Matteo Turchetta",
            "Andreas Krause"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1803.10056v2",
        "title": "Automated Speed and Lane Change Decision Making using Deep Reinforcement\n  Learning",
        "abstract": "  This paper introduces a method, based on deep reinforcement learning, for\nautomatically generating a general purpose decision making function. A Deep\nQ-Network agent was trained in a simulated environment to handle speed and lane\nchange decisions for a truck-trailer combination. In a highway driving case, it\nis shown that the method produced an agent that matched or surpassed the\nperformance of a commonly used reference model. To demonstrate the generality\nof the method, the exact same algorithm was also tested by training it for an\novertaking case on a road with oncoming traffic. Furthermore, a novel way of\napplying a convolutional neural network to high level input that represents\ninterchangeable objects is also introduced.\n",
        "published": "2018",
        "authors": [
            "Carl-Johan Hoel",
            "Krister Wolff",
            "Leo Laine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.02501v1",
        "title": "Simplifying Reward Design through Divide-and-Conquer",
        "abstract": "  Designing a good reward function is essential to robot planning and\nreinforcement learning, but it can also be challenging and frustrating. The\nreward needs to work across multiple different environments, and that often\nrequires many iterations of tuning. We introduce a novel divide-and-conquer\napproach that enables the designer to specify a reward separately for each\nenvironment. By treating these separate reward functions as observations about\nthe underlying true reward, we derive an approach to infer a common reward\nacross all environments. We conduct user studies in an abstract grid world\ndomain and in a motion planning domain for a 7-DOF manipulator that measure\nuser effort and solution quality. We show that our method is faster, easier to\nuse, and produces a higher quality solution than the typical method of\ndesigning a reward jointly across all environments. We additionally conduct a\nseries of experiments that measure the sensitivity of these results to\ndifferent properties of the reward design task, such as the number of\nenvironments, the number of feasible solutions per environment, and the\nfraction of the total features that vary within each environment. We find that\nindependent reward design outperforms the standard, joint, reward design\nprocess but works best when the design problem can be divided into simpler\nsubproblems.\n",
        "published": "2018",
        "authors": [
            "Ellis Ratner",
            "Dylan Hadfield-Menell",
            "Anca D. Dragan"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.02739v2",
        "title": "Discovering space - Grounding spatial topology and metric regularity in\n  a naive agent's sensorimotor experience",
        "abstract": "  In line with the sensorimotor contingency theory, we investigate the problem\nof the perception of space from a fundamental sensorimotor perspective. Despite\nits pervasive nature in our perception of the world, the origin of the concept\nof space remains largely mysterious. For example in the context of artificial\nperception, this issue is usually circumvented by having engineers pre-define\nthe spatial structure of the problem the agent has to face. We here show that\nthe structure of space can be autonomously discovered by a naive agent in the\nform of sensorimotor regularities, that correspond to so called compensable\nsensory experiences: these are experiences that can be generated either by the\nagent or its environment. By detecting such compensable experiences the agent\ncan infer the topological and metric structure of the external space in which\nits body is moving. We propose a theoretical description of the nature of these\nregularities and illustrate the approach on a simulated robotic arm equipped\nwith an eye-like sensor, and which interacts with an object. Finally we show\nhow these regularities can be used to build an internal representation of the\nsensor's external spatial configuration.\n",
        "published": "2018",
        "authors": [
            "Alban Laflaqui\u00e8re",
            "J. Kevin O'Regan",
            "Bruno Gas",
            "Alexander Terekhov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1806.07851v2",
        "title": "Sim-to-Real Reinforcement Learning for Deformable Object Manipulation",
        "abstract": "  We have seen much recent progress in rigid object manipulation, but\ninteraction with deformable objects has notably lagged behind. Due to the large\nconfiguration space of deformable objects, solutions using traditional\nmodelling approaches require significant engineering work. Perhaps then,\nbypassing the need for explicit modelling and instead learning the control in\nan end-to-end manner serves as a better approach? Despite the growing interest\nin the use of end-to-end robot learning approaches, only a small amount of work\nhas focused on their applicability to deformable object manipulation. Moreover,\ndue to the large amount of data needed to learn these end-to-end solutions, an\nemerging trend is to learn control policies in simulation and then transfer\nthem over to the real world. To-date, no work has explored whether it is\npossible to learn and transfer deformable object policies. We believe that if\nsim-to-real methods are to be employed further, then it should be possible to\nlearn to interact with a wide variety of objects, and not only rigid objects.\nIn this work, we use a combination of state-of-the-art deep reinforcement\nlearning algorithms to solve the problem of manipulating deformable objects\n(specifically cloth). We evaluate our approach on three tasks --- folding a\ntowel up to a mark, folding a face towel diagonally, and draping a piece of\ncloth over a hanger. Our agents are fully trained in simulation with domain\nrandomisation, and then successfully deployed in the real world without having\nseen any real deformable objects.\n",
        "published": "2018",
        "authors": [
            "Jan Matas",
            "Stephen James",
            "Andrew J. Davison"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.05924v2",
        "title": "Bipedal Walking Robot using Deep Deterministic Policy Gradient",
        "abstract": "  Machine learning algorithms have found several applications in the field of\nrobotics and control systems. The control systems community has started to show\ninterest towards several machine learning algorithms from the sub-domains such\nas supervised learning, imitation learning and reinforcement learning to\nachieve autonomous control and intelligent decision making. Amongst many\ncomplex control problems, stable bipedal walking has been the most challenging\nproblem. In this paper, we present an architecture to design and simulate a\nplanar bipedal walking robot(BWR) using a realistic robotics simulator, Gazebo.\nThe robot demonstrates successful walking behaviour by learning through several\nof its trial and errors, without any prior knowledge of itself or the world\ndynamics. The autonomous walking of the BWR is achieved using reinforcement\nlearning algorithm called Deep Deterministic Policy Gradient(DDPG). DDPG is one\nof the algorithms for learning controls in continuous action spaces. After\ntraining the model in simulation, it was observed that, with a proper shaped\nreward function, the robot achieved faster walking or even rendered a running\ngait with an average speed of 0.83 m/s. The gait pattern of the bipedal walker\nwas compared with the actual human walking pattern. The results show that the\nbipedal walking pattern had similar characteristics to that of a human walking\npattern. The video presenting our experiment is available at\nhttps://goo.gl/NHXKqR.\n",
        "published": "2018",
        "authors": [
            "Arun Kumar",
            "Navneet Paul",
            "S N Omkar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.06696v1",
        "title": "Integrating Algorithmic Planning and Deep Learning for Partially\n  Observable Navigation",
        "abstract": "  We propose to take a novel approach to robot system design where each\nbuilding block of a larger system is represented as a differentiable program,\ni.e. a deep neural network. This representation allows for integrating\nalgorithmic planning and deep learning in a principled manner, and thus combine\nthe benefits of model-free and model-based methods. We apply the proposed\napproach to a challenging partially observable robot navigation task. The robot\nmust navigate to a goal in a previously unseen 3-D environment without knowing\nits initial location, and instead relying on a 2-D floor map and visual\nobservations from an onboard camera. We introduce the Navigation Networks\n(NavNets) that encode state estimation, planning and acting in a single,\nend-to-end trainable recurrent neural network. In preliminary simulation\nexperiments we successfully trained navigation networks to solve the\nchallenging partially observable navigation task.\n",
        "published": "2018",
        "authors": [
            "Peter Karkus",
            "David Hsu",
            "Wee Sun Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.08048v1",
        "title": "Baidu Apollo EM Motion Planner",
        "abstract": "  In this manuscript, we introduce a real-time motion planning system based on\nthe Baidu Apollo (open source) autonomous driving platform. The developed\nsystem aims to address the industrial level-4 motion planning problem while\nconsidering safety, comfort and scalability. The system covers multilane and\nsingle-lane autonomous driving in a hierarchical manner: (1) The top layer of\nthe system is a multilane strategy that handles lane-change scenarios by\ncomparing lane-level trajectories computed in parallel. (2) Inside the\nlane-level trajectory generator, it iteratively solves path and speed\noptimization based on a Frenet frame. (3) For path and speed optimization, a\ncombination of dynamic programming and spline-based quadratic programming is\nproposed to construct a scalable and easy-to-tune framework to handle traffic\nrules, obstacle decisions and smoothness simultaneously. The planner is\nscalable to both highway and lower-speed city driving scenarios. We also\ndemonstrate the algorithm through scenario illustrations and on-road test\nresults.\n  The system described in this manuscript has been deployed to dozens of Baidu\nApollo autonomous driving vehicles since Apollo v1.5 was announced in September\n2017. As of May 16th, 2018, the system has been tested under 3,380 hours and\napproximately 68,000 kilometers (42,253 miles) of closed-loop autonomous\ndriving under various urban scenarios.\n  The algorithm described in this manuscript is available at\nhttps://github.com/ApolloAuto/apollo/tree/master/modules/planning.\n",
        "published": "2018",
        "authors": [
            "Haoyang Fan",
            "Fan Zhu",
            "Changchun Liu",
            "Liangliang Zhang",
            "Li Zhuang",
            "Dong Li",
            "Weicheng Zhu",
            "Jiangtao Hu",
            "Hongye Li",
            "Qi Kong"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1807.09962v1",
        "title": "Learning to guide task and motion planning using score-space\n  representation",
        "abstract": "  In this paper, we propose a learning algorithm that speeds up the search in\ntask and motion planning problems. Our algorithm proposes solutions to three\ndifferent challenges that arise in learning to improve planning efficiency:\nwhat to predict, how to represent a planning problem instance, and how to\ntransfer knowledge from one problem instance to another. We propose a method\nthat predicts constraints on the search space based on a generic representation\nof a planning problem instance, called score-space, where we represent a\nproblem instance in terms of the performance of a set of solutions attempted so\nfar. Using this representation, we transfer knowledge, in the form of\nconstraints, from previous problems based on the similarity in score space. We\ndesign a sequential algorithm that efficiently predicts these constraints, and\nevaluate it in three different challenging task and motion planning problems.\nResults indicate that our approach performs orders of magnitudes faster than an\nunguided planner\n",
        "published": "2018",
        "authors": [
            "Beomjoon Kim",
            "Zi Wang",
            "Leslie Pack Kaelbling",
            "Tomas Lozano-Perez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.01700v1",
        "title": "Combining Subgoal Graphs with Reinforcement Learning to Build a Rational\n  Pathfinder",
        "abstract": "  In this paper, we present a hierarchical path planning framework called SG-RL\n(subgoal graphs-reinforcement learning), to plan rational paths for agents\nmaneuvering in continuous and uncertain environments. By \"rational\", we mean\n(1) efficient path planning to eliminate first-move lags; (2) collision-free\nand smooth for agents with kinematic constraints satisfied. SG-RL works in a\ntwo-level manner. At the first level, SG-RL uses a geometric path-planning\nmethod, i.e., Simple Subgoal Graphs (SSG), to efficiently find optimal abstract\npaths, also called subgoal sequences. At the second level, SG-RL uses an RL\nmethod, i.e., Least-Squares Policy Iteration (LSPI), to learn near-optimal\nmotion-planning policies which can generate kinematically feasible and\ncollision-free trajectories between adjacent subgoals. The first advantage of\nthe proposed method is that SSG can solve the limitations of sparse reward and\nlocal minima trap for RL agents; thus, LSPI can be used to generate paths in\ncomplex environments. The second advantage is that, when the environment\nchanges slightly (i.e., unexpected obstacles appearing), SG-RL does not need to\nreconstruct subgoal graphs and replan subgoal sequences using SSG, since LSPI\ncan deal with uncertainties by exploiting its generalization ability to handle\nchanges in environments. Simulation experiments in representative scenarios\ndemonstrate that, compared with existing methods, SG-RL can work well on\nlarge-scale maps with relatively low action-switching frequencies and shorter\npath lengths, and SG-RL can deal with small changes in environments. We further\ndemonstrate that the design of reward functions and the types of training\nenvironments are important factors for learning feasible policies.\n",
        "published": "2018",
        "authors": [
            "Junjie Zeng",
            "Long Qin",
            "Yue Hu",
            "Cong Hu",
            "Quanjun Yin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.02790v1",
        "title": "RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through\n  Imitation",
        "abstract": "  Imitation Learning has empowered recent advances in learning robotic\nmanipulation tasks by addressing shortcomings of Reinforcement Learning such as\nexploration and reward specification. However, research in this area has been\nlimited to modest-sized datasets due to the difficulty of collecting large\nquantities of task demonstrations through existing mechanisms. This work\nintroduces RoboTurk to address this challenge. RoboTurk is a crowdsourcing\nplatform for high quality 6-DoF trajectory based teleoperation through the use\nof widely available mobile devices (e.g. iPhone). We evaluate RoboTurk on three\nmanipulation tasks of varying timescales (15-120s) and observe that our user\ninterface is statistically similar to special purpose hardware such as virtual\nreality controllers in terms of task completion times. Furthermore, we observe\nthat poor network conditions, such as low bandwidth and high delay links, do\nnot substantially affect the remote users' ability to perform task\ndemonstrations successfully on RoboTurk. Lastly, we demonstrate the efficacy of\nRoboTurk through the collection of a pilot dataset; using RoboTurk, we\ncollected 137.5 hours of manipulation data from remote workers, amounting to\nover 2200 successful task demonstrations in 22 hours of total system usage. We\nshow that the data obtained through RoboTurk enables policy learning on\nmulti-step manipulation tasks with sparse rewards and that using larger\nquantities of demonstrations during policy learning provides benefits in terms\nof both learning consistency and final performance. For additional results,\nvideos, and to download our pilot dataset, visit\n$\\href{http://roboturk.stanford.edu/}{\\texttt{roboturk.stanford.edu}}$\n",
        "published": "2018",
        "authors": [
            "Ajay Mandlekar",
            "Yuke Zhu",
            "Animesh Garg",
            "Jonathan Booher",
            "Max Spero",
            "Albert Tung",
            "Julian Gao",
            "John Emmons",
            "Anchit Gupta",
            "Emre Orbay",
            "Silvio Savarese",
            "Li Fei-Fei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.07214v1",
        "title": "Parameter Sharing Reinforcement Learning Architecture for Multi Agent\n  Driving Behaviors",
        "abstract": "  Multi-agent learning provides a potential framework for learning and\nsimulating traffic behaviors. This paper proposes a novel architecture to learn\nmultiple driving behaviors in a traffic scenario. The proposed architecture can\nlearn multiple behaviors independently as well as simultaneously. We take\nadvantage of the homogeneity of agents and learn in a parameter sharing\nparadigm. To further speed up the training process asynchronous updates are\nemployed into the architecture. While learning different behaviors\nsimultaneously, the given framework was also able to learn cooperation between\nthe agents, without any explicit communication. We applied this framework to\nlearn two important behaviors in driving: 1) Lane-Keeping and 2) Over-Taking.\nResults indicate faster convergence and learning of a more generic behavior,\nthat is scalable to any number of agents. When compared the results with\nexisting approaches, our results indicate equal and even better performance in\nsome cases.\n",
        "published": "2018",
        "authors": [
            "Meha Kaushik",
            "Phaniteja S",
            "K. Madhava Krishna"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.08086v1",
        "title": "Model Learning for Look-ahead Exploration in Continuous Control",
        "abstract": "  We propose an exploration method that incorporates look-ahead search over\nbasic learnt skills and their dynamics, and use it for reinforcement learning\n(RL) of manipulation policies . Our skills are multi-goal policies learned in\nisolation in simpler environments using existing multigoal RL formulations,\nanalogous to options or macroactions. Coarse skill dynamics, i.e., the state\ntransition caused by a (complete) skill execution, are learnt and are unrolled\nforward during lookahead search. Policy search benefits from temporal\nabstraction during exploration, though itself operates over low-level primitive\nactions, and thus the resulting policies does not suffer from suboptimality and\ninflexibility caused by coarse skill chaining. We show that the proposed\nexploration strategy results in effective learning of complex manipulation\npolicies faster than current state-of-the-art RL methods, and converges to\nbetter policies than methods that use options or parametrized skills as\nbuilding blocks of the policy itself, as opposed to guiding exploration. We\nshow that the proposed exploration strategy results in effective learning of\ncomplex manipulation policies faster than current state-of-the-art RL methods,\nand converges to better policies than methods that use options or parameterized\nskills as building blocks of the policy itself, as opposed to guiding\nexploration.\n",
        "published": "2018",
        "authors": [
            "Arpit Agarwal",
            "Katharina Muelling",
            "Katerina Fragkiadaki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.08586v2",
        "title": "Urban Driving with Multi-Objective Deep Reinforcement Learning",
        "abstract": "  Autonomous driving is a challenging domain that entails multiple aspects: a\nvehicle should be able to drive to its destination as fast as possible while\navoiding collision, obeying traffic rules and ensuring the comfort of\npassengers. In this paper, we present a deep learning variant of thresholded\nlexicographic Q-learning for the task of urban driving. Our multi-objective DQN\nagent learns to drive on multi-lane roads and intersections, yielding and\nchanging lanes according to traffic rules. We also propose an extension for\nfactored Markov Decision Processes to the DQN architecture that provides\nauxiliary features for the Q function. This is shown to significantly improve\ndata efficiency. We then show that the learned policy is able to zero-shot\ntransfer to a ring road without sacrificing performance.\n",
        "published": "2018",
        "authors": [
            "Changjian Li",
            "Krzysztof Czarnecki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.08955v1",
        "title": "Integrating Task-Motion Planning with Reinforcement Learning for Robust\n  Decision Making in Mobile Robots",
        "abstract": "  Task-motion planning (TMP) addresses the problem of efficiently generating\nexecutable and low-cost task plans in a discrete space such that the (initially\nunknown) action costs are determined by motion plans in a corresponding\ncontinuous space. However, a task-motion plan can be sensitive to unexpected\ndomain uncertainty and changes, leading to suboptimal behaviors or execution\nfailures. In this paper, we propose a novel framework, TMP-RL, which is an\nintegration of TMP and reinforcement learning (RL) from the execution\nexperience, to solve the problem of robust task-motion planning in dynamic and\nuncertain domains. TMP-RL features two nested planning-learning loops. In the\ninner TMP loop, the robot generates a low-cost, feasible task-motion plan by\niteratively planning in the discrete space and updating relevant action costs\nevaluated by the motion planner in continuous space. In the outer loop, the\nplan is executed, and the robot learns from the execution experience via\nmodel-free RL, to further improve its task-motion plans. RL in the outer loop\nis more accurate to the current domain but also more expensive, and using less\ncostly task and motion planning leads to a jump-start for learning in the real\nworld. Our approach is evaluated on a mobile service robot conducting\nnavigation tasks in an office area. Results show that TMP-RL approach\nsignificantly improves adaptability and robustness (in comparison to TMP\nmethods) and leads to rapid convergence (in comparison to task planning (TP)-RL\nmethods). We also show that TMP-RL can reuse learned values to smoothly adapt\nto new scenarios during long-term deployments.\n",
        "published": "2018",
        "authors": [
            "Yuqian Jiang",
            "Fangkai Yang",
            "Shiqi Zhang",
            "Peter Stone"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.09864v2",
        "title": "Hardware Conditioned Policies for Multi-Robot Transfer Learning",
        "abstract": "  Deep reinforcement learning could be used to learn dexterous robotic policies\nbut it is challenging to transfer them to new robots with vastly different\nhardware properties. It is also prohibitively expensive to learn a new policy\nfrom scratch for each robot hardware due to the high sample complexity of\nmodern state-of-the-art algorithms. We propose a novel approach called\n\\textit{Hardware Conditioned Policies} where we train a universal policy\nconditioned on a vector representation of robot hardware. We considered robots\nin simulation with varied dynamics, kinematic structure, kinematic lengths and\ndegrees-of-freedom. First, we use the kinematic structure directly as the\nhardware encoding and show great zero-shot transfer to completely novel robots\nnot seen during training. For robots with lower zero-shot success rate, we also\ndemonstrate that fine-tuning the policy network is significantly more\nsample-efficient than training a model from scratch. In tasks where knowing the\nagent dynamics is important for success, we learn an embedding for robot\nhardware and show that policies conditioned on the encoding of hardware tend to\ngeneralize and transfer well. The code and videos are available on the project\nwebpage: https://sites.google.com/view/robot-transfer-hcp.\n",
        "published": "2018",
        "authors": [
            "Tao Chen",
            "Adithyavairavan Murali",
            "Abhinav Gupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1811.11711v2",
        "title": "Neural probabilistic motor primitives for humanoid control",
        "abstract": "  We focus on the problem of learning a single motor module that can flexibly\nexpress a range of behaviors for the control of high-dimensional physically\nsimulated humanoids. To do this, we propose a motor architecture that has the\ngeneral structure of an inverse model with a latent-variable bottleneck. We\nshow that it is possible to train this model entirely offline to compress\nthousands of expert policies and learn a motor primitive embedding space. The\ntrained neural probabilistic motor primitive system can perform one-shot\nimitation of whole-body humanoid behaviors, robustly mimicking unseen\ntrajectories. Additionally, we demonstrate that it is also straightforward to\ntrain controllers to reuse the learned motor primitive space to solve tasks,\nand the resulting movements are relatively naturalistic. To support the\ntraining of our model, we compare two approaches for offline policy cloning,\nincluding an experience efficient method which we call linear feedback policy\ncloning. We encourage readers to view a supplementary video (\nhttps://youtu.be/CaDEf-QcKwA ) summarizing our results.\n",
        "published": "2018",
        "authors": [
            "Josh Merel",
            "Leonard Hasenclever",
            "Alexandre Galashov",
            "Arun Ahuja",
            "Vu Pham",
            "Greg Wayne",
            "Yee Whye Teh",
            "Nicolas Heess"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.00811v2",
        "title": "From exploration to control: learning object manipulation skills through\n  novelty search and local adaptation",
        "abstract": "  Programming a robot to deal with open-ended tasks remains a challenge, in\nparticular if the robot has to manipulate objects. Launching, grasping, pushing\nor any other object interaction can be simulated but the corresponding models\nare not reversible and the robot behavior thus cannot be directly deduced.\nThese behaviors are hard to learn without a demonstration as the search space\nis large and the reward sparse. We propose a method to autonomously generate a\ndiverse repertoire of simple object interaction behaviors in simulation. Our\ngoal is to bootstrap a robot learning and development process with limited\ninformation about what the robot has to achieve and how. This repertoire can be\nexploited to solve different tasks in reality thanks to a proposed adaptation\nmethod or could be used as a training set for data-hungry algorithms.\n  The proposed approach relies on the definition of a goal space and generates\na repertoire of trajectories to reach attainable goals, thus allowing the robot\nto control this goal space. The repertoire is built with an off-the-shelf\nsimulation thanks to a quality diversity algorithm. The result is a set of\nsolutions tested in simulation only. It may result in two different problems:\n(1) as the repertoire is discrete and finite, it may not contain the trajectory\nto deal with a given situation or (2) some trajectories may lead to a behavior\nin reality that differs from simulation because of a reality gap. We propose an\napproach to deal with both issues by using a local linearization of the mapping\nbetween the motion parameters and the observed effects. Furthermore, we present\nan approach to update the existing solutions repertoire with the tests done on\nthe real robot. The approach has been validated on two different experiments on\nthe Baxter robot: a ball launching and a joystick manipulation tasks.\n",
        "published": "2019",
        "authors": [
            "Seungsu Kim",
            "Alexandre Coninx",
            "Stephane Doncieux"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.07517v1",
        "title": "Robust Recovery Controller for a Quadrupedal Robot using Deep\n  Reinforcement Learning",
        "abstract": "  The ability to recover from a fall is an essential feature for a legged robot\nto navigate in challenging environments robustly. Until today, there has been\nvery little progress on this topic. Current solutions mostly build upon\n(heuristically) predefined trajectories, resulting in unnatural behaviors and\nrequiring considerable effort in engineering system-specific components. In\nthis paper, we present an approach based on model-free Deep Reinforcement\nLearning (RL) to control recovery maneuvers of quadrupedal robots using a\nhierarchical behavior-based controller. The controller consists of four neural\nnetwork policies including three behaviors and one behavior selector to\ncoordinate them. Each of them is trained individually in simulation and\ndeployed directly on a real system. We experimentally validate our approach on\nthe quadrupedal robot ANYmal, which is a dog-sized quadrupedal system with 12\ndegrees of freedom. With our method, ANYmal manifests dynamic and reactive\nrecovery behaviors to recover from an arbitrary fall configuration within less\nthan 5 seconds. We tested the recovery maneuver more than 100 times, and the\nsuccess rate was higher than 97 %.\n",
        "published": "2019",
        "authors": [
            "Joonho Lee",
            "Jemin Hwangbo",
            "Marco Hutter"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1901.09792v1",
        "title": "Sensorimotor learning for artificial body perception",
        "abstract": "  Artificial self-perception is the machine ability to perceive its own body,\ni.e., the mastery of modal and intermodal contingencies of performing an action\nwith a specific sensors/actuators body configuration. In other words, the\nspatio-temporal patterns that relate its sensors (e.g. visual, proprioceptive,\ntactile, etc.), its actions and its body latent variables are responsible of\nthe distinction between its own body and the rest of the world. This paper\ndescribes some of the latest approaches for modelling artificial body\nself-perception: from Bayesian estimation to deep learning. Results show the\npotential of these free-model unsupervised or semi-supervised\ncrossmodal/intermodal learning approaches. However, there are still challenges\nthat should be overcome before we achieve artificial multisensory body\nperception.\n",
        "published": "2019",
        "authors": [
            "German Diez-Valencia",
            "Takuya Ohashi",
            "Pablo Lanillos",
            "Gordon Cheng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.08705v2",
        "title": "A General Framework for Structured Learning of Mechanical Systems",
        "abstract": "  Learning accurate dynamics models is necessary for optimal, compliant control\nof robotic systems. Current approaches to white-box modeling using analytic\nparameterizations, or black-box modeling using neural networks, can suffer from\nhigh bias or high variance. We address the need for a flexible, gray-box model\nof mechanical systems that can seamlessly incorporate prior knowledge where it\nis available, and train expressive function approximators where it is not. We\npropose to parameterize a mechanical system using neural networks to model its\nLagrangian and the generalized forces that act on it. We test our method on a\nsimulated, actuated double pendulum. We show that our method outperforms a\nnaive, black-box model in terms of data-efficiency, as well as performance in\nmodel-based reinforcement learning. We also conduct a systematic study of our\nmethod's ability to incorporate available prior knowledge about the system to\nimprove data efficiency.\n",
        "published": "2019",
        "authors": [
            "Jayesh K. Gupta",
            "Kunal Menda",
            "Zachary Manchester",
            "Mykel J. Kochenderfer"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.09458v2",
        "title": "Long-Range Indoor Navigation with PRM-RL",
        "abstract": "  Long-range indoor navigation requires guiding robots with noisy sensors and\ncontrols through cluttered environments along paths that span a variety of\nbuildings. We achieve this with PRM-RL, a hierarchical robot navigation method\nin which reinforcement learning agents that map noisy sensors to robot controls\nlearn to solve short-range obstacle avoidance tasks, and then sampling-based\nplanners map where these agents can reliably navigate in simulation; these\nroadmaps and agents are then deployed on robots, guiding them along the\nshortest path where the agents are likely to succeed. Here we use Probabilistic\nRoadmaps (PRMs) as the sampling-based planner, and AutoRL as the reinforcement\nlearning method in the indoor navigation context. We evaluate the method in\nsimulation for kinematic differential drive and kinodynamic car-like robots in\nseveral environments, and on differential-drive robots at three physical sites.\nOur results show PRM-RL with AutoRL is more successful than several baselines,\nis robust to noise, and can guide robots over hundreds of meters in the face of\nnoise and obstacles in both simulation and on robots, including over 5.8\nkilometers of physical robot navigation. Video: https://youtu.be/xN-OWX5gKvQ\n",
        "published": "2019",
        "authors": [
            "Anthony Francis",
            "Aleksandra Faust",
            "Hao-Tien Lewis Chiang",
            "Jasmine Hsu",
            "J. Chase Kew",
            "Marek Fiser",
            "Tsang-Wei Edward Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1902.09628v1",
        "title": "Flappy Hummingbird: An Open Source Dynamic Simulation of Flapping Wing\n  Robots and Animals",
        "abstract": "  Insects and hummingbirds exhibit extraordinary flight capabilities and can\nsimultaneously master seemingly conflicting goals: stable hovering and\naggressive maneuvering, unmatched by small scale man-made vehicles. Flapping\nWing Micro Air Vehicles (FWMAVs) hold great promise for closing this\nperformance gap. However, design and control of such systems remain challenging\ndue to various constraints. Here, we present an open source high fidelity\ndynamic simulation for FWMAVs to serve as a testbed for the design,\noptimization and flight control of FWMAVs. For simulation validation, we\nrecreated the hummingbird-scale robot developed in our lab in the simulation.\nSystem identification was performed to obtain the model parameters. The force\ngeneration, open-loop and closed-loop dynamic response between simulated and\nexperimental flights were compared and validated. The unsteady aerodynamics and\nthe highly nonlinear flight dynamics present challenging control problems for\nconventional and learning control algorithms such as Reinforcement Learning.\nThe interface of the simulation is fully compatible with OpenAI Gym\nenvironment. As a benchmark study, we present a linear controller for hovering\nstabilization and a Deep Reinforcement Learning control policy for\ngoal-directed maneuvering. Finally, we demonstrate direct simulation-to-real\ntransfer of both control policies onto the physical robot, further\ndemonstrating the fidelity of the simulation.\n",
        "published": "2019",
        "authors": [
            "Fan Fei",
            "Zhan Tu",
            "Yilun Yang",
            "Jian Zhang",
            "Xinyan Deng"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.01055v6",
        "title": "QuaRL: Quantization for Fast and Environmentally Sustainable\n  Reinforcement Learning",
        "abstract": "  Deep reinforcement learning continues to show tremendous potential in\nachieving task-level autonomy, however, its computational and energy demands\nremain prohibitively high. In this paper, we tackle this problem by applying\nquantization to reinforcement learning. To that end, we introduce a novel\nReinforcement Learning (RL) training paradigm, \\textit{ActorQ}, to speed up\nactor-learner distributed RL training. \\textit{ActorQ} leverages 8-bit\nquantized actors to speed up data collection without affecting learning\nconvergence. Our quantized distributed RL training system, \\textit{ActorQ},\ndemonstrates end-to-end speedups \\blue{between 1.5 $\\times$ and 5.41$\\times$},\nand faster convergence over full precision training on a range of tasks\n(Deepmind Control Suite) and different RL algorithms (D4PG, DQN). Furthermore,\nwe compare the carbon emissions (Kgs of CO2) of \\textit{ActorQ} versus standard\nreinforcement learning \\blue{algorithms} on various tasks. Across various\nsettings, we show that \\textit{ActorQ} enables more environmentally friendly\nreinforcement learning by achieving \\blue{carbon emission improvements between\n1.9$\\times$ and 3.76$\\times$} compared to training RL-agents in full-precision.\nWe believe that this is the first of many future works on enabling\ncomputationally energy-efficient and sustainable reinforcement learning. The\nsource code is available here for the public to use:\n\\url{https://github.com/harvard-edge/QuaRL}.\n",
        "published": "2019",
        "authors": [
            "Srivatsan Krishnan",
            "Maximilian Lam",
            "Sharad Chitlangia",
            "Zishen Wan",
            "Gabriel Barth-Maron",
            "Aleksandra Faust",
            "Vijay Janapa Reddi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.01994v1",
        "title": "Zero Shot Learning on Simulated Robots",
        "abstract": "  In this work we present a method for leveraging data from one source to learn\nhow to do multiple new tasks. Task transfer is achieved using a self-model that\nencapsulates the dynamics of a system and serves as an environment for\nreinforcement learning. To study this approach, we train a self-models on\nvarious robot morphologies, using randomly sampled actions. Using a self-model,\nan initial state and corresponding actions, we can predict the next state. This\npredictive self-model is then used by a standard reinforcement learning\nalgorithm to accomplish tasks without ever seeing a state from the \"real\"\nenvironment. These trained policies allow the robots to successfully achieve\ntheir goals in the \"real\" environment. We demonstrate that not only is training\non the self-model far more data efficient than learning even a single task, but\nalso that it allows for learning new tasks without necessitating any additional\ndata collection, essentially allowing zero-shot learning of new tasks.\n",
        "published": "2019",
        "authors": [
            "Robert Kwiatkowski",
            "Hod Lipson"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.02812v1",
        "title": "Policies Modulating Trajectory Generators",
        "abstract": "  We propose an architecture for learning complex controllable behaviors by\nhaving simple Policies Modulate Trajectory Generators (PMTG), a powerful\ncombination that can provide both memory and prior knowledge to the controller.\nThe result is a flexible architecture that is applicable to a class of problems\nwith periodic motion for which one has an insight into the class of\ntrajectories that might lead to a desired behavior. We illustrate the basics of\nour architecture using a synthetic control problem, then go on to learn\nspeed-controlled locomotion for a quadrupedal robot by using Deep Reinforcement\nLearning and Evolutionary Strategies. We demonstrate that a simple linear\npolicy, when paired with a parametric Trajectory Generator for quadrupedal\ngaits, can induce walking behaviors with controllable speed from 4-dimensional\nIMU observations alone, and can be learned in under 1000 rollouts. We also\ntransfer these policies to a real robot and show locomotion with controllable\nforward velocity.\n",
        "published": "2019",
        "authors": [
            "Atil Iscen",
            "Ken Caluwaerts",
            "Jie Tan",
            "Tingnan Zhang",
            "Erwin Coumans",
            "Vikas Sindhwani",
            "Vincent Vanhoucke"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.03398v2",
        "title": "Toward Synergic Learning for Autonomous Manipulation of Deformable\n  Tissues via Surgical Robots: An Approximate Q-Learning Approach",
        "abstract": "  In this paper, we present a synergic learning algorithm to address the task\nof indirect manipulation of an unknown deformable tissue. Tissue manipulation\nis a common yet challenging task in various surgical interventions, which makes\nit a good candidate for robotic automation. We propose using a linear\napproximate Q-learning method in which human knowledge contributes to selecting\nuseful yet simple features of tissue manipulation while the algorithm learns to\ntake optimal actions and accomplish the task. The algorithm is implemented and\nevaluated on a simulation using the OpenCV and CHAI3D libraries. Successful\nsimulation results for four different configurations which are based on\nrealistic tissue manipulation scenarios are presented. Results indicate that\nwith a careful selection of relatively simple and intuitive features, the\ndeveloped Q-learning algorithm can successfully learn an optimal policy without\nany prior knowledge of tissue dynamics or camera intrinsic/extrinsic\ncalibration parameters.\n",
        "published": "2019",
        "authors": [
            "Sahba Aghajani Pedram",
            "Peter Walker Ferguson",
            "Changyeob Shin",
            "Ankur Mehta",
            "Erik P. Dutson",
            "Farshid Alambeigi",
            "Jacob Rosen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.03650v3",
        "title": "Multi-Head Attention for Multi-Modal Joint Vehicle Motion Forecasting",
        "abstract": "  This paper presents a novel vehicle motion forecasting method based on\nmulti-head attention. It produces joint forecasts for all vehicles on a road\nscene as sequences of multi-modal probability density functions of their\npositions. Its architecture uses multi-head attention to account for complete\ninteractions between all vehicles, and long short-term memory layers for\nencoding and forecasting. It relies solely on vehicle position tracks, does not\nneed maneuver definitions, and does not represent the scene with a spatial\ngrid. This allows it to be more versatile than similar model while combining\nany forecasting capabilities, namely joint forecast with interactions,\nuncertainty estimation, and multi-modality. The resulting prediction likelihood\noutperforms state-of-the-art models on the same dataset.\n",
        "published": "2019",
        "authors": [
            "Jean Mercat",
            "Thomas Gilles",
            "Nicole El Zoghby",
            "Guillaume Sandou",
            "Dominique Beauvois",
            "Guillermo Pita Gil"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.04365v1",
        "title": "Asking Easy Questions: A User-Friendly Approach to Active Reward\n  Learning",
        "abstract": "  Robots can learn the right reward function by querying a human expert.\nExisting approaches attempt to choose questions where the robot is most\nuncertain about the human's response; however, they do not consider how easy it\nwill be for the human to answer! In this paper we explore an information gain\nformulation for optimally selecting questions that naturally account for the\nhuman's ability to answer. Our approach identifies questions that optimize the\ntrade-off between robot and human uncertainty, and determines when these\nquestions become redundant or costly. Simulations and a user study show our\nmethod not only produces easy questions, but also ultimately results in faster\nreward learning.\n",
        "published": "2019",
        "authors": [
            "Erdem B\u0131y\u0131k",
            "Malayandi Palan",
            "Nicholas C. Landolfi",
            "Dylan P. Losey",
            "Dorsa Sadigh"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.04700v1",
        "title": "Assistive Gym: A Physics Simulation Framework for Assistive Robotics",
        "abstract": "  Autonomous robots have the potential to serve as versatile caregivers that\nimprove quality of life for millions of people worldwide. Yet, conducting\nresearch in this area presents numerous challenges, including the risks of\nphysical interaction between people and robots. Physics simulations have been\nused to optimize and train robots for physical assistance, but have typically\nfocused on a single task. In this paper, we present Assistive Gym, an open\nsource physics simulation framework for assistive robots that models multiple\ntasks. It includes six simulated environments in which a robotic manipulator\ncan attempt to assist a person with activities of daily living (ADLs): itch\nscratching, drinking, feeding, body manipulation, dressing, and bathing.\nAssistive Gym models a person's physical capabilities and preferences for\nassistance, which are used to provide a reward function. We present baseline\npolicies trained using reinforcement learning for four different commercial\nrobots in the six environments. We demonstrate that modeling human motion\nresults in better assistance and we compare the performance of different\nrobots. Overall, we show that Assistive Gym is a promising tool for assistive\nrobotics research.\n",
        "published": "2019",
        "authors": [
            "Zackory Erickson",
            "Vamsee Gangaram",
            "Ariel Kapusta",
            "C. Karen Liu",
            "Charles C. Kemp"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.04803v1",
        "title": "Autonomous Driving using Safe Reinforcement Learning by Incorporating a\n  Regret-based Human Lane-Changing Decision Model",
        "abstract": "  It is expected that many human drivers will still prefer to drive themselves\neven if the self-driving technologies are ready. Therefore, human-driven\nvehicles and autonomous vehicles (AVs) will coexist in a mixed traffic for a\nlong time. To enable AVs to safely and efficiently maneuver in this mixed\ntraffic, it is critical that the AVs can understand how humans cope with risks\nand make driving-related decisions. On the other hand, the driving environment\nis highly dynamic and ever-changing, and it is thus difficult to enumerate all\nthe scenarios and hard-code the controllers. To face up these challenges, in\nthis work, we incorporate a human decision-making model in reinforcement\nlearning to control AVs for safe and efficient operations. Specifically, we\nadapt regret theory to describe a human driver's lane-changing behavior, and\nfit the personalized models to individual drivers for predicting their\nlane-changing decisions. The predicted decisions are incorporated in the safety\nconstraints for reinforcement learning in training and in implementation. We\nthen use an extended version of double deep Q-network (DDQN) to train our AV\ncontroller within the safety set. By doing so, the amount of collisions in\ntraining is reduced to zero, while the training accuracy is not impinged.\n",
        "published": "2019",
        "authors": [
            "Dong Chen",
            "Longsheng Jiang",
            "Yue Wang",
            "Zhaojian Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.06001v1",
        "title": "Federated Transfer Reinforcement Learning for Autonomous Driving",
        "abstract": "  Reinforcement learning (RL) is widely used in autonomous driving tasks and\ntraining RL models typically involves in a multi-step process: pre-training RL\nmodels on simulators, uploading the pre-trained model to real-life robots, and\nfine-tuning the weight parameters on robot vehicles. This sequential process is\nextremely time-consuming and more importantly, knowledge from the fine-tuned\nmodel stays local and can not be re-used or leveraged collaboratively. To\ntackle this problem, we present an online federated RL transfer process for\nreal-time knowledge extraction where all the participant agents make\ncorresponding actions with the knowledge learned by others, even when they are\nacting in very different environments. To validate the effectiveness of the\nproposed approach, we constructed a real-life collision avoidance system with\nMicrosoft Airsim simulator and NVIDIA JetsonTX2 car agents, which cooperatively\nlearn from scratch to avoid collisions in indoor environment with obstacle\nobjects. We demonstrate that with the proposed framework, the simulator car\nagents can transfer knowledge to the RC cars in real-time, with 27% increase in\nthe average distance with obstacles and 42% decrease in the collision counts.\n",
        "published": "2019",
        "authors": [
            "Xinle Liang",
            "Yang Liu",
            "Tianjian Chen",
            "Ming Liu",
            "Qiang Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.07093v1",
        "title": "Explainable Semantic Mapping for First Responders",
        "abstract": "  One of the key challenges in the semantic mapping problem in postdisaster\nenvironments is how to analyze a large amount of data efficiently with minimal\nsupervision. To address this challenge, we propose a deep learning-based\nsemantic mapping tool consisting of three main ideas. First, we develop a\nfrugal semantic segmentation algorithm that uses only a small amount of labeled\ndata. Next, we investigate on the problem of learning to detect a new class of\nobject using just a few training examples. Finally, we develop an explainable\ncost map learning algorithm that can be quickly trained to generate\ntraversability cost maps using only raw sensor data such as aerial-view\nimagery. This paper presents an overview of the proposed idea and the lessons\nlearned.\n",
        "published": "2019",
        "authors": [
            "Jean Oh",
            "Martial Hebert",
            "Hae-Gon Jeon",
            "Xavier Perez",
            "Chia Dai",
            "Yeeho Song"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.09998v3",
        "title": "Learning Resilient Behaviors for Navigation Under Uncertainty",
        "abstract": "  Deep reinforcement learning has great potential to acquire complex, adaptive\nbehaviors for autonomous agents automatically. However, the underlying neural\nnetwork polices have not been widely deployed in real-world applications,\nespecially in these safety-critical tasks (e.g., autonomous driving). One of\nthe reasons is that the learned policy cannot perform flexible and resilient\nbehaviors as traditional methods to adapt to diverse environments. In this\npaper, we consider the problem that a mobile robot learns adaptive and\nresilient behaviors for navigating in unseen uncertain environments while\navoiding collisions. We present a novel approach for uncertainty-aware\nnavigation by introducing an uncertainty-aware predictor to model the\nenvironmental uncertainty, and we propose a novel uncertainty-aware navigation\nnetwork to learn resilient behaviors in the prior unknown environments. To\ntrain the proposed uncertainty-aware network more stably and efficiently, we\npresent the temperature decay training paradigm, which balances exploration and\nexploitation during the training process. Our experimental evaluation\ndemonstrates that our approach can learn resilient behaviors in diverse\nenvironments and generate adaptive trajectories according to environmental\nuncertainties.\n",
        "published": "2019",
        "authors": [
            "Tingxiang Fan",
            "Pinxin Long",
            "Wenxi Liu",
            "Jia Pan",
            "Ruigang Yang",
            "Dinesh Manocha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.10232v2",
        "title": "Bottom-Up Meta-Policy Search",
        "abstract": "  Despite of the recent progress in agents that learn through interaction,\nthere are several challenges in terms of sample efficiency and generalization\nacross unseen behaviors during training. To mitigate these problems, we propose\nand apply a first-order Meta-Learning algorithm called Bottom-Up Meta-Policy\nSearch (BUMPS), which works with two-phase optimization procedure: firstly, in\na meta-training phase, it distills few expert policies to create a meta-policy\ncapable of generalizing knowledge to unseen tasks during training; secondly, it\napplies a fast adaptation strategy named Policy Filtering, which evaluates few\npolicies sampled from the meta-policy distribution and selects which best\nsolves the task. We conducted all experiments in the RoboCup 3D Soccer\nSimulation domain, in the context of kick motion learning. We show that, given\nour experimental setup, BUMPS works in scenarios where simple multi-task\nReinforcement Learning does not. Finally, we performed experiments in a way to\nevaluate each component of the algorithm.\n",
        "published": "2019",
        "authors": [
            "Luckeciano C. Melo",
            "Marcos R. O. A. Maximo",
            "Adilson Marques da Cunha"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.10620v1",
        "title": "Learning Humanoid Robot Running Skills through Proximal Policy\n  Optimization",
        "abstract": "  In the current level of evolution of Soccer 3D, motion control is a key\nfactor in team's performance. Recent works takes advantages of model-free\napproaches based on Machine Learning to exploit robot dynamics in order to\nobtain faster locomotion skills, achieving running policies and, therefore,\nopening a new research direction in the Soccer 3D environment.\n  In this work, we present a methodology based on Deep Reinforcement Learning\nthat learns running skills without any prior knowledge, using a neural network\nwhose inputs are related to robot's dynamics. Our results outperformed the\nprevious state-of-the-art sprint velocity reported in Soccer 3D literature by a\nsignificant margin. It also demonstrated improvement in sample efficiency,\nbeing able to learn how to run in just few hours.\n  We reported our results analyzing the training procedure and also evaluating\nthe policies in terms of speed, reliability and human similarity. Finally, we\npresented key factors that lead us to improve previous results and shared some\nideas for future work.\n",
        "published": "2019",
        "authors": [
            "Luckeciano C. Melo",
            "Marcos R. O. A. Maximo"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.12908v3",
        "title": "Certified Adversarial Robustness for Deep Reinforcement Learning",
        "abstract": "  Deep Neural Network-based systems are now the state-of-the-art in many\nrobotics tasks, but their application in safety-critical domains remains\ndangerous without formal guarantees on network robustness. Small perturbations\nto sensor inputs (from noise or adversarial examples) are often enough to\nchange network-based decisions, which was already shown to cause an autonomous\nvehicle to swerve into oncoming traffic. In light of these dangers, numerous\nalgorithms have been developed as defensive mechanisms from these adversarial\ninputs, some of which provide formal robustness guarantees or certificates.\nThis work leverages research on certified adversarial robustness to develop an\nonline certified defense for deep reinforcement learning algorithms. The\nproposed defense computes guaranteed lower bounds on state-action values during\nexecution to identify and choose the optimal action under a worst-case\ndeviation in input space due to possible adversaries or noise. The approach is\ndemonstrated on a Deep Q-Network policy and is shown to increase robustness to\nnoise and adversaries in pedestrian collision avoidance scenarios and a classic\ncontrol task.\n",
        "published": "2019",
        "authors": [
            "Bj\u00f6rn L\u00fctjens",
            "Michael Everett",
            "Jonathan P. How"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1910.13399v1",
        "title": "Robust Model-free Reinforcement Learning with Multi-objective Bayesian\n  Optimization",
        "abstract": "  In reinforcement learning (RL), an autonomous agent learns to perform complex\ntasks by maximizing an exogenous reward signal while interacting with its\nenvironment. In real-world applications, test conditions may differ\nsubstantially from the training scenario and, therefore, focusing on pure\nreward maximization during training may lead to poor results at test time. In\nthese cases, it is important to trade-off between performance and robustness\nwhile learning a policy. While several results exist for robust, model-based\nRL, the model-free case has not been widely investigated. In this paper, we\ncast the robust, model-free RL problem as a multi-objective optimization\nproblem. To quantify the robustness of a policy, we use delay margin and gain\nmargin, two robustness indicators that are common in control theory. We show\nhow these metrics can be estimated from data in the model-free setting. We use\nmulti-objective Bayesian optimization (MOBO) to solve efficiently this\nexpensive-to-evaluate, multi-objective optimization problem. We show the\nbenefits of our robust formulation both in sim-to-real and pure hardware\nexperiments to balance a Furuta pendulum.\n",
        "published": "2019",
        "authors": [
            "Matteo Turchetta",
            "Andreas Krause",
            "Sebastian Trimpe"
        ]
    }
]