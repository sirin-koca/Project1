[
    {
        "id": "http://arxiv.org/abs/2004.00716v1",
        "title": "Constrained-Space Optimization and Reinforcement Learning for Complex\n  Tasks",
        "abstract": "  Learning from Demonstration is increasingly used for transferring operator\nmanipulation skills to robots. In practice, it is important to cater for\nlimited data and imperfect human demonstrations, as well as underlying safety\nconstraints. This paper presents a constrained-space optimization and\nreinforcement learning scheme for managing complex tasks. Through interactions\nwithin the constrained space, the reinforcement learning agent is trained to\noptimize the manipulation skills according to a defined reward function. After\nlearning, the optimal policy is derived from the well-trained reinforcement\nlearning agent, which is then implemented to guide the robot to conduct tasks\nthat are similar to the experts' demonstrations. The effectiveness of the\nproposed method is verified with a robotic suturing task, demonstrating that\nthe learned policy outperformed the experts' demonstrations in terms of the\nsmoothness of the joint motion and end-effector trajectories, as well as the\noverall task completion time.\n",
        "published": "2020",
        "authors": [
            "Ya-Yen Tsai",
            "Bo Xiao",
            "Edward Johns",
            "Guang-Zhong Yang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.00801v1",
        "title": "Exploration of Reinforcement Learning for Event Camera using Car-like\n  Robots",
        "abstract": "  We demonstrate the first reinforcement-learning application for robots\nequipped with an event camera. Because of the considerably lower latency of the\nevent camera, it is possible to achieve much faster control of robots compared\nwith the existing vision-based reinforcement-learning applications using\nstandard cameras. To handle a stream of events for reinforcement learning, we\nintroduced an image-like feature and demonstrated the feasibility of training\nan agent in a simulator for two tasks: fast collision avoidance and obstacle\ntracking. Finally, we set up a robot with an event camera in the real world and\nthen transferred the agent trained in the simulator, resulting in successful\nfast avoidance of randomly thrown objects. Incorporating event camera into\nreinforcement learning opens new possibilities for various robotics\napplications that require swift control, such as autonomous vehicles and\ndrones, through end-to-end learning approaches.\n",
        "published": "2020",
        "authors": [
            "Riku Arakawa",
            "Shintaro Shiba"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.04450v1",
        "title": "Risk-Aware High-level Decisions for Automated Driving at Occluded\n  Intersections with Reinforcement Learning",
        "abstract": "  Reinforcement learning is nowadays a popular framework for solving different\ndecision making problems in automated driving. However, there are still some\nremaining crucial challenges that need to be addressed for providing more\nreliable policies. In this paper, we propose a generic risk-aware DQN approach\nin order to learn high level actions for driving through unsignalized occluded\nintersections. The proposed state representation provides lane based\ninformation which allows to be used for multi-lane scenarios. Moreover, we\npropose a risk based reward function which punishes risky situations instead of\nonly collision failures. Such rewarding approach helps to incorporate risk\nprediction into our deep Q network and learn more reliable policies which are\nsafer in challenging situations. The efficiency of the proposed approach is\ncompared with a DQN learned with conventional collision based rewarding scheme\nand also with a rule-based intersection navigation policy. Evaluation results\nshow that the proposed approach outperforms both of these methods. It provides\nsafer actions than collision-aware DQN approach and is less overcautious than\nthe rule-based policy.\n",
        "published": "2020",
        "authors": [
            "Danial Kamran",
            "Carlos Fernandez Lopez",
            "Martin Lauer",
            "Christoph Stiller"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.04787v2",
        "title": "An End-to-End Learning Approach for Trajectory Prediction in Pedestrian\n  Zones",
        "abstract": "  This paper aims to explore the problem of trajectory prediction in\nheterogeneous pedestrian zones, where social dynamics representation is a big\nchallenge. Proposed is an end-to-end learning framework for prediction accuracy\nimprovement based on an attention mechanism to learn social interaction from\nmulti-factor inputs.\n",
        "published": "2020",
        "authors": [
            "Ha Q. Ngo",
            "Christoph Henke",
            "Frank Hees"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.08646v2",
        "title": "Macro-Action-Based Deep Multi-Agent Reinforcement Learning",
        "abstract": "  In real-world multi-robot systems, performing high-quality, collaborative\nbehaviors requires robots to asynchronously reason about high-level action\nselection at varying time durations. Macro-Action Decentralized Partially\nObservable Markov Decision Processes (MacDec-POMDPs) provide a general\nframework for asynchronous decision making under uncertainty in fully\ncooperative multi-agent tasks. However, multi-agent deep reinforcement learning\nmethods have only been developed for (synchronous) primitive-action problems.\nThis paper proposes two Deep Q-Network (DQN) based methods for learning\ndecentralized and centralized macro-action-value functions with novel\nmacro-action trajectory replay buffers introduced for each case. Evaluations on\nbenchmark problems and a larger domain demonstrate the advantage of learning\nwith macro-actions over primitive-actions and the scalability of our\napproaches.\n",
        "published": "2020",
        "authors": [
            "Yuchen Xiao",
            "Joshua Hoffman",
            "Christopher Amato"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.10439v1",
        "title": "Tactical Decision-Making in Autonomous Driving by Reinforcement Learning\n  with Uncertainty Estimation",
        "abstract": "  Reinforcement learning (RL) can be used to create a tactical decision-making\nagent for autonomous driving. However, previous approaches only output\ndecisions and do not provide information about the agent's confidence in the\nrecommended actions. This paper investigates how a Bayesian RL technique, based\non an ensemble of neural networks with additional randomized prior functions\n(RPF), can be used to estimate the uncertainty of decisions in autonomous\ndriving. A method for classifying whether or not an action should be considered\nsafe is also introduced. The performance of the ensemble RPF method is\nevaluated by training an agent on a highway driving scenario. It is shown that\nthe trained agent can estimate the uncertainty of its decisions and indicate an\nunacceptable level when the agent faces a situation that is far from the\ntraining distribution. Furthermore, within the training distribution, the\nensemble RPF agent outperforms a standard Deep Q-Network agent. In this study,\nthe estimated uncertainty is used to choose safe actions in unknown situations.\nHowever, the uncertainty information could also be used to identify situations\nthat should be added to the training process.\n",
        "published": "2020",
        "authors": [
            "Carl-Johan Hoel",
            "Krister Wolff",
            "Leo Laine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.10927v1",
        "title": "Cooperative Perception with Deep Reinforcement Learning for Connected\n  Vehicles",
        "abstract": "  Sensor-based perception on vehicles are becoming prevalent and important to\nenhance the road safety. Autonomous driving systems use cameras, LiDAR, and\nradar to detect surrounding objects, while human-driven vehicles use them to\nassist the driver. However, the environmental perception by individual vehicles\nhas the limitations on coverage and/or detection accuracy. For example, a\nvehicle cannot detect objects occluded by other moving/static obstacles. In\nthis paper, we present a cooperative perception scheme with deep reinforcement\nlearning to enhance the detection accuracy for the surrounding objects. By\nusing the deep reinforcement learning to select the data to transmit, our\nscheme mitigates the network load in vehicular communication networks and\nenhances the communication reliability. To design, test, and verify the\ncooperative perception scheme, we develop a Cooperative & Intelligent Vehicle\nSimulation (CIVS) Platform, which integrates three software components: traffic\nsimulator, vehicle simulator, and object classifier. We evaluate that our\nscheme decreases packet loss and thereby increases the detection accuracy by up\nto 12%, compared to the baseline protocol.\n",
        "published": "2020",
        "authors": [
            "Shunsuke Aoki",
            "Takamasa Higuchi",
            "Onur Altintas"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2004.11345v2",
        "title": "Model-Based Meta-Reinforcement Learning for Flight with Suspended\n  Payloads",
        "abstract": "  Transporting suspended payloads is challenging for autonomous aerial vehicles\nbecause the payload can cause significant and unpredictable changes to the\nrobot's dynamics. These changes can lead to suboptimal flight performance or\neven catastrophic failure. Although adaptive control and learning-based methods\ncan in principle adapt to changes in these hybrid robot-payload systems, rapid\nmid-flight adaptation to payloads that have a priori unknown physical\nproperties remains an open problem. We propose a meta-learning approach that\n\"learns how to learn\" models of altered dynamics within seconds of\npost-connection flight data. Our experiments demonstrate that our online\nadaptation approach outperforms non-adaptive methods on a series of challenging\nsuspended payload transportation tasks. Videos and other supplemental material\nare available on our website: https://sites.google.com/view/meta-rl-for-flight\n",
        "published": "2020",
        "authors": [
            "Suneel Belkhale",
            "Rachel Li",
            "Gregory Kahn",
            "Rowan McAllister",
            "Roberto Calandra",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1511.08967v1",
        "title": "Robotic Search & Rescue via Online Multi-task Reinforcement Learning",
        "abstract": "  Reinforcement learning (RL) is a general and well-known method that a robot\ncan use to learn an optimal control policy to solve a particular task. We would\nlike to build a versatile robot that can learn multiple tasks, but using RL for\neach of them would be prohibitively expensive in terms of both time and\nwear-and-tear on the robot. To remedy this problem, we use the Policy Gradient\nEfficient Lifelong Learning Algorithm (PG-ELLA), an online multi-task RL\nalgorithm that enables the robot to efficiently learn multiple consecutive\ntasks by sharing knowledge between these tasks to accelerate learning and\nimprove performance. We implemented and evaluated three RL methods--Q-learning,\npolicy gradient RL, and PG-ELLA--on a ground robot whose task is to find a\ntarget object in an environment under different surface conditions. In this\npaper, we discuss our implementations as well as present an empirical analysis\nof their learning performance.\n",
        "published": "2015",
        "authors": [
            "Lisa Lee"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1512.00573v1",
        "title": "Object-based World Modeling in Semi-Static Environments with Dependent\n  Dirichlet-Process Mixtures",
        "abstract": "  To accomplish tasks in human-centric indoor environments, robots need to\nrepresent and understand the world in terms of objects and their attributes. We\nrefer to this attribute-based representation as a world model, and consider how\nto acquire it via noisy perception and maintain it over time, as objects are\nadded, changed, and removed in the world. Previous work has framed this as\nmultiple-target tracking problem, where objects are potentially in motion at\nall times. Although this approach is general, it is computationally expensive.\nWe argue that such generality is not needed in typical world modeling tasks,\nwhere objects only change state occasionally. More efficient approaches are\nenabled by restricting ourselves to such semi-static environments.\n  We consider a previously-proposed clustering-based world modeling approach\nthat assumed static environments, and extend it to semi-static domains by\napplying a dependent Dirichlet-process (DDP) mixture model. We derive a novel\nMAP inference algorithm under this model, subject to data association\nconstraints. We demonstrate our approach improves computational performance in\nsemi-static environments.\n",
        "published": "2015",
        "authors": [
            "Lawson L. S. Wong",
            "Thanard Kurutach",
            "Leslie Pack Kaelbling",
            "Tom\u00e1s Lozano-P\u00e9rez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.02169v2",
        "title": "Bayesian Optimisation for Safe Navigation under Localisation Uncertainty",
        "abstract": "  In outdoor environments, mobile robots are required to navigate through\nterrain with varying characteristics, some of which might significantly affect\nthe integrity of the platform. Ideally, the robot should be able to identify\nareas that are safe for navigation based on its own percepts about the\nenvironment while avoiding damage to itself. Bayesian optimisation (BO) has\nbeen successfully applied to the task of learning a model of terrain\ntraversability while guiding the robot through more traversable areas. An\nissue, however, is that localisation uncertainty can end up guiding the robot\nto unsafe areas and distort the model being learnt. In this paper, we address\nthis problem and present a novel method that allows BO to consider localisation\nuncertainty by applying a Gaussian process model for uncertain inputs as a\nprior. We evaluate the proposed method in simulation and in experiments with a\nreal robot navigating over rough terrain and compare it against standard BO\nmethods.\n",
        "published": "2017",
        "authors": [
            "Rafael Oliveira",
            "Lionel Ott",
            "Vitor Guizilini",
            "Fabio Ramos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.03153v2",
        "title": "MBMF: Model-Based Priors for Model-Free Reinforcement Learning",
        "abstract": "  Reinforcement Learning is divided in two main paradigms: model-free and\nmodel-based. Each of these two paradigms has strengths and limitations, and has\nbeen successfully applied to real world domains that are appropriate to its\ncorresponding strengths. In this paper, we present a new approach aimed at\nbridging the gap between these two paradigms. We aim to take the best of the\ntwo paradigms and combine them in an approach that is at the same time\ndata-efficient and cost-savvy. We do so by learning a probabilistic dynamics\nmodel and leveraging it as a prior for the intertwined model-free optimization.\nAs a result, our approach can exploit the generality and structure of the\ndynamics model, but is also capable of ignoring its inevitable inaccuracies, by\ndirectly incorporating the evidence provided by the direct observation of the\ncost. Preliminary results demonstrate that our approach outperforms purely\nmodel-based and model-free approaches, as well as the approach of simply\nswitching from a model-based to a model-free setting.\n",
        "published": "2017",
        "authors": [
            "Somil Bansal",
            "Roberto Calandra",
            "Kurtland Chua",
            "Sergey Levine",
            "Claire Tomlin"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.07979v3",
        "title": "Multi-task Learning with Gradient Guided Policy Specialization",
        "abstract": "  We present a method for efficient learning of control policies for multiple\nrelated robotic motor skills. Our approach consists of two stages, joint\ntraining and specialization training. During the joint training stage, a neural\nnetwork policy is trained with minimal information to disambiguate the motor\nskills. This forces the policy to learn a common representation of the\ndifferent tasks. Then, during the specialization training stage we selectively\nsplit the weights of the policy based on a per-weight metric that measures the\ndisagreement among the multiple tasks. By splitting part of the control policy,\nit can be further trained to specialize to each task. To update the control\npolicy during learning, we use Trust Region Policy Optimization with\nGeneralized Advantage Function (TRPOGAE). We propose a modification to the\ngradient update stage of TRPO to better accommodate multi-task learning\nscenarios. We evaluate our approach on three continuous motor skill learning\nproblems in simulation: 1) a locomotion task where three single legged robots\nwith considerable difference in shape and size are trained to hop forward, 2) a\nmanipulation task where three robot manipulators with different sizes and joint\ntypes are trained to reach different locations in 3D space, and 3) locomotion\nof a two-legged robot, whose range of motion of one leg is constrained in\ndifferent ways. We compare our training method to three baselines. The first\nbaseline uses only joint training for the policy, the second trains independent\npolicies for each task, and the last randomly selects weights to split. We show\nthat our approach learns more efficiently than each of the baseline methods.\n",
        "published": "2017",
        "authors": [
            "Wenhao Yu",
            "C. Karen Liu",
            "Greg Turk"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.08126v1",
        "title": "Self-supervised learning: When is fusion of the primary and secondary\n  sensor cue useful?",
        "abstract": "  Self-supervised learning (SSL) is a reliable learning mechanism in which a\nrobot enhances its perceptual capabilities. Typically, in SSL a trusted,\nprimary sensor cue provides supervised training data to a secondary sensor cue.\nIn this article, a theoretical analysis is performed on the fusion of the\nprimary and secondary cue in a minimal model of SSL. A proof is provided that\ndetermines the specific conditions under which it is favorable to perform\nfusion. In short, it is favorable when (i) the prior on the target value is\nstrong or (ii) the secondary cue is sufficiently accurate. The theoretical\nfindings are validated with computational experiments. Subsequently, a\nreal-world case study is performed to investigate if fusion in SSL is also\nbeneficial when assumptions of the minimal model are not met. In particular, a\nflying robot learns to map pressure measurements to sonar height measurements\nand then fuses the two, resulting in better height estimation. Fusion is also\nbeneficial in the opposite case, when pressure is the primary cue. The analysis\nand results are encouraging to study SSL fusion also for other robots and\nsensors.\n",
        "published": "2017",
        "authors": [
            "G. C. H. E. de Croon"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.08292v1",
        "title": "Underwater Multi-Robot Convoying using Visual Tracking by Detection",
        "abstract": "  We present a robust multi-robot convoying approach that relies on visual\ndetection of the leading agent, thus enabling target following in unstructured\n3-D environments. Our method is based on the idea of tracking-by-detection,\nwhich interleaves efficient model-based object detection with temporal\nfiltering of image-based bounding box estimation. This approach has the\nimportant advantage of mitigating tracking drift (i.e. drifting away from the\ntarget object), which is a common symptom of model-free trackers and is\ndetrimental to sustained convoying in practice. To illustrate our solution, we\ncollected extensive footage of an underwater robot in ocean settings, and\nhand-annotated its location in each frame. Based on this dataset, we present an\nempirical comparison of multiple tracker variants, including the use of several\nconvolutional neural networks, both with and without recurrent connections, as\nwell as frequency-based model-free trackers. We also demonstrate the\npracticality of this tracking-by-detection strategy in real-world scenarios by\nsuccessfully controlling a legged underwater robot in five degrees of freedom\nto follow another robot's independent motion.\n",
        "published": "2017",
        "authors": [
            "Florian Shkurti",
            "Wei-Di Chang",
            "Peter Henderson",
            "Md Jahidul Islam",
            "Juan Camilo Gamboa Higuera",
            "Jimmy Li",
            "Travis Manderson",
            "Anqi Xu",
            "Gregory Dudek",
            "Junaed Sattar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.08430v1",
        "title": "Towards continuous control of flippers for a multi-terrain robot using\n  deep reinforcement learning",
        "abstract": "  In this paper we focus on developing a control algorithm for multi-terrain\ntracked robots with flippers using a reinforcement learning (RL) approach. The\nwork is based on the deep deterministic policy gradient (DDPG) algorithm,\nproven to be very successful in simple simulation environments. The algorithm\nworks in an end-to-end fashion in order to control the continuous position of\nthe flippers. This end-to-end approach makes it easy to apply the controller to\na wide array of circumstances, but the huge flexibility comes to the cost of an\nincreased difficulty of solution. The complexity of the task is enlarged even\nmore by the fact that real multi-terrain robots move in partially observable\nenvironments. Notwithstanding these complications, being able to smoothly\ncontrol a multi-terrain robot can produce huge benefits in impaired people\ndaily lives or in search and rescue situations.\n",
        "published": "2017",
        "authors": [
            "Giuseppe Paolo",
            "Lei Tai",
            "Ming Liu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.10087v2",
        "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning\n  and Demonstrations",
        "abstract": "  Dexterous multi-fingered hands are extremely versatile and provide a generic\nway to perform a multitude of tasks in human-centric environments. However,\neffectively controlling them remains challenging due to their high\ndimensionality and large number of potential contacts. Deep reinforcement\nlearning (DRL) provides a model-agnostic approach to control complex dynamical\nsystems, but has not been shown to scale to high-dimensional dexterous\nmanipulation. Furthermore, deployment of DRL on physical systems remains\nchallenging due to sample inefficiency. Consequently, the success of DRL in\nrobotics has thus far been limited to simpler manipulators and tasks. In this\nwork, we show that model-free DRL can effectively scale up to complex\nmanipulation tasks with a high-dimensional 24-DoF hand, and solve them from\nscratch in simulated experiments. Furthermore, with the use of a small number\nof human demonstrations, the sample complexity can be significantly reduced,\nwhich enables learning with sample sizes equivalent to a few hours of robot\nexperience. The use of demonstrations result in policies that exhibit very\nnatural movements and, surprisingly, are also substantially more robust.\n",
        "published": "2017",
        "authors": [
            "Aravind Rajeswaran",
            "Vikash Kumar",
            "Abhishek Gupta",
            "Giulia Vezzani",
            "John Schulman",
            "Emanuel Todorov",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1709.10489v3",
        "title": "Self-supervised Deep Reinforcement Learning with Generalized Computation\n  Graphs for Robot Navigation",
        "abstract": "  Enabling robots to autonomously navigate complex environments is essential\nfor real-world deployment. Prior methods approach this problem by having the\nrobot maintain an internal map of the world, and then use a localization and\nplanning method to navigate through the internal map. However, these approaches\noften include a variety of assumptions, are computationally intensive, and do\nnot learn from failures. In contrast, learning-based methods improve as the\nrobot acts in the environment, but are difficult to deploy in the real-world\ndue to their high sample complexity. To address the need to learn complex\npolicies with few samples, we propose a generalized computation graph that\nsubsumes value-based model-free methods and model-based methods, with specific\ninstantiations interpolating between model-free and model-based. We then\ninstantiate this graph to form a navigation model that learns from raw images\nand is sample efficient. Our simulated car experiments explore the design\ndecisions of our navigation model, and show our approach outperforms\nsingle-step and $N$-step double Q-learning. We also evaluate our approach on a\nreal-world RC car and show it can learn to navigate through a complex indoor\nenvironment with a few hours of fully autonomous, self-supervised training.\nVideos of the experiments and code can be found at github.com/gkahn13/gcg\n",
        "published": "2017",
        "authors": [
            "Gregory Kahn",
            "Adam Villaflor",
            "Bosen Ding",
            "Pieter Abbeel",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.00269v2",
        "title": "On Training Flexible Robots using Deep Reinforcement Learning",
        "abstract": "  The use of robotics in controlled environments has flourished over the last\nseveral decades and training robots to perform tasks using control strategies\ndeveloped from dynamical models of their hardware have proven very effective.\nHowever, in many real-world settings, the uncertainties of the environment, the\nsafety requirements and generalized capabilities that are expected of robots\nmake rigid industrial robots unsuitable. This created great research interest\ninto developing control strategies for flexible robot hardware for which\nbuilding dynamical models are challenging. In this paper, inspired by the\nsuccess of deep reinforcement learning (DRL) in other areas, we systematically\nstudy the efficacy of policy search methods using DRL in training flexible\nrobots. Our results indicate that DRL is successfully able to learn efficient\nand robust policies for complex tasks at various degrees of flexibility. We\nalso note that DRL using Deep Deterministic Policy Gradients can be sensitive\nto the choice of sensors and adding more informative sensors does not\nnecessarily make the task easier to learn.\n",
        "published": "2019",
        "authors": [
            "Zach Dwiel",
            "Madhavun Candadai",
            "Mariano Phielipp"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.01180v1",
        "title": "Conservative Q-Improvement: Reinforcement Learning for an Interpretable\n  Decision-Tree Policy",
        "abstract": "  There is a growing desire in the field of reinforcement learning (and machine\nlearning in general) to move from black-box models toward more \"interpretable\nAI.\" We improve interpretability of reinforcement learning by increasing the\nutility of decision tree policies learned via reinforcement learning. These\npolicies consist of a decision tree over the state space, which requires fewer\nparameters to express than traditional policy representations. Existing methods\nfor creating decision tree policies via reinforcement learning focus on\naccurately representing an action-value function during training, but this\nleads to much larger trees than would otherwise be required. To address this\nshortcoming, we propose a novel algorithm which only increases tree size when\nthe estimated discounted future reward of the overall policy would increase by\na sufficient amount. Through evaluation in a simulated environment, we show\nthat its performance is comparable or superior to traditional tree-based\napproaches and that it yields a more succinct policy. Additionally, we discuss\ntuning parameters to control the tradeoff between optimizing for smaller tree\nsize or for overall reward.\n",
        "published": "2019",
        "authors": [
            "Aaron M. Roth",
            "Nicholay Topin",
            "Pooyan Jamshidi",
            "Manuela Veloso"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.03423v7",
        "title": "On-Policy Robot Imitation Learning from a Converging Supervisor",
        "abstract": "  Existing on-policy imitation learning algorithms, such as DAgger, assume\naccess to a fixed supervisor. However, there are many settings where the\nsupervisor may evolve during policy learning, such as a human performing a\nnovel task or an improving algorithmic controller. We formalize imitation\nlearning from a \"converging supervisor\" and provide sublinear static and\ndynamic regret guarantees against the best policy in hindsight with labels from\nthe converged supervisor, even when labels during learning are only from\nintermediate supervisors. We then show that this framework is closely connected\nto a class of reinforcement learning (RL) algorithms known as dual policy\niteration (DPI), which alternate between training a reactive learner with\nimitation learning and a model-based supervisor with data from the learner.\nExperiments suggest that when this framework is applied with the\nstate-of-the-art deep model-based RL algorithm PETS as an improving supervisor,\nit outperforms deep RL baselines on continuous control tasks and provides up to\nan 80-fold speedup in policy evaluation.\n",
        "published": "2019",
        "authors": [
            "Ashwin Balakrishna",
            "Brijen Thananjeyan",
            "Jonathan Lee",
            "Felix Li",
            "Arsh Zahed",
            "Joseph E. Gonzalez",
            "Ken Goldberg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.03613v2",
        "title": "Data Efficient Reinforcement Learning for Legged Robots",
        "abstract": "  We present a model-based framework for robot locomotion that achieves walking\nbased on only 4.5 minutes (45,000 control steps) of data collected on a\nquadruped robot. To accurately model the robot's dynamics over a long horizon,\nwe introduce a loss function that tracks the model's prediction over multiple\ntimesteps. We adapt model predictive control to account for planning latency,\nwhich allows the learned model to be used for real time control. Additionally,\nto ensure safe exploration during model learning, we embed prior knowledge of\nleg trajectories into the action space. The resulting system achieves fast and\nrobust locomotion. Unlike model-free methods, which optimize for a particular\ntask, our planner can use the same learned dynamics for various tasks, simply\nby changing the reward function. To the best of our knowledge, our approach is\nmore than an order of magnitude more sample efficient than current model-free\nmethods.\n",
        "published": "2019",
        "authors": [
            "Yuxiang Yang",
            "Ken Caluwaerts",
            "Atil Iscen",
            "Tingnan Zhang",
            "Jie Tan",
            "Vikas Sindhwani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.04457v1",
        "title": "Partially Observable Planning and Learning for Systems with Non-Uniform\n  Dynamics",
        "abstract": "  We propose a neural network architecture, called TransNet, that combines\nplanning and model learning for solving Partially Observable Markov Decision\nProcesses (POMDPs) with non-uniform system dynamics. The past decade has seen a\nsubstantial advancement in solving POMDP problems. However, constructing a\nsuitable POMDP model remains difficult. Recently, neural network architectures\nhave been proposed to alleviate the difficulty in acquiring such models.\nAlthough the results are promising, existing architectures restrict the type of\nsystem dynamics that can be learned --that is, system dynamics must be the same\nin all parts of the state space. TransNet relaxes such a restriction. Key to\nthis relaxation is a novel neural network module that classifies the state\nspace into classes and then learns the system dynamics of the different\nclasses. TransNet uses this module together with the overall architecture of\nQMDP-Net[1] to allow solving POMDPs that have more expressive dynamic models,\nwhile maintaining efficient data requirement. Its evaluation on typical\nbenchmarks in robot navigation with initially unknown system and environment\nmodels indicates that TransNet substantially out-performs the quality of the\ngenerated policies and learning efficiency of the state-of-the-art method\nQMDP-Net.\n",
        "published": "2019",
        "authors": [
            "Nicholas Collins",
            "Hanna Kurniawati"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.04799v2",
        "title": "RL-RRT: Kinodynamic Motion Planning via Learning Reachability Estimators\n  from RL Policies",
        "abstract": "  This paper addresses two challenges facing sampling-based kinodynamic motion\nplanning: a way to identify good candidate states for local transitions and the\nsubsequent computationally intractable steering between these candidate states.\nThrough the combination of sampling-based planning, a Rapidly Exploring\nRandomized Tree (RRT) and an efficient kinodynamic motion planner through\nmachine learning, we propose an efficient solution to long-range planning for\nkinodynamic motion planning. First, we use deep reinforcement learning to learn\nan obstacle-avoiding policy that maps a robot's sensor observations to actions,\nwhich is used as a local planner during planning and as a controller during\nexecution. Second, we train a reachability estimator in a supervised manner,\nwhich predicts the RL policy's time to reach a state in the presence of\nobstacles. Lastly, we introduce RL-RRT that uses the RL policy as a local\nplanner, and the reachability estimator as the distance function to bias\ntree-growth towards promising regions. We evaluate our method on three\nkinodynamic systems, including physical robot experiments. Results across all\nthree robots tested indicate that RL-RRT outperforms state of the art\nkinodynamic planners in efficiency, and also provides a shorter path finish\ntime than a steering function free method. The learned local planner policy and\naccompanying reachability estimator demonstrate transferability to the\npreviously unseen experimental environments, making RL-RRT fast because the\nexpensive computations are replaced with simple neural network inference.\nVideo: https://youtu.be/dDMVMTOI8KY\n",
        "published": "2019",
        "authors": [
            "Hao-Tien Lewis Chiang",
            "Jasmine Hsu",
            "Marek Fiser",
            "Lydia Tapia",
            "Aleksandra Faust"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.05246v2",
        "title": "Deep Reinforcement-Learning-based Driving Policy for Autonomous Road\n  Vehicles",
        "abstract": "  In this work the problem of path planning for an autonomous vehicle that\nmoves on a freeway is considered. The most common approaches that are used to\naddress this problem are based on optimal control methods, which make\nassumptions about the model of the environment and the system dynamics. On the\ncontrary, this work proposes the development of a driving policy based on\nreinforcement learning. In this way, the proposed driving policy makes minimal\nor no assumptions about the environment, since a priori knowledge about the\nsystem dynamics is not required. Driving scenarios where the road is occupied\nboth by autonomous and manual driving vehicles are considered. To the best of\nour knowledge, this is one of the first approaches that propose a reinforcement\nlearning driving policy for mixed driving environments. The derived\nreinforcement learning policy, firstly, is compared against an optimal policy\nderived via dynamic programming, and, secondly, its efficiency is evaluated\nunder realistic scenarios generated by the established SUMO microscopic traffic\nflow simulator. Finally, some initial results regarding the effect of\nautonomous vehicles' behavior on the overall traffic flow are presented.\n",
        "published": "2019",
        "authors": [
            "Konstantinos Makantasis",
            "Maria Kontorinaki",
            "Ioannis Nikolos"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.06013v3",
        "title": "Motion Planning Networks: Bridging the Gap Between Learning-based and\n  Classical Motion Planners",
        "abstract": "  This paper describes Motion Planning Networks (MPNet), a computationally\nefficient, learning-based neural planner for solving motion planning problems.\nMPNet uses neural networks to learn general near-optimal heuristics for path\nplanning in seen and unseen environments. It takes environment information such\nas raw point-cloud from depth sensors, as well as a robot's initial and desired\ngoal configurations and recursively calls itself to bidirectionally generate\nconnectable paths. In addition to finding directly connectable and near-optimal\npaths in a single pass, we show that worst-case theoretical guarantees can be\nproven if we merge this neural network strategy with classical sample-based\nplanners in a hybrid approach while still retaining significant computational\nand optimality improvements. To train the MPNet models, we present an active\ncontinual learning approach that enables MPNet to learn from streaming data and\nactively ask for expert demonstrations when needed, drastically reducing data\nfor training. We validate MPNet against gold-standard and state-of-the-art\nplanning methods in a variety of problems from 2D to 7D robot configuration\nspaces in challenging and cluttered environments, with results showing\nsignificant and consistently stronger performance metrics, and motivating\nneural planning in general as a modern strategy for solving motion planning\nproblems efficiently.\n",
        "published": "2019",
        "authors": [
            "Ahmed H. Qureshi",
            "Yinglong Miao",
            "Anthony Simeonov",
            "Michael C. Yip"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.07500v2",
        "title": "Learning Variable Impedance Control for Contact Sensitive Tasks",
        "abstract": "  Reinforcement learning algorithms have shown great success in solving\ndifferent problems ranging from playing video games to robotics. However, they\nstruggle to solve delicate robotic problems, especially those involving contact\ninteractions. Though in principle a policy directly outputting joint torques\nshould be able to learn to perform these tasks, in practice we see that it has\ndifficulty to robustly solve the problem without any given structure in the\naction space. In this paper, we investigate how the choice of action space can\ngive robust performance in presence of contact uncertainties. We propose\nlearning a policy giving as output impedance and desired position in joint\nspace and compare the performance of that approach to torque and position\ncontrol under different contact uncertainties. Furthermore, we propose an\nadditional reward term designed to regularize these variable impedance control\npolicies, giving them interpretability and facilitating their transfer to real\nsystems. We present extensive experiments in simulation of both floating and\nfixed-base systems in tasks involving contact uncertainties, as well as results\nfor running the learned policies on a real system.\n",
        "published": "2019",
        "authors": [
            "Miroslav Bogdanovic",
            "Majid Khadiv",
            "Ludovic Righetti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.09620v3",
        "title": "Rapid trial-and-error learning with simulation supports flexible tool\n  use and physical reasoning",
        "abstract": "  Many animals, and an increasing number of artificial agents, display\nsophisticated capabilities to perceive and manipulate objects. But human beings\nremain distinctive in their capacity for flexible, creative tool use -- using\nobjects in new ways to act on the world, achieve a goal, or solve a problem. To\nstudy this type of general physical problem solving, we introduce the Virtual\nTools game. In this game, people solve a large range of challenging physical\npuzzles in just a handful of attempts. We propose that the flexibility of human\nphysical problem solving rests on an ability to imagine the effects of\nhypothesized actions, while the efficiency of human search arises from rich\naction priors which are updated via observations of the world. We instantiate\nthese components in the \"Sample, Simulate, Update\" (SSUP) model and show that\nit captures human performance across 30 levels of the Virtual Tools game. More\nbroadly, this model provides a mechanism for explaining how people condense\ngeneral physical knowledge into actionable, task-specific plans to achieve\nflexible and efficient physical problem-solving.\n",
        "published": "2019",
        "authors": [
            "Kelsey R. Allen",
            "Kevin A. Smith",
            "Joshua B. Tenenbaum"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1907.11740v1",
        "title": "Environment Probing Interaction Policies",
        "abstract": "  A key challenge in reinforcement learning (RL) is environment generalization:\na policy trained to solve a task in one environment often fails to solve the\nsame task in a slightly different test environment. A common approach to\nimprove inter-environment transfer is to learn policies that are invariant to\nthe distribution of testing environments. However, we argue that instead of\nbeing invariant, the policy should identify the specific nuances of an\nenvironment and exploit them to achieve better performance. In this work, we\npropose the 'Environment-Probing' Interaction (EPI) policy, a policy that\nprobes a new environment to extract an implicit understanding of that\nenvironment's behavior. Once this environment-specific information is obtained,\nit is used as an additional input to a task-specific policy that can now\nperform environment-conditioned actions to solve a task. To learn these\nEPI-policies, we present a reward function based on transition predictability.\nSpecifically, a higher reward is given if the trajectory generated by the\nEPI-policy can be used to better predict transitions. We experimentally show\nthat EPI-conditioned task-specific policies significantly outperform commonly\nused policy generalization methods on novel testing environments.\n",
        "published": "2019",
        "authors": [
            "Wenxuan Zhou",
            "Lerrel Pinto",
            "Abhinav Gupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.01618v1",
        "title": "Speech Driven Backchannel Generation using Deep Q-Network for Enhancing\n  Engagement in Human-Robot Interaction",
        "abstract": "  We present a novel method for training a social robot to generate\nbackchannels during human-robot interaction. We address the problem within an\noff-policy reinforcement learning framework, and show how a robot may learn to\nproduce non-verbal backchannels like laughs, when trained to maximize the\nengagement and attention of the user. A major contribution of this work is the\nformulation of the problem as a Markov decision process (MDP) with states\ndefined by the speech activity of the user and rewards generated by quantified\nengagement levels. The problem that we address falls into the class of\napplications where unlimited interaction with the environment is not possible\n(our environment being a human) because it may be time-consuming, costly,\nimpracticable or even dangerous in case a bad policy is executed. Therefore, we\nintroduce deep Q-network (DQN) in a batch reinforcement learning framework,\nwhere an optimal policy is learned from a batch data collected using a more\ncontrolled policy. We suggest the use of human-to-human dyadic interaction\ndatasets as a batch of trajectories to train an agent for engaging\ninteractions. Our experiments demonstrate the potential of our method to train\na robot for engaging behaviors in an offline manner.\n",
        "published": "2019",
        "authors": [
            "Nusrah Hussain",
            "Engin Erzin",
            "T. Metin Sezgin",
            "Yucel Yemez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.01887v4",
        "title": "DoorGym: A Scalable Door Opening Environment And Baseline Agent",
        "abstract": "  In order to practically implement the door opening task, a policy ought to be\nrobust to a wide distribution of door types and environment settings.\nReinforcement Learning (RL) with Domain Randomization (DR) is a promising\ntechnique to enforce policy generalization, however, there are only a few\naccessible training environments that are inherently designed to train agents\nin domain randomized environments. We introduce DoorGym, an open-source door\nopening simulation framework designed to utilize domain randomization to train\na stable policy. We intend for our environment to lie at the intersection of\ndomain transfer, practical tasks, and realism. We also provide baseline\nProximal Policy Optimization and Soft Actor-Critic implementations, which\nachieves success rates between 0% up to 95% for opening various types of doors\nin this environment. Moreover, the real-world transfer experiment shows the\ntrained policy is able to work in the real world. Environment kit available\nhere: https://github.com/PSVL/DoorGym/\n",
        "published": "2019",
        "authors": [
            "Yusuke Urakami",
            "Alec Hodgkinson",
            "Casey Carlin",
            "Randall Leu",
            "Luca Rigazio",
            "Pieter Abbeel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.03343v1",
        "title": "Fully Convolutional Search Heuristic Learning for Rapid Path Planners",
        "abstract": "  Path-planning algorithms are an important part of a wide variety of robotic\napplications, such as mobile robot navigation and robot arm manipulation.\nHowever, in large search spaces in which local traps may exist, it remains\nchallenging to reliably find a path while satisfying real-time constraints.\nEfforts to speed up the path search have led to the development of many\npractical path-planning algorithms. These algorithms often define a search\nheuristic to guide the search towards the goal. The heuristics should be\ncarefully designed for each specific problem to ensure reliability in the\nvarious situations encountered in the problem. However, it is often difficult\nfor humans to craft such robust heuristics, and the search performance often\ndegrades under conditions that violate the heuristic assumption. Rather than\nmanually designing the heuristics, in this work, we propose a learning approach\nto acquire these search heuristics. Our method represents the environment\ncontaining the obstacles as an image, and this image is fed into fully\nconvolutional neural networks to produce a search heuristic image where every\npixel represents a heuristic value (cost-to-go value to a goal) in the form of\na vertex of a search graph. Training the heuristic is performed using\npreviously collected planning results. Our preliminary experiments (2D grid\nworld navigation experiments) demonstrate significant reduction in the search\ncosts relative to a hand-designed heuristic.\n",
        "published": "2019",
        "authors": [
            "Yuka Ariki",
            "Takuya Narihira"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.03731v1",
        "title": "Learning to Explore in Motion and Interaction Tasks",
        "abstract": "  Model free reinforcement learning suffers from the high sampling complexity\ninherent to robotic manipulation or locomotion tasks. Most successful\napproaches typically use random sampling strategies which leads to slow policy\nconvergence. In this paper we present a novel approach for efficient\nexploration that leverages previously learned tasks. We exploit the fact that\nthe same system is used across many tasks and build a generative model for\nexploration based on data from previously solved tasks to improve learning new\ntasks. The approach also enables continuous learning of improved exploration\nstrategies as novel tasks are learned. Extensive simulations on a robot\nmanipulator performing a variety of motion and contact interaction tasks\ndemonstrate the capabilities of the approach. In particular, our experiments\nsuggest that the exploration strategy can more than double learning speed,\nespecially when rewards are sparse. Moreover, the algorithm is robust to task\nvariations and parameter tuning, making it beneficial for complex robotic\nproblems.\n",
        "published": "2019",
        "authors": [
            "Miroslav Bogdanovic",
            "Ludovic Righetti"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.05224v2",
        "title": "Multi-Agent Manipulation via Locomotion using Hierarchical Sim2Real",
        "abstract": "  Manipulation and locomotion are closely related problems that are often\nstudied in isolation. In this work, we study the problem of coordinating\nmultiple mobile agents to exhibit manipulation behaviors using a reinforcement\nlearning (RL) approach. Our method hinges on the use of hierarchical sim2real\n-- a simulated environment is used to learn low-level goal-reaching skills,\nwhich are then used as the action space for a high-level RL controller, also\ntrained in simulation. The full hierarchical policy is then transferred to the\nreal world in a zero-shot fashion. The application of domain randomization\nduring training enables the learned behaviors to generalize to real-world\nsettings, while the use of hierarchy provides a modular paradigm for learning\nand transferring increasingly complex behaviors. We evaluate our method on a\nnumber of real-world tasks, including coordinated object manipulation in a\nmulti-agent setting. See videos at\nhttps://sites.google.com/view/manipulation-via-locomotion\n",
        "published": "2019",
        "authors": [
            "Ofir Nachum",
            "Michael Ahn",
            "Hugo Ponte",
            "Shixiang Gu",
            "Vikash Kumar"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.06769v2",
        "title": "Continuous Relaxation of Symbolic Planner for One-Shot Imitation\n  Learning",
        "abstract": "  We address one-shot imitation learning, where the goal is to execute a\npreviously unseen task based on a single demonstration. While there has been\nexciting progress in this direction, most of the approaches still require a few\nhundred tasks for meta-training, which limits the scalability of the\napproaches. Our main contribution is to formulate one-shot imitation learning\nas a symbolic planning problem along with the symbol grounding problem. This\nformulation disentangles the policy execution from the inter-task\ngeneralization and leads to better data efficiency. The key technical challenge\nis that the symbol grounding is prone to error with limited training data and\nleads to subsequent symbolic planning failures. We address this challenge by\nproposing a continuous relaxation of the discrete symbolic planner that\ndirectly plans on the probabilistic outputs of the symbol grounding model. Our\ncontinuous relaxation of the planner can still leverage the information\ncontained in the probabilistic symbol grounding and significantly improve over\nthe baseline planner for the one-shot imitation learning tasks without using\nlarge training data.\n",
        "published": "2019",
        "authors": [
            "De-An Huang",
            "Danfei Xu",
            "Yuke Zhu",
            "Animesh Garg",
            "Silvio Savarese",
            "Li Fei-Fei",
            "Juan Carlos Niebles"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.07088v4",
        "title": "Adaptive Robot-Assisted Feeding: An Online Learning Framework for\n  Acquiring Previously Unseen Food Items",
        "abstract": "  A successful robot-assisted feeding system requires bite acquisition of a\nwide variety of food items. It must adapt to changing user food preferences\nunder uncertain visual and physical environments. Different food items in\ndifferent environmental conditions require different manipulation strategies\nfor successful bite acquisition. Therefore, a key challenge is how to handle\npreviously unseen food items with very different success rate distributions\nover strategy. Combining low-level controllers and planners into discrete\naction trajectories, we show that the problem can be represented using a linear\ncontextual bandit setting. We construct a simulated environment using a doubly\nrobust loss estimate from previously seen food items, which we use to tune the\nparameters of off-the-shelf contextual bandit algorithms. Finally, we\ndemonstrate empirically on a robot-assisted feeding system that, even starting\nwith a model trained on thousands of skewering attempts on dissimilar\npreviously seen food items, $\\epsilon$-greedy and LinUCB algorithms can quickly\nconverge to the most successful manipulation strategy.\n",
        "published": "2019",
        "authors": [
            "Ethan K. Gordon",
            "Xiang Meng",
            "Matt Barnes",
            "Tapomayukh Bhattacharjee",
            "Siddhartha S. Srinivasa"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.07931v1",
        "title": "Design Space of Behaviour Planning for Autonomous Driving",
        "abstract": "  We explore the complex design space of behaviour planning for autonomous\ndriving. Design choices that successfully address one aspect of behaviour\nplanning can critically constrain others. To aid the design process, in this\nwork we decompose the design space with respect to important choices arising\nfrom the current state of the art approaches, and describe the resulting\ntrade-offs. In doing this, we also identify interesting directions of future\nwork.\n",
        "published": "2019",
        "authors": [
            "Marko Ilievski",
            "Sean Sedwards",
            "Ashish Gaurav",
            "Aravind Balakrishnan",
            "Atrisha Sarkar",
            "Jaeyoung Lee",
            "Fr\u00e9d\u00e9ric Bouchard",
            "Ryan De Iaco",
            "Krzysztof Czarnecki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.09171v3",
        "title": "Planning Beyond the Sensing Horizon Using a Learned Context",
        "abstract": "  Last-mile delivery systems commonly propose the use of autonomous robotic\nvehicles to increase scalability and efficiency. The economic inefficiency of\ncollecting accurate prior maps for navigation motivates the use of planning\nalgorithms that operate in unmapped environments. However, these algorithms\ntypically waste time exploring regions that are unlikely to contain the\ndelivery destination. Context is key information about structured environments\nthat could guide exploration toward the unknown goal location, but the abstract\nidea is difficult to quantify for use in a planning algorithm. Some approaches\nspecifically consider contextual relationships between objects, but would\nperform poorly in object-sparse environments like outdoors. Recent deep\nlearning-based approaches consider context too generally, making\ntraining/transferability difficult. Therefore, this work proposes a novel\nformulation of utilizing context for planning as an image-to-image translation\nproblem, which is shown to extract terrain context from semantic gridmaps, into\na metric that an exploration-based planner can use. The proposed framework has\nthe benefit of training on a static dataset instead of requiring a\ntime-consuming simulator. Across 42 test houses with layouts from satellite\nimages, the trained algorithm enables a robot to reach its goal 189\\% faster\nthan with a context-unaware planner, and within 63\\% of the optimal path\ncomputed with a prior map. The proposed algorithm is also implemented on a\nvehicle with a forward-facing camera in a high-fidelity, Unreal simulation of\nneighborhood houses.\n",
        "published": "2019",
        "authors": [
            "Michael Everett",
            "Justin Miller",
            "Jonathan P. How"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.10010v2",
        "title": "Research on Autonomous Maneuvering Decision of UCAV based on Approximate\n  Dynamic Programming",
        "abstract": "  Unmanned aircraft systems can perform some more dangerous and difficult\nmissions than manned aircraft systems. In some highly complicated and\nchangeable tasks, such as air combat, the maneuvering decision mechanism is\nrequired to sense the combat situation accurately and make the optimal strategy\nin real-time. This paper presents a formulation of a 3-D one-on-one air combat\nmaneuvering problem and an approximate dynamic programming approach for\ncomputing an optimal policy on autonomous maneuvering decision making. The\naircraft learns combat strategies in a Reinforcement Leaning method, while\nsensing the environment, taking available maneuvering actions and getting\nfeedback reward signals. To solve the problem of dimensional explosion in the\nair combat, the proposed method is implemented through feature selection,\ntrajectory sampling, function approximation and Bellman backup operation in the\nair combat simulation environment. This approximate dynamic programming\napproach provides a fast response to a rapidly changing tactical situation,\nlearns in long-term planning, without any explicitly coded air combat rule\nbase.\n",
        "published": "2019",
        "authors": [
            "Zhencai Hu",
            "Peng Gao",
            "Fei Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.10125v1",
        "title": "Proactive Intention Recognition for Joint Human-Robot Search and Rescue\n  Missions through Monte-Carlo Planning in POMDP Environments",
        "abstract": "  Proactively perceiving others' intentions is a crucial skill to effectively\ninteract in unstructured, dynamic and novel environments. This work proposes a\nfirst step towards embedding this skill in support robots for search and rescue\nmissions. Predicting the responders' intentions, indeed, will enable\nexploration approaches which will identify and prioritise areas that are more\nrelevant for the responder and, thus, for the task, leading to the development\nof safer, more robust and efficient joint exploration strategies. More\nspecifically, this paper presents an active intention recognition paradigm to\nperceive, even under sensory constraints, not only the target's position but\nalso the first responder's movements, which can provide information on his/her\nintentions (e.g. reaching the position where he/she expects the target to be).\nThis mechanism is implemented by employing an extension of Monte-Carlo-based\nplanning techniques for partially observable environments, where the reward\nfunction is augmented with an entropy reduction bonus. We test in simulation\nseveral configurations of reward augmentation, both information theoretic and\nnot, as well as belief state approximations and obtain substantial improvements\nover the basic approach.\n",
        "published": "2019",
        "authors": [
            "Dimitri Ognibene",
            "Lorenzo Mirante",
            "Letizia Marchegiani"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1908.10398v1",
        "title": "A Data-Efficient Deep Learning Approach for Deployable Multimodal Social\n  Robots",
        "abstract": "  The deep supervised and reinforcement learning paradigms (among others) have\nthe potential to endow interactive multimodal social robots with the ability of\nacquiring skills autonomously. But it is still not very clear yet how they can\nbe best deployed in real world applications. As a step in this direction, we\npropose a deep learning-based approach for efficiently training a humanoid\nrobot to play multimodal games---and use the game of `Noughts & Crosses' with\ntwo variants as a case study. Its minimum requirements for learning to perceive\nand interact are based on a few hundred example images, a few example\nmultimodal dialogues and physical demonstrations of robot manipulation, and\nautomatic simulations. In addition, we propose novel algorithms for robust\nvisual game tracking and for competitive policy learning with high winning\nrates, which substantially outperform DQN-based baselines. While an automatic\nevaluation shows evidence that the proposed approach can be easily extended to\nnew games with competitive robot behaviours, a human evaluation with 130 humans\nplaying with the Pepper robot confirms that highly accurate visual perception\nis required for successful game play.\n",
        "published": "2019",
        "authors": [
            "Heriberto Cuay\u00e1huitl"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1406.2616v3",
        "title": "PlanIt: A Crowdsourcing Approach for Learning to Plan Paths from Large\n  Scale Preference Feedback",
        "abstract": "  We consider the problem of learning user preferences over robot trajectories\nfor environments rich in objects and humans. This is challenging because the\ncriterion defining a good trajectory varies with users, tasks and interactions\nin the environment. We represent trajectory preferences using a cost function\nthat the robot learns and uses it to generate good trajectories in new\nenvironments. We design a crowdsourcing system - PlanIt, where non-expert users\nlabel segments of the robot's trajectory. PlanIt allows us to collect a large\namount of user feedback, and using the weak and noisy labels from PlanIt we\nlearn the parameters of our model. We test our approach on 122 different\nenvironments for robotic navigation and manipulation tasks. Our extensive\nexperiments show that the learned cost function generates preferred\ntrajectories in human environments. Our crowdsourcing system is publicly\navailable for the visualization of the learned costs and for providing\npreference feedback: \\url{http://planit.cs.cornell.edu}\n",
        "published": "2014",
        "authors": [
            "Ashesh Jain",
            "Debarghya Das",
            "Jayesh K Gupta",
            "Ashutosh Saxena"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1504.03071v2",
        "title": "Robobarista: Object Part based Transfer of Manipulation Trajectories\n  from Crowd-sourcing in 3D Pointclouds",
        "abstract": "  There is a large variety of objects and appliances in human environments,\nsuch as stoves, coffee dispensers, juice extractors, and so on. It is\nchallenging for a roboticist to program a robot for each of these object types\nand for each of their instantiations. In this work, we present a novel approach\nto manipulation planning based on the idea that many household objects share\nsimilarly-operated object parts. We formulate the manipulation planning as a\nstructured prediction problem and design a deep learning model that can handle\nlarge noise in the manipulation demonstrations and learns features from three\ndifferent modalities: point-clouds, language and trajectory. In order to\ncollect a large number of manipulation demonstrations for different objects, we\ndeveloped a new crowd-sourcing platform called Robobarista. We test our model\non our dataset consisting of 116 objects with 249 parts along with 250 language\ninstructions, for which there are 1225 crowd-sourced manipulation\ndemonstrations. We further show that our robot can even manipulate objects it\nhas never seen before.\n",
        "published": "2015",
        "authors": [
            "Jaeyong Sung",
            "Seok Hyun Jin",
            "Ashutosh Saxena"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1504.05811v1",
        "title": "Learning of Behavior Trees for Autonomous Agents",
        "abstract": "  Definition of an accurate system model for Automated Planner (AP) is often\nimpractical, especially for real-world problems. Conversely, off-the-shelf\nplanners fail to scale up and are domain dependent. These drawbacks are\ninherited from conventional transition systems such as Finite State Machines\n(FSMs) that describes the action-plan execution generated by the AP. On the\nother hand, Behavior Trees (BTs) represent a valid alternative to FSMs\npresenting many advantages in terms of modularity, reactiveness, scalability\nand domain-independence. In this paper, we propose a model-free AP framework\nusing Genetic Programming (GP) to derive an optimal BT for an autonomous agent\nto achieve a given goal in unknown (but fully observable) environments. We\nillustrate the proposed framework using experiments conducted with an open\nsource benchmark Mario AI for automated generation of BTs that can play the\ngame character Mario to complete a certain level at various levels of\ndifficulty to include enemies and obstacles.\n",
        "published": "2015",
        "authors": [
            "Michele Colledanchise",
            "Ramviyas Parasuraman",
            "Petter \u00d6gren"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1604.06778v3",
        "title": "Benchmarking Deep Reinforcement Learning for Continuous Control",
        "abstract": "  Recently, researchers have made significant progress combining the advances\nin deep learning for learning feature representations with reinforcement\nlearning. Some notable examples include training agents to play Atari games\nbased on raw pixel data and to acquire advanced manipulation skills using raw\nsensory inputs. However, it has been difficult to quantify progress in the\ndomain of continuous control due to the lack of a commonly adopted benchmark.\nIn this work, we present a benchmark suite of continuous control tasks,\nincluding classic tasks like cart-pole swing-up, tasks with very high state and\naction dimensionality such as 3D humanoid locomotion, tasks with partial\nobservations, and tasks with hierarchical structure. We report novel findings\nbased on the systematic evaluation of a range of implemented reinforcement\nlearning algorithms. Both the benchmark and reference implementations are\nreleased at https://github.com/rllab/rllab in order to facilitate experimental\nreproducibility and to encourage adoption by other researchers.\n",
        "published": "2016",
        "authors": [
            "Yan Duan",
            "Xi Chen",
            "Rein Houthooft",
            "John Schulman",
            "Pieter Abbeel"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1712.01106v1",
        "title": "Transferring Autonomous Driving Knowledge on Simulated and Real\n  Intersections",
        "abstract": "  We view intersection handling on autonomous vehicles as a reinforcement\nlearning problem, and study its behavior in a transfer learning setting. We\nshow that a network trained on one type of intersection generally is not able\nto generalize to other intersections. However, a network that is pre-trained on\none intersection and fine-tuned on another performs better on the new task\ncompared to training in isolation. This network also retains knowledge of the\nprior task, even though some forgetting occurs. Finally, we show that the\nbenefits of fine-tuning hold when transferring simulated intersection handling\nknowledge to a real autonomous vehicle.\n",
        "published": "2017",
        "authors": [
            "David Isele",
            "Akansel Cosgun"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1712.03632v1",
        "title": "Robust Deep Reinforcement Learning with Adversarial Attacks",
        "abstract": "  This paper proposes adversarial attacks for Reinforcement Learning (RL) and\nthen improves the robustness of Deep Reinforcement Learning algorithms (DRL) to\nparameter uncertainties with the help of these attacks. We show that even a\nnaively engineered attack successfully degrades the performance of DRL\nalgorithm. We further improve the attack using gradient information of an\nengineered loss function which leads to further degradation in performance.\nThese attacks are then leveraged during training to improve the robustness of\nRL within robust control framework. We show that this adversarial training of\nDRL algorithms like Deep Double Q learning and Deep Deterministic Policy\nGradients leads to significant increase in robustness to parameter variations\nfor RL benchmarks such as Cart-pole, Mountain Car, Hopper and Half Cheetah\nenvironment.\n",
        "published": "2017",
        "authors": [
            "Anay Pattanaik",
            "Zhenyi Tang",
            "Shuijing Liu",
            "Gautham Bommannan",
            "Girish Chowdhary"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.04322v1",
        "title": "Reinforcement Learning in Topology-based Representation for Human Body\n  Movement with Whole Arm Manipulation",
        "abstract": "  Moving a human body or a large and bulky object can require the strength of\nwhole arm manipulation (WAM). This type of manipulation places the load on the\nrobot's arms and relies on global properties of the interaction to\nsucceed---rather than local contacts such as grasping or non-prehensile\npushing. In this paper, we learn to generate motions that enable WAM for\nholding and transporting of humans in certain rescue or patient care scenarios.\nWe model the task as a reinforcement learning problem in order to provide a\nbehavior that can directly respond to external perturbation and human motion.\nFor this, we represent global properties of the robot-human interaction with\ntopology-based coordinates that are computed from arm and torso positions.\nThese coordinates also allow transferring the learned policy to other body\nshapes and sizes. For training and evaluation, we simulate a dynamic sea rescue\nscenario and show in quantitative experiments that the policy can solve unseen\nscenarios with differently-shaped humans, floating humans, or with perception\nnoise. Our qualitative experiments show the subsequent transporting after\nholding is achieved and we demonstrate that the policy can be directly\ntransferred to a real world setting.\n",
        "published": "2018",
        "authors": [
            "Weihao Yuan",
            "Kaiyu Hang",
            "Haoran Song",
            "Danica Kragic",
            "Michael Y. Wang",
            "Johannes A. Stork"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.04558v1",
        "title": "Coordinated Heterogeneous Distributed Perception based on Latent Space\n  Representation",
        "abstract": "  We investigate a reinforcement approach for distributed sensing based on the\nlatent space derived from multi-modal deep generative models. Our contribution\nprovides insights to the following benefits: Detections can be exchanged\neffectively between robots equipped with uni-modal sensors due to a shared\nlatent representation of information that is trained by a Variational Auto\nEncoder (VAE). Sensor-fusion can be applied asynchronously due to the\ngenerative feature of the VAE. Deep Q-Networks (DQNs) are trained to minimize\nuncertainty in latent space by coordinating robots to a Point-of-Interest (PoI)\nwhere their sensor modality can provide beneficial information about the PoI.\nAdditionally, we show that the decrease in uncertainty can be defined as the\ndirect reward signal for training the DQN.\n",
        "published": "2018",
        "authors": [
            "Timo Korthals",
            "J\u00fcrgen Leitner",
            "Ulrich R\u00fcckert"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.07004v1",
        "title": "Leveraging Contact Forces for Learning to Grasp",
        "abstract": "  Grasping objects under uncertainty remains an open problem in robotics\nresearch. This uncertainty is often due to noisy or partial observations of the\nobject pose or shape. To enable a robot to react appropriately to unforeseen\neffects, it is crucial that it continuously takes sensor feedback into account.\nWhile visual feedback is important for inferring a grasp pose and reaching for\nan object, contact feedback offers valuable information during manipulation and\ngrasp acquisition. In this paper, we use model-free deep reinforcement learning\nto synthesize control policies that exploit contact sensing to generate robust\ngrasping under uncertainty. We demonstrate our approach on a multi-fingered\nhand that exhibits more complex finger coordination than the commonly used\ntwo-fingered grippers. We conduct extensive experiments in order to assess the\nperformance of the learned policies, with and without contact sensing. While it\nis possible to learn grasping policies without contact sensing, our results\nsuggest that contact feedback allows for a significant improvement of grasping\nrobustness under object pose uncertainty and for objects with a complex shape.\n",
        "published": "2018",
        "authors": [
            "Hamza Merzic",
            "Miroslav Bogdanovic",
            "Daniel Kappler",
            "Ludovic Righetti",
            "Jeannette Bohg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.10093v2",
        "title": "Pay attention! - Robustifying a Deep Visuomotor Policy through\n  Task-Focused Attention",
        "abstract": "  Several recent studies have demonstrated the promise of deep visuomotor\npolicies for robot manipulator control. Despite impressive progress, these\nsystems are known to be vulnerable to physical disturbances, such as accidental\nor adversarial bumps that make them drop the manipulated object. They also tend\nto be distracted by visual disturbances such as objects moving in the robot's\nfield of view, even if the disturbance does not physically prevent the\nexecution of the task. In this paper, we propose an approach for augmenting a\ndeep visuomotor policy trained through demonstrations with Task Focused visual\nAttention (TFA). The manipulation task is specified with a natural language\ntext such as `move the red bowl to the left'. This allows the visual attention\ncomponent to concentrate on the current object that the robot needs to\nmanipulate. We show that even in benign environments, the TFA allows the policy\nto consistently outperform a variant with no attention mechanism. More\nimportantly, the new policy is significantly more robust: it regularly recovers\nfrom severe physical disturbances (such as bumps causing it to drop the object)\nfrom which the baseline policy, i.e. with no visual attention, almost never\nrecovers. In addition, we show that the proposed policy performs correctly in\nthe presence of a wide class of visual disturbances, exhibiting a behavior\nreminiscent of human selective visual attention experiments. Our proposed\napproach consists of a VAE-GAN network which encodes the visual input and feeds\nit to a Motor network that moves the robot joints. Also, our approach benefits\nfrom a teacher network for the TFA that leverages textual input command to\nrobustify the visual encoder against various types of disturbances.\n",
        "published": "2018",
        "authors": [
            "Pooya Abolghasemi",
            "Amir Mazaheri",
            "Mubarak Shah",
            "Ladislau B\u00f6l\u00f6ni"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.10124v2",
        "title": "Learning Navigation Behaviors End-to-End with AutoRL",
        "abstract": "  We learn end-to-end point-to-point and path-following navigation behaviors\nthat avoid moving obstacles. These policies receive noisy lidar observations\nand output robot linear and angular velocities. The policies are trained in\nsmall, static environments with AutoRL, an evolutionary automation layer around\nReinforcement Learning (RL) that searches for a deep RL reward and neural\nnetwork architecture with large-scale hyper-parameter optimization. AutoRL\nfirst finds a reward that maximizes task completion, and then finds a neural\nnetwork architecture that maximizes the cumulative of the found reward.\nEmpirical evaluations, both in simulation and on-robot, show that AutoRL\npolicies do not suffer from the catastrophic forgetfulness that plagues many\nother deep reinforcement learning algorithms, generalize to new environments\nand moving obstacles, are robust to sensor, actuator, and localization noise,\nand can serve as robust building blocks for larger navigation tasks. Our\npath-following and point-to-point policies are respectively 23% and 26% more\nsuccessful than comparison methods across new environments. Video at:\nhttps://youtu.be/0UwkjpUEcbI\n",
        "published": "2018",
        "authors": [
            "Hao-Tien Lewis Chiang",
            "Aleksandra Faust",
            "Marek Fiser",
            "Anthony Francis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.10141v7",
        "title": "Developmental Bayesian Optimization of Black-Box with Visual\n  Similarity-Based Transfer Learning",
        "abstract": "  We present a developmental framework based on a long-term memory and\nreasoning mechanisms (Vision Similarity and Bayesian Optimisation). This\narchitecture allows a robot to optimize autonomously hyper-parameters that need\nto be tuned from any action and/or vision module, treated as a black-box. The\nlearning can take advantage of past experiences (stored in the episodic and\nprocedural memories) in order to warm-start the exploration using a set of\nhyper-parameters previously optimized from objects similar to the new unknown\none (stored in a semantic memory). As example, the system has been used to\noptimized 9 continuous hyper-parameters of a professional software (Kamido)\nboth in simulation and with a real robot (industrial robotic arm Fanuc) with a\ntotal of 13 different objects. The robot is able to find a good object-specific\noptimization in 68 (simulation) or 40 (real) trials. In simulation, we\ndemonstrate the benefit of the transfer learning based on visual similarity, as\nopposed to an amnesic learning (i.e. learning from scratch all the time).\nMoreover, with the real robot, we show that the method consistently outperforms\nthe manual optimization from an expert with less than 2 hours of training time\nto achieve more than 88% of success.\n",
        "published": "2018",
        "authors": [
            "Maxime Petit",
            "Amaury Depierre",
            "Xiaofang Wang",
            "Emmanuel Dellandr\u00e9a",
            "Liming Chen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.10788v1",
        "title": "Learning and Acting in Peripersonal Space: Moving, Reaching, and\n  Grasping",
        "abstract": "  The young infant explores its body, its sensorimotor system, and the\nimmediately accessible parts of its environment, over the course of a few\nmonths creating a model of peripersonal space useful for reaching and grasping\nobjects around it. Drawing on constraints from the empirical literature on\ninfant behavior, we present a preliminary computational model of this learning\nprocess, implemented and evaluated on a physical robot. The learning agent\nexplores the relationship between the configuration space of the arm, sensing\njoint angles through proprioception, and its visual perceptions of the hand and\ngrippers. The resulting knowledge is represented as the peripersonal space\n(PPS) graph, where nodes represent states of the arm, edges represent safe\nmovements, and paths represent safe trajectories from one pose to another. In\nour model, the learning process is driven by intrinsic motivation. When\nrepeatedly performing an action, the agent learns the typical result, but also\ndetects unusual outcomes, and is motivated to learn how to make those unusual\nresults reliable. Arm motions typically leave the static background unchanged,\nbut occasionally bump an object, changing its static position. The reach action\nis learned as a reliable way to bump and move an object in the environment.\nSimilarly, once a reliable reach action is learned, it typically makes a\nquasi-static change in the environment, moving an object from one static\nposition to another. The unusual outcome is that the object is accidentally\ngrasped (thanks to the innate Palmar reflex), and thereafter moves dynamically\nwith the hand. Learning to make grasps reliable is more complex than for\nreaches, but we demonstrate significant progress. Our current results are steps\ntoward autonomous sensorimotor learning of motion, reaching, and grasping in\nperipersonal space, based on unguided exploration and intrinsic motivation.\n",
        "published": "2018",
        "authors": [
            "Jonathan Juett",
            "Benjamin Kuipers"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1809.11169v2",
        "title": "Propagation Networks for Model-Based Control Under Partial Observation",
        "abstract": "  There has been an increasing interest in learning dynamics simulators for\nmodel-based control. Compared with off-the-shelf physics engines, a learnable\nsimulator can quickly adapt to unseen objects, scenes, and tasks. However,\nexisting models like interaction networks only work for fully observable\nsystems; they also only consider pairwise interactions within a single time\nstep, both restricting their use in practical systems. We introduce Propagation\nNetworks (PropNet), a differentiable, learnable dynamics model that handles\npartially observable scenarios and enables instantaneous propagation of signals\nbeyond pairwise interactions. Experiments show that our propagation networks\nnot only outperform current learnable physics engines in forward simulation,\nbut also achieve superior performance on various control tasks. Compared with\nexisting model-free deep reinforcement learning algorithms, model-based control\nwith propagation networks is more accurate, efficient, and generalizable to\nnew, partially observable scenes and tasks.\n",
        "published": "2018",
        "authors": [
            "Yunzhu Li",
            "Jiajun Wu",
            "Jun-Yan Zhu",
            "Joshua B. Tenenbaum",
            "Antonio Torralba",
            "Russ Tedrake"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.01127v2",
        "title": "Learning to Predict Ego-Vehicle Poses for Sampling-Based Nonholonomic\n  Motion Planning",
        "abstract": "  Sampling-based motion planning is an effective tool to compute safe\ntrajectories for automated vehicles in complex environments. However, a fast\nconvergence to the optimal solution can only be ensured with the use of\nproblem-specific sampling distributions. Due to the large variety of driving\nsituations within the context of automated driving, it is very challenging to\nmanually design such distributions. This paper introduces therefore a\ndata-driven approach utilizing a deep convolutional neural network (CNN): Given\nthe current driving situation, future ego-vehicle poses can be directly\ngenerated from the output of the CNN allowing to guide the motion planner\nefficiently towards the optimal solution. A benchmark highlights that the CNN\npredicts future vehicle poses with a higher accuracy compared to uniform\nsampling and a state-of-the-art A*-based approach. Combining this CNN-guided\nsampling with the motion planner Bidirectional RRT* reduces the computation\ntime by up to an order of magnitude and yields a faster convergence to a lower\ncost as well as a success rate of 100 % in the tested scenarios.\n",
        "published": "2018",
        "authors": [
            "Holger Banzhaf",
            "Paul Sanzenbacher",
            "Ulrich Baumann",
            "J. Marius Z\u00f6llner"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1812.07084v2",
        "title": "Learning Constraints from Demonstrations",
        "abstract": "  We extend the learning from demonstration paradigm by providing a method for\nlearning unknown constraints shared across tasks, using demonstrations of the\ntasks, their cost functions, and knowledge of the system dynamics and control\nconstraints. Given safe demonstrations, our method uses hit-and-run sampling to\nobtain lower cost, and thus unsafe, trajectories. Both safe and unsafe\ntrajectories are used to obtain a consistent representation of the unsafe set\nvia solving an integer program. Our method generalizes across system dynamics\nand learns a guaranteed subset of the constraint. We also provide theoretical\nanalysis on what subset of the constraint can be learnable from safe\ndemonstrations. We demonstrate our method on linear and nonlinear system\ndynamics, show that it can be modified to work with suboptimal demonstrations,\nand that it can also be used to learn constraints in a feature space.\n",
        "published": "2018",
        "authors": [
            "Glen Chou",
            "Dmitry Berenson",
            "Necmiye Ozay"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.01959v1",
        "title": "Learning Exploration Policies for Navigation",
        "abstract": "  Numerous past works have tackled the problem of task-driven navigation. But,\nhow to effectively explore a new environment to enable a variety of down-stream\ntasks has received much less attention. In this work, we study how agents can\nautonomously explore realistic and complex 3D environments without the context\nof task-rewards. We propose a learning-based approach and investigate different\npolicy architectures, reward functions, and training paradigms. We find that\nthe use of policies with spatial memory that are bootstrapped with imitation\nlearning and finally finetuned with coverage rewards derived purely from\non-board sensors can be effective at exploring novel environments. We show that\nour learned exploration policies can explore better than classical approaches\nbased on geometry alone and generic learning-based exploration techniques.\nFinally, we also show how such task-agnostic exploration can be used for\ndown-stream tasks. Code and Videos are available at:\nhttps://sites.google.com/view/exploration-for-nav.\n",
        "published": "2019",
        "authors": [
            "Tao Chen",
            "Saurabh Gupta",
            "Abhinav Gupta"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.02526v2",
        "title": "Safety-Guided Deep Reinforcement Learning via Online Gaussian Process\n  Estimation",
        "abstract": "  An important facet of reinforcement learning (RL) has to do with how the\nagent goes about exploring the environment. Traditional exploration strategies\ntypically focus on efficiency and ignore safety. However, for practical\napplications, ensuring safety of the agent during exploration is crucial since\nperforming an unsafe action or reaching an unsafe state could result in\nirreversible damage to the agent. The main challenge of safe exploration is\nthat characterizing the unsafe states and actions is difficult for large\ncontinuous state or action spaces and unknown environments. In this paper, we\npropose a novel approach to incorporate estimations of safety to guide\nexploration and policy search in deep reinforcement learning. By using a cost\nfunction to capture trajectory-based safety, our key idea is to formulate the\nstate-action value function of this safety cost as a candidate Lyapunov\nfunction and extend control-theoretic results to approximate its derivative\nusing online Gaussian Process (GP) estimation. We show how to use these\nstatistical models to guide the agent in unknown environments to obtain\nhigh-performance control policies with provable stability certificates.\n",
        "published": "2019",
        "authors": [
            "Jiameng Fan",
            "Wenchao Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.03227v4",
        "title": "Pixel-Attentive Policy Gradient for Multi-Fingered Grasping in Cluttered\n  Scenes",
        "abstract": "  Recent advances in on-policy reinforcement learning (RL) methods enabled\nlearning agents in virtual environments to master complex tasks with\nhigh-dimensional and continuous observation and action spaces. However,\nleveraging this family of algorithms in multi-fingered robotic grasping remains\na challenge due to large sim-to-real fidelity gaps and the high sample\ncomplexity of on-policy RL algorithms. This work aims to bridge these gaps by\nfirst reinforcement-learning a multi-fingered robotic grasping policy in\nsimulation that operates in the pixel space of the input: a single depth image.\nUsing a mapping from pixel space to Cartesian space according to the depth map,\nthis method transfers to the real world with high fidelity and introduces a\nnovel attention mechanism that substantially improves grasp success rate in\ncluttered environments. Finally, the direct-generative nature of this method\nallows learning of multi-fingered grasps that have flexible end-effector\npositions, orientations and rotations, as well as all degrees of freedom of the\nhand.\n",
        "published": "2019",
        "authors": [
            "Bohan Wu",
            "Iretiayo Akinola",
            "Peter K. Allen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.03591v1",
        "title": "Learning to Identify Object Instances by Touch: Tactile Recognition via\n  Multimodal Matching",
        "abstract": "  Much of the literature on robotic perception focuses on the visual modality.\nVision provides a global observation of a scene, making it broadly useful.\nHowever, in the domain of robotic manipulation, vision alone can sometimes\nprove inadequate: in the presence of occlusions or poor lighting, visual object\nidentification might be difficult. The sense of touch can provide robots with\nan alternative mechanism for recognizing objects. In this paper, we study the\nproblem of touch-based instance recognition. We propose a novel framing of the\nproblem as multi-modal recognition: the goal of our system is to recognize,\ngiven a visual and tactile observation, whether or not these observations\ncorrespond to the same object. To our knowledge, our work is the first to\naddress this type of multi-modal instance recognition problem on such a\nlarge-scale with our analysis spanning 98 different objects. We employ a robot\nequipped with two GelSight touch sensors, one on each finger, and a\nself-supervised, autonomous data collection procedure to collect a dataset of\ntactile observations and images. Our experimental results show that it is\npossible to accurately recognize object instances by touch alone, including\ninstances of novel objects that were never seen during training. Our learned\nmodel outperforms other methods on this complex task, including that of human\nvolunteers.\n",
        "published": "2019",
        "authors": [
            "Justin Lin",
            "Roberto Calandra",
            "Sergey Levine"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.04413v1",
        "title": "Building an Affordances Map with Interactive Perception",
        "abstract": "  Robots need to understand their environment to perform their task. If it is\npossible to pre-program a visual scene analysis process in closed environments,\nrobots operating in an open environment would benefit from the ability to learn\nit through their interaction with their environment. This ability furthermore\nopens the way to the acquisition of affordances maps in which the action\ncapabilities of the robot structure its visual scene understanding. We propose\nan approach to build such affordances maps by relying on an interactive\nperception approach and an online classification. In the proposed formalization\nof affordances, actions and effects are related to visual features, not\nobjects, and they can be combined. We have tested the approach on three action\nprimitives and on a real PR2 robot.\n",
        "published": "2019",
        "authors": [
            "Leni K. Le Goff",
            "Oussama Yaakoubi",
            "Alexandre Coninx",
            "Stephane Doncieux"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.05881v2",
        "title": "Can User-Centered Reinforcement Learning Allow a Robot to Attract\n  Passersby without Causing Discomfort?",
        "abstract": "  The aim of our study was to develop a method by which a social robot can\ngreet passersby and get their attention without causing them to suffer\ndiscomfort.A number of customer services have recently come to be provided by\nsocial robots rather than people, including, serving as receptionists, guides,\nand exhibitors. Robot exhibitors, for example, can explain products being\npromoted by the robot owners. However, a sudden greeting by a robot can startle\npassersby and cause discomfort to passersby.Social robots should thus adapt\ntheir mannerisms to the situation they face regarding passersby.We developed a\nmethod for meeting this requirement on the basis of the results of related\nwork. Our proposed method, user-centered reinforcement learning, enables robots\nto greet passersby and get their attention without causing them to suffer\ndiscomfort (p<0.01) .The results of an experiment in the field, an office\nentrance, demonstrated that our method meets this requirement.\n",
        "published": "2019",
        "authors": [
            "Yasunori Ozaki",
            "Tatsuya Ishihara",
            "Narimune Matsumura",
            "Tadashi Nunobiki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.06278v2",
        "title": "gym-gazebo2, a toolkit for reinforcement learning using ROS 2 and Gazebo",
        "abstract": "  This paper presents an upgraded, real world application oriented version of\ngym-gazebo, the Robot Operating System (ROS) and Gazebo based Reinforcement\nLearning (RL) toolkit, which complies with OpenAI Gym. The content discusses\nthe new ROS 2 based software architecture and summarizes the results obtained\nusing Proximal Policy Optimization (PPO). Ultimately, the output of this work\npresents a benchmarking system for robotics that allows different techniques\nand algorithms to be compared using the same virtual conditions. We have\nevaluated environments with different levels of complexity of the Modular\nArticulated Robotic Arm (MARA), reaching accuracies in the millimeter scale.\nThe converged results show the feasibility and usefulness of the gym-gazebo 2\ntoolkit, its potential and applicability in industrial use cases, using modular\nrobots.\n",
        "published": "2019",
        "authors": [
            "Nestor Gonzalez Lopez",
            "Yue Leire Erro Nuin",
            "Elias Barba Moral",
            "Lander Usategui San Juan",
            "Alejandro Solano Rueda",
            "V\u00edctor Mayoral Vilches",
            "Risto Kojcev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.06282v2",
        "title": "ROS2Learn: a reinforcement learning framework for ROS 2",
        "abstract": "  We propose a novel framework for Deep Reinforcement Learning (DRL) in modular\nrobotics to train a robot directly from joint states, using traditional robotic\ntools. We use an state-of-the-art implementation of the Proximal Policy\nOptimization, Trust Region Policy Optimization and Actor-Critic\nKronecker-Factored Trust Region algorithms to learn policies in four different\nModular Articulated Robotic Arm (MARA) environments. We support this process\nusing a framework that communicates with typical tools used in robotics, such\nas Gazebo and Robot Operating System 2 (ROS 2). We evaluate several algorithms\nin modular robots with an empirical study in simulation.\n",
        "published": "2019",
        "authors": [
            "Yue Leire Erro Nuin",
            "Nestor Gonzalez Lopez",
            "Elias Barba Moral",
            "Lander Usategui San Juan",
            "Alejandro Solano Rueda",
            "V\u00edctor Mayoral Vilches",
            "Risto Kojcev"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.08693v1",
        "title": "Using Local Experiences for Global Motion Planning",
        "abstract": "  Sampling-based planners are effective in many real-world applications such as\nrobotics manipulation, navigation, and even protein modeling. However, it is\noften challenging to generate a collision-free path in environments where key\nareas are hard to sample. In the absence of any prior information,\nsampling-based planners are forced to explore uniformly or heuristically, which\ncan lead to degraded performance. One way to improve performance is to use\nprior knowledge of environments to adapt the sampling strategy to the problem\nat hand. In this work, we decompose the workspace into local primitives,\nmemorizing local experiences by these primitives in the form of local samplers,\nand store them in a database. We synthesize an efficient global sampler by\nretrieving local experiences relevant to the given situation. Our method\ntransfers knowledge effectively between diverse environments that share local\nprimitives and speeds up the performance dramatically. Our results show, in\nterms of solution time, an improvement of multiple orders of magnitude in two\ntraditionally challenging high-dimensional problems compared to\nstate-of-the-art approaches.\n",
        "published": "2019",
        "authors": [
            "Constantinos Chamzas",
            "Anshumali Shrivastava",
            "Lydia E. Kavraki"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1903.10654v3",
        "title": "Failure-Scenario Maker for Rule-Based Agent using Multi-agent\n  Adversarial Reinforcement Learning and its Application to Autonomous Driving",
        "abstract": "  We examine the problem of adversarial reinforcement learning for multi-agent\ndomains including a rule-based agent. Rule-based algorithms are required in\nsafety-critical applications for them to work properly in a wide range of\nsituations. Hence, every effort is made to find failure scenarios during the\ndevelopment phase. However, as the software becomes complicated, finding\nfailure cases becomes difficult. Especially in multi-agent domains, such as\nautonomous driving environments, it is much harder to find useful failure\nscenarios that help us improve the algorithm. We propose a method for\nefficiently finding failure scenarios; this method trains the adversarial\nagents using multi-agent reinforcement learning such that the tested rule-based\nagent fails. We demonstrate the effectiveness of our proposed method using a\nsimple environment and autonomous driving simulator.\n",
        "published": "2019",
        "authors": [
            "Akifumi Wachi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.00260v1",
        "title": "Transferable Force-Torque Dynamics Model for Peg-in-hole Task",
        "abstract": "  We present a learning-based force-torque dynamics to achieve model-based\ncontrol for contact-rich peg-in-hole task using force-only inputs. Learning the\nforce-torque dynamics is challenging because of the ambiguity of the\nlow-dimensional 6-d force signal and the requirement of excessive training\ndata. To tackle these problems, we propose a multi-pose force-torque state\nrepresentation, based on which a dynamics model is learned with the data\ngenerated in a sample-efficient offline fashion. In addition, by training the\ndynamics model with peg-and-holes of various shapes, scales, and elasticities,\nthe model could quickly transfer to new peg-and-holes after a small number of\ntrials. Extensive experiments show that our dynamics model could adapt to\nunseen peg-and-holes with 70% fewer samples required compared to learning from\nscratch. Along with the learned dynamics, model predictive control and\nmodel-based reinforcement learning policies achieve over 80% insertion success\nrate. Our video is available at https://youtu.be/ZAqldpVZgm4.\n",
        "published": "2019",
        "authors": [
            "Junfeng Ding",
            "Chen Wang",
            "Cewu Lu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.01603v3",
        "title": "Dream to Control: Learning Behaviors by Latent Imagination",
        "abstract": "  Learned world models summarize an agent's experience to facilitate learning\ncomplex behaviors. While learning world models from high-dimensional sensory\ninputs is becoming feasible through deep learning, there are many potential\nways for deriving behaviors from them. We present Dreamer, a reinforcement\nlearning agent that solves long-horizon tasks from images purely by latent\nimagination. We efficiently learn behaviors by propagating analytic gradients\nof learned state values back through trajectories imagined in the compact state\nspace of a learned world model. On 20 challenging visual control tasks, Dreamer\nexceeds existing approaches in data-efficiency, computation time, and final\nperformance.\n",
        "published": "2019",
        "authors": [
            "Danijar Hafner",
            "Timothy Lillicrap",
            "Jimmy Ba",
            "Mohammad Norouzi"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.01715v1",
        "title": "Human-Robot Collaboration via Deep Reinforcement Learning of Real-World\n  Interactions",
        "abstract": "  We present a robotic setup for real-world testing and evaluation of\nhuman-robot and human-human collaborative learning. Leveraging the\nsample-efficiency of the Soft Actor-Critic algorithm, we have implemented a\nrobotic platform able to learn a non-trivial collaborative task with a human\npartner, without pre-training in simulation, and using only 30 minutes of\nreal-world interactions. This enables us to study Human-Robot and Human-Human\ncollaborative learning through real-world interactions. We present preliminary\nresults, showing that state-of-the-art deep learning methods can take\nhuman-robot collaborative learning a step closer to that of humans interacting\nwith each other.\n",
        "published": "2019",
        "authors": [
            "Jonas Tjomsland",
            "Ali Shafti",
            "A. Aldo Faisal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.02059v2",
        "title": "Learning to Dynamically Coordinate Multi-Robot Teams in Graph Attention\n  Networks",
        "abstract": "  Increasing interest in integrating advanced robotics within manufacturing has\nspurred a renewed concentration in developing real-time scheduling solutions to\ncoordinate human-robot collaboration in this environment. Traditionally, the\nproblem of scheduling agents to complete tasks with temporal and spatial\nconstraints has been approached either with exact algorithms, which are\ncomputationally intractable for large-scale, dynamic coordination, or\napproximate methods that require domain experts to craft heuristics for each\napplication. We seek to overcome the limitations of these conventional methods\nby developing a novel graph attention network formulation to automatically\nlearn features of scheduling problems to allow their deployment. To learn\neffective policies for combinatorial optimization problems via machine\nlearning, we combine imitation learning on smaller problems with deep\nQ-learning on larger problems, in a non-parametric framework, to allow for\nfast, near-optimal scheduling of robot teams. We show that our network-based\npolicy finds at least twice as many solutions over prior state-of-the-art\nmethods in all testing scenarios.\n",
        "published": "2019",
        "authors": [
            "Zheyuan Wang",
            "Matthew Gombolay"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.02877v2",
        "title": "Training Agents using Upside-Down Reinforcement Learning",
        "abstract": "  We develop Upside-Down Reinforcement Learning (UDRL), a method for learning\nto act using only supervised learning techniques. Unlike traditional\nalgorithms, UDRL does not use reward prediction or search for an optimal\npolicy. Instead, it trains agents to follow commands such as \"obtain so much\ntotal reward in so much time.\" Many of its general principles are outlined in a\ncompanion report; the goal of this paper is to develop a practical learning\nalgorithm and show that this conceptually simple perspective on agent training\ncan produce a range of rewarding behaviors for multiple episodic environments.\nExperiments show that on some tasks UDRL's performance can be surprisingly\ncompetitive with, and even exceed that of some traditional baseline algorithms\ndeveloped over decades of research. Based on these results, we suggest that\nalternative approaches to expected reward maximization have an important role\nto play in training useful autonomous agents.\n",
        "published": "2019",
        "authors": [
            "Rupesh Kumar Srivastava",
            "Pranav Shyam",
            "Filipe Mutz",
            "Wojciech Ja\u015bkowski",
            "J\u00fcrgen Schmidhuber"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.03509v2",
        "title": "Driving Style Encoder: Situational Reward Adaptation for General-Purpose\n  Planning in Automated Driving",
        "abstract": "  General-purpose planning algorithms for automated driving combine mission,\nbehavior, and local motion planning. Such planning algorithms map features of\nthe environment and driving kinematics into complex reward functions. To\nachieve this, planning experts often rely on linear reward functions. The\nspecification and tuning of these reward functions is a tedious process and\nrequires significant experience. Moreover, a manually designed linear reward\nfunction does not generalize across different driving situations. In this work,\nwe propose a deep learning approach based on inverse reinforcement learning\nthat generates situation-dependent reward functions. Our neural network\nprovides a mapping between features and actions of sampled driving policies of\na model-predictive control-based planner and predicts reward functions for\nupcoming planning cycles. In our evaluation, we compare the driving style of\nreward functions predicted by our deep network against clustered and linear\nreward functions. Our proposed deep learning approach outperforms clustered\nlinear reward functions and is at par with linear reward functions with\na-priori knowledge about the situation.\n",
        "published": "2019",
        "authors": [
            "Sascha Rosbach",
            "Vinit James",
            "Simon Gro\u00dfjohann",
            "Silviu Homoceanu",
            "Xing Li",
            "Stefan Roth"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.07670v1",
        "title": "To Follow or not to Follow: Selective Imitation Learning from\n  Observations",
        "abstract": "  Learning from demonstrations is a useful way to transfer a skill from one\nagent to another. While most imitation learning methods aim to mimic an expert\nskill by following the demonstration step-by-step, imitating every step in the\ndemonstration often becomes infeasible when the learner and its environment are\ndifferent from the demonstration. In this paper, we propose a method that can\nimitate a demonstration composed solely of observations, which may not be\nreproducible with the current agent. Our method, dubbed selective imitation\nlearning from observations (SILO), selects reachable states in the\ndemonstration and learns how to reach the selected states. Our experiments on\nboth simulated and real robot environments show that our method reliably\nperforms a new task by following a demonstration. Videos and code are available\nat https://clvrai.com/silo .\n",
        "published": "2019",
        "authors": [
            "Youngwoon Lee",
            "Edward S. Hu",
            "Zhengyu Yang",
            "Joseph J. Lim"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.08578v1",
        "title": "Taming an autonomous surface vehicle for path following and collision\n  avoidance using deep reinforcement learning",
        "abstract": "  In this article, we explore the feasibility of applying proximal policy\noptimization, a state-of-the-art deep reinforcement learning algorithm for\ncontinuous control tasks, on the dual-objective problem of controlling an\nunderactuated autonomous surface vehicle to follow an a priori known path while\navoiding collisions with non-moving obstacles along the way. The artificial\nintelligent agent, which is equipped with multiple rangefinder sensors for\nobstacle detection, is trained and evaluated in a challenging, stochastically\ngenerated simulation environment based on the OpenAI gym python toolkit.\nNotably, the agent is provided with real-time insight into its own reward\nfunction, allowing it to dynamically adapt its guidance strategy. Depending on\nits strategy, which ranges from radical path-adherence to radical obstacle\navoidance, the trained agent achieves an episodic success rate between 84 and\n100%.\n",
        "published": "2019",
        "authors": [
            "Eivind Meyer",
            "Haakon Robinson",
            "Adil Rasheed",
            "Omer San"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.11032v1",
        "title": "Towards Practical Multi-Object Manipulation using Relational\n  Reinforcement Learning",
        "abstract": "  Learning robotic manipulation tasks using reinforcement learning with sparse\nrewards is currently impractical due to the outrageous data requirements. Many\npractical tasks require manipulation of multiple objects, and the complexity of\nsuch tasks increases with the number of objects. Learning from a curriculum of\nincreasingly complex tasks appears to be a natural solution, but unfortunately,\ndoes not work for many scenarios. We hypothesize that the inability of the\nstate-of-the-art algorithms to effectively utilize a task curriculum stems from\nthe absence of inductive biases for transferring knowledge from simpler to\ncomplex tasks. We show that graph-based relational architectures overcome this\nlimitation and enable learning of complex tasks when provided with a simple\ncurriculum of tasks with increasing numbers of objects. We demonstrate the\nutility of our framework on a simulated block stacking task. Starting from\nscratch, our agent learns to stack six blocks into a tower. Despite using\nstep-wise sparse rewards, our method is orders of magnitude more data-efficient\nand outperforms the existing state-of-the-art method that utilizes human\ndemonstrations. Furthermore, the learned policy exhibits zero-shot\ngeneralization, successfully stacking blocks into taller towers and previously\nunseen configurations such as pyramids, without any further training.\n",
        "published": "2019",
        "authors": [
            "Richard Li",
            "Allan Jabri",
            "Trevor Darrell",
            "Pulkit Agrawal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/1912.12204v1",
        "title": "Federated Imitation Learning: A Novel Framework for Cloud Robotic\n  Systems with Heterogeneous Sensor Data",
        "abstract": "  Humans are capable of learning a new behavior by observing others to perform\nthe skill. Similarly, robots can also implement this by imitation learning.\nFurthermore, if with external guidance, humans can master the new behavior more\nefficiently. So, how can robots achieve this? To address the issue, we present\na novel framework named FIL. It provides a heterogeneous knowledge fusion\nmechanism for cloud robotic systems. Then, a knowledge fusion algorithm in FIL\nis proposed. It enables the cloud to fuse heterogeneous knowledge from local\nrobots and generate guide models for robots with service requests. After that,\nwe introduce a knowledge transfer scheme to facilitate local robots acquiring\nknowledge from the cloud. With FIL, a robot is capable of utilizing knowledge\nfrom other robots to increase its imitation learning in accuracy and\nefficiency. Compared with transfer learning and meta-learning, FIL is more\nsuitable to be deployed in cloud robotic systems. Finally, we conduct\nexperiments of a self-driving task for robots (cars). The experimental results\ndemonstrate that the shared model generated by FIL increases imitation learning\nefficiency of local robots in cloud robotic systems.\n",
        "published": "2019",
        "authors": [
            "Boyi Liu",
            "Lujia Wang",
            "Ming Liu",
            "Cheng-Zhong Xu"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.01982v1",
        "title": "Intrinsic Motivation and Episodic Memories for Robot Exploration of\n  High-Dimensional Sensory Spaces",
        "abstract": "  This work presents an architecture that generates curiosity-driven\ngoal-directed exploration behaviours for an image sensor of a microfarming\nrobot. A combination of deep neural networks for offline unsupervised learning\nof low-dimensional features from images, and of online learning of shallow\nneural networks representing the inverse and forward kinematics of the system\nhave been used. The artificial curiosity system assigns interest values to a\nset of pre-defined goals, and drives the exploration towards those that are\nexpected to maximise the learning progress. We propose the integration of an\nepisodic memory in intrinsic motivation systems to face catastrophic forgetting\nissues, typically experienced when performing online updates of artificial\nneural networks. Our results show that adopting an episodic memory system not\nonly prevents the computational models from quickly forgetting knowledge that\nhas been previously acquired, but also provides new avenues for modulating the\nbalance between plasticity and stability of the models.\n",
        "published": "2020",
        "authors": [
            "Guido Schillaci",
            "Antonio Pico Villalpando",
            "Verena Vanessa Hafner",
            "Peter Hanappe",
            "David Colliaux",
            "Timoth\u00e9e Wintz"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.02970v2",
        "title": "Closed-loop deep learning: generating forward models with\n  back-propagation",
        "abstract": "  A reflex is a simple closed loop control approach which tries to minimise an\nerror but fails to do so because it will always react too late. An adaptive\nalgorithm can use this error to learn a forward model with the help of\npredictive cues. For example a driver learns to improve their steering by\nlooking ahead to avoid steering in the last minute. In order to process complex\ncues such as the road ahead deep learning is a natural choice. However, this is\nusually only achieved indirectly by employing deep reinforcement learning\nhaving a discrete state space. Here, we show how this can be directly achieved\nby embedding deep learning into a closed loop system and preserving its\ncontinuous processing. We show specifically how error back-propagation can be\nachieved in z-space and in general how gradient based approaches can be\nanalysed in such closed loop scenarios. The performance of this learning\nparadigm is demonstrated using a line-follower both in simulation and on a real\nrobot that show very fast and continuous learning.\n",
        "published": "2020",
        "authors": [
            "Sama Daryanavard",
            "Bernd Porr"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.03359v1",
        "title": "Deep Interactive Reinforcement Learning for Path Following of Autonomous\n  Underwater Vehicle",
        "abstract": "  Autonomous underwater vehicle (AUV) plays an increasingly important role in\nocean exploration. Existing AUVs are usually not fully autonomous and generally\nlimited to pre-planning or pre-programming tasks. Reinforcement learning (RL)\nand deep reinforcement learning have been introduced into the AUV design and\nresearch to improve its autonomy. However, these methods are still difficult to\napply directly to the actual AUV system because of the sparse rewards and low\nlearning efficiency. In this paper, we proposed a deep interactive\nreinforcement learning method for path following of AUV by combining the\nadvantages of deep reinforcement learning and interactive RL. In addition,\nsince the human trainer cannot provide human rewards for AUV when it is running\nin the ocean and AUV needs to adapt to a changing environment, we further\npropose a deep reinforcement learning method that learns from both human\nrewards and environmental rewards at the same time. We test our methods in two\npath following tasks---straight line and sinusoids curve following of AUV by\nsimulating in the Gazebo platform. Our experimental results show that with our\nproposed deep interactive RL method, AUV can converge faster than a DQN learner\nfrom only environmental reward. Moreover, AUV learning with our deep RL from\nboth human and environmental rewards can also achieve a similar or even better\nperformance than that with the deep interactive RL method and can adapt to the\nactual environment by further learning from environmental rewards.\n",
        "published": "2020",
        "authors": [
            "Qilei Zhang",
            "Jinying Lin",
            "Qixin Sha",
            "Bo He",
            "Guangliang Li"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2001.06627v1",
        "title": "Multi-agent Motion Planning for Dense and Dynamic Environments via Deep\n  Reinforcement Learning",
        "abstract": "  This paper introduces a hybrid algorithm of deep reinforcement learning (RL)\nand Force-based motion planning (FMP) to solve distributed motion planning\nproblem in dense and dynamic environments. Individually, RL and FMP algorithms\neach have their own limitations. FMP is not able to produce time-optimal paths\nand existing RL solutions are not able to produce collision-free paths in dense\nenvironments. Therefore, we first tried improving the performance of recent RL\napproaches by introducing a new reward function that not only eliminates the\nrequirement of a pre supervised learning (SL) step but also decreases the\nchance of collision in crowded environments. That improved things, but there\nwere still a lot of failure cases. So, we developed a hybrid approach to\nleverage the simpler FMP approach in stuck, simple and high-risk cases, and\ncontinue using RL for normal cases in which FMP can't produce optimal path.\nAlso, we extend GA3C-CADRL algorithm to 3D environment. Simulation results show\nthat the proposed algorithm outperforms both deep RL and FMP algorithms and\nproduces up to 50% more successful scenarios than deep RL and up to 75% less\nextra time to reach goal than FMP.\n",
        "published": "2020",
        "authors": [
            "Samaneh Hosseini Semnani",
            "Hugh Liu",
            "Michael Everett",
            "Anton de Ruiter",
            "Jonathan P. How"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.01156v2",
        "title": "Real-World Human-Robot Collaborative Reinforcement Learning",
        "abstract": "  The intuitive collaboration of humans and intelligent robots (embodied AI) in\nthe real-world is an essential objective for many desirable applications of\nrobotics. Whilst there is much research regarding explicit communication, we\nfocus on how humans and robots interact implicitly, on motor adaptation level.\nWe present a real-world setup of a human-robot collaborative maze game,\ndesigned to be non-trivial and only solvable through collaboration, by limiting\nthe actions to rotations of two orthogonal axes, and assigning each axes to one\nplayer. This results in neither the human nor the agent being able to solve the\ngame on their own. We use deep reinforcement learning for the control of the\nrobotic agent, and achieve results within 30 minutes of real-world play,\nwithout any type of pre-training. We then use this setup to perform systematic\nexperiments on human/agent behaviour and adaptation when co-learning a policy\nfor the collaborative game. We present results on how co-policy learning occurs\nover time between the human and the robotic agent resulting in each\nparticipant's agent serving as a representation of how they would play the\ngame. This allows us to relate a person's success when playing with different\nagents than their own, by comparing the policy of the agent with that of their\nown agent.\n",
        "published": "2020",
        "authors": [
            "Ali Shafti",
            "Jonas Tjomsland",
            "William Dudley",
            "A. Aldo Faisal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.03133v2",
        "title": "DeFINE: Delayed Feedback based Immersive Navigation Environment for\n  Studying Goal-Directed Human Navigation",
        "abstract": "  With the advent of consumer-grade products for presenting an immersive\nvirtual environment (VE), there is a growing interest in utilizing VEs for\ntesting human navigation behavior. However, preparing a VE still requires a\nhigh level of technical expertise in computer graphics and virtual reality,\nposing a significant hurdle to embracing the emerging technology. To address\nthis issue, this paper presents Delayed Feedback based Immersive Navigation\nEnvironment (DeFINE), a framework that allows for easy creation and\nadministration of navigation tasks within customizable VEs via intuitive\ngraphical user interfaces and simple settings files. Importantly, DeFINE has a\nbuilt-in capability to provide performance feedback to participants during an\nexperiment, a feature that is critically missing in other similar frameworks.\nTo show the usability of DeFINE from both experimentalists' and participants'\nperspectives, a demonstration was made in which participants navigated to a\nhidden goal location with feedback that differentially weighted speed and\naccuracy of their responses. In addition, the participants evaluated DeFINE in\nterms of its ease of use, required workload, and proneness to induce\ncybersickness. The demonstration exemplified typical experimental manipulations\nDeFINE accommodates and what types of data it can collect for characterizing\nparticipants' task performance. With its out-of-the-box functionality and\npotential customizability due to open-source licensing, DeFINE makes VEs more\naccessible to many researchers.\n",
        "published": "2020",
        "authors": [
            "Kshitij Tiwari",
            "Ville Kyrki",
            "Allen Cheung",
            "Naohide Yamamoto"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.03779v2",
        "title": "Deep Adversarial Reinforcement Learning for Object Disentangling",
        "abstract": "  Deep learning in combination with improved training techniques and high\ncomputational power has led to recent advances in the field of reinforcement\nlearning (RL) and to successful robotic RL applications such as in-hand\nmanipulation. However, most robotic RL relies on a well known initial state\ndistribution. In real-world tasks, this information is however often not\navailable. For example, when disentangling waste objects the actual position of\nthe robot w.r.t.\\ the objects may not match the positions the RL policy was\ntrained for. To solve this problem, we present a novel adversarial\nreinforcement learning (ARL) framework. The ARL framework utilizes an\nadversary, which is trained to steer the original agent, the protagonist, to\nchallenging states. We train the protagonist and the adversary jointly to allow\nthem to adapt to the changing policy of their opponent. We show that our method\ncan generalize from training to test scenarios by training an end-to-end system\nfor robot control to solve a challenging object disentangling task. Experiments\nwith a KUKA LBR+ 7-DOF robot arm show that our approach outperforms the\nbaseline method in disentangling when starting from different initial states\nthan provided during training.\n",
        "published": "2020",
        "authors": [
            "Melvin Laux",
            "Oleg Arenz",
            "Jan Peters",
            "Joni Pajarinen"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.04663v2",
        "title": "Fast Online Adaptation in Robotics through Meta-Learning Embeddings of\n  Simulated Priors",
        "abstract": "  Meta-learning algorithms can accelerate the model-based reinforcement\nlearning (MBRL) algorithms by finding an initial set of parameters for the\ndynamical model such that the model can be trained to match the actual dynamics\nof the system with only a few data-points. However, in the real world, a robot\nmight encounter any situation starting from motor failures to finding itself in\na rocky terrain where the dynamics of the robot can be significantly different\nfrom one another. In this paper, first, we show that when meta-training\nsituations (the prior situations) have such diverse dynamics, using a single\nset of meta-trained parameters as a starting point still requires a large\nnumber of observations from the real system to learn a useful model of the\ndynamics. Second, we propose an algorithm called FAMLE that mitigates this\nlimitation by meta-training several initial starting points (i.e., initial\nparameters) for training the model and allows the robot to select the most\nsuitable starting point to adapt the model to the current situation with only a\nfew gradient steps. We compare FAMLE to MBRL, MBRL with a meta-trained model\nwith MAML, and model-free policy search algorithm PPO for various simulated and\nreal robotic tasks, and show that FAMLE allows the robots to adapt to novel\ndamages in significantly fewer time-steps than the baselines.\n",
        "published": "2020",
        "authors": [
            "Rituraj Kaushik",
            "Timoth\u00e9e Anne",
            "Jean-Baptiste Mouret"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.06085v2",
        "title": "Learning to Generalize Across Long-Horizon Tasks from Human\n  Demonstrations",
        "abstract": "  Imitation learning is an effective and safe technique to train robot policies\nin the real world because it does not depend on an expensive random exploration\nprocess. However, due to the lack of exploration, learning policies that\ngeneralize beyond the demonstrated behaviors is still an open challenge. We\npresent a novel imitation learning framework to enable robots to 1) learn\ncomplex real world manipulation tasks efficiently from a small number of human\ndemonstrations, and 2) synthesize new behaviors not contained in the collected\ndemonstrations. Our key insight is that multi-task domains often present a\nlatent structure, where demonstrated trajectories for different tasks intersect\nat common regions of the state space. We present Generalization Through\nImitation (GTI), a two-stage offline imitation learning algorithm that exploits\nthis intersecting structure to train goal-directed policies that generalize to\nunseen start and goal state combinations. In the first stage of GTI, we train a\nstochastic policy that leverages trajectory intersections to have the capacity\nto compose behaviors from different demonstration trajectories together. In the\nsecond stage of GTI, we collect a small set of rollouts from the unconditioned\nstochastic policy of the first stage, and train a goal-directed agent to\ngeneralize to novel start and goal configurations. We validate GTI in both\nsimulated domains and a challenging long-horizon robotic manipulation domain in\nthe real world. Additional results and videos are available at\nhttps://sites.google.com/view/gti2020/ .\n",
        "published": "2020",
        "authors": [
            "Ajay Mandlekar",
            "Danfei Xu",
            "Roberto Mart\u00edn-Mart\u00edn",
            "Silvio Savarese",
            "Li Fei-Fei"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.08003v2",
        "title": "CARPAL: Confidence-Aware Intent Recognition for Parallel Autonomy",
        "abstract": "  Predicting driver intentions is a difficult and crucial task for advanced\ndriver assistance systems. Traditional confidence measures on predictions often\nignore the way predicted trajectories affect downstream decisions for safe\ndriving. In this paper, we propose a novel multi-task intent recognition neural\nnetwork that predicts not only probabilistic driver trajectories, but also\nutility statistics associated with the predictions for a given downstream task.\nWe establish a decision criterion for parallel autonomy that takes into account\nthe role of driver trajectory prediction in real-time decision making by\nreasoning about estimated task-specific utility statistics. We further improve\nthe robustness of our system by considering uncertainties in downstream\nplanning tasks that may lead to unsafe decisions. We test our online system on\na realistic urban driving dataset, and demonstrate its advantage in terms of\nrecall and fall-out metrics compared to baseline methods, and demonstrate its\neffectiveness in intervention and warning use cases.\n",
        "published": "2020",
        "authors": [
            "Xin Huang",
            "Stephen G. McGill",
            "Jonathan A. DeCastro",
            "Luke Fletcher",
            "John J. Leonard",
            "Brian C. Williams",
            "Guy Rosman"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.10923v1",
        "title": "Autonomous UAV Navigation: A DDPG-based Deep Reinforcement Learning\n  Approach",
        "abstract": "  In this paper, we propose an autonomous UAV path planning framework using\ndeep reinforcement learning approach. The objective is to employ a self-trained\nUAV as a flying mobile unit to reach spatially distributed moving or static\ntargets in a given three dimensional urban area. In this approach, a Deep\nDeterministic Policy Gradient (DDPG) with continuous action space is designed\nto train the UAV to navigate through or over the obstacles to reach its\nassigned target. A customized reward function is developed to minimize the\ndistance separating the UAV and its destination while penalizing collisions.\nNumerical simulations investigate the behavior of the UAV in learning the\nenvironment and autonomously determining trajectories for different selected\nscenarios.\n",
        "published": "2020",
        "authors": [
            "Omar Bouhamed",
            "Hakim Ghazzai",
            "Hichem Besbes",
            "Yehia Massoud"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2003.11334v3",
        "title": "ACNMP: Skill Transfer and Task Extrapolation through Learning from\n  Demonstration and Reinforcement Learning via Representation Sharing",
        "abstract": "  To equip robots with dexterous skills, an effective approach is to first\ntransfer the desired skill via Learning from Demonstration (LfD), then let the\nrobot improve it by self-exploration via Reinforcement Learning (RL). In this\npaper, we propose a novel LfD+RL framework, namely Adaptive Conditional Neural\nMovement Primitives (ACNMP), that allows efficient policy improvement in novel\nenvironments and effective skill transfer between different agents. This is\nachieved through exploiting the latent representation learned by the underlying\nConditional Neural Process (CNP) model, and simultaneous training of the model\nwith supervised learning (SL) for acquiring the demonstrated trajectories and\nvia RL for new trajectory discovery. Through simulation experiments, we show\nthat (i) ACNMP enables the system to extrapolate to situations where pure LfD\nfails; (ii) Simultaneous training of the system through SL and RL preserves the\nshape of demonstrations while adapting to novel situations due to the shared\nrepresentations used by both learners; (iii) ACNMP enables order-of-magnitude\nsample-efficient RL in extrapolation of reaching tasks compared to the existing\napproaches; (iv) ACNMPs can be used to implement skill transfer between robots\nhaving different morphology, with competitive learning speeds and importantly\nwith less number of assumptions compared to the state-of-the-art approaches.\nFinally, we show the real-world suitability of ACNMPs through real robot\nexperiments that involve obstacle avoidance, pick and place and pouring\nactions.\n",
        "published": "2020",
        "authors": [
            "M. Tuluhan Akbulut",
            "Erhan Oztop",
            "M. Yunus Seker",
            "Honghu Xue",
            "Ahmet E. Tekden",
            "Emre Ugur"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.03979v1",
        "title": "Visual Prediction of Priors for Articulated Object Interaction",
        "abstract": "  Exploration in novel settings can be challenging without prior experience in\nsimilar domains. However, humans are able to build on prior experience quickly\nand efficiently. Children exhibit this behavior when playing with toys. For\nexample, given a toy with a yellow and blue door, a child will explore with no\nclear objective, but once they have discovered how to open the yellow door,\nthey will most likely be able to open the blue door much faster. Adults also\nexhibit this behavior when entering new spaces such as kitchens. We develop a\nmethod, Contextual Prior Prediction, which provides a means of transferring\nknowledge between interactions in similar domains through vision. We develop\nagents that exhibit exploratory behavior with increasing efficiency, by\nlearning visual features that are shared across environments, and how they\ncorrelate to actions. Our problem is formulated as a Contextual Multi-Armed\nBandit where the contexts are images, and the robot has access to a\nparameterized action space. Given a novel object, the objective is to maximize\nreward with few interactions. A domain which strongly exhibits correlations\nbetween visual features and motion is kinemetically constrained mechanisms. We\nevaluate our method on simulated prismatic and revolute joints.\n",
        "published": "2020",
        "authors": [
            "Caris Moses",
            "Michael Noseworthy",
            "Leslie Pack Kaelbling",
            "Tom\u00e1s Lozano-P\u00e9rez",
            "Nicholas Roy"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.04271v1",
        "title": "Multi-Task Reinforcement Learning based Mobile Manipulation Control for\n  Dynamic Object Tracking and Grasping",
        "abstract": "  Agile control of mobile manipulator is challenging because of the high\ncomplexity coupled by the robotic system and the unstructured working\nenvironment. Tracking and grasping a dynamic object with a random trajectory is\neven harder. In this paper, a multi-task reinforcement learning-based mobile\nmanipulation control framework is proposed to achieve general dynamic object\ntracking and grasping. Several basic types of dynamic trajectories are chosen\nas the task training set. To improve the policy generalization in practice,\nrandom noise and dynamics randomization are introduced during the training\nprocess. Extensive experiments show that our policy trained can adapt to unseen\nrandom dynamic trajectories with about 0.1m tracking error and 75\\% grasping\nsuccess rate of dynamic objects. The trained policy can also be successfully\ndeployed on a real mobile manipulator.\n",
        "published": "2020",
        "authors": [
            "Cong Wang",
            "Qifeng Zhang",
            "Qiyan Tian",
            "Shuo Li",
            "Xiaohui Wang",
            "David Lane",
            "Yvan Petillot",
            "Ziyang Hong",
            "Sen Wang"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.06248v2",
        "title": "Graph Neural Networks for Motion Planning",
        "abstract": "  This paper investigates the feasibility of using Graph Neural Networks (GNNs)\nfor classical motion planning problems. We propose guiding both continuous and\ndiscrete planning algorithms using GNNs' ability to robustly encode the\ntopology of the planning space using a property called permutation invariance.\nWe present two techniques, GNNs over dense fixed graphs for low-dimensional\nproblems and sampling-based GNNs for high-dimensional problems. We examine the\nability of a GNN to tackle planning problems such as identifying critical nodes\nor learning the sampling distribution in Rapidly-exploring Random Trees (RRT).\nExperiments with critical sampling, a pendulum and a six DoF robot arm show\nGNNs improve on traditional analytic methods as well as learning approaches\nusing fully-connected or convolutional neural networks.\n",
        "published": "2020",
        "authors": [
            "Arbaaz Khan",
            "Alejandro Ribeiro",
            "Vijay Kumar",
            "Anthony G. Francis"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.06444v2",
        "title": "Learning compositional models of robot skills for task and motion\n  planning",
        "abstract": "  The objective of this work is to augment the basic abilities of a robot by\nlearning to use sensorimotor primitives to solve complex long-horizon\nmanipulation problems. This requires flexible generative planning that can\ncombine primitive abilities in novel combinations and thus generalize across a\nwide variety of problems. In order to plan with primitive actions, we must have\nmodels of the actions: under what circumstances will executing this primitive\nsuccessfully achieve some particular effect in the world?\n  We use, and develop novel improvements on, state-of-the-art methods for\nactive learning and sampling. We use Gaussian process methods for learning the\nconstraints on skill effectiveness from small numbers of expensive-to-collect\ntraining examples. Additionally, we develop efficient adaptive sampling methods\nfor generating a comprehensive and diverse sequence of continuous candidate\ncontrol parameter values (such as pouring waypoints for a cup) during planning.\nThese values become end-effector goals for traditional motion planners that\nthen solve for a full robot motion that performs the skill. By using learning\nand planning methods in conjunction, we take advantage of the strengths of each\nand plan for a wide variety of complex dynamic manipulation tasks. We\ndemonstrate our approach in an integrated system, combining traditional\nrobotics primitives with our newly learned models using an efficient robot task\nand motion planner. We evaluate our approach both in simulation and in the real\nworld through measuring the quality of the selected primitive actions. Finally,\nwe apply our integrated system to a variety of long-horizon simulated and\nreal-world manipulation problems.\n",
        "published": "2020",
        "authors": [
            "Zi Wang",
            "Caelan Reed Garrett",
            "Leslie Pack Kaelbling",
            "Tom\u00e1s Lozano-P\u00e9rez"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.06620v1",
        "title": "From proprioception to long-horizon planning in novel environments: A\n  hierarchical RL model",
        "abstract": "  For an intelligent agent to flexibly and efficiently operate in complex\nenvironments, they must be able to reason at multiple levels of temporal,\nspatial, and conceptual abstraction. At the lower levels, the agent must\ninterpret their proprioceptive inputs and control their muscles, and at the\nhigher levels, the agent must select goals and plan how they will achieve those\ngoals. It is clear that each of these types of reasoning is amenable to\ndifferent types of representations, algorithms, and inputs. In this work, we\nintroduce a simple, three-level hierarchical architecture that reflects these\ndistinctions. The low-level controller operates on the continuous\nproprioceptive inputs, using model-free learning to acquire useful behaviors.\nThese in turn induce a set of mid-level dynamics, which are learned by the\nmid-level controller and used for model-predictive control, to select a\nbehavior to activate at each timestep. The high-level controller leverages a\ndiscrete, graph representation for goal selection and path planning to specify\ntargets for the mid-level controller. We apply our method to a series of\nnavigation tasks in the Mujoco Ant environment, consistently demonstrating\nsignificant improvements in sample-efficiency compared to prior model-free,\nmodel-based, and hierarchical RL methods. Finally, as an illustrative example\nof the advantages of our architecture, we apply our method to a complex maze\nenvironment that requires efficient exploration and long-horizon planning.\n",
        "published": "2020",
        "authors": [
            "Nishad Gothoskar",
            "Miguel L\u00e1zaro-Gredilla",
            "Dileep George"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.07508v1",
        "title": "Realistic Physics Based Character Controller",
        "abstract": "  Over the course of the last several years there was a strong interest in\napplication of modern optimal control techniques to the field of character\nanimation. This interest was fueled by introduction of efficient learning based\nalgorithms for policy optimization, growth in computation power, and game\nengine improvements. It was shown that it is possible to generate natural\nlooking control of a character by using two ingredients. First, the simulated\nagent must adhere to a motion capture dataset. And second, the character aims\nto track the control input from the user. The paper aims at closing the gap\nbetween the researchers and users by introducing an open source implementation\nof physics based character control in Unity framework that has a low entry\nbarrier and a steep learning curve.\n",
        "published": "2020",
        "authors": [
            "Joe Booth",
            "Vladimir Ivanov"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.09540v1",
        "title": "COLREG-Compliant Collision Avoidance for Unmanned Surface Vehicle using\n  Deep Reinforcement Learning",
        "abstract": "  Path Following and Collision Avoidance, be it for unmanned surface vessels or\nother autonomous vehicles, are two fundamental guidance problems in robotics.\nFor many decades, they have been subject to academic study, leading to a vast\nnumber of proposed approaches. However, they have mostly been treated as\nseparate problems, and have typically relied on non-linear first-principles\nmodels with parameters that can only be determined experimentally. The rise of\nDeep Reinforcement Learning (DRL) in recent years suggests an alternative\napproach: end-to-end learning of the optimal guidance policy from scratch by\nmeans of a trial-and-error based approach. In this article, we explore the\npotential of Proximal Policy Optimization (PPO), a DRL algorithm with\ndemonstrated state-of-the-art performance on Continuous Control tasks, when\napplied to the dual-objective problem of controlling an underactuated\nAutonomous Surface Vehicle in a COLREGs compliant manner such that it follows\nan a priori known desired path while avoiding collisions with other vessels\nalong the way. Based on high-fidelity elevation and AIS tracking data from the\nTrondheim Fjord, an inlet of the Norwegian sea, we evaluate the trained agent's\nperformance in challenging, dynamic real-world scenarios where the ultimate\nsuccess of the agent rests upon its ability to navigate non-uniform marine\nterrain while handling challenging, but realistic vessel encounters.\n",
        "published": "2020",
        "authors": [
            "Eivind Meyer",
            "Amalie Heiberg",
            "Adil Rasheed",
            "Omer San"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.09786v1",
        "title": "Reinforcement Learning with Uncertainty Estimation for Tactical\n  Decision-Making in Intersections",
        "abstract": "  This paper investigates how a Bayesian reinforcement learning method can be\nused to create a tactical decision-making agent for autonomous driving in an\nintersection scenario, where the agent can estimate the confidence of its\nrecommended actions. An ensemble of neural networks, with additional randomized\nprior functions (RPF), are trained by using a bootstrapped experience replay\nmemory. The coefficient of variation in the estimated $Q$-values of the\nensemble members is used to approximate the uncertainty, and a criterion that\ndetermines if the agent is sufficiently confident to make a particular decision\nis introduced. The performance of the ensemble RPF method is evaluated in an\nintersection scenario, and compared to a standard Deep Q-Network method. It is\nshown that the trained ensemble RPF agent can detect cases with high\nuncertainty, both in situations that are far from the training distribution,\nand in situations that seldom occur within the training distribution. In this\nstudy, the uncertainty information is used to choose safe actions in unknown\nsituations, which removes all collisions from within the training distribution,\nand most collisions outside of the distribution.\n",
        "published": "2020",
        "authors": [
            "Carl-Johan Hoel",
            "Tommy Tram",
            "Jonas Sj\u00f6berg"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.12136v2",
        "title": "Safe Reinforcement Learning via Curriculum Induction",
        "abstract": "  In safety-critical applications, autonomous agents may need to learn in an\nenvironment where mistakes can be very costly. In such settings, the agent\nneeds to behave safely not only after but also while learning. To achieve this,\nexisting safe reinforcement learning methods make an agent rely on priors that\nlet it avoid dangerous situations during exploration with high probability, but\nboth the probabilistic guarantees and the smoothness assumptions inherent in\nthe priors are not viable in many scenarios of interest such as autonomous\ndriving. This paper presents an alternative approach inspired by human\nteaching, where an agent learns under the supervision of an automatic\ninstructor that saves the agent from violating constraints during learning. In\nthis model, we introduce the monitor that neither needs to know how to do well\nat the task the agent is learning nor needs to know how the environment works.\nInstead, it has a library of reset controllers that it activates when the agent\nstarts behaving dangerously, preventing it from doing damage. Crucially, the\nchoices of which reset controller to apply in which situation affect the speed\nof agent learning. Based on observing agents' progress, the teacher itself\nlearns a policy for choosing the reset controllers, a curriculum, to optimize\nthe agent's final policy reward. Our experiments use this framework in two\nenvironments to induce curricula for safe and efficient learning.\n",
        "published": "2020",
        "authors": [
            "Matteo Turchetta",
            "Andrey Kolobov",
            "Shital Shah",
            "Andreas Krause",
            "Alekh Agarwal"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.12983v2",
        "title": "dm_control: Software and Tasks for Continuous Control",
        "abstract": "  The dm_control software package is a collection of Python libraries and task\nsuites for reinforcement learning agents in an articulated-body simulation. A\nMuJoCo wrapper provides convenient bindings to functions and data structures.\nThe PyMJCF and Composer libraries enable procedural model manipulation and task\nauthoring. The Control Suite is a fixed set of tasks with standardised\nstructure, intended to serve as performance benchmarks. The Locomotion\nframework provides high-level abstractions and examples of locomotion tasks. A\nset of configurable manipulation tasks with a robot arm and snap-together\nbricks is also included. dm_control is publicly available at\nhttps://www.github.com/deepmind/dm_control\n",
        "published": "2020",
        "authors": [
            "Yuval Tassa",
            "Saran Tunyasuvunakool",
            "Alistair Muldal",
            "Yotam Doron",
            "Piotr Trochim",
            "Siqi Liu",
            "Steven Bohez",
            "Josh Merel",
            "Tom Erez",
            "Timothy Lillicrap",
            "Nicolas Heess"
        ]
    },
    {
        "id": "http://arxiv.org/abs/2006.13704v1",
        "title": "Efficient Sampling-Based Maximum Entropy Inverse Reinforcement Learning\n  with Application to Autonomous Driving",
        "abstract": "  In the past decades, we have witnessed significant progress in the domain of\nautonomous driving. Advanced techniques based on optimization and reinforcement\nlearning (RL) become increasingly powerful at solving the forward problem:\ngiven designed reward/cost functions, how should we optimize them and obtain\ndriving policies that interact with the environment safely and efficiently.\nSuch progress has raised another equally important question: \\emph{what should\nwe optimize}? Instead of manually specifying the reward functions, it is\ndesired that we can extract what human drivers try to optimize from real\ntraffic data and assign that to autonomous vehicles to enable more naturalistic\nand transparent interaction between humans and intelligent agents. To address\nthis issue, we present an efficient sampling-based maximum-entropy inverse\nreinforcement learning (IRL) algorithm in this paper. Different from existing\nIRL algorithms, by introducing an efficient continuous-domain trajectory\nsampler, the proposed algorithm can directly learn the reward functions in the\ncontinuous domain while considering the uncertainties in demonstrated\ntrajectories from human drivers. We evaluate the proposed algorithm on real\ndriving data, including both non-interactive and interactive scenarios. The\nexperimental results show that the proposed algorithm achieves more accurate\nprediction performance with faster convergence speed and better generalization\ncompared to other baseline IRL algorithms.\n",
        "published": "2020",
        "authors": [
            "Zheng Wu",
            "Liting Sun",
            "Wei Zhan",
            "Chenyu Yang",
            "Masayoshi Tomizuka"
        ]
    }
]